[
  {
    "id":2412.00129,
    "research_type":"applied",
    "start_id":"b23",
    "start_title":"DARWIN Series: Domain Specific Large Language Models for Natural Science",
    "start_abstract":"Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b29"
      ],
      "title":[
        "Study of Fermion pair production in e+e- collisions at 130-183 GeV"
      ],
      "abstract":[
        "The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings."
      ],
      "categories":[
        "astro-ph.HE"
      ]
    },
    "list":{
      "title":[
        "Non-Affine Extensions of the Raychaudhuri Equation in the K-essence\n  Framework",
        "Time-series attribution maps with regularized contrastive learning",
        "Ion-kinetic-energy sampling in a 22-pole trap using ring-electrode\n  evaporation",
        "Improved Quantum Computation using Operator Backpropagation",
        "Reservoir Computing and Photoelectrochemical Sensors: A Marriage of\n  Convenience",
        "On the localization length of finite-volume random block Schr\\\"odinger\n  operators",
        "Theory of neutrino slow flavor evolution. Part II. Space-time evolution\n  of linear instabilities",
        "Thermodynamic analysis and shadow bound of black holes surrounded by a\n  dark matter halo",
        "Canonical forms of oriented matroids",
        "Beyond surveys: A High-Precision Wealth Inequality Mapping of China's\n  Rural Households Derived from Satellite and Street View Imageries",
        "More on the corner-vector construction for spherical designs",
        "Enabling GPU Portability into the Numba-JITed Monte Carlo Particle\n  Transport Code MC\/DC",
        "BCAT: A Block Causal Transformer for PDE Foundation Models for Fluid\n  Dynamics",
        "A new practical and effective source-independent full-waveform inversion\n  with a velocity-distribution supported deep image prior: Applications to two\n  real datasets",
        "Particle-based plasma simulation using a graph neural network",
        "A Statistical Interpretation of Multi-Item Rating and Recommendation\n  Problems",
        "Conjecture on Supersequence Lower Bound related to Connell Sequence",
        "A spectral clustering-type algorithm for the consistent estimation of\n  the Hurst distribution in moderately high dimensions",
        "Limit theorems for the fluctuation of the mixed elephant random walk in\n  the superdiffusive case",
        "Complete Hamiltonian Framework of Relativistic Hierarchical Triple\n  Systems: Capabilities and Limitations of Secular Perturbation Theory",
        "Collapsing of K3 Surfaces and Special K\\\"ahler Structures",
        "Wavelength-Dependent Photodissociation of Iodomethylbutane",
        "Finding Quasars Behind the Galactic Plane: Spectroscopic Identifications\n  of ~1300 New Quasars at |b|<=20 degree from LAMOST DR10",
        "First-Ever Deployment of a SiPM-on-Tile Calorimeter in a Collider: A\n  Parasitic Test with 200 GeV $pp$ Collisions at RHIC",
        "Time-Resolved Measurements of Cumulative Effects in Gas Dynamics Induced\n  by High-Repetition-Rate Femtosecond Laser Filamentation",
        "Pseudo-Goldstone Dark Matter from Primordial Black Holes: Gravitational\n  Wave Signatures and Implications for KM3-230213A Event at KM3NeT",
        "Quantum Schrodinger bridges: large deviations and time-symmetric\n  ensembles",
        "Dissecting stellar populations with manifold learning I. Validation of\n  the method on a synthetic Milky Way-like galaxy",
        "Late-time cosmic acceleration from quantum gravity"
      ],
      "abstract":[
        "The Raychaudhuri equation (RE) governs the evolution of geodesic congruences\nin curved spacetime. Here, we extend the classical RE by incorporating\nnon-affine parametrization within the framework of k-essence scalar field\ndynamics. The non-affine parametrization introduces deviations from purely\ngeodesic congruences (motion), allowing investigation of non-gravitational\ninteractions and external forces. Using a DBI-type k-essence Lagrangian, we\nanalyze the behavior of non-geodesic flow curves in the background FLRW metric,\nelucidating their role in cosmic acceleration and structure formation. The\nemergent metric formalism is used to derive a modified RE, revealing new\ngeometric and dynamical features induced by the k-essence field. The\ncosmological implications of our model are studied by constraining key\nparameters using observational data from the PANTHEON+SHOES+BAO and Hubble\ndatasets. Our results suggest that non-affine parametrization, coupled with\nk-essence dynamics, can provide a viable explanation for late-time cosmic\nacceleration while addressing the Hubble tension. Further, we reinterpret the\nmodified RE as an anti-damped harmonic oscillator, revealing quantum-like\neffects in cosmic expansion. These results suggest a deep connection between\nscalar field dynamics and modified gravity, offering new perspectives on the\nnature of the universe's expansion history.",
        "Gradient-based attribution methods aim to explain decisions of deep learning\nmodels but so far lack identifiability guarantees. Here, we propose a method to\ngenerate attribution maps with identifiability guarantees by developing a\nregularized contrastive learning algorithm trained on time-series data plus a\nnew attribution method called Inverted Neuron Gradient (collectively named\nxCEBRA). We show theoretically that xCEBRA has favorable properties for\nidentifying the Jacobian matrix of the data generating process. Empirically, we\ndemonstrate robust approximation of zero vs. non-zero entries in the\nground-truth attribution map on synthetic datasets, and significant\nimprovements across previous attribution methods based on feature ablation,\nShapley values, and other gradient-based methods. Our work constitutes a first\nexample of identifiable inference of time-series attribution maps and opens\navenues to a better understanding of time-series data, such as for neural\ndynamics and decision-processes within neural networks.",
        "We present an experimental method for the characterization of the kinetic\nenergies of ions confined in a 22-pole radio frequency trap by inducing a small\npotential barrier using the surrounding ring electrodes, allowing the selective\nextraction of ions. Energy sampling experiments have been performed on buffer\ngas thermalized He$^+$ ions at trap temperatures between 10-180 K, resulting in\ndistinct extraction curves as a function of the potential barrier, and a\ndifferentiated behavior depending on the escape time from the trap. The\nexperiments are complemented by Monte Carlo simulations of the ion trajectories\ninside the calculated trap potential and allow us to investigate the properties\nof the sampling method, the role of ion motion coupling, and the impact of\nresidual buffer gas collisions on the observed results. The technique has also\nbeen successfully applied to identify energetic H$_3^+$ ions produced in an\nexothermic reaction inside the trap. Upon calibration, this method can provide\nrelative kinetic energy distributions or be used to filter the maximum desired\nkinetic energy of the ions inside the trap.",
        "Decoherence of quantum hardware is currently limiting its practical\napplications. At the same time, classical algorithms for simulating quantum\ncircuits have progressed substantially. Here, we demonstrate a hybrid framework\nthat integrates classical simulations with quantum hardware to improve the\ncomputation of an observable's expectation value by reducing the quantum\ncircuit depth. In this framework, a quantum circuit is partitioned into two\nsubcircuits: one that describes the backpropagated Heisenberg evolution of an\nobservable, executed on a classical computer, while the other is a\nSchr\\\"odinger evolution run on quantum processors. The overall effect is to\nreduce the depths of the circuits executed on quantum devices, trading this\nwith classical overhead and an increased number of circuit executions. We\ndemonstrate the effectiveness of this method on a Hamiltonian simulation\nproblem, achieving more accurate expectation value estimates compared to using\nquantum hardware alone.",
        "Sensing technology is an important aspect of information processing. Current\ndevelopment in artificial intelligence systems (especially those aimed at\nmedical and environmental applications) requires a lot of data on the chemical\ncomposition of biological fluids or environmental samples. These complex\nmatrices require advanced sensing devices, and photoelectrochemical ones seem\nto have potential to overcome at least some of the obstacles. Furthermore, the\ndevelopment of artificial intelligence (AI) technology for autonomous robotics\nrequires technology mimicking human senses, also those operating at the\nmolecular level, such as gustation and olfaction. Again, photoelectrochemical\nsensing can provide some suitable solutions. In this review, we introduce the\nidea of integration of photoelectrochemical sensors with some unconventional\ncomputing paradigm - reservoir computing. This approach should not only boost\nthe performance of the sensors itself, but also open new pathways through\nscience. Integration of sensing devices with computing systems will also\ncontribute to a better understanding (or at least mimicking) of the human\nsenses and neuromorphic sensory information processing. Although reservoir\nsystems can be considered magic \"black boxes\" and their operation is at the\nsame time simple and hard to comprehend, this combination is expected to open a\nnew era of effective information harvesting and processing systems.",
        "We study a general class of random block Schr\\\"odinger operators (RBSOs) in\ndimensions 1 and 2, which naturally extend the Anderson model by replacing the\nrandom potential with a random block potential. Specifically, we focus on two\nRBSOs -- the block Anderson and Wegner orbital models -- defined on the\n$d$-dimensional torus $(\\mathbb Z\/L\\mathbb Z)^d$. They take the form $H=V +\n\\lambda \\Psi$, where $V$ is a block potential with i.i.d. $W^d\\times W^d$\nGaussian diagonal blocks, $\\Psi$ describes interactions between neighboring\nblocks, and $\\lambda>0$ is a coupling parameter. We normalize the blocks of\n$\\Psi$ so that each block has a Hilbert-Schmidt norm of the same order as the\nblocks of $V$. Assuming $W\\ge L^\\delta$ for a small constant $\\delta>0$ and\n$\\lambda\\gg W^{-d\/2}$, we establish the following results. In dimension $d=2$,\nwe prove delocalization and quantum unique ergodicity for bulk eigenvectors,\nwhich, combined with the localization result from arXiv:1608.02922 under the\ncondition $\\lambda\\ll W^{-d\/2}$, rigorously establishes the Anderson\nlocalization-delocalization transition as $\\lambda$ crosses the critical\nthreshold $W^{-d\/2}$. In dimension $d=1$, we show that the localization length\nof bulk eigenvectors is at least of order $(W\\lambda)^2$, which is conjectured\nto be the correct scaling for the localization length.",
        "Slow flavor evolution (defined as driven by neutrino masses and not\nnecessarily ``slow'') is receiving fresh attention in the context of compact\nastrophysical environments. In Part~I of this series, we have studied the\nslow-mode dispersion relation following our recently developed analogy to\nplasma waves. The concept of resonance between flavor waves in the linear\nregime and propagating neutrinos is the defining feature of this approach. It\nis best motivated for weak instabilities, which probably is the most relevant\nregime in self-consistent astrophysical environments because these will try to\neliminate the cause of instability. We here go beyond the dispersion relation\nalone (which by definition applies to infinite media) and consider the group\nvelocities of unstable modes that determines whether the instability relaxes\nwithin the region where it first appears (absolute), or away from it\n(convective). We show that all weak instabilities are convective so that their\nfurther evolution is not local. Therefore, studying their consequences\nnumerically in small boxes from given initial conditions may not always be\nappropriate.",
        "We perform the thermodynamic analysis of a black hole (BH) immersed in a dark\nmatter halo (DMH), showing that the BH could not be in thermal equilibrium with\nthe DMH in any regions outside the event horizon. This means that the\nthermodynamic influence of the environment (DMH) is relatively small on the BH\nand it does not alter the nature of BH with negative heat capacity. The\nNewtonian ($1\/a_0$) approximation gives us a correct thermodynamic description\nfor the BH surrounded by DMH because the first law of thermodynamics and Smarr\nformula are satisfied. We use the Newtonian Helmholtz free energy to show that\nthere is no phase transition to other BH with positive heat capacity surrounded\nby a DMH. We investigate the shadow bound of favored region for the BH immersed\nin the DMH by introducing three critical impact parameters.",
        "Positive geometries are semialgebraic sets equipped with a canonical\ndifferential form whose residues mirror the boundary structure of the geometry.\nEvery full-dimensional projective polytope is a positive geometry. Motivated by\nthe canonical forms of polytopes, we construct a canonical form for any tope of\nan oriented matroid, inside the Orlik--Solomon algebra of the underlying\nmatroid. Using these canonical forms, we construct bases for the Orlik--Solomon\nalgebra of a matroid, and for the Aomoto cohomology. These bases of canonical\nforms are a foundational input in the theory of matroid amplitudes introduced\nby the second author.",
        "Wide coverage and high-precision rural household wealth data is an important\nsupport for the effective connection between the national macro rural\nrevitalization policy and micro rural entities, which helps to achieve precise\nallocation of national resources. However, due to the large number and wide\ndistribution of rural areas, wealth data is difficult to collect and scarce in\nquantity. Therefore, this article attempts to integrate \"sky\" remote sensing\nimages with \"ground\" village street view imageries to construct a fine-grained\n\"computable\" technical route for rural household wealth. With the intelligent\ninterpretation of rural houses as the core, the relevant wealth elements of\nimage data were extracted and identified, and regressed with the household\nwealth indicators of the benchmark questionnaire to form a high-precision\ntownship scale wealth prediction model (r=0.85); Furthermore, a national and\ntownship scale map of rural household wealth in China was promoted and drawn.\nBased on this, this article finds that there is a \"bimodal\" pattern in the\ndistribution of wealth among rural households in China, which is reflected in a\npolarization feature of \"high in the south and low in the north, and high in\nthe east and low in the west\" in space. This technological route may provide\nalternative solutions with wider spatial coverage and higher accuracy for\nhigh-cost manual surveys, promote the identification of shortcomings in rural\nconstruction, and promote the precise implementation of rural policies.",
        "This paper explores a full generalization of the classical corner-vector\nmethod for constructing weighted spherical designs, which we call the {\\it\ngeneralized corner-vector method}. First we establish a uniform upper bound for\nthe degree of designs obtained from the proposed method. Our proof is a hybrid\nargument that employs techniques in analysis and combinatorics, especially a\nfamous result by Xu(1998) on the interrelation between spherical designs and\nsimplical designs, and the cross-ratio comparison method for Hilbert identities\nintroduced by Nozaki and Sawa(2013). We extensively study conditions for the\nexistence of designs obtained from our method, and present many curious\nexamples of degree $7$ through $13$, some of which are, to our surprise,\ncharacterized in terms of integral lattices.",
        "The Center for Exascale Monte Carlo Neutron Transport is developing Monte\nCarlo \/ Dynamic Code (MC\/DC) as a portable Monte Carlo neutron transport\npackage for rapid numerical methods exploration on CPU- and GPU-based\nhigh-performance computers. In this paper, we describe MC\/DC's current\nevent-based GPU algorithm as well as the just-in-time (JIT) compilation scheme\nwe use to enable GPU operability on Nvidia and AMD GPUs from MC\/DC's Python\nsource. To analyze performance, we conduct runtime tests of the C5G7\nk-eigenvalue benchmark problem and a continuous-energy infinite pin cell on\nNvidia Tesla V100 GPU, AMD MI250X GPU, and the AMD MI300A APU and make\ncomparison to a dual-socket Intel Xeon Sapphire Rapid CPU node. We found that\nfor the multi-group C5G7 benchmark problem, we respectively see a 15$\\times$,\n0.7$\\times$, 12$\\times$ speedup on a V100, MI250X, and MI300A over 112 Intel\nXeon CPU cores. For the continuous-energy infinite pin-cell benchmark, we found\nspeedups of 5$\\times$, 3$\\times$, 4$\\times$ on a V100, MI250X, and MI300A,\nrespectively, over the same CPU node.",
        "We introduce BCAT, a PDE foundation model designed for autoregressive\nprediction of solutions to two dimensional fluid dynamics problems. Our\napproach uses a block causal transformer architecture to model next frame\npredictions, leveraging previous frames as contextual priors rather than\nrelying solely on sub-frames or pixel-based inputs commonly used in image\ngeneration methods. This block causal framework more effectively captures the\nspatial dependencies inherent in nonlinear spatiotemporal dynamics and physical\nphenomena. In an ablation study, next frame prediction demonstrated a 2.9x\naccuracy improvement over next token prediction. BCAT is trained on a diverse\nrange of fluid dynamics datasets, including incompressible and compressible\nNavier-Stokes equations across various geometries and parameter regimes, as\nwell as the shallow-water equations. The model's performance was evaluated on 6\ndistinct downstream prediction tasks and tested on about 8K trajectories to\nmeasure robustness on a variety of fluid dynamics simulations. BCAT achieved an\naverage relative error of 1.92% across all evaluation tasks, outperforming\nprior approaches on standard benchmarks.",
        "Full-waveform inversion (FWI) is an advanced technique for reconstructing\nhigh-resolution subsurface physical parameters by progressively minimizing the\ndiscrepancy between observed and predicted seismic data. However, conventional\nFWI encounters challenges in real data applications, primarily due to its\nconventional objective of direct measurements of the data misfit. Accurate\nestimation of the source wavelet is essential for effective data fitting,\nalongside the need for low-frequency data and a reasonable initial model to\nprevent cycle skipping. Additionally, wave equation solvers often struggle to\naccurately simulate the amplitude of observed data in real applications. To\naddress these challenges, we introduce a correlation-based source-independent\nobjective function for FWI that aims to mitigate source uncertainty and\namplitude dependency, which effectively enhances its practicality for real data\napplications. We develop a deep-learning framework constrained by this new\nobjective function with a velocity-distribution supported deep image prior,\nwhich reparameterizes velocity inversion into trainable parameters within an\nautoencoder, thereby reducing the nonlinearity in the conventional FWI's\nobjective function. We demonstrate the superiority of our proposed method using\nsynthetic data from benchmark velocity models and, more importantly, two real\ndatasets. These examples highlight its effectiveness and practicality even\nunder challenging conditions, such as missing low frequencies, a crude initial\nvelocity model, and an incorrect source wavelet.",
        "A surrogate model for particle-in-cell plasma simulations based on a graph\nneural network is presented. The graph is constructed in such a way as to\nenable the representation of electromagnetic fields on a fixed spatial grid.\nThe model is applied to simulate beams of electrons in one dimension over a\nwide range of temperatures, drift momenta and densities, and is shown to\nreproduce two-stream instabilities - a common and fundamental plasma\ninstability. Qualitatively, the characteristic phase-space mixing of\ncounterpropagating electron beams is observed. Quantitatively, the model's\nperformance is evaluated in terms of the accuracy of its predictions of number\ndensity distributions, the electric field, and their Fourier decompositions,\nparticularly the growth rate of the fastest-growing unstable mode, as well as\nparticle position, momentum distributions, energy conservation and run time.\nThe model achieves high accuracy with a time step longer than conventional\nsimulation by two orders of magnitude. This work demonstrates that complex\nplasma dynamics can be learned and shows promise for the development of fast\ndifferentiable simulators suitable for solving forward and inverse problems in\nplasma physics.",
        "Ordinal user-provided ratings across multiple items are frequently\nencountered in both scientific and commercial applications. Whilst recommender\nsystems are known to do well on these type of data from a predictive point of\nview, their typical reliance on large sample sizes and frequent lack of\ninterpretability and uncertainty quantification limits their applicability in\ninferential problems. Taking a fully Bayesian approach, this article introduces\na novel statistical method that is designed with interpretability and\nuncertainty quantification in mind. Whilst parametric assumptions ensure that\nthe method is applicable to data with modest sample sizes, the model is\nsimultaneously designed to remain flexible in order to handle a wide variety of\nsituations. Model performance, i.e. parameter estimation and prediction, is\nshown by means of a simulation study, both on simulated data and against\ncommonly used recommender systems on real data. These simulations indicate that\nthe proposed method performs competitively. Finally, to illustrate the\napplicability of the proposed method on real life problems that are of interest\nto economists, the method is applied on speed dating data, where novel insights\ninto the partner preference problem are obtained. An R package containing the\nproposed methodology can be found on\nhttps:\/\/CRAN.R-project.org\/package=StatRec.",
        "This paper proves the minimum size of a supersequence over a set of eight\nelements is 52. This disproves a conjecture that the lower bound of the\nsupersequence is the partial sum of the geometric Connell sequence. By studying\nthe internal distribution of individual elements within sub-strings of the\nsupersequence called segments, the proof provides important results on the\ninternal structure that could help to understand the general lower bound\nproblem for finite sets.",
        "Scale invariance (fractality) is a prominent feature of the large-scale\nbehavior of many stochastic systems. In this work, we construct an algorithm\nfor the statistical identification of the Hurst distribution (in particular,\nthe scaling exponents) undergirding a high-dimensional fractal system. The\nalgorithm is based on wavelet random matrices, modified spectral clustering and\na model selection step for picking the value of the clustering precision\nhyperparameter. In a moderately high-dimensional regime where the dimension,\nthe sample size and the scale go to infinity, we show that the algorithm\nconsistently estimates the Hurst distribution. Monte Carlo simulations show\nthat the proposed methodology is efficient for realistic sample sizes and\noutperforms another popular clustering method based on mixed-Gaussian modeling.\nWe apply the algorithm in the analysis of real-world macroeconomic time series\nto unveil evidence for cointegration.",
        "Motivated by the previous results by Coletti--de Lima--Gava--Luiz (2020) and\nShiozawa (2022), we study the fluctuation of the mixed elephant random walk in\nthe superdiffusive case, and prove the Central Limit Theorem and the Law of\nIterated Logarithm with subtracting a random drift.",
        "Relativistic secular perturbation theory has ignited significant interest in\nuncovering intricate cross-term effects, especially the interplay between 1PN\nand quadrupole terms. While most existing studies rely on the Lagrangian\nplanetary perturbation method for computing cross terms, a comprehensive\nHamiltonian framework for the field has been missing. In this work, we\nintroduce a framework based on von Zeipel transformation, utilizing two\nsequential canonical transformations to systematically compute cross terms to\narbitrary orders. Our results reveal secular cross terms up to\nquadrupole-squared order, showcasing remarkable consistency with both the\nLagrangian method [1] and the effective-field-theory approach [2]. We present\nleading-order periodic cross terms arising from the interactions between 1PN\nand quadrupole, and present estimates of higher-order cross terms. It is\ndemonstrated that this method not only accurately predicts the long-term\nevolution of hierarchical systems but also captures fast oscillations observed\nin N-body simulations. We identify and validate resonances caused by\nquadrupole-squared effects, highlighting both consistencies and discrepancies\nwhen compared to N-body simulations. These discrepancies underscore the\nimportance of mean-motion resonances, a factor overlooked in current secular\nperturbation frameworks. Finally, we provide a comprehensive review of the\nsubtleties and limitations inherent to secular perturbation theory, paving the\nway for future research and advancements in this field.",
        "In this article, we classify all 2-dimensional Gromov-Hausdorff limit spaces\nof hyperk\\\"ahler K3 surfaces and explore their relations with Jacobian elliptic\nK3 surfaces.",
        "Ultrashort XUV pulses of the Free-Electron-LASer in Hamburg (FLASH) were used\nto investigate laser-induced fragmentation patterns of the prototypical chiral\nmolecule 1-iodo-2-methyl-butane (C$_5$H$_{11}$I) in a pump-probe scheme. Ion\nvelocity-map images and mass spectra of optical-laser-induced fragmentation\nwere obtained for subsequent FEL exposure with photon energies of 63 eV and 75\neV. These energies specifically address the iodine 4d edge of neutral and\nsingly charged iodine, respectively. The presented ion spectra for two optical\npump-laser wavelengths, i.e., 800 nm and 267 nm, reveal substantially different\ncationic fragment yields in dependence on the wavelength and intensity. For the\ncase of 800-nm-initiated fragmentation, the molecule dissociates notably slower\nthan for the 267-nm pump. The results underscore the importance of considering\noptical-laser wavelength and intensity in the dissociation dynamics of this\nprototypical chiral molecule that is a promising candidate for future studies\nof its asymmetric nature.",
        "Quasars behind the Galactic plane (GPQs) are excellent tracers to probe the\nchemistry and kinematics of the interstellar\/intergalactic medium (ISM\/IGM) of\nthe Milky Way along sight lines via absorption line spectroscopy. Moreover, the\nquasars located at low Galactic latitudes will fill the gap in the spatial\ndistribution of known quasars near the Galactic plane, and can be used to\nconstruct an astrometric reference frame for accurate measurements of proper\nmotions (PMs) of stars, and substructures of the Milky Way. We started a survey\nof background quasars in the low Galactic latitude region since the LAMOST\nphase II survey in 2017. Quasar candidates have been selected from the optical\nand infrared photometric data of Pan-STARRS1 and WISE surveys based on their\nvariability and color properties. In this paper, we present a sample of 1982\nspectroscopically confirmed GPQs with |b| <= 20 degree based on LAMOST Data\nRelease 10 (DR10). Among them, 1338 are newly discovered. Most GPQs are located\naround 240<l<90 degree, and the spatial distributions are non-uniform. These\nGPQs have a magnitude distribution with a peak at i-mag 19.0, and mostly around\n18.0-19.5mag. The peak of redshift distributions is around ~1.5, and most GPQs\nhave redshifts between 0.3 and 2.5. Our finding demonstrates the potential\ndiscovery space for the GPQs from the spectroscopic surveys and the promising\napplications for future research.",
        "We describe the testing of a prototype SiPM-on-tile iron-scintillator\ncalorimeter at the Relativistic Heavy Ion Collider (RHIC) during its 200 GeV\n$pp$ run in 2024. The prototype, measuring $20 \\times 20 \\, \\text{cm}^{2}$ and\n24 radiation lengths in depth, was positioned in the STAR experimental hall,\napproximately 8 m from the interaction point and 65 cm from the beam line,\ncovering a pseudorapidity range of about $3.1<\\eta<3.4$. By using the dark\ncurrent of a reference SiPM as a radiation monitor, we estimate that the\nprototype was exposed to a fluence of about $10^{10}$ 1-MeV\n$n_{\\mathrm{eq}}$\/cm$^2$. Channel-by-channel calibration was performed in a\ndata-driven way with the signature from minimum-ionizing particles during\nbeam-on conditions. A Geant4 detector simulation, with inputs from the Pythia8\nevent generator, describes measurements of energy spectra and hit\nmultiplicities reasonably well. These results mark the first deployment,\ncommissioning, calibration, and long-term operation of a SiPM-on-tile\ncalorimeter in a collider environment. This experimental campaign will guide\ndetector designs and operational strategies for the ePIC detector at the future\nEIC, as well as other applications.",
        "The advent of high-average-power, ultrafast ytterbium-based lasers allows us\nto generate laser filaments at repetition rates ranging from 10s of kHz up to\n100s of kHz. At such high repetition rates, the inter-pulse time lies below the\ntime required for the total diffusion of the deposited heat by each laser\npulse, leading to cumulative hydrodynamic effects that have so far been rarely\nstudied. Here, we present, to the best of our knowledge, the first experimental\ntime-resolved measurements of these dynamics in air for laser repetition rates\nbetween 1 kHz and 100 kHz. We measure the change in the air refractive index\ncaused by the localized heat deposition and the length of the\nfilament-generated plasma channel, with which we can infer the corresponding\nchange in air density. We observe that at repetition rates above 10 kHz,\nstationary density depletions with vanishing dynamics emerge. Our findings are\nof wide relevance for the fields of high-repetition-rate laser filamentation\nand its applications, as well as THz generation from laser-induced plasma\nsources.",
        "In many well-motivated new physics models, the pseudo-Nambu-Goldstone boson\n(pNGB) from U(1) symmetry breaking emerges as a promising dark matter\ncandidate. Its coupling, suppressed by the symmetry breaking scale, prevents\nthermal equilibrium in the early Universe for high scale symmetry breaking.\nThus, pNGB dark matter is predominantly produced via non-thermal mechanisms,\nsuch as the freeze-in process through a new portal coupling. In this work, we\nexplore a novel mechanism for the production of pNGB dark matter even with\nfeeble Higgs portal coupling-arising from Hawking radiation or superradiance of\nprimordial black holes. We systematically investigate the production of light\nand heavy pNGB dark matter, both for Schwarzschild and Kerr black holes. We\nalso discuss its potential gravitational wave signatures from domain wall\ncollapse, density perturbations, and Hawking radiation. If the ultraviolet (UV)\nmodel is considered, the recent $\\mathcal{O}$(100) PeV neutrino event\nKM3-230213A at KM3NeT can be naturally explained.",
        "Quantum counterparts of Schrodinger's classical bridge problem have been\naround for the better part of half a century. During that time, several quantum\napproaches to this multifaceted classical problem have been introduced. In the\npresent work, we unify, extend, and interpret several such approaches through a\nclassical large deviations perspective. To this end, we consider time-symmetric\nensembles that are pre- and post-selected before and after a Markovian\nexperiment is performed. The Schrodinger bridge problem is that of finding the\nmost likely joint distribution of initial and final outcomes that is consistent\nwith obtained endpoint results. The derived distribution provides quantum\nMarkovian dynamics that bridge the observed endpoint states in the form of\ndensity matrices. The solution retains its classical structure in that density\nmatrices can be expressed as the product of forward-evolving and\nbackward-evolving matrices. In addition, the quantum Schrodinger bridge allows\ninference of the most likely distribution of outcomes of an intervening\nmeasurement with unknown results. This distribution may be written as a product\nof forward- and backward-evolving expressions, in close analogy to the\nclassical setting, and in a time-symmetric way. The derived results are\nillustrated through a two-level amplitude damping example.",
        "Different stellar populations may be identified through differences in\nchemical, kinematic, and chronological properties, suggesting the interplay of\nvarious physical mechanisms that led to their origin and subsequent evolution.\nAs such, the identification of stellar populations is key for gaining insight\ninto the evolutionary history of the Milky Way galaxy. This task is complicated\nby the fact that stellar populations share significant overlap in their\nchrono-chemo-kinematic properties, hindering efforts to identify and define\nstellar populations. Our goal is to offer a novel and effective methodology\nthat can provide deeper insight into the nonlinear and nonparametric properties\nof the multidimensional physical parameters that define stellar populations.\nFor this purpose we explore the ability of manifold learning to differentiate\nstellar populations with minimal assumptions about their number and nature.\nManifold learning is an unsupervised machine learning technique that seeks to\nintelligently identify and disentangle manifolds hidden within the input data.\nTo test this method, we make use of Gaia DR3-like synthetic stellar samples\ngenerated from the FIRE-2 cosmological simulations. These represent red-giant\nstars constrained by asteroseismic data from TESS. We reduce the 5-dimensional\ninput chrono-chemo-kinematic parameter space into 2-dimensional latent space\nembeddings generated by manifold learning. We then study these embeddings to\nassess how accurately they represent the original data and whether they contain\nmeaningful information that can be used to discern stellar populations. We\nconclude that manifold learning possesses promising abilities to differentiate\nstellar populations when considering realistic observational constraints.",
        "We deepen the analysis of the cosmological acceleration produced by quantum\ngravity dynamics in the formalism of group field theory condensate cosmology,\ntreated at the coarse-grained level via a phenomenological model, in the\nlanguage of hydrodynamics on minisuperspace. Specifically, we conduct a\ndetailed analysis of the late-time evolution, which shows a phantom-like phase\nfollowed by an asymptotic De Sitter expansion. We argue that the model\nindicates a recent occurrence of the phantom crossing and we extract a more\nprecise expression for the effective cosmological constant, linking its value\nto other parameters in the model and to the scale of the quantum bounce in the\nearly universe evolution. Additionally, we show how the phantom phase produced\nby our quantum gravity dynamics increases the inferred value of the current\nHubble parameter based on observed data, indicating a possible quantum gravity\nmechanism for alleviating the Hubble tension. Our results represent a concrete\nexample of how quantum gravity can provide an explanation for large-scale\ncosmological puzzles, in an emergent spacetime scenario."
      ]
    }
  },
  {
    "id":2412.00129,
    "research_type":"applied",
    "start_id":"b29",
    "start_title":"Study of Fermion pair production in e+e- collisions at 130-183 GeV",
    "start_abstract":"The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings.",
    "start_categories":[
      "astro-ph.HE"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b23"
      ],
      "title":[
        "DARWIN Series: Domain Specific Large Language Models for Natural Science"
      ],
      "abstract":[
        "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "Reframing Dense Action Detection (RefDense): A Paradigm Shift in Problem\n  Solving & a Novel Optimization Strategy",
        "The Architecture and Evaluation of Bayesian Neural Networks",
        "Improving Influence-based Instruction Tuning Data Selection for Balanced\n  Learning of Diverse Capabilities",
        "$\\ell_{p}$ has nontrivial Euclidean distortion growth when $2<p<4$",
        "The Strong Cosmic Censorship Conjecture",
        "Towards Invisible Backdoor Attack on Text-to-Image Diffusion Model",
        "Adaptive Camera Sensor for Vision Models",
        "Gaussian Process Upper Confidence Bound Achieves Nearly-Optimal Regret\n  in Noise-Free Gaussian Process Bandits",
        "Elliptic curves in game theory",
        "StaICC: Standardized Evaluation for Classification Task in In-context\n  Learning",
        "End-to-end Training for Text-to-Image Synthesis using Dual-Text\n  Embeddings",
        "ExplainReduce: Summarising local explanations via proxies",
        "Polynomial invariants of $\\operatorname{GL}_{2}$: Conjugation over\n  finite fields",
        "Continuity of asymptotic entropy on wreath products",
        "Uncertainty Guarantees on Automated Precision Weeding using Conformal\n  Prediction",
        "REGNav: Room Expert Guided Image-Goal Navigation",
        "Intolerable Risk Threshold Recommendations for Artificial Intelligence",
        "LoRa Fine Synchronization with Two-Pass Time and Frequency Offset\n  Estimation",
        "Partial Channel Network: Compute Fewer, Perform Better",
        "Comparison of 2D Regular Lattices for the CPWL Approximation of\n  Functions",
        "DCAT: Dual Cross-Attention Fusion for Disease Classification in\n  Radiological Images with Uncertainty Estimation",
        "Strichartz estimates for the half Klein-Gordon equation on\n  asymptotically flat backgrounds and applications to cubic Dirac equations",
        "Improving the Transferability of Adversarial Attacks by an Input\n  Transpose",
        "From Deep Additive Kernel Learning to Last-Layer Bayesian Neural\n  Networks via Induced Prior Approximation",
        "Generating with Fairness: A Modality-Diffused Counterfactual Framework\n  for Incomplete Multimodal Recommendations",
        "A Scalable System for Visual Analysis of Ocean Data",
        "Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in\n  Large Language Models",
        "Linear-Time User-Level DP-SCO via Robust Statistics",
        "WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in\n  Post-Training"
      ],
      "abstract":[
        "Dense action detection involves detecting multiple co-occurring actions while\naction classes are often ambiguous and represent overlapping concepts. We argue\nthat handling the dual challenge of temporal and class overlaps is too complex\nto effectively be tackled by a single network. To address this, we propose to\ndecompose the task of detecting dense ambiguous actions into detecting dense,\nunambiguous sub-concepts that form the action classes (i.e., action entities\nand action motions), and assigning these sub-tasks to distinct sub-networks. By\nisolating these unambiguous concepts, the sub-networks can focus exclusively on\nresolving a single challenge, dense temporal overlaps. Furthermore,\nsimultaneous actions in a video often exhibit interrelationships, and\nexploiting these relationships can improve the method performance. However,\ncurrent dense action detection networks fail to effectively learn these\nrelationships due to their reliance on binary cross-entropy optimization, which\ntreats each class independently. To address this limitation, we propose\nproviding explicit supervision on co-occurring concepts during network\noptimization through a novel language-guided contrastive learning loss. Our\nextensive experiments demonstrate the superiority of our approach over\nstate-of-the-art methods, achieving substantial improvements of 3.8% and 1.7%\non average across all metrics on the challenging benchmark datasets, Charades\nand MultiTHUMOS.",
        "As modern neural networks get more complex, specifying a model with high\npredictive performance and sound uncertainty quantification becomes a more\nchallenging task. Despite some promising theoretical results on the true\nposterior predictive distribution of Bayesian neural networks, the properties\nof even the most commonly used posterior approximations are often questioned.\nComputational burdens and intractable posteriors expose miscalibrated Bayesian\nneural networks to poor accuracy and unreliable uncertainty estimates.\nApproximate Bayesian inference aims to replace unknown and intractable\nposterior distributions with some simpler but feasible distributions. The\ndimensions of modern deep models coupled with the lack of identifiability make\nMarkov chain Monte Carlo tremendously expensive and unable to fully explore the\nmultimodal posterior. On the other hand, variational inference benefits from\nimproved computational complexity but lacks the asymptotical guarantees of\nsampling-based inference and tends to concentrate around a single mode. The\nperformance of both approaches heavily depends on architectural choices; this\npaper aims to shed some light on this, by considering the computational costs,\naccuracy and uncertainty quantification in different scenarios including large\nwidth and out-of-sample data. To improve posterior exploration, different model\naveraging and ensembling techniques are studied, along with their benefits on\npredictive performance. In our experiments, variational inference overall\nprovided better uncertainty quantification than Markov chain Monte Carlo;\nfurther, stacking and ensembles of variational approximations provided\ncomparable to Markov chain Monte Carlo accuracy at a much-reduced cost.",
        "Selecting appropriate training data is crucial for effective instruction\nfine-tuning of large language models (LLMs), which aims to (1) elicit strong\ncapabilities, and (2) achieve balanced performance across a diverse range of\ntasks. Influence-based methods show promise in achieving (1) by estimating the\ncontribution of each training example to the model's predictions, but often\nstruggle with (2). Our systematic investigation reveals that this\nunderperformance can be attributed to an inherent bias where certain tasks\nintrinsically have greater influence than others. As a result, data selection\nis often biased towards these tasks, not only hurting the model's performance\non others but also, counterintuitively, harms performance on these\nhigh-influence tasks themselves.\n  As a remedy, we propose BIDS, a Balanced and Influential Data Selection\nalgorithm. BIDS first normalizes influence scores of the training data, and\nthen iteratively balances data selection by choosing the training example with\nthe highest influence on the most underrepresented task. Experiments with both\nLlama-3 and Mistral-v0.3 on seven benchmarks spanning five diverse capabilities\nshow that BIDS consistently outperforms both state-of-the-art influence-based\nalgorithms and other non-influence-based selection frameworks. Surprisingly,\ntraining on a 15% subset selected by BIDS can even outperform full-dataset\ntraining with a much more balanced performance. Our analysis further highlights\nthe importance of both instance-level normalization and iterative optimization\nof selected data for balanced learning of diverse capabilities.",
        "We prove that if $2 < p < 4$, then every $n$-point subset of $\\ell_{p}$\nembeds into a Hilbert space with distortion $o(\\log n)$, i.e., with distortion\nthat is asymptotically smaller than what Bourgain's embedding theorem provides\nfor arbitrary $n$-point metric spaces. This has been previously unknown for any\n$2<p<\\infty$. We also prove that for every $2<p<\\infty$ the largest possible\nseparation modulus of an $n$-point subset of $\\ell_{p}$ is bounded from above\nand from below by positive constant multiples of $\\sqrt{\\log n}$ which may\ndepend only on $p$.",
        "In the wake of major breakthroughs in General Relativity during the 1960s,\nRoger Penrose introduced Strong Cosmic Censorship, a profound conjecture\nregarding the deterministic nature of the theory. Penrose's proposal has since\nopened far-reaching new mathematical avenues, revealing connections to\nfundamental questions about black holes and the nature of gravitational\nsingularities. We review recent advances arising from modern techniques in the\ntheory of partial differential equations as applied to Strong Cosmic\nCensorship, maintaining a focus on the context of gravitational collapse that\ngave birth to the conjecture.",
        "Backdoor attacks targeting text-to-image diffusion models have advanced\nrapidly, enabling attackers to implant malicious triggers into these models to\nmanipulate their outputs. However, current backdoor samples often exhibit two\nkey abnormalities compared to benign samples: 1) Semantic Consistency, where\nbackdoor prompts tend to generate images with similar semantic content even\nwith significant textual variations to the prompts; 2) Attention Consistency,\nwhere the trigger induces consistent structural responses in the\ncross-attention maps. These consistencies leave detectable traces for\ndefenders, making backdoors easier to identify. To enhance the stealthiness of\nbackdoor samples, we propose a novel Invisible Backdoor Attack (IBA) by\nexplicitly mitigating these consistencies. Specifically, our approach leverages\nsyntactic structures as backdoor triggers to amplify the sensitivity to textual\nvariations, effectively breaking down the semantic consistency. Besides, a\nregularization method based on Kernel Maximum Mean Discrepancy (KMMD) is\nproposed to align the distribution of cross-attention responses between\nbackdoor and benign samples, thereby disrupting attention consistency.\nExtensive experiments demonstrate that our IBA achieves a 97.5% attack success\nrate while exhibiting stronger resistance to defenses, with an average of over\n98% backdoor samples bypassing three state-of-the-art detection mechanisms. The\ncode is available at https:\/\/github.com\/Robin-WZQ\/IBA.",
        "Domain shift remains a persistent challenge in deep-learning-based computer\nvision, often requiring extensive model modifications or large labeled datasets\nto address. Inspired by human visual perception, which adjusts input quality\nthrough corrective lenses rather than over-training the brain, we propose Lens,\na novel camera sensor control method that enhances model performance by\ncapturing high-quality images from the model's perspective rather than relying\non traditional human-centric sensor control. Lens is lightweight and adapts\nsensor parameters to specific models and scenes in real-time. At its core, Lens\nutilizes VisiT, a training-free, model-specific quality indicator that\nevaluates individual unlabeled samples at test time using confidence scores\nwithout additional adaptation costs. To validate Lens, we introduce ImageNet-ES\nDiverse, a new benchmark dataset capturing natural perturbations from varying\nsensor and lighting conditions. Extensive experiments on both ImageNet-ES and\nour new ImageNet-ES Diverse show that Lens significantly improves model\naccuracy across various baseline schemes for sensor control and model\nmodification while maintaining low latency in image captures. Lens effectively\ncompensates for large model size differences and integrates synergistically\nwith model improvement techniques. Our code and dataset are available at\ngithub.com\/Edw2n\/Lens.git.",
        "We study the noise-free Gaussian Process (GP) bandits problem, in which the\nlearner seeks to minimize regret through noise-free observations of the\nblack-box objective function lying on the known reproducing kernel Hilbert\nspace (RKHS). Gaussian process upper confidence bound (GP-UCB) is the\nwell-known GP-bandits algorithm whose query points are adaptively chosen based\non the GP-based upper confidence bound score. Although several existing works\nhave reported the practical success of GP-UCB, the current theoretical results\nindicate its suboptimal performance. However, GP-UCB tends to perform well\nempirically compared with other nearly optimal noise-free algorithms that rely\non a non-adaptive sampling scheme of query points. This paper resolves this gap\nbetween theoretical and empirical performance by showing the nearly optimal\nregret upper bound of noise-free GP-UCB. Specifically, our analysis shows the\nfirst constant cumulative regret in the noise-free settings for the squared\nexponential kernel and Mat\\'ern kernel with some degree of smoothness.",
        "We investigate Spohn curves, the algebro-geometric models of dependency\nequilibria for $2 \\times 2$ normal-form games. These curves arise as the\nintersection of two quadrics in $\\mathbb{P}^3$ and are generically elliptic\ncurves. We compute and verify the $j$-invariant for elliptic curves arising as\nthe intersection of quadrics in $\\mathbb P^3$ using two different\nimplementations: by computing the Aronhold invariants and the discriminant (in\nMathematica) and using algorithms for the arithmetic of elliptic curves\n(in-built in Pari\/GP). We define an equivalency of generic $2\\times 2$ games\nbased on the $j$-invariant of the Spohn curve. Additionally, we examine the\nreduction of Spohn curves to plane curves and analyze conditions under which\nthey are reducible. Notably, we prove that the real points are dense on the\nSpohn curve in all cases. Our examples and computations are further supported\nby Macaulay2.",
        "Classification tasks are widely investigated in the In-Context Learning (ICL)\nparadigm. However, current efforts are evaluated on disjoint benchmarks and\nsettings, while their performances are significantly influenced by some trivial\nvariables, such as prompt templates, data sampling, instructions, etc., which\nleads to significant inconsistencies in the results reported across various\nliterature, preventing fair comparison or meta-analysis across different\npapers. Therefore, this paper proposes a standardized and easy-to-use\nevaluation toolkit (StaICC) for in-context classification. Including, for the\nnormal classification task, we provide StaICC-Normal, selecting 10 widely used\ndatasets, and generating prompts with a fixed form, to mitigate the variance\namong the experiment implementations. To enrich the usage of our benchmark, we\nalso provide a sub-benchmark StaICC-Diag for diagnosing ICL from several\naspects, aiming for a more robust inference processing.",
        "Text-to-Image (T2I) synthesis is a challenging task that requires modeling\ncomplex interactions between two modalities ( i.e., text and image). A common\nframework adopted in recent state-of-the-art approaches to achieving such\nmultimodal interactions is to bootstrap the learning process with pre-trained\nimage-aligned text embeddings trained using contrastive loss. Furthermore,\nthese embeddings are typically trained generically and reused across various\nsynthesis models. In contrast, we explore an approach to learning text\nembeddings specifically tailored to the T2I synthesis network, trained in an\nend-to-end fashion. Further, we combine generative and contrastive training and\nuse two embeddings, one optimized to enhance the photo-realism of the generated\nimages, and the other seeking to capture text-to-image alignment. A\ncomprehensive set of experiments on three text-to-image benchmark datasets\n(Oxford-102, Caltech-UCSD, and MS-COCO) reveal that having two separate\nembeddings gives better results than using a shared one and that such an\napproach performs favourably in comparison with methods that use text\nrepresentations from a pre-trained text encoder trained using a discriminative\napproach. Finally, we demonstrate that such learned embeddings can be used in\nother contexts as well, such as text-to-image manipulation.",
        "Most commonly used non-linear machine learning methods are closed-box models,\nuninterpretable to humans. The field of explainable artificial intelligence\n(XAI) aims to develop tools to examine the inner workings of these closed\nboxes. An often-used model-agnostic approach to XAI involves using simple\nmodels as local approximations to produce so-called local explanations;\nexamples of this approach include LIME, SHAP, and SLISEMAP. This paper shows\nhow a large set of local explanations can be reduced to a small \"proxy set\" of\nsimple models, which can act as a generative global explanation. This reduction\nprocedure, ExplainReduce, can be formulated as an optimisation problem and\napproximated efficiently using greedy heuristics.",
        "Consider the conjugation action of $\\operatorname{GL}_{2}(K)$ on the\npolynomial ring $K[X_{2 \\times 2}]$. When $K$ is an infinite field, the ring of\ninvariants is a polynomial ring generated by the trace and the determinant. We\ndescribe the ring of invariants when $K$ is a finite field, and show that it is\na hypersurface.",
        "We prove the continuity of asymptotic entropy as a function of the step\ndistribution for non-degenerate probability measures with finite entropy on\nwreath products $ A \\wr B = \\bigoplus_B A \\rtimes B $, where $A$ is any\ncountable group and $B$ is a countable hyper-FC-central group that contains a\nfinitely generated subgroup of at least cubic growth. As one step in proving\nthe above, we show that on any countable group $G$ the probability that the\n$\\mu$-random walk on $G$ never returns to the identity is continuous in $\\mu$,\nfor measures $\\mu$ such that the semigroup generated by the support of $\\mu$\ncontains a finitely generated subgroup of at least cubic growth. Finally, we\nshow that among random walks on a group $G$ that admit a separable completely\nmetrizable space $X$ as a model for their Poisson boundary, the weak continuity\nof the associated harmonic measures on $X$ implies the continuity of the\nasymptotic entropy. This result recovers the continuity of asymptotic entropy\non known cases, such as Gromov hyperbolic groups and acylindrically hyperbolic\ngroups, and extends it to new classes of groups, including linear groups and\ngroups acting on $\\mathrm{CAT}(0)$ spaces.",
        "Precision agriculture in general, and precision weeding in particular, have\ngreatly benefited from the major advancements in deep learning and computer\nvision. A large variety of commercial robotic solutions are already available\nand deployed. However, the adoption by farmers of such solutions is still low\nfor many reasons, an important one being the lack of trust in these systems.\nThis is in great part due to the opaqueness and complexity of deep neural\nnetworks and the manufacturers' inability to provide valid guarantees on their\nperformance. Conformal prediction, a well-established methodology in the\nmachine learning community, is an efficient and reliable strategy for providing\ntrustworthy guarantees on the predictions of any black-box model under very\nminimal constraints. Bridging the gap between the safe machine learning and\nprecision agriculture communities, this article showcases conformal prediction\nin action on the task of precision weeding through deep learning-based image\nclassification. After a detailed presentation of the conformal prediction\nmethodology and the development of a precision spraying pipeline based on a\n''conformalized'' neural network and well-defined spraying decision rules, the\narticle evaluates this pipeline on two real-world scenarios: one under\nin-distribution conditions, the other reflecting a near out-of-distribution\nsetting. The results show that we are able to provide formal, i.e. certifiable,\nguarantees on spraying at least 90% of the weeds.",
        "Image-goal navigation aims to steer an agent towards the goal location\nspecified by an image. Most prior methods tackle this task by learning a\nnavigation policy, which extracts visual features of goal and observation\nimages, compares their similarity and predicts actions. However, if the agent\nis in a different room from the goal image, it's extremely challenging to\nidentify their similarity and infer the likely goal location, which may result\nin the agent wandering around. Intuitively, when humans carry out this task,\nthey may roughly compare the current observation with the goal image, having an\napproximate concept of whether they are in the same room before executing the\nactions. Inspired by this intuition, we try to imitate human behaviour and\npropose a Room Expert Guided Image-Goal Navigation model (REGNav) to equip the\nagent with the ability to analyze whether goal and observation images are taken\nin the same room. Specifically, we first pre-train a room expert with an\nunsupervised learning technique on the self-collected unlabelled room images.\nThe expert can extract the hidden room style information of goal and\nobservation images and predict their relationship about whether they belong to\nthe same room. In addition, two different fusion approaches are explored to\nefficiently guide the agent navigation with the room relation knowledge.\nExtensive experiments show that our REGNav surpasses prior state-of-the-art\nworks on three popular benchmarks.",
        "Frontier AI models -- highly capable foundation models at the cutting edge of\nAI development -- may pose severe risks to public safety, human rights,\neconomic stability, and societal value in the coming years. These risks could\narise from deliberate adversarial misuse, system failures, unintended cascading\neffects, or simultaneous failures across multiple models.\n  In response to such risks, at the AI Seoul Summit in May 2024, 16 global AI\nindustry organizations signed the Frontier AI Safety Commitments, and 27\nnations and the EU issued a declaration on their intent to define these\nthresholds. To fulfill these commitments, organizations must determine and\ndisclose ``thresholds at which severe risks posed by a model or system, unless\nadequately mitigated, would be deemed intolerable.''\n  To assist in setting and operationalizing intolerable risk thresholds, we\noutline key principles and considerations; for example, to aim for ``good, not\nperfect'' thresholds in the face of limited data on rapidly advancing AI\ncapabilities and consequently evolving risks. We also propose specific\nthreshold recommendations, including some detailed case studies, for a subset\nof risks across eight risk categories: (1) Chemical, Biological, Radiological,\nand Nuclear (CBRN) Weapons, (2) Cyber Attacks, (3) Model Autonomy, (4)\nPersuasion and Manipulation, (5) Deception, (6) Toxicity, (7) Discrimination,\nand (8) Socioeconomic Disruption. Our goal is to serve as a starting point or\nsupplementary resource for policymakers and industry leaders, encouraging\nproactive risk management that prioritizes preventing intolerable risks (ex\nante) rather than merely mitigating them after they occur (ex post).",
        "LoRa is currently one of the most widely used low-power wide-area network\n(LPWAN) technologies. The physical layer leverages a chirp spread spectrum\nmodulation to achieve long-range communication with low power consumption.\nSynchronization at long distances is a challenging task as the spread signal\ncan lie multiple orders of magnitude below the thermal noise floor. Multiple\nresearch works have proposed synchronization algorithms for LoRa under\ndifferent hardware impairments. However, the impact of sampling frequency\noffset (SFO) has mostly either been ignored or tracked only during the data\nphase, but it often harms synchronization. In this work, we extend existing\nsynchronization algorithms for LoRa to estimate and compensate SFO already in\nthe preamble and show that this early compensation has a critical impact on the\nestimation of other impairments such as carrier frequency offset and sampling\ntime offset. Therefore it is critical to recover long-range signals.",
        "Designing a module or mechanism that enables a network to maintain low\nparameters and FLOPs without sacrificing accuracy and throughput remains a\nchallenge. To address this challenge and exploit the redundancy within feature\nmap channels, we propose a new solution: partial channel mechanism (PCM).\nSpecifically, through the split operation, the feature map channels are divided\ninto different parts, with each part corresponding to different operations,\nsuch as convolution, attention, pooling, and identity mapping. Based on this\nassumption, we introduce a novel partial attention convolution (PATConv) that\ncan efficiently combine convolution with visual attention. Our exploration\nindicates that the PATConv can completely replace both the regular convolution\nand the regular visual attention while reducing model parameters and FLOPs.\nMoreover, PATConv can derive three new types of blocks: Partial\nChannel-Attention block (PAT_ch), Partial Spatial-Attention block (PAT_sp), and\nPartial Self-Attention block (PAT_sf). In addition, we propose a novel dynamic\npartial convolution (DPConv) that can adaptively learn the proportion of split\nchannels in different layers to achieve better trade-offs. Building on PATConv\nand DPConv, we propose a new hybrid network family, named PartialNet, which\nachieves superior top-1 accuracy and inference speed compared to some SOTA\nmodels on ImageNet-1K classification and excels in both detection and\nsegmentation on the COCO dataset. Our code is available at\nhttps:\/\/github.com\/haiduo\/PartialNet.",
        "We investigate the approximation error of functions with continuous and\npiecewise-linear (CPWL) representations. We focus on the CPWL search spaces\ngenerated by translates of box splines on two-dimensional regular lattices. We\ncompute the approximation error in terms of the stepsize and angles that define\nthe lattice. Our results show that hexagonal lattices are optimal, in the sense\nthat they minimize the asymptotic approximation error.",
        "Accurate and reliable image classification is crucial in radiology, where\ndiagnostic decisions significantly impact patient outcomes. Conventional deep\nlearning models tend to produce overconfident predictions despite underlying\nuncertainties, potentially leading to misdiagnoses. Attention mechanisms have\nemerged as powerful tools in deep learning, enabling models to focus on\nrelevant parts of the input data. Combined with feature fusion, they can be\neffective in addressing uncertainty challenges. Cross-attention has become\nincreasingly important in medical image analysis for capturing dependencies\nacross features and modalities. This paper proposes a novel dual\ncross-attention fusion model for medical image analysis by addressing key\nchallenges in feature integration and interpretability. Our approach introduces\na bidirectional cross-attention mechanism with refined channel and spatial\nattention that dynamically fuses feature maps from EfficientNetB4 and ResNet34\nleveraging multi-network contextual dependencies. The refined features through\nchannel and spatial attention highlights discriminative patterns crucial for\naccurate classification. The proposed model achieved AUC of 99.75%, 100%,\n99.93% and 98.69% and AUPR of 99.81%, 100%, 99.97%, and 96.36% on Covid-19,\nTuberculosis, Pneumonia Chest X-ray images and Retinal OCT images respectively.\nThe entropy values and several high uncertain samples give an interpretable\nvisualization from the model enhancing transparency. By combining multi-scale\nfeature extraction, bidirectional attention and uncertainty estimation, our\nproposed model strongly impacts medical image analysis.",
        "The aim of this paper is to establish the $L^2_t$-endpoint Strichartz\nestimate for (half) Klein-Gordon equations on a weakly asymptotically flat\nspace-time. As an application we prove small data global well-posedness and\nscattering for massive cubic Dirac equations in the full subcritical range in\nthis setting.\n  Crucial ingredient is a parametrix contruction following the work of\nMetcalfe-Tataru and Xue and complements Strichartz estimates obtained by\nZheng-Zhang. The proof of the global result for the cubic Dirac equation\nfollows the strategy developed by Machihara-Nakanishi-Ozawa in the Euclidean\nsetting.",
        "Deep neural networks (DNNs) are highly susceptible to adversarial\nexamples--subtle perturbations applied to inputs that are often imperceptible\nto humans yet lead to incorrect model predictions. In black-box scenarios,\nhowever, existing adversarial examples exhibit limited transferability and\nstruggle to effectively compromise multiple unseen DNN models. Previous\nstrategies enhance the cross-model generalization of adversarial examples by\nintroducing versatility into adversarial perturbations, thereby improving\ntransferability. However, further refining perturbation versatility often\ndemands intricate algorithm development and substantial computation\nconsumption. In this work, we propose an input transpose method that requires\nalmost no additional labor and computation costs but can significantly improve\nthe transferability of existing adversarial strategies. Even without adding\nadversarial perturbations, our method demonstrates considerable effectiveness\nin cross-model attacks. Our exploration finds that on specific datasets, a mere\n$1^\\circ$ left or right rotation might be sufficient for most adversarial\nexamples to deceive unseen models. Our further analysis suggests that this\ntransferability improvement triggered by rotating only $1^\\circ$ may stem from\nvisible pattern shifts in the DNN's low-level feature maps. Moreover, this\ntransferability exhibits optimal angles that, when identified under\nunrestricted query conditions, could potentially yield even greater\nperformance.",
        "With the strengths of both deep learning and kernel methods like Gaussian\nProcesses (GPs), Deep Kernel Learning (DKL) has gained considerable attention\nin recent years. From the computational perspective, however, DKL becomes\nchallenging when the input dimension of the GP layer is high. To address this\nchallenge, we propose the Deep Additive Kernel (DAK) model, which incorporates\ni) an additive structure for the last-layer GP; and ii) induced prior\napproximation for each GP unit. This naturally leads to a last-layer Bayesian\nneural network (BNN) architecture. The proposed method enjoys the\ninterpretability of DKL as well as the computational advantages of BNN.\nEmpirical results show that the proposed approach outperforms state-of-the-art\nDKL methods in both regression and classification tasks.",
        "Incomplete scenario is a prevalent, practical, yet challenging setting in\nMultimodal Recommendations (MMRec), where some item modalities are missing due\nto various factors. Recently, a few efforts have sought to improve the\nrecommendation accuracy by exploring generic structures from incomplete data.\nHowever, two significant gaps persist: 1) the difficulty in accurately\ngenerating missing data due to the limited ability to capture modality\ndistributions; and 2) the critical but overlooked visibility bias, where items\nwith missing modalities are more likely to be disregarded due to the\nprioritization of items' multimodal data over user preference alignment. This\nbias raises serious concerns about the fair treatment of items. To bridge these\ntwo gaps, we propose a novel Modality-Diffused Counterfactual (MoDiCF)\nframework for incomplete multimodal recommendations. MoDiCF features two key\nmodules: a novel modality-diffused data completion module and a new\ncounterfactual multimodal recommendation module. The former, equipped with a\nparticularly designed multimodal generative framework, accurately generates and\niteratively refines missing data from learned modality-specific distribution\nspaces. The latter, grounded in the causal perspective, effectively mitigates\nthe negative causal effects of visibility bias and thus assures fairness in\nrecommendations. Both modules work collaboratively to address the two\naforementioned significant gaps for generating more accurate and fair results.\nExtensive experiments on three real-world datasets demonstrate the superior\nperformance of MoDiCF in terms of both recommendation accuracy and fairness.\nThe code and processed datasets are released at\nhttps:\/\/github.com\/JinLi-i\/MoDiCF.",
        "Oceanographers rely on visual analysis to interpret model simulations,\nidentify events and phenomena, and track dynamic ocean processes. The ever\nincreasing resolution and complexity of ocean data due to its dynamic nature\nand multivariate relationships demands a scalable and adaptable visualization\ntool for interactive exploration. We introduce pyParaOcean, a scalable and\ninteractive visualization system designed specifically for ocean data analysis.\npyParaOcean offers specialized modules for common oceanographic analysis tasks,\nincluding eddy identification and salinity movement tracking. These modules\nseamlessly integrate with ParaView as filters, ensuring a user-friendly and\neasy-to-use system while leveraging the parallelization capabilities of\nParaView and a plethora of inbuilt general-purpose visualization\nfunctionalities. The creation of an auxiliary dataset stored as a Cinema\ndatabase helps address I\/O and network bandwidth bottlenecks while supporting\nthe generation of quick overview visualizations. We present a case study on the\nBay of Bengal (BoB) to demonstrate the utility of the system and scaling\nstudies to evaluate the efficiency of the system.",
        "Fine-tuning large language models (LLMs) based on human preferences, commonly\nachieved through reinforcement learning from human feedback (RLHF), has been\neffective in improving their performance. However, maintaining LLM safety\nthroughout the fine-tuning process remains a significant challenge, as\nresolving conflicts between safety and helpfulness can be non-trivial.\nTypically, the safety alignment of LLM is trained on data with safety-related\ncategories. However, our experiments find that naively increasing the scale of\nsafety training data usually leads the LLMs to an ``overly safe'' state rather\nthan a ``truly safe'' state, boosting the refusal rate through extensive\nsafety-aligned data without genuinely understanding the requirements for safe\nresponses. Such an approach can inadvertently diminish the models' helpfulness.\nTo understand the phenomenon, we first investigate the role of safety data by\ncategorizing them into three different groups, and observe that each group\nbehaves differently as training data scales up. To boost the balance between\nsafety and helpfulness, we propose an Equilibrate RLHF framework including a\nFine-grained Data-centric (FDC) approach that achieves better safety alignment\neven with fewer training data, and an Adaptive Message-wise Alignment (AMA)\napproach, which selectively highlight the key segments through a gradient\nmasking strategy. Extensive experimental results demonstrate that our approach\nsignificantly enhances the safety alignment of LLMs while balancing safety and\nhelpfulness.",
        "User-level differentially private stochastic convex optimization (DP-SCO) has\ngarnered significant attention due to the paramount importance of safeguarding\nuser privacy in modern large-scale machine learning applications. Current\nmethods, such as those based on differentially private stochastic gradient\ndescent (DP-SGD), often struggle with high noise accumulation and suboptimal\nutility due to the need to privatize every intermediate iterate. In this work,\nwe introduce a novel linear-time algorithm that leverages robust statistics,\nspecifically the median and trimmed mean, to overcome these challenges. Our\napproach uniquely bounds the sensitivity of all intermediate iterates of SGD\nwith gradient estimation based on robust statistics, thereby significantly\nreducing the gradient estimation noise for privacy purposes and enhancing the\nprivacy-utility trade-off. By sidestepping the repeated privatization required\nby previous methods, our algorithm not only achieves an improved theoretical\nprivacy-utility trade-off but also maintains computational efficiency. We\ncomplement our algorithm with an information-theoretic lower bound, showing\nthat our upper bound is optimal up to logarithmic factors and the dependence on\n$\\epsilon$. This work sets the stage for more robust and efficient\nprivacy-preserving techniques in machine learning, with implications for future\nresearch and application in the field.",
        "Language model (LLM) post-training, from DPO to distillation, can refine\nbehaviors and unlock new skills, but the open science supporting these\npost-training techniques is still in its infancy. One limiting factor has been\nthe difficulty of conducting large-scale comparative analyses of synthetic data\ngenerating models and LLM judges. To close this gap, we introduce WILDCHAT-50M,\nthe largest public chat dataset to date. We extend the existing WildChat\ndataset to include responses not only from GPT, but from over 50 different\nopen-weight models, ranging in size from 0.5B to 104B parameters. We conduct an\nextensive comparative analysis and demonstrate the potential of this dataset by\ncreating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3\nSFT mixture from Allen AI with only 40% as many samples. Our dataset, samples\nand code are available at https:\/\/github.com\/penfever\/wildchat-50m."
      ]
    }
  },
  {
    "id":2411.19475,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy",
    "start_abstract":"ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications.",
    "start_categories":[
      "astro-ph.CO"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "ImageNet: A large-scale hierarchical image database"
      ],
      "abstract":[
        "The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Transformers trained on proteins can learn to attend to Euclidean\n  distance",
        "ProgCo: Program Helps Self-Correction of Large Language Models",
        "Unveiling and Causalizing CoT: A Causal Pespective",
        "Visual Attention Exploration in Vision-Based Mamba Models",
        "Kernel EDMD for data-driven nonlinear Koopman MPC with stability\n  guarantees",
        "\"Once Upon a Time...\" Literary Narrative Connectedness Progresses with\n  Grade Level: Potential Impact on Reading Fluency and Literacy Skills",
        "Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language\n  Embedding Registration",
        "EILID: Execution Integrity for Low-end IoT Devices",
        "RECALL: Library-Like Behavior In Language Models is Enhanced by\n  Self-Referencing Causal Cycles",
        "Targetless Intrinsics and Extrinsic Calibration of Multiple LiDARs and\n  Cameras with IMU using Continuous-Time Estimation",
        "Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment,\n  Distribution, and Discovery",
        "On singular supports in mixed characteristic",
        "Addressing Out-of-Label Hazard Detection in Dashcam Videos: Insights\n  from the COOOL Challenge",
        "Algebraic families of higher dimensional $\\mathbb{A}^{1}$-contractible\n  affine varieties non-isomorphic to affine spaces",
        "On the Semantic Security of NTRU -- with a gentle introduction to\n  cryptography",
        "RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head\n  Avatars",
        "Discrete Gaussian Process Representations for Optimising UAV-based\n  Precision Weed Mapping",
        "Data re-uploading in Quantum Machine Learning for time series:\n  application to traffic forecasting",
        "Explicit bounds on the transcendental Brauer group of K3 surfaces with\n  principal complex multiplication",
        "N-player and mean field games among fund managers considering excess\n  logarithmic returns",
        "Bounded-Confidence Models of Multi-Dimensional Opinions with\n  Topic-Weighted Discordance",
        "Identity-aware Feature Decoupling Learning for Clothing-change Person\n  Re-identification",
        "P4sim: Programming Protocol-independent Packet Processors in ns-3",
        "Dollarized Economies in Latin America. An Inflationary Analysis of Pre,\n  During and Post Pandemic",
        "Sakshm AI: Advancing AI-Assisted Coding Education for Engineering\n  Students in India Through Socratic Tutoring and Comprehensive Feedback",
        "Understanding and Rectifying Safety Perception Distortion in VLMs",
        "On a Gelfand-Tsetlin representation of $\\mathfrak{sl}_3$ in the space of\n  sections of a local system with two monodromy parameters",
        "Normalization through Fine-tuning: Understanding Wav2vec 2.0 Embeddings\n  for Phonetic Analysis",
        "A Transformer-Based Framework for Greek Sign Language Production using\n  Extended Skeletal Motion Representations"
      ],
      "abstract":[
        "While conventional Transformers generally operate on sequence data, they can\nbe used in conjunction with structure models, typically SE(3)-invariant or\nequivariant graph neural networks (GNNs), for 3D applications such as protein\nstructure modelling. These hybrids typically involve either (1)\npreprocessing\/tokenizing structural features as input for Transformers or (2)\ntaking Transformer embeddings and processing them within a structural\nrepresentation. However, there is evidence that Transformers can learn to\nprocess structural information on their own, such as the AlphaFold3 structural\ndiffusion model. In this work we show that Transformers can function\nindependently as structure models when passed linear embeddings of coordinates.\nWe first provide a theoretical explanation for how Transformers can learn to\nfilter attention as a 3D Gaussian with learned variance. We then validate this\ntheory using both simulated 3D points and in the context of masked token\nprediction for proteins. Finally, we show that pre-training protein Transformer\nencoders with structure improves performance on a downstream task, yielding\nbetter performance than custom structural models. Together, this work provides\na basis for using standard Transformers as hybrid structure-language models.",
        "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools.",
        "Although Chain-of-Thought (CoT) has achieved remarkable success in enhancing\nthe reasoning ability of large language models (LLMs), the mechanism of CoT\nremains a ``black box''. Even if the correct answers can frequently be\nobtained, existing CoTs struggle to make the reasoning understandable to human.\nIn this paper, we unveil and causalize CoT from a causal perspective to ensure\nboth correctness and understandability of all reasoning steps (to the best of\nour knowledge, the first such). We model causality of CoT via structural causal\nmodels (SCM) to unveil the reasoning mechanism of CoT. To measure the causality\nof CoT, we define the CoT Average Causal Effect (CACE) to test the causal\nrelations between steps. For those steps without causality (wrong or\nunintelligible steps), we design a role-playing causal query algorithm to\ncausalize these steps, resulting a causalized CoT with all steps correct and\nunderstandable. Experimental results on both open-source and closed-source LLMs\ndemonstrate that the causal errors commonly in steps are effectively corrected\nand the reasoning ability of LLMs is significantly improved.",
        "State space models (SSMs) have emerged as an efficient alternative to\ntransformer-based models, offering linear complexity that scales better than\ntransformers. One of the latest advances in SSMs, Mamba, introduces a selective\nscan mechanism that assigns trainable weights to input tokens, effectively\nmimicking the attention mechanism. Mamba has also been successfully extended to\nthe vision domain by decomposing 2D images into smaller patches and arranging\nthem as 1D sequences. However, it remains unclear how these patches interact\nwith (or attend to) each other in relation to their original 2D spatial\nlocation. Additionally, the order used to arrange the patches into a sequence\nalso significantly impacts their attention distribution. To better understand\nthe attention between patches and explore the attention patterns, we introduce\na visual analytics tool specifically designed for vision-based Mamba models.\nThis tool enables a deeper understanding of how attention is distributed across\npatches in different Mamba blocks and how it evolves throughout a Mamba model.\nUsing the tool, we also investigate the impact of different patch-ordering\nstrategies on the learned attention, offering further insights into the model's\nbehavior.",
        "Extended dynamic mode decomposition (EDMD) is a popular data-driven method to\npredict the action of the Koopman operator, i.e., the evolution of an\nobservable function along the flow of a dynamical system. In this paper, we\nleverage a recently-introduced kernel EDMD method for control systems for\ndata-driven model predictive control. Building upon pointwise error bounds\nproportional in the state, we rigorously show practical asymptotic stability of\nthe origin w.r.t. the MPC closed loop without stabilizing terminal conditions.\nThe key novelty is that we avoid restrictive invariance conditions. Last, we\nverify our findings by numerical simulations.",
        "Selecting an appropriate book is crucial for fostering reading habits in\nchildren. While children exhibit varying levels of complexity when generating\noral narratives, the question arises: do children's books also differ in\nnarrative complexity? This study explores the narrative dynamics of literary\ntexts used in schools, focusing on how their complexity evolves across\ndifferent grade levels. Using Word-Recurrence Graph Analysis, we examined a\ndataset of 1,627 literary texts spanning 13 years of education. The findings\nreveal significant exponential growth in connectedness, particularly during the\nfirst three years of schooling, mirroring patterns observed in children's oral\nnarratives. These results highlight the potential of literary texts as a tool\nto support the development of literacy skills.",
        "We introduce Dr. Splat, a novel approach for open-vocabulary 3D scene\nunderstanding leveraging 3D Gaussian Splatting. Unlike existing\nlanguage-embedded 3DGS methods, which rely on a rendering process, our method\ndirectly associates language-aligned CLIP embeddings with 3D Gaussians for\nholistic 3D scene understanding. The key of our method is a language feature\nregistration technique where CLIP embeddings are assigned to the dominant\nGaussians intersected by each pixel-ray. Moreover, we integrate Product\nQuantization (PQ) trained on general large-scale image data to compactly\nrepresent embeddings without per-scene optimization. Experiments demonstrate\nthat our approach significantly outperforms existing approaches in 3D\nperception benchmarks, such as open-vocabulary 3D semantic segmentation, 3D\nobject localization, and 3D object selection tasks. For video results, please\nvisit : https:\/\/drsplat.github.io\/",
        "Prior research yielded many techniques to mitigate software compromise for\nlow-end Internet of Things (IoT) devices. Some of them detect software\nmodifications via remote attestation and similar services, while others\npreventatively ensure software (static) integrity. However, achieving run-time\n(dynamic) security, e.g., control-flow integrity (CFI), remains a challenge.\n  Control-flow attestation (CFA) is one approach that minimizes the burden on\ndevices. However, CFA is not a real-time countermeasure against run-time\nattacks since it requires communication with a verifying entity. This poses\nsignificant risks if safety- or time-critical tasks have memory\nvulnerabilities.\n  To address this issue, we construct EILID - a hybrid architecture that\nensures software execution integrity by actively monitoring control-flow\nviolations on low-end devices. EILID is built atop CASU, a prevention-based\n(i.e., active) hybrid Root-of-Trust (RoT) that guarantees software\nimmutability. EILID achieves fine-grained backward-edge and function-level\nforward-edge CFI via semi-automatic code instrumentation and a secure shadow\nstack.",
        "We introduce the concept of the self-referencing causal cycle (abbreviated\nRECALL) - a mechanism that enables large language models (LLMs) to bypass the\nlimitations of unidirectional causality, which underlies a phenomenon known as\nthe reversal curse. When an LLM is prompted with sequential data, it often\nfails to recall preceding context. For example, when we ask an LLM to recall\nthe line preceding \"O say does that star-spangled banner yet wave\" in the U.S.\nNational Anthem, it often fails to correctly return \"Gave proof through the\nnight that our flag was still there\" - this is due to the reversal curse. It\noccurs because language models such as ChatGPT and Llama generate text based on\npreceding tokens, requiring facts to be learned and reproduced in a consistent\ntoken order. While the reversal curse is often viewed as a limitation, we offer\nevidence of an alternative view: it is not always an obstacle in practice. We\nfind that RECALL is driven by what we designate as cycle tokens - sequences\nthat connect different parts of the training data, enabling recall of preceding\ntokens from succeeding ones. Through rigorous probabilistic formalization and\ncontrolled experiments, we demonstrate how the cycles they induce influence a\nmodel's ability to reproduce information. To facilitate reproducibility, we\nprovide our code and experimental details at\nhttps:\/\/anonymous.4open.science\/r\/remember-B0B8\/.",
        "Accurate spatiotemporal calibration is a prerequisite for multisensor fusion.\nHowever, sensors are typically asynchronous, and there is no overlap between\nthe fields of view of cameras and LiDARs, posing challenges for intrinsic and\nextrinsic parameter calibration. To address this, we propose a calibration\npipeline based on continuous-time and bundle adjustment (BA) capable of\nsimultaneous intrinsic and extrinsic calibration (6 DOF transformation and time\noffset). We do not require overlapping fields of view or any calibration board.\nFirstly, we establish data associations between cameras using Structure from\nMotion (SFM) and perform self-calibration of camera intrinsics. Then, we\nestablish data associations between LiDARs through adaptive voxel map\nconstruction, optimizing for extrinsic calibration within the map. Finally, by\nmatching features between the intensity projection of LiDAR maps and camera\nimages, we conduct joint optimization for intrinsic and extrinsic parameters.\nThis pipeline functions in texture-rich structured environments, allowing\nsimultaneous calibration of any number of cameras and LiDARs without the need\nfor intricate sensor synchronization triggers. Experimental results demonstrate\nour method's ability to fulfill co-visibility and motion constraints between\nsensors without accumulating errors.",
        "Autonomous LLM-based agents have emerged as a powerful paradigm for complex\ntask execution, yet the field lacks standardized tools for development,\ndeployment, distribution and discovery of agents. We present Cerebrum, an Agent\nSDK for AIOS that addresses this gap through three key components: (1) a\ncomprehensive SDK featuring a modular four-layer architecture for agent\ndevelopment, encompassing LLM, memory, storage, and tool management; (2) a\ncommunity-driven Agent Hub for sharing and discovering agents, complete with\nversion control and dependency management; (3) an interactive web interface for\ntesting and evaluating agents. The platform's effectiveness is demonstrated\nthrough implementations of various agent architectures, including Chain of\nThought (CoT), ReAct, and tool-use agents. Cerebrum advances the field by\nproviding a unified framework that standardizes agent development while\nmaintaining flexibility for researchers and developers to innovate and\ndistribute their agents. The live website is at https:\/\/app.aios.foundation,\nthe code is at https:\/\/github.com\/agiresearch\/Cerebrum, and video is at\nhttps:\/\/app.aios.foundation\/video-demo.",
        "We fix an excellent regular noetherian scheme $S$ over ${\\mathbf Z}_{(p)}$\nsatisfying a certain finiteness condition. For a constructible \\'etale sheaf\n${\\cal F}$ on a regular scheme $X$ of finite type over $S$, we introduce a\nvariant of the singular support relatively to $S$ and prove the existence of a\nsaturated relative variant of the singular support by adopting the method of\nBeilinson using the Radon transform. We may deduce the existence of the\nsingular support itself, if we admit an expected property on the micro support\nof tensor product and if the scheme $X$ is sufficiently ramified over the base\n$S$.",
        "This paper presents a novel approach for hazard analysis in dashcam footage,\naddressing the detection of driver reactions to hazards, the identification of\nhazardous objects, and the generation of descriptive captions. We first\nintroduce a method for detecting driver reactions through speed and sound\nanomaly detection, leveraging unsupervised learning techniques. For hazard\ndetection, we employ a set of heuristic rules as weak classifiers, which are\ncombined using an ensemble method. This ensemble approach is further refined\nwith differential privacy to mitigate overconfidence, ensuring robustness\ndespite the lack of labeled data. Lastly, we use state-of-the-art\nvision-language models for hazard captioning, generating descriptive labels for\nthe detected hazards. Our method achieved the highest scores in the Challenge\non Out-of-Label in Autonomous Driving, demonstrating its effectiveness across\nall three tasks. Source codes are publicly available at\nhttps:\/\/github.com\/ffyyytt\/COOOL_2025.",
        "We construct algebraic families of smooth affine $\\mathbb{A}^1$-contractible\nvarieties of every dimension $n\\geq 4$ over fields of characteristic zero which\nare non-isomorphic to affine spaces and potential counterexamples to the\nZariski Cancellation Problem. We further prove that these families of varieties\nare also counter examples to the generalized Cancellation problem.",
        "This paper provides an explanation of NTRU, a post quantum encryption scheme,\nwhile also providing a gentle introduction to cryptography. NTRU is a very\nefficient lattice based cryptosystem that appears to be safe against attacks by\nquantum computers. NTRU's efficiency suggests that it is a strong candidate as\nan alternative to RSA, ElGamal, and ECC for the post quantum world. The paper\nbegins with an introduction to cryptography and security proofs for\ncryptographic schemes before explaining the NTRU cryptosystem and culminating\nwith a proof that the original presentation of NTRU is not IND-CPA secure. We\nwill conclude by mentioning padding schemes to NTRU that are provably IND-CCA2\nsecure in the random oracle model. The paper is designed to be accessible to\nanyone with minimal background in abstract algebra and number theory - no\nprevious knowledge of cryptography is assumed. Given the author's lack of\nfamiliarity with the subject, this paper aims to be an expository work rather\nthan to provide new insights to the subject matter.",
        "We present Reduced Gaussian Blendshapes Avatar (RGBAvatar), a method for\nreconstructing photorealistic, animatable head avatars at speeds sufficient for\non-the-fly reconstruction. Unlike prior approaches that utilize linear bases\nfrom 3D morphable models (3DMM) to model Gaussian blendshapes, our method maps\ntracked 3DMM parameters into reduced blendshape weights with an MLP, leading to\na compact set of blendshape bases. The learned compact base composition\neffectively captures essential facial details for specific individuals, and\ndoes not rely on the fixed base composition weights of 3DMM, leading to\nenhanced reconstruction quality and higher efficiency. To further expedite the\nreconstruction process, we develop a novel color initialization estimation\nmethod and a batch-parallel Gaussian rasterization process, achieving\nstate-of-the-art quality with training throughput of about 630 images per\nsecond. Moreover, we propose a local-global sampling strategy that enables\ndirect on-the-fly reconstruction, immediately reconstructing the model as video\nstreams in real time while achieving quality comparable to offline settings.\nOur source code is available at https:\/\/github.com\/gapszju\/RGBAvatar.",
        "Accurate agricultural weed mapping using UAVs is crucial for precision\nfarming applications. Traditional methods rely on orthomosaic stitching from\nrigid flight paths, which is computationally intensive and time-consuming.\nGaussian Process (GP)-based mapping offers continuous modelling of the\nunderlying variable (i.e. weed distribution) but requires discretisation for\npractical tasks like path planning or visualisation. Current implementations\noften default to quadtrees or gridmaps without systematically evaluating\nalternatives. This study compares five discretisation methods: quadtrees,\nwedgelets, top-down binary space partition (BSP) trees using least square error\n(LSE), bottom-up BSP trees using graph merging, and variable-resolution\nhexagonal grids. Evaluations on real-world weed distributions measure visual\nsimilarity, mean squared error (MSE), and computational efficiency. Results\nshow quadtrees perform best overall, but alternatives excel in specific\nscenarios: hexagons or BSP LSE suit fields with large, dominant weed patches,\nwhile quadtrees are optimal for dispersed small-scale distributions. These\nfindings highlight the need to tailor discretisation approaches to weed\ndistribution patterns (patch size, density, coverage) rather than relying on\ndefault methods. By choosing representations based on the underlying\ndistribution, we can improve mapping accuracy and efficiency for precision\nagriculture applications.",
        "Accurate traffic forecasting plays a crucial role in modern Intelligent\nTransportation Systems (ITS), as it enables real-time traffic flow management,\nreduces congestion, and improves the overall efficiency of urban transportation\nnetworks. With the rise of Quantum Machine Learning (QML), it has emerged a new\nparadigm possessing the potential to enhance predictive capabilities beyond\nwhat classical machine learning models can achieve. In the present work we\npursue a heuristic approach to explore the potential of QML, and focus on a\nspecific transport issue. In particular, as a case study we investigate a\ntraffic forecast task for a major urban area in Athens (Greece), for which we\npossess high-resolution data. In this endeavor we explore the application of\nQuantum Neural Networks (QNN), and, notably, we present the first application\nof quantum data re-uploading in the context of transport forecasting. This\ntechnique allows quantum models to better capture complex patterns, such as\ntraffic dynamics, by repeatedly encoding classical data into a quantum state.\nAside from providing a prediction model, we spend considerable effort in\ncomparing the performance of our hybrid quantum-classical neural networks with\nclassical deep learning approaches. Our results show that hybrid models achieve\ncompetitive accuracy with state-of-the-art classical methods, especially when\nthe number of qubits and re-uploading blocks is increased. While the classical\nmodels demonstrate lower computational demands, we provide evidence that\nincreasing the complexity of the quantum model improves predictive accuracy.\nThese findings indicate that QML techniques, and specifically the data\nre-uploading approach, hold promise for advancing traffic forecasting models\nand could be instrumental in addressing challenges inherent in ITS\nenvironments.",
        "Let $X$ be a K3 surface defined over a number field $k$, with principal\ncomplex multiplication by a CM field $E$. We find explicit bounds, in terms of\n$k$ and $E$, on the size of the transcendental Brauer group\n$\\operatorname{Br}(X)\/\\operatorname{Br}_1(X)$ of $X$. Bounding the size of this\ngroup is important for computing the Brauer--Manin obstruction, which is\nconjectured by Skorobogatov to be the only obstruction to the Hasse principle\nfor K3 surfaces. Our methods are built on top of earlier work by Valloni, who\nrelated the group $\\operatorname{Br}(X)\/\\operatorname{Br}_1(X)$ to the\narithmetic structure of the CM field $E$. It is from this arithmetic structure\nthat we deduce our bounds.",
        "This paper studies the competition among multiple fund managers with relative\nperformance over the excess logarithmic return. Fund managers compete with each\nother and have expected utility or mean-variance criteria for excess\nlogarithmic return.\n  Each fund manager possesses a unique risky asset, and all fund managers can\nalso invest in a public risk-free asset and a public risk asset. We construct\nboth an $n$-player game and a mean field game (MFG) to address the competition\nproblem under these two criteria. We explicitly define and rigorously solve the\nequilibrium and mean field equilibrium (MFE) for each criteria. In the four\nmodels, the excess logarithmic return as the evaluation criterion of the fund\nleads to the { allocation fractions} being constant. The introduction of the\npublic risky asset yields different outcomes, with competition primarily\naffecting the investment in public assets, particularly evident in the MFG. We\ndemonstrate that the MFE of the MFG represents the limit of the $n$-player\ngame's equilibrium as the competitive scale $n$ approaches infinity. Finally,\nthe sensitivity analyses of the equilibrium are given.",
        "People's opinions on a wide range of topics often evolve over time through\ntheir interactions with others. Models of opinion dynamics primarily focus on\none-dimensional opinions which represent opinions on one topic. However,\nopinions on various topics are rarely isolated; instead, they can be\ninterdependent and exhibit correlations. In a bounded-confidence model (BCM) of\nopinion dynamics, agents influence each other's opinions only if their opinions\nare sufficiently similar. We extend classical agent-based BCMs -- namely, the\nHegeselmann--Krause BCM, which has synchronous interactions, and the\nDeffuant--Weisbuch BCM, which has asynchronous interactions -- to a\nmultidimensional setting, in which the opinions are multidimensional vectors\nrepresenting opinions of different topics and opinions on different topics are\ninterdependent. To measure opinion differences between agents, we introduce\ntopic-weighted discordance functions that account for opinion differences in\nall topics. We use the regions of receptiveness to characterize the\nsteady-state opinion clusters and provide an analytical approach to compute\nthese regions. In addition, we numerically simulate our models on various\nnetworks with initial opinions drawn from a variety of distributions. When\ninitial opinions are correlated across different topics, our topic-weighted\nBCMs yield significantly different results in both transient and steady states\ncompared to baseline models, where the dynamics of each opinion topic are\nindependent.",
        "Clothing-change person re-identification (CC Re-ID) has attracted increasing\nattention in recent years due to its application prospect. Most existing works\nstruggle to adequately extract the ID-related information from the original RGB\nimages. In this paper, we propose an Identity-aware Feature Decoupling (IFD)\nlearning framework to mine identity-related features. Particularly, IFD\nexploits a dual stream architecture that consists of a main stream and an\nattention stream. The attention stream takes the clothing-masked images as\ninputs and derives the identity attention weights for effectively transferring\nthe spatial knowledge to the main stream and highlighting the regions with\nabundant identity-related information. To eliminate the semantic gap between\nthe inputs of two streams, we propose a clothing bias diminishing module\nspecific to the main stream to regularize the features of clothing-relevant\nregions. Extensive experimental results demonstrate that our framework\noutperforms other baseline models on several widely-used CC Re-ID datasets.",
        "Programmable data planes enable users to design data plane algorithms for\nnetwork devices, providing extensive flexibility for network customization.\nProgramming Protocol-Independent Packet Processors (P4) has become the most\nwidely adopted abstraction, programming language, and framework for data plane\nprogramming. However, existing simulation platforms lack high-performance\nsupport for P4-based networks. This paper introduces P4sim, a high-performance\nP4-driven simulation framework built on bmv2 and NS4, seamlessly integrated\nwith ns-3. It improves queue modeling, time scheduling, and P4 architecture\nsupport, extending compatibility to V1model, PSA, and PNA. P4sim enables\nefficient packet processing, accurate time tracking, and seamless interaction\nbetween P4-enabled hosts and switches. We evaluate the P4sim in terms of\nperformance and queue management and demonstrate its capabilities using two\ncommon use cases: Basic Tunneling and Load Balancing. The results highlight the\nP4sim as a powerful tool for advancing research and education in programmable\nnetworks.",
        "Given the hyperinflation that most of the Latin American countries suffered\nin the 90 and their decision towards adopting dollarization and in most cases\nkeeping their own currency, this paper analyzes the effectiveness of\ndollarization as a protective mechanism against economic disruptions in Latin\nAmerican countries. It assesses the context that led Latin American dollarized\ncountries to dollarize and analyzes CPI, GDP, and the poverty rates pre,\nduring, and postpandemic in Latin American countries, considering those that\nare dollarized and those that are not, and evaluating its relation to the US.\nInterviews were carried out with experts in the field. It assesses the\nadvantages and disadvantages of dollarization regarding global crises. The data\nwas compared and analyzed to check if there were patterns that support the\npaper objective which is that dollarization might serve as a protective\nmechanism against economic disruption. It was found that dollarization protects\nthe economy against inflation, however, it does not fully protect the economy\nwhen considering economic performance and poverty. In conclusion, this research\nconcludes that dollarization does not completely serve as a protective\nmechanism against economic disruptions nonetheless, it found that a bigger role\nis played by domestic policies and government action.",
        "The advent of Large Language Models (LLMs) is reshaping education,\nparticularly in programming, by enhancing problem-solving, enabling\npersonalized feedback, and supporting adaptive learning. Existing AI tools for\nprogramming education struggle with key challenges, including the lack of\nSocratic guidance, direct code generation, limited context retention, minimal\nadaptive feedback, and the need for prompt engineering. To address these\nchallenges, we introduce Sakshm AI, an intelligent tutoring system for learners\nacross all education levels. It fosters Socratic learning through Disha, its\ninbuilt AI chatbot, which provides context-aware hints, structured feedback,\nand adaptive guidance while maintaining conversational memory and supporting\nlanguage flexibility. This study examines 1170 registered participants,\nanalyzing platform logs, engagement trends, and problem-solving behavior to\nassess Sakshm AI's impact. Additionally, a structured survey with 45 active\nusers and 25 in-depth interviews was conducted, using thematic encoding to\nextract qualitative insights. Our findings reveal how AI-driven Socratic\nguidance influences problem-solving behaviors and engagement, offering key\nrecommendations for optimizing AI-based coding platforms. This research\ncombines quantitative and qualitative insights to inform AI-assisted education,\nproviding a framework for scalable, intelligent tutoring systems that improve\nlearning outcomes. Furthermore, Sakshm AI represents a significant step toward\nSustainable Development Goal 4 Quality Education, providing an accessible and\nstructured learning tool for undergraduate students, even without expert\nguidance. This is one of the first large-scale studies examining AI-assisted\nprogramming education across multiple institutions and demographics.",
        "Recent studies reveal that vision-language models (VLMs) become more\nsusceptible to harmful requests and jailbreak attacks after integrating the\nvision modality, exhibiting greater vulnerability than their text-only LLM\nbackbones. To uncover the root cause of this phenomenon, we conduct an in-depth\nanalysis and identify a key issue: multimodal inputs introduce an\nmodality-induced activation shift toward a \"safer\" direction compared to their\ntext-only counterparts, leading VLMs to systematically overestimate the safety\nof harmful inputs. We refer to this issue as safety perception distortion. To\nmitigate such distortion, we propose Activation Shift Disentanglement and\nCalibration (ShiftDC), a training-free method that decomposes and calibrates\nthe modality-induced activation shift to reduce the impact of modality on\nsafety. By isolating and removing the safety-relevant component, ShiftDC\nrestores the inherent safety alignment of the LLM backbone while preserving the\nvision-language capabilities of VLMs. Empirical results demonstrate that\nShiftDC significantly enhances alignment performance on safety benchmarks\nwithout impairing model utility.",
        "We construct a Gelfand-Tsetlin representation of $\\mathfrak{sl}_3$ in the\nspace of sections of a local system. The local system lives on an open part of\nthe flag variety given by the intersection of three translates of the big cell\nand has two complex monodromy parameters. We analyze the structure of this\nrepresentation.",
        "Phonetic normalization plays a crucial role in speech recognition and\nanalysis, ensuring the comparability of features derived from raw audio data.\nHowever, in the current paradigm of fine-tuning pre-trained large transformer\nmodels, phonetic normalization is not deemed a necessary step; instead, it is\nimplicitly executed within the models. This study investigates the\nnormalization process within transformer models, especially wav2vec 2.0.\nThrough a comprehensive analysis of embeddings from models fine-tuned for\nvarious tasks, our results demonstrate that fine-tuning wav2vec 2.0 effectively\nachieves phonetic normalization by selectively suppressing task-irrelevant\ninformation. We found that models fine-tuned for multiple tasks retain\ninformation for both tasks without compromising performance, and that\nsuppressing task-irrelevant information is not necessary for effective\nclassification. These findings provide new insights into how phonetic\nnormalization can be flexibly achieved in speech models and how it is realized\nin human speech perception.",
        "Sign Languages are the primary form of communication for Deaf communities\nacross the world. To break the communication barriers between the Deaf and\nHard-of-Hearing and the hearing communities, it is imperative to build systems\ncapable of translating the spoken language into sign language and vice versa.\nBuilding on insights from previous research, we propose a deep learning model\nfor Sign Language Production (SLP), which to our knowledge is the first attempt\non Greek SLP. We tackle this task by utilizing a transformer-based architecture\nthat enables the translation from text input to human pose keypoints, and the\nopposite. We evaluate the effectiveness of the proposed pipeline on the Greek\nSL dataset Elementary23, through a series of comparative analyses and ablation\nstudies. Our pipeline's components, which include data-driven gloss generation,\ntraining through video to text translation and a scheduling algorithm for\nteacher forcing - auto-regressive decoding seem to actively enhance the quality\nof produced SL videos."
      ]
    }
  },
  {
    "id":2411.19475,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"ImageNet: A large-scale hierarchical image database",
    "start_abstract":"The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy"
      ],
      "abstract":[
        "ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications."
      ],
      "categories":[
        "astro-ph.CO"
      ]
    },
    "list":{
      "title":[
        "Improved Decoding of Tanner Codes",
        "Anomaly Detection to identify Transients in LSST Time Series Data",
        "Cherenkov detector with wavelength-shifting fiber readout for muon\n  tomography applications",
        "Diagnosing Quantum Many-body Chaos in Non-Hermitian Quantum Spin Chain\n  via Krylov Complexity",
        "Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson\n  Inverse Problems",
        "Energy Reconstruction of Non-fiducial Electron-Positron Events in the\n  DAMPE Experiment Using Convolutional Neural Networks",
        "Knowledge Phenomenology Research of Future Industrial Iconic Product\n  Innovation",
        "Impact of pH and chloride content on the biodegradation of magnesium\n  alloys for medical implants: An in vitro and phase-field study",
        "Concentration of Measure for Distributions Generated via Diffusion\n  Models",
        "A Comprehensive Framework for Electroweak Phase Transitions: Thermal\n  History and Dynamics from Bubble Nucleation to Percolation",
        "On the inverse-closedness of operator-valued matrices with polynomial\n  off-diagonal decay",
        "Multivariate Frequent Stability and Diam-Mean Equicontinuity",
        "Global existence for semi-linear hyperbolic equations in a neighbourhood\n  of future null infinity",
        "Growth Laws and Universality in 2-TIPS: Microscopic and Coarse grained\n  approach",
        "High pressure structural and lattice dynamics study of\n  {\\alpha}-In$_2$Se$_3$",
        "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 {\\deg}C",
        "Physical Interpretations of Integration Constants in the Solutions of\n  Einstein Equations",
        "Dynamic Routing in Space-Ground Integrated Quantum Networks",
        "Breakdown of broken-symmetry approach to exchange interaction",
        "Multimodal Stock Price Prediction",
        "Fundamental Oscillations of Massive Boson Stars and Distinguishability",
        "Properties of high-redshift Type II supernovae discovered by the JADES\n  transient survey",
        "Phylogenetic Corrections and Higher-Order Sequence Statistics in Protein\n  Families: The Potts Model vs MSA Transformer",
        "Late-time behaviors of scalar field modes for a collapsing null shell\n  spacetime and for the Unruh state in Schwarzschild spacetime",
        "Enhanced collective vibrations in granular materials",
        "Unstable accretion in TW Hya: 3D simulations and comparisons with\n  observations",
        "Quantum Transport in Reduced Graphene Oxide Measured by Scanning Probe\n  Microscopy",
        "Upgrades and maintenance of the CRYRING@ESR electron cooler for improved\n  internal electron target operation",
        "Bose-Bose gases with nonuniversal corrections to the interactions: a\n  droplet phase"
      ],
      "abstract":[
        "In this paper, we present improved decoding algorithms for expander-based\nTanner codes.\n  We begin by developing a randomized linear-time decoding algorithm that,\nunder the condition that $ \\delta d_0 > 2 $, corrects up to $ \\alpha n $ errors\nfor a Tanner code $ T(G, C_0) $, where $ G $ is a $ (c, d, \\alpha, \\delta)\n$-bipartite expander with $n$ left vertices, and $ C_0 \\subseteq \\mathbb{F}_2^d\n$ is a linear inner code with minimum distance $ d_0 $. This result improves\nupon the previous work of Cheng, Ouyang, Shangguan, and Shen (RANDOM 2024),\nwhich required $ \\delta d_0 > 3 $.\n  We further derandomize the algorithm to obtain a deterministic linear-time\ndecoding algorithm with the same decoding radius. Our algorithm improves upon\nthe previous deterministic algorithm of Cheng et al.\\ by achieving a decoding\nradius of $ \\alpha n $, compared with the previous radius of $\n\\frac{2\\alpha}{d_0(1 + 0.5c\\delta) }n$.\n  Additionally, we investigate the size-expansion trade-off introduced by the\nrecent work of Chen, Cheng, Li, and Ouyang (IEEE TIT 2023), and use it to\nprovide new bounds on the minimum distance of Tanner codes. Specifically, we\nprove that the minimum distance of a Tanner code $T(G,C_0)$ is approximately\n$f_\\delta^{-1} \\left( \\frac{1}{d_0} \\right) \\alpha n $, where $ f_\\delta(\\cdot)\n$ is the Size-Expansion Function. As another application, we improve the\ndecoding radius of our decoding algorithms from $\\alpha n$ to approximately\n$f_\\delta^{-1}(\\frac{2}{d_0})\\alpha n$.",
        "We introduce a novel approach to detecting microlensing events and other\ntransients in light curves, utilising the isolation forest (iForest) algorithm\nfor anomaly detection. Focusing on the Legacy Survey of Space and Time by the\nVera C. Rubin Observatory, we show that an iForest trained on signal-less light\ncurves can efficiently identify microlensing events by different types of dark\nobjects and binaries, as well as variable stars. We further show that the\niForest has real-time applicability through a drip-feed analysis, demonstrating\nits potential as a valuable tool for LSST alert brokers to efficiently\nprioritise and classify transient candidates for follow-up observations.",
        "Cherenkov detectors have been extensively developed and utilized in various\nscientific fields, including particle physics, astrophysics, and nuclear\nengineering. These detectors operate based on Cherenkov radiation, which is\nemitted when a charged particle traverses a dielectric medium at a velocity\ngreater than the phase velocity of light in that medium. In this work, we\npresent the development of a Cherenkov radiation detector designed for a muon\ntomography system with high spatial resolution, employing wavelength-shifting\n(WLS) fiber readout. The detector consists of two large-area Cherenkov\nradiators, each measuring 1 m x 1 m, with each read out by WLS fibers arranged\northogonally to determine the x and y coordinates of muon hit positions. The\nsystem is modeled using the GEANT4 simulation package, and the achieved\nposition resolution is 1.8 mm+-0.1 (FWHM). This design enables precise tracking\nof muon trajectories, making it suitable for high-resolution imaging\napplications in muon tomography.",
        "We investigate the phase transitions from chaotic to non-chaotic dynamics in\na quantum spin chain with a local non-Hermitian disorder, which can be realized\nwith a Rydberg atom array setting. As the disorder strength increases, the\nemergence of non-chaotic dynamics is qualitatively captured through the\nsuppressed growth of Krylov complexity, and quantitatively identified through\nthe reciprocity breaking of Krylov space. We further find that the localization\nin Krylov space generates another transition in the weak disorder regime,\nsuggesting a weak ergodicity breaking. Our results closely align with\nconventional methods, such as the entanglement entropy and complex level\nspacing statistics, and pave the way to explore non-Hermitian phase transitions\nusing Krylov complexity and associated metrics.",
        "This paper introduces a novel plug-and-play (PnP) Langevin sampling\nmethodology for Bayesian inference in low-photon Poisson imaging problems, a\nchallenging class of problems with significant applications in astronomy,\nmedicine, and biology. PnP Langevin sampling algorithms offer a powerful\nframework for Bayesian image restoration, enabling accurate point estimation as\nwell as advanced inference tasks, including uncertainty quantification and\nvisualization analyses, and empirical Bayesian inference for automatic model\nparameter tuning. However, existing PnP Langevin algorithms are not well-suited\nfor low-photon Poisson imaging due to high solution uncertainty and poor\nregularity properties, such as exploding gradients and non-negativity\nconstraints. To address these challenges, we propose two strategies for\nextending Langevin PnP sampling to Poisson imaging models: (i) an accelerated\nPnP Langevin method that incorporates boundary reflections and a Poisson\nlikelihood approximation and (ii) a mirror sampling algorithm that leverages a\nRiemannian geometry to handle the constraints and the poor regularity of the\nlikelihood without approximations. The effectiveness of these approaches is\ndemonstrated through extensive numerical experiments and comparisons with\nstate-of-the-art methods.",
        "The Dark Matter Particle Explorer (DAMPE) is a space-based Cosmic-Ray (CR)\nobservatory with the aim, among others, to study Cosmic-Ray Electrons (CREs) up\nto 10 TeV. Due to the low CRE rate at multi-TeV energies, we aim to increasing\nthe acceptance by selecting events outside the fiducial volume. The complex\ntopology of non-fiducial events requires the development of a novel energy\nreconstruction method. We propose the usage of Convolutional Neural Networks\nfor a regression task to recover an accurate estimation of the initial energy.",
        "Iconic products, as innovative carriers supporting the development of future\nindustries, are key breakthrough points for driving the transformation of new\nquality productive forces. This article is grounded in the philosophy of\ntechnology and examines the evolution of human civilization to accurately\nidentify the patterns of product innovation. By integrating theories from\nsystems science, it analyzes the intrinsic logical differences between\ntraditional products and iconic products. The study finds that iconic products\nare based on a comprehensive knowledge system that integrates explicit and\ntacit knowledge, enabling them to adapt to complex dynamic environments.\nTherefore, based on the method of phenomenological essence reduction and the\nprocess of specialized knowledge acquisition, this study establishes the first\nprinciple of knowledge phenomenology: \"knowledge generation-moving from the\ntacit to the explicit-moving from the explicit to the tacit-fusion of the\nexplicit and tacit.\" Grounded in knowledge phenomenology, it reconstructs the\nproduct design evolution process and establishes a forward innovative design\nframework for iconic products, consisting of \"design problem space-explicit\nknowledge space-tacit knowledge space-innovative solution space.\" Furthermore,\nbased on FBS design theory, it develops a disruptive technology innovation\nforecasting framework of \"technology problem space-knowledge base\nprediction-application scenario prediction-coupled technology prediction,\"\nwhich collectively advances the innovation systems engineering of iconic\nproducts. In light of the analysis of the global future industrial competitive\nlandscape, it proposes a strategy for enhancing embodied intelligence in iconic\nproducts.",
        "The individual contributions of pH and chloride concentration to the\ncorrosion kinetics of bioabsorbable magnesium (Mg) alloys remain unresolved\ndespite their significant roles as driving factors in Mg corrosion. This study\ndemonstrates and quantifies hitherto unknown separate effects of pH and\nchloride content on the corrosion of Mg alloys pertinent to biomedical implant\napplications. The experimental setup designed for this purpose enables the\nquantification of the dependence of corrosion on pH and chloride concentration.\nThe in vitro tests conclusively demonstrate that variations in chloride\nconcentration, relevant to biomedical applications, have a negligible effect on\ncorrosion kinetics. The findings identify pH as a critical factor in the\ncorrosion of bioabsorbable Mg alloys. A variationally consistent phase-field\nmodel is developed for assessing the degradation of Mg alloys in biological\nfluids. The model accurately predicts the corrosion performance of Mg alloys\nobserved during the experiments, including their dependence on pH and chloride\nconcentration. The capability of the framework to account for mechano-chemical\neffects during corrosion is demonstrated in practical orthopaedic applications\nconsidering bioabsorbable Mg alloy implants for bone fracture fixation and\nporous scaffolds for bone tissue engineering. The strategy has the potential to\nassess the in vitro and in vivo service life of bioabsorbable Mg-based\nbiomedical devices.",
        "We show via a combination of mathematical arguments and empirical evidence\nthat data distributions sampled from diffusion models satisfy a Concentration\nof Measure Property saying that any Lipschitz $1$-dimensional projection of a\nrandom vector is not too far from its mean with high probability. This implies\nthat such models are quite restrictive and gives an explanation for a fact\npreviously observed in the literature that conventional diffusion models cannot\ncapture \"heavy-tailed\" data (i.e. data $\\mathbf{x}$ for which the norm\n$\\|\\mathbf{x}\\|_2$ does not possess a sub-Gaussian tail) well. We then proceed\nto train a generalized linear model using stochastic gradient descent (SGD) on\nthe diffusion-generated data for a multiclass classification task and observe\nempirically that a Gaussian universality result holds for the test error.\n  In other words, the test error depends only on the first and second order\nstatistics of the diffusion-generated data in the linear setting. Results of\nsuch forms are desirable because they allow one to assume the data itself is\nGaussian for analyzing performance of the trained classifier. Finally, we note\nthat current approaches to proving universality do not apply to this case as\nthe covariance matrices of the data tend to have vanishing minimum singular\nvalues for the diffusion-generated data, while the current proofs assume that\nthis is not the case (see Subsection 3.4 for more details). This leaves\nextending previous mathematical universality results as an intriguing open\nquestion.",
        "The electroweak phase transition (EWPT) is crucial for cosmology and particle\nphysics, with a profound impact on electroweak baryogenesis, symmetry breaking,\nand gravitational wave (GW) signals. However, many studies overlook key aspects\nof EWPT dynamics, leading to misidentified patterns and overestimated GW\nsignals. To address these gaps, we present a comprehensive framework for\nanalyzing EWPTs, focusing on the vacuum's thermal history and dynamics from\nbubble nucleation to percolation. Using the $\\mathbb{Z}_2$-odd real scalar\nsinglet model, we demonstrate the occurrence of spontaneous $\\mathbb{Z}_2$\nsymmetry breaking in the high-temperature vacuum, leading to diverse EWPT\nprocesses, including multi-step transitions and inverse symmetry breaking. We\nidentify four distinct EWPT patterns, each characterized by unique\nsymmetry-breaking mechanisms and associated with bubbles exhibiting distinct\nfield configurations, which can be analyzed using a formalism based on energy\ndensity distributions developed here. A key finding is that bubble nucleation\nfails in extremely strong phase transitions (PTs) with low nucleation rates, or\nin ultra-fast PTs involving inverse $s$-bubbles that collapse instantly upon\nformation, both of which lead to false vacuum trapping and the absence of\nobservable GW signals. In first-order PTs where nucleation succeeds, stronger\ntransitions occur later in the universe's evolution, while weaker transitions\nproceed more rapidly. Multi-step transitions involving (inverse) $\\mathbb{Z}_2$\nsymmetry breaking give rise to complex transition sequences and exotic bubble\ndynamics, such as sequential nucleation or the coexistence of bubbles from\ndifferent vacua -- phenomena with significant implications for GW spectra, dark\nmatter, and baryogenesis. This work advances our understanding of EWPT dynamics\nand lays the groundwork for future studies of EWPTs in BSM physics.",
        "We give a self-contained proof of a recently established\n$\\mathcal{B}(\\mathcal{H})$-valued version of Jaffards Lemma. That is, we show\nthat the Jaffard algebra of $\\mathcal{B}(\\mathcal{H})$-valued matrices, whose\noperator norms of their respective entries decay polynomially off the diagonal,\nis a Banach algebra which is inverse-closed in the Banach algebra\n$\\mathcal{B}(\\ell^2(X;\\mathcal{H}))$ of all bounded linear operators on\n$\\ell^2(X;\\mathcal{H})$, the Bochner-space of square-summable\n$\\mathcal{H}$-valued sequences.",
        "In this paper, we introduce and investigate multivariate versions of frequent\nstability and diam-mean equicontinuity. Given a natural number $m > 1$, we call\nthose notions \"frequent $m$-stability\" and \"diam-mean $m$-equicontinuity\". We\nuse these dynamical rigidity properties to characterise systems whose factor\nmap to the maximal equicontinuous factor (MEF) is finite-to-one for a residual\nset, called \"almost finite-to-one extensions\", or a set of full measure, called\n\"almost surely finite-to-one extensions\". In the case of a $\\sigma$-compact,\nlocally compact, abelian acting group it is shown that frequently\n$(m+1)$-stable systems are equivalently characterised as almost $m$-to-one\nextensions of their MEF. Similarly, it is shown that a system is diam-mean\n$(m+1)$-equicontinuous if and only if it is an almost surely $m$-to-one\nextension of its MEF.",
        "In this paper, we establish the global existence of a semi-linear class of\nhyperbolic equations in 3+1 dimensions, that satisfy the bounded weak null\ncondition. We propose a conformal compactification of the future directed\nnull-cone in Minkowski spacetime, enabling us to establish the solution to the\nwave equation in a neighbourhood of future null infinity. Using this framework,\nwe formulate a conformal symmetric hyperbolic Fuchsian system of equations. The\nexistence of solutions to this Fuchsian system follows from an application of\nthe existence theory developed in [1], and [2].",
        "Two temperature induced phase separation(2-TIPS) is a phenomenon observed in\nmixtures of active and passive particles modeled by scalar activity where the\ntemperature of the particle is proportional to its activity. The binary mixture\nof 'hot' and 'cold' particles phase separate when the relative temperature\ndifference between hot and cold particles defined as activity $\\chi$ exceeds a\ndensity dependent critical value. The study of kinetics in 2-TIPS, a\nnon-equilibrium phase separation, is of fundamental importance in statistical\nphysics. In this paper, we investigate 2-TIPS kinetics using molecular dynamics\n(MD) and coarse-grained (CG) modeling in 3D and 2D. The coarse-grained model\ncouples two passive Model B equations for hot and cold particles, with coupling\nterms emulating the energy transfer between them by raising the temperature of\ncold particles and lowering that of hot particles, a key observation from the\nMD simulations. MD simulations reveal that at high densities, phase separation\nbegins immediately after the quench, forming bi-continuous domains rich in hot\nor cold particles, similar to spinodal decomposition in passive systems. These\ninterconnected domains are also observed in the coarse-grained model for the\nmixture's critical composition. Both MD and CG models show dynamic scaling of\nthe correlation function, indicating self-similar domain growth. Regardless of\ndimensionality, both methods report algebraic growth in domain length with a\ngrowth exponent of $1\/3$, known as the Lifshitz-Slyozov exponent, widely\nobserved in passive systems. Our results demonstrate that the universality of\nphase separation kinetics observed in passive systems also extends to the\nnon-equilibrium binary mixture undergoing 2-TIPS.",
        "Layered $\\alpha$-In$_2$Se$_3$has been studied using a concomitant in-situ\nsynchrotron angle dispersive powder x-ray diffraction and Raman spectroscopy\nstudy in a diamond anvil cell up to 60+ GPa, at room temperature. Helium, that\nremains fairly hydrostatic up to the highest pressure in this study, was used\nas the pressure-transmitting medium. The results from both experimental methods\nreveal a pressure-induced structural phase transition from\n$\\alpha$-In$_2$Se$_3$ to a monoclinic $\\beta$'-In2Se3 structure at $\\approx$1\nGPa, in agreement with previous studies. Based on our detailed measurements\nusing both experimental techniques and F-f formalism, the $\\beta$'-In$_2$Se$_3$\nstructure remains stable up to 45 GPa, without a clear indication of a phase\ntransition towards the previously reported $\\beta$-In2Se3 phase. Above this\npressure, In$_2$Se$_3$ adopts a disordered solid-solution-like orthorhombic\nstructure, phase IV. The results are discussed in comparison with the relevant\nprevious studies of $\\alpha$-In$_2$Se$_3$ under pressure.",
        "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2\/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C.",
        "As in other partial differential equations, one ends up with some arbitrary\nconstants or arbitrary functions when one integrates Einstein's equations, or\nmore generally field equations of any other gravity. Interpretation of these\narbitrary constants and functions as some physical quantities that can in\nprinciple be measured is a non-trivial matter. Concentrating on the case of\nconstants, one usually identifies them as conserved mass, momentum, angular\nmomentum, center of mass, or some other hairs of the solution. This can be done\nvia the Arnowitt-Deser-Misner (ADM)-type construction based on pure geometry;\nand the solution is typically a black hole. Hence, one talks about the black\nhole mass and angular momentum etc. Here we show that there are several\nmisunderstandings: First of all, the physical interpretation of the constants\nof a given geometry depends not only on pure geometry, i.e. the metric, but\nalso on the theory under-consideration. This becomes quite important especially\nwhen there is a cosmological constant. Secondly, one usually assigns the\nmaximally symmetric spacetime, say the flat or the (anti)-de Sitter spacetime,\nto have a zero mass and angular momentum and linear momentum. This declares the\nmaximally symmetric spacetime to be the vacuum of the theory, but such an\nassignment depends on the coordinates in the ADM-type constructions and their\nextensions: in fact, one can introduce large gauge transformations (new\ncoordinates) which map, say, the flat spacetime to flat spacetime but the\nresultant flat spacetime can have a nontrivial mass and angular momentum, if\nthe new coordinates are such that the metric components do not decay properly.\nThese issues, which are often overlooked, will be examined in detail, and a\nresolution, via the use of a divergence-free rank $(0,4)$-tensor will be shown\nfor the case of anti-de Sitter spacetimes.",
        "Quantum networks emerge as fundamental frameworks for addressing various\nlarge-scale problems. There are two primary architectures: space-based quantum\nnetworks, which deploy satellites with free space channels to interconnect\nusers, and ground-based quantum networks, which utilize optical fibers to\ninterconnect users. In this paper, we explore space-ground integrated quantum\nnetworks that incorporate both satellites and optical fibers into the\ninfrastructure. This integrated network features three forms of communication:\nusing only free space links, only ground links, or a hybrid usage of free space\nand ground links. We formulate the routing problem in space-ground integrated\nquantum networks as an integer programming and propose two solutions: using a\nlinear relaxation and a greedy algorithm. The linear relaxation algorithm\nallows timely scheduling of additional entanglement purification, whereas the\ngreedy algorithm enables quick scheduling. Simulation results demonstrate their\neffective balancing between network throughput and communication fidelity.",
        "Broken-symmetry (BS) approaches are widely employed to evaluate Heisenberg\nexchange parameters, primarily in combination with DFT calculations. For many\nmagnetic materials, BS-DFT calculations give reasonable estimations of exchange\nparameters although systematic failures have also been reported. While the\nlatter were attributed to deficiencies of approximate exchange-correlation\nfunctional, we prove here by treating a simple model system that the\nbroken-symmetry methodology has serious problems. Detailed analysis clarifies\nthe intrinsic issue with the broken-symmetry treatment of low-spin states. It\nshows, in particular, that the error in the BS calculation of exchange\nparameter scales with the degree of covalency between the magnetic and the\nbridging orbitals. As a possible tool to overcome this intrinsic drawback of\nsingle-determinant BS approaches, we propose their extension to a minimal\nmulticonfigurational version.",
        "In an era where financial markets are heavily influenced by many static and\ndynamic factors, it has become increasingly critical to carefully integrate\ndiverse data sources with machine learning for accurate stock price prediction.\nThis paper explores a multimodal machine learning approach for stock price\nprediction by combining data from diverse sources, including traditional\nfinancial metrics, tweets, and news articles. We capture real-time market\ndynamics and investor mood through sentiment analysis on these textual data\nusing both ChatGPT-4o and FinBERT models. We look at how these integrated data\nstreams augment predictions made with a standard Long Short-Term Memory (LSTM\nmodel) to illustrate the extent of performance gains. Our study's results\nindicate that incorporating the mentioned data sources considerably increases\nthe forecast effectiveness of the reference model by up to 5%. We also provide\ninsights into the individual and combined predictive capacities of these\nmodalities, highlighting the substantial impact of incorporating sentiment\nanalysis from tweets and news articles. This research offers a systematic and\neffective framework for applying multimodal data analytics techniques in\nfinancial time series forecasting that provides a new view for investors to\nleverage data for decision-making.",
        "Massive Boson Stars are self-gravitating configurations of self-interacting\nscalar fields. The equation of state of massive boson stars and their masses,\nradii, modeled by a self-interacting scalar field with potential of the form\n$V(\\phi) = \\frac{1}{2}m^2|\\phi|^2 + \\frac{1}{4}\\lambda |\\phi|^4$ are known to\nfollow scaling relations. The non-radial fundamental oscillations of such\nmassive BSs have been studied only for a few select model parameters so far. In\nthis work, we demonstrate for the first time that the $f$-mode characteristics\nalso follow a scaling in the strong interaction limit ($\\lambda \\gg\nm^2\/M_{Pl}^2$). This opens up the outstanding prospect of studying the\n$f$-modes of massive BSs throughout the scalar DM parameter space. We study the\nimplications of this finding by carrying out a detailed study of massive BS\n$f$-modes in a separate work. Here, having introduced this scaling, we use it\nto compare boson star oscillations with the neutron star and black hole\nquasinormal modes, thus providing a smoking gun for the distinguishability of\nBSs using gravitational waves.",
        "In this work we estimate the explosion and progenitor properties of six Type\nII supernovae (SNe) at 0.675 <= z <= 3.61 discovered by the JWST Advanced Deep\nExtragalactic Survey (JADES) transient survey by modeling their light curves.\nThis high-redshift Type II SN sample allows us to compare low-redshift Type II\nSNe to their high-redshift counterparts. Two Type II SNe are found to have high\nexplosion energies of 3e51 erg, while the other four Type II SNe are estimated\nto have typical explosion energies found in the local Universe [(0.5-2)e51\nerg]. The fraction of Type II SNe with high explosion energies might be higher\nat high redshifts because of, e.g., lower metallicity, but it is still\ndifficult to draw a firm conclusion because of the small sample size and\npotential observational biases. We found it difficult to constrain the\nprogenitor masses for Type II SNe in our sample because of the sparse\nlight-curve data. We found two Type II SN light curves can be better reproduced\nby introducing confined, dense circumstellar matter. Thus, the confined, dense\ncircumstellar matter frequently observed in nearby Type II SNe is likely to\nexist in Type II SNe at high redshifts as well. Two Type II SNe are estimated\nto have high host galaxy extinctions, showing the ability of JWST to discover\ndust-obscured SNe at high redshifts. More high-redshift Type II SNe are\nrequired to investigate the differences in the properties of Type II SNe near\nand far, but here we show the first glimpse into the high-redshift population\nof Type II SNe.",
        "Recent generative learning models applied to protein multiple sequence\nalignment (MSA) datasets include simple and interpretable physics-based Potts\ncovariation models and other machine learning models such as MSA-Transformer\n(MSA-T). The best models accurately reproduce MSA statistics induced by the\nbiophysical constraints within proteins, raising the question of which\nfunctional forms best model the underlying physics. The Potts model is usually\nspecified by an effective potential including pairwise residue-residue\ninteraction terms, but it has been suggested that MSA-T can capture the effects\ninduced by effective potentials which include more than pairwise interactions\nand implicitly account for phylogenetic structure in the MSA. Here we compare\nthe ability of the Potts model and MSA-T to reconstruct higher-order sequence\nstatistics reflecting complex biological sequence constraints. We find that the\nmodel performance depends greatly on the treatment of phylogenetic\nrelationships between the sequences, which can induce non-biophysical\nmutational covariation in MSAs. When using explicit corrections for\nphylogenetic dependencies, we find the Potts model outperforms MSA-T in\ndetecting epistatic interactions of biophysical origin.",
        "The behaviors of the modes for a massless minimally coupled scalar field are\ninvestigated for the Unruh state for Schwarzschild spacetime and the \"in\"\nvacuum state for a spacetime in which a spherically symmetric null shell\ncollapses to form a nonrotating black hole. In both cases there are two\ndifferent sets of solutions to the mode equation that make up the state. For\nboth spacetimes, one set of modes oscillates forever with no damping of the\noscillations and the other set approaches zero at late times. The difference\nbetween a mode that oscillates forever in the null-shell spacetime and the\ncorresponding mode for the Unruh state vanishes as a power law in time. The\nmodes that approach zero at late times also vanish at late times as a power law\nin time. In all cases the power-law damping is preceded by a period of\noscillations that appear to be due to quasi-normal modes.",
        "Granular materials are defined as collections of macroscopic dissipative\nparticles. Although these systems are ubiquitous in our lives, the nature and\nthe causes of their non-trivial collective dynamics still remain elusive and\nhave attracted significant interest in non-equilibrium physics. Here, we focus\non the vibrational dynamics of granular materials. While the vibrational\ndynamics of random packings have been examined concerning the jamming\ntransition, previous research has overlooked the role of contact dissipations.\nWe conducted numerical and analytical investigations into the vibrational\ndynamics of random packings influenced by the normal dissipative force, which\nis the simplest model for contact dissipations. Our findings reveal that the\nkinetic energy per mode diverges in the low-frequency range, following the\nscaling law $\\mathcal{K}_l \\propto \\omega^{-2}_l$ with the frequency\n$\\omega_l$, indicating that low-frequency modes experience strong excitation\nand that the equipartition of energy is violated. Additionally, the spatial\nstructure factor of the velocity field displays the scaling law $S_v(q) \\propto\nq^{-2}$ with the wavenumber $q$, which signifies that the velocity field has an\ninfinitely long range. We demonstrate that these phenomena arise from the\neffects of weaker damping on softer modes, where the particle displacements\nparallel to the contacts are minimal in the low-frequency modes, rendering\nnormal dissipation ineffective at dampening these modes.",
        "We investigate the origin of photometric variability in the classical T Tauri\nstar TW Hya by comparing light curves obtained by TESS and ground-based\ntelescopes with light curves created using three-dimensional (3D)\nmagnetohydrodynamic (MHD) simulations. TW Hya is modeled as a rotating star\nwith a dipole magnetic moment, slightly tilted about the rotational axis. We\nobserved that for various model parameters, matter accretes in the unstable\nregime and produces multiple hot spots on the star's surface, which leads to\nstochastic-looking light curves similar to the observed ones. Wavelet and\nFourier spectra of observed and modeled light curves show multiple\nquasiperiodic oscillations (QPOs) with quasiperiods from less than 0.1 to 9\ndays. Models show that variation in the strength and tilt of the dipole\nmagnetosphere leads to different periodograms, where the period of the star may\ndominate or be hidden. The amplitude of QPOs associated with the stellar period\ncan be smaller than that of other QPOs if the tilt of the dipole magnetosphere\nis small and when the unstable regime is stronger. In models with small\nmagnetospheres, the short-period QPOs associated with rotation of the inner\ndisc dominate and can be mistaken for a stellar period. We show that\nlonger-period (5-9 days) QPOs can be caused by waves forming beyond the\ncorotation radius.",
        "We report combined scanning probe microscopy and transport measurements to\ninvestigate the local electronic transport properties of reduced graphene oxide\n(rGO) devices. We demonstrate that the quantum transport properties in these\nmaterials can be significantly tuned by the electrostatic potential applied by\nan atomic force microscope (AFM) conducting tip. Scanning gate microscopy\nmeasurements show a distinct p-type response, where the AFM tip locally gates\nthe rGO, thereby modulating the transport current. Additional scanning\nimpedance microscopy measurements indicate shifts in the Fermi energy under\ndifferent gating conditions, highlighting the strong influence of local\nelectrostatic potentials on the transport characteristics of rGO. We\ndemonstrate that the interplay between the tip-induced Fermi level shifts and\ndefect-mediated scattering processes plays a key role in determining the\nobserved transport behavior. Our findings emphasize the crucial role of\nscattering mechanisms, particularly resonant scattering caused by impurities or\nstructural defects, in determining low-dimensional transport behavior in rGO.\nNotably, rGO exhibits resonant scattering effects akin to those seen in\none-dimensional systems like carbon nanotubes, despite its two-dimensional\nstructure. These insights advance our current understanding of charge transport\nin rGO, and have important implications for its use in nanoscale electronics,\nflexible sensors, and tunable optoelectronic devices.",
        "The electron cooler of the CRYRING@ESR storage ring at the GSI-FAIR\naccelerator complex is a unique instrument, which not only provides beam\ncooling of the stored ions, but also serves as a low-energy electron target for\nDielectronic Recombination experiments. The minimisation of vacuum\ncontamination and the response to rapid energy changes are key requirements of\nthe cooler in electron target mode. Therefore, a test bench was prepared to\nstudy the outgassing behaviour of the electron gun components and a setup was\nconstructed to evaluate the drift-electrode-modulation of the acceleration\nvoltage of the cooler. The vacuum studies showed that the electron gun cathode\nwas severely malfunctioning and resulted in its replacement. The\ndrift-electrode-modulation of the acceleration voltage showed significant\nimprovements compared to direct modulation of the terminal voltage of the\ncooler.",
        "Through an effective quantum field theory within Bogoliubov's framework and\ntaking into account nonuniversal effects of the interatomic potential we\nanalytically derive the leading Gaussian zero- and finite-temperature\ncorrections to the equation of state of ultracold interacting Bose-Bose gases.\nWe calculate the ground-state energy per particle at zero and low temperature\nfor three-, two- and one-dimensional two-component bosonic gases. By tuning the\nnonuniversal contribution to the interactions we address and establish\nconditions under which the formation and stability of a self-bound liquidlike\nphase or droplet with nonuniversal corrections to the interactions DNUC) is\nfavorable. At zero temperature in three-dimensions and considering the\nnonuniversal corrections to the attractive interactions as a fitting parameter\nthe energy per particle for DNUC is in good agreement with some diffusion Monte\nCarlo results. In two dimensions the DNUC present small deviations regarding\nconventional droplets. For the one-dimensional DNUC the handling of the\nnonuniversal effects to the interactions achieves a qualitative agreement with\nthe trend of some available Monte Carlo data in usual droplets. We also\nintroduce some improved Gross-Pitaevskii equations to describe self-trapped\nDNUC in three, two and one dimension. We briefly discuss some aspects at low\ntemperature regarding nonuniversal corrections to the interactions in Bose-Bose\ngases. We derive the dependencies on the nonuniversal contribution to the\ninteractions but also on the difference between intra- and inter-species\ncoupling constants. This last dependence crucially affect the three- and the\ntwo-dimensional DNUC driving thus to a thermal-induced instability. This\nthermal instability is also present in one-dimensional Bose-Bose gases, but it\nis not relevant on the formation of DNUC..."
      ]
    }
  },
  {
    "id":2411.17595,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Machine learning model to predict oncologic outcomes for drugs in randomized clinical trials",
    "start_abstract":"Abstract Predicting oncologic outcome is challenging due to the diversity of cancer histologies and complex network underlying biological factors. In this study, we determine whether machine learning (ML) can extract meaningful associations between clinical trial, drug\u2010related biomarker molecular profile information. We analyzed therapeutic trials corresponding 1102 outcomes from 104 758 patients with advanced colorectal adenocarcinoma, pancreatic melanoma nonsmall\u2010cell lung cancer. For each intervention arm, a dataset following attributes was curated: line treatment, number cytotoxic chemotherapies, small\u2010molecule inhibitors, or monoclonal antibody agents, drug class, alteration status arm's population, type, probability sensitivity (PDS) (integrating genomic, transcriptomic proteomic biomarkers in population interest) outcome. A total 467 progression\u2010free survival (PFS) 369 overall (OS) data points were used as training sets build our ML (random forest) model. Cross\u2010validation for PFS OS, obtaining correlation coefficients ( r ) 0.82 0.70, respectively (outcome vs model's parameters). 156 110 OS test sets. The Spearman s predicted actual statistically significant (PFS: = 0.879, OS: 0.878, P &lt; .0001). better arm 81% N 59\/73, z 5.24, .0001) 71% (OS: 37\/52, 2.91, .004) randomized trials. success algorithm predict may be exploitable model optimize trial design pharmaceutical agents.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Lint: Llm interaction network for clinical trial outcome prediction"
      ],
      "abstract":[
        "Clinical trial outcome prediction aims to predict the success probability of a clinical trial that reaches its desirable endpoint. Most of the effort focuses on developing machine learning models for making accurate predictions with diverse data sources, including clinical trial descriptions, drug molecules, and target disease conditions. Accurate trial outcome prediction helps trial planning and asset portfolio prioritization. Previous works have focused on small-molecule drugs; however, biologics are a quickly growing intervention type that lacks information that is traditionally known for drugs, like molecular properties. Additionally, traditional methods like graph neural networks are much more difficult to apply to biologics data which are a fast-growing type of drug. To address these points, we propose a Language Interaction Network (LINT), a novel method for trial outcome prediction using only free-text descriptions. We validate the effectiveness of LINT with thorough experiments across three trial phases. Specifically, LINT obtains 0.770, 0.740, and 0.748 ROC-AUC scores on phase I, II, and III, respectively, for clinical trials with biologic interventions."
      ],
      "categories":[
        "Clinical trials"
      ]
    },
    "list":{
      "title":[
        "Orbital Wigner functions and quantum transport in multiband systems",
        "Determination of $\\alpha_s(M_Z)$ via a high-precision effective coupling\n  $\\alpha^{g_1}_s(Q)$",
        "Some Constructions on Quantum Principal Bundles",
        "Dielectric nanotomography based on electrostatic force microscopy: A\n  numerical analysis",
        "Monotonicity of the Relative Entropy and the Two-sided Bogoliubov\n  Inequality in von Neumann Algebras",
        "Beyond Freeze-Out: A Novel Freeze-in Mechanism for Dark Matter via\n  Supercooled Phase Transitions",
        "Quantum Perspectivism vs Nietzschean Perspectivism",
        "Topological blocking at the Bi(111) surface due to surface relaxation",
        "Gate Tunable Josephson Diode Effect in Josephson Junctions made from\n  InAs Nanosheets",
        "Intrinsic regularity in the discrete log-Sobolev inequality",
        "Stability of the long-range corrected exchange-correlation functional in\n  time-dependent density-functional theory",
        "Study of amorphous alumina coatings for next-generation nuclear\n  reactors: hightemperature in-situ and post-mortem Raman spectroscopy and\n  X-ray diffraction",
        "Inside Out: Externalizing Assumptions in Data Analysis as Validation\n  Checks",
        "Quantum trajectories and Page-curve entanglement dynamics",
        "Thermodynamic properties of fcc lead: A scalar and fully relativistic\n  first principle study",
        "PLS-based approach for fair representation learning",
        "High Order Boundary Extrapolation Technique for Finite Difference\n  Methods on Complex Domains with Cartesian Meshes",
        "Relative phase between $s_{\\pm}$ superconducting order parameter\n  components in a two-band model with impurities",
        "Blind free deconvolution over one-parameter sparse families via\n  eigenmatrix",
        "Modelling lined rock caverns subject to hydrogen embrittlement and\n  cyclic pressurisation in fractured rock masses",
        "Limit theorems for the fluctuation of the mixed elephant random walk in\n  the superdiffusive case",
        "Overdamped van der Waals Josephson junctions by area engineering",
        "Supertetragonal BaSnO3 induced giant ferroelectricity in SrTiO3\/BaSnO3\n  superlattices",
        "Entangled mixed-state datasets generation by quantum machine learning",
        "Avoiding Overfitting in Variable-Order Markov Models: a Cross-Validation\n  Approach",
        "Evidence of a diffuse, extended continuum source in quasars from the\n  relative sizes of the broad line region and the UV-optical continuum source\n  measured with microlensing",
        "The Gordon-Litherland pairing and its many applications",
        "The Convergence of Dynamic Routing between Capsules",
        "Magnon cat states in a cavity-magnon-qubit system via two-magnon driving\n  and dissipation"
      ],
      "abstract":[
        "Traditional theories of electron transport in crystals are based on the\nBoltzmann equation and do not capture physics arising from quantum coherence.\nWe introduce a transport formalism based on ''orbital Wigner functions'', which\naccurately captures quantum coherent physics in multiband fermionic systems. We\nillustrate the power of this approach compared to traditional semiclassical\ntransport theory by testing it numerically against microscopic simulations of\none-dimensional, non-interacting, two-band systems -- the simplest systems\ncapable of exhibiting inter-orbital coherence. We show that orbital Wigner\nfunctions accurately capture strongly non-equilibrium features of electron\ndynamics that lie beyond conventional Boltzmann theory, such as the ballistic\ntransport of a relative phase between microscopic orbitals and topological\nThouless pumping of charge both at non-zero temperature and away from the\nadiabatic limit. Our approach is motivated in part by modern ultracold atom\nexperiments that can prepare and measure far-from-equilibrium charge transport\nand phase coherence in multiband fermionic systems, calling for correspondingly\nprecise theories of transport. The quantitative accuracy exhibited by our\napproach, together with its capacity to capture nontrivial physics even at the\nballistic scale, establishes orbital Wigner functions as an ideal starting\npoint for developing a fully systematic theory of transport in crystals.",
        "We propose a novel method to determine the strong coupling of quantum\nchromodynamics (QCD) and fix its running behavior at all scales by using the\nBjorken sum rules (BSR). The BSR defines an effective coupling\n$\\alpha^{g_1}_s(Q)$ which includes the nonperturbative high-twist corrections\nand perturbative QCD (pQCD) corrections to the leading-twist part. For the\nleading-twist part of $\\alpha^{g_1}_s(Q)$, we adopt the infinite-order\nscale-setting procedure of the principle of maximum conformality\n($\\rm{PMC}_\\infty$) to deal with its pQCD corrections, which reveals the\nintrinsic conformality of series and eliminates conventional renormalization\nscheme-and-scale ambiguities. Using the $\\rm{PMC}_\\infty$ approach, we not only\neliminate \\textit{the first kind of residual scale dependence} due to\nuncalculated higher-order terms, but also resolve the previous\n``self-consistence problem\". The holographic light-front QCD model is used for\n$\\alpha^{g_1}_s(Q)$ in the infrared region, which also reveals a conformal\nbehavior at $Q\\to 0$. As a combination, we obtain a precise $\\alpha^{g_1}_s(Q)$\nat all scales, which matches well with the known experimental data with\n$p$-value $\\sim99\\%$, we determine the strong coupling constant at the critical\nscale $M_Z$, $\\alpha_s(M_Z)=0.1191\\pm{0.0012}\\mp0.0006$, where the first error\ncomes from $\\Delta\\kappa$ of LFHQCD model and the second error is from\n\\textit{the second kind of residual scale dependence} that is negligible.",
        "This paper works as an appendix of the paper titled Geometry of Associated\nQuantum Vector Bundles and the Quantum Gauge Group. Here, we are going to prove\nfour statements in the theory of quantum principal bundles:: 1) The universal\ndifferential envelope $\\ast$--calculus of a matrix (compact) Lie group, for the\nclassical bicovariant $\\ast$--First Order Differential Calculus, is the algebra\nof differential forms. 2) An example of a quantum principal bundle in which the\nspace of base forms is not generated by the base space. 3) The group\nisomorphism between convolution-invertible maps and covariant left module\nisomorphisms at the level of differential calculus 4) The way the maps $\\{T^V_k\n\\}$ from Remark 3.1 look in differential geometry.",
        "Electrostatic force microscopy (EFM) can image nanoscale objects buried below\nthe surface. Here, we theoretically show that this capability can be used to\nobtain nanotomographic information, i.e., the physical dimensions and\ndielectric properties, of buried nano-objects. These results constitute a first\nstep toward implementing a nondestructive dielectric nanotomography technique\nbased on EFM with applications in materials sciences and life sciences.",
        "This text studies, on the one hand, certain monotonicity properties of the\nAraki-Uhlmann relative entropy and, on the other hand, unbounded perturbation\ntheory of KMS-states which facilitates a proof of the two-sided Bogoliubov\ninequality in general von Neumann algebras. After introducing the necessary\nbackground from the theory of operator algebras and Tomita-Takesaki modular\ntheory, the relative entropy functional is defined and its basic properties are\nstudied. In particular, a full and detailed proof of Uhlmann's important\nmonotonicity theorem for the relative entropy is provided. This theorem will\nthen be used to derive a number of monotonicity inequalities for the relative\nentropy of normal functionals induced by vectors of the form $V \\varOmega, V\n\\varPhi \\in \\mathcal{H}$, where $V \\in \\mathscr{B}(\\mathcal{H})$ is a suitable\ntransformation. After that, an introduction to perturbation theory in von\nNeumann algebras is given, with an emphasis on unbounded perturbations of\nKMS-states following the framework of Derezi\\'{n}ski-Jak\\v{s}i\\'{c}-Pillet.\nThis mathematical apparatus will then be used to extend the two-sided\nBogoliubov inequality for the relative free energy, which was very recently\nproved for quantum-mechanical systems, to arbitrary von Neumann algebras.",
        "We investigate a novel freeze-in mechanism for Weakly Interacting Massive\nParticles (WIMPs), facilitated by a supercooled first-order phase transition\n(FOPT) in the early universe. Unlike the conventional freeze-out and freeze-in\nscenarios, this mechanism allows WIMPs to acquire their relic abundance through\na non-equilibrium production process following a rapid entropy injection. We\nexplore multiple dark matter (DM) candidates, including vector, fermionic, and\nscalar-mediated models, and identify fermionic DM with a pseudoscalar mediator\nas the most viable candidate for this mechanism. The study demonstrates that\nFOPT dilutes preexisting DM density and simultaneously induces a rapid mass\nincrease, preventing thermal re-equilibration and enabling DM production via\nfreeze-in. Additionally, we analyze the gravitational wave (GW) signals\nassociated with FOPT, identifying parameter regions detectable by future GW\nobservatories such as LISA and UDECIGO. This framework offers a compelling\nalternative to traditional WIMP and Feebly Interacting Massive Particle (FIMP)\nscenarios, providing new avenues for DM model building and experimental\nverification.",
        "This is a work of hard physical philosophy, where Quantum Perspectivism is\nshown to function as both an interpretation of quantum mechanics and a physical\nmodel for understanding Nietzsche's perspectivism. This framework combines\nquantum logic, the principle of complementarity, and contextuality to examine\nhow perspectives construct reality. In this model, measurements correspond to\nPerspectives and Meta-Perspectives, represented as Boolean subalgebras and\nHilbert sub-lattices within the Hilbert lattice, respectively. The Hilbert\nlattice itself is reinterpreted as Jung's Unus Mundus, a unified ontological\nreality. A metaphysical observation, made by a metaphysical observer, of a\ngiven system (World) is identified with the set of all corresponding\nmeta-perspectives in the Hilbert lattice\/Unus Mundus, the ocean of reality.\n  Perspectives, likened to islands in this ocean, correspond to single\nmeasurements of a system, capturing the logical structure of observed\nproperties. Meta-perspectives, analogous to continents, represent the synthesis\nof multiple measurements, providing a broader yet inherently incomplete\nunderstanding of the system. This structure emphasizes the complementarity and\ncontextual dependencies of measurements while exposing the limitations of\nclassical objectivity in the quantum domain. Advocating for a perspectival view\nof both the world and truth, Quantum Perspectivism unites quantum mechanics and\nNietzschean philosophy into a cohesive framework for exploring the interplay\nbetween consciousness, observation, and reality.",
        "The topological characteristics of Bi and its alloys with Sb have fueled\nintense debate since the prediction of three-dimensional topological\ninsulators. However, a definitive resolution has not been reached to date.\nHere, we provide theoretical evidence that surface relaxation conceals the\nunderlying bulk topology of pure Bi. Using density functional theory\ncalculations for thin Bi(111) films (up to 17 bilayers), we first demonstrate a\nsubstantial inter-bilayer expansion near the surface. Motivated by this\nfinding, we extend our analysis to thick Bi(111) films (up to 250 bilayers)\nincorporating relaxation layers, within the framework of a relativistic\nempirical tight-binding model. Our results reveal that these relaxation layers\ntopologically block the emergence of surface state and significantly suppress\nthe one-particle spectrum of surface states, thereby obscuring the experimental\nidentification of Bi's topological properties. This phenomenon, which we term\n\"topological blocking\", provides crucial insights into the long-standing\ndifficulty of observing surface states of Bi(111) at the $\\bar{M}$ point.\nFurthermore, it establishes a framework for understanding and predicting the\ntopological behavior in systems where surface relaxation disrupts the bulk-edge\ncorrespondence.",
        "We report the observation of Josephson diode effect (JDE) in hybrid devices\nmade from semiconductor InAs nanosheets and superconductor Al contacts. By\napplying an in-plane magnetic field ($B_{\\mathrm{xy}}$), we detect\nnon-reciprocal superconducting switching current as well as non-reciprocal\nsuperconducting retrapping current. The strength of the JDE depends on the\nangle between the in-plane magnetic field and the bias current\n($I_{\\mathrm{b}}$), reaching its maximum when $B_{\\mathrm{xy}} \\perp\nI_{\\mathrm{b}}$ and dropping to nearly zero when $B_{\\mathrm{xy}}\\parallel\nI_{\\mathrm{b}}$. Additionally, the diode efficiency is tunable via an\nelectrostatic gate with a complete suppression at certain gate voltages. Our\nfindings indicate that the observed JDE in InAs nanosheet-based Josephson\njunctions most likely arises from the Rashba spin-orbit interaction (SOI) in\nthe nanosheets. Such gate-tunable JDE in Josephson junctions made from\nsemiconductor material with SOI is useful not only for constructing advanced\nsuperconducting electronics but also for detecting novel superconducting\nstates.",
        "The chain rule lies at the heart of the powerful Gamma calculus for Markov\ndiffusions on manifolds, providing remarkable connections between several\nfundamental notions such as Bakry-\\'Emery curvature, entropy decay, and\nhypercontractivity. For Markov chains on finite state spaces, approximate\nversions of this chain rule have recently been put forward, with an extra cost\nthat depends on the log-Lipschitz regularity of the considered observable.\nMotivated by those findings, we here investigate the regularity of extremizers\nin the discrete log-Sobolev inequality. Specifically, we show that their\nlog-Lipschitz constant is bounded by a universal multiple of $\\log d$, where\n$d$ denotes the inverse of the smallest non-zero transition probability. As a\nconsequence, we deduce that the log-Sobolev constant of any reversible Markov\nchain on a finite state space is at least a universal multiple of $\\kappa\/\\log\nd$, where $\\kappa$ is the Bakry-\\'Emery curvature. This is a sharp discrete\nanalogue of what is perhaps the most emblematic application of the\nBakry-\\'Emery theory for diffusions. We also obtain a very simple proof of the\nmain result in \\cite{MR4620718}, which asserts that the log-Sobolev constant\nand its modified version agree up to a $\\log d$ factor. Our work consolidates\nthe role of the sparsity parameter $\\log d$ as a universal cost for\ntransferring results from Markov diffusions to discrete chains.",
        "Excitonic effects in the optical absorption spectra of solids can be\ndescribed with time-dependent density-functional theory (TDDFT) in the\nlinear-response regime, using a simple class of approximate, long-range\ncorrected (LRC) exchange-correlation functionals. It was recently demonstrated\nthat the LRC approximation can also be employed in real-time TDDFT to describe\nexciton dynamics. Here, we investigate the numerical stability of the\ntime-dependent LRC approach using a two-dimensional model solid. It is found\nthat the time-dependent Kohn-Sham equation with an LRC vector potential becomes\nmore and more prone to instabilities for increasing exciton binding energies.\nThe origin of these instabilities is traced back to time-averaged violations of\nthe zero-force theorem, which leads to a simple and robust numerical\nstabilization scheme. This explains and justifies a recently proposed method by\nDewhurst et al., arXiv:2401.16140.",
        "The present work focuses on the investigation of the thermal stability and\nstructural integrity of amorphous alumina coatings intended for use as\nprotective coatings on cladding tubes in Generation IV nuclear reactors,\nspecifically in the Lead-cooled Fast Reactor (LFR) type. Hightemperature Raman\nspectroscopy and high-temperature X-ray diffraction analyses were carried out\nup to 1050 C on a 5 um coating deposited by the pulsed laser deposition (PLD)\ntechnique on a 316L steel substrate. The experiments involved the in-situ\nexamination of structural changes in the material under increasing temperature,\nalong with ex-situ Raman imaging of the surface and cross-section of the\ncoating after thermal treatments of different lengths. As it was expected, the\npresence of alpha-alumina was detected with the addition of other polymorphs,\ngamma- and theta-Al2O3, found in the material after longer high-temperature\nexposure. The use of two structural analysis methods and two lasers excitation\nwavelengths with Raman spectroscopy allowed us to detect all the mentioned\nphases despite different mode activity. Alumina analysis was based on the\nemission spectra, while substrate oxidation products were identified through\nthe structural bands. The experiments depicted a dependence of the phase\ncomposition of oxidation products and alumina's degree of crystallization on\nthe length of the treatment. Nevertheless, the observed structural changes did\nnot occur rapidly, and the coating's integrity remained intact. Moreover,\noxidation signs occurred locally at temperatures exceeding the LFR reactor's\nworking temperature, confirming the material's great potential as a protective\ncoating in the operational conditions of LFR nuclear reactors.",
        "In data analysis, unexpected results often prompt researchers to revisit\ntheir procedures to identify potential issues. While some researchers may\nstruggle to identify the root causes, experienced researchers can often quickly\ndiagnose problems by checking a few key assumptions. These checked assumptions,\nor expectations, are typically informal, difficult to trace, and rarely\ndiscussed in publications. In this paper, we introduce the term *analysis\nvalidation checks* to formalize and externalize these informal assumptions. We\nthen introduce a procedure to identify a subset of checks that best predict the\noccurrence of unexpected outcomes, based on simulations of the original data.\nThe checks are evaluated in terms of accuracy, determined by binary\nclassification metrics, and independence, which measures the shared information\namong checks. We demonstrate this approach with a toy example using step count\ndata and a generalized linear model example examining the effect of particulate\nmatter air pollution on daily mortality.",
        "We consider time dynamics of entanglement entropy between a filled fermionic\nsystem and an empty reservoir. We consider scenarios (i) where the system is\nsubjected to a dephasing mechanism and the reservoir is clean, thereby\nemulating expansion of effectively interacting fermions in vacuum, and (ii)\nwhere both the system and the reservoir are subjected to dephasing and thereby\nenabling us to address how the entanglement between the part of the effectively\ninteracting system and its complement evolves in time. We consider two\ndifferent kinds of quantum trajectory approaches, namely stochastic unitary\nunraveling and quantum state diffusion. For both protocols, we observe and\ncharacterize the full Page curve-like dynamics for the entanglement entropy.\nDepending on the protocol and the setup, we observe very distinct\ncharacteristics of the Page curve and the associated Page time and Page value.\nWe also compute the number of fermions leaking to the reservoir and the\nassociated current and shed light on their plausible connections with\nentanglement entropy. Our findings are expected to hold for a wide variety of\ngeneric interacting quantum systems.",
        "This study investigates the thermodynamic properties of face-centered cubic\nlead (fcc-Pb) using ab-initio methods within the quasi-harmonic approximation\n(QHA), examining the influence of spin-orbit coupling (SOC) and the\nexchange-correlation functionals. Two types of ultrasoft pseudopotential\n(US-PP) are considered: one that excludes (scalar relativistic PP) and one that\nincludes the SOC effects (fully relativistic PP). Further, for each PP, we test\nthe performance of three popular exchange-correlation functionals:\nPerdew-Burke-Ernzerhof generalized gradient approximation (PBE) (Perdew et al.\nPhys. Rev. Lett. 77, 3865 (1996)), PBE modified for dense solids (PBEsol)\n(Perdew et al. Phys. Rev. Lett. 100, 136406 (2008)), and local density\napproximation (LDA) (Perdew et al. Phys. Rev. B 23, 5048 (1981)). We calculate\nthe Helmholtz free energy, incorporating lattice vibrations (phonons) and\nelectronic excitation contributions. The estimated equation of state (at 4 K\nand 301 K), phonon dispersions (at 100 K and 300 K), mode-Gr\\\"uneisen\nparameters ({\\gamma}q{\\eta}) (at 100 K), volume thermal expansion coefficient\n(\\b{eta}), isobaric heat capacity (CP), bulk modulus (BS), and thermodynamic\naverage Gr\\\"uneisen parameter ({\\gamma}) are compared with the available\nexperimental and theoretical studies. Moreover, the 0 K pressure-dependent\nelastic constant-coefficient (Cij) of fcc lead and Pugh ratio, Debye\ntemperature, and longitudinal and transverse sound velocities for\npolycrystalline lead are presented. The contributions of electronic excitations\nin all the thermodynamic properties are found to be negligible. With increasing\npressure, the role of spin-orbit effects decreases but does not vanish. Our\nfindings demonstrate that SOC leads to results distinct from the SR approach,\nbut agreement with the experiment is not consistently improved by including\nSOC.",
        "We revisit the problem of fair representation learning by proposing Fair\nPartial Least Squares (PLS) components. PLS is widely used in statistics to\nefficiently reduce the dimension of the data by providing representation\ntailored for the prediction. We propose a novel method to incorporate fairness\nconstraints in the construction of PLS components. This new algorithm provides\na feasible way to construct such features both in the linear and the non linear\ncase using kernel embeddings. The efficiency of our method is evaluated on\ndifferent datasets, and we prove its superiority with respect to standard fair\nPCA method.",
        "The application of suitable numerical boundary conditions for hyperbolic\nconservation laws on domains with complex geometry has become a problem with\ncertain difficulty that has been tackled in different ways according to the\nnature of the numerical methods and mesh type. In this paper we present a\ntechnique for the extrapolation of information from the interior of the\ncomputational domain to ghost cells designed for structured Cartesian meshes\n(which, as opposed to non-structured meshes, cannot be adapted to the\nmorphology of the domain boundary). This technique is based on the application\nof Lagrange interpolation with a filter for the detection of discontinuities\nthat permits a data dependent extrapolation, with higher order at smooth\nregions and essentially non oscillatory properties near discontinuities.",
        "We obtain solutions for Eliashberg equations within the Nambu representation\nfor a two-band model of iron-based superconductors with nonmagnetic impurities.\nTwo cases of a transition between $s_{\\pm}$ and $s_{++}$ states are considered:\n(i) the transition is accompanied by the abrupt change of the order parameter\nsign within one of the bands and (ii) the change is smooth. For both cases, we\nstudied the role of a gauge defined by the coefficients preceding the Pauli\nmatrices $\\hat\\tau_1$ and $\\hat\\tau_2$ in a self-energy expansion, which\ncorrespond to the components of the order parameter. We show that the absolute\nvalue of the order parameter is conserved for solutions in the clean and in the\nBorn limits. In an intermediate case, between the Born and unitary limits,\nresult depends on the solution for the clean limit. We show that a common gauge\nfor the Eliashberg equations in which one of the order parameter components\nvanishes is essential for adequate description of the multiband superconducting\nsystems.",
        "This note considers the blind free deconvolution problems of sparse spectral\nmeasures from one-parameter families. These problems pose significant\nchallenges since they involve nonlinear sparse recovery. The main technical\ntool is the eigenmatrix method for solving unstructured sparse recovery\nproblems. The key idea is to turn the nonlinear inverse problem into a linear\ninverse problem by leveraging the R-transform for free addition and the\nS-transform for free product. The resulting linear problem is solved with the\neigenmatrix method tailored to the domain of the parametric family. Numerical\nresults are provided for both the additive and multiplicative free\ndeconvolutions.",
        "The technology of lined rock cavern (LRC) with great geographical flexibility\nis a promising, cost-effective solution to underground hydrogen storage.\nHowever, the air-tight steel tanks used in this technology are susceptible to\nmaterial degradation due to hydrogen embrittlement (HE), potentially leading to\nleakage and structural failure, especial for LRCs constructed in complex\ngeological conditions. In this paper, we develop a 2D multiscale numerical\nmodel based on the finite element method to assess the impact of HE on the LRC\nperformance in fractured rock masses under cyclic gas pressurisation. Within\nthis framework, a large-scale model is used to simulate the deformation and\ndamage evolution of both fractured rock and an LRC under in-situ stresses and\ninternal gas pressurisation, while a small-scale model captures HE in the steel\nlining of the LRC. Our simulations reveal that damage in the rock, concrete,\nand steel degradation is strongly affected by pre-existing fractures and\nin-situ stresses. Our results also reveal the presence of a strong positive\nfeedback between hydrogen concentration and stress redistribution in the steel\nlining. Moreover, a comparison between models with and without considering HE\nilluminates that hydrogen concentration significantly contributes to steel\ndegradation, particularly during the long-term LRC operation, highlighting the\ncritical role of HE in the safety and performance of the LRC. The findings and\ninsights obtained from our work have important implications for the design\noptimisation and performance assessment of LRCs for sustainable underground\nhydrogen storage.",
        "Motivated by the previous results by Coletti--de Lima--Gava--Luiz (2020) and\nShiozawa (2022), we study the fluctuation of the mixed elephant random walk in\nthe superdiffusive case, and prove the Central Limit Theorem and the Law of\nIterated Logarithm with subtracting a random drift.",
        "Van der Waals (vW) Josephson junctions (JJs) realized by stacking materials\nsuch as few-layered NbSe2, offers a new landscape to realize superconducting\nquantum devices with superior properties owing to its crystalline nature and\ndefect-free junctions. For quantum technology, overdamped JJs are highly\nsought-after, whose realization demands precise control of junction capacitance\nby engineering the junction area using microfabrication techniques. NbSe2 is\nhighly reactive and susceptible to damage during microfabrication processes. In\nthis manuscript, we demonstrate both underdamped and overdamped NbSe2-NbSe2 JJs\nby controlling the junction area. We devise a minimally invasive\nmicrofabrication procedure, post-junction formation, to precisely control the\njunction area. The McCumber parameter characterizing the damping is extracted\nfrom the electrical transport measurements down to 130 mK. The results show\nthat our sample fabrication recipe has preserved the material qualities and\npaved the way for the realization of scalable JJ devices on NbSe2 and similar\nsystems.",
        "Perovskite BaSnO3 has an Sn s-orbital conduction band minimum, which makes it\nof interest as a transparent-conducting oxide parent compound but also\ncontraindicates the ferroelectric instability characteristic of the related\ncompound BaTiO3. In this work, we studied the effect of (001) compressive\nstrain on BaSnO3 using first-principles methods. We found that, with low\ncompressive strain, symmetry breaking takes cubic BaSnO3 to a nonpolar\ntetragonal state, with a first-order phase transition to a hidden\nhighly-polarized ferroelectric supertetragonal state at about -5%. Based on the\nfacts that the mismatch of lattice constant in experiment between BaSnO3 and\nSrTiO3 is about -5.2% and coherent growth of BaSnO3 on SrTiO3 has been\nexperimentally realized for BaSnO3 layers thinner than 3 unit-cells, we studied\na series of SrTiO3\/BaSnO3 superlattices with one or two unit-cells of BaSnO3\nand several unit-cells of SrTiO3. We found that the superlattices are\nferroelectric with large polarizations. We propose that the origin of\nferroelectricity in the superlattices is the mechanical and electrical coupling\nof the BaSnO3 and SrTiO3 layers, with polarized supertetragonal state of BaSnO3\ninduced by compressive-strain from the SrTiO3 layers and polarization of the\nSrTiO3 layers by the polar BaSnO3 layers. Due to the distinctive electronic\nstates in the BaSnO3 layers, the realization of ferroelectricity holds promise\nfor the design of novel electronic devices.",
        "The advancement of classical machine learning is inherently linked to the\nestablishment and progression of classical dataset. In quantum machine learning\n(QML), there is an analogous imperative for the development of quantum\nentangled datasets comprised with huge quantity and high quality. Especially\nfor multipartite mixed-state datasets, due to the lack of suitable entanglement\ncriteria, previous researchers often could only perform classification tasks on\ndatasets extended based on Werner states or other well-structured states. This\npaper is dedicated to provide a method for generating mixed-state datasets for\nentangled-separable classification tasks. This method is based on supervised\nquantum machine learning and the concentratable entanglement measures. It\nfurthers the assembly of quantum entangled datasets, inspires the discovery of\nnew entanglement criteria with both classical and quantum machine learning, and\nprovides a valuable resource for benchmarking QML models, thereby opening new\navenues for exploring the rich structure of quantum entanglement in mixed\nstates. Additionally, we benchmark several machine learning models using this\ndataset, offering guidance and suggestions for the selection of QML models.",
        "Higher$\\text{-}$order Markov chain models are widely used to represent agent\ntransitions in dynamic systems, such as passengers in transport networks. They\ncapture transitions in complex systems by considering not only the current\nstate but also the path of previously visited states. For example, the\nlikelihood of train passengers traveling from Paris (current state) to Rome\ncould increase significantly if their journey originated in Italy (prior\nstate). Although this approach provides a more faithful representation of the\nsystem than first$\\text{-}$order models, we find that commonly used\nmethods$-$relying on Kullback$\\text{-}$Leibler divergence$-$frequently overfit\nthe data, mistaking fluctuations for higher$\\text{-}$order dependencies and\nundermining forecasts and resource allocation. Here, we introduce DIVOP\n(Detection of Informative Variable$\\text{-}$Order Paths), an algorithm that\nemploys cross$\\text{-}$validation to robustly distinguish meaningful\nhigher$\\text{-}$order dependencies from noise. In both synthetic and\nreal$\\text{-}$world datasets, DIVOP outperforms two\nstate$\\text{-}$of$\\text{-}$the$\\text{-}$art algorithms by achieving higher\nprecision, recall, and sparser representations of the underlying dynamics. When\napplied to global corporate ownership data, DIVOP reveals that tax havens\nappear in 82$\\%$ of all significant higher$\\text{-}$order dependencies,\nunderscoring their outsized influence in corporate networks. By mitigating\noverfitting, DIVOP enables more reliable multi$\\text{-}$step predictions and\ndecision$\\text{-}$making, paving the way toward deeper insights into the hidden\nstructures that drive modern interconnected systems.",
        "Microlensing by stars in the lens galaxy of a gravitationally lensed quasar\nis a phenomenon that can selectively magnify quasar subregions, producing\nobservable changes in the continuum brightness or distortions in the emission\nline profiles. Hence, microlensing allows us to probe the inner quasar regions.\nIn this paper, we report measurements of the ratio of the broad emission line\nregion (BLR) radius to the continuum source radius in eight lensed quasars, for\nthe CIV, MgII, and H$\\alpha$ emission lines and their respective underlying\ncontinua at $\\lambda\\lambda$ 1550\\AA , 2800\\AA , and 6563 \\AA . The\nmicrolensing-induced line profile distortions and continuum magnifications were\nobserved in the same single-epoch datasets, and simultaneously compared with\nmicrolensing simulations. We found that, on average, the inner radius of the\nBLR starts at the end of the UV-optical continuum source, independently of the\nline ionization and the wavelength of the continuum. The half-light radius of\nthe BLR is, on average, a factor of six larger than the half-light radius of\nthe continuum source, independently of the quasar's bolometric luminosity. We\nalso found a correlation between the BLR radius and the continuum source\nradius, supporting the idea that the dominant contribution to the UV-optical\ncontinuum may come from the BLR itself. Our results independently confirm the\nresults of reverberation mapping studies, and extend them to higher-redshift,\nhigher-luminosity quasars.",
        "Gordon and Litherland's paper $\\textit{On the Signature of a link}$\nintroduced a bilinear form that simultaneously unifies both the quadratic forms\nof Trotter and Goeritz. This remarkable pairing of combinatorics and topology\nhas had widespread application in low-dimensional topology. In this expository\nnote, we give a picture proof (via Kirby diagrams) of their main result and\ndiscuss the numerous ways their theorem has been put to good use.",
        "Capsule networks(CapsNet) are recently proposed neural network models with\nnew processing layers, specifically for entity representation and discovery of\nimages. It is well known that CapsNet have some advantages over traditional\nneural networks, especially in generalization capability. At the same time,\nsome studies report negative experimental results. The causes of this\ncontradiction have not been thoroughly analyzed. The preliminary experimental\nresults show that the behavior of routing algorithms does not always produce\ngood results as expected, and in most cases, different routing algorithms do\nnot change the classification results, but simply polarize the link strength,\nespecially when they continue to repeat without stopping. To realize the true\npotential of the CapsNet, deep mathematical analysis of the routing algorithms\nis crucial. In this paper, we will give the objective function that is\nminimized by the dynamic routing algorithm, which is a concave function. The\ndynamic routing algorithm can be regarded as nonlinear gradient method to\nsolving an optimization algorithm under linear constraints, and its convergence\ncan be strictly proved mathematically. Furthermore, the mathematically rigorous\nproof of the convergence is given for this class of iterative routing\nprocedures. We analyze the relation between the objective function and the\nconstraints solved by the dynamic routing algorithm in detail, and perform the\ncorresponding routing experiment to analyze the effect of our convergence\nproof.",
        "We propose an efficient method for dissipative generation of magnonic cat\nstates in a cavity-magnon-qubit hybrid system by exploiting a two-magnon\ndriving and dissipation mechanism. When both the magnon and qubit are driven, a\ncoherent nonlinear two-magnon interaction is induced, wherein the qubit and the\nmagnon mode exchange energy through magnon pairs. The dissipation of the qubit\nis exploited to steer the magnon mode into a quantum superposition of distinct\ncoherent states, where the magnon mode evolves into either an even or odd cat\nstate, depending on the parity of the magnon initial state. For the case where\nthe magnon initial state is a superposition state, e.g., of $|0\\rangle$ and\n$|1\\rangle$, the magnon mode can evolve into a weighted mixture of the even and\nodd cat states. We also find that magnon squeezed states may emerge during the\nshort-time evolution, showcasing the capability of our mechanism in preparing\ndiverse magnon non-classical states. Magnonic cat and squeezed states are\nmacroscopic quantum states and find applications in macroscopic quantum studies\nand quantum sensing, e.g., in the dark matter search using ferromagnetic axion\nhaloscopes."
      ]
    }
  },
  {
    "id":2411.17595,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Lint: Llm interaction network for clinical trial outcome prediction",
    "start_abstract":"Clinical trial outcome prediction aims to predict the success probability of a clinical trial that reaches its desirable endpoint. Most of the effort focuses on developing machine learning models for making accurate predictions with diverse data sources, including clinical trial descriptions, drug molecules, and target disease conditions. Accurate trial outcome prediction helps trial planning and asset portfolio prioritization. Previous works have focused on small-molecule drugs; however, biologics are a quickly growing intervention type that lacks information that is traditionally known for drugs, like molecular properties. Additionally, traditional methods like graph neural networks are much more difficult to apply to biologics data which are a fast-growing type of drug. To address these points, we propose a Language Interaction Network (LINT), a novel method for trial outcome prediction using only free-text descriptions. We validate the effectiveness of LINT with thorough experiments across three trial phases. Specifically, LINT obtains 0.770, 0.740, and 0.748 ROC-AUC scores on phase I, II, and III, respectively, for clinical trials with biologic interventions.",
    "start_categories":[
      "Clinical trials"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Machine learning model to predict oncologic outcomes for drugs in randomized clinical trials"
      ],
      "abstract":[
        "Abstract Predicting oncologic outcome is challenging due to the diversity of cancer histologies and complex network underlying biological factors. In this study, we determine whether machine learning (ML) can extract meaningful associations between clinical trial, drug\u2010related biomarker molecular profile information. We analyzed therapeutic trials corresponding 1102 outcomes from 104 758 patients with advanced colorectal adenocarcinoma, pancreatic melanoma nonsmall\u2010cell lung cancer. For each intervention arm, a dataset following attributes was curated: line treatment, number cytotoxic chemotherapies, small\u2010molecule inhibitors, or monoclonal antibody agents, drug class, alteration status arm's population, type, probability sensitivity (PDS) (integrating genomic, transcriptomic proteomic biomarkers in population interest) outcome. A total 467 progression\u2010free survival (PFS) 369 overall (OS) data points were used as training sets build our ML (random forest) model. Cross\u2010validation for PFS OS, obtaining correlation coefficients ( r ) 0.82 0.70, respectively (outcome vs model's parameters). 156 110 OS test sets. The Spearman s predicted actual statistically significant (PFS: = 0.879, OS: 0.878, P &lt; .0001). better arm 81% N 59\/73, z 5.24, .0001) 71% (OS: 37\/52, 2.91, .004) randomized trials. success algorithm predict may be exploitable model optimize trial design pharmaceutical agents."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Uniform local well-posedness and liviscid limit for the KdV-Burgers\n  equation on $\\mathbb{T}$",
        "What is a Social Media Bot? A Global Comparison of Bot and Human\n  Characteristics",
        "SIGMA: Sheaf-Informed Geometric Multi-Agent Pathfinding",
        "FedMABench: Benchmarking Mobile Agents on Decentralized Heterogeneous\n  User Data",
        "Low-energy insulating reconstructions of Si(111)-7x7 surface with and\n  without stacking fault discovered by graph theory",
        "Arbitrary control of the flow of light using pseudomagnetic fields in\n  photonic crystals at telecommunication wavelengths",
        "Advancing Medical Representation Learning Through High-Quality Data",
        "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument\n  Generation with Large Language Models",
        "Output-Feedback Full-State Targeting Model Predictive Control for\n  Station-Keeping on Near-Rectilinear Halo Orbits",
        "The multiple Markov theorem on Angelesco sets",
        "Impact of Nonreciprocal Hopping on Localization in Non-Hermitian\n  Quasiperiodic Systems",
        "Efficient Gradient-Based Inference for Manipulation Planning in Contact\n  Factor Graphs",
        "Enhanced collective vibrations in granular materials",
        "Efficient Traffic Prediction Through Spatio-Temporal Distillation",
        "Forgetting Any Data at Any Time: A Theoretically Certified Unlearning\n  Framework for Vertical Federated Learning",
        "Minimax Rate-Optimal Inference for Individualized Quantile Treatment\n  Effects in High-dimensional Models",
        "Anomalous exchange correlation of quasiparticles with entangled Nambu\n  spinors",
        "Photometric Determination of Unresolved Main-sequence Binaries in the\n  Pleiades: Binary Fraction and Mass Ratio Distribution",
        "Stone Soup Multi-Target Tracking Feature Extraction For Autonomous\n  Search And Track In Deep Reinforcement Learning Environment",
        "Load-Balancing versus Anycast: A First Look at Operational Challenges",
        "Decentralized Personalization for Federated Medical Image Segmentation\n  via Gossip Contrastive Mutual Learning",
        "The Query\/Hit Model for Sequential Hypothesis Testing",
        "Cryogenic Nano-Imaging of Excitons in a Monolayer Semiconductor",
        "Searching for continuous gravitational waves from slowly spinning\n  neutron stars with DECIGO, Big Bang Observer, Einstein Telescope and Cosmic\n  Explorer",
        "Construction of a Small-Scale Vacuum Generation System and Using It as\n  an Educational Device to Demonstrate Features of the Vacuum",
        "Understanding and Mitigating Membership Inference Risks of Neural\n  Ordinary Differential Equations",
        "Singularities of two-dimensional Nijenhuis operators",
        "Overcoming user-rate limit of quantum network",
        "ELIZA Reanimated: The world's first chatbot restored on the world's\n  first time sharing system"
      ],
      "abstract":[
        "This article investigates the uniform well-posedness and inviscid limit\nproblem for the Korteweg-de Vries-Burgers equation on a torus, we consider the\nKdV-Burgers equation $$\\partial_t\nu(t,x)+\\partial_x^3u(t,x)-\\varepsilon\\partial_x^2u(t,x)=\\dfrac{1}{2}\\partial_x(u(t,x))^2,\n\\quad u(0)=\\phi, $$ where $\\varepsilon\\in(0, 1]$ represents the diffusion\ncoefficient, and $u(t,x):\\mathbb{R}^{+}\\times\\mathbb{T}\\rightarrow \\mathbb{R}$\nis a real-valued function, we show that it is uniformly local well-posed in\n$H^s$ with $s\\geq 0$ for all $\\varepsilon\\in[0,1]$. Moreover, we prove that\nthere exists some $T>0$ such that for any $s\\geq 0$, its solution converges in\n$C([0,T];H^s)$ to that of the KdV equation if $\\varepsilon$ tends to $0$.",
        "Chatter on social media is 20% bots and 80% humans. Chatter by bots and\nhumans is consistently different: bots tend to use linguistic cues that can be\neasily automated while humans use cues that require dialogue understanding.\nBots use words that match the identities they choose to present, while humans\nmay send messages that are not related to the identities they present. Bots and\nhumans differ in their communication structure: sampled bots have a star\ninteraction structure, while sampled humans have a hierarchical structure.\nThese conclusions are based on a large-scale analysis of social media tweets\nacross ~200mil users across 7 events. Social media bots took the world by storm\nwhen social-cybersecurity researchers realized that social media users not only\nconsisted of humans but also of artificial agents called bots. These bots wreck\nhavoc online by spreading disinformation and manipulating narratives. Most\nresearch on bots are based on special-purposed definitions, mostly predicated\non the event studied. This article first begins by asking, \"What is a bot?\",\nand we study the underlying principles of how bots are different from humans.\nWe develop a first-principle definition of a social media bot. With this\ndefinition as a premise, we systematically compare characteristics between bots\nand humans across global events, and reflect on how the software-programmed bot\nis an Artificial Intelligent algorithm, and its potential for evolution as\ntechnology advances. Based on our results, we provide recommendations for the\nuse and regulation of bots. Finally, we discuss open challenges and future\ndirections: Detect, to systematically identify these automated and potentially\nevolving bots; Differentiate, to evaluate the goodness of the bot in terms of\ntheir content postings and relationship interactions; Disrupt, to moderate the\nimpact of malicious bots.",
        "The Multi-Agent Path Finding (MAPF) problem aims to determine the shortest\nand collision-free paths for multiple agents in a known, potentially\nobstacle-ridden environment. It is the core challenge for robotic deployments\nin large-scale logistics and transportation. Decentralized learning-based\napproaches have shown great potential for addressing the MAPF problems,\noffering more reactive and scalable solutions. However, existing learning-based\nMAPF methods usually rely on agents making decisions based on a limited field\nof view (FOV), resulting in short-sighted policies and inefficient cooperation\nin complex scenarios. There, a critical challenge is to achieve consensus on\npotential movements between agents based on limited observations and\ncommunications. To tackle this challenge, we introduce a new framework that\napplies sheaf theory to decentralized deep reinforcement learning, enabling\nagents to learn geometric cross-dependencies between each other through local\nconsensus and utilize them for tightly cooperative decision-making. In\nparticular, sheaf theory provides a mathematical proof of conditions for\nachieving global consensus through local observation. Inspired by this, we\nincorporate a neural network to approximately model the consensus in latent\nspace based on sheaf theory and train it through self-supervised learning.\nDuring the task, in addition to normal features for MAPF as in previous works,\neach agent distributedly reasons about a learned consensus feature, leading to\nefficient cooperation on pathfinding and collision avoidance. As a result, our\nproposed method demonstrates significant improvements over state-of-the-art\nlearning-based MAPF planners, especially in relatively large and complex\nscenarios, demonstrating its superiority over baselines in various simulations\nand real-world robot experiments.",
        "Mobile agents have attracted tremendous research participation recently.\nTraditional approaches to mobile agent training rely on centralized data\ncollection, leading to high cost and limited scalability. Distributed training\nutilizing federated learning offers an alternative by harnessing real-world\nuser data, providing scalability and reducing costs. However, pivotal\nchallenges, including the absence of standardized benchmarks, hinder progress\nin this field.\n  To tackle the challenges, we introduce FedMABench, the first benchmark for\nfederated training and evaluation of mobile agents, specifically designed for\nheterogeneous scenarios. FedMABench features 6 datasets with 30+ subsets, 8\nfederated algorithms, 10+ base models, and over 800 apps across 5 categories,\nproviding a comprehensive framework for evaluating mobile agents across diverse\nenvironments. Through extensive experiments, we uncover several key insights:\nfederated algorithms consistently outperform local training; the distribution\nof specific apps plays a crucial role in heterogeneity; and, even apps from\ndistinct categories can exhibit correlations during training. FedMABench is\npublicly available at: https:\/\/github.com\/wwh0411\/FedMABench with the datasets\nat: https:\/\/huggingface.co\/datasets\/wwh0411\/FedMABench.",
        "The 7x7 reconstruction of Si(111) surface is one of the most fascinating\nconfiguration in nature, whose STM image has been well-understood by the famous\ndimer-adatom-stacking-fault model (DAS). However, the electronic property of\nthe DAS model is always confirmed to be metallic by first-principles\ncalculations, while some experiments detected insulating features. It is still\nchallenge to predict DAS-like reconstructions through traditional method to\nsolve such a puzzle. Here, we show that 7x7 reconstructions can be quickly\ndiscovered by graph theory as implemented in the graph-space based RG2 code for\ncrystal structure prediction. Two groups of reconstructions with (DAS-d8-T12,\nDAS-d8-T9H3-A, DAS-d8-T9H3-B and DAS-d8-T6H6) and without (AB-d10-T12,\nAB-d10-T9H3, AA-d10-T12 and AA-d10-T9H3) stacking-fault are discovered. They\npossess energetic stabilities comparable to the well-known DAS (DAS-d8-T12) and\nshow similar STM patterns, providing a plausible explanation for the\nexperimentally observed 7x7 reconstruction on the Si(111) surface. The\nfirst-principles calculations show that DAS-d8-T12, DAS-d8-T6H6, AB-d10-T12,\nand AA-d10-T12 are metallic, while DAS-d8-T9H3-A, DAS-d8-T9H3-B, AB-d10-T9H3\nand AA-d10-T9H3 are insulating phases with gaps of 0.043 eV, 0.182 eV, 0.043 eV\nand 0.059 eV, respectively. Our work demonstrates the predictability of the\nSi(111)-7x7 reconstruction and provides the structural candidates for\nunderstanding the experimentally observed metal-to-insulator transition.",
        "In photonics, the idea of controlling light in a similar way that magnetic\nfields control electrons has always been attractive. It can be realized by\nsynthesizing pseudomagnetic fields (PMFs) in photonic crystals (PhCs). Previous\nworks mainly focus on the Landau levels and the robust transport of the chiral\nstates. More versatile control over light using complex nonuniform PMFs such as\nthe flexible splitting and routing of light has been elusive, which hinders\ntheir application in practical photonic integrated circuits. Here we propose an\nuniversal and systematic methodology to design nonuniform PMFs and arbitrarily\ncontrol the flow of light in silicon PhCs at telecommunication wavelengths. As\nproofs of concept, a low-loss S-bend and a highly efficient 50:50 power\nsplitter based on PMFs are experimentally demonstrated. A high-speed data\ntransmission experiment is performed on these devices to prove their\napplicability in real communication systems. The proposed method offers a new\nparadigm for the exploration of fundamental physics and the development of\nnovel nanophotonic devices.",
        "Despite the growing scale of medical Vision-Language datasets, the impact of\ndataset quality on model performance remains under-explored. We introduce\nOpen-PMC, a high-quality medical dataset from PubMed Central, containing 2.2\nmillion image-text pairs, enriched with image modality annotations, subfigures,\nand summarized in-text references. Notably, the in-text references provide\nricher medical context, extending beyond the abstract information typically\nfound in captions. Through extensive experiments, we benchmark Open-PMC against\nlarger datasets across retrieval and zero-shot classification tasks. Our\nresults show that dataset quality-not just size-drives significant performance\ngains. We complement our benchmark with an in-depth analysis of feature\nrepresentation. Our findings highlight the crucial role of data curation\nquality in advancing multimodal medical AI. We release Open-PMC, along with the\ntrained models and our codebase.",
        "This paper investigates the role of dynamic external knowledge integration in\nimproving counter-argument generation using Large Language Models (LLMs). While\nLLMs have shown promise in argumentative tasks, their tendency to generate\nlengthy, potentially unfactual responses highlights the need for more\ncontrolled and evidence-based approaches. We introduce a new manually curated\ndataset of argument and counter-argument pairs specifically designed to balance\nargumentative complexity with evaluative feasibility. We also propose a new\nLLM-as-a-Judge evaluation methodology that shows a stronger correlation with\nhuman judgments compared to traditional reference-based metrics. Our\nexperimental results demonstrate that integrating dynamic external knowledge\nfrom the web significantly improves the quality of generated counter-arguments,\nparticularly in terms of relatedness, persuasiveness, and factuality. The\nfindings suggest that combining LLMs with real-time external knowledge\nretrieval offers a promising direction for developing more effective and\nreliable counter-argumentation systems.",
        "We develop a model predictive control (MPC) policy for station-keeping (SK)\non a Near-Rectilinear Halo Orbit (NRHO). The proposed policy achieves\nfull-state tracking of a reference NRHO via a two-maneuver control horizon\nplaced one revolution apart. Our method abides by the typical mission\nrequirement that at most one maneuver is used for SK during each NRHO\nrevolution. Simultaneously, the policy has sufficient controllability for\nfull-state tracking, making it immune to phase deviation issues in the\nalong-track direction of the reference NRHO, a common drawback of existing SK\nmethods with a single maneuver per revolution. We report numerical simulations\nwith a navigation filter to demonstrate the MPC's performance with output\nfeedback. Our approach successfully maintains the spacecraft's motion in the\nvicinity of the reference in both space and phase, with tighter tracking than\nstate-of-the-art SK methods and comparable delta-V performance.",
        "By addressing a long-standing open problem, listed in a highly regarded\ncollection of open questions in the field and described as a \"worthwhile\nresearch project\", this note extends Markov's theorem (Markoff, Math. Ann.,\n27:177-182, 1886) on the variation of zeros of orthogonal polynomials on the\nreal line to the setting of multiple orthogonal polynomials on Angelesco sets.\nThe analysis reveals that the only distinction from the classical 1886 result\nlies in establishing sufficient conditions for a given\n$\\mathcal{Z}$-matrix--which, in the Markov case, is the identity matrix--to be\nan $\\mathcal{M}$-matrix. In contrast to most existing studies, which often\npresent highly technical proofs for specific results, this note seeks to\nprovide a simple proof of a general result without imposing restrictions on the\nweight functions (such as their potential \"classical\" nature), the number of\nintervals, or the structure of the partition.",
        "We study the non-Hermitian Aubry-Andr\\'e-Harper model, incorporating complex\nphase modulation, unmodulated and modulated nonreciprocal hopping. Using\nAvila's global theory, we derive analytical phase boundaries and map out the\nphase diagrams, revealing extended, localized, critical, and skin phases unique\nto non-Hermitian systems. For complex phase modulation, we determine\nlocalization lengths through Lyapunov exponents and show that topological\ntransitions align with localization transitions. In the nonreciprocal case, we\nuse similarity transformations to confirm phase boundaries consistent with\nAvila's theory and uncover asymmetric localization behaviors. Importantly,\nmodulated nonreciprocal hopping transforms both extended and critical phases\ninto skin phases under open boundary conditions. These results highlight the\ninterplay between topology, localization, and non-Hermitian effects, offering\nnew perspectives on quasiperiodic systems.",
        "This paper presents a framework designed to tackle a range of planning\nproblems arise in manipulation, which typically involve complex\ngeometric-physical reasoning related to contact and dynamic constraints. We\nintroduce the Contact Factor Graph (CFG) to graphically model these diverse\nfactors, enabling us to perform inference on the graphs to approximate the\ndistribution and sample appropriate solutions. We propose a novel approach that\ncan incorporate various phenomena of contact manipulation as differentiable\nfactors, and develop an efficient inference algorithm for CFG that leverages\nthis differentiability along with the conditional probabilities arising from\nthe structured nature of contact. Our results demonstrate the capability of our\nframework in generating viable samples and approximating posterior\ndistributions for various manipulation scenarios.",
        "Granular materials are defined as collections of macroscopic dissipative\nparticles. Although these systems are ubiquitous in our lives, the nature and\nthe causes of their non-trivial collective dynamics still remain elusive and\nhave attracted significant interest in non-equilibrium physics. Here, we focus\non the vibrational dynamics of granular materials. While the vibrational\ndynamics of random packings have been examined concerning the jamming\ntransition, previous research has overlooked the role of contact dissipations.\nWe conducted numerical and analytical investigations into the vibrational\ndynamics of random packings influenced by the normal dissipative force, which\nis the simplest model for contact dissipations. Our findings reveal that the\nkinetic energy per mode diverges in the low-frequency range, following the\nscaling law $\\mathcal{K}_l \\propto \\omega^{-2}_l$ with the frequency\n$\\omega_l$, indicating that low-frequency modes experience strong excitation\nand that the equipartition of energy is violated. Additionally, the spatial\nstructure factor of the velocity field displays the scaling law $S_v(q) \\propto\nq^{-2}$ with the wavenumber $q$, which signifies that the velocity field has an\ninfinitely long range. We demonstrate that these phenomena arise from the\neffects of weaker damping on softer modes, where the particle displacements\nparallel to the contacts are minimal in the low-frequency modes, rendering\nnormal dissipation ineffective at dampening these modes.",
        "Graph neural networks (GNNs) have gained considerable attention in recent\nyears for traffic flow prediction due to their ability to learn spatio-temporal\npattern representations through a graph-based message-passing framework.\nAlthough GNNs have shown great promise in handling traffic datasets, their\ndeployment in real-life applications has been hindered by scalability\nconstraints arising from high-order message passing. Additionally, the\nover-smoothing problem of GNNs may lead to indistinguishable region\nrepresentations as the number of layers increases, resulting in performance\ndegradation. To address these challenges, we propose a new knowledge\ndistillation paradigm termed LightST that transfers spatial and temporal\nknowledge from a high-capacity teacher to a lightweight student. Specifically,\nwe introduce a spatio-temporal knowledge distillation framework that helps\nstudent MLPs capture graph-structured global spatio-temporal patterns while\nalleviating the over-smoothing effect with adaptive knowledge distillation.\nExtensive experiments verify that LightST significantly speeds up traffic flow\npredictions by 5X to 40X compared to state-of-the-art spatio-temporal GNNs, all\nwhile maintaining superior accuracy.",
        "Privacy concerns in machine learning are heightened by regulations such as\nthe GDPR, which enforces the \"right to be forgotten\" (RTBF), driving the\nemergence of machine unlearning as a critical research field. Vertical\nFederated Learning (VFL) enables collaborative model training by aggregating a\nsample's features across distributed parties while preserving data privacy at\neach source. This paradigm has seen widespread adoption in healthcare, finance,\nand other privacy-sensitive domains. However, existing VFL systems lack robust\nmechanisms to comply with RTBF requirements, as unlearning methodologies for\nVFL remain underexplored. In this work, we introduce the first VFL framework\nwith theoretically guaranteed unlearning capabilities, enabling the removal of\nany data at any time. Unlike prior approaches -- which impose restrictive\nassumptions on model architectures or data types for removal -- our solution is\nmodel- and data-agnostic, offering universal compatibility. Moreover, our\nframework supports asynchronous unlearning, eliminating the need for all\nparties to be simultaneously online during the forgetting process. These\nadvancements address critical gaps in current VFL systems, ensuring compliance\nwith RTBF while maintaining operational flexibility.We make all our\nimplementations publicly available at\nhttps:\/\/github.com\/wangln19\/vertical-federated-unlearning.",
        "The quantification of treatment effects plays an important role in a wide\nrange of applications, including policy making and bio-pharmaceutical research.\nIn this article, we study the quantile treatment effect (QTE) while addressing\ntwo specific types of heterogeneities: (a) personalized heterogeneity, which\ncaptures the varying treatment effects for different individuals, and (b)\nquantile heterogeneity, which accounts for how the impact of covariates varies\nacross different quantile levels. A well-designed debiased estimator for the\nindividualized quantile treatment effect (IQTE) is proposed to capture such\nheterogeneities effectively. We show that this estimator converges weakly to a\nGaussian process as a function of the quantile levels and propose valid\nstatistical inference methods, including the construction of confidence\nintervals and the development of hypothesis testing decision rules. In\naddition, the minimax optimality frameworks for these inference procedures are\nestablished. Specifically, we derive the minimax optimal rates for the expected\nlength of confidence intervals and the magnitude of the detection boundary for\nhypothesis testing procedures, illustrating the superiority of the proposed\nestimator. The effectiveness of our methods is demonstrated through extensive\nsimulations and an analysis of the National Health and Nutrition Examination\nSurvey (NHANES) datasets.",
        "Entanglement of spin degree of freedom can drastically alter the orbital\nexchange symmetry of electrons, switching their bunching and antibunching\nbehaviors and the resultant current correlations in the Hanbury-Brown-Twiss\ninterferometry. Here, we investigate the exchange correlation of quasiparticles\nwith entanglement encoded in the Nambu spinors, or the electron-hole degree of\nfreedom. In contrast to the conventional correspondence between spin\nentanglement and current correlation, we find that singlet (triplet)\nentanglement of Nambu spinors results in suppressed (enhanced) current\ncorrelation. This effect arises because the charge degree of freedom itself\nencodes the entanglement. We propose implementing this phenomenon in the edge\nstates of a quantum Hall system, where the entangled states of the Nambu\nspinors can be continuously tuned by gate voltages. Our study reveals a novel\nrelationship between entanglement and charge correlations, offering an\neffective approach for detecting entanglement of Nambu spinors.",
        "Accurate determination of binary fractions ($f_{\\rm b}$) and mass ratio ($q$)\ndistributions is crucial for understanding the dynamical evolution of open\nclusters. We present an improved multiband fitting technique to enhance the\nanalysis of binary properties. This approach enables an accurate photometric\ndetermination of $f_{\\rm b}$ and $q$ distribution in a cluster. The detectable\nmass ratio can be down to the $q_{\\rm lim}$, limited by the minimum stellar\nmass in theoretical models. First, we derived an empirical model for magnitudes\nof Gaia DR3 and 2MASS bands that match the photometry of single stars in the\nPleiades. We then performed a multiband fitting for each cluster member,\nderiving the probability density function (PDF) of its primary mass\n($\\mathcal{M}_1$) and $q$ in the Bayesian framework. 1154 main-sequence (MS)\nsingle stars or unresolved MS+MS binaries are identified as members of the\nPleiades. By stacking their PDFs, we conducted a detailed analysis of binary\nproperties of the cluster. We found the $f_{\\rm b}$ of this sample is $0.34 \\pm\n0.02$. The $q$ distribution exhibits a three-segment power-law profile: an\ninitial increase, followed by a decrease, and then another increase. This\ndistribution can be interpreted as a fiducial power-law profile with an\nexponent of -1.0 that is determined in the range of $0.3 < q < 0.8$, but with a\ndeficiency of binaries at lower $q$ and an excess at higher $q$. The variations\nof $f_{\\rm b}$ and $q$ with $\\mathcal{M}_1$ reveal a complex binary\ndistribution within the Pleiades, which might be attributed to a combination of\nprimordial binary formation mechanisms, dynamical interactions, and the\nobservational limit of photometric binaries imposed by $q_{\\rm lim}\n(\\mathcal{M}_1)$.",
        "Management of sensing resources is a non-trivial problem for future military\nair assets with future systems deploying heterogeneous sensors to generate\ninformation of the battlespace. Machine learning techniques including deep\nreinforcement learning (DRL) have been identified as promising approaches, but\nrequire high-fidelity training environments and feature extractors to generate\ninformation for the agent. This paper presents a deep reinforcement learning\ntraining approach, utilising the Stone Soup tracking framework as a feature\nextractor to train an agent for a sensor management task. A general framework\nfor embedding Stone Soup tracker components within a Gymnasium environment is\npresented, enabling fast and configurable tracker deployments for RL training\nusing Stable Baselines3. The approach is demonstrated in a sensor management\ntask where an agent is trained to search and track a region of airspace\nutilising track lists generated from Stone Soup trackers. A sample\nimplementation using three neural network architectures in a search-and-track\nscenario demonstrates the approach and shows that RL agents can outperform\nsimple sensor search and track policies when trained within the Gymnasium and\nStone Soup environment.",
        "Load Balancing (LB) is a routing strategy that increases performance by\ndistributing traffic over multiple outgoing links. In this work, we introduce a\nnovel methodology to detect the influence of LB on anycast routing, which can\nbe used by operators to detect network regions that experience anycast routing\ninstability. We use our methodology to measure the effects of LB-behavior on\nanycast routing at a global scale, covering both IPv4 and IPv6. Our results\nshow that LB-induced anycast routing instability is widespread. The results\nalso show our method can detect LB implementations on the global Internet,\nincluding detection and classification of Points-of-Presence (PoP) and egress\nselection techniques deployed by hypergiants, cloud providers, and network\noperators. We observe LB-induced routing instability directs distinct flows to\ndifferent anycast sites with significant latency inflation. In cases with two\npaths between an anycast instance and a load-balanced destination, we observe\nan average RTT difference of 30 ms with 8% of load-balanced destinations seeing\nRTT differences of over 100 ms. Being able to detect these cases can help\nanycast operators significantly improve their service for affected clients.",
        "Federated Learning (FL) presents a promising avenue for collaborative model\ntraining among medical centers, facilitating knowledge exchange without\ncompromising data privacy. However, vanilla FL is prone to server failures and\nrarely achieves optimal performance on all participating sites due to\nheterogeneous data distributions among them. To overcome these challenges, we\npropose Gossip Contrastive Mutual Learning (GCML), a unified framework to\noptimize personalized models in a decentralized environment, where Gossip\nProtocol is employed for flexible and robust peer-to-peer communication. To\nmake efficient and reliable knowledge exchange in each communication without\nthe global knowledge across all the sites, we introduce deep contrast mutual\nlearning (DCML), a simple yet effective scheme to encourage knowledge transfer\nbetween the incoming and local models through collaborative training on local\ndata. By integrating DCML with other efforts to optimize site-specific models\nby leveraging useful information from peers, we evaluated the performance and\nefficiency of the proposed method on three publicly available datasets with\ndifferent segmentation tasks. Our extensive experimental results show that the\nproposed GCML framework outperformed both centralized and decentralized FL\nmethods with significantly reduced communication overhead, indicating its\npotential for real-world deployment.",
        "This work introduces the Query\/Hit (Q\/H) learning model. The setup consists\nof two agents. One agent, Alice, has access to a streaming source, while the\nother, Bob, does not have direct access to the source. Communication occurs\nthrough sequential Q\/H pairs: Bob sends a sequence of source symbols (queries),\nand Alice responds with the waiting time until each query appears in the source\nstream (hits). This model is motivated by scenarios with communication,\ncomputation, and privacy constraints that limit real-time access to the source.\nThe error exponent for sequential hypothesis testing under the Q\/H model is\ncharacterized, and a querying strategy, the Dynamic Scout-Sentinel Algorithm\n(DSSA), is proposed. The strategy employs a mutual information neural estimator\nto compute the error exponent associated with each query and to select the\nquery with the highest efficiency. Extensive empirical evaluations on both\nsynthetic and real-world datasets -- including mouse movement trajectories,\ntypesetting patterns, and touch-based user interactions -- are provided to\nevaluate the performance of the proposed strategy in comparison with baselines,\nin terms of probability of error, query choice, and time-to-detection.",
        "Excitons, Coulomb bound electron-hole pairs, dominate the optical response of\ntwo-dimensional semiconductors across near-infrared and visible frequencies due\nto their large binding energy and prominent oscillator strength. Previous\nmeasurements of excitons in 2D semiconductors have primarily relied on\nfar-field optical spectroscopy techniques which are diffraction limited to\nseveral hundred nanometers. To precisely image nanoscale spatial disorder\nrequires an order of magnitude increase in resolution capabilities. Here, we\npresent a study of the exciton spectra of monolayer MoSe2 in the visible range\nusing a cryogenic scattering-type scanning near field optical microscope\n(s-SNOM) operating down to 11 K. By mapping the spatial variation in the\nexciton resonance across an hBN encapsulated MoSe2 monolayer, we achieve sub-50\nnm spatial resolution and energy resolution below 1 meV. We further investigate\nthe material's near-field spectra and dielectric function, demonstrating the\nability of cryogenic visible s-SNOM to reveal nanoscale disorder. Comparison to\nroom temperature measurements illustrate the enhanced capabilities of cryogenic\ns-SNOM to reveal fine-scale material heterogeneity.",
        "We consider stably rotating highly magnetised neutron stars and glitching\npulsars. We discuss the prospects for detecting continuous gravitational waves\nfrom these sources below 20 Hz with next-generation ground-based facilities\nsuch as the Einstein Telescope and Cosmic Explorer and space-based\nobservatories such as DECIGO and Big Bang Observer. We demonstrate that these\nconstitute interesting science targets. We use a robust sensitivity estimation\nmethod for future searches based on demonstrated performance. We show that the\nspin-down upper limit on the gravitational wave amplitude of more than 90% of\nall highly magnetised pulsars and magnetars suitable for a years-long fully\ncoherent search, exceeds the smallest gravitational wave amplitude estimated\ndetectable with DECIGO and Big Bang Observer. We find that the hidden magnetar\ncandidate PSR J1852+0040 can be detected by Cosmic Explorer if it is emitting\nat least at 20% of its spin-down luminosity. Finally, post-glitch transient\ncontinuous gravitational waves from magnetars are an interesting target for\ndeci-Hz detectors, with all but one of the recorded glitches giving rise to a\nspin-down limit signal above the smallest detectable level.",
        "We developed a vacuum generation system composed of a reciprocating\ncompressor (3 tons of refrigeration) with an inverted-function that is ready to\nbe hooked flexibly to a gas-tight container to create an evacuated enclosed\natmosphere, without strict limitation of the size of that container. The\nevacuated container (or vacuum chamber) can serve in different purposes such as\neducational demonstration of the vacuum properties, extraction of perfumes from\nherbal resources, and preserving food. We tested the device and found it can\nreach a vacuum level of 26 inches of mercury in an environment with an\natmospheric pressure of 28.5 inches of mercury. We compared the performance of\nour vacuum device to a rotary-vane vacuum pump of 1\/4 horsepowers and found\nthat the vacuum pump reaches a set test vacuum level of 25 inches of mercury\nbefore the compressor. We then demonstrated experimentally some features of the\nvacuum using the inverted compressor or the vane vacuum pump. These experiments\nserve some topics in physics for school students as well as two core subjects\nof mechanical engineering, namely fluid mechanics and thermodynamics.",
        "Neural ordinary differential equations (NODEs) are an emerging paradigm in\nscientific computing for modeling dynamical systems. By accurately learning\nunderlying dynamics in data in the form of differential equations, NODEs have\nbeen widely adopted in various domains, such as healthcare, finance, computer\nvision, and language modeling. However, there remains a limited understanding\nof the privacy implications of these fundamentally different models,\nparticularly with regard to their membership inference risks.\n  In this work, we study the membership inference risks associated with NODEs.\nWe first comprehensively evaluate NODEs against membership inference attacks.\nWe show that NODEs are twice as resistant to these privacy attacks compared to\nconventional feedforward models such as ResNets. By analyzing the variance in\nmembership risks across different NODE models, we identify the factors that\ncontribute to their lower risks. We then demonstrate, both theoretically and\nempirically, that membership inference risks can be further mitigated by\nutilizing a stochastic variant of NODEs: Neural stochastic differential\nequations (NSDEs). We show that NSDEs are differentially-private (DP) learners\nthat provide the same provable privacy guarantees as DP-SGD, the de-facto\nmechanism for training private models. NSDEs are also effective in mitigating\nexisting membership inference attacks, demonstrating risks comparable to\nprivate models trained with DP-SGD while offering an improved privacy-utility\ntrade-off. Moreover, we propose a drop-in-replacement strategy that efficiently\nintegrates NSDEs into conventional feedforward models to enhance their privacy.",
        "A Nijenhuis operator $L$ is a $(1,1)$-tensor field on a smooth manifold $M$\nwith vanishing Nijenhuis torsion ${ {\\mathcal N_L}}$. At each point $x\\in M$,\nthe algebraic type of $L(x)$ is characterized by its Jordan normal form. In\nthis paper, we study singularities of a two-dimensional Nijenhuis operator in\nthe case when its trace has a non-zero differential at the singular point. A\ndescription of such singularities reduces to studying the smoothness of some\nfunction, which is a fraction depending on partial derivatives of the\ndeterminant of $L$. We completely describe singularities for some special\nclasses of functions. We also obtained interesting examples of Nijenhuis\noperators and their singularities.",
        "Quantum networks revolutionize the way of information transmission and are an\nessential step in building a quantum internet. Generally, the information\ncapacity per user-channel in a quantum network drastically decreases with the\nincrease of network capacity, making it difficultly scale to large-user\nscenarios. To break this limit, we develop a quantum network architecture in\nwhich the information capacity per user-channel is independent of the network\ncapacity (NCI-QN), and all previous quantum networks can be regarded as special\nexamples. Three aspects are investigated. Firstly, a quantum network scheme\nformulated in a comprehensive multi-mode time-frequency representation is\npresented. Then, information characteristics of the proposed quantum network\nare delineated by expanding the well-known Pirandola-Laurenza-Ottaviani-Banchi\n(PLOB) bound and the Holevo bound from the linear combination of point-to-point\nlinks to a complex network architecture, demonstrating clear proofs of the\nnetwork capacity independence. Finally, a practical NCI-QN with a network\ncapacity of 19 is experimentally demonstrated using optical frequency comb in\nquantum key distribution network scenarios, in which the security under the\nasymptotic case, finite-size effect, composable security, and composable\nfinite-size security are verified, with a secret key rate up to 8.75 Gbps. This\nachievement overcomes the user-rate limit of quantum network, which is the\nkeystone for the development of the quantum internet.",
        "ELIZA, created by Joseph Weizenbaum at MIT in the early 1960s, is usually\nconsidered the world's first chatbot. It was developed in MAD-SLIP on MIT's\nCTSS, the world's first time-sharing system, on an IBM 7094. We discovered an\noriginal ELIZA printout in Prof. Weizenbaum's archives at MIT, including an\nearly version of the famous DOCTOR script, a nearly complete version of the\nMAD-SLIP code, and various support functions in MAD and FAP. Here we describe\nthe reanimation of this original ELIZA on a restored CTSS, itself running on an\nemulated IBM 7094. The entire stack is open source, so that any user of a\nunix-like OS can run the world's first chatbot on the world's first\ntime-sharing system."
      ]
    }
  },
  {
    "id":2411.15395,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Brain\u2013Computer Interface Spellers: A Review",
    "start_abstract":"A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b17"
      ],
      "title":[
        "Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication"
      ],
      "abstract":[
        "Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Multilingual Non-Autoregressive Machine Translation without Knowledge\n  Distillation",
        "PanopticSplatting: End-to-End Panoptic Gaussian Splatting",
        "Emergent effects of scaling on the functional hierarchies within large\n  language models",
        "CLDyB: Towards Dynamic Benchmarking for Continual Learning with\n  Pre-trained Models",
        "Evaluating Developer-written Unit Test Case Reduction for Java -- A\n  Replication Study",
        "Non-Gaussianities as a Signature of Quantumness of Quantum Cosmology",
        "Propagation of Gravitational Waves on a Geometric Condensate background\n  of $(R + \\alpha R^{2})$ Origin",
        "A Search for Eclipse Cycles Similar to the Hypersaros: Columbus and the\n  Lunar Eclipse of March 14, 2025",
        "Realistic Clothed Human and Object Joint Reconstruction from a Single\n  Image",
        "Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct\n  Preference Optimization",
        "A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention\n  Logit Interpolation (GALI)",
        "Demystifying integrable QFTs in AdS: No-go theorems for higher-spin\n  charges",
        "CoopDETR: A Unified Cooperative Perception Framework for 3D Detection\n  via Object Query",
        "Predicting Human Choice Between Textually Described Lotteries",
        "A Study of the Efficacy of Generative Flow Networks for Robotics and\n  Machine Fault-Adaptation",
        "Estimating relapse time distribution from longitudinal biomarker\n  trajectories using iterative regression and continuous time Markov processes",
        "Spatial Context-Driven Positive Pair Sampling for Enhanced\n  Histopathology Image Classification",
        "Some results of CCD-photometry of variable stars at the Astronomical\n  Institute of Karazin Kharkiv National University",
        "$CP$ violation in the $HZZ$ vertex and left-right asymmetries",
        "Riemannian Metric Learning: Closer to You than You Imagine",
        "Broadband transient full-Stokes luminescence spectroscopy with high\n  sensitivity",
        "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
        "Increasing the distance of topological codes with time vortex defects",
        "PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on\n  UML Flowcharts",
        "Adaptivity and Convergence of Probability Flow ODEs in Diffusion\n  Generative Models",
        "Team A at SemEval-2025 Task 11: Breaking Language Barriers in Emotion\n  Detection with Multilingual Models",
        "qReduMIS: A Quantum-Informed Reduction Algorithm for the Maximum\n  Independent Set Problem",
        "Promising High Temperature Thermoelectric Performance of Alkali\n  Metal-based Zintl phases X$_2$AgY (X = Na, K; Y = Sb, Bi): Insights from\n  First-Principles Studies",
        "Representation-based Reward Modeling for Efficient Safety Alignment of\n  Large Language Model"
      ],
      "abstract":[
        "Multilingual neural machine translation (MNMT) aims at using one single model\nfor multiple translation directions. Recent work applies non-autoregressive\nTransformers to improve the efficiency of MNMT, but requires expensive\nknowledge distillation (KD) processes. To this end, we propose an M-DAT\napproach to non-autoregressive multilingual machine translation. Our system\nleverages the recent advance of the directed acyclic Transformer (DAT), which\ndoes not require KD. We further propose a pivot back-translation (PivotBT)\napproach to improve the generalization to unseen translation directions.\nExperiments show that our M-DAT achieves state-of-the-art performance in\nnon-autoregressive MNMT.",
        "Open-vocabulary panoptic reconstruction is a challenging task for\nsimultaneous scene reconstruction and understanding. Recently, methods have\nbeen proposed for 3D scene understanding based on Gaussian splatting. However,\nthese methods are multi-staged, suffering from the accumulated errors and the\ndependence of hand-designed components. To streamline the pipeline and achieve\nglobal optimization, we propose PanopticSplatting, an end-to-end system for\nopen-vocabulary panoptic reconstruction. Our method introduces query-guided\nGaussian segmentation with local cross attention, lifting 2D instance masks\nwithout cross-frame association in an end-to-end way. The local cross attention\nwithin view frustum effectively reduces the training memory, making our model\nmore accessible to large scenes with more Gaussians and objects. In addition,\nto address the challenge of noisy labels in 2D pseudo masks, we propose label\nblending to promote consistent 3D segmentation with less noisy floaters, as\nwell as label warping on 2D predictions which enhances multi-view coherence and\nsegmentation accuracy. Our method demonstrates strong performances in 3D scene\npanoptic reconstruction on the ScanNet-V2 and ScanNet++ datasets, compared with\nboth NeRF-based and Gaussian-based panoptic reconstruction methods. Moreover,\nPanopticSplatting can be easily generalized to numerous variants of Gaussian\nsplatting, and we demonstrate its robustness on different Gaussian base models.",
        "Large language model (LLM) architectures are often described as functionally\nhierarchical: Early layers process syntax, middle layers begin to parse\nsemantics, and late layers integrate information. The present work revisits\nthese ideas. This research submits simple texts to an LLM (e.g., \"A church and\norgan\") and extracts the resulting activations. Then, for each layer, support\nvector machines and ridge regressions are fit to predict a text's label and\nthus examine whether a given layer encodes some information. Analyses using a\nsmall model (Llama-3.2-3b; 28 layers) partly bolster the common hierarchical\nperspective: Item-level semantics are most strongly represented early (layers\n2-7), then two-item relations (layers 8-12), and then four-item analogies\n(layers 10-15). Afterward, the representation of items and simple relations\ngradually decreases in deeper layers that focus on more global information.\nHowever, several findings run counter to a steady hierarchy view: First,\nalthough deep layers can represent document-wide abstractions, deep layers also\ncompress information from early portions of the context window without\nmeaningful abstraction. Second, when examining a larger model\n(Llama-3.3-70b-Instruct), stark fluctuations in abstraction level appear: As\ndepth increases, two-item relations and four-item analogies initially increase\nin their representation, then markedly decrease, and afterward increase again\nmomentarily. This peculiar pattern consistently emerges across several\nexperiments. Third, another emergent effect of scaling is coordination between\nthe attention mechanisms of adjacent layers. Across multiple experiments using\nthe larger model, adjacent layers fluctuate between what information they each\nspecialize in representing. In sum, an abstraction hierarchy often manifests\nacross layers, but large models also deviate from this structure in curious\nways.",
        "The advent of the foundation model era has sparked significant research\ninterest in leveraging pre-trained representations for continual learning (CL),\nyielding a series of top-performing CL methods on standard evaluation\nbenchmarks. Nonetheless, there are growing concerns regarding potential data\ncontamination during the pre-training stage. Furthermore, standard evaluation\nbenchmarks, which are typically static, fail to capture the complexities of\nreal-world CL scenarios, resulting in saturated performance. To address these\nissues, we describe CL on dynamic benchmarks (CLDyB), a general computational\nframework based on Markov decision processes for evaluating CL methods\nreliably. CLDyB dynamically identifies inherently difficult and\nalgorithm-dependent tasks for the given CL methods, and determines challenging\ntask orders using Monte Carlo tree search. Leveraging CLDyB, we first conduct a\njoint evaluation of multiple state-of-the-art CL methods, leading to a set of\ncommonly challenging and generalizable task sequences where existing CL methods\ntend to perform poorly. We then conduct separate evaluations of individual CL\nmethods using CLDyB, discovering their respective strengths and weaknesses. The\nsource code and generated task sequences are publicly accessible at\nhttps:\/\/github.com\/szc12153\/CLDyB.",
        "Abstract: Failing test case reduction can promote efficient debugging because\na developer may not need to observe components that are not relevant to\ninducing failure. Failing test case reduction can also improve the efficiency\nof fault localization. These considerations have prompted researchers to study\nthe reduction process, the reduction output, and the removed entities. Christi\net al. studied test reduction using a tool called ReduSharptor for C# tests.\nThey considered the test to be an Abstract Syntax Tree (AST). Based on that,\nthey studied the reduction outcome and removed entities in terms of Leaf nodes\nand Non-Leaf nodes of the AST. They claimed that (1) leaf nodes are removed in\nlarge numbers, and (2) the probability of removal is slightly higher than\nnon-leaf nodes. We replicate their results using a different test case\nreduction tool, ReduJavator, for Java unit tests. We evaluate test reduction\nusing 30 randomly chosen bugs from the Defects4J database and 30 mutants for 6\nopen-source projects. Our results confirm their first claim: leaf nodes are\nremoved in large numbers. Our results are inconclusive regarding their second\nclaim; we cannot confirm that the probability of removal is higher for non-leaf\nnodes.",
        "We show that the consistent application of the rules of quantum mechanics to\ncosmological systems inevitably results in the so-called multiverse states in\nwhich neither the background spacetime nor the inhomogeneous perturbation are\nin definite states. We study the multiverse states as perturbations to the\nusually employed so-called Born-Oppenheimer states that are products of a wave\nfunction of the background and a wave function of the perturbation. The\nobtained corrections involve integrals over \\emph{virtual backgrounds} that\nrepresent the effect of quantum background fluctuations on the perturbation\nstate. They resemble loop corrections in quantum field theory. This approach\ndemonstrates the inevitable existence of very specific non-Gaussian features in\nprimordial fluctuations. We express the resulting non-Gaussian perturbation as\na nonlinear function of the Gaussian perturbation obtained within the\nBorn-Oppenheimer approximation, and compute its trispectrum, to show that the\nmultiverse scenario leads to testable and distinct signatures in cosmological\nperturbations. Our approach applies both to inflationary and alternative\ncosmologies.",
        "In this paper we propose a new paradigm for cosmology: a time dependent\nscalar condensate background originated from the quadratic $(R + \\alpha R^2)$\nStarobinski model, where $R$ is the Ricci scalar and $\\alpha$ the coupling\nconstant. In weak gravity limit the system decouples into a conventional\ngraviton and a higher derivative scalar. It was shown earlier through works\nfrom our group, \\cite{ssg,sg,us}, that the latter can sustain an oscillatory\nlowest energy configuration or a {\\it{Geometric Condensate}} as it consists\nentirely of metric degrees of freedom. In the present work, we study\nGravitational Wave propagation in this condensate background. We show that the\nexplicit time dependent nature of the condensate can generate curvature and\nradiation-like contributions in the scale factor evolution in FLRW cosmology.\nSubsequently the condensate leaves its signature on the Gravitational Wave\nprofile as it propagates in the condensate modified FLRW spacetime. The wave\nprofile is calculated analytically in terms of Whittaker functions. The main\nnovelty of the Geometric Condensate scheme is that no external (condensate)\nmatter from outside has been considered.",
        "The total lunar eclipse on March 14, 2025 UT occurs nearly exactly 521 years\n(one Hypersaros) after a similar eclipse on March 1, 1504 UT that is renowned\nfor its importance to the voyage of Columbus to Jamaica. Eclipses separated by\na Hypersaros have similar depths, appear very close to the same location in the\nsky, and occur at nearly the same time of year. This paper summarizes the\nresults from a search for analogous cycles within the Five Millennium Catalogs\nof Lunar and Solar Eclipses. Under the two simple constraints of similar\neclipse dates relative to the vernal equinox and similar paths of the Moon\nthrough the Earth's shadow, the most common time intervals between lunar\neclipses separated by less than 1000 years are the 521-year Hypersaros and a\n633-yr period of the Icosa-Inex-Triple-Saros (IITS). Notable cycles at longer\nperiods occur at 1154, 1284, 1787, 1917, and 2308 years.",
        "Recent approaches to jointly reconstruct 3D humans and objects from a single\nRGB image represent 3D shapes with template-based or coarse models, which fail\nto capture details of loose clothing on human bodies. In this paper, we\nintroduce a novel implicit approach for jointly reconstructing realistic 3D\nclothed humans and objects from a monocular view. For the first time, we model\nboth the human and the object with an implicit representation, allowing to\ncapture more realistic details such as clothing. This task is extremely\nchallenging due to human-object occlusions and the lack of 3D information in 2D\nimages, often leading to poor detail reconstruction and depth ambiguity. To\naddress these problems, we propose a novel attention-based neural implicit\nmodel that leverages image pixel alignment from both the input human-object\nimage for a global understanding of the human-object scene and from local\nseparate views of the human and object images to improve realism with, for\nexample, clothing details. Additionally, the network is conditioned on semantic\nfeatures derived from an estimated human-object pose prior, which provides 3D\nspatial information about the shared space of humans and objects. To handle\nhuman occlusion caused by objects, we use a generative diffusion model that\ninpaints the occluded regions, recovering otherwise lost details. For training\nand evaluation, we introduce a synthetic dataset featuring rendered scenes of\ninter-occluded 3D human scans and diverse objects. Extensive evaluation on both\nsynthetic and real-world datasets demonstrates the superior quality of the\nproposed human-object reconstructions over competitive methods.",
        "The emergence of large Vision Language Models (VLMs) has broadened the scope\nand capabilities of single-modal Large Language Models (LLMs) by integrating\nvisual modalities, thereby unlocking transformative cross-modal applications in\na variety of real-world scenarios. Despite their impressive performance, VLMs\nare prone to significant hallucinations, particularly in the form of\ncross-modal inconsistencies. Building on the success of Reinforcement Learning\nfrom Human Feedback (RLHF) in aligning LLMs, recent advancements have focused\non applying direct preference optimization (DPO) on carefully curated datasets\nto mitigate these issues. Yet, such approaches typically introduce preference\nsignals in a brute-force manner, neglecting the crucial role of visual\ninformation in the alignment process. In this paper, we introduce Re-Align, a\nnovel alignment framework that leverages image retrieval to construct a\ndual-preference dataset, effectively incorporating both textual and visual\npreference signals. We further introduce rDPO, an extension of the standard\ndirect preference optimization that incorporates an additional visual\npreference objective during fine-tuning. Our experimental results demonstrate\nthat Re-Align not only mitigates hallucinations more effectively than previous\nmethods but also yields significant performance gains in general visual\nquestion-answering (VQA) tasks. Moreover, we show that Re-Align maintains\nrobustness and scalability across a wide range of VLM sizes and architectures.\nThis work represents a significant step forward in aligning multimodal LLMs,\npaving the way for more reliable and effective cross-modal applications. We\nrelease all the code in https:\/\/github.com\/taco-group\/Re-Align.",
        "Transformer-based Large Language Models (LLMs) struggle to process inputs\nexceeding their training context window, with performance degrading due to\npositional out-of-distribution (O.O.D.) that disrupt attention computations.\nExisting solutions, fine-tuning and training-free methods, are limited by\ncomputational inefficiency, attention logit outliers or loss of local\npositional information. To address this, we propose Greedy Attention Logit\nInterpolation (GALI), a training-free length extrapolation method that\nmaximizes the utilization of pretrained positional intervals while avoiding\nattention logit outliers through attention logit interpolation. The result\ndemonstrates that GALI consistently outperforms state-of-the-art training-free\nmethods. Our findings reveal that LLMs interpret positional intervals unevenly\nwithin their training context window, suggesting that extrapolating within a\nsmaller positional interval range yields superior results-even for\nshort-context tasks. GALI represents a significant step toward resolving the\npositional O.O.D. challenge, enabling more reliable long-text understanding in\nLLMs. Our implementation of GALI, along with the experiments from our paper, is\nopen-sourced at https:\/\/github.com\/AcademyCityL\/GALI.",
        "Higher-spin conserved currents and charges feature prominently in integrable\n2d QFTs in flat space. Motivated by the question of integrable field theories\nin AdS space, we consider the consequences of higher-spin currents for QFTs in\nAdS$_2$, and find that their effect is much more constraining than in flat\nspace. Specifically, it is impossible to preserve: (a)~any higher-spin charges\nwhen deforming a massive free field by interactions, or (b)~any spin-4 charges\nwhen deforming a CFT by a Virasoro primary. Therefore, in these settings, there\nare no integrable theories in AdS with higher-spin conserved charges. Along the\nway, we explain how higher-spin charges lead to integer spacing in the spectrum\nof primaries, sum rules on the OPE data, and constraints on correlation\nfunctions. We also explain a key difference between AdS and flat space: in AdS\none cannot `partially' conserve a higher-spin current along particular\ndirections, since the AdS isometries imply full conservation.",
        "Cooperative perception enhances the individual perception capabilities of\nautonomous vehicles (AVs) by providing a comprehensive view of the environment.\nHowever, balancing perception performance and transmission costs remains a\nsignificant challenge. Current approaches that transmit region-level features\nacross agents are limited in interpretability and demand substantial bandwidth,\nmaking them unsuitable for practical applications. In this work, we propose\nCoopDETR, a novel cooperative perception framework that introduces object-level\nfeature cooperation via object query. Our framework consists of two key\nmodules: single-agent query generation, which efficiently encodes raw sensor\ndata into object queries, reducing transmission cost while preserving essential\ninformation for detection; and cross-agent query fusion, which includes Spatial\nQuery Matching (SQM) and Object Query Aggregation (OQA) to enable effective\ninteraction between queries. Our experiments on the OPV2V and V2XSet datasets\ndemonstrate that CoopDETR achieves state-of-the-art performance and\nsignificantly reduces transmission costs to 1\/782 of previous methods.",
        "Predicting human decision-making under risk and uncertainty is a\nlong-standing challenge in cognitive science, economics, and AI. While prior\nresearch has focused on numerically described lotteries, real-world decisions\noften rely on textual descriptions. This study conducts the first large-scale\nexploration of human decision-making in such tasks using a large dataset of\none-shot binary choices between textually described lotteries. We evaluate\nmultiple computational approaches, including fine-tuning Large Language Models\n(LLMs), leveraging embeddings, and integrating behavioral theories of choice\nunder risk. Our results show that fine-tuned LLMs, specifically RoBERTa and\nGPT-4o outperform hybrid models that incorporate behavioral theory, challenging\nestablished methods in numerical settings. These findings highlight fundamental\ndifferences in how textual and numerical information influence decision-making\nand underscore the need for new modeling strategies to bridge this gap.",
        "Advancements in robotics have opened possibilities to automate tasks in\nvarious fields such as manufacturing, emergency response and healthcare.\nHowever, a significant challenge that prevents robots from operating in\nreal-world environments effectively is out-of-distribution (OOD) situations,\nwherein robots encounter unforseen situations. One major OOD situations is when\nrobots encounter faults, making fault adaptation essential for real-world\noperation for robots. Current state-of-the-art reinforcement learning\nalgorithms show promising results but suffer from sample inefficiency, leading\nto low adaptation speed due to their limited ability to generalize to OOD\nsituations. Our research is a step towards adding hardware fault tolerance and\nfast fault adaptability to machines. In this research, our primary focus is to\ninvestigate the efficacy of generative flow networks in robotic environments,\nparticularly in the domain of machine fault adaptation. We simulated a robotic\nenvironment called Reacher in our experiments. We modify this environment to\nintroduce four distinct fault environments that replicate real-world\nmachines\/robot malfunctions. The empirical evaluation of this research\nindicates that continuous generative flow networks (CFlowNets) indeed have the\ncapability to add adaptive behaviors in machines under adversarial conditions.\nFurthermore, the comparative analysis of CFlowNets with reinforcement learning\nalgorithms also provides some key insights into the performance in terms of\nadaptation speed and sample efficiency. Additionally, a separate study\ninvestigates the implications of transferring knowledge from pre-fault task to\npost-fault environments. Our experiments confirm that CFlowNets has the\npotential to be deployed in a real-world machine and it can demonstrate\nadaptability in case of malfunctions to maintain functionality.",
        "Biomarker measurements obtained by blood sampling are often used as a\nnon-invasive means of monitoring tumour progression in cancer patients.\nDiseases evolve dynamically over time, and studying longitudinal observations\nof specific biomarkers can help to understand patients response to treatment\nand predict disease progression. We propose a novel iterative regression-based\nmethod to estimate changes in patients status within a cohort that includes\ncensored patients, and illustrate it on clinical data from myeloma cases. We\nformulate the relapse time estimation problem in the framework of Piecewise\nDeterministic Markov processes (PDMP), where the Euclidean component is a\nsurrogate biomarker for patient state. This approach enables continuous-time\nestimation of the status-change dates, which in turn allows for accurate\ninference of the relapse time distribution. A key challenge lies in the partial\nobservability of the process, a complexity that has been rarely addressed in\nprevious studies. . We evaluate the performance of our procedure through a\nsimulation study and compare it with different approaches. This work is a proof\nof concept on biomarker trajectories with simple behaviour, but our method can\neasily be extended to more complex dynamics.",
        "Deep learning has demonstrated great promise in cancer classification from\nwhole-slide images (WSIs) but remains constrained by the need for extensive\nannotations. Annotation-free methods, such as multiple instance learning (MIL)\nand self-supervised learning (SSL), have emerged to address this challenge;\nhowever, current SSL techniques often depend on synthetic augmentations or\ntemporal context, which may not adequately capture the intricate spatial\nrelationships inherent to histopathology. In this work, we introduce a novel\nspatial context-driven positive pair sampling strategy for SSL that leverages\nthe natural coherence of adjacent patches in WSIs. By constructing biologically\nrelevant positive pairs from spatially proximate patches, our approach\nharnesses inherent spatial coherence to enhance patch-level representations,\nultimately boosting slide-level classification performance. Experiments on\nmultiple datasets reveal that our strategy improves classification accuracy by\n5\\% to 10\\% over the standard method, paving the way for more clinically\nrelevant AI models in cancer diagnosis. The code is available at\nhttps:\/\/anonymous.4open.science\/r\/contextual-pairs-E72F\/.",
        "We presented photometric observations for the one UV Ceti type and three W\nUrsae Majoris-type variable stars. The flare of the UV Ceti type star lasted\nabout two hours, and the star changed magnitude to 3.9 within about two\nminutes. The values of color indices V-R, the rotational periods and the\ncomposite lightcurves have been obtained for the EW stars. Using a relation of\nan absolute magnitude-period obtained by Mateo and Rucinski (2017) and\ninterstellar extinction from the three-dimensional map of Milky Way dust\n(http:\/\/argonaut.skymaps.info) and Green et al. (2019), we have calculated the\nabsolute magnitudes of the EW stars and distances to them. The parallaxes\nobtained from our data differ from those given in Gaia DR 3, which may be due\nto insufficient quality calibration of the absolute magnitude-period relation\nand with the estimations of interstellar extinction.",
        "We calculate new contributions to the $HZZ$ vertex from the Flavor Changing\nNeutral Current (FCNC) of the Higgs and $Z$ bosons. It is found that the\n$h_2^V$ and $h_3^V$ ($V=H$, $Z$) form factors can be induced through these\ncouplings, and we present our results in terms of the Passarino-Veltman scalar\nfunctions. Using the current limits on $H\\overline{t}c$ and $Z\\overline{t}c$\ncouplings, we determine that the new contributions to the $CP$-conserving form\nfactor $h_2^V$ are small in comparison to the Standard Model (SM) predictions.\nHowever, for the $CP$-violating form factor $h_3^V$, the contributions can\nreach values as large as $10^{-6}$, five orders of magnitude larger than in the\nSM. Furthermore, we examine how these results influence the left-right\nasymmetries in the processes $H^\\ast\\to ZZ$ and $Z^\\ast\\to ZH$. Our findings\nindicate that significant deviations from the SM predictions may arise when\nFCNC contributions are considered.",
        "Riemannian metric learning is an emerging field in machine learning,\nunlocking new ways to encode complex data structures beyond traditional\ndistance metric learning. While classical approaches rely on global distances\nin Euclidean space, they often fall short in capturing intrinsic data geometry.\nEnter Riemannian metric learning: a powerful generalization that leverages\ndifferential geometry to model the data according to their underlying\nRiemannian manifold. This approach has demonstrated remarkable success across\ndiverse domains, from causal inference and optimal transport to generative\nmodeling and representation learning. In this review, we bridge the gap between\nclassical metric learning and Riemannian geometry, providing a structured and\naccessible overview of key methods, applications, and recent advances. We argue\nthat Riemannian metric learning is not merely a technical refinement but a\nfundamental shift in how we think about data representations. Thus, this review\nshould serve as a valuable resource for researchers and practitioners\ninterested in exploring Riemannian metric learning and convince them that it is\ncloser to them than they might imagine-both in theory and in practice.",
        "Materials emitting circularly polarized light (CPL) are highly sought after\nfor applications ranging from efficient displays to quantum information\ntechnologies. Established methods for time-resolved CPL characterization have\nsignificant limitations, preventing in-depth photophysical insight necessary\nfor materials development. We have designed and built a high-sensitivity (noise\nlevel 10^-4), broadband (ca. 400-900 nm), transient (ns resolution, ms range)\nfull-Stokes (CPL and linear polarizations) spectroscopy setup. We demonstrate\nits broad applicability by measuring compounds with low dissymmetry factors\nacross various timescales, as well as tracking the temporal evolution of linear\npolarization components alongside associated CPL artefacts. We have written\nopen-source software and used stock optical components to make transient CPL\nspectroscopy practically accessible to a wide audience, enabling the study of\nchiral materials in numerous diverse applications.",
        "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https:\/\/github.com\/sail-sg\/LongSpec.",
        "We propose modifying topological quantum error correcting codes by\nincorporating space-time defects, termed ``time vortices,'' to reduce the\nnumber of physical qubits required to achieve a desired logical error rate. A\ntime vortex is inserted by adding a spatially varying delay to the periodic\nmeasurement sequence defining the code such that the delay accumulated on a\nhomologically non-trivial cycle is an integer multiple of the period. We\nanalyze this construction within the framework of the Floquet color code and\noptimize the embedding of the code on a torus along with the choice of the\nnumber of time vortices inserted in each direction. Asymptotically, the\nvortexed code requires less than half the number of qubits as the vortex-free\ncode to reach a given code distance. We benchmark the performance of the\nvortexed Floquet color code by Monte Carlo simulations with a circuit-level\nnoise model and demonstrate that the smallest vortexed code (with $30$ qubits)\noutperforms the vortex-free code with $42$ qubits.",
        "Process-driven dialogue systems, which operate under strict predefined\nprocess constraints, are essential in customer service and equipment\nmaintenance scenarios. Although Large Language Models (LLMs) have shown\nremarkable progress in dialogue and reasoning, they still struggle to solve\nthese strictly constrained dialogue tasks. To address this challenge, we\nconstruct Process Flow Dialogue (PFDial) dataset, which contains 12,705\nhigh-quality Chinese dialogue instructions derived from 440 flowcharts\ncontaining 5,055 process nodes. Based on PlantUML specification, each UML\nflowchart is converted into atomic dialogue units i.e., structured five-tuples.\nExperimental results demonstrate that a 7B model trained with merely 800\nsamples, and a 0.5B model trained on total data both can surpass 90% accuracy.\nAdditionally, the 8B model can surpass GPT-4o up to 43.88% with an average of\n11.00%. We further evaluate models' performance on challenging backward\ntransitions in process flows and conduct an in-depth analysis of various\ndataset formats to reveal their impact on model performance in handling\ndecision and sequential branches. The data is released in\nhttps:\/\/github.com\/KongLongGeFDU\/PFDial.",
        "Score-based generative models, which transform noise into data by learning to\nreverse a diffusion process, have become a cornerstone of modern generative AI.\nThis paper contributes to establishing theoretical guarantees for the\nprobability flow ODE, a widely used diffusion-based sampler known for its\npractical efficiency. While a number of prior works address its general\nconvergence theory, it remains unclear whether the probability flow ODE sampler\ncan adapt to the low-dimensional structures commonly present in natural image\ndata. We demonstrate that, with accurate score function estimation, the\nprobability flow ODE sampler achieves a convergence rate of $O(k\/T)$ in total\nvariation distance (ignoring logarithmic factors), where $k$ is the intrinsic\ndimension of the target distribution and $T$ is the number of iterations. This\ndimension-free convergence rate improves upon existing results that scale with\nthe typically much larger ambient dimension, highlighting the ability of the\nprobability flow ODE sampler to exploit intrinsic low-dimensional structures in\nthe target distribution for faster sampling.",
        "This paper describes the system submitted by Team A to SemEval 2025 Task 11,\n``Bridging the Gap in Text-Based Emotion Detection.'' The task involved\nidentifying the perceived emotion of a speaker from text snippets, with each\ninstance annotated with one of six emotions: joy, sadness, fear, anger,\nsurprise, or disgust. A dataset provided by the task organizers served as the\nfoundation for training and evaluating our models. Among the various approaches\nexplored, the best performance was achieved using multilingual embeddings\ncombined with a fully connected layer. This paper details the system\narchitecture, discusses experimental results, and highlights the advantages of\nleveraging multilingual representations for robust emotion detection in text.",
        "We propose and implement a quantum-informed reduction algorithm for the\nmaximum independent set problem that integrates classical kernelization\ntechniques with information extracted from quantum devices. Our larger\nframework consists of dedicated application, algorithm, and hardware layers,\nand easily generalizes to the maximum weight independent set problem. In this\nhybrid quantum-classical framework, which we call qReduMIS, the quantum\ncomputer is used as a co-processor to inform classical reduction logic about\nfrozen vertices that are likely (or unlikely) to be in large independent sets,\nthereby opening up the reduction space after removal of targeted subgraphs. We\nsystematically assess the performance of qReduMIS based on experiments with up\nto 231 qubits run on Rydberg quantum hardware available through Amazon Braket.\nOur experiments show that qReduMIS can help address fundamental performance\nlimitations faced by a broad set of (quantum) solvers including Rydberg quantum\ndevices. We outline implementations of qReduMIS with alternative platforms,\nsuch as superconducting qubits or trapped ions, and we discuss potential future\nextensions.",
        "In the quest for novel thermoelectric materials to harvest waste\nenvironmental heat, we investigate alkali metal-based Zintl phases X$_2$AgY (X\n= Na, K, and Y = Sb, Bi) utilizing first-principles methods. We obtain\nsignificantly low lattice thermal conductivity values ranging 0.9-0.5 W\nm$^{-1}$ K$^{-1}$ at 300~K, challenging established thermoelectric materials\nsuch as SnSe, PbTe, Bi$_2$Te$_3$ as well as other Zintl phases. We trace such\nastonishingly low values to lattice anharmonicity, large phonon scattering\nphase space, low phonon velocities, and lifetimes. In K-based materials, the\nlow phonon velocities are further linked to flattened phonon modes arising from\nthe gap in the optical spectrum. Furthermore, the existence of bonding\nheterogeneity could hamper heat conduction in these materials. In addition, an\navoided crossing in the phonon dispersions suggesting rattling behavior,\nobserved in all materials except Na$_2$AgSb, suppresses the dispersion of\nacoustic modes, further reducing the phonon velocities. When combined with\nelectrical transport calculations, the materials exhibit high figure of merit\nvalues at 700~K, i.e., $ZT\\sim2.1$ for Na$_2$AgSb, $1.7$ for Na$_2$AgBi, $0.9$\nfor K$_2$AgSb, and $1.0$ for K$_2$AgBi. Our predicted $ZT$ values are\ncompetitive with state-of-the-art thermoelectric materials such as\nMg$_3$Sb$_2$, ZrCoBi, PbTe, SnSe, and as well as with contemporary Zintl\nphases. Our findings underscore the potential of light alkali metal atoms\ncombined with Ag-Bi\/Sb type frameworks to achieve superior thermoelectric\nperformance, paving the way for material design for specific operating\nconditions.",
        "Reinforcement Learning (RL) algorithms for safety alignment of Large Language\nModels (LLMs), such as Direct Preference Optimization (DPO), encounter the\nchallenge of distribution shift. Current approaches typically address this\nissue through online sampling from the target policy, which requires\nsignificant computational resources. In this paper, we hypothesize that during\noff-policy training, while the ranking order of output generated by policy\nchanges, their overall distribution remains relatively stable. This stability\nallows the transformation of the sampling process from the target policy into a\nre-ranking of preference data. Building on this hypothesis, We propose a new\nframework that leverages the model's intrinsic safety judgment capability to\nextract reward signals, which are then used to calculate label confidence for\npreferences reordering. Extensive experimental results and theoretical analysis\ndemonstrate that the proposed method effectively addresses the distribution\nshift issue, remarkably enhancing the safety performance while reducing about\n300x computational overheads."
      ]
    }
  },
  {
    "id":2411.15395,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication",
    "start_abstract":"Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Brain\u2013Computer Interface Spellers: A Review"
      ],
      "abstract":[
        "A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Applying a star formation model calibrated on high-resolution\n  interstellar medium simulations to cosmological simulations of galaxy\n  formation",
        "Neural network-based prediction of particle-induced fission cross\n  sections for r-process nucleosynthesis trained with dynamical reaction models",
        "Improving the trivial bound for $\\ell$-torsion in class groups",
        "Real-time simulation of jet energy loss and entropy production in\n  high-energy scattering with matter",
        "Cluster Ages to Reconstruct the Milky Way Assembly (CARMA). III. NGC 288\n  as the first Splashed globular cluster",
        "Compare Similarities Between DNA Sequences Using Permutation-Invariant\n  Quantum Kernel",
        "Asymptotic behavior of clusters in hierarchical species sampling models",
        "Feedback-augmented Non-homogeneous Hidden Markov Models for Longitudinal\n  Causal Inference",
        "B-fields And dust in interstelLar fiLAments using Dust POLarization\n  (BALLAD-POL): III. Grain alignment and disruption mechanisms in G34.43+0.24\n  using polarization observations from JCMT\/POL-2",
        "Optimal joint reconstruction from CMB observations: application to\n  cosmic birefringence, patchy reionization and CMB lensing",
        "Nearsightedness in Materials with Indirect Band Gap",
        "A tracking algorithm for finite-size particles",
        "Female and Combined Male-Female Injury Risk Functions for the Anterior\n  Pelvis Under Frontal Lap Belt Loading Conditions",
        "Cohering Disaggregation and Uncertainty Quantification for Spatially\n  Misaligned Data",
        "Cooperation and competing of antipolar charge order and superconducting\n  states in a quasi-2D superatomic metallic crystal",
        "Adaptive GSIS for rarefied gas flow simulations",
        "Exploring Generative Networks for Manifolds with Non-Trivial Topology",
        "On localizing subcategories of Lie superalgebra representations",
        "Optimised Graph Convolution for Calorimetry Event Classification",
        "Image Data Augmentation for the TAIGA-IACT Experiment with Conditional\n  Generative Adversarial Networks",
        "Stokes flow in the electronic fluid with odd viscosity",
        "Bifurcations in Bosonic Stars: chains and rings from spherical solutions",
        "A-priori estimates for generalized Korteweg-de Vries equations in\n  $H^{-1}(\\mathbb{R})$",
        "Data Sharing in the PRIMED Consortium: Design, implementation, and\n  recommendations for future policymaking",
        "Space compatibility of emerging, wide-bandgap, ultralow-loss integrated\n  photonics",
        "Efficient parameter inference in networked dynamical systems via steady\n  states: A surrogate objective function approach integrating mean-field and\n  nonlinear least squares",
        "On a class of globally analytic Hypoelliptic operators with non-negative\n  characteristic form",
        "Managing target of opportunity (ToO) observations at Observatorio\n  Astrof\\'isico de Javalambre (OAJ)",
        "Twenty years of Ne\\v{s}et\\v{r}il's classification programme of Ramsey\n  classes"
      ],
      "abstract":[
        "Modern high-resolution simulations of the interstellar medium (ISM) have\nshown that key factors in governing star formation are the competing influences\nof radiative dissipation, pressure support driven by stellar feedback, and the\nrelentless pull of gravity. Cosmological simulations of galaxy formation, such\nas IllustrisTNG or ASTRID, are however not able to resolve this physics in\ndetail and therefore need to rely on approximate treatments. These have often\ntaken the form of empirical subgrid models of the ISM expressed in terms of an\neffective equation of state (EOS) that relates the mean ISM pressure to the\nmean gas density. Here we seek to improve these heuristic models by directly\nfitting their key ingredients to results of the high-resolution TIGRESS\nsimulations, which have shown that the dynamical equilibrium of the ISM can be\nunderstood in terms of a pressure-regulated, feedback modulated (PRFM) model\nfor star formation. Here we explore a simple subgrid model that draws on the\nPRFM concept but uses only local quantities. It accurately reproduces PRFM for\npure gas disks, while it predicts slightly less star formation than PRFM in the\npresence of an additional thin stellar disk. We compare the properties of this\nmodel with the older Springel and Hernquist and TNG prescriptions, and apply\nall three to isolated simulations of disk galaxies as well as to a set of\nhigh-resolution zoom-in simulations carried out with a novel 'multi-zoom'\ntechnique that we introduce in this study. The softer EOS implied by TIGRESS\nproduces substantially thinner disk galaxies, which has important ramifications\nfor disk stability and galaxy morphology. The total stellar mass of galaxies is\nhowever hardly modified at low redshift, reflecting the dominating influence of\nlarge-scale gaseous inflows and outflows to galaxies, which are not sensitive\nto the EOS itself",
        "Large-scale computations of fission properties play a crucial role in nuclear\nreaction network calculations simulating rapid neutron-capture process\n(r-process) nucleosynthesis. Due to the large number of fissioning nuclei\ncontributing to the r-process, a description of particle-induced fission\nreactions is computationally challenging. In this work, we use theoretical\ncalculations based on the INCL+ABLA models to train neural networks (NN). The\nresults for the prediction of proton-induced spallation reactions, in\nparticular fission, utilizing a large variety of NN models across the\nhyper-parameter space are presented, which are relevant for r-process\ncalculations.",
        "For any number field $K$ with $D_K=|\\mathrm{Disc}(K)|$ and any integer $\\ell\n\\geq 2$, we improve over the commonly cited trivial bound\n$|\\mathrm{Cl}_K[\\ell]| \\leq |\\mathrm{Cl}_K| \\ll_{[K:\\mathbb{Q}],\\varepsilon}\nD_K^{1\/2+\\varepsilon}$ on the $\\ell$-torsion subgroup of the class group of $K$\nby showing that $|\\mathrm{Cl}_K[\\ell]| = o_{[K:\\mathbb{Q}],\\ell}(D_K^{1\/2})$.\nIn fact, we obtain an explicit log-power saving. This is the first general\nunconditional saving over the trivial bound that holds for all $K$ and all\n$\\ell$.",
        "In analogy to high-energy nuclear scattering experiments, we study a\nreal-time scattering process between a propagating state and a dense target in\n$1+1$-d massive QED. In our setup, we identify three distinct regimes that\nqualitatively characterize the evolution: for a dilute medium, the incoming\nprobe state evolves nearly ballistically; in an intermediate setting, it\ntraverses the matter, locally exciting it; and for dense targets, one\napproaches a black-disk limit, where the matter acts as a strong wall\npotential. We find evidence that the probe's energy loss rate scales linearly\nwith the path length in the medium, and we study how the entanglement entropy\nreveals the mixing between the probe and medium states. With the goal of one\nday replicating high-energy nuclear experiments in quantum devices, we briefly\ndiscuss how the current tensor network-based simulations can be translated to a\nquantum simulator.",
        "The globular clusters (GCs) system of the Milky Way (MW) comprises a mixture\nof both in situ and accreted clusters. Tracing the origin of GCs provides\ninvaluable insights into the formation history of the MW. However, reconciling\ndiverse strands of evidence is often challenging: a notable example is NGC 288,\nwhere despite significant efforts in the literature, the available\nchrono-chemodynamical data have yet to provide a definitive conclusion\nregarding its origin. On one side, all post-Gaia dynamical studies indicate an\naccreted origin for NGC 288 from in the Gaia-Sausage-Enceladus (GSE) dwarf\ngalaxy. On the other, NGC 288 has been found to be 2.5 Gyr older than other GSE\nGCs at the same metallicity, this suggesting a different, possibly in situ\norigin. In this work, we address the unresolved question on the origin of NGC\n288 by analyzing its chrono-chemical properties in an unprecedentedly\nhomogeneous framework. First, we compare the location of NGC 288 in the\nage-metallicity plane with that of other two in situ GCs at similar\nmetallicity, namely NGC 6218 and NGC 6362. The age estimates obtained within\nthe homogeneous framework of the CARMA collaboration show that the three\nclusters are coeval, this reinforcing the contrast with the dynamical\ninterpretation. Then, we compare the abundances with the sample of in situ and\naccreted clusters at similar metallicity presented in Ceccarelli et al. 2024,\nfinding again consistency with the chemistry of in situ systems. To reconcile\nthese results with its orbital properties, we propose a scenario where NGC 288\nformed in the proto-disc of the MW, and then was dynamically heated by the\ninteraction with the GSE merger, a fate similar to that of proto-disc stars\nexperiencing the so-called Splash event. NGC 288 therefore demonstrates the\nimportance of a homogeneous chrono-chemodynamical information in the\ninterpretation of the origin of MW GCs.",
        "Computing the similarity between two DNA sequences is of vital importance in\nbioscience. However, traditional computational methods can be\nresource-intensive due to the enormous sequence length encountered in practice.\nRecently, applied quantum algorithms have been anticipated to provide potential\nadvantages over classical approaches. In this paper, we propose a\npermutation-invariant variational quantum kernel method specifically designed\nfor DNA comparison. To represent the four nucleotide bases in DNA sequences\nwith quantum states, we introduce a novel, theoretically motivated encoding\nscheme: the four distinct bases are encoded using the states of symmetric,\ninformationally complete, positive operator-valued measures (SIC-POVMs). This\nencoding ensures mutual equality: each pair of symbols is equidistant on the\nBloch sphere. Also, since permutation invariance is inherent to common DNA\nsimilarity measures such as Levenshtein distance, we realize it by using a\nspecially designed parameterized quantum layer. We show that our novel encoding\nmethod and parameterized layers used in the quantum kernel model can\neffectively capture the symmetric characteristics of the pairwise DNA sequence\ncomparison task. We validate our model through numerical experiments, which\nyield promising results on length-$8$ DNA sequences.",
        "Consider a sample of size $N$ from a population governed by a hierarchical\nspecies sampling model. We study the large $N$ asymptotic behavior of the\nnumber ${\\bf K}_N$ of clusters and the number ${\\bf M}_{r,N}$ of clusters with\nfrequency $r$ in the sample. In particular, we show almost sure and $L^p$\nconvergence for ${\\bf M}_{r,N}$, obtain Gaussian fluctuation theorems for ${\\bf\nK}_N$, and establish large deviation principles for both ${\\bf K}_N$ and ${\\bf\nM}_{r,N}$. Our approach relies on a random sample size representation of the\nnumber of clusters through the corresponding non-hierarchical species sampling\nmodel.",
        "Hidden Markov models are widely used for modeling sequential data but\ntypically have limited applicability in observational causal inference due to\ntheir strong conditional independence assumptions. I introduce\nfeedback-augmented non-homogeneous hidden Markov model (FAN-HMM), which\nincorporate time-varying covariates and feedback mechanisms from past\nobservations to latent states and future responses. Integrating these models\nwith the structural causal model framework allows flexible causal inference in\nlongitudinal data with time-varying unobserved heterogeneity and multiple\ncausal pathways. I show how, in a common case of categorical response\nvariables, long-term causal effects can be estimated efficiently without the\nneed for simulating counterfactual trajectories. Using simulation experiments,\nI study the performance of FAN-HMM under the common misspecification of the\nnumber of latent states, and finally apply the proposed approach to estimate\nthe effect of the 2013 parental leave reform on fathers' paternal leave uptake\nin Finnish workplaces.",
        "Polarization of starlight and thermal dust emission due to aligned\nnon-spherical grains helps us to trace magnetic field (B-field) morphology in\nmolecular clouds and to study grain alignment mechanisms. In this work, we\nstudy grain alignment and disruption mechanisms in a filamentary infrared dark\ncloud G34.43+0.24 using thermal dust polarization observations from JCMT\/POL-2\nat 850 $\\mu\\text{m}$. We study in three sub-regions as North harboring MM3\ncore, Center harboring MM1 and MM2 cores and South having no core. We find the\ndecrease in polarization fraction P with increasing total intensity and gas\ncolumn density, known as polarization hole. To disentangle the effect of\nmagnetic field tangling on the polarization hole, we estimate the polarization\nangle dispersion function. We find depolarizations in North and Center regions\nare due to decrease in net alignment efficiency of grains but in South region,\neffect of magnetic field tangling is significant to cause depolarization. To\ntest whether RAdiative Torque (RAT) mechanism can reproduce the observational\ndata, we calculate minimum alignment and disruption sizes of grains using RAT\ntheory and our study finds that RAT alignment mechanism can explain the\ndepolarizations in North and Center regions where B-field tangling effect is\nless important, except for core regions. We find hints of RAdiative Torque\nDisruption (RAT-D) in the core regions of MM3 in North, MM1 and MM2 in Center.\nWe also find that the high P value of around 8-20% in the outer regions of the\nfilament can be explained potentially by magnetically enhanced RAT alignment\nmechanism.",
        "Line-of-sight distortions of the cosmic microwave background (CMB), including\ngravitational lensing, cosmic birefringence, and patchy screening, encode\ncrucial cosmological information. While quadratic estimators (QE) have been\nexcellent tools for extracting these signals, they become suboptimal for\ncurrent- and next-generation CMB surveys, failing to maximize signal-to-noise\nand suffering from bias contamination that standard bias-hardening techniques\ncannot mitigate. We present a joint maximum a posteriori framework that\nsimultaneously reconstructs multiple distortion fields while explicitly\naccounting for their mutual contamination. For cosmic birefringence searches,\nour method achieves up to 2.5 improvement in reconstruction noise compared to\nthe QE, while significantly reducing CMB lensing-induced biases up to a factor\nof 5. These gains in lensing biases manifest not only in deep polarization\nsurveys like CMB-S4 and SPT-3G, but also in higher-noise experiments like\nSimons Observatory, where our method reduces lensing-induced biases by a factor\nof two thanks to the power of delensing. Our code provides a first step to\nrobust analyses of CMB secondary anisotropies, their cross-correlations with\nlarge-scale structure, and ultimately enabling more sensitive searches for\nprimordial $B$-modes.",
        "We investigate the nearsightedness property in the linear tight binding model\nat zero Fermi-temperature. We focus on the decay property of the density matrix\nfor materials with indirect band gaps. By representing the density matrix in\nreciprocal space, we establish a qualitatively sharp estimate for the\nexponential decay rate in homogeneous systems. An extending result under\nperturbations is also derived. This work refines the estimates presented in\n(Ortner, Thomas & Chen 2020), particularly for systems with small band gaps.",
        "Particle-wall interactions play a crucially important role in various\napplications such as microfluidic devices for cell sorting, particle\nseparation, entire class of hydrodynamic filtration and its derivatives, etc.\nYet, accurate implementation of interactions between wall and finite-size\nparticle is not trivial when working with the currently available particle\ntracking algorithms\/packages as they typically work with point-wise particles.\nHerein, we report a particle tracking algorithm that takes into account\ninteractions between particles of finite size and solid objects existing inside\ncomputational domain. A particle is modeled as a set of circumferential points\non its perimeter. While fluid-particle interactions are captured during the\ntrack of particle center, interactions between particle and nearby solid\nobjects are modeled explicitly by examining circumferential points and applying\na reflection scheme as needed to ensure impenetrability of solid objects. We\nalso report a modified variant of auxiliary structured grid method to locate\nhosting cells, which in conjunction with a boundary condition scheme enables\nthe capture of interactions between particle and solid objects. As a\nproof-of-concept, we numerically and experimentally study the motion of\nparticles within a microfluidic deterministic lateral displacement device. The\nmodeling results successfully demonstrate the zig-zag and bumping displacement\nmodes observed in our experiments. We also study a microfluidic device with\npinched flow numerically and validate our results against experimental data\nfrom the literature. By demonstrating an almost 8x speedup on a system with 8\nPerformance threads, our investigations suggest that the particle tracking\nalgorithm and its implementation code can benefit from parallel processing on\nmulti-thread systems by using the OpenMP application programming interface.",
        "Purpose: Iliac wing fractures due to lap belt loading have been observed in\nlaboratory settings for 50 years and recent data suggest they are also\noccurring in the field. Automated driving systems (ADS) and other occupant\ncompartment advancements are expected to offer enhanced flexibility in seating\norientation, which could place a greater reliance on the seatbelt to restrain\noccupants. Such changes may increase seatbelt loads and create new challenges\nin successfully restraining occupants and mitigating injury to areas such as\nthe pelvis. Injury criteria exist for component-level male iliac wing fractures\nresulting from frontal lap belt loading, but not for females. Methods: This\nstudy explored female iliac wing fracture tolerance in the same loading\nenvironment as a previous study that explored the fracture tolerance of\nisolated male iliac wings. Male and female fracture data were combined to\nevaluate the effect of sex. Injury risk functions were created by fitting\nWeibull survival models to data that integrated censored and exact failure\nobservations. Results: Twenty female iliac wings were tested; fourteen of them\nsustained fracture with known failure forces (exact), but the remaining six\nwings either (1) did not fracture, or (2) fractured after an event that changed\nthe boundary conditions (right censored). The fracture tolerance of the tested\nspecimens ranged widely (1134 - 8759 N) and averaged 4240 N (SD 2516 N).\nConclusion: Female data and combined male-female data were analyzed. Age was\nthe only covariate investigated in this study that had a statistically\nsignificant effect and improved the predictive performance of the models.",
        "Spatial misalignment problems arise from both data aggregation and attempts\nto align misaligned data, leading to information loss. We propose a Bayesian\ndisaggregation framework that links misaligned data to a continuous domain\nmodel using an iteratively linearised integration method via integrated nested\nLaplace approximation (INLA). The framework supports point pattern and\naggregated count models under four covariate field scenarios: \\textit{Raster at\nFull Resolution (RastFull), Raster Aggregation (RastAgg), Polygon Aggregation\n(PolyAgg), and Point Values (PointVal)}. The first three involve aggregation,\nwhile the latter two have incomplete fields. For PolyAgg and PointVal, we\nestimate the full covariate field using \\textit{Value Plugin, Joint\nUncertainty, and Uncertainty Plugin} methods, with the latter two accounting\nfor uncertainty propagation. These methods demonstrate superior performance,\nand remain more robust even under model misspecification (i.e.\\ modelling a\nnonlinear field as linear).\n  In landslide studies, landslide occurrences are often aggregated into counts\nbased on slope units, reducing spatial detail. The results indicate that point\npattern observations and full-resolution covariate fields should be\nprioritized. For incomplete fields, methods incorporating uncertainty\npropagation are preferred. This framework supports landslide susceptibility and\nother spatial mapping, integrating seamlessly with INLA-extension packages.",
        "Exploring various unexpected new quantum states and their corresponding\nextraordinary physics in low dimensional quantum materials, and investing them\ninto application fields, is a primary concern of condensed matter physics.\nPreviously, we performed joint experiments and theoretical calculations to\ninvestigate a superatomic crystal of Au6Te12Se8 with low symmetry, which stacks\nthrough non-covalent inter-cube quasi-bonds. Au6Te12Se8 exhibits a triple-cube\ncharge density wave and spatially polarized metallic states interweaving at 9\nK. In addition, it undergoes a BKT phase transition at 2.8 K to form a\nquasi-two-dimensional superconducting state. The subsequent high-pressure\nexperimental results indicate that the two quantum states above compete with\nsuperconductivity. Here, we experimentally revealed competition and coexistence\namong superconductivity, triple-cube charge density waves, and polarized\nmetallic states during further temperature-decreasing process, as examined\nusing ultra-low temperature scanning tunneling microscopy\/spectroscopy,\ntransport measurement, and Raman spectra. An extraordinary inversion symmetry\nbroken emerged inside the triple-cube-period of the original CDW at 300 mK,\nleading to the polarized metallic states transforming into parallel\npolarization states. The evidence of spontaneous polarization coexisting with\nsuperconductivity in real space is extremely rare and has been revealed by STM.\nIn addition, transport and Raman measurements indicate that there are multiple\nphase transition temperatures from 80K to that below the superconducting\ntransition temperature of Au6Te12Se8, which may also contain more unknown\nexotic quantum states, providing a platform for further exploration of\nintriguing physical properties.",
        "The parallel solver of the general synthetic iterative scheme (GSIS), as\nrecently developed by Zhang \\textit{et. al.} in Comput. Fluids 281 (2024)\n106374, is an efficient method to find the solution of the Boltzmann equation\ndeterministically. However, it consumes a significant computational memory due\nto the discretization of molecular velocity space in hypersonic flows. In this\npaper, we address this issue by introducing the adaptive GSIS, where the\nBoltzmann equation is applied only in rarefied regions when the local Knudsen\nnumber exceeds a reference value, $\\text{Kn}{ref}$. In contrast, the\nNavier-Stokes equations, with and without the high-order corrections to the\nconstitutive relations, are applied in the continuum and rarefied regimes,\nrespectively. Numerical results indicate that setting $\\text{Kn}{ref}=0.01$\nyields acceptable outcomes. With the adaptive GSIS, the computational memory\nand time can be significantly reduced in near-continuum flows, e.g. 24 and 7\ntimes, respectively. in the simulation of rarefied gas flow passing the\nInternational Space Station.",
        "The expressive power of neural networks in modelling non-trivial\ndistributions can in principle be exploited to bypass topological freezing and\ncritical slowing down in simulations of lattice field theories. Some popular\napproaches are unable to sample correctly non-trivial topology, which may lead\nto some classes of configurations not being generated. In this contribution, we\npresent a novel generative method inspired by a model previously introduced in\nthe ML community (GFlowNets). We demonstrate its efficiency at exploring\nergodically configuration manifolds with non-trivial topology through\napplications such as triple ring models and two-dimensional lattice scalar\nfield theory.",
        "We state and prove a stratification result that allows us to classify the\ntensor ideal localizing subcategories for the stable module category\n$\\text{Stab}(\\mathcal{C}_{(\\mathfrak{g}, \\mathfrak{g}_{\\bar 0})})$ of Lie\nsuperalgbera representations which are semisimple as representations of\n$\\mathfrak{g}_{\\bar 0}$ under the hypotheses that $\\mathfrak{g}$ is a classical\nLie superalgebra with a splitting detecting subalgebra $\\mathfrak{z} \\leq\n\\mathfrak{g}$, as well as a natural hypothesis on realization of supports. This\nextends the work of the author and Nakano where a similar classification was\nobtained for the stable category of modules over a detecting subalgebra\nemploying stratification in the sense of Benson, Iyengar, and Krause. Our new\nresult involves making use of a more general stratification framework in weakly\nNoetherian contexts developed by Barthel, Heard, and Sanders using the\nBalmer-Favi notion of support for big objects in tensor triangulated\ncategories, as well as the recently developed homological stratification of\nBarthel, Heard, Sanders, and Zou in using the homological spectrum.",
        "In the recent years, high energy physics discoveries have been driven by the\nincreasing of luminosity and\/or detector granularity. This evolution gives\naccess to bigger statistics and data samples, but can make it hard to process\nresults with current methods and algorithms. Graph convolution networks, have\nbeen shown to be powerful tools to address these challenges. We present our\ngraph convolution framework for particle identification and energy regression\nin high granularity calorimeters. In particular, we introduce our algorithm for\noptimised graph construction in resource constrained environments. We also\nintroduce our implementation of graph convolution and pooling layers. We\nobserve satisfying accuracies, and discuss possible application to other high\ngranularity particle detector challenges.",
        "Modern Imaging Atmospheric Cherenkov Telescopes (IACTs) generate a huge\namount of data that must be classified automatically, ideally in real time.\nCurrently, machine learning-based solutions are increasingly being used to\nsolve classification problems. However, these classifiers require proper\ntraining data sets to work correctly. The problem with training neural networks\non real IACT data is that these data need to be pre-labeled, whereas such\nlabeling is difficult and its results are estimates. In addition, the\ndistribution of incoming events is highly imbalanced. Firstly, there is an\nimbalance in the types of events, since the number of detected gamma quanta is\nsignificantly less than the number of protons. Secondly, the energy\ndistribution of particles of the same type is also imbalanced, since\nhigh-energy particles are extremely rare. This imbalance results in poorly\ntrained classifiers that, once trained, do not handle rare events correctly.\nUsing only conventional Monte Carlo event simulation methods to solve this\nproblem is possible, but extremely resource-intensive and time-consuming. To\naddress this issue, we propose to perform data augmentation with artificially\ngenerated events of the desired type and energy using conditional generative\nadversarial networks (cGANs), distinguishing classes by energy values. In the\npaper, we describe a simple algorithm for generating balanced data sets using\ncGANs. Thus, the proposed neural network model produces both imbalanced data\nsets for physical analysis as well as balanced data sets suitable for training\nother neural networks.",
        "We investigate the transition between elastic and viscous regimes for\ntime-reversal broken Weyl semimetals. In these materials, Hall transport occurs\nthrough two parallel channels: the Fermi sea and the Fermi surface. The Fermi\nsea part remains unaffected by electron-electron scattering, whereas the Fermi\nsurface is influenced by it. We model the disorder by dilute impenetrable\nspherical impurities. We analyze the flow of an electronic fluid with a finite\nodd viscosity in the presence of such disorder and compute the conductivity\ntensor. We find that in the generic case of finite intrinsic conductivity, the\nHall angle in the viscous regime is parametrically suppressed compared to the\nelastic regime. In the special case where the intrinsic conductivity vanishes,\nthe ratio between the transverse and the longitudinal resistivities matches the\nratio between the odd and even components of the viscosity tensor.",
        "We study the bifurcation phenomena between spherical and axisymmetric bosonic\nstars. By numerically solving for the zero-modes of spherical bosonic stars\nunder specific axially symmetric perturbations, we discover that excited state\nspherical bosonic stars bifurcate into two types of axisymmetric bosonic stars\nunder $\\ell=2$ perturbations, with matter distributions resembling chains and\nrings, respectively. Meanwhile, $\\ell=4$ axisymmetric perturbations lead\nspherical scalar bosonic stars to bifurcate into a new type of axisymmetric\nbosonic stars, exhibiting a mixed chain-like and ring-like matter distribution,\nwhich we refer to as gyroscope-like. Additionally, for the first time, we have\nconstructed chains of scalar bosonic stars with 7 constituents and their\ncorresponding ring-like scalar bosonic stars. Our results provide an\nexplanation for the bifurcations in bosonic stars from the perspective of\nperturbations, and by analyzing physical quantities such as quadrupoles and\nenergy densities we systematically discuss the impact of axisymmetric\nperturbations on spherical bosonic stars.",
        "We prove local-in-time a-priori estimates in $H^{-1}(\\mathbb{R})$ for a\nfamily of generalized Korteweg--de Vries equations. This is the first estimate\nfor any non-integrable perturbation of the KdV equation that matches the\nregularity of the sharp well-posedness theory for KdV. In particular, we show\nthat our analysis applies to models for long waves in a shallow channel of\nwater with an uneven bottom.\n  The proof of our main result is based upon a bootstrap argument for the\nrenormalized perturbation determinant coupled with a local smoothing norm.",
        "Sharing diverse genomic and other biomedical datasets is critical to advance\nscientific discoveries and their equitable translation to improve human health.\nHowever, data sharing remains challenging in the context of legacy datasets,\nevolving policies, multi-institutional consortium science, and international\nstakeholders. The NIH-funded Polygenic Risk Methods in Diverse Populations\n(PRIMED) Consortium was established to improve the performance of polygenic\nrisk estimates for a broad range of health and disease outcomes with global\nimpacts. Improving polygenic risk score performance across genetically diverse\npopulations requires access to large, diverse cohorts. We report on the design\nand implementation of data sharing policies and procedures developed in PRIMED\nto aggregate and analyze data from multiple, heterogeneous sources while\nadhering to existing data sharing policies for each integrated dataset. We\ndescribe two primary data sharing mechanisms: coordinated dbGaP applications\nand a Consortium Data Sharing Agreement, as well as provide alternatives when\nindividual-level data cannot be shared within the Consortium (e.g., federated\nanalyses). We also describe technical implementation of Consortium data sharing\nin the NHGRI Analysis Visualization and Informatics Lab-space (AnVIL) cloud\nplatform, to share derived individual-level data, genomic summary results, and\nmethods workflows with appropriate permissions. As a Consortium making\nsecondary use of pre-existing data sources, we also discuss challenges and\npropose solutions for release of individual- and summary-level data products to\nthe broader scientific community. We make recommendations for ongoing and\nfuture policymaking with the goal of informing future consortia and other\nresearch activities.",
        "Integrated photonics has revolutionized optical communication, sensing, and\ncomputation, offering miniaturized and lightweight solutions for spacecraft\nwith limited size and payload. Novel chip-scale instruments based on\nultralow-loss integrated photonic platforms, including lasers, frequency combs\nand atomic traps, have been developed for space applications. Therefore,\nquantifying the space compatibility of ultralow-loss photonic integrated\ncircuits (PICs), particularly their radiation resistance, is critical. This\nstudy experimentally evaluates the radiation resistance of ultralow-loss\nSi$_3$N$_4$, 4H-SiC, and LiNbO$_3$ PICs under intense $\\gamma$-ray and\nhigh-energy proton irradiation. Results show that proton irradiation with $1.1\n\\times 10^{10}$ $\\mathrm{p\/cm^2}$ total flux does not significantly increase\noptical loss or alter the refractive index of these PICs, while $\\gamma$-ray\nirradiation with 1.2 Mrad accumulated dose only marginally increases their\noptical loss. These findings provide preliminary evidence of the excellent\nspace compatibility of ultralow-loss Si$_3$N$_4$, 4H-SiC, and LiNbO$_3$ PICs,\nhighlighting their potential for compact and lightweight space systems.",
        "In networked dynamical systems, inferring governing parameters is crucial for\npredicting nodal dynamics, such as gene expression levels, species abundance,\nor population density. While many parameter estimation techniques rely on\ntime-series data, particularly systems that converge over extreme time ranges,\nonly noisy steady-state data is available, requiring a new approach to infer\ndynamical parameters from noisy observations of steady states. However, the\ntraditional optimization process is computationally demanding, requiring\nrepeated simulation of coupled ordinary differential equations (ODEs). To\novercome these limitations, we introduce a surrogate objective function that\nleverages decoupled equations to compute steady states, significantly reducing\ncomputational complexity. Furthermore, by optimizing the surrogate objective\nfunction, we obtain steady states that more accurately approximate the ground\ntruth than noisy observations and predict future equilibria when topology\nchanges. We empirically demonstrate the effectiveness of the proposed method\nacross ecological, gene regulatory, and epidemic networks. Our approach\nprovides an efficient and effective way to estimate parameters from\nsteady-state data and has the potential to improve predictions in networked\ndynamical systems.",
        "The global analytic hypoellipticity is proved for a class of second order\npartial differential equations with non-negative characteristic form globally\ndefined on the torus. The class considered in this work generalizes at some\ndegree the class of sum of squares considered by Bove-Chinni and also by\nCordaro-Himonas.",
        "The Observatorio Astrof\\'isico de Javalambre (OAJ) is a Spanish astronomical\nICTS (Unique Scientific and Technical Infrastructures) located in the Sierra de\nJavalambre in Teruel (Spain). It has been particularly conceived for carrying\nout large-sky multi-filter surveys. As an ICTS, the OAJ offers Open Time to the\nastronomical community, offering more than 25% through Legacy Surveys, Regular\nPrograms (RP) and Director discretionary time (DDT). Regarding the RP, a new\ncall for proposals is made public each semester accepting only proposals under\nthe modality of Target of Opportunity (ToO).\n  This contribution summarizes how ToOs are managed at OAJ presenting the\ndifferent applications designed and implemented at the observatory to deal with\nthem: the Proposal Preparation portal (to request observing time), the Phase2\nObserving tool and the submitphase2 web service (to trigger the ToOs), the TAC\nTracking portal (for telescope operators to support the observations) and the\nTACData portal (to publish and offer the images and their data products).",
        "In the 1970s, structural Ramsey theory emerged as a new branch of\ncombinatorics. This development came with the isolation of the concepts of the\n$\\mathbf{A}$-Ramsey property and Ramsey class. Following the influential\nNe\\v{s}et\\v{r}il-R\\\"{o}dl theorem, several Ramsey classes have been identified.\nIn the 1980s Ne\\v{s}et\\v{r}il, inspired by a seminar of Lachlan, discovered a\ncrucial connection between Ramsey classes and Fra\\\"{\\i}ss\\'{e} classes and, in\nhis 1989 paper, connected the classification programme of homogeneous\nstructures to structural Ramsey theory. In 2005, Kechris, Pestov, and\nTodor\\v{c}evi\\'{c} revitalized the field by connecting Ramsey classes to\ntopological dynamics. This breakthrough motivated Ne\\v{s}et\\v{r}il to propose a\nprogram for classifying Ramsey classes. We review the progress made on this\nprogram in the past two decades, list open problems, and discuss recent\nextensions to new areas, namely the extension property for partial\nautomorphisms (EPPA), and big Ramsey structures."
      ]
    }
  },
  {
    "id":2411.08063,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials",
    "start_abstract":"The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "A deep-learning approach to realizing functionality in nanoelectronic devices"
      ],
      "abstract":[
        "Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "On conductor submonoids of factorial monoids",
        "Depth of powers of edge ideals of edge-weighted integrally closed cycles",
        "Small Models Struggle to Learn from Strong Reasoners",
        "Spatial-temporal models for forest inventory data",
        "Supervised Manifold Learning for Functional Data",
        "DreamFLEX: Learning Fault-Aware Quadrupedal Locomotion Controller for\n  Anomaly Situation in Rough Terrains",
        "Encrypted Vector Similarity Computations Using Partially Homomorphic\n  Encryption: Applications and Performance Analysis",
        "FinTSB: A Comprehensive and Practical Benchmark for Financial Time\n  Series Forecasting",
        "Integrative Learning of Intensity Fluctuations of Quantum Dots under\n  Excitation via a Tailored Mixture Hidden Markov Model",
        "Contracting Strategies for Electrolyzers to Secure Grid Connection: The\n  Dutch Case",
        "Naked Eye Three-dimensional Display System Based on Time-multiplexed\n  Technology",
        "EHCTNet: Enhanced Hybrid of CNN and Transformer Network for Remote\n  Sensing Image Change Detection",
        "Unfitted boundary algebraic equation method based on difference\n  potentials and lattice Green's function in 3D",
        "PIXELS: Progressive Image Xemplar-based Editing with Latent Surgery",
        "Ultrafast pulsed laser evaluation of Single Event Transients in\n  opto-couplers",
        "ScamFerret: Detecting Scam Websites Autonomously with Large Language\n  Models",
        "A Family of Semi-norms in $C^*$-algebras",
        "Domain Adaptation for Japanese Sentence Embeddings with Contrastive\n  Learning based on Synthetic Sentence Generation",
        "Graph Generative Pre-trained Transformer",
        "\"You don't need a university degree to comprehend data protection this\n  way\": LLM-Powered Interactive Privacy Policy Assessment",
        "City Models: Past, Present and Future Prospects",
        "CAT: Content-Adaptive Image Tokenization",
        "Supervised Quadratic Feature Analysis: An Information Geometry Approach\n  to Dimensionality Reduction",
        "Retrieval-Augmented Perception: High-Resolution Image Perception Meets\n  Visual RAG",
        "A Novel Phenomenological Model of Equalization-enhanced Phase Noise",
        "UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion",
        "Digital Twin Calibration with Model-Based Reinforcement Learning",
        "Teacher-student training improves accuracy and efficiency of machine\n  learning inter-atomic potentials",
        "CoT-Valve: Length-Compressible Chain-of-Thought Tuning"
      ],
      "abstract":[
        "We give affirmative answers to Conjecture 4.16 in [1] and to Conjecture 2.3\nin [3].",
        "This paper gives some exact formulas for the depth of powers of the edge\nideal of an edge-weighted integrally closed cycle.",
        "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
        "The USDA Forest Inventory and Analysis (FIA) program conducts a national\nforest inventory for the United States through a network of permanent field\nplots. FIA produces estimates of area averages\/totals for plot-measured forest\nvariables through design-based inference, assuming a fixed population and a\nprobability sample of field plot locations. The fixed-population assumption and\ncharacteristics of the FIA sampling scheme make it difficult to estimate change\nin forest variables over time using design-based inference. We propose\nspatial-temporal models based on Gaussian processes as a flexible tool for\nforest inventory data, capable of inferring forest variables and change thereof\nover arbitrary spatial and temporal domains. It is shown to be beneficial for\nthe covariance function governing the latent Gaussian process to account for\nvariation at multiple scales, separating spatially local variation from\necosystem-scale variation. We demonstrate a model for forest biomass density,\ninferring 20 years of biomass change within two US National Forests.",
        "Classification is a core topic in functional data analysis. A large number of\nfunctional classifiers have been proposed in the literature, most of which are\nbased on functional principal component analysis or functional regression. In\ncontrast, we investigate this topic from the perspective of manifold learning.\nIt is assumed that functional data lie on an unknown low-dimensional manifold,\nand we expect that better classifiers can be built upon the manifold structure.\nTo this end, we propose a novel proximity measure that takes the label\ninformation into account to learn the low-dimensional representations, also\nknown as the supervised manifold learning outcomes. When the outcomes are\ncoupled with multivariate classifiers, the procedure induces a family of new\nfunctional classifiers. In theory, we show that our functional classifier\ninduced by the $k$-NN classifier is asymptotically optimal. In practice, we\nshow that our method, coupled with several classical multivariate classifiers,\nachieves outstanding classification performance compared to existing functional\nclassifiers in both synthetic and real data examples.",
        "Recent advances in quadrupedal robots have demonstrated impressive agility\nand the ability to traverse diverse terrains. However, hardware issues, such as\nmotor overheating or joint locking, may occur during long-distance walking or\ntraversing through rough terrains leading to locomotion failures. Although\nseveral studies have proposed fault-tolerant control methods for quadrupedal\nrobots, there are still challenges in traversing unstructured terrains. In this\npaper, we propose DreamFLEX, a robust fault-tolerant locomotion controller that\nenables a quadrupedal robot to traverse complex environments even under joint\nfailure conditions. DreamFLEX integrates an explicit failure estimation and\nmodulation network that jointly estimates the robot's joint fault vector and\nutilizes this information to adapt the locomotion pattern to faulty conditions\nin real-time, enabling quadrupedal robots to maintain stability and performance\nin rough terrains. Experimental results demonstrate that DreamFLEX outperforms\nexisting methods in both simulation and real-world scenarios, effectively\nmanaging hardware failures while maintaining robust locomotion performance.",
        "This paper explores the use of partially homomorphic encryption (PHE) for\nencrypted vector similarity search, with a focus on facial recognition and\nbroader applications like reverse image search, recommendation engines, and\nlarge language models (LLMs). While fully homomorphic encryption (FHE) exists,\nwe demonstrate that encrypted cosine similarity can be computed using PHE,\noffering a more practical alternative. Since PHE does not directly support\ncosine similarity, we propose a method that normalizes vectors in advance,\nenabling dot product calculations as a proxy. We also apply min-max\nnormalization to handle negative dimension values.\n  Experiments on the Labeled Faces in the Wild (LFW) dataset use DeepFace's\nFaceNet128d, FaceNet512d, and VGG-Face (4096d) models in a two-tower setup.\nPre-encrypted embeddings are stored in one tower, while an edge device captures\nimages, computes embeddings, and performs encrypted-plaintext dot products via\nadditively homomorphic encryption. We implement this with LightPHE, evaluating\nPaillier, Damgard-Jurik, and Okamoto-Uchiyama schemes, excluding others due to\nperformance or decryption complexity. Tests at 80-bit and 112-bit security\n(NIST-secure until 2030) compare PHE against FHE (via TenSEAL), analyzing\nencryption, decryption, operation time, cosine similarity loss, key\/ciphertext\nsizes.\n  Results show PHE is less computationally intensive, faster, and produces\nsmaller ciphertexts\/keys, making it well-suited for memory-constrained\nenvironments and real-world privacy-preserving encrypted similarity search.",
        "Financial time series (FinTS) record the behavior of human-brain-augmented\ndecision-making, capturing valuable historical information that can be\nleveraged for profitable investment strategies. Not surprisingly, this area has\nattracted considerable attention from researchers, who have proposed a wide\nrange of methods based on various backbones. However, the evaluation of the\narea often exhibits three systemic limitations: 1. Failure to account for the\nfull spectrum of stock movement patterns observed in dynamic financial markets.\n(Diversity Gap), 2. The absence of unified assessment protocols undermines the\nvalidity of cross-study performance comparisons. (Standardization Deficit), and\n3. Neglect of critical market structure factors, resulting in inflated\nperformance metrics that lack practical applicability. (Real-World Mismatch).\nAddressing these limitations, we propose FinTSB, a comprehensive and practical\nbenchmark for financial time series forecasting (FinTSF). To increase the\nvariety, we categorize movement patterns into four specific parts, tokenize and\npre-process the data, and assess the data quality based on some sequence\ncharacteristics. To eliminate biases due to different evaluation settings, we\nstandardize the metrics across three dimensions and build a user-friendly,\nlightweight pipeline incorporating methods from various backbones. To\naccurately simulate real-world trading scenarios and facilitate practical\nimplementation, we extensively model various regulatory constraints, including\ntransaction fees, among others. Finally, we conduct extensive experiments on\nFinTSB, highlighting key insights to guide model selection under varying market\nconditions. Overall, FinTSB provides researchers with a novel and comprehensive\nplatform for improving and evaluating FinTSF methods. The code is available at\nhttps:\/\/github.com\/TongjiFinLab\/FinTSBenchmark.",
        "Semiconductor nano-crystals, known as quantum dots (QDs), have garnered\nsignificant interest in various scientific fields due to their unique\nfluorescence properties. One captivating characteristic of QDs is their ability\nto emit photons under continuous excitation. The intensity of photon emission\nfluctuates during the excitation, and such a fluctuation pattern can vary\nacross different dots even under the same experimental conditions. What adding\nto the complication is that the processed intensity series are non-Gaussian and\ntruncated due to necessary thresholding and normalization. As such,\nconventional approaches in the chemistry literature, typified by single-dot\nanalysis of raw intensity data with Gaussian hidden Markov models (HMM), cannot\nmeet the many analytical challenges and may fail to capture any novel yet rare\nfluctuation patterns among QDs. Collaborating with scientists in the chemistry\nfield, we have developed an integrative learning approach to simultaneously\nanalyzing intensity series of multiple QDs. Our approach still inherits the HMM\nas the skeleton to model the intensity fluctuations of each dot, and based on\nthe data structure and the hypothesized collective behaviors of the QDs, our\napproach asserts that (i) under each hidden state, the normalized intensity\nfollows a 0\/1 inflated Beta distribution, (ii) the state distributions are\nshared across all the QDs, and (iii) the patterns of transitions can vary\nacross QDs. These unique features allow for a precise characterization of the\nintensity fluctuation patterns and facilitate the clustering of the QDs. With\nexperimental data collected on 128 QDs, our methods reveal several QD clusters\ncharacterized by unique transition patterns across three intensity states. The\nresults provide deeper insight into QD behaviors and their design\/application\npotentials.",
        "In response to increasing grid congestion in the Netherlands, non-firm\nconnection and transport agreements (CTAs) and capacity restriction contracts\n(CRCs) have been introduced, allowing consumer curtailment in exchange for grid\ntariff discounts or per-MW compensations. This study examines the interaction\nbetween an electrolyzer project, facing sizing and contracting decisions, and a\nnetwork operator, responsible for contract activations and determining grid\nconnection capacity, under the new Dutch regulations. The interaction is\nmodeled using two bilevel optimization problems with alternating\nleader-follower roles. Results highlight a trade-off between CRC income and\nnon-firm CTA tariff discounts, showing that voluntary congestion management by\nthe network operator increases electrolyzer profitability at CRC prices below\n10 euro per MW but reduces it at higher prices. Furthermore, the network\noperator benefits more from reacting to the electrolyzer owner's CTA decisions\nthan from leading the interaction at CRC prices above 10 euro per MW. Ignoring\nthe other party's optimization problem overestimates profits for both the\nnetwork operator and the electrolyzer owner, emphasizing the importance of\ncoordinated decision-making.",
        "Our group is developing a multi-user eye-tracked 3D display, an evolution of\nthe single-user eye-tracked 3D display that we have already successfully\ndeveloped. This display utilizes a slanted lenticular setup, where multiple\nperspective views are shown across the viewing field. Due to the constraints of\nthe lenticular lens parameters, identical views are repeated across the field,\nlimiting eye tracking to a single user. However, this limitation can be\naddressed using spatio-temporal multiplexing, where view zone groups are\npresented sequentially with a high frame rate liquid crystal display (LCD) and\ndriver, in combination with a synchronized directional light emitting diode\n(LED) array. In this paper, we describe the operation and results of the\nbacklight drive electronics, where a prototype using a white LED illumination\nmatrix, a simplified LCD panel, and a linear Fresnel lens array serves as a\ntest bed.",
        "Remote sensing (RS) change detection incurs a high cost because of false\nnegatives, which are more costly than false positives. Existing frameworks,\nstruggling to improve the Precision metric to reduce the cost of false\npositive, still have limitations in focusing on the change of interest, which\nleads to missed detections and discontinuity issues. This work tackles these\nissues by enhancing feature learning capabilities and integrating the frequency\ncomponents of feature information, with a strategy to incrementally boost the\nRecall value. We propose an enhanced hybrid of CNN and Transformer network\n(EHCTNet) for effectively mining the change information of interest. Firstly, a\ndual branch feature extraction module is used to extract the multi scale\nfeatures of RS images. Secondly, the frequency component of these features is\nexploited by a refined module I. Thirdly, an enhanced token mining module based\non the Kolmogorov Arnold Network is utilized to derive semantic information.\nFinally, the semantic change information's frequency component, beneficial for\nfinal detection, is mined from the refined module II. Extensive experiments\nvalidate the effectiveness of EHCTNet in comprehending complex changes of\ninterest. The visualization outcomes show that EHCTNet detects more intact and\ncontinuous changed areas and perceives more accurate neighboring distinction\nthan state of the art models.",
        "This work presents an unfitted boundary algebraic equation (BAE) method for\nsolving three-dimensional elliptic partial differential equations on complex\ngeometries using finite difference on structured meshes. We demonstrate that\nreplacing finite auxiliary domains with free-space LGFs streamlines the\ncomputation of difference potentials, enabling matrix-free implementations and\nsignificant cost reductions. We establish theoretical foundations by showing\nthe equivalence between direct formulations in difference potentials framework\nand indirect single\/double layer formulations and analyzing their spectral\nproperties. The spectral analysis demonstrates that discrete double layer\nformulations provide better-conditioned systems for iterative solvers,\nsimilarly as in boundary integral method. The method is validated through\nmatrix-free numerical experiments on both Poisson and modified Helmholtz\nequations in 3D implicitly defined geometries, showing optimal convergence\nrates and computational efficiency. This framework naturally extends to\nunbounded domains and provides a foundation for applications to more complex\nsystems like Helmholtz and Stokes equations.",
        "Recent advancements in language-guided diffusion models for image editing are\noften bottle-necked by cumbersome prompt engineering to precisely articulate\ndesired changes. An intuitive alternative calls on guidance from in-the-wild\nimage exemplars to help users bring their imagined edits to life. Contemporary\nexemplar-based editing methods shy away from leveraging the rich latent space\nlearnt by pre-existing large text-to-image (TTI) models and fall back on\ntraining with curated objective functions to achieve the task. Though somewhat\neffective, this demands significant computational resources and lacks\ncompatibility with diverse base models and arbitrary exemplar count. On further\ninvestigation, we also find that these techniques restrict user control to only\napplying uniform global changes over the entire edited region. In this paper,\nwe introduce a novel framework for progressive exemplar-driven editing with\noff-the-shelf diffusion models, dubbed PIXELS, to enable customization by\nproviding granular control over edits, allowing adjustments at the pixel or\nregion level. Our method operates solely during inference to facilitate\nimitative editing, enabling users to draw inspiration from a dynamic number of\nreference images, or multimodal prompts, and progressively incorporate all the\ndesired changes without retraining or fine-tuning existing TTI models. This\ncapability of fine-grained control opens up a range of new possibilities,\nincluding selective modification of individual objects and specifying gradual\nspatial changes. We demonstrate that PIXELS delivers high-quality edits\nefficiently, leading to a notable improvement in quantitative metrics as well\nas human evaluation. By making high-quality image editing more accessible,\nPIXELS has the potential to enable professional-grade edits to a wider audience\nwith the ease of using any open-source image generation model.",
        "We build a 1064 nm fiber laser system-based testing facility for emulating\nSETs in different electronics components and ICs. Using these facilities, we\ntested the 4N35 optocoupler to observe SETs for the first time.",
        "With the rise of sophisticated scam websites that exploit human psychological\nvulnerabilities, distinguishing between legitimate and scam websites has become\nincreasingly challenging. This paper presents ScamFerret, an innovative agent\nsystem employing a large language model (LLM) to autonomously collect and\nanalyze data from a given URL to determine whether it is a scam. Unlike\ntraditional machine learning models that require large datasets and feature\nengineering, ScamFerret leverages LLMs' natural language understanding to\naccurately identify scam websites of various types and languages without\nrequiring additional training or fine-tuning. Our evaluation demonstrated that\nScamFerret achieves 0.972 accuracy in classifying four scam types in English\nand 0.993 accuracy in classifying online shopping websites across three\ndifferent languages, particularly when using GPT-4. Furthermore, we confirmed\nthat ScamFerret collects and analyzes external information such as web content,\nDNS records, and user reviews as necessary, providing a basis for identifying\nscam websites from multiple perspectives. These results suggest that LLMs have\nsignificant potential in enhancing cybersecurity measures against sophisticated\nscam websites.",
        "We introduce a new family of non-negative real-valued functions on a\n$C^*$-algebra $\\mathcal{A}$, i.e., for $0\\leq \\mu \\leq 1,$\n$$\\|a\\|_{\\sigma_{\\mu}}= \\text{sup}\\left\\lbrace \\sqrt{|f(a)|^2 \\sigma_{\\mu}\nf(a^*a)}: f\\in \\mathcal{A}', \\, f(1)=\\|f\\|=1 \\right\\rbrace, \\quad $$ where\n$a\\in \\mathcal{A}$ and $\\sigma_{\\mu}$ is an interpolation path of the symmetric\nmean $\\sigma$. These functions are semi-norms as they satisfy the norm axioms,\nexcept for the triangle inequality. Special cases satisfying triangle\ninequality, and a complete equality characterization is also discussed. Various\nbounds and relationships will be established for this new family, with a\nconnection to the existing literature in the algebra of all bounded linear\noperators on a Hilbert space.",
        "Several backbone models pre-trained on general domain datasets can encode a\nsentence into a widely useful embedding. Such sentence embeddings can be\nfurther enhanced by domain adaptation that adapts a backbone model to a\nspecific domain. However, domain adaptation for low-resource languages like\nJapanese is often difficult due to the scarcity of large-scale labeled\ndatasets. To overcome this, this paper introduces SDJC (Self-supervised Domain\nadaptation for Japanese sentence embeddings with Contrastive learning) that\nutilizes a data generator to generate sentences, which have the same syntactic\nstructure to a sentence in an unlabeled specific domain corpus but convey\ndifferent semantic meanings. Generated sentences are then used to boost\ncontrastive learning that adapts a backbone model to accurately discriminate\nsentences in the specific domain. In addition, the components of SDJC like a\nbackbone model and a method to adapt it need to be carefully selected, but no\nbenchmark dataset is available for Japanese. Thus, a comprehensive Japanese STS\n(Semantic Textual Similarity) benchmark dataset is constructed by combining\ndatasets machine-translated from English with existing datasets. The\nexperimental results validates the effectiveness of SDJC on two domain-specific\ndownstream tasks as well as the usefulness of the constructed dataset.\nDatasets, codes and backbone models adapted by SDJC are available on our github\nrepository https:\/\/github.com\/ccilab-doshisha\/SDJC.",
        "Graph generation is a critical task in numerous domains, including molecular\ndesign and social network analysis, due to its ability to model complex\nrelationships and structured data. While most modern graph generative models\nutilize adjacency matrix representations, this work revisits an alternative\napproach that represents graphs as sequences of node set and edge set. We\nadvocate for this approach due to its efficient encoding of graphs and propose\na novel representation. Based on this representation, we introduce the Graph\nGenerative Pre-trained Transformer (G2PT), an auto-regressive model that learns\ngraph structures via next-token prediction. To further exploit G2PT's\ncapabilities as a general-purpose foundation model, we explore fine-tuning\nstrategies for two downstream applications: goal-oriented generation and graph\nproperty prediction. We conduct extensive experiments across multiple datasets.\nResults indicate that G2PT achieves superior generative performance on both\ngeneric graph and molecule datasets. Furthermore, G2PT exhibits strong\nadaptability and versatility in downstream tasks from molecular design to\nproperty prediction.",
        "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present the first qualitative user study on Large Language Model\n(LLM)-driven privacy policy assessment. To this end, we build and evaluate an\nLLM-based privacy policy assessment browser extension, which helps users\nunderstand the essence of a lengthy, complex privacy policy while browsing. The\ntool integrates a dashboard and an LLM chat. In our qualitative user study\n(N=22), we evaluate usability, understandability of the information our tool\nprovides, and its impacts on awareness. While providing a comprehensible quick\noverview and a chat for in-depth discussion improves privacy awareness, users\nnote issues with building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools.",
        "We attempt to take a comprehensive look at the challenges of representing the\nspatio-temporal structures and dynamic processes defining a city's overall\ncharacteristics. For the task of urban planning and urban operation, we take\nthe stance that even if the necessary representations of these structures and\nprocesses can be achieved, the most important representation of the relevant\nmindsets of the citizens are, unfortunately, mostly neglected.\n  After a review of major \"traditional\" urban models of structures behind urban\nscale, form, and dynamics, we turn to major recent modeling approaches\ntriggered by recent advances in AI that enable multi-modal generative models.\nSome of these models can create representations of geometries, networks and\nimages, and reason flexibly at a human-compatible semantic level. They provide\nhuge amounts of knowledge extracted from Terabytes of text and image documents\nand cover the required rich representation spectrum including geographic\nknowledge by different knowledge sources, degrees of granularity and scales.\n  We then discuss what these new opportunities mean for the modeling challenges\nposed by cities, in particular with regard to the role and impact of citizens\nand their interactions within the city infrastructure. We propose to integrate\nthese possibilities with existing approaches, such as agent-based models, which\nopens up new modeling spaces including rich citizen models which are able to\nalso represent social interactions.\n  Finally, we put forward some thoughts about a vision of a \"social AI in a\ncity ecosystem\" that adds relevant citizen models to state-of-the-art\nstructural and process models. This extended city representation will enable\nurban planners to establish citizen-oriented planning of city infrastructures\nfor human culture, city resilience and sustainability.",
        "Most existing image tokenizers encode images into a fixed number of tokens or\npatches, overlooking the inherent variability in image complexity. To address\nthis, we introduce Content-Adaptive Tokenizer (CAT), which dynamically adjusts\nrepresentation capacity based on the image content and encodes simpler images\ninto fewer tokens. We design a caption-based evaluation system that leverages\nlarge language models (LLMs) to predict content complexity and determine the\noptimal compression ratio for a given image, taking into account factors\ncritical to human perception. Trained on images with diverse compression\nratios, CAT demonstrates robust performance in image reconstruction. We also\nutilize its variable-length latent representations to train Diffusion\nTransformers (DiTs) for ImageNet generation. By optimizing token allocation,\nCAT improves the FID score over fixed-ratio baselines trained with the same\nflops and boosts the inference throughput by 18.5%.",
        "Supervised dimensionality reduction aims to map labeled data to a\nlow-dimensional feature space while maximizing class discriminability. Despite\nthe availability of methods for learning complex non-linear features (e.g. Deep\nLearning), there is an enduring demand for dimensionality reduction methods\nthat learn linear features due to their interpretability, low computational\ncost, and broad applicability. However, there is a gap between methods that\noptimize linear separability (e.g. LDA), and more flexible but computationally\nexpensive methods that optimize over arbitrary class boundaries (e.g.\nmetric-learning methods). Here, we present Supervised Quadratic Feature\nAnalysis (SQFA), a dimensionality reduction method for learning linear features\nthat maximize the differences between class-conditional first- and second-order\nstatistics, which allow for quadratic discrimination. SQFA exploits the\ninformation geometry of second-order statistics in the symmetric positive\ndefinite manifold. We show that SQFA features support quadratic\ndiscriminability in real-world problems. We also provide a theoretical link,\nbased on information geometry, between SQFA and the Quadratic Discriminant\nAnalysis (QDA) classifier.",
        "High-resolution (HR) image perception remains a key challenge in multimodal\nlarge language models (MLLMs). To overcome the limitations of existing methods,\nthis paper shifts away from prior dedicated heuristic approaches and revisits\nthe most fundamental idea to HR perception by enhancing the long-context\ncapability of MLLMs, driven by recent advances in long-context techniques like\nretrieval-augmented generation (RAG) for general LLMs. Towards this end, this\npaper presents the first study exploring the use of RAG to address HR\nperception challenges. Specifically, we propose Retrieval-Augmented Perception\n(RAP), a training-free framework that retrieves and fuses relevant image crops\nwhile preserving spatial context using the proposed Spatial-Awareness Layout.\nTo accommodate different tasks, the proposed Retrieved-Exploration Search\n(RE-Search) dynamically selects the optimal number of crops based on model\nconfidence and retrieval scores. Experimental results on HR benchmarks\ndemonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving\na 43% improvement on $V^*$ Bench and 19% on HR-Bench.",
        "We show that equalization-enhanced phase noise manifests as a time-varying,\nfrequency-dependent phase error, which can be modeled and reversed by a\ntime-varying all-pass finite impulse response filter.",
        "Capturing high dynamic range (HDR) scenes is one of the most important issues\nin camera design. Majority of cameras use exposure fusion technique, which\nfuses images captured by different exposure levels, to increase dynamic range.\nHowever, this approach can only handle images with limited exposure difference,\nnormally 3-4 stops. When applying to very high dynamic scenes where a large\nexposure difference is required, this approach often fails due to incorrect\nalignment or inconsistent lighting between inputs, or tone mapping artifacts.\nIn this work, we propose UltraFusion, the first exposure fusion technique that\ncan merge input with 9 stops differences. The key idea is that we model the\nexposure fusion as a guided inpainting problem, where the under-exposed image\nis used as a guidance to fill the missing information of over-exposed highlight\nin the over-exposed region. Using under-exposed image as a soft guidance,\ninstead of a hard constrain, our model is robust to potential alignment issue\nor lighting variations. Moreover, utilizing the image prior of the generative\nmodel, our model also generates natural tone mapping, even for very\nhigh-dynamic range scene. Our approach outperforms HDR-Transformer on latest\nHDR benchmarks. Moreover, to test its performance in ultra high dynamic range\nscene, we capture a new real-world exposure fusion benchmark, UltraFusion\nDataset, with exposure difference up to 9 stops, and experiments show that\n\\model~can generate beautiful and high-quality fusion results under various\nscenarios. An online demo is provided at\nhttps:\/\/openimaginglab.github.io\/UltraFusion\/.",
        "This paper presents a novel methodological framework, called the\nActor-Simulator, that incorporates the calibration of digital twins into\nmodel-based reinforcement learning for more effective control of stochastic\nsystems with complex nonlinear dynamics. Traditional model-based control often\nrelies on restrictive structural assumptions (such as linear state transitions)\nand fails to account for parameter uncertainty in the model. These issues\nbecome particularly critical in industries such as biopharmaceutical\nmanufacturing, where process dynamics are complex and not fully known, and only\na limited amount of data is available. Our approach jointly calibrates the\ndigital twin and searches for an optimal control policy, thus accounting for\nand reducing model error. We balance exploration and exploitation by using\npolicy performance as a guide for data collection. This dual-component approach\nprovably converges to the optimal policy, and outperforms existing methods in\nextensive numerical experiments based on the biopharmaceutical manufacturing\ndomain.",
        "Machine learning inter-atomic potentials (MLIPs) are revolutionizing the\nfield of molecular dynamics (MD) simulations. Recent MLIPs have tended towards\nmore complex architectures trained on larger datasets. The resulting increase\nin computational and memory costs may prohibit the application of these MLIPs\nto perform large-scale MD simulations. Here, we present a teacher-student\ntraining framework in which the latent knowledge from the teacher (atomic\nenergies) is used to augment the students' training. We show that the\nlight-weight student MLIPs have faster MD speeds at a fraction of the memory\nfootprint compared to the teacher models. Remarkably, the student models can\neven surpass the accuracy of the teachers, even though both are trained on the\nsame quantum chemistry dataset. Our work highlights a practical method for\nMLIPs to reduce the resources required for large-scale MD simulations.",
        "Chain-of-Thought significantly enhances a model's reasoning capability, but\nit also comes with a considerable increase in inference costs due to long\nchains. With the observation that the reasoning path can be easily compressed\nunder easy tasks but struggle on hard tasks, we explore the feasibility of\nelastically controlling the length of reasoning paths with only one model,\nthereby reducing the inference overhead of reasoning models dynamically based\non task difficulty. We introduce a new tuning and inference strategy named\nCoT-Valve, designed to allow models to generate reasoning chains of varying\nlengths. To achieve this, we propose to identify a direction in the parameter\nspace that, when manipulated, can effectively control the length of generated\nCoT. Moreover, we show that this property is valuable for compressing the\nreasoning chain. We construct datasets with chains from long to short for the\nsame questions and explore two enhanced strategies for CoT-Valve: (1) a precise\nlength-compressible CoT tuning method, and (2) a progressive chain length\ncompression approach. Our experiments show that CoT-Valve successfully enables\ncontrollability and compressibility of the chain and shows better performance\nthan the prompt-based control. We applied this method to QwQ-32B-Preview,\nreducing reasoning chains on GSM8K from 741 to 225 tokens with a minor\nperformance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with\nonly one additional incorrect answer."
      ]
    }
  },
  {
    "id":2411.08063,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"A deep-learning approach to realizing functionality in nanoelectronic devices",
    "start_abstract":"Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials"
      ],
      "abstract":[
        "The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Skeletal Torus Actions and GKM Structures on Quiver Grassmannians of\n  String Representations",
        "Data Augmentation and Regularization for Learning Group Equivariance",
        "Nitrogen-Vacancy Centers in Epitaxial Laterally Overgrown Diamond:\n  Towards Up-scaling of Color Center-based Quantum Technologies",
        "Towards a complexity-theoretic dichotomy for TQFT invariants",
        "Stabilization of quantum properties under intrinsic decoherence in\n  presence of external magnetic fields",
        "Searching for Inflationary Physics with the CMB Trispectrum: 3.\n  Constraints from Planck",
        "Directional optical parametric amplification in a hyperbolic\n  metamaterial",
        "Laser cooled 137BaF molecules for measuring nuclear-spin-dependent\n  parity violation",
        "Interplay of ALP Couplings at a Muon Collider",
        "Change of some cropping systems in a long-term trial comparing different\n  systems: rationale and implications for statistical analysis",
        "Lie Symmetry Analysis, Parametric Reduction and Conservation Laws of\n  (3+1) Dimensional Nonlinear Dispersive Soliton Equation",
        "Deep Learning-Powered Electrical Brain Signals Analysis: Advancing\n  Neurological Diagnostics",
        "Learning by Confusion: The Phase Diagram of the Holstein Model",
        "Decaying turbulence beneath surface waves",
        "Thermodynamic uncertainty relations for three-terminal systems with\n  broken time-reversal symmetry",
        "Observation of Fermi acceleration with cold atoms",
        "Various Architectures of Colloidal Cu3(MoO4)2(OH)2 and Cu3Mo2O9; Thermal\n  Stability, Photoluminescence and Magnetic Properties of Cu3(MoO4)2(OH)2 and\n  Cu3Mo2O9 Nanosheets",
        "Odd spanning trees of a graph",
        "Theory of Magnon Purcell Effect in Cavity Magnonic System",
        "Instabilities of the kinematic state of the atmospheres of some single\n  C-rich post-AGB stars",
        "Stochastic Optimal Control of Iron Condor Portfolios for Profitability\n  and Risk Management",
        "Narrowline Laser Cooling and Spectroscopy of Molecules via Stark States",
        "Interfacial superconductivity and a Se-vacancy ordered insulating phase\n  in the FeSe\/PbOx heterostructures",
        "Objective Mackey and Tambara functors via parametrized categories",
        "Doubly-Robust Functional Average Treatment Effect Estimation",
        "Global existence for multi-dimensional partially diffusive systems",
        "Introducing APERTURE: A GPU-based General Relativistic Particle-in-Cell\n  Simulation Framework",
        "Convergence Analysis of alpha-SVRG under Strong Convexity",
        "Driven Polymer Translocation through a Nanopore from a Confining Channel"
      ],
      "abstract":[
        "Quiver Grassmannians of equioriented type $\\texttt{A}$ and nilpotent\nequioriented type $\\tilde{\\texttt{A}}$ quiver representations are\nGKM-varieties. In particular, they have a cellular decomposition and admit a\ntorus action with finitely many fixed points and one-dimensional orbits (i.e.\nskeletal action). We examine the case of string representations and provide a\nclassification of all corresponding quiver Grassmannians with a GKM-variety\nstructure.",
        "In many machine learning tasks, known symmetries can be used as an inductive\nbias to improve model performance. In this paper, we consider learning group\nequivariance through training with data augmentation. We summarize results from\na previous paper of our own, and extend the results to show that equivariance\nof the trained model can be achieved through training on augmented data in\ntandem with regularization.",
        "Providing high-quality, single-crystal diamond (SCD) with a large area is\ndesirable for up-scaling quantum technology applications that rely on color\ncenters in diamond. Growth methods aiming to increase the area of SCD are an\nactive research area. Native color centers offer a sensitive probe for local\ncrystal quality in such novel materials e.g., via their reaction to stress. In\nthis work, we investigate individual native nitrogen-vacancy (NV) centers in\nSCD layers manufactured via laterally overgrowing hole arrays in a\nheteroepitaxially grown large-scale substrate. Heteroepitaxy has become a\ncommon tool for growing large SCDs; however, achieving the high crystal quality\nneeded for quantum applications remains a challenge. In the overgrown layer, we\nidentify NV centers with spin-decoherence times in the order of hundreds of\nmicroseconds, comparable to high-purity homoepitaxial SCD. We quantify the\neffective crystal strain in different regions of the overgrown layer,\nindicating a low stress overall and a stress reduction in the diamond layer\nabove the holes.",
        "We show that for any fixed $(2+1)$-dimensional TQFT over $\\mathbb{C}$ of\neither Turaev-Viro-Barrett-Westbury or Reshetikhin-Turaev type, the problem of\n(exactly) computing its invariants on closed 3-manifolds is either solvable in\npolynomial time, or else it is $\\#\\mathsf{P}$-hard to (exactly) contract\ncertain tensors that are built from the TQFT's fusion category. Our proof is an\napplication of a dichotomy result of Cai and Chen [J. ACM, 2017] concerning\nweighted constraint satisfaction problems over $\\mathbb{C}$. We leave for\nfuture work the issue of reinterpreting the conditions of Cai and Chen that\ndistinguish between the two cases (i.e. $\\#\\mathsf{P}$-hard tensor contractions\nvs. polynomial time invariants) in terms of fusion categories. We expect that\nwith more effort, our reduction can be improved so that one gets a dichotomy\ndirectly for TQFTs' invariants of 3-manifolds rather than more general tensors\nbuilt from the TQFT's fusion category.",
        "The dynamical behavior of quantum state properties under intrinsic\ndecoherence models can be modified by the presence of external magnetic fields.\nAlthough generically external magnetic fields are detrimental to preserve\nquantumness in the presence of intrinsic decoherence, judicious adjustment of\nthe magnetic field can stabilize such features. This stabilization arises from\nnovel resonances between energy eigenstates resulting from the presence of an\nexternal magnetic field. Here, we present our findings using as a model system\ntwo spin 1-particles confined in a double-well potential under intrinsic\ndecoherence. We stress, however, that our results are generic and independent\non the used model.",
        "Is there new physics hidden in the four-point function of the cosmic\nmicrowave background (CMB)? We conduct a detailed analysis of the Planck PR4\ntemperature and polarization trispectrum for $\\ell\\in[2,2048]$. Using the\ntheoretical and computational tools developed in Paper 1 and Paper 2, we search\nfor 33 template amplitudes, encoding a variety of effects from inflationary\nself-interactions to particle exchange. We find no evidence for primordial\nnon-Gaussianity and set stringent constraints on both phenomenological\namplitudes and couplings in the inflationary Lagrangian. Due to the use of\noptimal estimators and polarization data, our constraints are highly\ncompetitive. For example, we find $\\sigma(g_{\\rm NL}^{\\rm loc})=4.8\\times 10^4$\nand $\\tau_{\\rm NL}^{\\rm loc} <1500$ (95\\% CL), a factor of two improvement on\nEffective Field Theory amplitudes, and a $43\\sigma$ detection of gravitational\nlensing. Many templates are analyzed for the first time, such as\ndirection-dependent trispectra and the collapsed limit of the `cosmological\ncollider', across a range of masses and spins. We perform a variety of\nvalidation tests; whilst our results are stable, the most relevant systematics\nare found to be lensing bias, residual foregrounds, and mismatch between\nsimulations and data. The techniques discussed in this series can be extended\nto future datasets, allowing the primordial Universe to be probed at even\nhigher sensitivity.",
        "Optical parametric amplification (OPA) comprises essentially a nonlinear\nfour-wave mixing process in which a \"pump\" and a \"signal\" field give rise to an\n\"idler\" field under certain phase-matching conditions. Here we use a photonic\ncrystal waveguide strongly-coupled with an excitonic reservoir to generate this\nprocess between different guided modes at optical wavelengths. Differently from\nclassical nonlinear optical crystals, where the pump and idler photons travel\nalmost collinearly, our exciton-polaritons are naturally separated in the\nwaveguide due to their opposite group velocities. Due to the high efficiency of\nthe process we can generate the idler field of the parametric process by\npumping with a continuous wave laser and choose its direction of propagation in\nthe waveguide by adjusting the angle of incidence of the seed laser. We show\nthe OPA process to be robust against surface defects of the waveguide and can\nlead to simple-to-fabricate devices compared to microcavities that take\nadvantage of strong signal-idler correlations in a propagating geometry. Our\nresults closely agree with mean-field numerical simulations.",
        "We demonstrate optical cycling and transverse laser cooling of a beam of\nfermionic 137BaF molecules. Their high masses and nuclear spins make these\nmolecules sensitive probes for parity violation and properties of the weak\ninteraction. However, the nuclear spins also lead to a quasi-closed cycling\ntransition currently involving up to 112 levels, which significantly exceeds\nthe complexity in other laser-cooled molecules. Optical cycling and cooling are\nfacilitated through carefully designed optical spectra tailored to this\nmolecular structure. Our results pave the way for efficient state preparation,\ndetection, and cooling in precision measurements using this species and other\nsimilar species.",
        "Axion-like particles can couple to Standard Model gluons, electroweak gauge\nbosons, and massive fermions. A future multi-TeV muon collider provides a\nfavorable environment to probe axion-like particles through multiple production\nchannels, including vector boson fusion via electroweak gauge boson couplings\nand the top-associated production mediated by direct fermionic couplings.\nMotivated by the quality issue of the QCD axion, we focus on axion-like\nparticles with masses and decay constants around the TeV scale. We explore how\ndifferent axion-like particle couplings shape its production and decay modes,\nrevealing a rich and intricate phenomenological landscape.",
        "The project Agriculture 4.0 without chemical synthetical plant protection\n(NOcsPS) tests a number of cropping systems that avoid the use of chemical\nsynthetical pesticides while at the same time using mineral fertilizers. The\nexperiment started in 2020 (sowing fall 2019). In 2024 (sowing fall 2023), some\nof the cropping systems were modified. Analysis of this experiment may be done\nusing linear mixed models. In order to include the data from 2020-2023 in joint\nanalyses with the data collected for the modified systems from 2024 onwards,\nthe mixed modelling approach needs to be reconsidered. In this paper, we\ndevelop models for this purpose. A key feature is the use of network\nmeta-analytic concepts that allow a combination of direct and indirect\ncomparisons among systems from the different years. The approach is first\nillustrated using a toy example. This is followed by detailed analyses of data\nfrom two the two trials sites Dahnsdorf and Hohenheim.",
        "The core focus of this research work is to obtain invariant solutions and\nconservation laws of the (3+1) dimensional ZK equation which describes the\nphenomenon of wave stability and solitons propagation. ZK equation is\nsignificant in fluid dynamics, nonlinear optics, and acoustics. ZK equation\nplays an important role in understanding the behavior of nonlinear, dispersive\nwaves especially in the presence of the magnetic field. Lie symmetry analysis\nhas been applied to the (3+1)-dimensional ZK equation to derive invariant\nsolutions. These solutions disclose how waves retain their shape as they\ntravel, how they interact in space, and the impact of magnetic fields on wave\npropagation. Using the Lie symmetry, (3+1)-dimensional ZK equation reduces into\nvarious ordinary differential equations. It also concludes with the\nconservation laws and non-linear self adjoint-ness property for the\n(3+1)-dimensional ZK Equation.",
        "Neurological disorders represent significant global health challenges,\ndriving the advancement of brain signal analysis methods. Scalp\nelectroencephalography (EEG) and intracranial electroencephalography (iEEG) are\nwidely used to diagnose and monitor neurological conditions. However, dataset\nheterogeneity and task variations pose challenges in developing robust deep\nlearning solutions. This review systematically examines recent advances in deep\nlearning approaches for EEG\/iEEG-based neurological diagnostics, focusing on\napplications across 7 neurological conditions using 46 datasets. We explore\ntrends in data utilization, model design, and task-specific adaptations,\nhighlighting the importance of pre-trained multi-task models for scalable,\ngeneralizable solutions. To advance research, we propose a standardized\nbenchmark for evaluating models across diverse datasets to enhance\nreproducibility. This survey emphasizes how recent innovations can transform\nneurological diagnostics and enable the development of intelligent, adaptable\nhealthcare solutions.",
        "We employ the \"learning by confusion\" technique, an unsupervised machine\nlearning approach for detecting phase transitions, to analyze quantum Monte\nCarlo simulations of the two-dimensional Holstein model--a fundamental model\nfor electron-phonon interactions on a lattice. Utilizing a convolutional neural\nnetwork, we conduct a series of binary classification tasks to identify\nHolstein critical points based on the neural network's learning accuracy. We\nfurther evaluate the effectiveness of various training datasets, including\nsnapshots of phonon fields and other measurements resolved in imaginary time,\nfor predicting distinct phase transitions and crossovers. Our results culminate\nin the construction of the finite-temperature phase diagram of the Holstein\nmodel.",
        "This paper explores decaying turbulence beneath surface waves that is\ninitially isotropic and shear-free. We start by presenting phenomenology\nrevealed by wave-averaged numerical simulations: an accumulation of angular\nmomentum in coherent vortices, suppression of kinetic energy dissipation, and\nthe development of depth-alternating jets. We interpret these features through\nan analogy with rotating turbulence (Holm 1996), wherein the curl of the Stokes\ndrift, $\\nabla \\times \\mathbf{u}^S$, takes on the role of the background\nvorticity (for example, $(f_0 + \\beta y) \\mathbf{\\hat z}$ on the\n$\\beta$-plane). We pursue this thread further by showing that a two-equation\nmodel proposed by (Bardina et al. 1985) for rotating turbulence reproduces the\nsimulated evolution of volume-integrated kinetic energy. This success of the\ntwo-equation model -- which explicitly parameterizes wave-driven suppression of\nkinetic energy dissipation -- carries implications for modeling turbulent\nmixing in the ocean surface boundary layer. We conclude with a discussion about\na wave-averaged analogue of the Rossby number appearing in the two-equation\nmodel, which we term the ``pseudovorticity number'' after the pseudovorticity\n$\\nabla \\times \\mathbf{u}^S$. The pseudovorticity number is related to the\nLangmuir number in an integral sense.",
        "We investigate the thermodynamic uncertainty relations (TURs) in steady-state\ntransport for three-terminal systems within the linear response regime,\nspecifically in the presence of broken time-reversal symmetry. To quantify the\nTUR, we introduce a dimensionless trade-off parameter $Q_J$, and derive new\nbounds of $Q_J$ for both particle and heat currents under a strong constraint\non the Onsager coefficients. Furthermore, we determine a universal lower bound\n$Q_J^{bound}\\geq1.5$ for three-terminal systems in the linear response regime\nwhen the time-reversal symmetry is broken.",
        "Cosmic rays are deemed to be generated by a process known as ``Fermi\nacceleration\", in which charged particles scatter against magnetic fluctuations\nin astrophysical plasmas. The process itself is however universal, has both\nclassical and quantum formulations, and is at the basis of dynamical systems\nwith interesting mathematical properties, such as the celebrated Fermi-Ulam\nmodel. Despite its effectiveness in accelerating particles, Fermi acceleration\nhas so far eluded unambiguous verifications in laboratory settings. Here, we\nrealize the first fully controllable Fermi accelerator by colliding ultracold\natoms against engineered movable potential barriers. We demonstrate that our\nFermi accelerator, which is only 100 um in size, can produce ultracold atomic\njets with velocities above half a meter per second. Adding dissipation, we also\nexperimentally test Bell's general argument for the ensuing energy spectra,\nwhich is at the basis of any model of cosmic ray acceleration. On the one hand,\nour work effectively opens the window to the study of high energy astrophysics\nwith cold atoms, offering new capabilities for the understanding of phenomena\nsuch as diffusive acceleration at collisionless shocks. On the other, the\nperformance of our Fermi accelerator is competitive with those of best-in-class\naccelerating methods used in quantum technology and quantum colliders, but with\nsubstantially simpler implementation and virtually no upper limit.",
        "The lindgrenite compounds [Cu3(MoO4)2(OH)2] with various architectures and\nhigh crystallinity were prepared by a simple surfactant-assisted hydrothermal\nmethod. Then, the Cu3Mo2O9 samples were prepared by calcination of the\nas-synthesized Cu3(MoO4)2(OH)2. The resulting samples have high crystallinity,\ncolloidal properties, high-yield, large-scale production capability with using\nof nontoxic and inexpensive reagents and water as an environmentally solvent.\nThe scanning electron microscope studies show that the as-prepared lindgrenite\nnanostructures are well crystallized with rod, sheet and hollow sphere\nmorphologies. Meanwhile, the photoluminescence and magnetic properties of the\nnanosheet samples have been investigated that the both of Cu3(MoO4)2(OH)2 and\nCu3Mo2O9 samples have super paramagnetic behavior at room temperature and in\ncomparison with previous works, Cu3(MoO4)2(OH)2 and Cu3Mo2O9 samples\nsynthesized by the surfactant-assisted hydrothermal method in this work have a\nvery obvious red-shifted PL emission and high intensity.",
        "A graph $G=(V,E)$ is said to be odd (or even, resp.) if $d_G(v)$ is odd (or\neven, resp.) for any $v\\in V$. Trivially, the order of an odd graph must be\neven. In this paper, we show that every 4-edge connected graph of even order\nhas a connected odd factor. A spanning tree $T$ of $G$ is called a\nhomeomorphically irreducible spanning tree (HIST by simply) if $T$ contains no\nvertex of degree two. Trivially, an odd spanning tree must be a HIST. In 1990,\nAlbertson, Berman, Hutchinson, and Thomassen showed that every connected graph\nof order $n$ with $\\delta(G)\\geq \\min\\{\\frac n 2, 4\\sqrt{2n}\\}$ contains a\nHIST.\n  We show that every complete bipartite graph with both parts being even has no\nodd spanning tree, thereby for any even integer $n$ divisible by 4, there\nexists a graph of order $n$ with the minimum degree $\\frac n 2$ having no odd\nspanning tree. Furthermore, we show that every graph of order $n$ with\n$\\delta(G)\\geq \\frac n 2 +1$ has an odd spanning tree. We also characterize all\nsplit graphs having an odd spanning tree. As an application, for any graph $G$\nwith diameter at least 4, $\\overline{G}$ has a spanning odd double star.\nFinally, we also give a necessary and sufficient condition for a triangle-free\ngraph $G$ whose complement contains an odd spanning tree. A number of related\nopen problems are proposed.",
        "We conduct a systematic analysis of cavity effects on the decay dynamics of\nan open magnonic system. The Purcell effect on the magnon oscillator decay is\nthoroughly examined for both driven and non-driven scenarios. Analytical\nconditions are determined to distinguish between strong and weak coupling\nregimes, corresponding to oscillatory and pure decay behaviors respectively.\nAdditionally, our theory also predicts the decay of the photon mode within the\ncavity-magnonic open system, demonstrating excellent agreement with existing\nexperimental data. Our findings and methodologies can provide valuable insights\nfor advancing research in cavity magnonic quantum control, quantum information\nprocessing, and the development of magnonic quantum devices.",
        "To search for and study the instabilities in the atmospheres of selected\npost-AGB stars, we have performed a long-term high-resolution spectroscopy\n(R=60000) with the spectrograph NES of the 6-meter BTA telescope. Low-amplitude\npulsations, splitting and\/or asymmetry of the absorption profiles with a low\nexcitation potential, as well as variability of a complex H$\\alpha$ profile\nhave been registered in the optical spectra of single stars associated with the\nIR sources IRASz02229+6208, IRAS 04296+3429, IRAS 07134+1005, IRAS 07430+1115,\nIRAS 19500-1709, IRAS 22223+4327, and IRAS 23304+6147 that had previously\nundergone the 3-d dredge-up. The maximum pulsation amplitude A$_{\\rm Vr}$ was\ndetected for the stars in the IRAS 07134+1005 and IRAS 19500-1709 systems,\nwhich have the maximum temperatures among the stars studied. Stratification of\nradial velocity in the atmosphere was found for two stars in the sample. The\nluminosity of the studied stars was estimated based on the intensity of the IR\noxygen triplet OI(7774). Moreover, a luminosity of log${\\rm\n(L\/L_{\\odot})}\\approx$3.1 was obtained for the star in the IRAS 07430+1115\nsystem within the typical values for post-AGB stars luminosity, which\neliminates the paradox of the luminosity and the initial mass of this object.",
        "Previous research on option strategies has primarily focused on their\nbehavior near expiration, with limited attention to the transient value process\nof the portfolio. In this paper, we formulate Iron Condor portfolio\noptimization as a stochastic optimal control problem, examining the impact of\nthe control process \\( u(k_i, \\tau) \\) on the portfolio's potential\nprofitability and risk. By assuming the underlying price process as a bounded\nmartingale within $[K_1, K_2]$, we prove that the portfolio with a strike\nstructure of $k_1 < k_2 = K_2 < S_t < k_3 = K_3 < k_4$ has a submartingale\nvalue process, which results in the optimal stopping time aligning with the\nexpiration date $\\tau = T$. Moreover, we construct a data generator based on\nthe Rough Heston model to investigate general scenarios through simulation. The\nresults show that asymmetric, left-biased Iron Condor portfolios with $\\tau =\nT$ are optimal in SPX markets, balancing profitability and risk management.\nDeep out-of-the-money strategies improve profitability and success rates at the\ncost of introducing extreme losses, which can be alleviated by using an optimal\nstopping strategy. Except for the left-biased portfolios $\\tau$ generally falls\nwithin the range of [50\\%,75\\%] of total duration. In addition, we validate\nthese findings through case studies on the actual SPX market, covering bullish,\nsideways, and bearish market conditions.",
        "The electronic energy level structure of yttrium monoxide (YO) provides\nlong-lived excited $^{2}\\Delta$ states ideal for high-precision molecular\nspectroscopy, narrowline laser cooling at the single photon-recoil limit, and\nstudying dipolar physics with unprecedented interaction strength. We use\nultracold laser-cooled YO molecules to study the Stark effect in the metastable\nA$^{\\prime}\\,^{2}\\Delta_{3\/2}\\,J=3\/2$ state by high-resolution laser\nspectroscopy. We determined the absolute transition frequency from this\nmetastable state to the X$\\,^2\\Sigma^+$ electronic ground state with a\nfractional uncertainty of 9 $\\times$ 10$^{-12}$. In the presence of weak\nelectric fields a linear Stark effect is observed in the\nA$^{\\prime}\\,^{2}\\Delta_{3\/2}$ state owing to the large electric dipole moment\nand near degenerate $\\Lambda$-doublet states. A quasi-closed photon cycling\nscheme is identified involving a narrowline transition to a single Stark state,\nand implemented in free space to demonstrate the first narrowline laser cooling\nof a molecule, reducing the temperature of sub-Doppler cooled YO in two\ndimensions.",
        "The discovery of high-temperature superconductivity in FeSe\/SrTiO3 has\nsparked significant interests in exploring new superconducting systems with\nengineered interfaces. Here, using molecular beam epitaxy growth, we\nsuccessfully fabricate FeSe\/PbOx heterostructures and discover\nsuperconductivities in three different monolayer FeSe-related interfaces. We\nobserve superconducting gaps of 13~14 meV in the monolayer FeSe films grown on\ntwo different phases of PbOx. Moreover, we discover a new insulating Fe10Se9\nphase with an ordered $\\sqrt{5}\\times\\sqrt{5}$ Se-vacancy structure. Our\nfirst-principles calculation suggests that this new insulating phase originates\nfrom electronic correlation. Intriguingly, an additional monolayer FeSe film\ngrown on the insulating Fe10Se9 also exhibits superconductivity with the gap\nsize of 5 meV. Our results suggest that the work function differences between\nthe monolayer FeSe and the substrates, which can induce band bending and charge\ntransfer, are crucial for the interfacial superconductivity.",
        "The first word in the title is intended in a sense suggested by Lawvere and\nSchanuel whereby finite sets are objective natural numbers. At the objective\nlevel, the axioms defining abstract Mackey and Tambara functors are\ncategorically familiar. The first step was taken by Harald Lindner in 1976 when\nhe recognized that Mackey functors, defined as pairs of functors, were single\nfunctors with domain a category of spans. We define objective Mackey and\nobjective Tambara functors as parametrized categories which have local finite\nproducts and satisfy some parametrized completeness and cocompleteness\nrestriction. However, we can replace the original parametrizing base for\nobjective Mackey functors by a bicategory of spans while the replacement for\nobjective Tambara functors is a bicategory obtained by iterating the span\nconstruction; these iterated spans are polynomials. There is an objective\nMackey functor of ordinary Mackey functors. We show that there is a\ndistributive law relating objective Mackey functors to objective Tambara\nfunctors analogous to the distributive law relating abelian groups to\ncommutative rings. We remark on hom enrichment matters involving the 2-category\n$\\mathrm{Cat}_{+}$ of categories admitting finite coproducts and functors\npreserving them, both as a closed base and as a skew-closed base.",
        "Understanding causal relationships in the presence of complex, structured\ndata remains a central challenge in modern statistics and science in general.\nWhile traditional causal inference methods are well-suited for scalar outcomes,\nmany scientific applications demand tools capable of handling functional data\n-- outcomes observed as functions over continuous domains such as time or\nspace. Motivated by this need, we propose DR-FoS, a novel method for estimating\nthe Functional Average Treatment Effect (FATE) in observational studies with\nfunctional outcomes. DR-FoS exhibits double robustness properties, ensuring\nconsistent estimation of FATE even if either the outcome or the treatment\nassignment model is misspecified. By leveraging recent advances in functional\ndata analysis and causal inference, we establish the asymptotic properties of\nthe estimator, proving its convergence to a Gaussian process. This guarantees\nvalid inference with simultaneous confidence bands across the entire functional\ndomain. Through extensive simulations, we show that DR-FoS achieves robust\nperformance under a wide range of model specifications. Finally, we illustrate\nthe utility of DR-FoS in a real-world application, analyzing functional\noutcomes to uncover meaningful causal insights in the SHARE (Survey of Health,\nAging and Retirement in Europe) dataset.",
        "In this work, we explore the global existence of strong solutions for a class\nof partially diffusive hyperbolic systems within the framework of critical\nhomogeneous Besov spaces. Our objective is twofold: first, to extend our recent\nfindings on the local existence presented in J.-P. Adogbo and R. Danchin. Local\nwell-posedness in the critical regularity setting for hyperbolic systems with\npartial diffusion. arXiv:2307.05981, 2024, and second, to refine and enhance\nthe analysis of Kawashima (S. Kawashima. Systems of a hyperbolic parabolic type\nwith applications to the equations of magnetohydrodynamics. PhD thesis, Kyoto\nUniversity, 1983).\n  To address the distinct behaviors of low and high frequency regimes, we\nemploy a hybrid Besov norm approach that incorporates different regularity\nexponents for each regime. This allows us to meticulously analyze the\ninteractions between these regimes, which exhibit fundamentally different\ndynamics.\n  A significant part of our methodology is based on the study of a Lyapunov\nfunctional, inspired by the work of Beauchard and Zuazua (K. Beauchard and E.\nZuazua. Large time asymptotics for partially dissipative hyperbolic system.\nArch. Rational Mech. Anal, 199:177-227, 2011.) and recent contributions (T.\nCrin-Barat and R. Danchin. Partially dissipative hyperbolic systems in the\ncritical regularity setting: the multi-dimensional case. J. Math. Pures Appl.\n(9), 165:1-41, 2022). To effectively handle the high-frequency components, we\nintroduce a parabolic mode with better smoothing properties, which plays a\ncentral role in our analysis.\n  Our results are particularly relevant for important physical systems, such as\nthe magnetohydrodynamics (MHD) system and the Navier-Stokes-Fourier equations.",
        "Low-luminosity Active Galactic Nuclei (AGN) are believed to be surrounded by\na collisionless, highly magnetized accretion flow. As a result,\nParticle-in-Cell simulations are the best tools to study the immediate vicinity\nof the event horizons of these supermassive black holes. We present a GPU-based\ngeneral relativistic particle-in-cell (GRPIC) code framework called Aperture.\nAperture is developed in C++, with compute kernels written in CUDA and HIP to\ntake advantage of the massive acceleration modern GPUs enable. The code is\norganized in a fully modular way, allowing easy extensions to new physics\nproblems. In this paper, we describe in detail the particle pusher, field\nsolver, and charge-conserving current deposition algorithms employed in\nAperture, and present test cases to validate their correctness. Then, we apply\nthe code to study spark gaps and plasma injection in black hole magnetospheres.\nWe find that the apparent location and time-evolution of the gap depend on the\nobserver. Our results reconcile the previous conflicting findings from 1D and\n2D simulations in the literature.",
        "Stochastic first-order methods for empirical risk minimization employ\ngradient approximations based on sampled data in lieu of exact gradients. Such\nconstructions introduce noise into the learning dynamics, which can be\ncorrected through variance-reduction techniques. There is increasing evidence\nin the literature that in many modern learning applications noise can have a\nbeneficial effect on optimization and generalization. To this end, the recently\nproposed variance-reduction technique, alpha-SVRG [Yin et al., 2023] allows for\nfine-grained control of the level of residual noise in the learning dynamics,\nand has been reported to empirically outperform both SGD and SVRG in modern\ndeep learning scenarios. By focusing on strongly convex environments, we first\nprovide a unified convergence rate expression for alpha-SVRG under fixed\nlearning rate, which reduces to that of either SGD or SVRG by setting alpha=0\nor alpha=1, respectively. We show that alpha-SVRG has faster convergence rate\ncompared to SGD and SVRG under suitable choice of alpha. Simulation results on\nlinear regression validate our theory.",
        "We consider the dynamics of pore-driven polymer translocation through a\nnanopore to semi-infinite space when the chain is initially confined and\nequilibrated in a narrow channel. To this end, we use Langevin dynamics (LD)\nsimulations and iso-flux tension propagation (IFTP) theory to characterize\nlocal and global dynamics of the translocating chain. The dynamics of the\nprocess can be described by the IFTP theory in very good agreement with the LD\nsimulations for all values of confinement in the channel. The theory reveals\nthat for channels with size comparable to or less than the end-to-end distance\nof the unconfined chain, in which the blob theory works, the scaling form of\nthe translocation time depends on both the chain contour length as well as the\nchannel width. %originating from the confinement of the spatial fluctuations of\nthe chain inside the channel. Conversely, for a very narrow channel the\ntranslocation time only depends on the chain contour length and is similar to\nthat of a rod due to the absence of spatial chain fluctuations."
      ]
    }
  },
  {
    "id":2412.13126,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge",
    "start_abstract":"Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models"
      ],
      "abstract":[
        "Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Top eigenvalue statistics of diluted Wishart matrices",
        "Quantum oscillations in a dipolar excitonic insulator",
        "Comparison theorems for the minimum eigenvalue of a random\n  positive-semidefinite matrix",
        "Emergent supercounterfluid and quantum phase diagram of two-component\n  interacting bosons in one-dimensional optical lattice",
        "Multivariate spatial models for small area estimation of\n  species-specific forest inventory parameters",
        "Bayesian optimization of electron energy from laser wakefield\n  accelerator",
        "Quantum model reduction for continuous-time quantum filters",
        "Meson Mixing Bounds on $Z^{\\prime}$ Mass in the Alignment Limit:\n  Establishing the Phenomenological Viability of the 331 Model",
        "Right-censored models on massive data",
        "A spectral boundary element method for acoustic interference problems",
        "Variational quantum thermalizers based on weakly-symmetric nonunitary\n  multi-qubit operations",
        "AI-assisted hyper-dimensional broadband quantum memory with efficiency\n  above 90% in warm atoms",
        "The putative center in NGC 1052",
        "Hopfological invariants for tame subextensions",
        "Detecting entanglement in any measurement using quantum networks",
        "Deciphering the dual chemotaxis strategy of bacteria in porous media",
        "A \"Black Hole Star\" Reveals the Remarkable Gas-Enshrouded Hearts of the\n  Little Red Dots",
        "On a Conjecture of Yui and Zagier II",
        "Solid-state dewetting of axisymmetric thin film on axisymmetric\n  curved-surface substrates: modeling and simulation",
        "Crosstalk analysis in single hole-spin qubits within highly anisotropic\n  g-tensors",
        "A Unified View of Optimal Kernel Hypothesis Testing",
        "Universal Quantum Computation with the $S_3$ Quantum Double: A\n  Pedagogical Exposition",
        "Embodying Newtonian Mechanics",
        "A Deep-Unfolding-Optimized Coordinate-Descent Data-Detector ASIC for\n  mmWave Massive MIMO",
        "An Empirically-parametrized Spatio-Temporal Extended-SIR Model for\n  Combined Dilution and Vaccination Mitigation for Rabies Outbreaks in Wild\n  Jackals",
        "The Type Ia Supernova and AGB-Regulated Interstellar Medium of Massive\n  Galaxies",
        "Revision of the linear stability paradox for known bounded shear flows",
        "Numerical analysis of variational-hemivariational inequalities with\n  applications in contact mechanics",
        "Matrix weighted inequalities for fractional type integrals associated to\n  operators with new classes of weights"
      ],
      "abstract":[
        "Using the replica method, we compute analytically the average largest\neigenvalue of diluted covariance matrices of the form $\\mathbf{J} =\n\\mathbf{X}^T \\mathbf{X}$, where $\\mathbf{X}$ is a $N\\times M$ sparse data\nmatrix, in the limit of large $N,M$ with fixed ratio. We allow for random\nnon-zero weights, provided they lead to an isolated largest eigenvalue. By\nformulating the problem as the optimisation of a quadratic Hamiltonian\nconstrained to the $N$-sphere at low temperatures, we derive a set of recursive\ndistributional equations for auxiliary probability density functions, which can\nbe efficiently solved using a population dynamics algorithm. The average\nlargest eigenvalue is identified with a Lagrange parameter that governs the\nconvergence of the algorithm. We find excellent agreement between our\nanalytical results and numerical results obtained from direct diagonalisation.",
        "Quantum oscillations in magnetization or resistivity are a defining feature\nof metals subject to an external magnetic field. The phenomenon is generally\nnot expected in insulators without a Fermi surface. The observations of quantum\noscillations in Kondo insulating materials have provided a rare counterexample\nand attracted much theoretical interest. However, the magnetic oscillations in\ncorrelated insulators remain poorly understood. Here we report the observations\nof resistivity quantum oscillations in an excitonic insulator realized in\nCoulomb-coupled electron-hole double layers with gate-tunability that allows\nthe phenomenon to be explored in a more controllable fashion than in bulk\nmaterials. When the cyclotron energy of the electrons or holes is tuned to be\ncomparable to or larger than the exciton binding energy, recurring transitions\nbetween excitonic insulators and electron-hole decoupled quantum Hall states\nare observed. Compressibility measurements show an oscillatory exciton binding\nenergy as a function of magnetic field and electron-hole pair density. Coulomb\ndrag measurements further reveal the formation of excitons with finite angular\nmomentum. Our results are qualitatively captured by mean-field theory\ncalculations. The study demonstrates a new platform for studying quantum\noscillations in correlated insulators.",
        "This paper establishes a new comparison principle for the minimum eigenvalue\nof a sum of independent random positive-semidefinite matrices. The principle\nstates that the minimum eigenvalue of the matrix sum is controlled by the\nminimum eigenvalue of a Gaussian random matrix that inherits its statistics\nfrom the summands. This methodology is powerful because of the vast arsenal of\ntools for treating Gaussian random matrices. As applications, the paper\npresents short, conceptual proofs of some old and new results in\nhigh-dimensional statistics. It also settles a long-standing open question in\ncomputational linear algebra about the injectivity properties of very sparse\nrandom matrices.",
        "Motivated by a recent experiment that realizes nearest-neighbor dipolar\ncouplings in an optical lattice [C. Lagoin, $\\textit{et al.}$, Nature\n$\\textbf{609}$, 485 (2022)], we study a one-dimensional version of the\ntwo-component extended Bose-Hubbard model via the density-matrix\nrenormalization group method. By using the nearest-neighbor and on-site\ninteraction parameters from the experiment, we start by mapping the quantum\nphase diagram in the hopping parameters $t_{A}\\mbox{-}t_{B}$ plane with boson\ndensities $\\rho_{A}=\\rho_{B}=1\/2$. In addition to the density wave phase\nreported in the experiment, we find several regimes of superfluidity when one\nor two hopping parameters are large enough, and interestingly there is a\nsupercounterfluid phase at moderate and comparable hopping parameters. The\nuniversality classes of these phase transitions are analyzed from the\ncorrelation functions, excitation gaps, and entanglement entropy. In\nparticular, a Berezinskii-Kosterlitz-Thouless type is recognized several\ngapped-to-gapless transitions. In addition, we also study the quantum phase\ntransitions when varying $\\rho_{B}$ from 0 to 1 while keeping $\\rho_A = 1\/2$.\nWe identify a supersolid phase in a wide range of $1\/2<\\rho_B<1$. Our work\npaves the way for realizing exotic many-body phases in cold atom experiments\nupon proper tuning of experimental parameters.",
        "National Forest Inventories (NFIs) provide statistically reliable information\non forest resources at national and other large spatial scales. As forest\nmanagement and conservation needs become increasingly complex, NFIs are being\ncalled upon to provide forest parameter estimates at spatial scales smaller\nthan current design-based estimation procedures can provide. This is\nparticularly true when estimates are desired by species or species groups. Here\nwe propose a multivariate spatial model for small area estimation of\nspecies-specific forest inventory parameters. The hierarchical Bayesian\nmodeling framework accounts for key complexities in species-specific forest\ninventory data, such as zero-inflation, correlations among species, and\nresidual spatial autocorrelation. Importantly, by fitting the model directly to\nthe individual plot-level data, the framework enables estimates of\nspecies-level forest parameters, with associated uncertainty, across any\nuser-defined small area of interest. A simulation study revealed minimal bias\nand higher accuracy of the proposed model-based approach compared to the\ndesign-based estimator and a non-parametric k-nearest neighbor (kNN) estimator.\nWe applied the model to estimate species-specific county-level aboveground\nbiomass for the 20 most abundant tree species in the southern United States\nusing Forest Inventory and Analysis (FIA) data. Biomass estimates from the\nproposed model had high correlations with design-based estimates and kNN\nestimates. Importantly, the proposed model provided large gains in precision\nacross all 20 species. On average across species, 91.5% of county-level biomass\nestimates had higher precision compared to the design-based estimates. The\nproposed framework improves the ability of NFI data users to generate\nspecies-level forest parameter estimates with reasonable precision at\nmanagement-relevant spatial scales.",
        "We employ Bayesian optimization combined with three-dimensional\nparticle-in-cell simulations to identify the optimal laser and plasma\nparameters that, for a given laser pulse energy, maximize the cut-off energy of\nan electron beam accelerated via laser wakefield acceleration. A Gaussian laser\ndriver with a matched spot size and amplitude is assumed, interacting with both\na uniform-density plasma and a preformed plasma channel of matched radius. To\ninterpret the simulation results quantitatively, we derive novel analytical\nexpressions for predicting the maximum electron energy and acceleration length,\ntaking into account the diffraction and energy depletion of the laser pulse.\nAdditionally, we discuss the potential scalability of the optimal parameters\nfor high-energy lasers.",
        "The use of quantum stochastic models is widespread in dynamical reduction,\nsimulation of open systems, feedback control and adaptive estimation. In many\napplications only part of the information contained in the filter's state is\nactually needed to reconstruct the target observable quantities; thus, filters\nof smaller dimensions could be in principle implemented to perform the same\ntask.In this work, we propose a systematic method to find, when possible,\nreduced-order quantum filters that are capable of exactly reproducing the\nevolution of expectation values of interest. In contrast with existing\nreduction techniques, the reduced model we obtain is exact and in the form of a\nBelavkin filtering equation, ensuring physical interpretability.This is\nattained by leveraging tools from the theory of both minimal realization and\nnon-commutative conditional expectations. The proposed procedure is tested on\nprototypical examples, laying the groundwork for applications in quantum\ntrajectory simulation and quantum feedback control.",
        "We perform a systematic study of flavor-changing neutral currents (FCNCs) in\nthe 331 model with right-handed neutrinos (331RHNs), analyzing constraints on\nthe $Z^\\prime$ boson mass from $K$-, $D$-, $B_d$-, and $B_s$-meson\noscillations. By explicitly incorporating scalar sector dynamics and quark\nrotation ambiguities ($V_L^{u,d}$), we demonstrate that $Z^\\prime$ mass limits\ndepend critically on the parametrization of Cabibbo-Kobayashi-Maskawa (CKM)\nmatrix factors. Three scenarios are explored: (i) $V_L^u =\nV_\\text{CKM}^\\dagger$ (FCNCs restricted to $D$-mesons), (ii) $V_L^d =\nV_\\text{CKM}$ (dominant $B_s$ constraints), and (iii) a hybrid mixing pattern.\nStrikingly, scenario (i) reduces the $Z^\\prime$ mass bound to $M_{Z^\\prime}\n\\gtrsim 600\\;\\text{GeV}$-two orders of magnitude below literature values-by\nleveraging large experimental uncertainties in $D$-$\\bar{D}$ oscillations.\nConversely, scenario (ii) requires $M_{Z^\\prime} \\gtrsim 165\\;\\text{TeV}$ due\nto stringent $B_s$ data. We further establish the alignment limit\n$\\cos(\\phi+\\varphi) = 0$ for the SM-like Higgs, showing its viability depends\non $V_L^{u,d}$ configurations, with $B_s$ systems enforcing\n$|\\cos(\\phi+\\varphi)| < 0.01$ in down-sector FCNC scenarios. Our analysis\nreveals that strategic choices of quark mixing matrices can suppress FCNC\nvisibility, reconciling the 331 framework with flavor data without ultra-heavy\n$Z^\\prime$ bosons. This work provides the first unified treatment of SM-like\nHiggs- and $Z^\\prime$-mediated FCNCs in 331 models, identifying viable\nparameter spaces for collider phenomenology.",
        "This article considers the automatic selection problem of the relevant\nexplanatory variables in a right-censored model on a massive database. We\npropose and study four aggregated censored adaptive LASSO estimators\nconstructed by dividing the observations in such a way as to keep the\nconsistency of the estimator of the survival curve. We show that these\nestimators have the same theoretical oracle properties as the one built on the\nfull database. Moreover, by Monte Carlo simulations we obtain that their\ncalculation time is smaller than that of the full database. The simulations\nconfirm also the theoretical properties. For optimal tuning parameter\nselection, we propose a BIC-type criterion.",
        "In this paper we consider high-frequency acoustic transmission problems with\njumping coefficients modelled by Helmholtz equations. The solution then is\nhighly oscillatory and, in addition, may be localized in a very small vicinity\nof interfaces (whispering gallery modes). For the reliable numerical\napproximation a) the PDE is tranformed in a classical single trace integral\nequation on the interfaces and b) a spectral Galerkin boundary element method\nis employed for its solution. We show that the resulting integral equation is\nwell posed and analyze the convergence of the boundary element method for the\nparticular case of concentric circular interfaces. We prove a condition on the\nnumber of degrees of freedom for quasi-optimal convergence. Numerical\nexperiments confirm the efficiency of our method and the sharpness of the\ntheoretical estimates.",
        "We propose incorporating multi-qubit nonunitary operations in Variational\nQuantum Thermalizers (VQTs). VQTs are hybrid quantum-classical algorithms that\ngenerate the thermal (Gibbs) state of a given Hamiltonian, with applications in\nquantum algorithms and simulations. However, current algorithms struggle at\nintermediate temperatures, where the target state is nonpure but exhibits\nentanglement. We devise multi-qubit nonunitary operations that harness weak\nsymmetries and thereby improve the performance of the algorithm. Utilizing\ndissipation engineering, we create these nonunitary multi-qubit operations\nwithout the need for measurements or additional qubits. To train the ansatz, we\ndevelop and benchmark novel methods for entropy estimation of quantum states,\nexpanding the toolbox for quantum state characterization. We demonstrate that\nour approach can prepare thermal states of paradigmatic spin models at all\ntemperatures. Our work thus creates new opportunities for simulating open\nquantum many-body systems.",
        "High-dimensional broadband quantum memory significantly expands quantum\ninformation processing capabilities, but the memory efficiency becomes\ninsufficient when extended to high dimensions. We demonstrate an efficient\nquantum memorize for hyper-dimensional photons encoded with orbital angular\nmomentum (OAM) and spin angular momentum (SAM). OAM information is encoded from\n-5 to +5, combined with spin angular momentum encoding, enabling up to 22\ndimensions. To ensure high memory efficiency, an artificial intelligent\nalgorithm, a modified Differential Evolution (DE) algorithm using Chebyshev\nsampling, is developed to obtain a perfect signal-control waveform matching.\nMemory efficiency is experimentally achieved 92% for single-mode Gaussian\nsignal, 91% for information dimension of 6 and 80% for dimensional number to\n22. The fidelity is achieved up to 99% for single-mode Gaussian signal, 96% for\nOAM information and 97% for SAM one, which is far beyond no-cloning limitation.\nOur results demonstrate superior performance and potential applications in\nhigh-dimensional quantum information processing. This achievement provides a\ncrucial foundation for future quantum communication and quantum computing.",
        "Many active galaxies harbor powerful relativistic jets, however, the detailed\nmechanisms of their formation and acceleration remain poorly understood. To\ninvestigate the area of jet acceleration and collimation with the highest\navailable angular resolution, we study the innermost region of the bipolar jet\nin the nearby low-ionization nuclear emission-line region (LINER) galaxy NGC\n1052. We combined observations of NGC 1052 taken with VLBA, GMVA, and EHT over\none week in the spring of 2017. For the first time, NGC 1052 was detected with\nthe EHT, providing a size of the central region in-between both jet bases of\n250 RS (Schwarzschild radii) perpendicular to the jet axes. This size estimate\nsupports previous studies of the jets expansion profile which suggest two\nbreaks of the profile at around 300 RS and 10000 RS distances to the core.\nFurthermore, we estimated the magnetic field to be 1.25 Gauss at a distance of\n22 {\\mu}as from the central engine by fitting a synchrotron-self absorption\nspectrum to the innermost emission feature, which shows a spectral turn-over at\nabout 130 GHz. Assuming a purely poloidal magnetic field, this implies an upper\nlimit on the magnetic field strength at the event horizon of 26000 Gauss, which\nis consistent with previous measurements. The complex, low-brightness,\ndouble-sided jet structure in NGC 1052 makes it a challenge to detect the\nsource at millimeter (mm) wavelengths. However, our first EHT observations have\ndemonstrated that detection is possible up to at least 230 GHz. This study\noffers a glimpse through the dense surrounding torus and into the innermost\ncentral region, where the jets are formed. This has enabled us to finally\nresolve this region and provide improved constraints on its expansion and\nmagnetic field strength.",
        "Let H be a finite dimensional Hopf algebra over a field K. In this paper, we\nstudy when an H-extension becomes a tame H-extension by calculating\nHopfological homology and Hopf-cyclic homology. In the (derived) category of\nH'-comodules for a Hopf algebra H', we take Hopf subalgebra H of H' and a\ncertain order A of H. We see the behavior of Hopfological homology for a tame\nA-subextension S\/R in terms of the surjectivity of trace map and of cyclic\nmodules, which induce Hopf-cyclic homology, for Hopf-Galois extensions with H\nin terms of relative Hopf modules.",
        "Entanglement is a key resource to demonstrate quantum advantage over\nclassical strategies. Entanglement in quantum states is one of the most\nwell-explored areas in quantum physics. However, a rigorous approach to\nunderstanding and detecting entanglement in composite quantum measurements is\nlacking. In this work, we focus on composite quantum measurements and classify\nthem into two classes: entangled and separable measurements. As done for\nquantum states, we define analogously a notion of witness that can be used to\ndetect entanglement in composite quantum measurements. Here, one does not need\nto trust the measurement to witness its entanglement but must trust the quantum\nstates. We then further extend this approach to show that any entangled\nmeasurement provides an advantage in network quantum steering without inputs,\nalso known as swap steering. Consequently, this provides a way to witness\nentanglement in any quantum measurement in a one-sided device-independent way.\nFinally, we consider the star network scenario and show that any rank-one\nprojective entangled quantum measurement gives a quantum advantage. Thus, one\ncan detect the entanglement in any rank-one projective measurement in a\ndevice-independent way.",
        "Chemotaxis of bacterial swimmers that move in a run-and-turn pattern is well\nstudied in uniform bulk fluid. It is primarily based on modulating the run time\nin dependence on the swimming direction with respect to the source of\nchemoattractant (run time bias). Here, we provide evidence that the\nlophotrichously flagellated soil bacterium Pseudomonas putida may also perform\nchemotaxis in porous media where the free path length is severely restricted.\nBesides the classical run time bias, we identify a second chemotactic strategy:\nthe change in swimming direction upon a turn event is adjusted, so that the\ndirection of the next run phase is biased towards the source of chemoattractant\n(turn angle bias). Agent based simulations, based on the experimentally\nobserved statistical properties of the swimming pattern, indicate that turn\nangle bias is the predominant chemotaxis strategy of bacteria in porous\nenvironments.",
        "The physical processes that led to the formation of billion solar mass black\nholes within the first 700 million years of cosmic time remain a puzzle.\nSeveral theoretical scenarios have been proposed to seed and rapidly grow black\nholes, but direct observations of these mechanisms remain elusive. Here we\npresent a source 660 million years after the Big Bang that displays singular\nproperties: among the largest Hydrogen Balmer breaks reported at any redshift,\nbroad multi-peaked H$\\beta$ emission, and Balmer line absorption in multiple\ntransitions. We model this source as a \"black hole star\" (BH*) where the Balmer\nbreak and absorption features are a result of extremely dense, turbulent gas\nforming a dust-free \"atmosphere\" around a supermassive black hole. This source\nmay provide evidence of an early black hole embedded in dense gas -- a\ntheoretical configuration proposed to rapidly grow black holes via\nsuper-Eddington accretion. Radiation from the BH* appears to dominate almost\nall observed light, leaving limited room for contribution from its host galaxy.\nWe demonstrate that the recently discovered \"Little Red Dots\" (LRDs) with\nperplexing spectral energy distributions can be explained as BH*s embedded in\nrelatively brighter host galaxies. This source provides evidence that black\nhole masses in the LRDs may be over-estimated by orders of magnitude -- the BH*\nis effectively dust-free contrary to the steep dust corrections applied while\nmodeling LRDs, and the physics that gives rise to the complex line shapes and\nluminosities may deviate from assumptions underlying standard scaling\nrelations.",
        "Yui and Zagier made some fascinating conjectures on the factorization on the\nnorm of the difference of Weber class invariants $ f(\\mathfrak a_1) -\nf(\\mathfrak a_2)$ based on their calculation in \\cite{YZ}. Here $\\mathfrak a_i$\nbelong two diferent ideal classes of discrimants $D_i$ in imagainary quadratic\nfields $\\mathbb{Q}(\\sqrt{D_i})$. In \\cite{LY}, we proved these conjectures and\ntheir generalizations when $(D_1, D_2) =1$ using the so-called big CM value\nformula of Borcherds lifting. In this sequel, we prove the conjectures when\n$\\mathbb{Q}(\\sqrt{D_1}) =\\mathbb{Q}(\\sqrt{D_2})$ using the so-called small CM\nvalue formula. In addition, we give a precise factorization formula for the\nresultant of two different Weber class invariant polynomials for distinct\norders.",
        "In this work, we consider the solid-state dewetting of an axisymmetric thin\nfilm on a curved-surface substrate, with the assumption that the substrate\nmorphology is also axisymmetric. Under the assumptions of axisymmetry, the\nsurface evolution problem on a curved-surface substrate can be reduced to a\ncurve evolution problem on a static curved substrate. Based on the\nthermodynamic variation of the anisotropic surface energy, we thoroughly derive\na sharp-interface model that is governed by anisotropic surface diffusion,\nalong with appropriate boundary conditions. The continuum system satisfies the\nlaws of energy decay and volume conservation, which motivates the design of a\nstructure-preserving numerical algorithm for simulating the mathematical model.\nBy introducing a symmetrized surface energy matrix, we derive a novel\nsymmetrized variational formulation. Then, by carefully discretizing the\nboundary terms of the variational formulation, we establish an unconditionally\nenergy-stable parametric finite element approximation of the axisymmetric\nsystem. By applying an ingenious correction method, we further develop another\nstructure-preserving method that can preserve both the energy stability and\nvolume conservation properties. Finally, we present extensive numerical\nexamples to demonstrate the convergence and structure-preserving properties of\nour proposed numerical scheme. Additionally, several interesting phenomena are\nexplored, including the migration of 'small' particles on a curved-surface\nsubstrate generated by curves with positive or negative curvature, pinch-off\nevents, and edge retraction.",
        "Spin qubits based on valence band hole states are highly promising for\nquantum information processing due to their strong spin-orbit coupling and\nultrafast operation speed. As these systems scale up, achieving high-fidelity\nsingle-qubit operations becomes essential. However, mitigating crosstalk\neffects from neighboring qubits in larger arrays, particularly for anisotropic\nqubits with strong spin-orbit coupling, presents a significant challenge. We\ninvestigate the impact of crosstalk on qubit fidelities during single-qubit\noperations and derive an analytical equation that serves as a synchronization\ncondition to eliminate crosstalk in anisotropic media. Our analysis proposes\noptimized driving field conditions that can robustly synchronize Rabi\noscillations and minimize crosstalk, showing a strong dependence on qubit\nanisotropy and the orientation of the external magnetic field. Taking\nexperimental data into our analysis, we identify a set of parameter values that\nenable nearly crosstalk-free single-qubit gates, thereby paving the way for\nscalable quantum computing architectures.",
        "This paper provides a unifying view of optimal kernel hypothesis testing\nacross the MMD two-sample, HSIC independence, and KSD goodness-of-fit\nframeworks. Minimax optimal separation rates in the kernel and $L^2$ metrics\nare presented, with two adaptive kernel selection methods (kernel pooling and\naggregation), and under various testing constraints: computational efficiency,\ndifferential privacy, and robustness to data corruption. Intuition behind the\nderivation of the power results is provided in a unified way accross the three\nframeworks, and open problems are highlighted.",
        "Non-Abelian topological order (TO) enables topologically protected quantum\ncomputation with its anyonic quasiparticles. Recently, TO with $S_3$ gauge\nsymmetry was identified as a sweet spot -- simple enough to emerge from\nfinite-depth adaptive circuits yet powerful enough to support a universal\ntopological gate-set. In these notes, we review how anyon braiding and\nmeasurement in $S_3$ TO are primitives for topological quantum computation and\nwe explicitly demonstrate universality. These topological operations are made\nconcrete in the $S_3$ quantum double lattice model, aided by the introduction\nof a generalized ribbon operator. This provides a roadmap for near-term quantum\nplatforms.",
        "Wellness and mindfulness act as buzzwords these days, often seen as separate\nfrom physics. Yet we know they are important, and everything is related to\nphysics! In this article, we will consider a few simple classroom activities\nthat can both help students internalize the basic physics of forces and motion\nand also help facilitate well-being in our classes.",
        "We present a 22 nm FD-SOI (fully depleted silicon-on-insulator)\napplication-specific integrated circuit (ASIC) implementation of a novel\nsoft-output Gram-domain block coordinate descent (GBCD) data detector for\nmassive multi-user (MU) multiple-input multiple-output (MIMO) systems. The ASIC\nsimultaneously addresses the high throughput requirements for millimeter wave\n(mmWave) communication, stringent area and power budget per subcarrier in an\northogonal frequency-division multiplexing (OFDM) system, and error-rate\nperformance challenges posed by realistic mmWave channels. The proposed GBCD\nalgorithm utilizes a posterior mean estimate (PME) denoiser and is optimized\nusing deep unfolding, which results in superior error-rate performance even in\nscenarios with highly correlated channels or where the number of user equipment\n(UE) data streams is comparable to the number of basestation (BS) antennas. The\nfabricated GBCD ASIC supports up to 16 UEs transmitting QPSK to 256-QAM symbols\nto a 128-antenna BS, and achieves a peak throughput of 7.1 Gbps at 367 mW. The\ncore area is only 0.97 mm$^2$ thanks to a reconfigurable array of processing\nelements that enables extensive resource sharing. Measurement results\ndemonstrate that the proposed GBCD data-detector ASIC achieves best-in-class\nthroughput and area efficiency.",
        "The transmission of zoonotic diseases between animals and humans poses an\nincreasing threat. Rabies is a prominent example with various instances\nglobally, facilitated by a surplus of meso-predators (commonly, facultative\nsynanthropic species e.g., golden jackals [Canis aureus, hereafter jackals])\nthanks to the abundance of anthropogenic resources leading to dense populations\nclose to human establishments. To mitigate rabies outbreaks and prevent human\ninfections, authorities target the jackal which is the main rabies vector in\nmany regions, through the dissemination of oral vaccines in known jackals'\nactivity centers, as well as opportunistic culling to reduce population\ndensity. Because dilution (i.e., culling) is not selective towards sick or\nun-vaccinated individuals, these two complementary epizootic intervention\npolicies (EIPs) can interfere with each other. Nonetheless, there is only\nlimited examination of the interactive effectiveness of these EIPs and their\npotential influence on rabies epizootic spread dynamics, highlighting the need\nto understand these measures and the spread of rabies in wild jackals. In this\nstudy, we introduce a novel spatio-temporal extended-SIR\n(susceptible-infected-recovered) model with a graph-based spatial framework for\nevaluating mitigation efficiency. We implement the model in a case study using\na jackal population in northern Israel, and using spatial and movement data\ncollected by Advanced Tracking and Localization of Animals in real-life Systems\n(ATLAS) telemetry. An agent-based simulation approach allows us to explore\nvarious biologically-realistic scenarios, and assess the impact of different\nEIPs configurations. Our model suggests that under biologically-realistic\nunderlying assumptions and scenarios, the effectiveness of both EIPs is not\ninfluenced much by the jackal population size but is sensitive to their\ndispersal between activity centers.",
        "Observations and theory suggest that Type Ia supernovae (SNIa) heating and\nmass loss from asymptotic giant branch (AGB) stars play a crucial role in the\ninterstellar medium (ISM) of massive galaxies. We perform 3D hydrodynamic\nsimulations of the central few kiloparsecs of massive galaxies, including\nradiative cooling and mass and energy injection from AGB winds and SNIa\n(resolving each SNIa remnant, a few $\\times10~\\mathrm{pc}$ in size), excluding\nblack hole feedback. We study systems with different initial core thermodynamic\nprofiles, focusing on NGC 1399. Our simulations reproduce its observed density\nand entropy profiles well. Over $100~\\mathrm{Myr}$, two steady-state profiles\nemerge, depending on the inner circumgalactic medium (CGM) pressure and the\nratio of Ia heating to cooling: (i) if SNIa heating is less than cooling, a\ncooling flow develops; (ii) if SNIa heating is comparable to or exceeds\ncooling, SNIa heating drives a slow subsonic outflow of AGB ejecta, with black\nhole accretion at small radii. This outflow, pressure-confined by the CGM,\nadapts the ISM to the CGM properties: a low entropy CGM results in a dense, low\nentropy ISM with higher black hole accretion, while a high entropy CGM leads to\na less dense, high entropy ISM with lower accretion. This suggests that the\nAGB-SNIa regulated ISM connects CGM and galaxy scales, potentially influencing\nblack hole feedback in massive halos. Approximate methods of modeling Ia\nheating, such as clustered SNIa and smoothly distributed heating, produce\nunrealistic ISM profiles over $100~\\mathrm{Myr}$, highlighting the importance\nof resolving SNIa in simulations.",
        "The well-known paradox of linear stability for the some bounded shear flows\nis not solved up to now and is bypassed on the basis of the non-linear\nmechanisms consideration. We prove that it is arising only due to an idealized\nassumption of an exact space periodicity for the small hydrodynamic\nperturbations. When finite non-zero viscosity is taken into account only\nquasi-periodic boundary conditions must be used. The conditions of linear\ninstability to the Hagen-Poiseuille flow and to the plane Couette flow are\nobtained.",
        "Variational-hemivariational inequalities are an important mathematical\nframework for nonsmooth problems. The framework can be used to study\napplication problems from physical sciences and engineering that involve\nnon-smooth and even set-valued relations, monotone or non-monotone, among\nphysical quantities. Since no analytic solution formulas are expected for\nvariational-hemivariational inequalities from applications, numerical methods\nare needed to solve the problems. This paper focuses on numerical analysis of\nvariational-hemivariational inequalities, reporting new results as well as\nsurveying some recent published results in the area. A general convergence\nresult is presented for Galerkin solutions of the inequalities under minimal\nsolution regularity conditions available from the well-posedness theory, and\nC\\'{e}a's inequalities are derived for error estimation of numerical solutions.\nThe finite element method and the virtual element method are taken as examples\nof numerical methods, optimal order error estimates for the linear element\nsolutions are derived when the methods are applied to solve three\nrepresentative contact problems under certain solution regularity assumptions.\nNumerical results are presented to show the performance of both the finite\nelement method and the virtual element method, including numerical convergence\norders of the numerical solutions that match the theoretical predictions.",
        "Let $e^{-tL}$ be a analytic semigroup generated by $-L$, where $L$ is a\nnon-negative self-adjoint operator on $L^2(\\mathbb{R}^d)$. Assume that the\nkernels of $e^{-tL}$, denoted by $p_t(x,y)$, only satisfy the upper bound: for\nall $N>0$, there are constants $c,C>0$ such that \\begin{align}\\label{upper\nbound}\n|p_t(x,y)|\\leq\\frac{C}{t^{d\/2}}e^{-\\frac{|x-y|^2}{ct}}\\Big(1+\\frac{\\sqrt{t}}{\\rho(x)}+\n\\frac{\\sqrt{t}}{\\rho(y)}\\Big)^{-N} \\end{align} holds for all\n$x,y\\in\\mathbb{R}^d$ and $t>0$. We first establish the quantitative matrix\nweighted inequalities for fractional type integrals associated to $L$ with new\nclasses of matrix weights, which are nontrivial extension of the results\nestablished by Li, Rahm and Wick [23]. Next, we give new two-weight bump\nconditions with Young functions satisfying wider conditions for fractional type\nintegrals associated to $L$, which cover the result obtained by Cruz-Uribe,\nIsralowitz and Moen [6]. We point out that the new classes of matrix weights\nand bump conditions are larger and weaker than the classical ones given in [17]\nand [6], respectively. As applications, our results can be applied to settings\nof magnetic Schr\\\"{o}dinger operator, Laguerre operators, etc."
      ]
    }
  },
  {
    "id":2412.13126,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models",
    "start_abstract":"Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge"
      ],
      "abstract":[
        "Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Quasi-periodic oscillations of GHz-band polarization in a black hole",
        "Principles and Metrics of Extreme Learning Machines Using a Highly\n  Nonlinear Fiber",
        "Molecular Mechanism Enabling Linearity and Symmetry in Neuromorphic\n  Elements",
        "VideoMerge: Towards Training-free Long Video Generation",
        "A redescription mining framework for post-hoc explaining and relating\n  deep learning models",
        "Quantum Chebyshev Probabilistic Models for Fragmentation Functions",
        "AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal\n  Understanding",
        "Learning Control of Neural Sound Effects Synthesis from Physically\n  Inspired Models",
        "Predicting Steady-State Behavior in Complex Networks with Graph Neural\n  Networks",
        "LanP: Rethinking the Impact of Language Priors in Large Vision-Language\n  Models",
        "The Vlasov Bivector: A Parameter-Free Approach to Vlasov Kinematics",
        "Beyond the Lungs: Extending the Field of View in Chest CT with Latent\n  Diffusion Models",
        "Pulmonary Tuberculosis Edge Diagnosis System Based on MindSpore\n  Framework: Low-cost and High-precision Implementation with Ascend 310 Chip",
        "Generalized Decision Focused Learning under Imprecise\n  Uncertainty--Theoretical Study",
        "How does Radiation Reaction Affect Relativistic Magnetized Shocks\n  Emission",
        "PISN 2018ibb: radioactive emission of [O III] lines",
        "Uniform estimates for elliptic equations with Carath\\'eodory\n  nonlinearities at the interior and on the boundary",
        "VAQUUM: Are Vague Quantifiers Grounded in Visual Data?",
        "An artificially intelligent magnetic resonance spectroscopy\n  quantification method: Comparison between QNet and LCModel on the cloud\n  computing platform CloudBrain-MRS",
        "Assessing the Impact of the Quality of Textual Data on Feature\n  Representation and Machine Learning Models",
        "Can Cross Encoders Produce Useful Sentence Embeddings?",
        "HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand\n  Rendering",
        "Certifying Pareto-Optimality in Multi-Objective Maximum Satisfiability",
        "Measurements of extreme first passage times in photon transport",
        "Cost Preserving Dependent Rounding for Allocation Problems",
        "A Survey of Direct Preference Optimization",
        "LU Decomposition and Generalized Autoone-Takagi Decomposition of Dual\n  Matrices and their Applications",
        "Orthogonal Alignment of Galaxy Group Angular Momentum with Cosmic\n  Filament Spines: An Observational Study",
        "Landau-Khalatnikov-Fradkin Transformations in Quantum Electrodynamics:\n  For Perturbation Theory and Dynamical Mass Generation"
      ],
      "abstract":[
        "Relativistic jets from accreting black holes (BHs) radiate non-thermal\nemission which is highly variable in different time scales. Magnetic fields\nanchored to a rotating BH or accretion disc accelerate and collimate jets of\nthe BH systems. Previous studies on black holes of different mass scales,\nincluding supermassive and stellar-mass black holes, only report flux\nquasi-periodic oscillations in radio, optical, X-ray and gamma-ray bands. No\nquasi-periodic variations in polarization have yet been detected in any black\nhole systems. Here, we report the first detection of GHz radio polarization\noscillations in GRS 1915+105, which harbors a spinning stellar-mass BH with a\nrelativistic jet. Our observations show that during the increasing phase of\nradio emission, linear polarization and flux exhibit similar oscillation\nperiods of $\\sim 17$ and $33$ seconds, and their variation patterns\nanti-correlate with each other. These rare, short-period oscillations in both\npolarization and flux would be important to understand instabilities and\nspecial dynamics in magnetized jets.",
        "Optical computing offers potential for ultra high-speed and low latency\ncomputation by leveraging the intrinsic properties of light. Here, we explore\nthe use of highly nonlinear optical fibers (HNLFs) as platforms for optical\ncomputing based on the concept of Extreme Learning Machines. Task-independent\nevaluations are introduced to the field for the first time and focus on the\nfundamental metrics of effective dimensionality and consistency, which we\nexperimentally characterize for different nonlinear and dispersive conditions.\nWe show that input power and fiber characteristics significantly influence the\ndimensionality of the computational system, with longer fibers and higher\ndispersion producing up to 100 principal components (PCs) at input power levels\nof 30 mW, where the PC correspond to the linearly independent dimensions of the\nsystem. The spectral distribution of the PC's eigenvectors reveals that the\nhigh-dimensional dynamics facilitating computing through dimensionality\nexpansion are located within 40~nm of the pump wavelength at 1560~nm, providing\ngeneral insight for computing with nonlinear Schr\\\"odinger equation systems.\nTask-dependent results demonstrate the effectiveness of HNLFs in classifying\nMNIST dataset images. Using input data compression through PC analysis, we\ninject MNIST images of various input dimensionality into the system and study\nthe impact of input power upon classification accuracy. At optimized power\nlevels we achieve a classification test accuracy of 88\\%, significantly\nsurpassing the baseline of 83.7\\% from linear systems. Noteworthy, we find that\nbest performance is not obtained at maximal input power, i.e. maximal system\ndimensionality, but at more than one order of magnitude lower. The same is\nconfirmed regarding the MNIST image's compression, where accuracy is\nsubstantially improved when strongly compressing the image to less than 50 PCs.",
        "For over a decade, linear and symmetric weight updates have remained the\nelusive holy grail in neuromorphic computing. Here, we unveil a kinetically\ncontrolled molecular mechanism driving a near-ideal neuromorphic element,\ncapable of precisely modulating conductance linearly across 16,500 analog\nlevels spanning four orders of magnitude. Our findings, supported by\nexperimental data and mathematical modelling, demonstrate how nonlinear\nprocesses such as nucleation can be orchestrated within small perturbation\nregimes to achieve linearity. This establishes a groundwork for routinely\nrealizing these long-sought neuromorphic features across a broad range of\nmaterial systems.",
        "Long video generation remains a challenging and compelling topic in computer\nvision. Diffusion based models, among the various approaches to video\ngeneration, have achieved state of the art quality with their iterative\ndenoising procedures. However, the intrinsic complexity of the video domain\nrenders the training of such diffusion models exceedingly expensive in terms of\nboth data curation and computational resources. Moreover, these models\ntypically operate on a fixed noise tensor that represents the video, resulting\nin predetermined spatial and temporal dimensions. Although several high quality\nopen-source pretrained video diffusion models, jointly trained on images and\nvideos of varying lengths and resolutions, are available, it is generally not\nrecommended to specify a video length at inference that was not included in the\ntraining set. Consequently, these models are not readily adaptable to the\ndirect generation of longer videos by merely increasing the specified video\nlength. In addition to feasibility challenges, long-video generation also\nencounters quality issues. The domain of long videos is inherently more complex\nthan that of short videos: extended durations introduce greater variability and\nnecessitate long-range temporal consistency, thereby increasing the overall\ndifficulty of the task. We propose VideoMerge, a training-free method that can\nbe seamlessly adapted to merge short videos generated by pretrained\ntext-to-video diffusion model. Our approach preserves the model's original\nexpressiveness and consistency while allowing for extended duration and dynamic\nvariation as specified by the user. By leveraging the strengths of pretrained\nmodels, our method addresses challenges related to smoothness, consistency, and\ndynamic content through orthogonal strategies that operate collaboratively to\nachieve superior quality.",
        "Deep learning models (DLMs) achieve increasingly high performance both on\nstructured and unstructured data. They significantly extended applicability of\nmachine learning to various domains. Their success in making predictions,\ndetecting patterns and generating new data made significant impact on science\nand industry. Despite these accomplishments, DLMs are difficult to explain\nbecause of their enormous size. In this work, we propose a novel framework for\npost-hoc explaining and relating DLMs using redescriptions. The framework\nallows cohort analysis of arbitrary DLMs by identifying statistically\nsignificant redescriptions of neuron activations. It allows coupling neurons to\na set of target labels or sets of descriptive attributes, relating layers\nwithin a single DLM or associating different DLMs. The proposed framework is\nindependent of the artificial neural network architecture and can work with\nmore complex target labels (e.g. multi-label or multi-target scenario).\nAdditionally, it can emulate both pedagogical and decompositional approach to\nrule extraction. The aforementioned properties of the proposed framework can\nincrease explainability and interpretability of arbitrary DLMs by providing\ndifferent information compared to existing explainable-AI approaches.",
        "We propose a quantum protocol for efficiently learning and sampling\nmultivariate probability distributions that commonly appear in high-energy\nphysics. Our approach introduces a bivariate probabilistic model based on\ngeneralized Chebyshev polynomials, which is (pre-)trained as an explicit\ncircuit-based model for two correlated variables, and sampled efficiently with\nthe use of quantum Chebyshev transforms. As a key application, we study the\nfragmentation functions~(FFs) of charged pions and kaons from single-inclusive\nhadron production in electron-positron annihilation. We learn the joint\ndistribution for the momentum fraction $z$ and energy scale $Q$ in several\nfragmentation processes. Using the trained model, we infer the correlations\nbetween $z$ and $Q$ from the entanglement of the probabilistic model, noting\nthat the developed energy-momentum correlations improve model performance.\nFurthermore, utilizing the generalization capabilities of the quantum Chebyshev\nmodel and extended register architecture, we perform a fine-grid multivariate\nsampling relevant for FF dataset augmentation. Our results highlight the\ngrowing potential of quantum generative modeling for addressing problems in\nscientific discovery and advancing data analysis in high-energy physics.",
        "Aligning visual features with language embeddings is a key challenge in\nvision-language models (VLMs). The performance of such models hinges on having\na good connector that maps visual features generated by a vision encoder to a\nshared embedding space with the LLM while preserving semantic similarity.\nExisting connectors, such as multilayer perceptrons (MLPs), often produce\nout-of-distribution or noisy inputs, leading to misalignment between the\nmodalities. In this work, we propose a novel vision-text alignment method,\nAlignVLM, that maps visual features to a weighted average of LLM text\nembeddings. Our approach leverages the linguistic priors encoded by the LLM to\nensure that visual features are mapped to regions of the space that the LLM can\neffectively interpret. AlignVLM is particularly effective for document\nunderstanding tasks, where scanned document images must be accurately mapped to\ntheir textual content. Our extensive experiments show that AlignVLM achieves\nstate-of-the-art performance compared to prior alignment methods. We provide\nfurther analysis demonstrating improved vision-text feature alignment and\nrobustness to noise.",
        "Sound effects model design commonly uses digital signal processing techniques\nwith full control ability, but it is difficult to achieve realism within a\nlimited number of parameters. Recently, neural sound effects synthesis methods\nhave emerged as a promising approach for generating high-quality and realistic\nsounds, but the process of synthesizing the desired sound poses difficulties in\nterms of control. This paper presents a real-time neural synthesis model guided\nby a physically inspired model, enabling the generation of high-quality sounds\nwhile inheriting the control interface of the physically inspired model. We\nshowcase the superior performance of our model in terms of sound quality and\ncontrol.",
        "In complex systems, information propagation can be defined as diffused or\ndelocalized, weakly localized, and strongly localized. This study investigates\nthe application of graph neural network models to learn the behavior of a\nlinear dynamical system on networks. A graph convolution and attention-based\nneural network framework has been developed to identify the steady-state\nbehavior of the linear dynamical system. We reveal that our trained model\ndistinguishes the different states with high accuracy. Furthermore, we have\nevaluated model performance with real-world data. In addition, to understand\nthe explainability of our model, we provide an analytical derivation for the\nforward and backward propagation of our framework.",
        "Large Vision-Language Models (LVLMs) have shown impressive performance in\nvarious tasks. However, LVLMs suffer from hallucination, which hinders their\nadoption in the real world. Existing studies emphasized that the strong\nlanguage priors of LVLMs can overpower visual information, causing\nhallucinations. However, the positive role of language priors is the key to a\npowerful LVLM. If the language priors are too weak, LVLMs will struggle to\nleverage rich parameter knowledge and instruction understanding abilities to\ncomplete tasks in challenging visual scenarios where visual information alone\nis insufficient. Therefore, we propose a benchmark called LanP to rethink the\nimpact of Language Priors in LVLMs. It is designed to investigate how strong\nlanguage priors are in current LVLMs. LanP consists of 170 images and 340\ncorresponding well-designed questions. Extensive experiments on 25 popular\nLVLMs reveal that many LVLMs' language priors are not strong enough to\neffectively aid question answering when objects are partially hidden. Many\nmodels, including GPT-4 Turbo, exhibit an accuracy below 0.5 in such a\nscenario.",
        "Plasma kinetics, for both flat and curved spacetime, is conventionally\nperformed on the mass shell, a 7--dimensional time-phase space with a Vlasov\nvector field, also known as the Liouville vector field. The choice of this\ntime-phase space encodes the parameterisation of the underling 2nd order\nordinary differential equations. By replacing the Vlasov vector on time-phase\nspace with a bivector on an 8--dimensional sub-bundle of the tangent bundle, we\ncreate a parameterisation free version of Vlasov theory. This has a number of\nadvantages, which include working for lightlike and ultra-relativistic\nparticles, non metric connections, and metric-free and premetric theories. It\nalso works for theories where no time-phase space can exist for topological\ntopological reasons. An example of this is when we wish to consider all\ngeodesics, including spacelike geodesics.\n  We extend the particle density function to a 6--form on the subbundle of the\ntangent space, and define the transport equations, which correspond to the\nVlasov equation. We then show how to define the corresponding 3--current on\nspacetime. We discuss the stress-energy tensor needed for the Einstein-Vlasov\nsystem.\n  This theory can be generalised to create parameterisation invariant Vlasov\ntheories for many 2nd order theories, on arbitrary manifolds. The relationship\nto sprays and semi-sprays is given and examples from Finsler geometry are also\ngiven.",
        "The interconnection between the human lungs and other organs, such as the\nliver and kidneys, is crucial for understanding the underlying risks and\neffects of lung diseases and improving patient care. However, most research\nchest CT imaging is focused solely on the lungs due to considerations of cost\nand radiation dose. This restricted field of view (FOV) in the acquired images\nposes challenges to comprehensive analysis and hinders the ability to gain\ninsights into the impact of lung diseases on other organs. To address this, we\npropose SCOPE (Spatial Coverage Optimization with Prior Encoding), a novel\napproach to capture the inter-organ relationships from CT images and extend the\nFOV of chest CT images. Our approach first trains a variational autoencoder\n(VAE) to encode 2D axial CT slices individually, then stacks the latent\nrepresentations of the VAE to form a 3D context for training a latent diffusion\nmodel. Once trained, our approach extends the FOV of CT images in the\nz-direction by generating new axial slices in a zero-shot manner. We evaluated\nour approach on the National Lung Screening Trial (NLST) dataset, and results\nsuggest that it effectively extends the FOV to include the liver and kidneys,\nwhich are not completely covered in the original NLST data acquisition.\nQuantitative results on a held-out whole-body dataset demonstrate that the\ngenerated slices exhibit high fidelity with acquired data, achieving an SSIM of\n0.81.",
        "Pulmonary Tuberculosis (PTB) remains a major challenge for global health,\nespecially in areas with poor medical resources, where access to specialized\nmedical knowledge and diagnostic tools is limited. This paper presents an\nauxiliary diagnosis system for pulmonary tuberculosis based on Huawei MindSpore\nframework and Ascend310 edge computing chip. Using MobileNetV3 architecture and\nSoftmax cross entropy loss function with momentum optimizer. The system\noperates with FP16 hybrid accuracy on the Orange pie AIPro (Atlas 200 DK) edge\ndevice and performs well. In the test set containing 4148 chest images, the\nmodel accuracy reached 99.1\\% (AUC = 0.99), and the equipment cost was\ncontrolled within \\$150, providing affordable AI-assisted diagnosis scheme for\nprimary care.",
        "Decision Focused Learning has emerged as a critical paradigm for integrating\nmachine learning with downstream optimisation. Despite its promise, existing\nmethodologies predominantly rely on probabilistic models and focus narrowly on\ntask objectives, overlooking the nuanced challenges posed by epistemic\nuncertainty, non-probabilistic modelling approaches, and the integration of\nuncertainty into optimisation constraints. This paper bridges these gaps by\nintroducing innovative frameworks: (i) a non-probabilistic lens for epistemic\nuncertainty representation, leveraging intervals (the least informative\nuncertainty model), Contamination (hybrid model), and probability boxes (the\nmost informative uncertainty model); (ii) methodologies to incorporate\nuncertainty into constraints, expanding Decision-Focused Learning's utility in\nconstrained environments; (iii) the adoption of Imprecise Decision Theory for\nambiguity-rich decision-making contexts; and (iv) strategies for addressing\nsparse data challenges. Empirical evaluations on benchmark optimisation\nproblems demonstrate the efficacy of these approaches in improving decision\nquality and robustness and dealing with said gaps.",
        "Relativistic magnetized shocks, through the Synchrotron Maser Instability\n(SMI) mechanism, represent a promising framework for generating coherent\nradiations, potentially accounting for the enigmatic Fast Radio Bursts\n(FRBs)-cosmic radio transients with extreme luminosity. This study investigates\nhow the radiation reaction (RR) effect, induced by high-energy photon emissions\nduring SMI, significantly modifies particle dynamics and emission properties in\nmagnetized shocks. Through comprehensive Particle-In-Cell (PIC) simulations, we\ndemonstrate that RR effects fundamentally alter coherent cyclotron motion at\nshock fronts, producing distinct observational signatures: spectral broadening,\npeak frequency upshift, and enhanced radiation intensity. Our findings suggest\nthat RR-mediated magnetized shocks could provide a natural explanation for the\nbimodal energy distribution observed in repeating FRB 121102 and the positive\ncorrelation of luminosity-bandwidth between repeating and one-off FRBs in\nCHIME\/FRB catalog. These results support the magnetized shock as a viable\nsource of FRBs.",
        "Supernova 2018ibb of the PISN category related to the dynamical instability\nof oxygen core in a supermassive star induced by pair-creation shows at the\nnebular stage strong [\\oiii] emission lines of an uncertain origin. I propose a\nsimple model that demonstrates a possibility of [O III] lines emission from the\nsupernova oxygen matter ionized and heated by the $^{56}$Co radioactive decay.\nThe reason is pinpointed by which the [O III] line luminosity among supernovae\nof PISN category can vary in a broad range.",
        "We establish an explicit uniform a priori estimate for weak solutions to\nslightly subcritical elliptic problems with nonlinearities simultaneously at\nthe interior and on the boundary. Our explicit $L^{\\infty}(\\Omega )$ a priori\nestimates are in terms of powers of their $H^{1}(\\Omega )$ norms. To prove our\nresult, we combine a De Giorgi-Nash-Moser's iteration scheme together with\nelliptic regularity and the Gagliardo-Nirenberg's interpolation inequality.",
        "Vague quantifiers such as \"a few\" and \"many\" are influenced by many\ncontextual factors, including how many objects are present in a given context.\nIn this work, we evaluate the extent to which vision-and-language models (VLMs)\nare compatible with humans when producing or judging the appropriateness of\nvague quantifiers in visual contexts. We release a novel dataset, VAQUUM,\ncontaining 20300 human ratings on quantified statements across a total of 1089\nimages. Using this dataset, we compare human judgments and VLM predictions\nusing three different evaluation methods. Our findings show that VLMs, like\nhumans, are influenced by object counts in vague quantifier use. However, we\nfind significant inconsistencies across models in different evaluation\nsettings, suggesting that judging and producing vague quantifiers rely on two\ndifferent processes.",
        "Objctives: This work aimed to statistically compare the metabolite\nquantification of human brain magnetic resonance spectroscopy (MRS) between the\ndeep learning method QNet and the classical method LCModel through an\neasy-to-use intelligent cloud computing platform CloudBrain-MRS. Materials and\nMethods: In this retrospective study, two 3 T MRI scanners Philips Ingenia and\nAchieva collected 61 and 46 in vivo 1H magnetic resonance (MR) spectra of\nhealthy participants, respectively, from the brain region of pregenual anterior\ncingulate cortex from September to October 2021. The analyses of Bland-Altman,\nPearson correlation and reasonability were performed to assess the degree of\nagreement, linear correlation and reasonability between the two quantification\nmethods. Results: Fifteen healthy volunteers (12 females and 3 males, age\nrange: 21-35 years, mean age\/standard deviation = 27.4\/3.9 years) were\nrecruited. The analyses of Bland-Altman, Pearson correlation and reasonability\nshowed high to good consistency and very strong to moderate correlation between\nthe two methods for quantification of total N-acetylaspartate (tNAA), total\ncholine (tCho), and inositol (Ins) (relative half interval of limits of\nagreement = 3.04%, 9.3%, and 18.5%, respectively; Pearson correlation\ncoefficient r = 0.775, 0.927, and 0.469, respectively). In addition,\nquantification results of QNet are more likely to be closer to the previous\nreported average values than those of LCModel. Conclusion: There were high or\ngood degrees of consistency between the quantification results of QNet and\nLCModel for tNAA, tCho, and Ins, and QNet generally has more reasonable\nquantification than LCModel.",
        "Background: Data collected in controlled settings typically results in\nhigh-quality datasets. However, in real-world applications, the quality of data\ncollection is often compromised. It is well established that the quality of a\ndataset significantly impacts the performance of machine learning models.\n  Methods: A rudimentary error rate metric was developed to evaluate textual\ndataset quality at the token level. Mixtral Large Language Model (LLM) was used\nto quantify and correct errors in low quality datasets. The study analyzed two\nhealthcare datasets: the high-quality MIMIC-III public hospital dataset and a\nlower-quality private dataset from Australian aged care homes. Errors were\nsystematically introduced into MIMIC at varying rates, while the ACH dataset\nquality was improved using the LLM.\n  Results: For the sampled 35,774 and 6,336 patients from the MIMIC and ACH\ndatasets respectively, we used Mixtral to introduce errors in MIMIC and correct\nerrors in ACH. Mixtral correctly detected errors in 63% of progress notes, with\n17% containing a single token misclassified due to medical terminology. LLMs\ndemonstrated potential for improving progress note quality by addressing\nvarious errors. Under varying error rates, feature representation performance\nwas tolerant to lower error rates (<10%) but declined significantly at higher\nrates.\n  Conclusions: The study revealed that models performed relatively well on\ndatasets with lower error rates (<10%), but their performance declined\nsignificantly as error rates increased (>=10%). Therefore, it is crucial to\nevaluate the quality of a dataset before utilizing it for machine learning\ntasks. For datasets with higher error rates, implementing corrective measures\nis essential to ensure the reliability and effectiveness of machine learning\nmodels.",
        "Cross encoders (CEs) are trained with sentence pairs to detect relatedness.\nAs CEs require sentence pairs at inference, the prevailing view is that they\ncan only be used as re-rankers in information retrieval pipelines. Dual\nencoders (DEs) are instead used to embed sentences, where sentence pairs are\nencoded by two separate encoders with shared weights at training, and a loss\nfunction that ensures the pair's embeddings lie close in vector space if the\nsentences are related. DEs however, require much larger datasets to train, and\nare less accurate than CEs. We report a curious finding that embeddings from\nearlier layers of CEs can in fact be used within an information retrieval\npipeline. We show how to exploit CEs to distill a lighter-weight DE, with a\n5.15x speedup in inference time.",
        "Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on\nrigid skeletal motion with an oversimplified non-rigid motion model, which\nfails to capture fine geometric and appearance details. Additionally, they\nperform densification based solely on per-point gradients and process poses\nindependently, ignoring spatial and temporal correlations. These limitations\nlead to geometric detail loss, temporal instability, and inefficient point\ndistribution. To address these issues, we propose HandSplat, a novel Gaussian\nSplatting-based framework that enhances both fidelity and stability for hand\nrendering. To improve fidelity, we extend standard 3DGS attributes with\nimplicit geometry and appearance embeddings for finer non-rigid motion modeling\nwhile preserving the static hand characteristic modeled by original 3DGS\nattributes. Additionally, we introduce a local gradient-aware densification\nstrategy that dynamically refines Gaussian density in high-variation regions.\nTo improve stability, we incorporate pose-conditioned attribute regularization\nto encourage attribute consistency across similar poses, mitigating temporal\nartifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat\nsurpasses existing methods in fidelity and stability while achieving real-time\nperformance. We will release the code and pre-trained models upon acceptance.",
        "Due to the wide employment of automated reasoning in the analysis and\nconstruction of correct systems, the results reported by automated reasoning\nengines must be trustworthy. For Boolean satisfiability (SAT) solvers - and\nmore recently SAT-based maximum satisfiability (MaxSAT) solvers -\ntrustworthiness is obtained by integrating proof logging into solvers, making\nsolvers capable of emitting machine-verifiable proofs to certify correctness of\nthe reasoning steps performed. In this work, we enable for the first time proof\nlogging based on the VeriPB proof format for multi-objective MaxSAT (MO-MaxSAT)\noptimization techniques. Although VeriPB does not offer direct support for\nmulti-objective problems, we detail how preorders in VeriPB can be used to\nprovide certificates for MO-MaxSAT algorithms computing a representative\nsolution for each element in the non-dominated set of the search space under\nPareto-optimality, without extending the VeriPB format or the proof checker. By\nimplementing VeriPB proof logging into a state-of-the-art multi-objective\nMaxSAT solver, we show empirically that proof logging can be made scalable for\nMO-MaxSAT with reasonable overhead.",
        "Photon transport through turbid media has typically been modeled through\ndiffusion or telegraph equations. These models describe behavior of the\naverage, or typical, photon with remarkable accuracy, however, we show here\nthat they fail to capture the Extreme First Passage Times (EFPTs) of photon\ntransport. By sending ultra-fast bursts of photons through a scattering medium\nand timing the arrival of the first passage photon, we measure the distribution\nof these EFPTs of photons in a random environment. Our measured EFPTs differ\nfrom those predicted by both the diffusion approximation and telegraph\nequation. Instead, we observe the EFPT as the time expected for light to travel\nthrough an index-averaged medium. These results reveal flaws in both models and\ninvite a re-examining of their underlying assumptions.",
        "We present a dependent randomized rounding scheme, which rounds fractional\nsolutions to integral solutions satisfying certain hard constraints on the\noutput while preserving Chernoff-like concentration properties. In contrast to\nprevious dependent rounding schemes, our algorithm guarantees that the cost of\nthe rounded integral solution does not exceed that of the fractional solution.\nOur algorithm works for a class of assignment problems with restrictions\nsimilar to those of prior works.\n  In a non-trivial combination of our general result with a classical approach\nfrom Shmoys and Tardos [Math. Programm.'93] and more recent linear programming\ntechniques developed for the restricted assignment variant by Bansal,\nSviridenko [STOC'06] and Davies, Rothvoss, Zhang [SODA'20], we derive a O(log\nn)-approximation algorithm for the Budgeted Santa Claus Problem. In this new\nvariant, the goal is to allocate resources with different values to players,\nmaximizing the minimum value a player receives, and satisfying a budget\nconstraint on player-resource allocation costs.",
        "Large Language Models (LLMs) have demonstrated unprecedented generative\ncapabilities, yet their alignment with human values remains critical for\nensuring helpful and harmless deployments. While Reinforcement Learning from\nHuman Feedback (RLHF) has emerged as a powerful paradigm for aligning LLMs with\nhuman preferences, its reliance on complex reward modeling introduces inherent\ntrade-offs in computational efficiency and training stability. In this context,\nDirect Preference Optimization (DPO) has recently gained prominence as a\nstreamlined alternative that directly optimizes LLMs using human preferences,\nthereby circumventing the need for explicit reward modeling. Owing to its\ntheoretical elegance and computational efficiency, DPO has rapidly attracted\nsubstantial research efforts exploring its various implementations and\napplications. However, this field currently lacks systematic organization and\ncomparative analysis. In this survey, we conduct a comprehensive overview of\nDPO and introduce a novel taxonomy, categorizing previous works into four key\ndimensions: data strategy, learning framework, constraint mechanism, and model\nproperty. We further present a rigorous empirical analysis of DPO variants\nacross standardized benchmarks. Additionally, we discuss real-world\napplications, open challenges, and future directions for DPO. This work\ndelivers both a conceptual framework for understanding DPO and practical\nguidance for practitioners, aiming to advance robust and generalizable\nalignment paradigms. All collected resources are available and will be\ncontinuously updated at\nhttps:\/\/github.com\/liushunyu\/awesome-direct-preference-optimization.",
        "This paper uses matrix transformations to provide the Autoone-Takagi\ndecomposition of dual complex symmetric matrices and extends it to dual\nquaternion $\\eta$-Hermitian matrices. The LU decomposition of dual matrices is\ngiven using the general solution of the Sylvester equation, and its equivalence\nto the existence of rank-k decomposition and dual Moore-Penrose generalized\ninverse (DMPGI) is proved. Similar methods are then used to provide the\nCholesky decomposition of dual real symmetric positive definite matrices. Both\nof our decompositions are driven by applications in numerical linear algebra.",
        "We investigate the alignment between the angular momenta of galaxy groups and\nthe spines of their associated cosmic filaments. Our results demonstrate a\nsignificant tendency for these two orientations to be perpendicular, indicating\nthat the rotation of a galaxy group does not originate from the spin of cosmic\nfilaments. Instead, it is driven by the orbital angular momentum contributed by\nmember galaxies as they accrete along the direction of the filament spines.\nMoreover, the strength of this perpendicular alignment signal varies with the\nrichness of the galaxy groups, with the most pronounced alignment observed\namong the wealthiest groups. This pronounced alignment is largely due to the\nmore coherent spatial distribution of member galaxies in richer groups relative\nto the filament spines. Our study provides valuable insights into the\nmechanisms of angular momentum acquisition in galaxy groups from an\nobservational standpoint.",
        "We carry out a comprehensive analysis of the Landau-Khalatnikov-Fradkin\ntransformations for a charged fermion propagator at the two-loop level in\nquantum electrodynamics (QED). Starting with an arbitrary covariant gauge $\\xi$\nand space-time dimension $d$, we provide its explicit expressions in three and\nfour-dimensional QED. We begin with the tree-level fermion propagator in the\nLandau gauge and gauge-transform it to obtain an analytical expression for an\nall order result in an arbitrary covariant gauge. We expand it out to two-loops\nboth for the massless and massive propagators in three and four space-time\ndimensions. In addition to comparing with all earlier results in the literature\nwherever possible, we also study constraints of multiplicative renormalizabilty\nof our results in four-dimensional QED which are logarithmically divergent.\nFinally, we analyze representative solutions of the fermion propagator which\ncorrespond to dynamical chiral symmetry breaking and mass generation in QED. We\nstudy the gauge dependence of these emergent solutions, that of the Euclidean\npole mass and the chiral fermion condensate."
      ]
    }
  },
  {
    "id":2411.17717,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology",
    "start_abstract":"Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's"
      ],
      "abstract":[
        "Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment ."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Search for medium effects using jet axis decorrelation in inclusive jets\n  from PbPb collisions at $\\sqrt{s_\\text{NN}}$ = 5.02 TeV",
        "Colorful Helly via induced matchings",
        "Superlubric Motion of Wave-like Domain Walls in Sliding Ferroelectrics",
        "Extended string-net models with all anyons at finite temperature",
        "Holographic inflation and holographic dark energy from entropy of the\n  anti-de Sitter black hole",
        "A framework for Tate modules of abelian varieties under isogeny",
        "Monolayer transition metal dichalcogenides under finite-pulse polarized\n  radiation",
        "Constrained Fuel and Time Optimal 6DOF Powered Descent Guidance Using\n  Indirect Optimization",
        "More resourceful states improve quantum channel discrimination",
        "Fingerprint Matrix Concept for Detecting, Localizing and Characterizing\n  Targets in Complex Media",
        "Observing Hot Holographic Quark Star With Gravitational Waves",
        "Predicting the depth of the most recent common ancestor of a random\n  sample of $k$ species: the impact of phylogenetic tree shape",
        "Generating Networks to Target Assortativity via Archimedean Copula\n  Graphons",
        "Can Dark Stars account for the star formation efficiency excess at very\n  high redshifts?",
        "COCOA: a compact Compton camera for astrophysical observation of\n  MeV-scale gamma rays",
        "Sampling the full hierarchical population posterior distribution in\n  gravitational-wave astronomy",
        "An Interior-Point Algorithm for Continuous Nonlinearly Constrained\n  Optimization with Noisy Function and Derivative Evaluations",
        "Dynamic Photometric Variability in Three Young Brown Dwarfs in Taurus:\n  Detection of Optical Flares with TESS data",
        "Multilingual Performance of a Multimodal Artificial Intelligence System\n  on Multisubject Physics Concept Inventories",
        "New Approaches to the Monotonicity Inequality for Linear Stochastic PDEs",
        "Betti numbers of normal edge rings (II)",
        "Pricing Quanto and Composite Contracts with Local-Correlation Models",
        "One-loop matching for leading-twist generalised\n  transverse-momentum-dependent distributions",
        "$L^{2}-$ Well-posedness and Bounded Controllability of KdV-B equation",
        "A visual representation of the properties of pre- and post- selected\n  entangled systems",
        "Spin-dependent dark matter scattering in quasi-two-dimensional magnets",
        "A pseudo-dynamic phase-field model for brittle fracture",
        "Coboundaries of 3-IETs",
        "Enhanced shot noise in graphene quantum point contacts with\n  electrostatic reconstruction"
      ],
      "abstract":[
        "The jet axis decorrelation in inclusive jets is studied using lead-lead\n(PbPb) collisions at a center-of-mass energy per nucleon pair of 5.02 TeV. The\njet axis decorrelation is defined as the angular difference between two\ndefinitions of the jet axis. It is obtained by applying two recombination\nschemes on all the constituents of a given jet reconstructed by the anti-\\kt\nsequential algorithm with a distance parameter of $R$ = 0.4. The data set,\ncorresponding to an integrated luminosity of 0.66 nb$^{-1}$, was collected in\n2018 with the CMS detector at the CERN LHC. The jet axis decorrelations are\nexamined across collision centrality selections and intervals of jet transverse\nmomentum. A centrality dependent evolution of the measured distributions is\nobserved, with a progressive narrowing seen in more central events. This\nnarrowing could result from medium-induced modification of the internal jet\nstructure or reflect color charge effects in energy loss. This new measurement\nprobes jet substructure in previously unexplored kinematic domains and show\ngreat promise for providing new insights on the color charge dependence of\nenergy loss to jet-quenching models.",
        "We establish a theorem regarding the maximum size of an {\\it{induced}}\nmatching in the bipartite complement of the incidence graph of a set system\n$(X,\\mathcal{F})$. We show that this quantity plus one provides an upper bound\non the colorful Helly number of this set system, i.e. the minimum positive\ninteger $N$ for which the following statement holds: if finite subfamilies\n$\\mathcal{F}_1,\\ldots, \\mathcal{F}_{N} \\subset \\mathcal{F}$ are such that\n$\\cap_{F \\in \\mathcal{F}_{i}} F = 0$ for every $i=1,\\ldots,N$, then there\nexists $F_i \\in \\mathcal{F}_i$ such that $F_1 \\cap \\ldots \\cap F_{N} =\n\\emptyset$. We will also discuss some natural refinements of this result and\napplications.",
        "Sliding ferroelectrics constructed from stacked nonpolar monolayers enable\nout-of-plane polarization in two dimensions with exceptional properties,\nincluding ultrafast switching speeds and fatigue-free behavior. However, the\nwidely accepted switching mechanism, which posits synchronized long-distance\nin-plane translation of entire atomic layers driven by an out-of-plane electric\nfield, has shown inconsistencies with experimental observations. We demonstrate\nthat this spinodal decomposition-like homogeneous switching process violates\nNeumann's principle and is unlikely to occur due to symmetry constraint.\nInstead, symmetry-breaking domain walls (DWs) and the tensorial nature of Born\neffective charges are critical for polarization reversal, underscoring the\nquantum nature of sliding ferroelectrics. Using the Bernal-stacked $h$-BN\nbilayer as a model system, we discover that the coherent propagation of wide,\nwave-like domain walls is the key mechanism for ferroelectric switching. This\nmechanism fundamentally differs from the layer-by-layer switching associated\nwith narrow domain walls, which has been established for over sixty years in\nperovskite ferroelectrics. Moreover, these wave-like DWs exhibit superlubric\ndynamics, achieving ultrahigh velocities of approximately 4000 m\/s at room\ntemperature and displaying an anomalous cooling-promoted switching speed. The\nunexpected emergence of DW superlubricity in sliding ferroelectrics presents\nnew avenues for enhancing key performance metrics and offers exciting\nopportunities for applications in cryogenic environments.",
        "String-net models describe a vast family of topological orders in two spatial\ndimensions, but fail to produce all the expected anyonic excitations. Following\narXiv:1502.03433, we consider an extended string-net model by attaching one\ntail to each plaquette of the lattice, allowing all anyons to emerge as\nelementary plaquette excitations for arbitrary input categories. The\ncorresponding tube algebra is the mathematical tool needed to construct the\nanyons from the input category and to obtain their internal multiplicities. We\nuse them to compute the energy level degeneracies and the partition function.\nIn the thermodynamic limit, the latter is dominated by the trivial (vacuum)\nanyon, so that the topological order is destroyed at any non-zero temperature.\nIn a finite-size system, order survives up to a finite temperature, similarly\nto the one-dimensional classical Ising model. We confirm this by computing\nthermal averages of topological projectors, Wegner-Wilson loops and the\ntopological mutual information. The results are also generalized to models with\nmultiple tails per plaquette.",
        "Based on the entropy of anti-de Sitter black hole, a new holographic dark\nenergy model has been proposed. When the Hubble horizon and particle horizon\nare chosen as the IR cutoff, the late-time accelerated expansion of universe is\nrealized. In this paper, we consider the Hubble horizon as the IR cutoff to\ninvestigate holographic inflation and slow-roll inflation in this model. We\nfind that slow-roll inflation with the chaotic potential $V_{0}\\phi^{n}$ is\nfavored by Planck results for some special cases, such as $n=1\/3$ and $n=1\/2$,\nwhile holographic inflation is not supported by Planck results. Then, we\nanalyze the reheating temperature and the number of reheating e-folds in this\nmodel, and we find that the results favor the cases $n=1\/3$ and $n=1\/2$.\nFinally, we use the dynamical analysis method, statefinder diagnostic pairs,\nand the Hubble diagram to analyze this model. Our results indicate that when\n$b^{2}$ takes a small value, this model cannot be distinguished from the\nstandard $\\Lambda$CDM model and can serve as an alternative to it.",
        "We explain the linear algebraic framework provided by Tate modules of\nisogenous abelian varieties in a category-theoretic way.",
        "Recent advances in time-resolved angle-resolved photoemission spectroscopy\nhave enabled access to ultrafast electron states and their spin dynamics in\nsolids. Atomically thin transition metal dichalcogenides are paradigmatic\ntwo-dimensional materials where electron momentum and spin degrees of freedom\nare coupled, being suitable candidates for time-resolved spectroscopy studies.\nIn this work, we present a thorough study of the electron dynamics when these\nmaterials are subject to an intense finite-pulse driving radiation. We extend\nthe scope of the conventional Floquet engineering and rely of the so-called\n$t-t^{\\prime}$ formalism to deal with driving fields described with two\ndistinct time scales, namely the envelope amplitude timescale and the time\nperiod of the external field. The interplay between the finite-pulse timescales\nand the intrinsic properties of the electrons gives rise to transient valley\npolarization and dynamical modifications of band structures, revealed by the\ntime-dependent circular dichroism of the sample.",
        "Powered descent guidance (PDG) problems subject to six-degrees-of-freedom\n(6DOF) dynamics allow for enforcement of practical attitude constraints.\nHowever, numerical solutions to 6DOF PDG problems are challenging due to fast\nrotational dynamics coupled with translational dynamics, and the presence of\nhighly nonlinear state\/control path inequality constraints. In this work,\nconstrained fuel- and time-optimal 6DOF PDG problems are solved leveraging a\nregularized indirect method, subject to inequality constraints on the thrust\nmagnitude, thruster gimbal angle, rocket tilt angle, glideslope angle, and\nangular velocity magnitude. To overcome the challenges associated with solving\nthe resulting multipoint boundary-value problems (MPBVPs), the state-only path\ninequality constraints (SOPICs) are enforced through an interior penalty\nfunction method, which embeds the resulting MPBVPs into a multi-parameter\nsmooth neighboring families of two-point BVPs. Extremal solutions are obtained\nusing an indirect multiple-shooting solution method with numerical\ncontinuation. Moreover, an empirical relation is derived for the\ndirectly-adjoined Lagrange multipliers associated with SOPICs. The fuel- and\ntime-optimal trajectories are compared against solutions of DIDO -- a capable\npseudospectral-based software for solving practical constrained optimal control\nproblems.",
        "One of the key issues in quantum discrimination problems is understanding the\nextent of the advantages in discrimination performance when using resource\nstates compared to resourceless states. We show that in any resource theory of\nstates, which may not be convex, the extent to which the maximum average\nsuccess probability can be improved in quantum channel discrimination problems\nwithout using auxiliary systems can be precisely quantified by the robustness\nmeasure. Furthermore, we demonstrate that the robustness measure can also\nquantify the improvement in channel discrimination problems that use auxiliary\nsystems. Using these findings, resources can be fully characterized to achieve\nhigher success probabilities than any state without the given resource in\nchannel discrimination problems.",
        "As waves propagate through a complex medium, they undergo multiple scattering\nevents. This phenomenon is detrimental to imaging, as it causes a full blurring\nof the image beyond a transport mean free path. Here, we show how to detect,\nlocalize, and characterize any scattering target through the reflection matrix\nof the complex medium in which this target is embedded and thus hidden from\ndirect view. More precisely, we introduce a fingerprint operator that contains\nthe specific signature of the target with respect to its environment. Applied\nto the recorded reflection matrix, this operator provides a likelihood index of\nthe target in any given state, despite the scattering fog induced by the\nsurrounding environment. This state can be the target position for localization\npurposes, its shape for characterization, or any other parameter that\ninfluences the target response. Our concept is versatile and broadly applicable\nto different type of waves for which multi-element technology allows a\nreflection matrix to be measured. We demonstrate this here explicitly by\nperforming different proof-of-concept experiments with ultrasound on targets\nburied inside a strongly scattering granular suspension, on lesion markers for\nclinical applications, and on the architecture of muscle tissue.",
        "We extract the equation of state for hot quark matter from a holographic\n$2+1$ flavor QCD model, which could form the core of a stable compact star. By\nadding a thin hadron shell, a new type of hybrid star is constructed. With the\ntemperature serving as a parameter, the EoS varies and we obtain stable stars\nwith the maximum mass of around 23 to 30 solar masses, and the compactness\naround $0.1$. The I-Love-Q-C relations are further discussed, and compared with\nthe neutron star cases. These compact stars are candidates for black hole\nmimickers, which could be observed by gravitational waves and distinguished by\nproperties like nonzero tidal Love number and electromagnetic signals.",
        "We consider the following question: how close to the ancestral root of a\nphylogenetic tree is the most recent common ancestor of $k$ species randomly\nsampled from the tips of the tree? For trees having shapes predicted by the\nYule-Harding model, it is known that the most recent common ancestor is likely\nto be close to (or equal to) the root of the full tree, even as $n$ becomes\nlarge (for $k$ fixed). However, this result does not extend to models of tree\nshape that more closely describe phylogenies encountered in evolutionary\nbiology. We investigate the impact of tree shape (via the Aldous\n$\\beta-$splitting model) to predict the number of edges that separate the most\nrecent common ancestor of a random sample of $k$ tip species and the root of\nthe parent tree they are sampled from. Both exact and asymptotic results are\npresented. We also briefly consider a variation of the process in which a\nrandom number of tip species are sampled.",
        "We develop an approach to generate random graphs to a target level of\nassortativity by using copula structures in graphons. Unlike existing random\ngraph generators, we do not use rewiring or binning approaches to generate the\ndesired random graph. Instead, we connect Archimedean bivariate copulas to\ngraphons in order to produce flexible models that can generate random graphs to\ntarget assortativity. We propose three models that use the copula distribution\nfunction, copula density function and their mixed tensor product to produce\nnetworks. We express the assortativity coefficient in terms of homomorphism\ndensities. Establishing this relationship forges a connection between the\nparameter of the copula and the frequency of subgraphs in the generated\nnetwork. Therefore, our method attains a desired the subgraph distribution as\nwell as the target assortativity. We establish the homomorphism densities and\nassortativity coefficient for each of the models. Numerical examples\ndemonstrate the ability of the proposed models to produce graphs with different\nlevels of assortativity.",
        "The James Webb Space Telescope (JWST) has recently conducted observations of\nmassive galaxies at high redshifts, revealing a notable anomaly in their star\nformation efficiency (SFE). Motivated by the recent identification of three\n$\\sim 10^{6}M_\\odot$ dark star candidates, we investigate whether dark stars\ncan be the origin of the SFE excess. It turns out that the excess can be\nreproduced by a group of dark stars with $M \\gtrsim 10^{3}\\, \\rm M_{\\odot}$,\nbecause of their domination in generating primary UV radiation in high-redshift\ngalaxies. The genesis of these dark stars is attributed to the capture of\nWeakly Interacting Massive Particles (WIMPs) within a mass range of tens of GeV\nto a few TeV. However, if the top-heavy initial mass function of dark stars\nholds up to $\\sim 10^{5}M_\\odot$, the relic black holes stemming from their\ncollapse would be too abundant to be consistent with the current observations\nof Massive Compact Halo Objects (MACHOs). We thus suggest that just a small\nfraction of SFE excess may be contributed by the very massive dark stars and\nthe majority likely originated from other reasons such as the Population III\nstars in view of their rather similar UV radiation efficiencies.",
        "COCOA (COmpact COmpton cAmera) is a next-generation, cost-effective gamma-ray\ntelescope designed for astrophysical observations in the MeV energy range. The\ndetector comprises a scatterer volume employing the LiquidO detection\ntechnology and an array of scintillating crystals acting as absorber.\nSurrounding plastic scintillator panels serve as a veto system for charged\nparticles. The detector's compact, scalable design enables flexible deployment\non microsatellites or high-altitude balloons. Gamma rays at MeV energies have\nnot been well explored historically (the so-called \"MeV gap\") and COCOA has the\npotential to improve the sensitivity in this energy band by up to two orders of\nmagnitude.",
        "We present a full sampling of the hierarchical population posterior\ndistribution of merging black holes using current gravitational-wave data. We\ndirectly tackle the the most relevant intrinsic parameter space made of the\nbinary parameters (masses, spin magnitudes, spin directions, redshift) of all\nthe events entering the GWTC-3 LIGO\/Virgo\/KAGRA catalog, as well as the\nhyperparameters of the underlying population of sources. This results in a\nparameter space of about 500 dimensions, in contrast with current\ninvestigations where the targeted dimensionality is drastically reduced by\nmarginalizing over all single-event parameters. In particular, we have direct\naccess to (i) population parameters, (ii) population-informed single-event\nparameters, and (iii) correlations between these two sets of parameters. Our\nimplementation relies on modern probabilistic programming languages and\nHamiltonian Monte Carlo, with a continuous interpolation of single-event\nposterior probabilities. Sampling the full hierarchical problem is feasible, as\ndemonstrated here, and advantageous as it removes some (but not all) of the\nMonte Carlo integrations that enter the likelihood together with the related\nvariances.",
        "An algorithm based on the interior-point methodology for solving continuous\nnonlinearly constrained optimization problems is proposed, analyzed, and\ntested. The distinguishing feature of the algorithm is that it presumes that\nonly noisy values of the objective and constraint functions and their\nfirst-order derivatives are available. The algorithm is based on a combination\nof a previously proposed interior-point algorithm that allows inexact\nsubproblem solutions and recently proposed algorithms for solving bound- and\nequality-constrained optimization problems with only noisy function and\nderivative values. It is shown that the new interior-point algorithm drives a\nstationarity measure below a threshold that depends on bounds on the noise in\nthe function and derivative values. The results of numerical experiments show\nthat the algorithm is effective across a wide range of problems.",
        "We present $I$-band time-series photometric variability studies of three\nknown nearby ($\\sim$ 140 pc) and young ( $\\sim$ 1 Myr) brown dwarfs (BD) in the\nTaurus star-forming region in the Perseus Molecular Cloud. From 10 nights of\nobservations over a time span of 10 years, with a typical run of 3 to 6 hours\neach night, we estimated that the BDs show unstable short-scale periodicity\nfrom 1.5 to 4.8 hours. Using the long-term photometry from the Transiting\nExoplanet Survey Satellite (TESS), we have conducted a time-resolved\nvariability analysis of CFHT-BD-Tau 3 and CFHT-BD-Tau 4, revealing orbital\nperiods of $\\sim$ 0.96 days and $\\sim$ 3 days respectively, consistent with\nearlier studies. We also found two superflares in TESS sector 43 data for\nCFHT-BD-Tau 4 and estimated the flare energies as $7.09\\times10^{35}$ erg and\n$3.75\\times10^{36}$ erg. A magnetic field of $\\sim3.39 ~kG$ is required to\ngenerate such flare energies on this BD. We performed spot modelling analysis\non CFHT-BD-Tau 3 and CFHT-BD-Tau 4 to address the variability detected in the\ndata using the package BASSMAN. Spectral energy distribution and infrared\ncolours of the sources suggest that they have a sufficient amount of\ncircumstellar material around them.",
        "We investigate the multilingual and multimodal performance of a large\nlanguage model-based artificial intelligence (AI) system, GPT-4o, on a diverse\nset of physics concept inventories spanning multiple languages and subject\nareas. The inventories taken from the PhysPort website cover the classical\nphysics topics of mechanics, electromagnetism, optics, and thermodynamics as\nwell as relativity, quantum mechanics, astronomy, mathematics, and laboratory\nskills. Unlike previous text-only studies, we uploaded the inventories as\nimages mirroring what a student would see on paper, assessing the system's\nmultimodal functionality. The AI is prompted in English and autonomously\nchooses the language of its response - either remaining in the nominal language\nof the test, switching entirely to English, or mixing languages - revealing\nadaptive behavior dependent on linguistic complexity and data availability. Our\nresults indicate some variation in performance across subject areas, with\nlaboratory skills standing out as the area of poorest performance. Furthermore,\nthe AI's performance on questions that require visual interpretation of images\nis worse than on purely text-based questions. Questions that are difficult for\nthe AI tend to be that way invariably of the inventory language. We also find\nlarge variations in performance across languages, with some appearing to\nbenefit substantially from language switching, a phenomenon similar to\ncode-switching ofhuman speakers. Overall, comparing the obtained AI results to\nthe existing literature, we find that the AI system outperforms average\nundergraduate students post-instruction in all subject areas but laboratory\nskills.",
        "The Monotonicity inequality is an important tool in the understanding of\nexistence and uniqueness of strong solutions for Stochastic PDEs. In this\narticle, we discuss three approaches to establish this deterministic inequality\nexplicitly.",
        "We compute the Betti numbers of the edge rings of multi-path graphs using the\ninduced-subgraph method introduced in \\cite{WL1}. Here, a multi-path graph\nrefers to a simple graph consisting of two vertices and multiple paths\nconnecting them, which generalizes the complete bipartite graph $K_{2,d}$.\nSpecial cases include the graph $G_{r,d}$ introduced in \\cite{GHK}, the graph\n$G_{r,s,d}$ introduced in \\cite{NN}, and the graph $B_{\\underline{\\ell},h}$\nintroduced in \\cite{LZ}.",
        "Pricing composite and quanto contracts requires a joint model of both the\nunderlying asset and the exchange rate. In this contribution, we explore the\npotential of local-correlation models to address the challenges of calibrating\nsynthetic quanto forward contracts and composite options quoted in the market.\nSpecifically, we design on-line calibration procedures for generic local and\nstochastic volatility models. The paper concludes with a numerical study\nassessing the calibration performance of these methodologies and comparing them\nto simpler approximations of the correlation structure.",
        "We present the one-loop matching coefficients necessary to match all of the\nleading-twist generalised transverse-momentum-dependent distributions (GTMDs)\nonto generalised parton distributions (GPDs). Matching functions are extracted\nby computing the first radiative corrections to partonic bilocal correlators\nwith staple-like Wilson lines, as appropriate for high-energy collisions. These\ncorrelators are characterised by a transverse displacement and skewed\nkinematics of external states. Using the proton helicity basis, they are\nparametrised in terms of GTMDs, which are subsequently related to leading-twist\nGPDs. Our results provide new insights into the complex dynamics of GTMDs\ngenerated by radiative corrections. In particular, we show that time-reversal\neven and odd contributions to GTMDs in the so-called ERBL region mix both under\nmatching and evolution. Finally, we present a selection of numerical results\nand comment on the quantitative behaviour of GTMDs.",
        "In this paper, the initial boundary value problem of the Korteweg-de Vries\nBurger equation on the negative half-plane is analyzed. Initially, the\nwell-posedness on $H^s(\\R^-)$ for $s\\geq 0$ of the IBVP is established to\nconcentrate on the $L^2(\\R^-)$ controllability problem when the controls are in\nthe Dirichlet and Newmann conditions at $x=0$.",
        "We introduce a visual representation for generating entangled-based quantum\neffects under pre- and post- selected states that allows us to reveal\nequivalence between seemingly different quantum effects. We show how to realize\nentangled quantum systems of an arbitrary number of qubits from a single or\npre-specified number of physical particles. We then show that a variation of\nthe quantum Cheshire cat experiment and Hardy's paradox are equivalent and\npropose a class of experiments that generalizes both experiments. We show that\nthe weak values of the products of projection operators allow us to get the\nweak value of each projection operator, implying that the weak value of the\nproduct of projection operators includes the entire information about the weak\nvalues in the system. In nature, interactions can only be acted between a pair\nof particles. We show how to realize quantum systems of multiwise interacted\nqubits, i.e., interactions that come in groups of n>2 qubits. In this way, we\nare able to propose unique quantum systems that consist of interacted groups of\nentangled states. The proposed framework opens the door toward a new way to\nexplore quantum systems of entangled particles and quantum phenomena that\nemerge from such a general setting.",
        "We study the prospects of detecting dark matter coupled to the spin of the\nelectron, such that it may scatter and excite magnons - collective excitations\nof electronic spins. We show that materials exhibiting long-range magnetic\norder where the spins are coupled only along a plane may act as directional\ndark matter detectors. These quasi-2D materials possess anisotropic dispersion\nrelations and structure functions which induce a sidereal modulation in the\nexcitation rate. We calculate the expected signal rate for some candidate\n(anti)ferromagnets, demonstrating a possible route to the direct detection of\nspin-dependent dark matter in the keV to MeV mass range.",
        "The enforcement of global energy conservation in phase-field fracture\nsimulations has been an open problem for the last 25 years. Specifically, the\noccurrence of unstable fracture is accompanied by a loss in total potential\nenergy, which suggests a violation of the energy conservation law. This\nphenomenon can occur even with purely quasi-static, displacement-driven loading\nconditions, where finite crack growth arises from an infinitesimal increase in\nload. While such behavior is typically seen in crack nucleation, it may also\noccur in other situations. Initial efforts to enforce energy conservation\ninvolved backtracking schemes based on global minimization, however in recent\nyears it has become clearer that unstable fracture, being an inherently dynamic\nphenomenon, cannot be adequately resolved within a purely quasi-static\nframework. Despite this, it remains uncertain whether transitioning to a fully\ndynamic framework would sufficiently address the issue. In this work, we\npropose a pseudo-dynamic framework designed to enforce energy balance without\nrelying on global minimization. This approach incorporates dynamic effects\nheuristically into an otherwise quasi-static model, allowing us to bypass\nsolving the full dynamic linear momentum equation. It offers the flexibility to\nsimulate crack evolution along a spectrum, ranging from full energy\nconservation at one extreme to maximal energy loss at the other. Using data\nfrom recent experiments, we demonstrate that our framework can closely\nreplicate experimental load-displacement curves, achieving results that are\nunattainable with classical phase-field models.",
        "In this note, we investigate the coboundaries of interval exchange\ntransformations of 3 intervals (3-IETs). More precisely, we show that a\ndifferentiable function with absolutely continuous derivative with bounded\nvariation, whose integral and integral of its derivative is 0, is a coboundary\nfor typical 3-IET if and only if the values at the endpoints of the domain are\nzero. We also show the existence of rare counterexamples for both cases of\npossible values at the endpoints of the interval. We obtain our result by\nstudying the properties of associated skew products.",
        "Shot noise measurements in quantum point contacts are a powerful tool to\ninvestigate charge transport in the integer and fractional quantum Hall regime,\nin particular to unveil the charge, quantum statistics and tunneling dynamics\nof edge excitations. In this letter, we describe shot noise measurements in a\ngraphene quantum point contact in the quantum Hall regime. At large magnetic\nfield, the competition between confinement and electronic interactions gives\nrise to a quantum dot located at the saddle point of the quantum point contact.\nWe show that the presence of this quantum dot leads to a $50-100~\\%$ increase\nin the shot noise, which we attribute to correlated charge tunneling. Our\nresults highlight the role played by the electrostatic environment in those\ngraphene devices."
      ]
    }
  },
  {
    "id":2411.17717,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's",
    "start_abstract":"Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment .",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology"
      ],
      "abstract":[
        "Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "CryptoX : Compositional Reasoning Evaluation of Large Language Models",
        "Invariant measure for the process viewed from the particle for 2D random\n  walks in Dirichlet environment",
        "Observation-Based Iterative Map for Solar Cycles. II. The Gnevyshev-Ohl\n  Rule and its Generation Mechanism",
        "Adaptive Variational Inference in Probabilistic Graphical Models: Beyond\n  Bethe, Tree-Reweighted, and Convex Free Energies",
        "On a class of high dimensional linear regression methods with debiasing\n  and thresholding",
        "Dynamic Refinement of Pressure Decomposition in Navier-Stokes Equations",
        "Homological data on the periodic structure of self-maps on wedge sums",
        "Making Them a Malicious Database: Exploiting Query Code to Jailbreak\n  Aligned Large Language Models",
        "Robust Multimodal Learning via Cross-Modal Proxy Tokens",
        "In-Context Meta LoRA Generation",
        "TinySense: A Lighter Weight and More Power-efficient Avionics System for\n  Flying Insect-scale Robots",
        "Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven\n  Surface Normal-aware Tracking and Mapping",
        "Comprehensive Analog Signal Processing Platform Enabled with Acoustic\n  Charge Transport in Two-dimensional Materials",
        "A Comparative Performance Analysis of Classification and Segmentation\n  Models on Bangladeshi Pothole Dataset",
        "ODPG: Outfitting Diffusion with Pose Guided Condition",
        "Fourier State Tomography of Polarization-Encoded Qubits",
        "Regulating Ai In Financial Services: Legal Frameworks And Compliance\n  Challenges",
        "Understanding entropy production via a thermal zero-player game",
        "Stochastic Schr\\\"{o}dinger equation for homodyne measurements of\n  strongly correlated systems",
        "Q-Sets, \\Delta-Sets, and L-Spaces",
        "Network Embedding Exploration Tool (NEExT)",
        "Two-component jet model for the afterglow emission of GRB 201216C and\n  GRB 221009A and implications for jet structure of very-high-energy gamma-ray\n  bursts",
        "Anomalous Dimension of a General Effective Gauge Theory I: Bosonic\n  Sector",
        "How Strategic Agents Respond: Comparing Analytical Models with\n  LLM-Generated Responses in Strategic Classification",
        "The order of appearance of the product of the first and second Lucas\n  numbers",
        "Securing DRAM at Scale: ARFM-Driven Row Hammer Defense with Unveiling\n  the Threat of Short tRC Patterns",
        "Assouad dimension of the Takagi function",
        "Mitigating Omitted Variable Bias in Empirical Software Engineering",
        "A multiwavelength light curve analysis of the classical nova V392 Per:\n  Optical contribution from an irradiated accretion disk during the nova wind\n  phase"
      ],
      "abstract":[
        "The compositional reasoning capacity has long been regarded as critical to\nthe generalization and intelligence emergence of large language models LLMs.\nHowever, despite numerous reasoning-related benchmarks, the compositional\nreasoning capacity of LLMs is rarely studied or quantified in the existing\nbenchmarks. In this paper, we introduce CryptoX, an evaluation framework that,\nfor the first time, combines existing benchmarks and cryptographic, to quantify\nthe compositional reasoning capacity of LLMs. Building upon CryptoX, we\nconstruct CryptoBench, which integrates these principles into several\nbenchmarks for systematic evaluation. We conduct detailed experiments on widely\nused open-source and closed-source LLMs using CryptoBench, revealing a huge gap\nbetween open-source and closed-source LLMs. We further conduct thorough\nmechanical interpretability experiments to reveal the inner mechanism of LLMs'\ncompositional reasoning, involving subproblem decomposition, subproblem\ninference, and summarizing subproblem conclusions. Through analysis based on\nCryptoBench, we highlight the value of independently studying compositional\nreasoning and emphasize the need to enhance the compositional reasoning\ncapabilities of LLMs.",
        "In this paper, we consider random walks in Dirichlet random environment\n(RWDE) on $\\mathbb{Z}^2$. We prove that, if the RWDE is recurrent (which is\nstrongly conjectured when the weights are symmetric), then there does not exist\nany invariant measure for the process viewed from the particle which is\nabsolutely continuous with respect to the static law of the environment.\nBesides, if the walk is directional transient and under condition\n$\\mathbf{(T')}$, we prove that there exists such an invariant probability\nmeasure if the trapping parameter verifies $\\kappa > 1$ or after acceleration\nof the process by a local function of the environment. This gives strong credit\nto a conjectural classification of cases of existence or non-existence of the\ninvariant measure for two dimensional RWDE. The proof is based on a new\nidentity, stated on general finite graphs, which is inspired by the\nrepresentation of the $\\star$-VRJP, a non-reversible generalization of the\nVertex reinforced Jump Process, in terms of random Schr\\\"odinger operators. In\nthe case of RWDE on 1D graph, the previous identity entails also a discrete\nanalogue of the Matsumoto-Yor property for Brownian motion.",
        "The Gnevyshev-Ohl (G-O) rule, also known as the even-odd effect, is an\nimportant observational phenomenon in solar cycles, suggesting that cycles with\neven indices tend to be followed by stronger cycles. The rule is considered to\nbe related to the solar dynamo, which drives the evolution of the Sun's\nlarge-scale magnetic field. However, observational studies of the G-O rule have\nrevealed inconsistencies, particularly regarding long-term variations and the\nunderlying physical mechanisms. In this study, we use an iterative map derived\nwithin the framework of the Babcock-Leighton (BL) dynamo to analyze the G-O\nrule. We investigate comprehensive and definitive forms of the G-O rule using\nboth a sufficiently large number of solar cycles and a limited number of solar\ncycles. Our findings indicate a higher probability for an arbitrary cycle to be\nfollowed by a stronger cycle instead of weaker, regardless of even or odd. Over\ntime spans comparable to historical observations, cycles exhibit periods that\nfollow both the G-O rule and the reversed G-O rule, without a statistically\nsignificant preference, consistent with the observed variability of the G-O\nrule. The occurrence of the reversed G-O rule is random, rather than periodic.\nThe G-O rule emerges as a result of the nonlinearity and stochasticity inherent\nin the BL mechanism. These results advance our understanding of the solar cycle\nand pave the way for improved solar dynamo modeling.",
        "Variational inference in probabilistic graphical models aims to approximate\nfundamental quantities such as marginal distributions and the partition\nfunction. Popular approaches are the Bethe approximation, tree-reweighted, and\nother types of convex free energies. These approximations are efficient but can\nfail if the model is complex and highly interactive. In this work, we analyze\ntwo classes of approximations that include the above methods as special cases:\nfirst, if the model parameters are changed; and second, if the entropy\napproximation is changed. We discuss benefits and drawbacks of either approach,\nand deduce from this analysis how a free energy approximation should ideally be\nconstructed. Based on our observations, we propose approximations that\nautomatically adapt to a given model and demonstrate their effectiveness for a\nrange of difficult problems.",
        "In this paper, we introduce a unified framework, inspired by classical\nregularization theory, for designing and analyzing a broad class of linear\nregression approaches. Our framework encompasses traditional methods like least\nsquares regression and Ridge regression, as well as innovative techniques,\nincluding seven novel regression methods such as Landweber and Showalter\nregressions. Within this framework, we further propose a class of debiased and\nthresholded regression methods to promote feature selection, particularly in\nterms of sparsity. These methods may offer advantages over conventional\nregression techniques, including Lasso, due to their ease of computation via a\nclosed-form expression. Theoretically, we establish consistency results and\nGaussian approximation theorems for this new class of regularization methods.\nExtensive numerical simulations further demonstrate that the debiased and\nthresholded counterparts of linear regression methods exhibit favorable finite\nsample performance and may be preferable in certain settings.",
        "In this work, the local decomposition of pressure in the Navier-Stokes\nequations is dynamically refined to prove that a relevant critical energy of a\nsuitable Leray-type solution inside a backward paraboloid, regardless of its\naperture is controlled near the vertex by a critical behavior confined to a\nneighborhood of the paraboloid's boundary. This neighborhood excludes the\ninterior near the vertex and remains separated from the temporal profile of the\nvertex, except at the vertex itself.",
        "In this article, we study the periodic points for continuous self-maps on the\nwedge sum of topological manifolds, exhibiting a particular combinatorial\nstructure. We compute explicitly the Lefschetz numbers, the Dold coefficients\nand consider its set of algebraic periods. Moreover, we study the special case\nof maps on the wedge sum of tori, and show some of the homological obstructions\npresent in defining these maps.",
        "Recent advances in large language models (LLMs) have demonstrated remarkable\npotential in the field of natural language processing. Unfortunately, LLMs face\nsignificant security and ethical risks. Although techniques such as safety\nalignment are developed for defense, prior researches reveal the possibility of\nbypassing such defenses through well-designed jailbreak attacks. In this paper,\nwe propose QueryAttack, a novel framework to examine the generalizability of\nsafety alignment. By treating LLMs as knowledge databases, we translate\nmalicious queries in natural language into structured non-natural query\nlanguage to bypass the safety alignment mechanisms of LLMs. We conduct\nextensive experiments on mainstream LLMs, and the results show that QueryAttack\nnot only can achieve high attack success rates (ASRs), but also can jailbreak\nvarious defense methods. Furthermore, we tailor a defense method against\nQueryAttack, which can reduce ASR by up to 64% on GPT-4-1106. Our code is\navailable at https:\/\/github.com\/horizonsinzqs\/QueryAttack.",
        "Multimodal models often experience a significant performance drop when one or\nmore modalities are missing during inference. To address this challenge, we\npropose a simple yet effective approach that enhances robustness to missing\nmodalities while maintaining strong performance when all modalities are\navailable. Our method introduces cross-modal proxy tokens (CMPTs), which\napproximate the class token of a missing modality by attending only to the\ntokens of the available modality. To efficiently learn the approximation for\nthe missing modality via CMPTs with minimal computational overhead, we employ\nlow-rank adapters in frozen unimodal encoders and jointly optimize an alignment\nloss with a task-specific loss. Extensive experiments on five multimodal\ndatasets show that our method outperforms state-of-the-art baselines across\nvarious missing rates while achieving competitive results in complete-modality\nsettings. Overall, our method offers a flexible and efficient solution for\nrobust multimodal learning. The code and pretrained models will be released on\nGitHub.",
        "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA.",
        "In this paper, we introduce advances in the sensor suite of an autonomous\nflying insect robot (FIR) weighing less than a gram. FIRs, because of their\nsmall weight and size, offer unparalleled advantages in terms of material cost\nand scalability. However, their size introduces considerable control\nchallenges, notably high-speed dynamics, restricted power, and limited payload\ncapacity. While there have been advancements in developing lightweight sensors,\noften drawing inspiration from biological systems, no sub-gram aircraft has\nbeen able to attain sustained hover without relying on feedback from external\nsensing such as a motion capture system. The lightest vehicle capable of\nsustained hovering -- the first level of ``sensor autonomy'' -- is the much\nlarger 28 g Crazyflie. Previous work reported a reduction in size of that\nvehicle's avionics suite to 187 mg and 21 mW. Here, we report a further\nreduction in mass and power to only 78.4 mg and 15 mW. We replaced the laser\nrangefinder with a lighter and more efficient pressure sensor, and built a\nsmaller optic flow sensor around a global-shutter imaging chip. A Kalman Filter\n(KF) fuses these measurements to estimate the state variables that are needed\nto control hover: pitch angle, translational velocity, and altitude. Our system\nachieved performance comparable to that of the Crazyflie's estimator while in\nflight, with root mean squared errors of 1.573 deg, 0.186 m\/s, and 0.136 m,\nrespectively, relative to motion capture.",
        "Simultaneous Localization and Mapping (SLAM) is essential for precise\nsurgical interventions and robotic tasks in minimally invasive procedures.\nWhile recent advancements in 3D Gaussian Splatting (3DGS) have improved SLAM\nwith high-quality novel view synthesis and fast rendering, these systems\nstruggle with accurate depth and surface reconstruction due to multi-view\ninconsistencies. Simply incorporating SLAM and 3DGS leads to mismatches between\nthe reconstructed frames. In this work, we present Endo-2DTAM, a real-time\nendoscopic SLAM system with 2D Gaussian Splatting (2DGS) to address these\nchallenges. Endo-2DTAM incorporates a surface normal-aware pipeline, which\nconsists of tracking, mapping, and bundle adjustment modules for geometrically\naccurate reconstruction. Our robust tracking module combines point-to-point and\npoint-to-plane distance metrics, while the mapping module utilizes normal\nconsistency and depth distortion to enhance surface reconstruction quality. We\nalso introduce a pose-consistent strategy for efficient and geometrically\ncoherent keyframe sampling. Extensive experiments on public endoscopic datasets\ndemonstrate that Endo-2DTAM achieves an RMSE of $1.87\\pm 0.63$ mm for depth\nreconstruction of surgical scenes while maintaining computationally efficient\ntracking, high-quality visual appearance, and real-time rendering. Our code\nwill be released at github.com\/lastbasket\/Endo-2DTAM.",
        "Two-dimensional Acoustic Charge Transport (2D-ACT) devices, which integrate\ntwo dimensional semiconductor field-effect transistor (FET) with high-frequency\nsurface acoustic wave (SAW) device provide a potential compact platform for the\nprocessing of analog signals in a wireless, non-contact, low-loss and real-time\nway. It is expected to be used in long-distance space communication and\nsensing. However, current investigations into 2D-ACT devices are still limited\nto the observation of DC acoustoelectric currents, and have yet to achieve\nreal-time electronic signal processing capabilities. In this paper, we have\ndesigned a hybrid acoustoelectric platform composed of two-dimensional\nsemiconductor FET and SAW device. The platform is capable of processing DC\nsignals, exhibiting ambipolar transport behavior. The sub-wavelength channel\nlength of the FET within the platform allows for the real-time observation of\ncarrier distribution at a microscopic scale in conjunction with the SAW\npotential, and facilitating the reproduction and intensity regulation of AC\nsignals. By adjusting the relative phase and intensity ratio of two\ncounter-propagating SAWs, the platform also enables the addition and\nsubtraction of AC signals.",
        "The study involves a comprehensive performance analysis of popular\nclassification and segmentation models, applied over a Bangladeshi pothole\ndataset, being developed by the authors of this research. This custom dataset\nof 824 samples, collected from the streets of Dhaka and Bogura performs\ncompetitively against the existing industrial and custom datasets utilized in\nthe present literature. The dataset was further augmented four-fold for\nsegmentation and ten-fold for classification evaluation. We tested nine\nclassification models (CCT, CNN, INN, Swin Transformer, ConvMixer, VGG16,\nResNet50, DenseNet201, and Xception) and four segmentation models (U-Net,\nResU-Net, U-Net++, and Attention-Unet) over both the datasets. Among the\nclassification models, lightweight models namely CCT, CNN, INN, Swin\nTransformer, and ConvMixer were emphasized due to their low computational\nrequirements and faster prediction times. The lightweight models performed\nrespectfully, oftentimes equating to the performance of heavyweight models. In\naddition, augmentation was found to enhance the performance of all the tested\nmodels. The experimental results exhibit that, our dataset performs on par or\noutperforms the similar classification models utilized in the existing\nliterature, reaching accuracy and f1-scores over 99%. The dataset also\nperformed on par with the existing datasets for segmentation, achieving model\nDice Similarity Coefficient up to 67.54% and IoU scores up to 59.39%.",
        "Virtual Try-On (VTON) technology allows users to visualize how clothes would\nlook on them without physically trying them on, gaining traction with the rise\nof digitalization and online shopping. Traditional VTON methods, often using\nGenerative Adversarial Networks (GANs) and Diffusion models, face challenges in\nachieving high realism and handling dynamic poses. This paper introduces\nOutfitting Diffusion with Pose Guided Condition (ODPG), a novel approach that\nleverages a latent diffusion model with multiple conditioning inputs during the\ndenoising process. By transforming garment, pose, and appearance images into\nlatent features and integrating these features in a UNet-based denoising model,\nODPG achieves non-explicit synthesis of garments on dynamically posed human\nimages. Our experiments on the FashionTryOn and a subset of the DeepFashion\ndataset demonstrate that ODPG generates realistic VTON images with fine-grained\ntexture details across various poses, utilizing an end-to-end architecture\nwithout the need for explicit garment warping processes. Future work will focus\non generating VTON outputs in video format and on applying our attention\nmechanism, as detailed in the Method section, to other domains with limited\ndata.",
        "Quantum state tomography is a central technique for the characterization and\nverification of quantum systems. Standard tomography is widely used for\nlow-dimensional systems, but for larger systems, it becomes impractical due to\nthe exponential scaling of experimental complexity with the number of qubits.\nHere, we present an experimental realization of Fourier-transform quantum state\ntomography for polarization-encoded photonic states. We validate the technique\nusing weak coherent states and entangled photon pairs generated by a quantum\ndot and spontaneous parametric down-conversion source in the telecom\nwavelength. The reconstructed density matrices show excellent agreement with\nthose obtained through conventional projective tomography, with calculated\nmetrics such as fidelity and concurrence matching within error bars, confirming\nthe reliability and accuracy of the technique. Fourier state tomography employs\nonly a single rotating waveplate per qubit, thereby avoiding repeated\nadjustments across multiple waveplates and ensuring that the number of physical\nmeasurement settings scales linearly with the number of qubits, despite the\nexponential growth of the underlying state space. This reduction in optical\nconfigurations simplifies experimental overhead, making Fourier state\ntomography a practical alternative for multi-qubit characterization.",
        "This article examines the evolving landscape of artificial intelligence (AI)\nregulation in financial services, detailing the legal frameworks and compliance\nchallenges posed by rapid technological adoption. By reviewing current\nlegislation, industry guidelines, and real-world use cases, it highlights how\nAI-driven processes, from fraud detection to algorithmic trading, offer\nefficiency gains yet introduce significant risks, including algorithmic bias,\ndata privacy breaches, and lack of transparency in automated decision-making.\nThe study compares regulatory approaches across major jurisdictions such as the\nEuropean Union, United States, and United Kingdom, identifying both universal\nconcerns, like the need for explainability and robust data protection, and\nregion-specific compliance requirements that impact the implementation of\nhigh-risk AI applications. Additionally, it underscores emerging areas of\nfocus, such as liability for AI-driven errors, systemic risks posed by\ninterlinked AI systems, and the ethical considerations of technology-driven\nfinancial exclusion. The findings reveal gaps in existing rules and emphasize\nthe necessity for adaptive, technology-neutral policies capable of fostering\ninnovation while safeguarding consumer rights and market integrity. The article\nconcludes by proposing a principled regulatory model that balances flexibility\nwith enforceable standards, advocating closer collaboration between\npolicymakers, financial institutions, and AI developers to ensure a secure,\nfair, and forward-looking framework for AI in finance.",
        "A new thermal bath scheme for Ising-Conway Entropy Game (ICEg) is introduced.\nNew game moves in sampling the given temperature is achieved via Monte Carlo\ndynamics of both Metropolis and Glauber as a stochastic game. This kind of\napproach makes the game an ideal tool for demonstrating thermal dependency of\nentropy production in a novel way. Using this new approach, Ising-Conway\nEntropy game's rate of entropy production depending on different temperatures\nare explored. Thermalized game is shown to be physically interesting and\nplausible test bed for studying complex dynamical systems in classical\nstatistical mechanics, that is conceptually simple, pedagogically accessible,\nyet realistic.",
        "We derive a stochastic Schr\\\"{o}dinger equation that describes the homodyne\nmeasurement record of a strongly interacting atomic system. We derive this\nequation for a general system, where we use the rotating wave approximation in\nthe linear atom-light interaction part, and the resulting equation is expressed\nin terms of the atomic operators only. Weak measurements are theoretically\ndescribed in terms of positive operator-valued measures. Among different weak\nmeasurement schemes, several earlier references studied the Gaussian quantum\ncontinuous measurement in detail. Here we consider a homodyne measurement\nsetup. We then demonstrate that the derived equation for this setup in the\nappropriate limit is the same as the one obtained while performing a Gaussian\nquantum continuous measurement.",
        "The question whether there is a Lindelof Q-set space or Lindelof $\\Delta$-set\nspace is considered. We show that J. Moore's ZFC $L$-space is not a Q-set space\nin ZFC and, assuming all Aronszajn trees are special, it is not a $\\Delta$-set\nspace.",
        "Many real-world and artificial systems and processes can be represented as\ngraphs. Some examples of such systems include social networks, financial\ntransactions, supply chains, and molecular structures. In many of these cases,\none needs to consider a collection of graphs, rather than a single network.\nThis could be a collection of distinct but related graphs, such as different\nprotein structures or graphs resulting from dynamic processes on the same\nnetwork. Examples of the latter include the evolution of social networks,\ncommunity-induced graphs, or ego-nets around various nodes. A significant\nchallenge commonly encountered is the absence of ground-truth labels for graphs\nor nodes, necessitating the use of unsupervised techniques to analyze such\nsystems. Moreover, even when ground-truth labels are available, many existing\ngraph machine learning methods depend on complex deep learning models,\ncomplicating model explainability and interpretability. To address some of\nthese challenges, we have introduced NEExT (Network Embedding Exploration Tool)\nfor embedding collections of graphs via user-defined node features. The\nadvantages of the framework are twofold: (i) the ability to easily define your\nown interpretable node-based features in view of the task at hand, and (ii)\nfast embedding of graphs provided by the Vectorizers library. In this paper, we\ndemonstrate the usefulness of NEExT on collections of synthetic and real-world\ngraphs. For supervised tasks, we demonstrate that performance in graph\nclassification tasks could be achieved similarly to other state-of-the-art\ntechniques while maintaining model interpretability. Furthermore, our framework\ncan also be used to generate high-quality embeddings in an unsupervised way,\nwhere target variables are not available.",
        "In recent years, afterglow emission in the very-high-energy (VHE) band above\n100 GeV have been clearly detected for at least five gamma-ray bursts (GRBs\n180720B, 190114C, 190829A, 201216C and 221009A). For some of these VHE GRBs, we\npreviously proposed a two-component jet model, consisting of two uniform jets\nwith narrow and wide opening angles to explain their multiwavelength afterglows\nincluding VHE gamma rays. In this paper, we show that the VHE emission from\nGRBs 201216C and 221009A can be also explained by our two-component jet model.\nWe find that the collimation-corrected kinetic energy of the five VHE GRBs have\ntypical values of 5\\times10^{49} erg and 5\\times10^{50} erg for the narrow and\nwide jets, respectively. We discuss the similarities and differences among the\nVHE GRBs, and the implications for the structure of their jets. In particular,\nthe narrow jet of GRB 221009A has a smaller opening angle, which can explain\nwhy its isotropic-equivalent energy is unusually large.",
        "We classify the physical operators of the most general bosonic effective\ngauge theory up to dimension six using on-shell methods. Based on this\nclassification, we compute the complete one-loop anomalous dimension employing\nboth on-shell unitarity-based and geometric techniques. Our analysis fully\naccounts for the mixing of operators with different dimensions. The results\nbroadly apply to any Effective Field Theory with arbitrary gauge symmetry and\nbosonic degrees of freedom. To illustrate their utility, we perform a complete\ncross-check of results on the renormalization of the Standard Model Effective\nField Theory (SMEFT), $O(n)$ scalar theory, and the SMEFT extended with an\naxion-like particle. Additionally, we present new results for axion-like\nparticles with CP-violating interactions.",
        "When machine learning (ML) algorithms are used to automate human-related\ndecisions, human agents may gain knowledge of the decision policy and behave\nstrategically to obtain desirable outcomes. Strategic Classification (SC) has\nbeen proposed to address the interplay between agents and decision-makers.\nPrior work on SC has relied on assumptions that agents are perfectly or\napproximately rational, responding to decision policies by maximizing their\nutilities. Verifying these assumptions is challenging due to the difficulty of\ncollecting real-world agent responses. Meanwhile, the growing adoption of large\nlanguage models (LLMs) makes it increasingly likely that human agents in SC\nsettings will seek advice from these tools. We propose using strategic advice\ngenerated by LLMs to simulate human agent responses in SC. Specifically, we\nexamine five critical SC scenarios -- hiring, loan applications, school\nadmissions, personal income, and public assistance programs -- and simulate how\nhuman agents with diverse profiles seek advice from LLMs. We then compare the\nresulting agent responses with the best responses generated by existing\ntheoretical models. Our findings reveal that: (i) LLMs and theoretical models\ngenerally lead to agent score or qualification changes in the same direction\nacross most settings, with both achieving similar levels of fairness; (ii)\nstate-of-the-art commercial LLMs (e.g., GPT-3.5, GPT-4) consistently provide\nhelpful suggestions, though these suggestions typically do not result in\nmaximal score or qualification improvements; and (iii) LLMs tend to produce\nmore diverse agent responses, often favoring more balanced effort allocation\nstrategies. These results suggest that theoretical models align with LLMs to\nsome extent and that leveraging LLMs to simulate more realistic agent responses\noffers a promising approach to designing trustworthy ML systems.",
        "Let $\\left(U_n\\right)_{n\\geq0}$ and $\\left(V_n\\right)_{n\\geq0}$ be the first\nand second Lucas sequences, respectively. Let $m$ be a positive integer. Then\nthe order of appearance of $m$ in the first Lucas sequence is defined as the\nsmallest positive integer $k$ such that $m$ divides $U_k$ and denoted by\n$\\tau(m)$. In this paper, we give explicit formulae for the terms $\\tau(U_m\nV_n)$, $\\tau(U_m U_n)$, $\\tau(V_m V_n)$ and $\\tau(U_nU_{n+p}U_{n+2p})$, where\n$p\\geq3$ is a prime number.",
        "To address the issue of powerful row hammer (RH) attacks, our study involved\nan extensive analysis of the prevalent attack patterns in the field. We\ndiscovered a strong correlation between the timing and density of the\nactive-to-active command period, ${tRC}$, and the likelihood of RH attacks. In\nthis paper, we introduce MARC, an innovative ARFM-driven RH mitigation IP that\nsignificantly reinforces existing RH mitigation IPs. MARC dynamically adjusts\nthe frequency of RFM in response to the severity of the RH attack environment,\noffering a tailored security solution that not only detects the threats but\nalso adapts to varying threat levels. MARC's detection mechanism has\ndemonstrated remarkable efficiency, identifying over 99\\% of attack patterns.\nMoreover, MARC is designed as a compact hardware module, facilitating tight\nintegration either on the memory controller-side or DRAM-side within the memory\nsystem. It only occupies a negligible hardware area of 3363~\\textit{$\\mu m^2$}.\nBy activating ARFM based on MARC's detection, the additional energy overhead is\nalso negligible in normal workloads. We conduct experiments to compare the\nhighest row count throughout the patterns, defined as max exposure, between the\nvanilla RH mitigation IPs and the MARC-enhanced versions of the same IPs,\nfocusing on both DRAM-side and memory controller-side. On the DRAM-side, MARC +\nprobabilistic scheme and MARC + counter-based tracking scheme achieve\n8.1$\\times$ and 1.5$\\times$ improvement in max exposure ratio compared to the\nvanilla IPs, respectively. On the memory controller-side, the MARC + PARA and\nMARC + Graphene achieve 50$\\times$ and 5.7$\\times$ improvement in max exposure\nratio compared to the vanilla IPs, respectively. MARC ensures optimal security\nwithout sacrificing system performance, making MARC a pioneering solution in\nthe realm of RH attack mitigation.",
        "For any integer $b\\geq2$ and real series $\\{c_n\\}$ such that\n$\\sum_{n=0}^\\infty|c_n|<\\infty$, the generalized Takagi function $f_{{\\mathbf\nc},b}(x)$ is defined by $$\n  f_{{\\mathbf c},b}(x):=\\sum_{n=0}^\\infty c_n\\phi(b^n x), \\quad x\\in [0,1], $$\nwhere $\\phi(x)=dist(x,\\mathbb{Z})$ is the distance from $x$ to the nearest\ninteger. The collection of functions with the form are called the Takagi class.\nIn this paper, we show that in the case that $\\varlimsup_{n \\to \\infty} b^n\n|c_n|<\\infty$, the Assouad dimension of the graph ${\\mathcal G} f_{{\\mathbf\nc},b}=\\{(x,f_{{\\mathbf c},b}(x)):x\\in[0,1]\\}$ for the generalized Takagi\nfunction $f_{{\\mathbf c},b}(x)$ is equal to one, that is, $$ \\dim_A {\\mathcal\nG} f_{{\\mathbf c},b}=1. $$ In particular, for each $0<a<1$ and integer $b \\geq\n2$, we define Takagi function $T_{a,b}$ as followed, $$\n  T_{a,b}(x):=\\sum_{n=0}^\\infty a^n \\phi(b^n x), \\quad x\\in [0,1]. $$ Then $\n  \\dim_A {\\mathcal G} T_{a,b}=1 $ if and only if $0<a \\leq 1\/b$.",
        "Omitted variable bias occurs when a statistical model leaves out variables\nthat are relevant determinants of the effects under study. This results in the\nmodel attributing the missing variables' effect to some of the included\nvariables -- hence over- or under-estimating the latter's true effect. Omitted\nvariable bias presents a significant threat to the validity of empirical\nresearch, particularly in non-experimental studies such as those prevalent in\nempirical software engineering.\n  This paper illustrates the impact of omitted variable bias on two case\nstudies in the software engineering domain, and uses them to present methods to\ninvestigate the possible presence of omitted variable bias, to estimate its\nimpact, and to mitigate its drawbacks. The analysis techniques we present are\nbased on causal structural models of the variables of interest, which provide a\npractical, intuitive summary of the key relations among variables.\n  This paper demonstrates a sequence of analysis steps that inform the design\nand execution of any empirical study in software engineering. An important\nobservation is that it pays off to invest effort investigating omitted variable\nbias before actually executing an empirical study, because this effort can lead\nto a more solid study design, and to a significant reduction in its threats to\nvalidity.",
        "The classical nova V392 Per 2018 is characterized by a very fast optical\ndecline, long binary orbital period of 3.23 days, detection of GeV gamma rays,\nand almost identical decay trends of $B$, $V$, and $I_{\\rm C}$ light curves.\nThe last feature is unique because most novae develop strong emission lines in\nthe nebular phase and these lines contribute especially to the $B$ and $V$\nbands and make large differences between the $BV$ and $I_{\\rm C}$ light curves.\nThis unique feature can be understood if the optical flux is dominated by\ncontinuum until the late phase of the nova outburst. Such continuum radiation\nis emitted by a bright accretion disk irradiated by a hydrogen burning white\ndwarf (WD) and viscous heating disk with high mass-accretion rate after the\nhydrogen burning ended. We present a comprehensive nova outburst model that\nreproduces all of these light curves. We determined the WD mass to be $M_{\\rm\nWD}=1.35$ - $1.37 ~M_\\odot$ and the distance modulus in the $V$ band to be\n$(m-M)_V=14.6 \\pm 0.2$; the distance is $d= 3.45\\pm 0.5$ kpc for the reddening\nof $E(B-V)=0.62$."
      ]
    }
  },
  {
    "id":2411.16728,
    "research_type":"applied",
    "start_id":"b10",
    "start_title":"FuXi-S2S: An accurate machine learning model for global subseasonal forecasts",
    "start_abstract":"Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b21"
      ],
      "title":[
        "Analysis methods for numerical weather prediction"
      ],
      "abstract":[
        "Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods."
      ],
      "categories":[
        "physics.data-an"
      ]
    },
    "list":{
      "title":[
        "Efficient First-Principles Framework for Overdamped Phonon Dynamics and\n  Anharmonic Electron-Phonon Coupling in Superionic Materials",
        "Euclid Quick Data Release (Q1) -- Data release overview",
        "On the conjecture of non-inner automorphisms of finite $p$-groups with a\n  non-trivial abelian direct factor",
        "D-HAT: a Diatom-inspired structure for a Helmet concept Against Trauma",
        "In Vivo Study of Bone Growth Around Additively Manufactured Implants\n  with Ti-6Al-4V and Bioactive Glass Powder Composites",
        "On the Isomorphism Problem of Cayley Graphs of Graph Products",
        "Revisiting gluon density from the BK equation with kinematical\n  constraint and large x terms",
        "CIBER 4th flight fluctuation analysis: Pseudo-power spectrum formalism,\n  improved source masking and validation on mocks",
        "Study of SARS-CoV-2 Spike Protein by Surface Enhanced Raman Spectroscopy\n  and Transmission Electron Microscopy",
        "State transfer of Grover walks on unitary and quadratic unitary Cayley\n  graphs over finite commutative rings",
        "An Automated Bandwidth Division for the LHCb Upgrade Trigger",
        "Vision-Aided Channel Prediction Based on Image Segmentation at Street\n  Intersection Scenarios",
        "Partially hyperbolic symplectomorphism with C^1 bundles",
        "Principles for Open Data Curation: A Case Study with the New York City\n  311 Service Request Data",
        "Can Yang-Baxter imply Lie algebra?",
        "Comparison of stochastic BGK and FP methods for the simulation of\n  non-equilibrium multi-species molecular gas flows",
        "Exact multiblack hole spacetimes in Einstein-ModMax theory",
        "Temperature-Dependent Calibration Procedures for the Silicon\n  Photomultiplier Readout of the Cosmic Ray Veto Detector for the Mu2e\n  Experiment",
        "Determining the Density of the Sun with Neutrinos",
        "Noise equals endogenous control",
        "Nonabelian Yang-Mills-Higgs and Plateau's problem in codimension three",
        "Gravity-Bench-v1: A Benchmark on Gravitational Physics Discovery for\n  Agents",
        "Transverse expansion of the metric at null hypersurfaces II. Existence\n  results and application to Killing horizons",
        "Visual tests using several safe confidence intervals",
        "Connecting SPDE to SGMs",
        "Universal point spread function engineering for 3D optical information\n  processing",
        "A Unifying Framework for Complex-Valued Eigenfunctions via The Cartan\n  Embedding",
        "On conservative algebras of 2-dimensional Algebras",
        "Bootstrap Nonparametric Inference under Data Integration"
      ],
      "abstract":[
        "Relying on the anharmonic special displacement method, we introduce an ab\ninitio quasistatic polymorphous framework to describe local disorder,\nanharmonicity, and electron-phonon coupling in superionic conductors. Using the\nexample of cubic Cu2Se, we show that positional polymorphism yields extremely\noverdamped anharmonic vibrations while preserving transverse acoustic phonons,\nconsistent with experiments. We also demonstrate well-defined electronic band\nstructures with large band gap openings due to polymorphism of 1.0 eV and\ncalculate anharmonic electron-phonon renormalization, yielding band gap\nnarrowing with increasing temperature in agreement with previous measurements.\nOur approach opens the way for efficient ab initio electronic structure\ncalculations in superionic crystals to elucidate their compelling high\nfigure-of-merit.",
        "The first Euclid Quick Data Release, Q1, comprises 63.1 sq deg of the Euclid\nDeep Fields (EDFs) to nominal wide-survey depth. It encompasses visible and\nnear-infrared space-based imaging and spectroscopic data, ground-based\nphotometry in the u, g, r, i and z bands, as well as corresponding masks.\nOverall, Q1 contains about 30 million objects in three areas near the ecliptic\npoles around the EDF-North and EDF-South, as well as the EDF-Fornax field in\nthe constellation of the same name. The purpose of this data release -- and its\nassociated technical papers -- is twofold. First, it is meant to inform the\ncommunity of the enormous potential of the Euclid survey data, to describe what\nis contained in these data, and to help prepare expectations for the\nforthcoming first major data release DR1. Second, it enables a wide range of\ninitial scientific projects with wide-survey Euclid data, ranging from the\nearly Universe to the Solar System. The Q1 data were processed with early\nversions of the processing pipelines, which already demonstrate good\nperformance, with numerous improvements in implementation compared to\npre-launch development. In this paper, we describe the sky areas released in\nQ1, the observations, a top-level view of the data processing of Euclid and\nassociated external data, the Q1 photometric masks, and how to access the data.\nWe also give an overview of initial scientific results obtained using the Q1\ndata set by Euclid Consortium scientists, and conclude with important caveats\nwhen using the data. As a complementary product, Q1 also contains observations\nof a star-forming area in Lynd's Dark Nebula 1641 in the Orion~A Cloud,\nobserved for technical purposes during Euclid's performance-verification phase.\nThis is a unique target, of a type not commonly found in Euclid's nominal sky\nsurvey.",
        "Let $p$ be a prime number. A longstanding conjecture asserts that every\nfinite non-abelian $p$-group has a non-inner automorphism of order $p$. In this\npaper, we prove that the conjecture is true when a finite non-abelian $p$-group\n$G$ has a non-trivial abelian direct factor. Moreover, we prove that the\nnon-inner automorphism is central and fixes $\\Phi(G)$ elementwise. As a\nconsequence, we prove that every group which is not purely non-abelian has a\nnon-inner central automorphism of order $p$ which fixes $\\Phi(G)$ elementwise.",
        "The primary objective of helmet design continues to be the prevention of\ntraumatic brain injuries. Yet, achieving an optimal user experience, including\naspects such as fit, thermal comfort, breathability, waterproofing, and\nreusability, is increasingly significant. Thus, designing helmets with\nmultifunctional performance represents the latest technological frontier for\nsafety devices. This study draws inspiration from the morphology of\nCoscinodiscus species diatoms to develop a biomimetic material replicating\ntheir cellular structure and multifunctionality. Unlike its biological\ncounterpart, the synthetic material is engineered as the inner liner for multi\nimpact helmets, suited for urban sports and micro mobility applications. The\narchitecture of the material is modeled using computer aided design tools, and\nits energy absorption capabilities are analyzed through finite element modeling\nand quasi static compression tests on 3D printed elastomeric samples.\nPerformance optimization is achieved through a parametric approach. The results\ndemonstrate that the material exhibits energy absorption comparable to cellular\nmaterials like honeycombs, while offering lightweight properties,\nbreathability, and resistance to atmospheric agents. This biomimetic design\nmarks a significant advancement in high performance safety equipment.",
        "Osseointegration is crucial to the success of biomedical implants. Additive\nmanufacturing of implants offers a high degree of design freedom, enabling\nprecise control over implant geometry and material composition. Bioactive glass\n(BG) can substantially enhance bone binding and bioactivity; however, limited\nresearch has been conducted on its incorporation into additively manufactured\nimplants. The performance of BG varies depending on the incorporation method,\nand the spatial and temporal evolution of its integration remains unclear. In\nthis study, we synthesized Ti-6Al-4V\/58S BG composites by using the selective\nlaser melting method and systematically compared the effects of BG coating and\ndoping in additively manufactured implants. In vivo histological results from\nanimal tests were statistically analyzed and discussed in terms of\nosseointegration over 4- and 12-week periods. Bone-to-implant contact (BIC) and\nbone density (BD) were used as quantitative metrics to evaluate interactions\nbetween the implants and surrounding bone. Our findings indicate that both\nBG-doped and BG-coated implants accelerated bone ingrowth during the early\nstages of healing. BG-coated implants demonstrated a greater improvement than\ndid pure 3D-printed Ti-6Al-4V implants. However, the effects of BG became\nnonsignificant during the later healing stage (12 weeks). This study provides a\nfoundation for systematically investigating BG incorporation methods in\n3D-printed biomedical implants and their effect on osseointegration.",
        "We investigate Cayley graphs of graph products by showing that graph products\nwith vertex groups that have isomorphic Cayley graphs yield isomorphic Cayley\ngraphs.",
        "We perform analysis of the small x non-linear evolution equation formulated\nin momentum space supplemented by higher order terms. The equation is defined\nin wide range of transverse momentum and longitudinal momentum fraction\nextending previous studies performed in \\cite{Kutak:2003bd,Kutak:2004ym}. The\nlinear part of the equation is motivated by the renormalization group improved\nsmall x approach which accounts for resummation of higher orders, and includes\ncollinear splitting function and kinematical constraint. The solution to the\nequation is then used to perform the fit to Deep Inelastic Scattering reduced\ncross section data.",
        "Precise, unbiased measurements of extragalactic background anisotropies\nrequire careful treatment of systematic effects in fluctuation-based,\nbroad-band intensity mapping measurements. In this paper we detail improvements\nin methodology for the Cosmic Infrared Background ExpeRiment (CIBER),\nconcentrating on flat field errors and source masking errors. In order to\nbypass the use of field differences, which mitigate flat field errors but\nreduce sensitivity, we characterize and correct for the flat field on\npseudo-power spectra, which includes both additive and multiplicative biases.\nTo more effectively mask point sources at 1.1 $\\mu$m and 1.8 $\\mu$m, we develop\na technique for predicting masking catalogs that utilizes optical and NIR\nphotometry through random forest regression. This allows us to mask over two\nVega magnitudes deeper than the completeness limits of 2MASS alone, with errors\nin the shot noise power remaining below $<10\\%$ at all masking depths\nconsidered. Through detailed simulations of CIBER observations, we validate our\nformalism and demonstrate unbiased recovery of the sky fluctuations on\nrealistic mocks. We demonstrate that residual flat field errors comprise\n$<20\\%$ of the final CIBER power spectrum uncertainty with this methodology.",
        "The spike protein (SP) of SARS-CoV-2 is the major molecular target for making\ndiagnostic tests, vaccines, and therapeutic development. We used a combination\nof transmission electron microscopy (TEM) and surface enhanced Raman microscopy\n(SERS) to study its structure. Using SERS on an aluminum substrate, we were\nable to detect a characteristic spectrum of SP mostly due to vibration of three\naromatic amino acids producing Raman shifts at 466 cm-1, 524 cm-1, 773 cm-1,\n831 cm-1, 1048 cm-1, 1308 cm-1, 1457 cm-1, and 1610 cm-1. Transmission Electron\nMicroscopy (TEM) of the SP showed periodic 2D-lattice orientation. The findings\nfrom this study have translational values for developing surface-enhanced Raman\nspectroscopy (SERS) based detectors for screening and testing SARS-CoV-2\nsignatures in diagnostic settings and contamination tracking.",
        "This paper focuses on periodicity and perfect state transfer of Grover walks\non two well-known families of Cayley graphs, namely, the unitary Cayley graphs\nand the quadratic unitary Cayley graphs. Let $R$ be a finite commutative ring.\nThe unitary Cayley graph $G_R$ has vertex set $R$, where two vertices $u$ and\n$v$ are adjacent if $u-v$ is a unit in $R$. We provide a necessary and\nsufficient condition for the periodicity of the Cayley graph $G_R$. We also\ncompletely determine the rings $R$ for which $G_R$ exhibits perfect state\ntransfer. The quadratic unitary Cayley graph $\\mathcal{G}_R$ has vertex set\n$R$, where two vertices $u$ and $v$ are adjacent if $u-v$ or $v-u$ is a square\nof some units in $R$. It is well known that any finite commutative ring $R$ can\nbe expressed as $R_1\\times\\cdots\\times R_s$, where each $R_i$ is a local ring\nwith maximal ideal $M_i$ for $i\\in\\{1,...,s\\}$. We characterize periodicity and\nperfect state transfer on $\\mathcal{G}_R$ under the condition that\n$|R_i|\/|M_i|\\equiv 1 \\pmod 4$ for $i\\in\\{1,...,s\\}$. Also, we characterize\nperiodicity and perfect state transfer on $\\mathcal{G}_R$, where $R$ can be\nexpressed as $R_0\\times\\cdots\\times R_s$ such that $|R_0|\/|M_0|\\equiv3\\pmod 4$,\nand $|R_i|\/|M_i|\\equiv1\\pmod4$ for $i\\in\\{1,..., s\\}$, where $R_i$ is a local\nring with maximal ideal $M_i$ for $i\\in\\{0,...,s\\}$.",
        "The upgraded Large Hadron Collider beauty (LHCb) experiment is the first\ndetector based at a hadron collider using a fully software based trigger. The\nfirst `High Level Trigger' stage (HLT1) reduces the event rate from 30 MHz to\napproximately 1 MHz based on reconstruction criteria from the tracking system\nand consists of O(100) trigger selections implemented on GPUs. These selections\nare further refined following the full offline-quality reconstruction at the\nsecond stage (HLT2) prior to saving for analysis. An automated bandwidth\ndivision has been performed to equitably divide this 1 MHz output rate between\nthe signals of interest to the LHCb physics program. This was achieved by\noptimising a set of trigger selections that maximise efficiency for signals of\ninterest to LHCb while keeping the total HLT1 readout capped to a maximum. The\nbandwidth division tool has been used to determine the optimal selection for 35\nselection algorithms over 80 characteristic physics channels.",
        "Intelligent vehicular communication with vehicle road collaboration\ncapability is a key technology enabled by 6G, and the integration of various\nvisual sensors on vehicles and infrastructures plays a crucial role. Moreover,\naccurate channel prediction is foundational to realizing intelligent vehicular\ncommunication. Traditional methods are still limited by the inability to\nbalance accuracy and operability based on substantial spectrum resource\nconsumption and highly refined description of environment. Therefore,\nleveraging out-of-band information introduced by visual sensors provides a new\nsolution and is increasingly applied across various communication tasks. In\nthis paper, we propose a computer vision (CV)-based prediction model for\nvehicular communications, realizing accurate channel characterization\nprediction including path loss, Rice K-factor and delay spread based on image\nsegmentation. First, we conduct extensive vehicle-to-infrastructure measurement\ncampaigns, collecting channel and visual data from various street intersection\nscenarios. The image-channel dataset is generated after a series of data\npost-processing steps. Image data consists of individual segmentation of target\nuser using YOLOv8 network. Subsequently, established dataset is used to train\nand test prediction network ResNet-32, where segmented images serve as input of\nnetwork, and various channel characteristics are treated as labels or target\noutputs of network. Finally, self-validation and cross-validation experiments\nare performed. The results indicate that models trained with segmented images\nachieve high prediction accuracy and remarkable generalization performance\nacross different streets and target users. The model proposed in this paper\noffers novel solutions for achieving intelligent channel\n  prediction in vehicular communications.",
        "We prove dynamical coherence for partial hyperbolic symplectomorphism in\ndimension 4 whose stable and unstable bundles are C^1.",
        "In the early 21st century, the open data movement began to transform\nsocieties and governments by promoting transparency, innovation, and public\nengagement. The City of New York (NYC) has been at the forefront of this\nmovement since the enactment of the Open Data Law in 2012, creating the NYC\nOpen Data portal. The portal currently hosts 2,700 datasets, serving as a\ncrucial resource for research across various domains, including health, urban\ndevelopment, and transportation. However, the effective use of open data relies\nheavily on data quality and usability, challenges that remain insufficiently\naddressed in the literature. This paper examines these challenges via a case\nstudy of the NYC 311 Service Request dataset, identifying key issues in data\nvalidity, consistency, and curation efficiency. We propose a set of data\ncuration principles, tailored for government-released open data, to address\nthese challenges. Our findings highlight the importance of harmonized field\ndefinitions, streamlined storage, and automated quality checks, offering\npractical guidelines for improving the reliability and utility of open\ndatasets.",
        "Quantum knot invariants (like colored HOMFLY-PT or Kauffman polynomials) are\na distinguished class of non-perturbative topological invariants. Any known way\nto construct them (via Chern-Simons theory or quantum R-matrix) starts with a\nfinite simple Lie algebra. Another set of knot invariants - of finite type - is\nrelated to quantum invariants via a perturbative expansion. However can all\nfinite type invariants be obtained in this way? Investigating this problem, P.\nVogel discovered a way to polynomially parameterize the expansion coefficients\nwith three parameters so that, at different specific values, this reproduces\nthe answers for all simple Lie (super)algebras. Then it is easy to construct a\npolynomial $P_{alg}$ that vanishes for all simple Lie algebras, and the\ncorresponding Vassiliev invariant would thus be absent from the perturbative\nexpansion.\n  We review these Vogel claims pointing out at least two interesting\nimplications of his construction. First, we discuss whether\ninfinite-dimensional Lie algebras might enlarge Chern-Simons theory. Second,\nVogel's construction implies an alternative axiomatization of simple Lie\nalgebras - when we start from knot invariants and arrive at Lie algebras and\ntheir classification, which is opposite to conventional logic that we mentioned\nat the beginning.",
        "Due to limited possibilities of experimental investigations for\nnon-equilibrium gas flows, numerical results are of highest interest. Although\nthe well-established Direct Simulation Monte Carlo (DSMC) method achieves\nhighly accurate solutions, the computational requirements increase excessively\nfor lower Knudsen regimes. Computationally more efficient simulations can be\nachieved with stochastic continuum-based methods using either the\nBhatnagar-Gross-Krook (BGK) or the Fokker-Planck (FP) approximations where,\ninstead of particle collisions, particle relaxation processes are considered.\nThis paper explains the implementation of different stochastic BGK and FP\nmethods in the open-source particle code PICLas for multi-species molecular gas\nflows. For verification, the results of different test cases are compared.",
        "Exact solutions describing multiple, electrically charged black holes (BHs)\nin a model of nonlinear electrodynamics (NLE) minimally coupled to Einstein's\ngravity are presented. The NLE model is ModMax theory, that has attracted much\nattention due to its duality and conformal invariance, features shared with\nstandard (linear) electrodynamics. In the nonextremal case, the solution has\nconical singularities, similarly to the multi Reissner-Nordstr\\\"om solution in\nEinstein-Maxwell theory. In the extremal case the solution is regular on and\noutside the event horizon; it is isometric to the Majumdar-Papapetrou solution,\nalthough the individual BHs have a nonunitary charge to mass ratio, due to\nscreening effects. Using the ModMax electromagnetic duality invariance,\nmagnetically charged and dyonic generalizations are also obtained. Finally, we\nconstruct multi-BH solutions with a positive cosmological constant.",
        "The cosmic ray veto detector for the Mu2e experiment consists of\nscintillation bars embedded with wavelength-shifting fibers and read out by\nsilicon photomultipliers (SiPMs). In this manuscript the calibration procedures\nof the SiPMs are described including corrections for the temperature dependence\nof their light yield. These corrections are needed as the SiPMs are not kept at\na constant temperature due to the complexity and cost of implementing a cooling\nsystem on such a large detector. Rather, it was decided to monitor the\ntemperature to allow the appropriate corrections to be made. The SiPM\ntemperature dependence has been measured in a dedicated experiment and the\ncalibration procedures were validated with data from production detectors\nawaiting installation at Fermilab.",
        "The discovery of solar neutrinos confirmed that the inner workings of the Sun\ngenerally match our theoretical understanding of the fusion process. Solar\nneutrinos have also played a role in discovering that neutrinos have mass and\nthat they oscillate. We combine the latest solar neutrino data along with other\noscillation data from reactors to determine the Sun's density profile. We\nderive constraints given the current data and show the anticipated improvements\nwith more reactor neutrino data from JUNO constraining the true oscillation\nparameters and more solar neutrino data from DUNE which should provide a\ncrucial measurement of $hep$ neutrinos.",
        "Stochastic systems have a control-theoretic interpretation in which noise\nplays the role of endogenous control. In the weak-noise limit, relevant at low\ntemperatures or in large populations, control is optimal and an exact\nmathematical mapping from noise to control is described, where the maximizing\nthe probability of a state becomes the control objective. In Langevin dynamics\nnoise is identified directly with control, while in general Markov jump\nprocesses, which include chemical reaction networks and electronic circuits, we\nuse the Doi-Zel'dovich-Grassberger-Goldenfeld-Peliti path integral to identify\nthe `response' or `tilt' field $\\pi$ as control, which is proportional to the\nnoise in the semiclassical limit. This solves the longstanding problem of\ninterpreting $\\pi$. We illustrate the mapping on multistable chemical reaction\nnetworks and systems with unstable fixed points. The noise-control mapping\nbuilds intuition for otherwise puzzling phenomena of stochastic systems: why\nthe probability is generically a non-smooth function of state out of thermal\nequilibrium; why biological mechanisms can work better in the presence of\nnoise; and how agentic behavior emerges naturally without recourse to\nmysticism.",
        "We investigate the asymptotic behavior of the\n$\\mathrm{SU}(2)$-Yang-Mills-Higgs energy $E(\\Phi,A)=\\int_M|d_A\\Phi|^2+|F_A|^2$\nin the large mass limit, proving convergence to the codimension-three area\nfunctional in the sense of De Giorgi's $\\Gamma$-convergence. More precisely,\nfor a compact manifold with boundary $M$ and any family of pairs\n$\\Phi_m\\in\\Omega^0(M;\\mathfrak{su}(2))$ and $A_m\\in\n\\Omega^1(M;\\mathfrak{su}(2))$ indexed by a mass parameter $m\\to\\infty$,\nsatisfying $$E(\\Phi_m,A_m)\\leq\nCm\\quad\\text{and}\\quad\\lim_{m\\to\\infty}\\frac{1}{m}\\int_M(m-|\\Phi_m|)^2=0,$$ we\nprove that the $(n-3)$-currents dual to $\\frac{1}{2\\pi\nm}\\mathrm{tr}(d_{A_m}\\Phi_m\\wedge F_{A_m})$ converge subsequentially to a\nrelative integral $(n-3)$-cycle $T$ of mass \\begin{equation}\n  \\mathbb{M}(T)\\leq \\liminf_{m\\to\\infty}\\frac{1}{4\\pi m}E(\\Phi_m,A_m),\n\\end{equation} and show conversely that any integral $(n-3)$-current $T$ with\n$[T]=0\\in H_{n-3}(M,\\partial M;\\mathbb{Z})$ admits such an approximation, with\nequality in the above inequality. In the special case of pairs $(\\Phi_m,A_m)$\nsatisfying the generalized monopole equation $*d_{A_m}\\Phi_m=F_{A_m}\\wedge\n\\Theta$ for a calibration form $\\Theta\\in \\Omega^{n-3}(M)$, we deduce that the\nlimit $\\nu=\\lim_{m\\to\\infty}\\frac{1}{2\\pi m}|d_{A_m}\\Phi_m|^2$ of the Dirichlet\nenergy measures satisfies $\\nu\\leq |T|$, with equality if and only if $T$ is\ncalibrated by $\\Theta$, giving evidence for predictions of Donaldson-Segal in\nthe settings of $G_2$-manifolds and Calabi-Yau $3$-folds.",
        "Modern science emerged from reasoning over repeatedly-observed planetary\nmotions. We present Gravity-Bench-v1, an environment-based benchmark that\nchallenges AI agents on tasks that parallel this historical development.\nGravity-Bench-v1 evaluates agents on the discovery of physics concealed within\na dynamic environment, using rigorous gravitational dynamics simulations.\nGravity-Bench includes out-of-distribution cases, i.e. with physics that\ndeviates from the real world, to evaluate true scientific generalization\ncapabilities. Agents must plan to collect data within an experimental budget\nand must perform a dynamic form of data analysis and reasoning to solve tasks\nefficiently. Our benchmark admits an open-ended space of solutions. PhD-level\nsolutions for each task are provided, to calibrate AI performance against human\nexpertise. Technically at an upper-undergraduate level, our benchmark proves\nchallenging to baseline AI agents. Gravity-Bench-v1 and planned extensions\nshould help map out AI progress towards scientific discovery capabilities.",
        "This paper finishes the series of two papers that we started with\n[arXiv:2405.05377], where we analyzed the transverse expansion of the metric at\na general null hypersurface. While [arXiv:2405.05377] focused on uniqueness\nresults, here we show existence of ambient manifolds given the full asymptotic\nexpansion at the null hypersurface. When such expansion fulfills a set of\n\"constraint equations\" we prove that the ambient manifold solves the Einstein\nequations to infinite order at the hypersurface. Our approach does not make any\nassumptions regarding the dimension or topology of the null hypersurface and is\nentirely covariant. Furthermore, when the hypersurface exhibits a product\ntopology we find the minimum amount of data on a cross-section that ensures the\nexistence of an ambient space solving the Einstein equations to infinite order\non the hypersurface. As an application we recall the notion of abstract Killing\nhorizon data (AKH) introduced in [arXiv:2405.05377], namely the minimal data\nneeded to define a non-degenerate Killing horizon from a detached viewpoint,\nand we prove that every AKH of arbitrary dimension and topology gives rise to\nan ambient space solving the {\\Lambda}-vacuum equations to infinite order and\nwith the given data as Killing horizon. Our result also includes the\npossibility of the Killing vector having zeroes at the horizon.",
        "We propose a new statistical hypothesis testing framework which decides\nvisually, using confidence intervals, whether the means of two samples are\nequal or if one is larger than the other. With our method, the user can at the\nsame time visualize the confidence region of the means and do a test to decide\nif the means of the two populations are significantly different or not by\nlooking whether the two confidence intervals overlap. To design this test we\nuse confidence intervals constructed using e-variables, which provide a measure\nof evidence in hypothesis testing. We propose both a sequential test and a\nnon-sequential test based on the overlap of confidence intervals and for each\nof these tests we give finite-time error bounds on the probabilities of error.\nWe also illustrate the practicality of our method by applying it to the\ncomparison of sequential learning algorithms.",
        "This paper investigates a Stochastic Partial Differential Equation (SPDE)\nderived from the Fokker-Planck equation associated with Score-based Generative\nModels. We modify the standard Fokker-Planck equation to better represent\npractical SGMs and introduce noise to mitigate potential discretization issues.\nThe primary goal is to prove the existence and uniqueness of solutions for this\nSPDE. This aspect requires careful consideration due to the time-dependent\noperator and unbounded domain. To overcome these hurdles, we employ a\nvariational approach and introduce a novel space inspired by Ornstein-Uhlenbeck\noperators. By demonstrating that this space and its subspace satisfy the\nnecessary assumptions, they establish the existence of a solution for the given\nSPDE.",
        "Point spread function (PSF) engineering has been pivotal in the remarkable\nprogress made in high-resolution imaging in the last decades. However, the\ndiversity in PSF structures attainable through existing engineering methods is\nlimited. Here, we report universal PSF engineering, demonstrating a method to\nsynthesize an arbitrary set of spatially varying 3D PSFs between the input and\noutput volumes of a spatially incoherent diffractive processor composed of\ncascaded transmissive surfaces. We rigorously analyze the PSF engineering\ncapabilities of such diffractive processors within the diffraction limit of\nlight and provide numerical demonstrations of unique imaging capabilities, such\nas snapshot 3D multispectral imaging without involving any spectral filters,\naxial scanning or digital reconstruction steps, which is enabled by the spatial\nand spectral engineering of 3D PSFs. Our framework and analysis would be\nimportant for future advancements in computational imaging, sensing and\ndiffractive processing of 3D optical information.",
        "In this work we find a unifying scheme for the known explicit complex-valued\neigenfunctions on the classical compact Riemannian symmetric spaces. For this\nwe employ the well-known Cartan embedding for those spaces. This also leads to\nthe construction of new eigenfunctions on the quaternionic Grassmannians.",
        "In 1990 Kantor introduced the conservative algebra $\\mathcal{W}(n)$ of all\nalgebras (i.e. bilinear maps) on the $n$-dimensional vector space. In case $n\n>1$ the algebra $\\mathcal{W}(n)$ does not belong to well known classes of\nalgebras (such as associative, Lie, Jordan, Leibniz algebras). We describe\n$\\frac{1}{2}$derivations, local (resp. $2$-local) $\\frac{1}{2}$-derivations and\nbiderivations of $\\mathcal{W}(2)$. We also study similar problems for the\nalgebra $\\mathcal{W}_2$ of all commutative algebras on the two-dimensional\nvector space and the algebra $\\mathcal{S}_2$ of all commutative algebras with\ntrace zero multiplication on the two-dimensional space.",
        "We propose multiplier bootstrap procedures for nonparametric inference and\nuncertainty quantification of the target mean function, based on a novel\nframework of integrating target and source data. We begin with the relatively\neasier covariate shift scenario with equal target and source mean functions and\npropose estimation and inferential procedures through a straightforward\ncombination of all target and source datasets. We next consider the more\ngeneral and flexible distribution shift scenario with arbitrary target and\nsource mean functions, and propose a two-step inferential procedure. First, we\nestimate the target-to-source differences based on separate portions of the\ntarget and source data. Second, the remaining source data are adjusted by these\ndifferences and combined with the remaining target data to perform the\nmultiplier bootstrap procedure. Our method enables local and global inference\non the target mean function without using asymptotic distributions. To justify\nour approach, we derive an optimal convergence rate for the nonparametric\nestimator and establish bootstrap consistency to estimate the asymptotic\ndistribution of the nonparametric estimator. The proof of global bootstrap\nconsistency involves a central limit theorem for quadratic forms with dependent\nvariables under a conditional probability measure. Our method applies to\narbitrary source and target datasets, provided that the data sizes meet a\nspecific quantitative relationship. Simulation studies and real data analysis\nare provided to examine the performance of our approach."
      ]
    }
  },
  {
    "id":2411.16728,
    "research_type":"applied",
    "start_id":"b21",
    "start_title":"Analysis methods for numerical weather prediction",
    "start_abstract":"Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods.",
    "start_categories":[
      "physics.data-an"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b10"
      ],
      "title":[
        "FuXi-S2S: An accurate machine learning model for global subseasonal forecasts"
      ],
      "abstract":[
        "Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Heavy-tailed random vectros: theory and applications",
        "QuESat: Satellite-Assisted Quantum Internet for Global-Scale\n  Entanglement Distribution",
        "Cheap Permutation Testing",
        "Efficient stochastic simulation of piecewise-deterministic Markov\n  processes and its application to the Morris-Lecar model of neural dynamics",
        "acoupi: An Open-Source Python Framework for Deploying Bioacoustic AI\n  Models on Edge Devices",
        "PolyhedronNet: Representation Learning for Polyhedra with\n  Surface-attributed Graph",
        "Splitting CEGM Amplitudes",
        "Using Covid-19 Response Policy to Estimate Open Water Swim Drafting\n  Effects in Triathlon",
        "Is Bellman Equation Enough for Learning Control?",
        "GPDFlow: Generative Multivariate Threshold Exceedance Modeling via\n  Normalizing Flows",
        "Non-polynomial conserved quantities for ODE systems and its application\n  to the long-time behavior of solutions to cubic NLS systems",
        "Representation in large language models",
        "LongReason: A Synthetic Long-Context Reasoning Benchmark via Context\n  Expansion",
        "BadJudge: Backdoor Vulnerabilities of LLM-as-a-Judge",
        "Positive self-commutators of positive operators",
        "Learning-based visibility prediction for terahertz communications in 6G\n  networks",
        "Memristor-Based Meta-Learning for Fast mmWave Beam Prediction in\n  Non-Stationary Environments",
        "DM-Adapter: Domain-Aware Mixture-of-Adapters for Text-Based Person\n  Retrieval",
        "Do computer vision foundation models learn the low-level characteristics\n  of the human visual system?",
        "Deterministic or probabilistic? The psychology of LLMs as random number\n  generators",
        "A solution to Haagerup's problem and positive Hahn-Banach separation\n  theorems in operator algebras",
        "LLM-Pack: Intuitive Grocery Handling for Logistics Applications",
        "The Quest for Visual Understanding: A Journey Through the Evolution of\n  Visual Question Answering",
        "Interview with Hyman Bass",
        "Semicomplete multipartite weakly distance-regular digraphs",
        "Large Capacity Data Hiding in Binary Image black and white mixed regions",
        "Self-Supervised Prompt Optimization",
        "FAST: Efficient Action Tokenization for Vision-Language-Action Models",
        "Deep Reinforcement Learning with Hybrid Intrinsic Reward Model"
      ],
      "abstract":[
        "In this paper we introduce and study several multivariate, heavy-tailed\ndistribution classes, and we explore their closure properties and their\napplications. We consider the class of multivariate, positively decreasing\ndistributions, and its intersection with other multivariate distribution\nclasses.",
        "Entanglement distribution across remote distances is critical for many\nquantum applications. Currently, the de facto approach for remote entanglement\ndistribution relies on optical fiber for on-the-ground entanglement\ndistribution. However, the fiber-based approach is incapable of global-scale\nentanglement distribution due to intrinsic limitations. This paper investigates\na new hybrid ground-satellite quantum network architecture (QuESat) for\nglobal-scale entanglement distribution, integrating an on-the-ground fiber\nnetwork with a global-scale passive optical network built with low-Earth-orbit\nsatellites. The satellite network provides dynamic construction of photon\nlightpaths based on near-vacuum beam guides constructed via adjustable arrays\nof lenses, forwarding photons from one ground station to another with very high\nefficiency over long distances compared to using fiber. To assess the\nfeasibility and effectiveness of QuESat for global communication, we formulate\nlightpath provisioning and entanglement distribution problems, considering the\norbital dynamics of satellites and the time-varying entanglement demands from\nground users. A two-stage algorithm is developed to dynamically configure the\nbeam guides and distribute entanglements, respectively. The algorithm combines\nrandomized and deterministic rounding for lightpath provisioning to enable\nglobal connectivity, with optimal entanglement swapping for distributing\nentanglements to meet users' demands. By developing a ground-satellite quantum\nnetwork simulator, QuESat achieves multi-fold improvements compared to repeater\nnetworks.",
        "Permutation tests are a popular choice for distinguishing distributions and\ntesting independence, due to their exact, finite-sample control of false\npositives and their minimax optimality when paired with U-statistics. However,\nstandard permutation tests are also expensive, requiring a test statistic to be\ncomputed hundreds or thousands of times to detect a separation between\ndistributions. In this work, we offer a simple approach to accelerate testing:\ngroup your datapoints into bins and permute only those bins. For U and\nV-statistics, we prove that these cheap permutation tests have two remarkable\nproperties. First, by storing appropriate sufficient statistics, a cheap test\ncan be run in time comparable to evaluating a single test statistic. Second,\ncheap permutation power closely approximates standard permutation power. As a\nresult, cheap tests inherit the exact false positive control and minimax\noptimality of standard permutation tests while running in a fraction of the\ntime. We complement these findings with improved power guarantees for standard\npermutation testing and experiments demonstrating the benefits of cheap\npermutations over standard maximum mean discrepancy (MMD), Hilbert-Schmidt\nindependence criterion (HSIC), random Fourier feature, Wilcoxon-Mann-Whitney,\ncross-MMD, and cross-HSIC tests.",
        "Piecewise-deterministic Markov processes combine continuous in time dynamics\nwith jump events, the rates of which generally depend on the continuous\nvariables and thus are not constants. This leads to a problem in a Monte-Carlo\nsimulation of such a system, where, at each step, one must find the time\ninstant of the next event. The latter is determined by an integral equation and\nusually is rather slow in numerical implementation. We suggest a reformulation\nof the next event problem as an ordinary differential equation where the\nindependent variable is not the time but the cumulative rate. This\nreformulation is similar to the H\\'enon approach to efficiently constructing\nthe Poincar\\'e map in deterministic dynamics. The problem is then reduced to a\nstandard numerical task of solving a system of ordinary differential equations\nwith given initial conditions on a prescribed interval. We illustrate the\nmethod with a stochastic Morris-Lecar model of neuron spiking with\nstochasticity in the opening and closing of voltage-gated ion channels.",
        "1. Passive acoustic monitoring (PAM) coupled with artificial intelligence\n(AI) is becoming an essential tool for biodiversity monitoring. Traditional PAM\nsystems require manual data offloading and impose substantial demands on\nstorage and computing infrastructure. The combination of on-device AI-based\nprocessing and network connectivity enables local data analysis and\ntransmission of only relevant information, greatly reducing storage needs.\nHowever, programming these devices for robust operation is challenging,\nrequiring expertise in embedded systems and software engineering. Despite the\nincrease in AI-based models for bioacoustics, their full potential remains\nunrealized without accessible tools to deploy them on custom hardware and\ntailor device behaviour to specific monitoring goals. 2. To address this\nchallenge, we develop acoupi, an open-source Python framework that simplifies\nthe creation and deployment of smart bioacoustic devices. acoupi integrates\naudio recording, AI-based data processing, data management, and real-time\nwireless messaging into a unified and configurable framework. By modularising\nkey elements of the bioacoustic monitoring workflow, acoupi allows users to\neasily customise, extend, or select specific components to fit their unique\nmonitoring needs. 3. We demonstrate the flexibility of acoupi by integrating\ntwo bioacoustic classifiers: BirdNET, for the classification of bird species,\nand BatDetect2, for the classification of UK bat species. We test the\nreliability of acoupi over a month-long deployment of two acoupi-powered\ndevices in a UK urban park. 4. acoupi can be deployed on low-cost hardware such\nas the Raspberry Pi and can be customised for various applications. acoupi\nstandardised framework and simplified tools facilitate the adoption of\nAI-powered PAM systems for researchers and conservationists. acoupi is on\nGitHub at https:\/\/github.com\/acoupi\/acoupi.",
        "Ubiquitous geometric objects can be precisely and efficiently represented as\npolyhedra. The transformation of a polyhedron into a vector, known as polyhedra\nrepresentation learning, is crucial for manipulating these shapes with\nmathematical and statistical tools for tasks like classification, clustering,\nand generation. Recent years have witnessed significant strides in this domain,\nyet most efforts focus on the vertex sequence of a polyhedron, neglecting the\ncomplex surface modeling crucial in real-world polyhedral objects. This study\nproposes \\textbf{PolyhedronNet}, a general framework tailored for learning\nrepresentations of 3D polyhedral objects. We propose the concept of the\nsurface-attributed graph to seamlessly model the vertices, edges, faces, and\ntheir geometric interrelationships within a polyhedron. To effectively learn\nthe representation of the entire surface-attributed graph, we first propose to\nbreak it down into local rigid representations to effectively learn each local\nregion's relative positions against the remaining regions without geometric\ninformation loss. Subsequently, we propose PolyhedronGNN to hierarchically\naggregate the local rigid representation via intra-face and inter-face\ngeometric message passing modules, to obtain a global representation that\nminimizes information loss while maintaining rotation and translation\ninvariance. Our experimental evaluations on four distinct datasets,\nencompassing both classification and retrieval tasks, substantiate\nPolyhedronNet's efficacy in capturing comprehensive and informative\nrepresentations of 3D polyhedral objects. Code and data are available at\n{https:\/\/github.com\/dyu62\/3D_polyhedron}.",
        "The CEGM formalism offers a general framework for scattering amplitudes,\nwhich rests on Grassmannians, moduli spaces and tropical geometry. The physical\nimplications of this generalization are still to be understood. Conventional\nwisdom says that key features of scattering amplitudes, like factorization at\ntheir poles into lower-point amplitudes, are associated to their singularities.\nThe factorization behavior of CEGM amplitudes at their poles is interesting but\ncomplicated. Recent developments have revealed important properties of standard\nparticle and string scattering amplitudes from factorizations, known as splits,\nthat happen away from poles. In this paper we introduce a kinematic subspace on\nwhich the CEGM amplitude splits into very simple rational functions. These\nfunctions, called simplex amplitudes, arise from stringy integrals for the\nmultivariate beta function, and also from restricting the biadjoint scalar\namplitude in quantum field theory to certain kinematic loci. Using split\nkinematics we also discover a specific class of zeros of the CEGM amplitude.\nOur construction rests on viewing positive moduli space as a product of\nsimplices, and it suggests a novel approach for deriving scattering amplitudes\nfrom tropical determinantal varieties.",
        "This study investigates the causal effects of open-water swim drafting by\nleveraging a natural experiment induced by staggered race starts during the\nCOVID-19 pandemic. Before 2020, athletes started in groups, enabling drafting\nbenefits, while pandemic-related restrictions significantly reduced these\nopportunities. Using agglomerative hierarchical clustering of swim-out times, I\nanalyze optimal drafting positions and estimate their impact on Swim-Out\nperformance. Our empirical findings reveal that swim drafting benefits were\nstatistically insignificant in 2020 but persisted post-pandemic at slightly\nreduced levels. I find that drafting becomes advantageous only from the third\ntrailing position onward, with earlier positions primarily serving to minimize\nfatigue. To mitigate endogeneity, I employ athlete and event fixed effects. The\nseemingly inverse decaying nature of drafting benefits partially addresses some\nconcerns of simultaneous reverse causality and omitted variable bias. This\nstudy provides the first largescale causal estimate of drafting effects in\nreal-world triathlon race settings.",
        "The Bellman equation and its continuous-time counterpart, the\nHamilton-Jacobi-Bellman (HJB) equation, serve as necessary conditions for\noptimality in reinforcement learning and optimal control. While the value\nfunction is known to be the unique solution to the Bellman equation in tabular\nsettings, we demonstrate that this uniqueness fails to hold in continuous state\nspaces. Specifically, for linear dynamical systems, we prove the Bellman\nequation admits at least $\\binom{2n}{n}$ solutions, where $n$ is the state\ndimension. Crucially, only one of these solutions yields both an optimal policy\nand a stable closed-loop system. We then demonstrate a common failure mode in\nvalue-based methods: convergence to unstable solutions due to the exponential\nimbalance between admissible and inadmissible solutions. Finally, we introduce\na positive-definite neural architecture that guarantees convergence to the\nstable solution by construction to address this issue.",
        "The multivariate generalized Pareto distribution (mGPD) is a common method\nfor modeling extreme threshold exceedance probabilities in environmental and\nfinancial risk management. Despite its broad applicability, mGPD faces\nchallenges due to the infinite possible parametrizations of its dependence\nfunction, with only a few parametric models available in practice. To address\nthis limitation, we introduce GPDFlow, an innovative mGPD model that leverages\nnormalizing flows to flexibly represent the dependence structure. Unlike\ntraditional parametric mGPD approaches, GPDFlow does not impose explicit\nparametric assumptions on dependence, resulting in greater flexibility and\nenhanced performance. Additionally, GPDFlow allows direct inference of marginal\nparameters, providing insights into marginal tail behavior. We derive tail\ndependence coefficients for GPDFlow, including a bivariate formulation, a\n$d$-dimensional extension, and an alternative measure for partial exceedance\ndependence. A general relationship between the bivariate tail dependence\ncoefficient and the generative samples from normalizing flows is discussed.\nThrough simulations and a practical application analyzing the risk among five\nmajor US banks, we demonstrate that GPDFlow significantly improves modeling\naccuracy and flexibility compared to traditional parametric methods.",
        "In this paper, we investigate the asymptotic behavior of small solutions to\nthe initial value problem for a system of cubic nonlinear Schrodinger equations\n(NLS) in one spatial dimension. We identify a new class of NLS systems for\nwhich the global boundedness and asymptotics of small solutions can be\nestablished, even in the absence of any effective conserved quantity. The key\nto this analysis lies in utilizing conserved quantities for the reduced\nordinary differential equation (ODE) systems derived from the original NLS\nsystems. In a previous study, the first author investigated conserved\nquantities expressed as quartic polynomials. In contrast, the conserved\nquantities considered in the present paper are of a different type and are not\nnecessarily polynomial.",
        "The extraordinary success of recent Large Language Models (LLMs) on a diverse\narray of tasks has led to an explosion of scientific and philosophical\ntheorizing aimed at explaining how they do what they do. Unfortunately,\ndisagreement over fundamental theoretical issues has led to stalemate, with\nentrenched camps of LLM optimists and pessimists often committed to very\ndifferent views of how these systems work. Overcoming stalemate requires\nagreement on fundamental questions, and the goal of this paper is to address\none such question, namely: is LLM behavior driven partly by\nrepresentation-based information processing of the sort implicated in\nbiological cognition, or is it driven entirely by processes of memorization and\nstochastic table look-up? This is a question about what kind of algorithm LLMs\nimplement, and the answer carries serious implications for higher level\nquestions about whether these systems have beliefs, intentions, concepts,\nknowledge, and understanding. I argue that LLM behavior is partially driven by\nrepresentation-based information processing, and then I describe and defend a\nseries of practical techniques for investigating these representations and\ndeveloping explanations on their basis. The resulting account provides a\ngroundwork for future theorizing about language models and their successors.",
        "Large language models (LLMs) have demonstrated remarkable progress in\nunderstanding long-context inputs. However, benchmarks for evaluating the\nlong-context reasoning abilities of LLMs fall behind the pace. Existing\nbenchmarks often focus on a narrow range of tasks or those that do not demand\ncomplex reasoning. To address this gap and enable a more comprehensive\nevaluation of the long-context reasoning capabilities of current LLMs, we\npropose a new synthetic benchmark, LongReason, which is constructed by\nsynthesizing long-context reasoning questions from a varied set of\nshort-context reasoning questions through context expansion. LongReason\nconsists of 794 multiple-choice reasoning questions with diverse reasoning\npatterns across three task categories: reading comprehension, logical\ninference, and mathematical word problems. We evaluate 21 LLMs on LongReason,\nrevealing that most models experience significant performance drops as context\nlength increases. Our further analysis shows that even state-of-the-art LLMs\nstill have significant room for improvement in providing robust reasoning\nacross different tasks. We have open-sourced LongReason under\nhttps:\/\/huggingface.co\/datasets\/lz1bytedance\/LongReason to support the\ncomprehensive evaluation of LLMs' long-context reasoning capabilities.",
        "This paper proposes a novel backdoor threat attacking the LLM-as-a-Judge\nevaluation regime, where the adversary controls both the candidate and\nevaluator model. The backdoored evaluator victimizes benign users by unfairly\nassigning inflated scores to adversary. A trivial single token backdoor\npoisoning 1% of the evaluator training data triples the adversary's score with\nrespect to their legitimate score. We systematically categorize levels of data\naccess corresponding to three real-world settings, (1) web poisoning, (2)\nmalicious annotator, and (3) weight poisoning. These regimes reflect a weak to\nstrong escalation of data access that highly correlates with attack severity.\nUnder the weakest assumptions - web poisoning (1), the adversary still induces\na 20% score inflation. Likewise, in the (3) weight poisoning regime, the\nstronger assumptions enable the adversary to inflate their scores from 1.5\/5 to\n4.9\/5. The backdoor threat generalizes across different evaluator\narchitectures, trigger designs, evaluation tasks, and poisoning rates. By\npoisoning 10% of the evaluator training data, we control toxicity judges\n(Guardrails) to misclassify toxic prompts as non-toxic 89% of the time, and\ndocument reranker judges in RAG to rank the poisoned document first 97% of the\ntime. LLM-as-a-Judge is uniquely positioned at the intersection of ethics and\ntechnology, where social implications of mislead model selection and evaluation\nconstrain the available defensive tools. Amidst these challenges, model merging\nemerges as a principled tool to offset the backdoor, reducing ASR to near 0%\nwhilst maintaining SOTA performance. Model merging's low computational cost and\nconvenient integration into the current LLM Judge training pipeline position it\nas a promising avenue for backdoor mitigation in the LLM-as-a-Judge setting.",
        "We consider a positive operator $A$ on a Hilbert lattice such that its\nself-commutator $C = A^* A - A A^*$ is positive. If $A$ is also idempotent,\nthen it is an orthogonal projection, and so $C = 0$. Similarly, if $A$ is power\ncompact, then $C = 0$ as well. We prove that every positive compact central\noperator on a separable infinite-dimensional Hilbert lattice $\\mathcal H$ is a\nself-commutator of a positive operator. We also show that every positive\ncentral operator on $\\mathcal H$ is a sum of two positive self-commutators of\npositive operators.",
        "Terahertz communications are envisioned as a key enabler for 6G networks. The\nabundant spectrum available in such ultra high frequencies has the potential to\nincrease network capacity to huge data rates. However, they are extremely\naffected by blockages, to the point of disrupting ongoing communications. In\nthis paper, we elaborate on the relevance of predicting visibility between\nusers and access points (APs) to improve the performance of THz-based networks\nby minimizing blockages, that is, maximizing network availability, while at the\nsame time keeping a low reconfiguration overhead. We propose a novel approach\nto address this problem, by combining a neural network (NN) for predicting\nfuture user-AP visibility probability, with a probability threshold for AP\nreselection to avoid unnecessary reconfigurations. Our experimental results\ndemonstrate that current state-of-the-art handover mechanisms based on received\nsignal strength are not adequate for THz communications, since they are\nill-suited to handle hard blockages. Our proposed NN-based solution\nsignificantly outperforms them, demonstrating the interest of our strategy as a\nresearch line.",
        "Traditional machine learning techniques have achieved great success in\nimproving data-rate performance and reducing latency in millimeter wave\n(mmWave) communications. However, these methods still face two key challenges:\n(i) their reliance on large-scale paired data for model training and tuning\nwhich limits performance gains and makes beam predictions outdated, especially\nin multi-user mmWave systems with large antenna arrays, and (ii) meta-learning\n(ML)-based beamforming solutions are prone to overfitting when trained on a\nlimited number of tasks. To address these issues, we propose a memristorbased\nmeta-learning (M-ML) framework for predicting mmWave beam in real time. The\nM-ML framework generates optimal initialization parameters during the training\nphase, providing a strong starting point for adapting to unknown environments\nduring the testing phase. By leveraging memory to store key data, M-ML ensures\nthe predicted beamforming vectors are wellsuited to episodically dynamic\nchannel distributions, even when testing and training environments do not\nalign. Simulation results show that our approach delivers high prediction\naccuracy in new environments, without relying on large datasets. Moreover, MML\nenhances the model's generalization ability and adaptability.",
        "Text-based person retrieval (TPR) has gained significant attention as a\nfine-grained and challenging task that closely aligns with practical\napplications. Tailoring CLIP to person domain is now a emerging research topic\ndue to the abundant knowledge of vision-language pretraining, but challenges\nstill remain during fine-tuning: (i) Previous full-model fine-tuning in TPR is\ncomputationally expensive and prone to overfitting.(ii) Existing\nparameter-efficient transfer learning (PETL) for TPR lacks of fine-grained\nfeature extraction. To address these issues, we propose Domain-Aware\nMixture-of-Adapters (DM-Adapter), which unifies Mixture-of-Experts (MOE) and\nPETL to enhance fine-grained feature representations while maintaining\nefficiency. Specifically, Sparse Mixture-of-Adapters is designed in parallel to\nMLP layers in both vision and language branches, where different experts\nspecialize in distinct aspects of person knowledge to handle features more\nfinely. To promote the router to exploit domain information effectively and\nalleviate the routing imbalance, Domain-Aware Router is then developed by\nbuilding a novel gating function and injecting learnable domain-aware prompts.\nExtensive experiments show that our DM-Adapter achieves state-of-the-art\nperformance, outperforming previous methods by a significant margin.",
        "Computer vision foundation models, such as DINO or OpenCLIP, are trained in a\nself-supervised manner on large image datasets. Analogously, substantial\nevidence suggests that the human visual system (HVS) is influenced by the\nstatistical distribution of colors and patterns in the natural world,\ncharacteristics also present in the training data of foundation models. The\nquestion we address in this paper is whether foundation models trained on\nnatural images mimic some of the low-level characteristics of the human visual\nsystem, such as contrast detection, contrast masking, and contrast constancy.\nSpecifically, we designed a protocol comprising nine test types to evaluate the\nimage encoders of 45 foundation and generative models. Our results indicate\nthat some foundation models (e.g., DINO, DINOv2, and OpenCLIP), share some of\nthe characteristics of human vision, but other models show little resemblance.\nFoundation models tend to show smaller sensitivity to low contrast and rather\nirregular responses to contrast across frequencies. The foundation models show\nthe best agreement with human data in terms of contrast masking. Our findings\nsuggest that human vision and computer vision may take both similar and\ndifferent paths when learning to interpret images of the real world. Overall,\nwhile differences remain, foundation models trained on vision tasks start to\nalign with low-level human vision, with DINOv2 showing the closest resemblance.",
        "Large Language Models (LLMs) have transformed text generation through\ninherently probabilistic context-aware mechanisms, mimicking human natural\nlanguage. In this paper, we systematically investigate the performance of\nvarious LLMs when generating random numbers, considering diverse configurations\nsuch as different model architectures, numerical ranges, temperature, and\nprompt languages. Our results reveal that, despite their stochastic\ntransformers-based architecture, these models often exhibit deterministic\nresponses when prompted for random numerical outputs. In particular, we find\nsignificant differences when changing the model, as well as the prompt\nlanguage, attributing this phenomenon to biases deeply embedded within the\ntraining data. Models such as DeepSeek-R1 can shed some light on the internal\nreasoning process of LLMs, despite arriving to similar results. These biases\ninduce predictable patterns that undermine genuine randomness, as LLMs are\nnothing but reproducing our own human cognitive biases.",
        "We affirmatively resolve a question posed by Uffe Haagerup in 1975 on the\npositive version of the bipolar theorem on the dual spaces of C$^*$-algebras.\nAs a direct consequence, we obtain a complete set of four positive Hahn-Banach\nseparation theorems on von Neumann algebras, their preduals, C$^*$-algebras,\nand their duals.",
        "Robotics and automation are increasingly influential in logistics but remain\nlargely confined to traditional warehouses. In grocery retail, advancements\nsuch as cashier-less supermarkets exist, yet customers still manually pick and\npack groceries. While there has been a substantial focus in robotics on the bin\npicking problem, the task of packing objects and groceries has remained largely\nuntouched. However, packing grocery items in the right order is crucial for\npreventing product damage, e.g., heavy objects should not be placed on top of\nfragile ones. However, the exact criteria for the right packing order are hard\nto define, in particular given the huge variety of objects typically found in\nstores. In this paper, we introduce LLM-Pack, a novel approach for grocery\npacking. LLM-Pack leverages language and vision foundation models for\nidentifying groceries and generating a packing sequence that mimics human\npacking strategy. LLM-Pack does not require dedicated training to handle new\ngrocery items and its modularity allows easy upgrades of the underlying\nfoundation models. We extensively evaluate our approach to demonstrate its\nperformance. We will make the source code of LLMPack publicly available upon\nthe publication of this manuscript.",
        "Visual Question Answering (VQA) is an interdisciplinary field that bridges\nthe gap between computer vision (CV) and natural language processing(NLP),\nenabling Artificial Intelligence(AI) systems to answer questions about images.\nSince its inception in 2015, VQA has rapidly evolved, driven by advances in\ndeep learning, attention mechanisms, and transformer-based models. This survey\ntraces the journey of VQA from its early days, through major breakthroughs,\nsuch as attention mechanisms, compositional reasoning, and the rise of\nvision-language pre-training methods. We highlight key models, datasets, and\ntechniques that shaped the development of VQA systems, emphasizing the pivotal\nrole of transformer architectures and multimodal pre-training in driving recent\nprogress. Additionally, we explore specialized applications of VQA in domains\nlike healthcare and discuss ongoing challenges, such as dataset bias, model\ninterpretability, and the need for common-sense reasoning. Lastly, we discuss\nthe emerging trends in large multimodal language models and the integration of\nexternal knowledge, offering insights into the future directions of VQA. This\npaper aims to provide a comprehensive overview of the evolution of VQA,\nhighlighting both its current state and potential advancements.",
        "Interview with Hyman Bass, whose mathematical life has spanned seven decades.",
        "A digraph is semicomplete multipartite if its underlying graph is a complete\nmultipartite graph. As a special case of semicomplete multipartite digraphs,\nJ{\\o}rgensen et al. \\cite{JG14} initiated the study of doubly regular team\ntournaments. As a natural extension, we introduce doubly regular team\nsemicomplete multipartite digraphs and show that such digraphs fall into three\ntypes. Furthermore, we give a characterization of all semicomplete multipartite\ncommutative weakly distance-regular digraphs.",
        "Information hiding technology utilizes the insensitivity of human sensory\norgans to redundant data, hiding confidential information in the redundant data\nof these public digital media, and then transmitting it. The carrier media\nafter hiding secret information only displays its own characteristics, which\ncan ensure the transmission of confidential information without being detected,\nthereby greatly improving the security of the information. In theory, any\ndigital media including image, video, audio, and text can serve as a host\ncarrier. Among them, hiding information in binary images poses great\nchallenges. As we know, any information hiding method involves modifying the\ndata of the host carrier. The more information hidden, the more data of the\nhost carrier are modified. In this paper, we propose information hiding in the\nblack-and-white mixed region of binary images, which can greatly reduce visual\ndistortion. In addition, we propose an efficient encoding to achieve\nhigh-capacity information hiding while ensuring image semantics. By selecting\nbinary images of different themes, we conduct experiments. The experimental\nresults prove the feasibility of our technique and verify the expected\nperformance. Since the candidate units for information hiding are selected from\nequally sized blocks that the image is divided into, and the hiding and\nextraction of information are based on a shared encoding table, the\ncomputational cost is very low, making it suitable for real-time information\nhiding applications.",
        "Well-designed prompts are crucial for enhancing Large language models' (LLMs)\nreasoning capabilities while aligning their outputs with task requirements\nacross diverse domains. However, manually designed prompts require expertise\nand iterative experimentation. While existing prompt optimization methods aim\nto automate this process, they rely heavily on external references such as\nground truth or by humans, limiting their applicability in real-world scenarios\nwhere such data is unavailable or costly to obtain. To address this, we propose\nSelf-Supervised Prompt Optimization (SPO), a cost-efficient framework that\ndiscovers effective prompts for both closed and open-ended tasks without\nrequiring external reference. Motivated by the observations that prompt quality\nmanifests directly in LLM outputs and LLMs can effectively assess adherence to\ntask requirements, we derive evaluation and optimization signals purely from\noutput comparisons. Specifically, SPO selects superior prompts through pairwise\noutput comparisons evaluated by an LLM evaluator, followed by an LLM optimizer\nthat aligns outputs with task requirements. Extensive experiments demonstrate\nthat SPO outperforms state-of-the-art prompt optimization methods, achieving\ncomparable or superior results with significantly lower costs (e.g., 1.1% to\n5.6% of existing methods) and fewer samples (e.g., three samples). The code is\navailable at https:\/\/github.com\/geekan\/MetaGPT\/blob\/main\/examples\/spo",
        "Autoregressive sequence models, such as Transformer-based vision-language\naction (VLA) policies, can be tremendously effective for capturing complex and\ngeneralizable robotic behaviors. However, such models require us to choose a\ntokenization of our continuous action signals, which determines how the\ndiscrete symbols predicted by the model map to continuous robot actions. We\nfind that current approaches for robot action tokenization, based on simple\nper-dimension, per-timestep binning schemes, typically perform poorly when\nlearning dexterous skills from high-frequency robot data. To address this\nchallenge, we propose a new compression-based tokenization scheme for robot\nactions, based on the discrete cosine transform. Our tokenization approach,\nFrequency-space Action Sequence Tokenization (FAST), enables us to train\nautoregressive VLAs for highly dexterous and high-frequency tasks where\nstandard discretization methods fail completely. Based on FAST, we release\nFAST+, a universal robot action tokenizer, trained on 1M real robot action\ntrajectories. It can be used as a black-box tokenizer for a wide range of robot\naction sequences, with diverse action spaces and control frequencies. Finally,\nwe show that, when combined with the pi0 VLA, our method can scale to training\non 10k hours of robot data and match the performance of diffusion VLAs, while\nreducing training time by up to 5x.",
        "Intrinsic reward shaping has emerged as a prevalent approach to solving\nhard-exploration and sparse-rewards environments in reinforcement learning\n(RL). While single intrinsic rewards, such as curiosity-driven or novelty-based\nmethods, have shown effectiveness, they often limit the diversity and\nefficiency of exploration. Moreover, the potential and principle of combining\nmultiple intrinsic rewards remains insufficiently explored. To address this\ngap, we introduce HIRE (Hybrid Intrinsic REward), a flexible and elegant\nframework for creating hybrid intrinsic rewards through deliberate fusion\nstrategies. With HIRE, we conduct a systematic analysis of the application of\nhybrid intrinsic rewards in both general and unsupervised RL across multiple\nbenchmarks. Extensive experiments demonstrate that HIRE can significantly\nenhance exploration efficiency and diversity, as well as skill acquisition in\ncomplex and dynamic settings."
      ]
    }
  },
  {
    "id":2411.02466,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"A review of artificial intelligence in prostate cancer detection on imaging",
    "start_abstract":"A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning"
      ],
      "abstract":[
        "Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Chaos in a Nonlinear Wavefunction Model: An Alternative to Born's\n  Probability Hypothesis",
        "Relative Reality",
        "A Study on the Line of Sight to Galaxies Detected at Gamma-ray Energies",
        "Planet formation and long-term stability in a very eccentric stellar\n  binary",
        "Fractional kinetic modelling of the adsorption and desorption process\n  from experimental SPR curves",
        "Noise equals endogenous control",
        "Coupling and Acceleration of Externally Injected Electron Beams in\n  Laser-Driven Plasma Wakefields",
        "$C^1$ Robust Rigidity for Bi-critical Circle Maps",
        "A quantum walk inspired model for distributed computing on arbitrary\n  graphs",
        "The hardcore brokers: Core-periphery structure and political\n  representation in Denmark's corporate elite network",
        "On $L_p$ Brunn-Minkowski type inequalities for a general class of\n  functionals",
        "A class of moving boundary problems with an exponential source term",
        "Scotogenic Froggatt-Nielsen and the Versatility of Soft Symmetry\n  Breaking",
        "Statistical Collusion by Collectives on Learning Platforms",
        "Discretionary vs nondiscretionary in fiscal mechanism. Non-automatic\n  fiscal stabilisers vs automatic fiscal stabilisers",
        "Decomposition results for multiplicative actions and applications",
        "Anomalous Chern-Simons orbital magnetoelectric coupling of\n  three-dimensional Chern insulators: gauge-discontinuity formalism and\n  adiabatic pumping",
        "Controlling the spontaneous emission of trapped ions",
        "Investigating the shadows of new regular black holes with a Minkowski\n  core: Effects of spherical accretion and core type differences",
        "Imaging thick objects with deep-sub-angstrom resolution and\n  deep-sub-picometer precision",
        "Measurement of $\\rm ^{6}H$ ground state energy in an electron scattering\n  experiment at MAMI-A1",
        "The collisionless hydrodynamics: On the nonexistence of collisionless\n  shocks",
        "NeuralFoil: An Airfoil Aerodynamics Analysis Tool Using Physics-Informed\n  Machine Learning",
        "The $M_{*}-M_{\\rm BH}$ Relation Evolution from z $\\sim$ 6 to the Present\n  Epoch",
        "Global Geometry within an SPDE Well-Posedness Problem",
        "Some Kummer extensions over maximal cyclotomic fields, a finiteness\n  theorem of Ribet and TKND-AVKF fields",
        "FOSS solution for Molecular Dynamics Simulation Automation and\n  Collaboration with MDSGAT",
        "A helical magnetic field in quasar NRAO150 revealed by Faraday rotation",
        "Light scalar beyond the Higgs mixing limit"
      ],
      "abstract":[
        "In a prior paper, the author described an instability in a nonlinear\nwavefunction model. Proposed in connection with the Measurement Problem, the\nmodel contained an external potential creating a ``classical'' instability.\nHowever, it is interesting to ask whether such models possess an intrinsic\nrandomness -- even ``chaos\" -- independent of external potentials. In this\nwork, I investigate the criterion analytically and simulate from a small (``3\nqubit\") model, demonstrating that the Lyapunov exponent -- a standard measure\nof ``chaos\" -- is positive. I also extend the instability criterion to models\nin the continuum. These results suggest that the boundary between classical and\nwavefunction physics may also constitute the threshold of chaos, and present an\nalternative to Max Born's ad hoc probability hypothesis: random outcomes in\nexperiments result not from ``wave-particle duality\" or ``the existence of the\nquantum,\" but from sensitive dependence on initial conditions, as is common in\nthe other sciences.",
        "The ``Hard Problem\" of consciousness refers to a long-standing enigma about\nhow qualia emerge from physical processes in the brain. Building on insights\nfrom the development of non-Euclidean geometry, this paper seeks to present a\nstructured and logically coherent theory of qualia to address this problem. The\nproposed theory starts with a definition on what it means for an entity to be\nnon-physical. A postulate about awareness is posed and utilized to rigorously\nprove that qualia are non-physical and thoughts are qualia. Then the paper\nintroduces a key concept: relative reality, meaning that perceptions of reality\nare relative to the observer and time. The concept is analyzed through a\nmathematical model grounded in Hilbert space theory. The model also sheds new\nlight on cognitive science and physics. In particular, the Schr\\\"{o}dinger\nequation can be derived easily through this model. Moreover, this model shows\nthat eigenstates also exist for classical energy-conserving systems. Analyses\non the G. P. Thomson experiment and the classical harmonic oscillator are made\nto illustrate this finding. The insight gained sheds new light on the\nBohr-Einstein debate concerning the interpretation of quantum mechanics. At\nlast, the paper proposes a postulate about qualia force and demonstrates that\nit constitutes a fundamental part of absolute reality, much like the four\nfundamental forces in nature.",
        "The large-scale Universal structure comprises strands of dark matter and\ngalaxies with large under-dense volumes known as voids. We measure the fraction\nof the line of sight that intersects voids for active galactic nuclei (AGN)\ndetected by Fermi Large Area Telescope (LAT) and quasars from the Sloan Digital\nSky Survey (SDSS). This ``voidiness'' fraction is a rudimentary proxy for the\ndensity along the line of sight to the galaxies. The voidiness of SDSS-observed\nquasars (QSOs) is distinctly different from randomly distributed source\npopulations, with a median p-value of $4.6\\times10^{-5}$ and $\\ll\n1\\times10^{-7}$, when compared with 500 simulated populations with randomly\nsimulated locations but matching redshifts in the $0.1\\leq z<0.4$ and $0.4\\leq\nz < 0.7$ intervals, respectively. A similar comparison of the voidiness for\nLAT-detected AGN shows median p-values greater than 0.05 in each redshift\ninterval. When comparing the SDSS QSO population to the LAT-detected AGN, we\nmitigate potential bias from a relationship between redshift and voidiness by\ncomparing the LAT-detected AGN to a ``redshift-matched'' set of SDSS QSOs. The\nLAT-detected AGN between a redshift of 0.4 and 0.7 show higher voidiness\ncompared to the redshift-matched SDSS QSO populations, with a median p-value of\n2.3$\\times10^{-5}$, (a $4.1\\sigma$ deviation). No deviation is found when\ncomparing the same populations between redshifts of 0.1 and 0.4 (p>0.05). We do\nnot study possible causes of this voidiness difference. It might relate to\npropagation effects from lower magnetic or radiative background fields within\nvoids or to an environment more favorable for gamma-ray production for AGN near\nvoids.",
        "Planets orbiting one of the two stars in a binary are vulnerable to\ngravitational perturbations from the other star. Particularly, highly eccentric\ncompanion stars risk disrupting planetary orbits, such as in the extreme system\nTOI 4633 where close encounters between the companion and a gas giant planet in\nthe habitable zone make it one of the most fragile systems discovered so far.\nHere, we report that TOI 4633's planet likely survived these encounters\nthroughout the system's age by orbiting retrograde relative to the binary,\nstabilised by the Coriolis force. Using direct $N$-body simulations, we show it\notherwise tends to collide with the binary stars or becomes free-floating after\ngetting ejected. A retrograde planetary orbit has profound implications for TOI\n4633's formation and evolution, suggesting an extraordinary history where its\neccentric companion was likely randomly captured after planet formation in a\nsingle-star system. Alternatively, if stars and planet are born in situ from\nthe same gas clump, we show the planet must have formed at sub-snow-line\ndistances, contrary to the conventional core-accretion model. Our study\nhighlights the importance of considering the long-term stability ($\\gtrsim\\rm\nGyr$) of planets in eccentric binaries and demonstrates that the mere existence\nin such dynamically hostile environments places strong constraints on their\norbital configuration and formation.",
        "The application of surface plasmon resonance (SPR) has transformed the field\nof study of interactions between a ligand immobilized on the surface of a\nsensor chip, designated as $L_S$, and an analyte in solution, referred to as\n$A$. This technique enables the real-time measurement of interactions with high\nsensitivity. The dynamics of adsorption-desorption process, $A+L_S \\rightarrow\nAL_S$, can be expressed mathematically as a set of coupled integer-order\ndifferential equations. However, this approach has limited ability to acoount\nfor temperature distribution, diffusion and transport effects involved in the\nreaction process. The fractional kinetic model provides a methodology for\nincorporating non-local effects into the problem. In this study, the proposed\nmodel was applied to analyze data to the interaction between Immobilized Baru\nProtein (IBP) and Congo Red dye (CR) at concentrations ranging from $7.5$ to\n$97.5$ $\\mu M$, at pH $7.4$ and $16^o$ C. The variation in the kinetic\nconstants was studied, and it was demonstrated that the integer-order model is\nunable to adequately represent the experimental data. This work has shown that\nthe fractional-order model is capable of capturing the complexity of the\nadsorption-desorption process involved in the SPR data.",
        "Stochastic systems have a control-theoretic interpretation in which noise\nplays the role of endogenous control. In the weak-noise limit, relevant at low\ntemperatures or in large populations, control is optimal and an exact\nmathematical mapping from noise to control is described, where the maximizing\nthe probability of a state becomes the control objective. In Langevin dynamics\nnoise is identified directly with control, while in general Markov jump\nprocesses, which include chemical reaction networks and electronic circuits, we\nuse the Doi-Zel'dovich-Grassberger-Goldenfeld-Peliti path integral to identify\nthe `response' or `tilt' field $\\pi$ as control, which is proportional to the\nnoise in the semiclassical limit. This solves the longstanding problem of\ninterpreting $\\pi$. We illustrate the mapping on multistable chemical reaction\nnetworks and systems with unstable fixed points. The noise-control mapping\nbuilds intuition for otherwise puzzling phenomena of stochastic systems: why\nthe probability is generically a non-smooth function of state out of thermal\nequilibrium; why biological mechanisms can work better in the presence of\nnoise; and how agentic behavior emerges naturally without recourse to\nmysticism.",
        "The multi-stage method of laser wakefield acceleration (LWFA) presents a\npromising approach for developing stable, full-optical, high-energy electron\naccelerators. By segmenting the acceleration process into several booster\nstages, each powered by independent laser drivers, this technique effectively\nmitigates challenges such as electron dephasing, pump depletion, and laser\ndiffraction. A critical aspect of multi-stage LWFA is the nonlinear interaction\nbetween the injected electron beam and the laser-driven wakefields in the\nbooster stage. This study investigates the injection and acceleration of\nexternal electron beams within wakefields in the booster stage using\nmulti-dimensional Particle-In-Cell (PIC) simulations. We provide both\nqualitative and quantitative descriptions of the observed physical processes.\nKey parameters influencing charge coupling process and the resultant beam\nquality have been identified. Furthermore, we have examined how off-axis\ninjection relative to the driver laser influences the acceleration process and\nbeam quality. Our findings provide valuable insights for advancing and\noptimizing multi-stage plasma-based accelerators.",
        "We prove that two topologically conjugate bi-critical circle maps whose\nsignatures are the same, and whose renormalizations converge together\nexponentially fast in the $C^2$-topology, are $C^1$ conjugate.",
        "A discrete time quantum walk is known to be the single-particle sector of a\nquantum cellular automaton. For a long time, these models have interested the\ncommunity for their nice properties such as locality or translation invariance.\nThis work introduces a model of distributed computation for arbitrary graphs\ninspired by quantum cellular automata. As a by-product, we show how this model\ncan reproduce the dynamic of a quantum walk on graphs. In this context, we\ninvestigate the communication cost for two interaction schemes. Finally, we\nexplain how this particular quantum walk can be applied to solve the search\nproblem and present numerical results on different types of topologies.",
        "Who represents the corporate elite in democratic governance? Prior studies\nfind a tightly integrated \"inner circle\" network representing the corporate\nelite politically across varieties of capitalism, yet they all rely on data\nfrom a highly select sample of leaders from only the largest corporations. We\ncast a wider net. Analyzing new data on all members of corporate boards in the\nDanish economy (200k directors in 120k boards), we locate 1500 directors that\noperate as brokers between local corporate networks. We measure their network\ncoreness using k-core detection and find a highly connected core of 275\ndirectors, half of which are affiliated with smaller firms or subsidiaries.\nAnalyses show a strong positive association between director coreness and the\nlikelihood of joining one of the 650 government committees epitomizing\nDenmark's social-corporatist model of governance (net of firm and director\ncharacteristics). The political network premium is largest for directors of\nsmaller firms or subsidiaries, indicating that network coreness is a key driver\nof business political representation, especially for directors without claims\nto market power or weight in formal interest organizations.",
        "In this work, the $L_p$ version (for $p> 1$) of the dimensional\nBrunn-Minkowski inequality for the standard Gaussian measure $\\gamma_n(\\cdot)$\non $\\mathbb{R}^n$ is shown. More precisely, we prove that for any $0$-symmetric\nconvex sets with nonempty interior, any $p>1$, and every $\\lambda \\in (0,1)$,\n\\[ \\gamma_n\\bigl((1-\\lambda)\\cdot K+_p \\lambda \\cdot L\\bigr)^{p\/n} \\geqslant\n(1-\\lambda ) \\gamma_n(K)^{p\/n} + \\lambda \\gamma_n(L)^{p\/n}, \\] with equality,\nfor some $\\lambda \\in (0,1)$ and $p>1$, if and only if $K=L$. This result,\nrecently established without the equality conditions by Hosle, Kolesnikov and\nLivshyts, by using a different and functional approach, turns out to be the\n$L_p$ extension of a celebrated result for the Minkowski sum (that is, for\n$p=1$) by Eskenazis and Moschidis (2021) on a problem by Gardner and Zvavitch\n(2010).\n  Moreover, an $L_p$ Brunn-Minkowski type inequality is obtained for the\nclassical Wills functional $\\mathcal{W}(\\cdot)$ of convex bodies.\n  These results are derived as a consequence of a more general approach, which\nprovides us with other remarkable examples of functionals satisfying $L_p$\nBrunn-Minkowski type inequalities, such as different absolutely continuous\nmeasures with radially decreasing densities.",
        "This work investigates a class of moving boundary problems related to a\nnonlinear evolution equation featuring an exponential source term. We establish\na connection to Stefan-type problems, for different boundary conditions at the\nfixed face, through the application of a reciprocal transformation alongside\nthe Cole-Hopf transformation. For specific cases, we derive explicit similarity\nsolutions in parametric form. This innovative approach enhances our\nunderstanding of the underlying dynamics and offers valuable insights into the\nbehavior of these systems.",
        "Preserving the unique role of the one Higgs doublet of the standard model, it\nis proposed that quark and lepton mass patterns, often ascribed to the\nFroggatt-Nielsen mechanism using nonrenormalizable higher-dimensional terms,\nmay be enforced in a renormalizable theory of just one Higgs doublet by the\nscotogenic mechanism with soft symmetry breaking in the dark sector. A revised\nversion of the original $A_4$ model of charged leptons and neutrinos is\ndiscussed.",
        "As platforms increasingly rely on learning algorithms, collectives may form\nand seek ways to influence these platforms to align with their own interests.\nThis can be achieved by coordinated submission of altered data. To evaluate the\npotential impact of such behavior, it is essential to understand the\ncomputations that collectives must perform to impact platforms in this way. In\nparticular, collectives need to make a priori assessments of the effect of the\ncollective before taking action, as they may face potential risks when\nmodifying their data. Moreover they need to develop implementable coordination\nalgorithms based on quantities that can be inferred from observed data. We\ndevelop a framework that provides a theoretical and algorithmic treatment of\nthese issues and present experimental results in a product evaluation domain.",
        "The goal of the present study is to increase the intelligibility of\nmacroeconomic phenomena triggered by governmental intervention in economy by\nmeans of fiscal policies. During cyclical movements, fiscal policy can play an\nimportant role in order to help stabilise the economy. But discretionary policy\nusually implies implementation lags and is not automatically reversed when\neconomic conditions change. In contrast, automatic fiscal stabilisers (SFA)\nensure a prompter, and self-correcting fiscal response. The present study aims\nto tackle the topic of discretionary vs nondiscretionary characteristic of\nfiscal stabilisers (SF). In this context, the scope of the research undertaking\nis to launch a scientific debate over the definitions of the concepts of\nnon-automatic fiscal stabilisers (SfnA) and SFAs. We describe how we can\nquantify the discretionary and non-discretionary character of the fiscal\npolicy, by the analysis of the structure of the conventional budget balance\n(SBc), budget balance associated with the current GDP. In the final part of\nthis article, we propose a quantitative equilibrium model for establishing the\nmathematical prerequisites for an SF to become automatic. Likewise, on the\nbasis of the proposed mathematical model we have performed a qualitative\nanalysis of the influence factors.",
        "Motivated by partition regularity problems of homogeneous quadratic\nequations, we prove multiple recurrence and convergence results for\nmultiplicative measure preserving actions with iterates given by rational\nsequences involving polynomials that factor into products of linear forms in\ntwo variables. We focus mainly on actions that are finitely generated, and the\nkey tool in our analysis is a decomposition result for any bounded measurable\nfunction into a sum of two components, one that mimics concentration properties\nof pretentious multiplicative functions and another that mimics vanishing\nproperties of aperiodic multiplicative functions. Crucial to part of our\narguments are some new seminorms that are defined by a mixture of addition and\nmultiplication of the iterates of the action, and we prove an inverse theorem\nthat explicitly characterizes the factor of the system on which these seminorms\nvanish.",
        "Chern-Simons orbital magnetoelectric (OME) coupling is usually the hallmark\nof nontrivial band topology in three-dimensional (3D) crystalline insulators.\nHowever, if a 3D insulator exhibits nonzero Chern number within any\ntwo-dimensional plane of the Brillouin zone, then traditionally the\nChern-Simons coupling becomes ill defined for such 3D Chern insulators due to\ntopological obstructions. In this work, by employing a ``gauge-discontinuity\"\nformalism, we resolve this long-standing issue and rigorously derive a\nquantized layer-resolved OME response in 3D Chern insulators. We demonstrate\nthat the difference of the layer-resolved OME coupling between adjacent layers\nis universally quantized in unit of $-C e^2\/h$, where $C$ is the Chern number.\nThis quantization arises from an anomalous contribution to the Chern-Simons OME\ncoupling, which is closely associated with the Berry curvature of the occupied\nbands and the hybrid Wannier centers along the direction of the Chern vector\n$(0,0, C)$. Furthermore, we demonstrate that the anomalous Chern-Simons\ncoupling can be transported by an exact integer quantum from one unit cell to\nits neighboring cell through an adiabatic cyclic pumping process, accompanied\nby a quantized displacement of Wannier center along the direction of the Chern\nvector. Our work provides a rigorous theoretical framework for understanding\nmagnetoelectric response in 3D Chern insulators and opens avenues for designing\ntopological quantum phenomena in layered systems.",
        "We propose an experimental setup for manipulating the spontaneous emission of\ntrapped ions, based on a spatial light modulator. Anticipated novelties include\nthe potential to entangle more than two ions through a single photon detection\nevent and control the visibility for spatially distinguishable emitters. The\nsetup can be adapted to most of the existing ion traps commonly used in quantum\ntechnology.",
        "We investigate the shadows and optical appearances of a new type of regular\nblack holes (BHs) with a Minkowski core under different spherical accretion.\nThese BHs are constructed by modifying the Newtonian potential based on the\nminimum observable length in the Generalized Uncertainty Principle (GUP). They\ncorrespond one-to-one with the traditional regular BHs with a de-Sitter (dS)\ncore (such as Bardeen\/Hayward BHs), characterized by quantum gravity effect\nparameter ($\\alpha_0$) and spacetime deformation factor ($n$). We find that the\ncharacteristic parameters give rise to some novel observable features. For\nthese new BHs, the shadow radius, photon sphere radius, and total observed\nintensity increase with the increase of $\\alpha_0$ but decrease with the\nincrease of $n$. Under different spherical accretion, the shadow and photon\nsphere radius are identical, but the total observed intensity under the static\nspherical accretion is greater than that under the infalling spherical\naccretion. In addition, we find that these regular BHs with different cores\nshow differences in shadows and optical appearances, especially under the\nstatic spherical accretion. Compared with Bardeen BH, the new BH has a smaller\ntotal observed intensity, dimmer maximum luminosity, and smaller shadow and\nphoton sphere radius. The larger $\\alpha_0$ leads to more significant\ndifferences, and a similar trend is also seen in the comparison with Hayward\nBH. Under the infalling spherical accretion, these regular BHs with different\ncores only have slight differences in total observed intensity, shadow and\nphoton sphere radius, which become more obvious when $\\alpha_0$ is relatively\nlarge. It suggests that the unique spacetime features of these regular BHs with\ndifferent cores can be distinguished through astronomical observations.",
        "Size effects are ubiquitous in the structural, mechanical, and physical\nproperties of materials, making it highly desirable to study the intrinsic\nproperties of thick objects through high-resolution structural analysis in\ntransmission electron microscopy. Although deep-sub-angstrom resolution has\nbeen achieved with multislice electron ptychography, the sample thickness is\ntypically very limited. By combining energy filtering and extended\nlocal-orbital ptychography (eLOP) that retrieves varying aberrations during\nelectron scanning, here we report ptychographic reconstructions for silicon as\nthick as 85 nm, approximately three times larger than usual thickness threshold\nfor conventional multislice electron ptychography. The elimination of\naberration variations contributes to accurate reconstructions with an\ninformation limit of 18 pm and atomic position precision of 0.39 pm. Accurate\nptychographic reconstructions for thick objects can facilitate the discovery or\ninterpretation of intrinsic structural and physical phenomena in solids, which\nis of great significance in physics, chemistry, materials science, and\nsemiconductor device engineering.",
        "For the first time the neutron-rich hydrogen isotope $\\rm ^{6}H$ was produced\nin an electron scattering experiment in the reaction $\\rm\n^{7}Li(e,~e'p\\pi^{+})^{6}H$ using the spectrometer facility of the A1\nCollaboration at the Mainz Microtron accelerator. By measuring the triple\ncoincidence between the scattered electron, the produced proton, and $\\pi^{+}$,\nthe missing mass spectrum of $\\rm ^{6}H$ was obtained. A clear peak above\n$^3$H+n+n+n energy threshold was seen resulting in a ground state energy of\n$\\rm ^{6}H$ at $2.3\\pm0.5({\\rm stat.})\\pm0.4({\\rm syst.})$ MeV with a width of\n$1.9\\pm1.0({\\rm stat.})\\pm0.4({\\rm syst.})$ MeV. This work challenges the\nunderstandings of multi-nucleon interactions and presents a new method to study\nlight neutron-rich nuclei with electron scattering experiments.",
        "Collisionless shocks, essential for astrophysics, perhaps do not exist as\nstatistically stationary solutions. If so, any quantitative statement about a\ncollisionless shock should be qualified by the age of the shock.\n  A theoretical description of the upstream of the 1+1 dimensional\nelectrostatic collisionless shock is developed -- collisionless hydrodynamics.\nPeculiarities of collisionless hydrodynamics prevent a shock formation when a\npiston is driven into cold plasma. An exact self-similar solution is found\ninstead; the spatial extent of the solution grows linearly in time.\n  Direct numerical simulations of plasma kinetics in 1+1 dimensions confirm the\nhydrodynamic result -- a statistically steady collisionless shock doesn't\nexist. Instead, at each fixed time, there is a continuous succession in space\nof marginally stable velocity distribution functions. The spatial support of\nthis continuous succession grows linearly in time.",
        "NeuralFoil is an open-source Python-based tool for rapid aerodynamics\nanalysis of airfoils, similar in purpose to XFoil. Speedups ranging from 8x to\n1,000x over XFoil are demonstrated, after controlling for equivalent accuracy.\nNeuralFoil computes both global and local quantities (lift, drag, velocity\ndistribution, etc.) over a broad input space, including: an 18-dimensional\nspace of airfoil shapes, possibly including control deflections; a 360 degree\nrange of angles of attack; Reynolds numbers from $10^2$ to $10^{10}$; subsonic\nflows up to the transonic drag rise; and with varying turbulence parameters.\nResults match those of XFoil closely: the mean relative error of drag is 0.37%\non simple cases, and remains as low as 2.0% on a test dataset with numerous\npost-stall and transitional cases. NeuralFoil facilitates gradient-based design\noptimization, due to its $C^\\infty$-continuous solutions,\nautomatic-differentiation-compatibility, and bounded computational cost without\nnon-convergence issues.\n  NeuralFoil is a hybrid of physics-informed machine learning techniques and\nanalytical models. Here, physics information includes symmetries that are\nstructurally embedded into the model architecture, feature engineering using\ndomain knowledge, and guaranteed extrapolation to known limit cases. This work\nalso introduces a new approach for surrogate model uncertainty quantification\nthat enables robust design optimization.\n  This work discusses the methodology and performance of NeuralFoil with\nseveral case studies, including a practical airfoil design optimization study\nincluding both aerodynamic and non-aerodynamic constraints. Here, NeuralFoil\noptimization is able to produce airfoils nearly identical in performance and\nshape to expert-designed airfoils within seconds; these\ncomputationally-optimized airfoils provide a useful starting point for further\nexpert refinement.",
        "The ratio between the stellar mass of a galaxy, $M_{*}$, and that of its\ncentral supermassive black hole (SMBH), $M_\\bullet$, the ``Magorrian''\nrelationship, traces their coevolution. JWST observations have suggested\nsignificant evolution in $M_\\bullet\/M_{*}$ relative to local scaling\nrelationships both in low-mass galaxies and in quasars at z $\\ge$ 4. We test\nthis possibility by (1) determining the preferred $M_\\bullet\/M_{*}$ scaling\nrelation among those proposed locally; and (2) providing uniform host galaxy\nstellar mass estimates. These steps reduce the prominence of the reported\nevolution. We then apply Monte Carlo simulations to account for observational\nbiases. We still find a significant increase over the local scaling relation in\n$M_\\bullet\/M_{*}$ for z $\\ge$ 4 SMBHs in very low-mass galaxies\n($\\log(M_*\/M_{\\odot})<10$). However, similarly high values of $M_\\bullet\/M_{*}$\nare also found in low mass galaxies at $z \\sim$ 0.5 to 3 that may be common at\ncosmic noon. Nonetheless, galaxies with similar behavior are rare locally and\nnot accounted for in the local scaling relations. In contrast, z $\\sim$ 6\nquasars can have $M_\\bullet\/M_{*}$ well above the local relation value, but\nthey can be explained as extreme cases still within the scaling relation for\ntheir higher mass host galaxies. Black holes in some of them and in the\nlow-mass systems may be undergoing very high accretion episodes that result in\nhigh $M_\\bullet\/M_{*}$ but that will be followed by quiescent periods when\ngrowth of the host drives the systems toward more typical $M_\\bullet\/M_{*}$\nvalues.",
        "On a closed Riemannian manifold, we construct a family of intrinsic Gaussian\nnoises indexed by a regularity parameter $\\alpha\\geq0$ to study the\nwell-posedness of the Parabolic Anderson model. We show that with rough initial\nconditions, the equation is well-posed assuming non-positive curvature with a\ncondition on $\\alpha$ similar to that of Riesz kernel-correlated noise in\nEuclidean space. The argument was made in direct mode, showing that it is\npossible to bypass Fourier analysis, which was used in all previous work with\nrough initial conditions. Non-positive curvature was used to overcome a new\ndifficulty introduced by non-uniqueness of geodesics in this setting, which\nrequired exploration of global geometry. The well-posedness argument also\nproduces exponentially growing in time upper bounds for the moments. Using the\ngood structure of our noise, we obtain new exponentially growing in time second\nmoment lower bounds for our solutions with bounded initial condition.",
        "It is a theorem of Ribet that an abelian variety defined over a number field\n$K$ has only finitely many torsion points with values in the maximal cyclotomic\nextension field $K^{\\mathrm{cyc}}$ of $K$. Recently, R\\\"ossler and Szamuely\ngeneralized Ribet's theorem in terms of the \\'etale cohomology with\n$\\mathbb{Q}\/\\mathbb{Z}$-coefficients of a smooth proper variety. In this paper,\nwe show that the same finiteness holds even after replacing $K^{\\mathrm{cyc}}$\nwith the field obtained by adjoining to $K$ all roots of all elements of a\ncertain subset of $K$. Furthermore, we give some new examples of TKND-AVKF\nfields; the notion of TKND-AVKF is introduced by Hoshi, Mochizuki and\nTsujimura, and TKND-AVKF fields are expected as one of suitable base fields for\nanabelian geometry.",
        "The process of setting up and successfully running Molecular Dynamics\nSimulations (MDS) is outlined to be incredibly labour and computationally\nexpensive with a very high barrier to entry for newcomers wishing to utilise\nthe benefits and insights of MDS. Here, presented, is a unique Free and\nOpen-Source Software (FOSS) solution that aims to not only reduce the barrier\nof entry for new Molecular Dynamics (MD) users, but also significantly reduce\nthe setup time and hardware utilisation overhead for even highly experienced MD\nresearchers. This is accomplished through the creation of the Molecular\nDynamics Simulation Generator and Analysis Tool (MDSGAT) which currently serves\nas a viable alternative to other restrictive or privatised MDS Graphical\nsolutions with a unique design that allows for seamless collaboration and\ndistribution of exact MD simulation setups and initialisation parameters\nthrough a single setup file. This solution is designed from the start with a\nmodular mindset allowing for additional software expansion to incorporate\nnumerous extra MDS packages and analysis methods over time",
        "Active Galactic Nuclei (AGN) are some of the most luminous and extreme\nenvironments in the Universe. The central engines of AGN, believed to be\nsuper-massive black-holes, are fed by accretion discs threaded by magnetic\nfields within a dense magneto-ionic medium. We report our findings from\npolarimetric Very-long-baseline Interferometry (VLBI) observations of quasar\nNRAO150 taken in October 2022 using a combined network of the Very Long\nBaseline Array (VLBA) and Effelsberg 100-m Radio Telescope. These observations\nare the first co-temporal multi-frequency polarimetric VLBI observations of\nNRAO150 at frequencies above 15GHz. We use the new VLBI polarization\ncalibration procedure, GPCAL, with polarization observations of frequencies of\n12GHz, 15GHz, 24GHz, and 43GHz of NRAO150. From these observations, we measure\nFaraday rotation. Using our measurement of Faraday rotation, we also derive the\nintrinsic electric vector position angle (EVPA0) for the source. As a\ncomplementary measurement we determine the behavior of polarization as a\nfunction of observed frequency. The polarization from NRAO150 only comes from\nthe core region, with a peak polarization intensity occurring at 24GHz. Across\nthe core region of NRAO150 we see clear gradients in Faraday rotation and EVPA0\nvalues that are aligned with the direction of the jet curving around the core\nregion. We find that for the majority of the polarized region the polarization\nfraction is greater at higher frequencies, with intrinsic polarization\nfractions in the core 3%. The Faraday rotation gradients and circular patterns\nin EVPA0 are strong evidence for a helical\/toroidal magnetic field, and the\npresence of low intrinsic polarization fractions indicate that the polarized\nemission and hence the helical\/toroidal magnetic field, occur within the\ninnermost jet.",
        "We explore the possibility that the interactions of a light scalar singlet,\nwhich mixes with the Standard Model~(SM) Higgs, also receive other UV\ncontributions of comparable size. We focus, in particular, on the flavor\naligned limit, where couplings of the light scalar to the SM are almost flavor\ndiagonal, but not necessarily proportional to the Higgs Yukawa couplings. The\nphenomenology of such a general flavor aligned light scalar differs from both\nthe Higgs-mixed scalar, as well as from a general axion-like particle. We\nexplore this for light scalar masses below a few hundred MeV, such that they\ncan be produced in kaon decays, and in decays of $\\eta$ and $\\eta'$ mesons, and\nthe transitions described using chiral perturbation theory. We then derive\nconstraints on the light scalar interactions, assuming that light scalar decays\nare either just into photons or are invisible. We also discuss several UV\nexamples of such light scalar models: a two-Higgs doublet model extended by a\nlight scalar, a light dilaton from the dark sector, and a SM extended by heavy\nvector-like quarks and a light scalar. For the latter we also performed\nmatching onto low energy theory at one-loop."
      ]
    }
  },
  {
    "id":2411.02466,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning",
    "start_abstract":"Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "A review of artificial intelligence in prostate cancer detection on imaging"
      ],
      "abstract":[
        "A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "A binary PSO based ensemble under-sampling model for rebalancing\n  imbalanced training data",
        "Enhanced Atom-by-Atom Assembly of Defect-Free Two-Dimensional\n  Mixed-Species Atomic Arrays",
        "Revisiting Ensemble Methods for Stock Trading and Crypto Trading Tasks\n  at ACM ICAIF FinRL Contest 2023-2024",
        "Soybean pod and seed counting in both outdoor fields and indoor\n  laboratories using unions of deep neural networks",
        "In the Picture: Medical Imaging Datasets, Artifacts, and their Living\n  Review",
        "Variations on hypergeometric functions",
        "Power-law banded random matrix ensemble as a model for quantum many-body\n  Hamiltonians",
        "Quantitative study on anomalous Nernst effect in Co thin films by laser\n  irradiation",
        "Deviations from the Porter-Thomas distribution due to non-statistical\n  $\\gamma$ decay below the $^{150}$Nd neutron separation threshold",
        "Enhanced Continual Learning of Vision-Language Models with Model Fusion",
        "Language Representation Favored Zero-Shot Cross-Domain Cognitive\n  Diagnosis",
        "Subjective and Objective Quality Assessment of Non-Uniformly Distorted\n  Omnidirectional Images",
        "Divide-and-Conquer: Tree-structured Strategy with Answer Distribution\n  Estimator for Goal-Oriented Visual Dialogue",
        "Dammann Metasurface Route to Overcoming the Uniformity Defects in\n  Two-Dimensional Beam Multipliers",
        "Parsings of Stationary Processes, Stopping Times and the Fundamental\n  Pointwise Convergence Theorems of Ergodic Theory",
        "Hiding in Plain Sight: RIS-Aided Target Obfuscation in ISAC",
        "Degradation-based Energy Management for Microgrids in the Presence of\n  Energy Storage Elements",
        "UMC: Unified Resilient Controller for Legged Robots with Joint\n  Malfunctions",
        "Autellix: An Efficient Serving Engine for LLM Agents as General Programs",
        "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question\n  Generation and Answering",
        "FACTER: Fairness-Aware Conformal Thresholding and Prompt Engineering for\n  Enabling Fair LLM-Based Recommender Systems",
        "Distributionally Robust Model Predictive Control with Mixture of\n  Gaussian Processes",
        "Micromagnetic formalism for magnetic multipoles",
        "Advancing vision-language models in front-end development via data\n  synthesis",
        "Neural Chaos: A Spectral Stochastic Neural Operator",
        "Concept camera for the next-generation mm-wave cosmological surveys",
        "\\'Etude statistique du facteur premier m\\'edian, 2 : lois locales",
        "The Derrida-Retaux model on a geometric Galton-Watson tree",
        "Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety\n  Perception"
      ],
      "abstract":[
        "Ensemble technique and under-sampling technique are both effective tools used\nfor imbalanced dataset classification problems. In this paper, a novel ensemble\nmethod combining the advantages of both ensemble learning for biasing\nclassifiers and a new under-sampling method is proposed. The under-sampling\nmethod is named Binary PSO instance selection; it gathers with ensemble\nclassifiers to find the most suitable length and combination of the majority\nclass samples to build a new dataset with minority class samples. The proposed\nmethod adopts multi-objective strategy, and contribution of this method is a\nnotable improvement of the performances of imbalanced classification, and in\nthe meantime guaranteeing a best integrity possible for the original dataset.\nWe experimented the proposed method and compared its performance of processing\nimbalanced datasets with several other conventional basic ensemble methods.\nExperiment is also conducted on these imbalanced datasets using an improved\nversion where ensemble classifiers are wrapped in the Binary PSO instance\nselection. According to experimental results, our proposed methods outperform\nsingle ensemble methods, state-of-the-art under-sampling methods, and also\ncombinations of these methods with the traditional PSO instance selection\nalgorithm.",
        "Defect-free single atom array in optical tweezers is a promising platform for\nscalable quantum computing, quantum simulation, and quantum metrology.\nExtending single-species array to mixed-species one promise to offer new\npossibilities. In our recent proof of principle realization of defect-free\ntwo-dimensional assembly of mixed-species $^{85}$Rb ($^{87}$Rb) atom arrays [C.\nSheng et\nal.\\href{https:\/\/journals.aps.org\/prl\/abstract\/10.1103\/PhysRevLett.128.083202}{{\\color{blue}\nPhys. Rev. Lett. 128, 083202(2022)}}], the filling fractions were limited by\nthe imperfect transfer of atoms and the occurrence of logjams during the atom\nrearrangement. In order to scale up the size of defect-free mixed-species atom\narray, we scale up the tweezer array and improve the atom transfer, and upgrade\nthe heuristic heteronuclear algorithm so as to facilitate multiple\nrearrangement cycles. Consequently, we successfully create defect-free atom\narrays with 120 mixed-species single atoms. The corresponding filling fraction\nand defect-free probability are improved to be 98.6(1)\\% and 14(2)\\%,\nrespectively. It is anticipated that the enhanced algorithm can be extended to\nother combinations of atomic species, and this mixed-species atom array is\nreadily for studies of many-body physics, quantum error correction, and quantum\nmetrology.",
        "Reinforcement learning has demonstrated great potential for performing\nfinancial tasks. However, it faces two major challenges: policy instability and\nsampling bottlenecks. In this paper, we revisit ensemble methods with massively\nparallel simulations on graphics processing units (GPUs), significantly\nenhancing the computational efficiency and robustness of trained models in\nvolatile financial markets. Our approach leverages the parallel processing\ncapability of GPUs to significantly improve the sampling speed for training\nensemble models. The ensemble models combine the strengths of component agents\nto improve the robustness of financial decision-making strategies. We conduct\nexperiments in both stock and cryptocurrency trading tasks to evaluate the\neffectiveness of our approach. Massively parallel simulation on a single GPU\nimproves the sampling speed by up to $1,746\\times$ using $2,048$ parallel\nenvironments compared to a single environment. The ensemble models have high\ncumulative returns and outperform some individual agents, reducing maximum\ndrawdown by up to $4.17\\%$ and improving the Sharpe ratio by up to $0.21$.\n  This paper describes trading tasks at ACM ICAIF FinRL Contests in 2023 and\n2024.",
        "Automatic counting soybean pods and seeds in outdoor fields allows for rapid\nyield estimation before harvesting, while indoor laboratory counting offers\ngreater accuracy. Both methods can significantly accelerate the breeding\nprocess. However, it remains challenging for accurately counting pods and seeds\nin outdoor fields, and there are still no accurate enough tools for counting\npods and seeds in laboratories. In this study, we developed efficient deep\nlearning models for counting soybean pods and seeds in both outdoor fields and\nindoor laboratories. For outdoor fields, annotating not only visible seeds but\nalso occluded seeds makes YOLO have the ability to estimate the number of\nsoybean seeds that are occluded. Moreover, we enhanced YOLO architecture by\nintegrating it with HQ-SAM (YOLO-SAM), and domain adaptation techniques\n(YOLO-DA), to improve model robustness and generalization across soybean images\ntaken in outdoor fields. Testing on soybean images from the outdoor field, we\nachieved a mean absolute error (MAE) of 6.13 for pod counting and 10.05 for\nseed counting. For the indoor setting, we utilized Mask-RCNN supplemented with\na Swin Transformer module (Mask-RCNN-Swin), models were trained exclusively on\nsynthetic training images generated from a small set of labeled data. This\napproach resulted in near-perfect accuracy, with an MAE of 1.07 for pod\ncounting and 1.33 for seed counting across actual laboratory images from two\ndistinct studies.",
        "Datasets play a critical role in medical imaging research, yet issues such as\nlabel quality, shortcuts, and metadata are often overlooked. This lack of\nattention may harm the generalizability of algorithms and, consequently,\nnegatively impact patient outcomes. While existing medical imaging literature\nreviews mostly focus on machine learning (ML) methods, with only a few focusing\non datasets for specific applications, these reviews remain static -- they are\npublished once and not updated thereafter. This fails to account for emerging\nevidence, such as biases, shortcuts, and additional annotations that other\nresearchers may contribute after the dataset is published. We refer to these\nnewly discovered findings of datasets as research artifacts. To address this\ngap, we propose a living review that continuously tracks public datasets and\ntheir associated research artifacts across multiple medical imaging\napplications. Our approach includes a framework for the living review to\nmonitor data documentation artifacts, and an SQL database to visualize the\ncitation relationships between research artifact and dataset. Lastly, we\ndiscuss key considerations for creating medical imaging datasets, review best\npractices for data annotation, discuss the significance of shortcuts and\ndemographic diversity, and emphasize the importance of managing datasets\nthroughout their entire lifecycle. Our demo is publicly available at\nhttp:\/\/130.226.140.142.",
        "We prove new integral formulas for generalized hypergeometric functions and\ntheir confuent variants. We apply them, via stationary phase formula, to study\nWKB expansions of solutions: for large argument in the confuent case and for\nlarge parameter in the general case. We also study variations of hypergeometric\nfunctions for small perturbations of hypergeometric equations, i.e., in\nexpansions of solutions in powers of a small parameter. Next, we present a new\nproof of a theorem due to Wasow about equivalence of the Airy equation with its\nperturbation; in particular, we explain that this result does not deal with the\nWKB solutions and the Stokes phenomenon. Finally, we study hypergeometric\nequations, one of second order and another of third order, which are related\nwith two generating functions for MZVs, one $\\Delta_2 (\\lambda )$ for $\\zeta(2,\n\\ldots , 2)$'s and another $\\Delta_3 (\\lambda )$ for $\\zeta(3, \\ldots , 3)$'s;\nin particular, we correct a statement from [ZZ3] that the function\n$\\Delta_3(\\lambda)$ admits a regular WKB expansion.",
        "Hamiltonians of one-dimensional, disordered single-particle systems with\nlong-range hopping terms can naturally be modeled by power-law banded random\nmatrices. In this picture, the phase diagram of a power-law banded random\nmatrix ensemble show ergodic, weakly ergodic, multifractal, and localized\nphases. Motivated by recent developments on ergodicity breaking and\nlocalization in interacting quantum many-body systems, we explore many-body\ninterpretations of the power-law banded random matrix ensemble. We discuss a\nnumber of ways to label the basis vectors with many-body configurations, and\ncompare the physical properties of the resulting Hamiltonians. We characterize\nthe scaling of the many-body eigenstate entanglement entropy with system size\nfor the different labeling schemes and in each of the phases. Using a scaling\nanalysis on the full sets of eigenstates, we subsequently provide a\nquantitative picture of the boundary between the different types of scaling\nbehavior that we observe for the spectral-bulk and spectral-edge eigenstates.",
        "The anomalous Nernst effect (ANE) generates electromotive forces transverse\nto temperature gradients and has attracted much attention for potential\napplications into novel thermoelectric power generators. ANE efficiency is\ngenerally characterized by uniform temperature gradients in a steady state\nprepared by heaters. However, although focusing laser beams on a magnetic film\ncan form much larger temperature gradients, the laser irradiation method has\nnot been sufficiently considered for quantifying the ANE coefficient due to the\ndifficulty in estimating the localized in-homogeneous temperature gradients. In\nthis study, we present a quantitative study of ANE in Ru(5\nnm)\/Co($t_{\\mathrm{Co}}$) ($t_{\\mathrm{Co}}$ = 3, 5, 7, 10, 20, 40, and 60 nm)\nbilayers on sapphire (0001) substrates by combining a laser irradiation\napproach with finite element analysis of temperature gradients under laser\nexcitation. We find that the estimated ANE coefficients are consistent with\npreviously reported values and one independently characterized using a heater.\nOur results also reveal the advantages of the laser irradiation method over the\nconventional method using heaters. Intensity-modulated laser beams can create\nac temperature gradients as large as approximately 10$^3$ K\/mm at a frequency\nof tens of kilohertz in a micrometer-scale region.",
        "We introduce a new method for the study of fluctuations of partial transition\nwidths based on nuclear resonance fluorescence experiments with\nquasimonochromatic linearly-polarized photon beams below particle separation\nthresholds. It is based on the average branching of decays of $J=1$ states of\nan even-even nucleus to the $2^+_1$ state in comparison to the ground state.\nBetween 5 and 7 MeV, an almost constant average branching ratio of 0.490(16) is\nobserved for the nuclide $^{150}$Nd. Assuming $\\chi^2$-distributed partial\ntransition widths, this average branching ratio is related to a degree of\nfreedom of $\\nu = 1.93(12)$, rejecting the validity of the Porter-Thomas\ndistribution, requiring $\\nu=1$. The observed deviation can be explained by\nnon-statistical effects in the $\\gamma$-decay behavior with contributions in\nthe range of 9.4(10)% up to 94(10)%.",
        "Vision-Language Models (VLMs) represent a breakthrough in artificial\nintelligence by integrating visual and textual modalities to achieve impressive\nzero-shot capabilities. However, VLMs are susceptible to catastrophic\nforgetting when sequentially fine-tuned on multiple downstream tasks. Existing\ncontinual learning methods for VLMs often rely heavily on additional reference\ndatasets, compromise zero-shot performance, or are limited to\nparameter-efficient fine-tuning scenarios. In this paper, we propose Continual\nDecoupling-Unifying (ConDU), a novel approach, by introducing model fusion into\ncontinual learning for VLMs. ConDU maintains a unified model along with task\ntriggers and prototype sets, employing an iterative process of decoupling\ntask-specific models for previous tasks and unifying them with the model for\nthe newly learned task. Additionally, we introduce an inference strategy for\nzero-shot scenarios by aggregating predictions from multiple decoupled\ntask-specific models. Extensive experiments across various settings show that\nConDU achieves up to a 2\\% improvement in average performance across all seen\ntasks compared to state-of-the-art baselines, while also enhancing zero-shot\ncapabilities relative to the original VLM.",
        "Cognitive diagnosis aims to infer students' mastery levels based on their\nhistorical response logs. However, existing cognitive diagnosis models (CDMs),\nwhich rely on ID embeddings, often have to train specific models on specific\ndomains. This limitation may hinder their directly practical application in\nvarious target domains, such as different subjects (e.g., Math, English and\nPhysics) or different education platforms (e.g., ASSISTments, Junyi Academy and\nKhan Academy). To address this issue, this paper proposes the language\nrepresentation favored zero-shot cross-domain cognitive diagnosis (LRCD).\nSpecifically, LRCD first analyzes the behavior patterns of students, exercises\nand concepts in different domains, and then describes the profiles of students,\nexercises and concepts using textual descriptions. Via recent advanced\ntext-embedding modules, these profiles can be transformed to vectors in the\nunified language space. Moreover, to address the discrepancy between the\nlanguage space and the cognitive diagnosis space, we propose language-cognitive\nmappers in LRCD to learn the mapping from the former to the latter. Then, these\nprofiles can be easily and efficiently integrated and trained with existing\nCDMs. Extensive experiments show that training LRCD on real-world datasets can\nachieve commendable zero-shot performance across different target domains, and\nin some cases, it can even achieve competitive performance with some classic\nCDMs trained on the full response data on target domains. Notably, we\nsurprisingly find that LRCD can also provide interesting insights into the\ndifferences between various subjects (such as humanities and sciences) and\nsources (such as primary and secondary education).",
        "Omnidirectional image quality assessment (OIQA) has been one of the hot\ntopics in IQA with the continuous development of VR techniques, and achieved\nmuch success in the past few years. However, most studies devote themselves to\nthe uniform distortion issue, i.e., all regions of an omnidirectional image are\nperturbed by the ``same amount'' of noise, while ignoring the non-uniform\ndistortion issue, i.e., partial regions undergo ``different amount'' of\nperturbation with the other regions in the same omnidirectional image.\nAdditionally, nearly all OIQA models are verified on the platforms containing a\nlimited number of samples, which largely increases the over-fitting risk and\ntherefore impedes the development of OIQA. To alleviate these issues, we\nelaborately explore this topic from both subjective and objective perspectives.\nSpecifically, we construct a large OIQA database containing 10,320\nnon-uniformly distorted omnidirectional images, each of which is generated by\nconsidering quality impairments on one or two camera len(s). Then we\nmeticulously conduct psychophysical experiments and delve into the influence of\nboth holistic and individual factors (i.e., distortion range and viewing\ncondition) on omnidirectional image quality. Furthermore, we propose a\nperception-guided OIQA model for non-uniform distortion by adaptively\nsimulating users' viewing behavior. Experimental results demonstrate that the\nproposed model outperforms state-of-the-art methods. The source code is\navailable at https:\/\/github.com\/RJL2000\/OIQAND.",
        "Goal-oriented visual dialogue involves multi-round interaction between\nartificial agents, which has been of remarkable attention due to its wide\napplications. Given a visual scene, this task occurs when a Questioner asks an\naction-oriented question and an Answerer responds with the intent of letting\nthe Questioner know the correct action to take. The quality of questions\naffects the accuracy and efficiency of the target search progress. However,\nexisting methods lack a clear strategy to guide the generation of questions,\nresulting in the randomness in the search process and inconvergent results. We\npropose a Tree-Structured Strategy with Answer Distribution Estimator (TSADE)\nwhich guides the question generation by excluding half of the current candidate\nobjects in each round. The above process is implemented by maximizing a binary\nreward inspired by the ``divide-and-conquer'' paradigm. We further design a\ncandidate-minimization reward which encourages the model to narrow down the\nscope of candidate objects toward the end of the dialogue. We experimentally\ndemonstrate that our method can enable the agents to achieve high task-oriented\naccuracy with fewer repeating questions and rounds compared to traditional\nergodic question generation approaches. Qualitative results further show that\nTSADE facilitates agents to generate higher-quality questions.",
        "Dammann gratings - beam-shaping optical elements acting as beam multipliers\nwith equal-power beams - are a key element in three-dimensional imaging based\non structured light and beam combiners for high-power laser applications.\nHowever, two-dimensional Dammann grating structures suffer from a significant\nreduction of the uniformity among the diffraction orders. Here, we report\nDammann metasurfaces based on the geometric phase as the structure realization\nfor the target phase profile, which outperform the capabilities of Dammann\ngratings by overcoming the uniformity defects in their two-dimensional\ndiffraction patterns. We showed that two-dimensional Dammann metasurfaces\nexhibit high uniformity and diffraction efficiency, in contrast to Dammann\ngratings, by overcoming the uniformity defects via a robust and highly precise\nphase imprint. Moreover, Dammann metasurfaces outperform their grating\ncounterparts by exhibiting a polarization-independent response and a broadband\noperation. This study reveals that by providing physics-driven solutions,\nmetasurfaces can outperform the capabilities of their bulk optics counterparts\nwhile facilitating virtually flat, ultrathin, and lightweight optics.",
        "The idea of a parsing of a stationary process according to a collection of\nwords is introduced, and the basic framework required for the asymptotic\nanalysis of these parsings is presented. We demonstrate how the pointwise\nergodic theorem and the Shannon-McMillan-Breiman theorem can be deduced from\ntheir respective weaker convergence in probability versions combined with our\nobservations regarding parsings, where the parsings are done according to\ncollections that originate in stopping times tailored for that purpose.",
        "Integrated sensing and communication (ISAC) has been identified as a\npromising technology for the sixth generation (6G) of communication networks.\nTarget privacy in ISAC is essential to ensure that only legitimate sensors can\ndetect the target while keeping it hidden from malicious ones. In this paper,\nwe consider a downlink reconfigurable intelligent surface (RIS)-assisted ISAC\nsystem capable of protecting a sensing region against an adversarial detector.\nThe RIS consists of both reflecting and sensing elements, adaptively changing\nthe element assignment based on system needs. To achieve this, we minimize the\nmaximum sensing signal-to-interference-plus-noise-ratio (SINR) at the\nadversarial detector within sample points in the sensing region, by optimizing\nthe transmit beamformer at the base station, the RIS phase shift matrix, the\nreceived beamformer at the RIS, and the division between reflecting and\nabsorptive elements at the RIS, where the latter function as sensing elements.\nAt the same time, the system is designed to maintain a minimum sensing SINR at\neach monitored location, as well as minimum communication SINR for each user.\nTo solve this challenging optimization problem, we develop an alternating\noptimization approach combined with a successive convex approximation based\nmethod tailored for each subproblem. Our results show that the proposed\napproach achieves a 25 dB reduction in the maximum sensing SINR at the\nadversarial detector compared to scenarios without sensing area protection.\nAlso, the optimal RIS element assignment can further improve sensing protection\nby 3 dB over RISs with fixed element configuration.",
        "Integration of Inverter-based Resources (IBRs) such as solar-powered plants\nwhich lack the intrinsic characteristics such as the inertial response of the\ntraditional synchronous-generator (SG) based sources presents a new challenge\nin the form of analyzing the grid stability under their presence. For example,\nsolar power is available for approximately from 9 AM-5 PM. However, the result\nof the rise in power consumption after 6 PM and the reverting back to the\nnon-renewable source of power generation during that period puts immense stress\non the grid, testing the ramp limitations of the SGs. Failure to meet the\nrequired power demand due to SG ramp limitations leads to failure of the power\ngrid and other catastrophes. Numerous mitigation techniques exist in order to\naddress the ramping issues with adding the energy storage elements (ESE) to the\ngrid being one. ESEs have higher ramping capabilities compared to the\ntraditional SGs. Also, the ESEs can store the energy and supply it to the grid\nwhen required making them extremely responsive to high ramp situations.\nHowever, the rate of degradation of the ESEs is faster than the SGs. This\nraises an important issue of addressing the degradation of the ESEs while\nmeeting the required power demand objectives and constraints. This work\nproposes a battery degradation-aware model predictive energy management\nstrategy and it is tested via a numerical simulation on multiple physical\nsystems such as Shipboard Power Systems (SPS). Moreover, the risk arising due\nto the fault in the IBR is also studied by means of a numerical simulation.\nOverall, the goal of this study is to make the existing power grid more robust,\nresilient, and risk-free from component degradation and eventual failures.",
        "Adaptation to unpredictable damages is crucial for autonomous legged robots,\nyet existing methods based on multi-policy or meta-learning frameworks face\nchallenges like limited generalization and complex maintenance. To address this\nissue, we first analyze and summarize eight types of damage scenarios,\nincluding sensor failures and joint malfunctions. Then, we propose a novel,\nmodel-free, two-stage training framework, Unified Malfunction Controller (UMC),\nincorporating a masking mechanism to enhance damage resilience. Specifically,\nthe model is initially trained with normal environments to ensure robust\nperformance under standard conditions. In the second stage, we use masks to\nprevent the legged robot from relying on malfunctioning limbs, enabling\nadaptive gait and movement adjustments upon malfunction. Experimental results\ndemonstrate that our approach improves the task completion capability by an\naverage of 36% for the transformer and 39% for the MLP across three locomotion\ntasks. The source code and trained models will be made available to the public.",
        "Large language model (LLM) applications are evolving beyond simple chatbots\ninto dynamic, general-purpose agentic programs, which scale LLM calls and\noutput tokens to help AI agents reason, explore, and solve complex tasks.\nHowever, existing LLM serving systems ignore dependencies between programs and\ncalls, missing significant opportunities for optimization. Our analysis reveals\nthat programs submitted to LLM serving engines experience long cumulative wait\ntimes, primarily due to head-of-line blocking at both the individual LLM\nrequest and the program. To address this, we introduce Autellix, an LLM serving\nsystem that treats programs as first-class citizens to minimize their\nend-to-end latencies. Autellix intercepts LLM calls submitted by programs,\nenriching schedulers with program-level context. We propose two scheduling\nalgorithms-for single-threaded and distributed programs-that preempt and\nprioritize LLM calls based on their programs' previously completed calls. Our\nevaluation demonstrates that across diverse LLMs and agentic workloads,\nAutellix improves throughput of programs by 4-15x at the same latency compared\nto state-of-the-art systems, such as vLLM.",
        "Precisely evaluating semantic alignment between text prompts and generated\nvideos remains a challenge in Text-to-Video (T2V) Generation. Existing\ntext-to-video alignment metrics like CLIPScore only generate coarse-grained\nscores without fine-grained alignment details, failing to align with human\npreference. To address this limitation, we propose ETVA, a novel Evaluation\nmethod of Text-to-Video Alignment via fine-grained question generation and\nanswering. First, a multi-agent system parses prompts into semantic scene\ngraphs to generate atomic questions. Then we design a knowledge-augmented\nmulti-stage reasoning framework for question answering, where an auxiliary LLM\nfirst retrieves relevant common-sense knowledge (e.g., physical laws), and then\nvideo LLM answers the generated questions through a multi-stage reasoning\nmechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's\ncorrelation coefficient of 58.47, showing a much higher correlation with human\njudgment than existing metrics which attain only 31.0. We also construct a\ncomprehensive benchmark specifically designed for text-to-video alignment\nevaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10\ncategories. Through a systematic evaluation of 15 existing text-to-video\nmodels, we identify their key capabilities and limitations, paving the way for\nnext-generation T2V generation.",
        "We propose FACTER, a fairness-aware framework for LLM-based recommendation\nsystems that integrates conformal prediction with dynamic prompt engineering.\nBy introducing an adaptive semantic variance threshold and a\nviolation-triggered mechanism, FACTER automatically tightens fairness\nconstraints whenever biased patterns emerge. We further develop an adversarial\nprompt generator that leverages historical violations to reduce repeated\ndemographic biases without retraining the LLM. Empirical results on MovieLens\nand Amazon show that FACTER substantially reduces fairness violations (up to\n95.5%) while maintaining strong recommendation accuracy, revealing semantic\nvariance as a potent proxy of bias.",
        "Despite the success of Gaussian process based Model Predictive Control (MPC)\nin robotic control, its applicability scope is greatly hindered by multimodal\ndisturbances that are prevalent in real-world settings. Here we propose a novel\nMixture of Gaussian Processes based Distributionally Robust MPC (MoGP-DR-MPC)\nframework for linear time invariant systems subject to potentially multimodal\nstate-dependent disturbances. This framework utilizes MoGP to automatically\ndetermine the number of modes from disturbance data. Using the mean and\nvariance information provided by each mode-specific predictive distribution, it\nconstructs a data-driven state-dependent ambiguity set, which allows for\nflexible and fine-grained disturbance modeling. Based on this ambiguity set, we\nimpose Distributionally Robust Conditional Value-at Risk (DR-CVaR) constraints\nto effectively achieve distributional robustness against errors in the\npredictive distributions. To address the computational challenge posed by these\nconstraints in the resulting MPC problem, we equivalently reformulate the\nDR-CVaR constraints into tractable second-order cone constraints. Furthermore,\nwe provide theoretical guarantees on the recursive feasibility and stability of\nthe proposed framework. The enhanced control performance of MoGP-DR-MPC is\nvalidated through both numerical experiments and simulations on a quadrotor\nsystem, demonstrating notable reductions in closed-loop cost by 17% and 4%\nrespectively compared against Gaussian process based MPC.",
        "Cluster magnetic multipoles are order parameters that describe the symmetry\nof spin arrangements in magnetic materials. High-order multipoles are\nparticularly important in non-collinear antiferromagnets, where they determine\nkey physical properties such as the anomalous Hall effect and the\nmagneto-optical Kerr effect. Although non-uniform multipole states have been\nobserved at the micrometer scale, their mesoscopic properties remain largely\nunexplored. In this work, we develop a general micromagnetic formalism for\ncluster magnetic multipoles, enabling the study of the spin dynamics in\nnon-uniform multipole systems. As an example, we apply this formalism to\ninvestigate magnetic-octupole domain-wall dynamics in the non-collinear\nantiferromagnet Mn3Sn. Our results capture key features of domain-wall motion,\nincluding deformation and the emergence of effective inertial mass. This study\nprovides a general framework for studying the dynamics of high-order cluster\nmagnetic multipoles, which is essential for determining the mesoscopic physical\nproperties of non-collinear magnetic materials.",
        "Modern front-end (FE) development, especially when leveraging the unique\nfeatures of frameworks like React and Vue, presents distinctive challenges.\nThese include managing modular architectures, ensuring synchronization between\ndata and visual outputs for declarative rendering, and adapting reusable\ncomponents to various scenarios. Such complexities make it particularly\ndifficult for state-of-the-art large vision-language models (VLMs) to generate\naccurate and functional code directly from design images. To address these\nchallenges, we propose a reflective agentic workflow that synthesizes\nhigh-quality image-text data to capture the diverse characteristics of FE\ndevelopment. This workflow automates the extraction of\nself-contained\\footnote{A \\textbf{self-contained} code snippet is one that\nencapsulates all necessary logic, styling, and dependencies, ensuring it\nfunctions independently without requiring external imports or context.} code\nsnippets from real-world projects, renders the corresponding visual outputs,\nand generates detailed descriptions that link design elements to functional\ncode. To further expand the scope and utility of the synthesis, we introduce\nthree data synthesis strategies: Evolution-based synthesis, which enables\nscalable and diverse dataset expansion; Waterfall-Model-based synthesis, which\ngenerates logically coherent code derived from system requirements; and\nAdditive Development synthesis, which iteratively increases the complexity of\nhuman-authored components. We build a large vision-language model, Flame,\ntrained on the synthesized datasets and demonstrate its effectiveness in\ngenerating React code via the $\\text{pass}@k$ metric. Our results suggest that\na code VLM trained to interpret images before code generation may achieve\nbetter performance.",
        "Building surrogate models with uncertainty quantification capabilities is\nessential for many engineering applications where randomness, such as\nvariability in material properties, is unavoidable. Polynomial Chaos Expansion\n(PCE) is widely recognized as a to-go method for constructing stochastic\nsolutions in both intrusive and non-intrusive ways. Its application becomes\nchallenging, however, with complex or high-dimensional processes, as achieving\naccuracy requires higher-order polynomials, which can increase computational\ndemands and or the risk of overfitting. Furthermore, PCE requires specialized\ntreatments to manage random variables that are not independent, and these\ntreatments may be problem-dependent or may fail with increasing complexity. In\nthis work, we adopt the spectral expansion formalism used in PCE; however, we\nreplace the classical polynomial basis functions with neural network (NN) basis\nfunctions to leverage their expressivity. To achieve this, we propose an\nalgorithm that identifies NN-parameterized basis functions in a purely\ndata-driven manner, without any prior assumptions about the joint distribution\nof the random variables involved, whether independent or dependent. The\nproposed algorithm identifies each NN-parameterized basis function\nsequentially, ensuring they are orthogonal with respect to the data\ndistribution. The basis functions are constructed directly on the joint\nstochastic variables without requiring a tensor product structure. This\napproach may offer greater flexibility for complex stochastic models, while\nsimplifying implementation compared to the tensor product structures typically\nused in PCE to handle random vectors. We demonstrate the effectiveness of the\nproposed scheme through several numerical examples of varying complexity and\nprovide comparisons with classical PCE.",
        "Past millimeter-wave galaxy surveys have probed the brightest starburst\ngalaxies only and suffered heavily from confusion. The interpretation of\nexisting surveys has also been hindered by the lack of reliable redshift\nindicators for measuring distances for the entire sample. Thanks to recent\nadvances in mm-wave detector technologies we can now overcome these\nlimitations, and conduct the first truly volumetric surveys of star-forming\ngalaxies at mm-wavelengths down to the L* luminosities of typical galaxies,\nwith ~1000 redshift slices spanning most of the Cosmic star-forming volume (z ~\n1--12) with nearly uniform mass and luminosity selection. We describe an\ninstrument concept capable of delivering such surveys with the technologies\navailable today, which can be built and operated on a ground-based mm-wave\nfacility in the near future. Such spectrometer cameras can resolve and redshift\nidentify up to to 25,000 star-forming galaxies per year even when operated on a\n10-m class telescope. On a larger aperture it can do the same faster or probe\neven deeper. We propose a loose, open-source collaboration to design, build,\nand operate one or several such cameras through the shared contributions of\nleading experts and telescopes from around the globe.",
        "We estimate the local laws of the distribution of the middle prime factor of\nan integer, defined according to multiplicity or not. An asymptotic estimate\nwith effective remainder is provided for a wide range of values. In particular\nthis enables to precisely describe the phase transition occurring in the\nrelevant distribution.",
        "We consider a generalized Derrida-Retaux model on a Galton-Watson tree with a\ngeometric offspring distribution. For a class of recursive systems, including\nthe Derrida-Retaux model with either a geometric or exponential initial\ndistribution, we characterize the critical curve using an involution-type\nequation and prove that the free energy satisfies the Derrida-Retaux\nconjecture.",
        "Recent advancements in large language models (LLMs) have expanded their role\nin robotic task planning. However, while LLMs have been explored for generating\nfeasible task sequences, their ability to ensure safe task execution remains\nunderdeveloped. Existing methods struggle with structured risk perception,\nmaking them inadequate for safety-critical applications where low-latency\nhazard adaptation is required. To address this limitation, we propose a\nGraphormer-enhanced risk-aware task planning framework that combines LLM-based\ndecision-making with structured safety modeling. Our approach constructs a\ndynamic spatio-semantic safety graph, capturing spatial and contextual risk\nfactors to enable online hazard detection and adaptive task refinement. Unlike\nexisting methods that rely on predefined safety constraints, our framework\nintroduces a context-aware risk perception module that continuously refines\nsafety predictions based on real-time task execution. This enables a more\nflexible and scalable approach to robotic planning, allowing for adaptive\nsafety compliance beyond static rules. To validate our framework, we conduct\nexperiments in the AI2-THOR environment. The experiments results validates\nimprovements in risk detection accuracy, rising safety notice, and task\nadaptability of our framework in continuous environments compared to static\nrule-based and LLM-only baselines. Our project is available at\nhttps:\/\/github.com\/hwj20\/GGTP"
      ]
    }
  },
  {
    "id":2411.02614,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Medical diffusion on a budget: textual inversion for medical image generation",
    "start_abstract":"Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy",
    "start_categories":[
      "q-bio.OT"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey"
      ],
      "abstract":[
        "Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Synthesis of omnidirectional path loss model based on directional model\n  and multi-elliptical geometry",
        "Asymmetric Orbifolds, Rank Reduction and Heterotic Islands",
        "Emergence of Order in Chemically Active Droplets: Temporal Dynamics and\n  Collective Behavior",
        "EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for\n  Visual Spatial Tasks",
        "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System",
        "URL Inspection Tasks: Helping Users Detect Phishing Links in Emails",
        "In Pursuit of Predictive Models of Human Preferences Toward AI Teammates",
        "Topologically protected synchronization in networks",
        "Halfspace Representations of Path Polytopes of Trees",
        "Will AI replace Software Engineers? Do not hold your breath",
        "Effective enhancement of the electron-phonon coupling driven by\n  nonperturbative electronic density fluctuations",
        "PCAP-Backdoor: Backdoor Poisoning Generator for Network Traffic in\n  CPS\/IoT Environments",
        "M-LLM Based Video Frame Selection for Efficient Video Understanding",
        "Which2comm: An Efficient Collaborative Perception Framework for 3D\n  Object Detection",
        "Variational Combinatorial Sequential Monte Carlo for Bayesian\n  Phylogenetics in Hyperbolic Space",
        "Conformal defects and RG flows in ABJM",
        "Output-Feedback Full-State Targeting Model Predictive Control for\n  Station-Keeping on Near-Rectilinear Halo Orbits",
        "Digital Phenotyping for Adolescent Mental Health: A Feasibility Study\n  Employing Machine Learning to Predict Mental Health Risk From Active and\n  Passive Smartphone Data",
        "Panopticon: Advancing Any-Sensor Foundation Models for Earth Observation",
        "Unsupervised Domain Adaptation with Dynamic Clustering and Contrastive\n  Refinement for Gait Recognition",
        "StrNim: a variant of Nim played on strings",
        "Abstract Rendering: Computing All that is Seen in Gaussian Splat Scenes",
        "GHOST 2.0: generative high-fidelity one shot transfer of heads",
        "Predicting the cryogenic performance of superconducting detectors by\n  their visual properties",
        "Dispersive and Strichartz estimates for Dirac equation in a cosmic\n  string spacetime",
        "DFT+DMFT study on pressure-induced valence instability of CeCoSi",
        "Quantum computation via Floquet-tailored Rydberg interactions",
        "Computational Discovery of Chiasmus in Ancient Religious Text",
        "Temporal Logic Guided Safe Navigation for Autonomous Vehicles"
      ],
      "abstract":[
        "Millimeter wave (mmWave) technology offers high throughput but has a limited\nradio range, necessitating the use of directional antennas or beamforming\nsystems such as massive MIMO. Path loss (PL) models using narrow-beam antennas\nare known as directional models, while those using omnidirectional antennas are\nreferred to as omnidirectional models. To standardize the analysis,\nomnidirectional PL models for mmWave ranges have been introduced, including TR\n38.901 by 3GPP, which is based on measurements from directional antennas.\nHowever, synthesizing these measurements can be complex and time-consuming.\nThis study proposes a numerical approach to derive an omnidirectional model\nfrom directional data using multi-elliptical geometry. We assessed the\neffectiveness of this method against existing PL models for mmWaves that are\navailable in the literature.",
        "We consider toroidal asymmetric orbifolds of the heterotic string preserving\nall 16 supercharges, developing a general formalism to study components of the\nmoduli space characterized by rank reduction of the gauge group. In particular\nwe construct six- and four-dimensional heterotic islands with no massless\nmoduli other than the dilaton. The formalism involves the Leech lattice, its\nautomorphisms and their corresponding invariant and normal, or coinvariant,\nsublattices.",
        "Collective behaviors such as swarming, chemical signaling, and clustering are\nfundamental to biological microorganisms, enabling hierarchical colony\nformation, coordinated motion, and enhanced nutrient accessibility crucial for\ntheir survival. Over the past few decades, extensive research has been\ndedicated to unraveling the mechanisms underlying these diverse collective\npatterns through experimental model systems. Among these, active droplets have\nemerged as valuable synthetic analogs, effectively replicating key biological\nattributes and serving as ideal platforms for investigating collective\nphenomena. This research explores the collective behavior of\n4-Cyano-4-pentyl-biphenyl (5CB) oil droplets across varying P\\'eclet ($Pe$)\nnumbers. At high $Pe$, droplets exhibit a pusher mode of propulsion and form\ndynamic chain-like patterns. Decreasing $Pe$ enhances repulsive interactions\namong droplets, resulting in the inhibition of clustering. In the low $Pe$\nregime, their repulsive interactions predominated by chemical field lead to the\nemergence of an ordered structure. Furthermore, we illustrate how active\ndroplets efficiently navigate within a soft structured environment. These\nfindings contribute to our comprehension of self-organized phenomena in active\nmatter systems and provide insights for designing strategies for controlled\nlocomotion in intricate fluidic environments.",
        "While multimodal large language models (MLLMs) have made groundbreaking\nprogress in embodied intelligence, they still face significant challenges in\nspatial reasoning for complex long-horizon tasks. To address this gap, we\npropose EmbodiedVSR (Embodied Visual Spatial Reasoning), a novel framework that\nintegrates dynamic scene graph-guided Chain-of-Thought (CoT) reasoning to\nenhance spatial understanding for embodied agents. By explicitly constructing\nstructured knowledge representations through dynamic scene graphs, our method\nenables zero-shot spatial reasoning without task-specific fine-tuning. This\napproach not only disentangles intricate spatial relationships but also aligns\nreasoning steps with actionable environmental dynamics. To rigorously evaluate\nperformance, we introduce the eSpatial-Benchmark, a comprehensive dataset\nincluding real-world embodied scenarios with fine-grained spatial annotations\nand adaptive task difficulty levels. Experiments demonstrate that our framework\nsignificantly outperforms existing MLLM-based methods in accuracy and reasoning\ncoherence, particularly in long-horizon tasks requiring iterative environment\ninteraction. The results reveal the untapped potential of MLLMs for embodied\nintelligence when equipped with structured, explainable reasoning mechanisms,\npaving the way for more reliable deployment in real-world spatial applications.\nThe codes and datasets will be released soon.",
        "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline. This paper initially introduces a dual-metric\nevaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable\nthe direct quantification of chunking quality. Leveraging this assessment\nmethod, we highlight the inherent limitations of traditional and semantic\nchunking in handling complex contextual nuances, thereby substantiating the\nnecessity of integrating LLMs into chunking process. To address the inherent\ntrade-off between computational efficiency and chunking precision in LLM-based\napproaches, we devise the granularity-aware Mixture-of-Chunkers (MoC)\nframework, which consists of a three-stage processing mechanism. Notably, our\nobjective is to guide the chunker towards generating a structured list of\nchunking regular expressions, which are subsequently employed to extract chunks\nfrom the original text. Extensive experiments demonstrate that both our\nproposed metrics and the MoC framework effectively settle challenges of the\nchunking task, revealing the chunking kernel while enhancing the performance of\nthe RAG system.",
        "The most widespread type of phishing attack involves email messages with\nlinks pointing to malicious content. Despite user training and the use of\ndetection techniques, these attacks are still highly effective. Recent studies\nshow that it is user inattentiveness, rather than lack of education, that is\none of the key factors in successful phishing attacks. To this end, we develop\na novel phishing defense mechanism based on URL inspection tasks: small\nchallenges (loosely inspired by CAPTCHAs) that, to be solved, require users to\ninteract with, and understand, the basic URL structure. We implemented and\nevaluated three tasks that act as ``barriers'' to visiting the website: (1)\ncorrect click-selection from a list of URLs, (2) mouse-based highlighting of\nthe domain-name URL component, and (3) re-typing the domain-name. These tasks\nfollow best practices in security interfaces and warning design.\n  We assessed the efficacy of these tasks through an extensive on-line user\nstudy with 2,673 participants from three different cultures, native languages,\nand alphabets. Results show that these tasks significantly decrease the rate of\nsuccessful phishing attempts, compared to the baseline case. Results also\nshowed the highest efficacy for difficult URLs, such as typo-squats, with which\nparticipants struggled the most. This highlights the importance of (1) slowing\ndown users while focusing their attention and (2) helping them understand the\nURL structure (especially, the domain-name component thereof) and matching it\nto their intent.",
        "We seek measurable properties of AI agents that make them better or worse\nteammates from the subjective perspective of human collaborators. Our\nexperiments use the cooperative card game Hanabi -- a common benchmark for\nAI-teaming research. We first evaluate AI agents on a set of objective metrics\nbased on task performance, information theory, and game theory, which are\nmeasurable without human interaction. Next, we evaluate subjective human\npreferences toward AI teammates in a large-scale (N=241) human-AI teaming\nexperiment. Finally, we correlate the AI-only objective metrics with the human\nsubjective preferences. Our results refute common assumptions from prior\nliterature on reinforcement learning, revealing new correlations between AI\nbehaviors and human preferences. We find that the final game score a human-AI\nteam achieves is less predictive of human preferences than esoteric measures of\nAI action diversity, strategic dominance, and ability to team with other AI. In\nthe future, these correlations may help shape reward functions for training\nhuman-collaborative AI.",
        "In a graph, we say that two nodes are topologically equivalent if their sets\nof first neighbors, excluding the two nodes, coincide. We prove that\nnonlinearly coupled oscillators located on a group of topologically equivalent\nnodes can get easily synchronized when the group forms a fully connected\nsubgraph (or combinations of these), regardless of the status of all the other\noscillators. More generally, any change occurring in the inner part of the\nremainder of the graph will not alter the synchronization status of the group.\nTypically, the group can synchronize when $k^{(\\mathrm{OUT})}\\leq\nk^{(\\mathrm{IN})}$, $k^{(\\mathrm{IN})}$ and $k^{(\\mathrm{OUT})}$ being the\ncommon internal and outgoing degree of each node in the group, respectively.\nSimulations confirm our analysis and suggest that groups of topologically\nequivalent nodes play the role of independent pacemakers.",
        "Given a tree $T$, its path polytope is the convex hull of the edge indicator\nvectors for the paths between any two distinct leaves in $T$. These polytopes\narise naturally in polyhedral geometry and applications, such as phylogenetics,\ntropical geometry, and algebraic statistics. We provide a minimal halfspace\nrepresentation of these polytopes. The construction is made inductively using\ntoric fiber products.",
        "Artificial Intelligence (AI) technology such as Large Language Models (LLMs)\nhave become extremely popular in creating code. This has led to the conjecture\nthat future software jobs will be exclusively conducted by LLMs, and the\nsoftware industry will cease to exist. But software engineering is much more\nthan producing code -- notably, \\emph{maintaining} large software and keeping\nit reliable is a major part of software engineering, which LLMs are not yet\ncapable of.",
        "We present a dynamical mean-field study of the nonperturbative electronic\nmechanisms, which may lead to significant enhancements of the electron-phonon\ncoupling in correlated electron systems. Analyzing the effects of electronic\ncorrelations on the lowest-order electron-phonon processes, we show that in the\nproximity of the Mott metal-to-insulator transition of the doped square lattice\nHubbard model, where the isothermal charge response becomes particularly large\nat small momenta, the coupling of electrons to the lattice is strongly\nincreased. This, in turn, induces significant corrections to both the\nelectronic self-energy and phonon-mediated pairing interaction, indicating the\npossible onset of a strong interplay between lattice and electronic degrees of\nfreedom even for small values of the bare electron-phonon coupling.",
        "The rapid expansion of connected devices has made them prime targets for\ncyberattacks. To address these threats, deep learning-based, data-driven\nintrusion detection systems (IDS) have emerged as powerful tools for detecting\nand mitigating such attacks. These IDSs analyze network traffic to identify\nunusual patterns and anomalies that may indicate potential security breaches.\nHowever, prior research has shown that deep learning models are vulnerable to\nbackdoor attacks, where attackers inject triggers into the model to manipulate\nits behavior and cause misclassifications of network traffic. In this paper, we\nexplore the susceptibility of deep learning-based IDS systems to backdoor\nattacks in the context of network traffic analysis. We introduce\n\\texttt{PCAP-Backdoor}, a novel technique that facilitates backdoor poisoning\nattacks on PCAP datasets. Our experiments on real-world Cyber-Physical Systems\n(CPS) and Internet of Things (IoT) network traffic datasets demonstrate that\nattackers can effectively backdoor a model by poisoning as little as 1\\% or\nless of the entire training dataset. Moreover, we show that an attacker can\nintroduce a trigger into benign traffic during model training yet cause the\nbackdoored model to misclassify malicious traffic when the trigger is present.\nFinally, we highlight the difficulty of detecting this trigger-based backdoor,\neven when using existing backdoor defense techniques.",
        "Recent advances in Multi-Modal Large Language Models (M-LLMs) show promising\nresults in video reasoning. Popular Multi-Modal Large Language Model (M-LLM)\nframeworks usually apply naive uniform sampling to reduce the number of video\nframes that are fed into an M-LLM, particularly for long context videos.\nHowever, it could lose crucial context in certain periods of a video, so that\nthe downstream M-LLM may not have sufficient visual information to answer a\nquestion. To attack this pain point, we propose a light-weight M-LLM -based\nframe selection method that adaptively select frames that are more relevant to\nusers' queries. In order to train the proposed frame selector, we introduce two\nsupervision signals (i) Spatial signal, where single frame importance score by\nprompting a M-LLM; (ii) Temporal signal, in which multiple frames selection by\nprompting Large Language Model (LLM) using the captions of all frame\ncandidates. The selected frames are then digested by a frozen downstream video\nM-LLM for visual reasoning and question answering. Empirical results show that\nthe proposed M-LLM video frame selector improves the performances various\ndownstream video Large Language Model (video-LLM) across medium (ActivityNet,\nNExT-QA) and long (EgoSchema, LongVideoBench) context video question answering\nbenchmarks.",
        "Collaborative perception allows real-time inter-agent information exchange\nand thus offers invaluable opportunities to enhance the perception capabilities\nof individual agents. However, limited communication bandwidth in practical\nscenarios restricts the inter-agent data transmission volume, consequently\nresulting in performance declines in collaborative perception systems. This\nimplies a trade-off between perception performance and communication cost. To\naddress this issue, we propose Which2comm, a novel multi-agent 3D object\ndetection framework leveraging object-level sparse features. By integrating\nsemantic information of objects into 3D object detection boxes, we introduce\nsemantic detection boxes (SemDBs). Innovatively transmitting these\ninformation-rich object-level sparse features among agents not only\nsignificantly reduces the demanding communication volume, but also improves 3D\nobject detection performance. Specifically, a fully sparse network is\nconstructed to extract SemDBs from individual agents; a temporal fusion\napproach with a relative temporal encoding mechanism is utilized to obtain the\ncomprehensive spatiotemporal features. Extensive experiments on the V2XSet and\nOPV2V datasets demonstrate that Which2comm consistently outperforms other\nstate-of-the-art methods on both perception performance and communication cost,\nexhibiting better robustness to real-world latency. These results present that\nfor multi-agent collaborative 3D object detection, transmitting only\nobject-level sparse features is sufficient to achieve high-precision and robust\nperformance.",
        "Hyperbolic space naturally encodes hierarchical structures such as\nphylogenies (binary trees), where inward-bending geodesics reflect paths\nthrough least common ancestors, and the exponential growth of neighborhoods\nmirrors the super-exponential scaling of topologies. This scaling challenge\nlimits the efficiency of Euclidean-based approximate inference methods.\nMotivated by the geometric connections between trees and hyperbolic space, we\ndevelop novel hyperbolic extensions of two sequential search algorithms:\nCombinatorial and Nested Combinatorial Sequential Monte Carlo (\\textsc{Csmc}\nand \\textsc{Ncsmc}). Our approach introduces consistent and unbiased\nestimators, along with variational inference methods (\\textsc{H-Vcsmc} and\n\\textsc{H-Vncsmc}), which outperform their Euclidean counterparts. Empirical\nresults demonstrate improved speed, scalability and performance in\nhigh-dimensional phylogenetic inference tasks.",
        "Defects play a central role in many contexts, from condensed matter to\nquantum gravity. The situations in which the bulk theory is conformal and the\ndefect inherits part of this symmetry -- the so-called defect conformal field\ntheories (dCFTs) -- have recently received a lot of attention, also thanks to\nnew powerful methods to tackle them, like supersymmetric localization,\nintegrability or the bootstrap. A dCFT may be deformed by turning on marginally\nrelevant operators, which trigger RG flows connecting different fixed points. A\nnatural arena where this phenomenon can be explored are 3-dimensional\nChern-Simons theories coupled to matter. These are in fact known to display a\nplethora of Wilson loops that can be used to define 1-dimensional dCFTs living\non their contours. Here we discuss a few examples from an intricate web of RG\nflows connecting the dCFTs defined on the BPS and non-BPS Wilson loops of ABJM\ntheory. We compute the anomalous dimensions of the deforming operators,\nestablish g-theorems along the flows, and also discuss the role played by\ncohomological anomalies and framing.",
        "We develop a model predictive control (MPC) policy for station-keeping (SK)\non a Near-Rectilinear Halo Orbit (NRHO). The proposed policy achieves\nfull-state tracking of a reference NRHO via a two-maneuver control horizon\nplaced one revolution apart. Our method abides by the typical mission\nrequirement that at most one maneuver is used for SK during each NRHO\nrevolution. Simultaneously, the policy has sufficient controllability for\nfull-state tracking, making it immune to phase deviation issues in the\nalong-track direction of the reference NRHO, a common drawback of existing SK\nmethods with a single maneuver per revolution. We report numerical simulations\nwith a navigation filter to demonstrate the MPC's performance with output\nfeedback. Our approach successfully maintains the spacecraft's motion in the\nvicinity of the reference in both space and phase, with tighter tracking than\nstate-of-the-art SK methods and comparable delta-V performance.",
        "Background: Adolescents are particularly vulnerable to mental disorders, with\nover 75% of cases manifesting before the age of 25. Research indicates that\nonly 18 to 34% of young people experiencing high levels of depression or\nanxiety symptoms seek support. Digital tools leveraging smartphones offer\nscalable and early intervention opportunities. Objective: Using a novel machine\nlearning framework, this study evaluated the feasibility of integrating active\nand passive smartphone data to predict mental disorders in non-clinical\nadolescents. Specifically, we investigated the utility of the Mindcraft app in\npredicting risks for internalising and externalising disorders, eating\ndisorders, insomnia and suicidal ideation. Methods: Participants (N=103; mean\nage 16.1 years) were recruited from three London schools. Participants\ncompleted the Strengths and Difficulties Questionnaire, the Eating Disorders-15\nQuestionnaire, Sleep Condition Indicator Questionnaire and indicated the\npresence\/absence of suicidal ideation. They used the Mindcraft app for 14 days,\ncontributing active data via self-reports and passive data from smartphone\nsensors. A contrastive pretraining phase was applied to enhance user-specific\nfeature stability, followed by supervised fine-tuning. The model evaluation\nemployed leave-one-subject-out cross-validation using balanced accuracy as the\nprimary metric. Results: The integration of active and passive data achieved\nsuperior performance compared to individual data sources, with mean balanced\naccuracies of 0.71 for SDQ-High risk, 0.67 for insomnia, 0.77 for suicidal\nideation and 0.70 for eating disorders. The contrastive learning framework\nstabilised daily behavioural representations, enhancing predictive robustness.\nThis study demonstrates the potential of integrating active and passive\nsmartphone data with advanced machine-learning techniques for predicting mental\nhealth risks.",
        "Earth observation (EO) data features diverse sensing platforms with varying\nspectral bands, spatial resolutions, and sensing modalities. While most prior\nwork has constrained inputs to fixed sensors, a new class of any-sensor\nfoundation models able to process arbitrary sensors has recently emerged.\nContributing to this line of work, we propose Panopticon, an any-sensor\nfoundation model built on the DINOv2 framework. We extend DINOv2 by (1)\ntreating images of the same geolocation across sensors as natural\naugmentations, (2) subsampling channels to diversify spectral input, and (3)\nadding a cross attention over channels as a flexible patch embedding mechanism.\nBy encoding the wavelength and modes of optical and synthetic aperture radar\nsensors, respectively, Panopticon can effectively process any combination of\narbitrary channels. In extensive evaluations, we achieve state-of-the-art\nperformance on GEO-Bench, especially on the widely-used Sentinel-1 and\nSentinel-2 sensors, while out-competing other any-sensor models, as well as\ndomain adapted fixed-sensor models on unique sensor configurations. Panopticon\nenables immediate generalization to both existing and future satellite\nplatforms, advancing sensor-agnostic EO.",
        "Gait recognition is an emerging identification technology that distinguishes\nindividuals at long distances by analyzing individual walking patterns.\nTraditional techniques rely heavily on large-scale labeled datasets, which\nincurs high costs and significant labeling challenges. Recently, researchers\nhave explored unsupervised gait recognition with clustering-based unsupervised\ndomain adaptation methods and achieved notable success. However, these methods\ndirectly use pseudo-label generated by clustering and neglect pseudolabel noise\ncaused by domain differences, which affects the effect of the model training\nprocess. To mitigate these issues, we proposed a novel model called GaitDCCR,\nwhich aims to reduce the influence of noisy pseudo labels on clustering and\nmodel training. Our approach can be divided into two main stages: clustering\nand training stage. In the clustering stage, we propose Dynamic Cluster\nParameters (DCP) and Dynamic Weight Centroids (DWC) to improve the efficiency\nof clustering and obtain reliable cluster centroids. In the training stage, we\nemploy the classical teacher-student structure and propose Confidence-based\nPseudo-label Refinement (CPR) and Contrastive Teacher Module (CTM) to encourage\nnoisy samples to converge towards clusters containing their true identities.\nExtensive experiments on public gait datasets have demonstrated that our simple\nand effective method significantly enhances the performance of unsupervised\ngait recognition, laying the foundation for its application in the\nreal-world.The code is available at https:\/\/github.com\/YanSun-github\/GaitDCCR",
        "We propose a variant of Nim, named StrNim. Whereas a position in Nim is a\ntuple of non-negative integers, that in StrNim is a string, a sequence of\ncharacters. In every turn, each player shrinks the string, by removing a\nsubstring repeating the same character. As a first study on this new game, we\npresent some sufficient conditions for the positions to be P-positions.",
        "We introduce abstract rendering, a method for computing a set of images by\nrendering a scene from a continuously varying range of camera positions. The\nresulting abstract image-which encodes an infinite collection of possible\nrenderings-is represented using constraints on the image matrix, enabling\nrigorous uncertainty propagation through the rendering process. This capability\nis particularly valuable for the formal verification of vision-based autonomous\nsystems and other safety-critical applications. Our approach operates on\nGaussian splat scenes, an emerging representation in computer vision and\nrobotics. We leverage efficient piecewise linear bound propagation to abstract\nfundamental rendering operations, while addressing key challenges that arise in\nmatrix inversion and depth sorting-two operations not directly amenable to\nstandard approximations. To handle these, we develop novel linear relational\nabstractions that maintain precision while ensuring computational efficiency.\nThese abstractions not only power our abstract rendering algorithm but also\nprovide broadly applicable tools for other rendering problems. Our\nimplementation, AbstractSplat, is optimized for scalability, handling up to\n750k Gaussians while allowing users to balance memory and runtime through tile\nand batch-based computation. Compared to the only existing abstract image\nmethod for mesh-based scenes, AbstractSplat achieves 2-14x speedups while\npreserving precision. Our results demonstrate that continuous camera motion,\nrotations, and scene variations can be rigorously analyzed at scale, making\nabstract rendering a powerful tool for uncertainty-aware vision applications.",
        "While the task of face swapping has recently gained attention in the research\ncommunity, a related problem of head swapping remains largely unexplored. In\naddition to skin color transfer, head swap poses extra challenges, such as the\nneed to preserve structural information of the whole head during synthesis and\ninpaint gaps between swapped head and background. In this paper, we address\nthese concerns with GHOST 2.0, which consists of two problem-specific modules.\nFirst, we introduce enhanced Aligner model for head reenactment, which\npreserves identity information at multiple scales and is robust to extreme pose\nvariations. Secondly, we use a Blender module that seamlessly integrates the\nreenacted head into the target background by transferring skin color and\ninpainting mismatched regions. Both modules outperform the baselines on the\ncorresponding tasks, allowing to achieve state of the art results in head\nswapping. We also tackle complex cases, such as large difference in hair styles\nof source and target. Code is available at\nhttps:\/\/github.com\/ai-forever\/ghost-2.0",
        "The testing and quality assurance of cryogenic superconducting detectors is a\ntime- and labor-intensive process. As experiments deploy increasingly larger\narrays of detectors, new methods are needed for performing this testing\nquickly. Here, we propose a process for flagging under-performing detector\nwafers before they are ever tested cryogenically. Detectors are imaged under an\noptical microscope, and computer vision techniques are used to analyze the\nimages, searching for visual defects and other predictors of poor performance.\nPipeline performance is verified via a suite of images with simulated defects,\nyielding a detection accuracy of 98.6%. Lastly, results from running the\npipeline on prototype microwave kinetic inductance detectors from the planned\nSPT-3G+ experiment are presented.",
        "In this work we study the Dirac equation on the cosmic string background,\nwhich models a one--dimensional topological defect in the spacetime. We first\ndefine the Dirac operator in this setting, classifying all of its selfadjoint\nextensions, and we give an explicit kernel for the propagator. Secondly, we\nprove dispersive estimates for the flow, with and without weights. Finally, we\nprove Strichartz estimates for the flow in a sharp restricted set of indices,\nwhich are different from the classical Euclidean ones.",
        "Rare-earth compounds RCoSi exhibit unique properties, with distinct\nstructural behaviors depending on whether R is a light, middle or heavy\nrare-earth element. Among them, CeCoSi undergoes a structural phase transition\nunder high pressure, with the phase transition pressure increasing as\ntemperature rises. Some experimental studies suggest that the transition is\nclosely related to the behavior of Ce-4f electrons. In this work, we\nsystematically studied the evolution of the electronic structure of CeCoSi with\ntemperature and pressure. First, we used the DFT+DMFT to calculate the\nenergy-volume curve of CeCoSi, which was in good agreement with the\nexperimental results and far superior to the DFT method. Next, we studied the\nelectronic structure of CeCoSi under different pressures and temperatures using\nDFT+DMFT. Our results show that CeCoSi is a Kondo metal with hybridization of\nCe-4f and Co-3d. As pressure increases, the renormalization factor Z of\nCe-4f5\/2 increases, the occupancy number of Ce-4f electrons decreases, and\nCeCoSi transitions to a mixed-valence state at ~5.5 GPa in 100 K. The pressure\nof the quantum phase transition PQ is slightly higher than the experimentally\nobserved structural phase transition pressure PS, and the PQ increases with\nincreasing temperature, which is consistent with the behavior of PS in\nexperiment. In addition, the hybridization strength of Ce-4f in the\nmixed-valence state is significantly greater than in the Kondo metal state. Our\nresults suggest that the valence instability of Ce-4f is the cause of the\nstructural phase transition. As pressure increases, Ce-4f electrons delocalize\nand CeCoSi transitions to mixed-valence state. This valence instability may\ncause redistribution of electron density, thus inducing a structural phase\ntransition. Our work reveals the cause of the structural phase transition of\nCeCoSi under high pressure.",
        "Rydberg atoms stand out as a highly promising platform for realizing quantum\ncomputation with significant advantages in constructing high-fidelity quantum\ngates. Floquet frequency modulation (FFM), in Rydberg-atom systems, provides a\nunique platform for achieving precise quantum control and uncovering exotic\nphysical phenomena, paving the way for innovative methodologies in quantum\ndynamics research. This work introduces a method to realize controlled\narbitrary phase gates in Rydberg atoms by manipulating system dynamics using\nFFM. Notably, this method eliminates the need for laser addressing of\nindividual atoms, significantly enhancing convenience for future practical\napplications. Furthermore, this approach can be integrated with soft quantum\ncontrol strategies to enhance the fidelity and robustness of the resultant\ncontrolled-phase gates. Finally, as an example, this methodology is applied in\nGrover-Long algorithm to search target items with zero failure rate,\ndemonstrating its substantial significance for future quantum information\nprocessing applications. This work leveraging Rydberg atoms and Floquet\nfrequency modulation may herald a new era of scalable and reliable quantum\ncomputing.",
        "Chiasmus, a debated literary device in Biblical texts, has captivated mystics\nwhile sparking ongoing scholarly discussion. In this paper, we introduce the\nfirst computational approach to systematically detect chiasmus within Biblical\npassages. Our method leverages neural embeddings to capture lexical and\nsemantic patterns associated with chiasmus, applied at multiple levels of\ntextual granularity (half-verses, verses). We also involve expert annotators to\nreview a subset of the detected patterns. Despite its computational efficiency,\nour method achieves robust results, with high inter-annotator agreement and\nsystem precision@k of 0.80 at the verse level and 0.60 at the half-verse level.\nWe further provide a qualitative analysis of the distribution of detected\nchiasmi, along with selected examples that highlight the effectiveness of our\napproach.",
        "Safety verification for autonomous vehicles (AVs) and ground robots is\ncrucial for ensuring reliable operation given their uncertain environments.\nFormal language tools provide a robust and sound method to verify safety rules\nfor such complex cyber-physical systems. In this paper, we propose a hybrid\napproach that combines the strengths of formal verification languages like\nLinear Temporal Logic (LTL) and Signal Temporal Logic (STL) to generate safe\ntrajectories and optimal control inputs for autonomous vehicle navigation. We\nimplement a symbolic path planning approach using LTL to generate a formally\nsafe reference trajectory. A mixed integer linear programming (MILP) solver is\nthen used on this reference trajectory to solve for the control inputs while\nsatisfying the state, control and safety constraints described by STL. We test\nour proposed solution on two environments and compare the results with popular\npath planning algorithms. In contrast to conventional path planning algorithms,\nour formally safe solution excels in handling complex specification scenarios\nwhile ensuring both safety and comparable computation times."
      ]
    }
  },
  {
    "id":2411.02614,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey",
    "start_abstract":"Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Medical diffusion on a budget: textual inversion for medical image generation"
      ],
      "abstract":[
        "Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy"
      ],
      "categories":[
        "q-bio.OT"
      ]
    },
    "list":{
      "title":[
        "Bayesian optimisation of poloidal field coil positions in tokamaks",
        "Quantum metric non-linear Hall effect in an antiferromagnetic\n  topological insulator thin-film EuSn2As2",
        "Monotone conservative strategies in data assimilation",
        "Swimming mode determines how well mesoscale swimmers shield their odor\n  in turbulence",
        "On the Picard numbers of moduli spaces of one-dimensional sheaves on\n  surfaces",
        "Regular evolution algebras are closed under subalgebras",
        "Generalization error bound for denoising score matching under relaxed\n  manifold assumption",
        "The unification of gravity and the spin-1 field",
        "Void spin distribution as a powerful probe of $\\sigma_{8}$",
        "Decay of mass for a semilinear heat equation with mixed local-nonlocal\n  operators",
        "PUATE: Semiparametric Efficient Average Treatment Effect Estimation from\n  Treated (Positive) and Unlabeled Units",
        "The TES-based Cryogenic AntiCoincidence Detector of ATHENA X-IFU:\n  Validation of the thermal end-to-end simulator towards the updated\n  Demonstration Model (DM 1.1)",
        "OGBoost: A Python Package for Ordinal Gradient Boosting",
        "Regulation of Algorithmic Collusion, Refined: Testing Pessimistic\n  Calibrated Regret",
        "Spectral synthesis for exponentials in weighted $L^2$-spaces",
        "Bridging the Data Gap in AI Reliability Research and Establishing\n  DR-AIR, a Comprehensive Data Repository for AI Reliability",
        "Towards an AI co-scientist",
        "Fiducial Inference for Random-Effects Calibration Models: Advancing\n  Reliable Quantification in Environmental Analytical Chemistry",
        "Wavefront shaping enhanced nano-optomechanics down to the quantum\n  precision limit",
        "On uniqueness of functions in the extended Selberg class with moving\n  targets",
        "On refactorization problems and rational Lax matrices of quadrirational\n  Yang-Baxter maps",
        "Effects of valley splitting on resonant-tunneling readout of spin qubits",
        "Proposal of the KOTO II experiment",
        "A unified approach to hypergeometric class functions",
        "Hardy-Hilbert type inequalities on homogeneous groups-An introduction\n  and generalization to the kernel case",
        "Exploring the mechanism of phase transitions between the hexagonal\n  close-packed and the cuboidal structures",
        "$K$-theoretic pullbacks for Lagrangians on derived critical loci",
        "$^{56}$Ni production in long-lived binary neutron star merger remnants",
        "The EXO-UV program: lastest advances of experimental studies to\n  investigate the biological impact of UV radiation on exoplanets"
      ],
      "abstract":[
        "The tokamak is a world-leading concept for producing sustainable energy via\nmagnetically-confined nuclear fusion. Identifying where to position the magnets\nwithin a tokamak, specifically the poloidal field (PF) coils, is a design\nproblem which requires balancing a number of competing economic, physical, and\nengineering objectives and constraints. In this paper, we show that\nmulti-objective Bayesian optimisation (BO), an iterative optimisation technique\nutilising probabilistic machine learning models, can effectively explore this\ncomplex design space and return several optimal PF coil sets. These solutions\nspan the Pareto front, a subset of the objective space that optimally satisfies\nthe specified objective functions. We outline an easy-to-use BO framework and\ndemonstrate that it outperforms alternative optimisation techniques while using\nsignificantly fewer computational resources. Our results show that BO is a\npromising technique for fusion design problems that rely on computationally\ndemanding high-fidelity simulations.",
        "The quantum geometric structure of electrons introduces fundamental insights\ninto understanding quantum effects in materials. One notable manifestation is\nthe non-linear Hall effect (NLHE), which has drawn considerable interest for\nits potential to overcome the intrinsic limitations of semiconductor diodes at\nlow input power and high frequency. In this study, we investigate NLHE stemming\nfrom the real part of the quantum geometric tensor, specifically the quantum\nmetric, in an antiferromagnetic topological material, EuSn2As2, using density\nfunctional theory. Our calculations predict a remarkable NLHE arising from a\nsymmetry-protected, single Type-II surface Dirac cone in the\neven-numbered-layer two-dimensional slab thin-film, yielding a non-linear Hall\nconductivity exceeding 20 mA\/V2-an order of magnitude larger than previously\nreported. This single Dirac band dispersion represents the simplest model for\ngenerating NLHE, positioning the EuSn2As2 thin-film as a hydrogen atom for NLHE\nsystems. Additionally, we observe NLHE from band-edge states near the Fermi\nlevel. Our findings also reveal that 30% phosphorus (P) doping can double the\nnon-linear Hall conductivity. With its substantial and tunable NLHE, EuSn2As2\nthin-films present promising applications in antiferromagnetic spintronics and\nrectification devices.",
        "This paper studies whether numerically preserving monotonic properties can\noffer modelling advantages in data assimilation, particularly when the signal\nor data is a realization of a stochastic partial differential equation (SPDE)\nor partial differential equation (PDE) with a monotonic property. We\ninvestigate the combination of stochastic Strong Stability Preserving (SSP)\ntime-stepping, nonlinear solving strategies and data assimilation. Experimental\nresults indicate that a particle filter whose ensemble members are solved\nmonotonically can increase forecast skill when the reference data (not\nnecessarily observations) also has a monotone property. Additionally, more\nadvanced techniques used to avoid the degeneracy of the filter\n(tempering-jittering) are shown to be compatible with a conservative monotone\napproach.",
        "Marine organisms manipulate their surrounding flow through their swimming\ndynamics, which affects the transport of their own odor cues. We demonstrate by\ndirect numerical simulations how a group of mesoscale swimmers immersed in a\nturbulent flow alters the shape of the odor plume they release in the water.\nOdor mixing is enhanced by increased velocity fluctuations and a\nswimmer-induced flow circulation which widen the odor plume at close range\nwhile speeding up dilution of the chemical trace. Beyond a short-range increase\nin the likelihood of being detected, swimming considerably reduces detections\nwith effects that can persist at distances of the order of ten times the size\nof the group or more. We find that puller-like swimmers are more effective at\nolfactory shielding than pusher-like swimmers. We trace this difference back to\nthe dynamics at the swimmer location, which tends to trap odor at the source\nfor pushers and to dilute it for pullers. Olfactory shielding is robust to\nchanges in the conditions, and is more pronounced for weak turbulent Reynolds\nnumbers and large swimmer Reynolds numbers. Our results suggest that olfactory\nshielding may play a role in the emergence of different swimming modalities by\nmarine organisms.",
        "Motivated by asymptotic phenomena of moduli spaces of higher rank stable\nsheaves on algebraic surfaces, we study the Picard number of the moduli space\nof one-dimensional stable sheaves supported in a sufficiently positive divisor\nclass on a surface. We give an asymptotic lower bound of the Picard number in\ngeneral. In some special cases, we show that this lower bound is attained based\non the geometry of moduli spaces of stable pairs and relative Hilbert schemes\nof points. Additionally, we discuss several related questions and provide\nexamples where the asymptotic irreducibility of the moduli space fails,\nhighlighting a notable distinction from the higher rank case.",
        "The main goal of this note is to show that subalgebras of regular evolution\nalgebras are themselves evolution algebras. This allows us to assume, without\nloss of generality, that every subalgebra in the regular setting has a basis\nconsisting of vectors with disjoint supports. Finally, we use this result to\ncharacterise the existence of codimension-one subalgebras in regular evolution\nalgebras.",
        "We examine theoretical properties of the denoising score matching estimate.\nWe model the density of observations with a nonparametric Gaussian mixture. We\nsignificantly relax the standard manifold assumption allowing the samples step\naway from the manifold. At the same time, we are still able to leverage a nice\ndistribution structure. We derive non-asymptotic bounds on the approximation\nand generalization errors of the denoising score matching estimate. The rates\nof convergence are determined by the intrinsic dimension. Furthermore, our\nbounds remain valid even if we allow the ambient dimension grow polynomially\nwith the sample size.",
        "Unifying the massive spin-1 field with gravity requires the implementation of\na regular vector field that satisfies the spin-1 Proca equation and is a\nfundamental part of the spacetime metric. That vector field is one of the pair\nof vectors in the line element field (\\textbf{X},-\\textbf{X}), which is\nparamount to the existence of all Lorentzian metrics and Modified General\nRelativity (MGR). Symmetrization of the spin-1 Klein-Gordon equation in a\ncurved Lorentzian spacetime introduces the Lie derivative of the metric along\nthe flow of one of the regular vectors in the line element field. The Proca\nequation in curved spacetime can then be described geometrically in terms of\nthe line element vector, the Lie derivative of the Lorentzian metric, and the\nRicci tensor, which unifies gravity and the spin-1 field. Related issues\nconcerning charge conservation and the Lorenz constraint, singularities in a\nspherically symmetric curved spacetime, and geometrical implications of MGR to\nquantum theory are discussed. A geometrical unification of gravity with quantum\nfield theory is presented.",
        "We present a numerical proof of the concept that the void spin distributions\ncan in principle provide a tight constraint on the amplitude of matter density\nfluctuation on the scale of $8\\,h^{-1}{\\rm Mpc}$ ($\\sigma_{8}$) without being\nseverely deteriorated by the degeneracies of $\\sigma_{8}$ with cold dark matter\ndensity parameter multiplied by the dimensionless Hubble parameter square\n($\\Omega_{\\rm cdm}h^{2}$), total neutrino mass ($M_{\\nu}$) and dark energy\nequation of state ($w$). Applying the Void-Finder algorithm to a total of $15$\nAbacusSummit $N$-body simulations of $15$ different cosmological models, we\nidentify the giant voids to measure their spins, defined as the magnitudes of\nrescaled specific angular momenta of void halos. The $15$ cosmologies include\nthe Planck $\\Lambda$CDM and $14$ non-Planck models, each of which differs among\none another only in one of $\\{\\sigma_{8},\\ \\Omega_{\\rm cdm}h^{2},\\ M_{\\nu},\\\nw\\}$. The probability density distribution of void spins is determined for each\nmodel and found to be well approximated by the generalized Gamma distribution\nwith two characteristic parameters, $k$ and $\\theta$. It turns out that the\nbest-fit values of $k$ and $\\theta$ exhibit very sensitive dependences only on\n$\\sigma_{8}$, being almost insensitive to $\\Omega_{\\rm cdm}h^{2}$, $M_{\\nu}$,\n$w$. This exclusive $\\sigma_{8}$-dependence of the void spin distributions is\nconfirmed to be robust against the variation of the mass and number cuts of\nvoid halos. We also test an observational feasibility of estimating the void\nspins from real data on the galaxy redshifts.",
        "In this paper, we are concerned with the Cauchy problem for the\nreaction-diffusion equation $\\partial_t u+t^\\beta\\mathcal{L} u= - h(t)u^p$\nposed on $\\mathbb{R}^N$, driven by the mixed local-nonlocal operator\n$\\mathcal{L}=-\\Delta+(-\\Delta)^{\\alpha\/2}$, $\\alpha\\in(0,2)$, and supplemented\nwith a nonnegative integrable initial data, where $p>1$, $\\beta\\geq 0$, and\n$h:(0,\\infty)\\to(0,\\infty)$ is a locally integrable function. We study the\nlarge time behavior of non-negative solutions and show that the nonlinear term\ndetermines the large time asymptotic for $p\\leq 1+{\\alpha}\/{N(\\beta+1)},$ while\nthe classical\/anomalous diffusion effects win if $p>1+{\\alpha}\/{N(\\beta+1)}$.",
        "The estimation of average treatment effects (ATEs), defined as the difference\nin expected outcomes between treatment and control groups, is a central topic\nin causal inference. This study develops semiparametric efficient estimators\nfor ATE estimation in a setting where only a treatment group and an unknown\ngroup-comprising units for which it is unclear whether they received the\ntreatment or control-are observable. This scenario represents a variant of\nlearning from positive and unlabeled data (PU learning) and can be regarded as\na special case of ATE estimation with missing data. For this setting, we derive\nsemiparametric efficiency bounds, which provide lower bounds on the asymptotic\nvariance of regular estimators. We then propose semiparametric efficient ATE\nestimators whose asymptotic variance aligns with these efficiency bounds. Our\nfindings contribute to causal inference with missing data and weakly supervised\nlearning.",
        "The Cryogenic AntiCoincidence Detector (CryoAC) is a key element of the X-ray\nIntegral Field Unit (X-IFU) on board the future ATHENA X-ray observatory. It is\na TES-based detector designed to reduce the particle background of the\ninstrument, thereby increasing its sensitivity. The detector design is driven\nby an end-to-end simulator which includes the electro-thermal modelling of the\ndetector and the dynamics of its readout chain. Here, we present the\nmeasurements carried out on the last CryoAC single pixel prototype, namely\nDM127, in order to evaluate the critical thermal parameters of the detector and\nconsequently to tune and validate the CryoAC end-to-end simulator.",
        "This paper introduces OGBoost, a scikit-learn-compatible Python package for\nordinal regression using gradient boosting. Ordinal variables (e.g., rating\nscales, quality assessments) lie between nominal and continuous data,\nnecessitating specialized methods that reflect their inherent ordering. Built\non a coordinate-descent approach for optimization and the latent-variable\nframework for ordinal regression, OGBoost performs joint optimization of a\nlatent continuous regression function (functional gradient descent) and a\nthreshold vector that converts the latent continuous value into discrete class\nprobabilities (classical gradient descent). In addition to the stanadard\nmethods for scikit-learn classifiers, the GradientBoostingOrdinal class\nimplements a \"decision_function\" that returns the (scalar) value of the latent\nfunction for each observation, which can be used as a high-resolution\nalternative to class labels for comparing and ranking observations. The class\nhas the option to use cross-validation for early stopping rather than a single\nholdout validation set, a more robust approach for small and\/or imbalanced\ndatasets. Furthermore, users can select base learners with different underlying\nalgorithms and\/or hyperparameters for use throughout the boosting iterations,\nresulting in a `heterogeneous' ensemble approach that can be used as a more\nefficient alternative to hyperparameter tuning (e.g. via grid search). We\nillustrate the capabilities of OGBoost through examples, using the wine quality\ndataset from the UCI respository. The package is available on PyPI and can be\ninstalled via \"pip install ogboost\".",
        "We study the regulation of algorithmic (non-)collusion amongst sellers in\ndynamic imperfect price competition by auditing their data as introduced by\nHartline et al. [2024].\n  We develop an auditing method that tests whether a seller's pessimistic\ncalibrated regret is low. The pessimistic calibrated regret is the highest\ncalibrated regret of outcomes compatible with the observed data. This method\nrelaxes the previous requirement that a pricing algorithm must use\nfully-supported price distributions to be auditable. This method is at least as\npermissive as any auditing method that has a high probability of failing\nalgorithmic outcomes with non-vanishing calibrated regret. Additionally, we\nstrengthen the justification for using vanishing calibrated regret, versus\nvanishing best-in-hindsight regret, as the non-collusion definition, by showing\nthat even without any side information, the pricing algorithms that only\nsatisfy weaker vanishing best-in-hindsight regret allow an opponent to\nmanipulate them into posting supra-competitive prices. This manipulation cannot\nbe excluded with a non-collusion definition of vanishing best-in-hindsight\nregret.\n  We motivate and interpret the approach of auditing algorithms from their data\nas suggesting a per se rule. However, we demonstrate that it is possible for\nalgorithms to pass the audit by pretending to have higher costs than they\nactually do. For such scenarios, the rule of reason can be applied to bound the\nrange of costs to those that are reasonable for the domain.",
        "We prove that for a some natural class of weights $\\K$ and any weighted space\n$$L^2(w) = \\left\\{ f : (-\\pi,\\pi) \\to \\CC \\colon\\int_{-\\pi}^{\\pi} {{|f(t)|^2}\n\\over {w(t)}} dt < \\infty \\right\\},$$ where $w \\in \\K$, there exists a complete\nand minimal system $\\{e^{i\\lambda_nt}\\}_{n\\in \\Z}$ of exponentials, which does\nnot admit spectral synthesis.",
        "Artificial intelligence (AI) technology and systems have been advancing\nrapidly. However, ensuring the reliability of these systems is crucial for\nfostering public confidence in their use. This necessitates the modeling and\nanalysis of reliability data specific to AI systems. A major challenge in AI\nreliability research, particularly for those in academia, is the lack of\nreadily available AI reliability data. To address this gap, this paper focuses\non conducting a comprehensive review of available AI reliability data and\nestablishing DR-AIR: a data repository for AI reliability. Specifically, we\nintroduce key measurements and data types for assessing AI reliability, along\nwith the methodologies used to collect these data. We also provide a detailed\ndescription of the currently available datasets with illustrative examples.\nFurthermore, we outline the setup of the DR-AIR repository and demonstrate its\npractical applications. This repository provides easy access to datasets\nspecifically curated for AI reliability research. We believe these efforts will\nsignificantly benefit the AI research community by facilitating access to\nvaluable reliability data and promoting collaboration across various academic\ndomains within AI. We conclude our paper with a call to action, encouraging the\nresearch community to contribute and share AI reliability data to further\nadvance this critical field of study.",
        "Scientific discovery relies on scientists generating novel hypotheses that\nundergo rigorous experimental validation. To augment this process, we introduce\nan AI co-scientist, a multi-agent system built on Gemini 2.0. The AI\nco-scientist is intended to help uncover new, original knowledge and to\nformulate demonstrably novel research hypotheses and proposals, building upon\nprior evidence and aligned to scientist-provided research objectives and\nguidance. The system's design incorporates a generate, debate, and evolve\napproach to hypothesis generation, inspired by the scientific method and\naccelerated by scaling test-time compute. Key contributions include: (1) a\nmulti-agent architecture with an asynchronous task execution framework for\nflexible compute scaling; (2) a tournament evolution process for self-improving\nhypotheses generation. Automated evaluations show continued benefits of\ntest-time compute, improving hypothesis quality. While general purpose, we\nfocus development and validation in three biomedical areas: drug repurposing,\nnovel target discovery, and explaining mechanisms of bacterial evolution and\nanti-microbial resistance. For drug repurposing, the system proposes candidates\nwith promising validation findings, including candidates for acute myeloid\nleukemia that show tumor inhibition in vitro at clinically applicable\nconcentrations. For novel target discovery, the AI co-scientist proposed new\nepigenetic targets for liver fibrosis, validated by anti-fibrotic activity and\nliver cell regeneration in human hepatic organoids. Finally, the AI\nco-scientist recapitulated unpublished experimental results via a parallel in\nsilico discovery of a novel gene transfer mechanism in bacterial evolution.\nThese results, detailed in separate, co-timed reports, demonstrate the\npotential to augment biomedical and scientific discovery and usher an era of AI\nempowered scientists.",
        "This article addresses calibration challenges in analytical chemistry by\nemploying a random-effects calibration curve model and its generalizations to\ncapture variability in analyte concentrations. The model is motivated by\nspecific issues in analytical chemistry, where measurement errors remain\nconstant at low concentrations but increase proportionally as concentrations\nrise. To account for this, the model permits the parameters of the calibration\ncurve, which relate instrument responses to true concentrations, to vary across\ndifferent laboratories, thereby reflecting real-world variability in\nmeasurement processes. Traditional large-sample interval estimation methods are\ninadequate for small samples, leading to the use of an alternative approach,\nnamely the fiducial approach. The calibration curve that accurately captures\nthe heteroscedastic nature of the data, results in more reliable estimates\nacross diverse laboratory conditions. It turns out that the fiducial approach,\nwhen used to construct a confidence interval for an unknown concentration,\nproduces a slightly wider width while achieving the desired coverage\nprobability. Applications considered include the determination of the presence\nof an analyte and the interval estimation of an unknown true analyte\nconcentration. The proposed method is demonstrated for both simulated and real\ninterlaboratory data, including examples involving copper and cadmium in\ndistilled water.",
        "We introduce wavefront shaping as a tool for optimizing the sensitivity in\nnano-optomechanical measurement schemes. We perform multimode output analysis\nof an optomechanical system consisting of a focused laser beam coupled to the\ntransverse motion of a tapered cantilever, and demonstrate that wavefront\nshaping enables a 350-fold enhancement of the measurement signal-to-noise\n(+25.5 dB) compared to standard split-detection, close to the quantum precision\nlimit. Our results open new perspectives in terms of sensitivity and control of\nthe optomechanical interaction.",
        "We study the question of when two functions L_1,L_2 in the extended Selberg\nclass are identical in terms of the zeros of L_i-h(i=1,2). Here, the\nmeromorphic function h is called moving target. With the assumption on the\ngrowth order of h, we prove that L_1\\equiv L_2 if L_1-h and L_2-h have the same\nzeros counting multiplicities. Moreover, we also construct some examples to\nshow that the assumption is necessary. Compared with the known methods in the\nliterature of this area, we developed a new strategy which is based on the\ntranscendental directions first proposed in the study of distribution of Julia\nset in complex dynamical system. This may be of independent interest.",
        "We present rational Lax representations for one-component parametric\nquadrirational Yang-Baxter maps in both the abelian and non-abelian settings.\nWe show that from the Lax matrices of a general class of non-abelian involutive\nYang-Baxter maps ($\\mathcal{K}$-list), by considering the symmetries of the\n$\\mathcal{K}$-list maps, we obtain compatible refactorization problems with\nrational Lax matrices for other classes of non-abelian involutive Yang-Baxter\nmaps ($\\Lambda$, $\\mathcal{H}$ and $\\mathcal{F}$ lists). In the abelian\nsetting, this procedure generates rational Lax representations for the abelian\nYang-Baxter maps of the $F$ and $H$ lists. Additionally, we provide examples of\nnon-involutive (abelian and non-abelian) multi-parametric Yang-Baxter maps,\nalong with their Lax representations, which lie outside the preceding lists.",
        "The effect of valley splitting on the readout of qubit states is\ntheoretically investigated in a three-quantum-dot (QD) system. A single unit of\nthe three-QD system consists of qubit-QDs and a channel-QD that is connected to\na conventional transistor. The nonlinear source--drain current characteristics\nunder resonant-tunneling effects are used to distinguish different qubit\nstates. Using nonequilibrium Green functions, the current formula for the\nthree-QD system is derived when each QD has two valley energy levels. Two\nvalley states in each QD are considered to be affected by variations in the\nfabrication process. We found that when valley splitting is smaller than Zeeman\nsplitting, the current nonlinearity can improve the readout, provided that the\nnonuniformity of the valley energy levels is small. Conversely, when the valley\nsplitting is larger than the Zeeman splitting, the nonuniformity degraded the\nreadout. In both cases, we showed that there are regions where the measurement\ntime $t_{\\rm meas}$ is much less than the decoherence time $t_{\\rm dec}$ such\nthat $t_{\\rm dec}\/t_{\\rm meas}>100$. This suggests that less than 1\\%\nmeasurement error is anticipated, which opens up the possibility for\nimplementing surface codes even in the presence of valley splitting.",
        "The KOTO II experiment is proposed to measure the branching ratio of the\ndecay $K_L\\to\\pi^0\\nu\\bar{\\nu}$ at J-PARC. With a beamline to extract\nlong-lived neutral kaons at 5 degrees from a production target, the single\nevent sensitivity of the decay is $8.5\\times 10^{-13}$, which is much smaller\nthan the Standard Model prediction $3\\times 10^{-11}$. This allows searches for\nnew physics beyond the Standard Model and the first discovery of the decay with\na significance exceeding $5\\sigma$. As the only experiment proposed in the\nworld dedicated to rare kaon decays, KOTO II will be indispensable in the quest\nfor a complete understanding of flavor dynamics in the quark sector. Moreover,\nby combining efforts from the kaon community worldwide, we plan to develop the\nKOTO II detector further and expand the physics reach of the experiment to\ninclude measurements of the branching ratio of the $K_L\\to\\pi^0\\ell^+\\ell^-$\ndecays, studies of other $K_L$ decays, and searches for dark photons, axions,\nand axion-like particles. KOTO II will therefore obtain a comprehensive\nunderstanding of $K_L$ decays, providing further constraints on new physics\nscenarios with existing $K^+$ results.",
        "Hypergeometric class equations are given by second\n  order differential operators in one variable whose coefficient at the second\nderivative\n  is a polynomial of degree $\\leq2$, at the first derivative of degree\n  $\\leq1$ and the free term is a number. Their solutions, called hypergeometric\nclass functions, include the Gauss hypergeometric function and its various\nlimiting cases. The paper presents a unified approach to these functions. The\nmain structure behind this approach is a family of complex 4-dimensional Lie\nalgebras, originally due to Willard Miller. Hypergeometric class functions can\nbe interpreted as eigenfunctions of the quadratic Casimir operator in a\nrepresentation of Miller's Lie algebra given by differential operators in three\ncomplex variables. One obtains a unified treatment of various properties of\nhypergeometric class functions such as recurrence relations, discrete\nsymmetries, power series expansions, integral representations, generating\nfunctions and orthogonality of polynomial solutions.",
        "There is a lot of information available concerning Hardy-Hilbert type\ninequalities in one or more dimensions. In this paper we introduce the\ndevelopment of such inequalities on homogeneous groups. Moreover, we point out\na unification of several of the Hardy-Hilbert type inequalities in the\nclassical case to a general kernel case. Finally, we generalize these results\nto the homogeneous group case.",
        "By introducing appropriate lattice parameters for a bi-lattice smoothly\nconnecting the hexagonal close-packed (hcp) with the cuboidal structures,\nnamely the body-centered (bcc) and the face centered cubic (fcc) lattices, we\nwere able to map out the minimum energy path for a Burgers-Bain type of phase\ntransition. We demonstrate that for three different models applied, i.e. the\nkissing hard-sphere model, the Lennard-Jones potential, and density functional\ntheory for metallic lithium, the direct transition path is always from hcp to\nfcc with a separate path leading from fcc to bcc. This solves, at least for the\nmodels considered here, a long-standing controversy of whether or not fcc acts\nas an intermediate phase in martensitic type of phase transitions.",
        "Given a regular function $\\phi$ on a smooth stack, and a $(-1)$-shifted\nLagrangian $M$ on the derived critical locus of $\\phi$, under fairly general\nhypotheses, we construct a pullback map from the Grothendieck group of coherent\nmatrix factorizations of $\\phi$ to that of coherent sheaves on $M$. This map\nsatisfies a functoriality property with respect to the composition of\nLagrangian correspondences, as well as the usual bivariance and base-change\nproperties.\n  We provide three applications of the construction, one in the definition of\nquantum $K$-theory of critical loci (Landau-Ginzburg models), paving the way to\ngeneralize works of Okounkov school from Nakajima quiver varieties to quivers\nwith potentials, one in establishing a degeneration formula for $K$-theoretic\nDonaldson-Thomas theory of local Calabi-Yau 4-folds, the other in confirming a\n$K$-theoretic version of Joyce-Safronov conjecture.",
        "We investigate the nucleosynthesis and kilonova emission based on\nnumerical-relativity binary neutron star merger simulations that incorporate a\ntwo-moment neutrino-transport scheme. Unlike in previous works with simpler\nneutrino treatments, a massive, fast (up to $v=0.3c$), proton-rich\nneutrino-driven wind develops in the post-merger phase of the simulations as\nlong as the merger remnant does not collapse to a black hole. We evolve the\nejecta for 100 days after the merger using 2D ray-by-ray\nradiation-hydrodynamics simulations coupled in-situ to a complete nuclear\nnetwork. The most abundant nucleosynthesis products are He, $^{56}$Ni, and\n$^{56}$Co. We find a total yield of $\\sim 10^{-3} M_\\odot$ of $^{56}$Ni for all\nmergers that produce massive neutron star remnants, independently of the mass\nratio and equation of state. After a few days, the decay of $^{56}$Ni and later\n$^{56}$Co becomes the primary source of heating in the matter expanding above\nthe remnant. As a result, the kilonova light curve flattens on timescales of\ndays for polar observation angles. The observation of this effect could serve\nas smoking gun for the presence of a long-lived neutron star remnant in future\nkilonova observations.",
        "The EXO-UV program is an international, interdisciplinary collaboration\nbetween astrophysicists and biologists aimed at expanding the characterization\nof ultraviolet radiation (UVR) environments on exoplanets. This approach\ncombines astrophysical studies with biological experiments to better understand\nthe potential impacts of UVR on exoplanetary surfaces. UVR is particularly\nrelevant because it reaches the surface of planets and can influence their\nhabitability. The specific wavelengths within the UVR spectrum depend on the\nplanet's atmospheric composition and the spectral energy distribution of its\nhost star. Additionally, high UVR fluxes emitted during flares and superflares\nare of particular interest due to the limited information available regarding\ntheir biological impact. The EXO-UV program has successfully led to the first\nexperimental study examining the biological effects of high UVR fluences, such\nas those produced by flares and superflares. Future experimental studies aim to\ninvestigate the biological effects of repetitive flares. In this paper, we\nreview the latest results from our EXO-UV program."
      ]
    }
  },
  {
    "id":2411.18767,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction",
    "start_abstract":"Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer"
      ],
      "abstract":[
        "Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "FOCUS: First Order Concentrated Updating Scheme",
        "Ab Initio theory of Electron-phonon-coupling-induced Giant Magnetic\n  Moments of Chiral Phonons in Magnetic Materials",
        "Equivalence between exponential concentration in quantum machine\n  learning kernels and barren plateaus in variational algorithms",
        "The Marginal Importance of Distortions and Alignment in CASSI systems",
        "Multi-scale physics of cryogenic liquid helium-4: Inverse\n  coarse-graining properties of smoothed particle hydrodynamics",
        "Stability of difference equations with interspecific density dependence,\n  competition, and maturation delays",
        "Design of a quantum diamond microscope with efficient scanning confocal\n  readout",
        "Timing and spectral studies of the Be\/X-ray binary EXO 2030+375 using\n  Insight-HXMT observations",
        "Numerical Study On Temperature Variations Of Superheated Steam Flowing\n  Through A Regulation Valve",
        "Visualizing quantum entanglement in Bose-Einstein condensates without\n  state vectors",
        "Runout of liquefaction-induced tailings dam failure: Influence of\n  earthquake motions and residual strength",
        "Multifunctional Altermagnet with Large Out-of-Plane Piezoelectric\n  Response in Janus V$_{2}$AsBrO Monolayer",
        "On finitary power monoids of linearly orderable monoids",
        "On the role of 5-wave resonances in the nonlinear dynamics of the\n  Fermi-Pasta-Ulam-Tsingou lattice",
        "Influences of accretion flow and dilaton charge on the images of\n  Einstein-Maxwell-dilation black hole",
        "Preservation of log-concavity on gamma polynomials",
        "An unstructured block-based adaptive mesh refinement approach for\n  explicit discontinuous Galerkin method",
        "Reinforcement Learning Based Goodput Maximization with Quantized\n  Feedback in URLLC",
        "Privacy Preservation in MIMO-OFDM Localization Systems: A Beamforming\n  Approach",
        "Tropical Poincar\\'e bundle, Fourier-Mukai transform, and a generalized\n  Poincar\\'e formula",
        "Note on surface defects in multiscalar critical models",
        "Advances in multiparameter quantum sensing and metrology",
        "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models",
        "Generative Adversarial Networks for High-Dimensional Item Factor\n  Analysis: A Deep Adversarial Learning Algorithm",
        "Coalitional control: a bottom-up approach",
        "Threshold Quantum Secret Sharing",
        "Array oscillator in coupled waveguides with nonlinear gain and radiation\n  resistances saturating at exceptional point",
        "A note on the differential spectrum of a class of locally APN functions",
        "The Simultaneous Operation of a Controllable Segmented Primary Mirror\n  and Single Conjugate Adaptive Optics System part 1 -- Design Concept and\n  Sensitivity Analysis"
      ],
      "abstract":[
        "Large language models (LLMs) demonstrate remarkable performance, and\nimproving their pre-training process appears to be key to enhancing their\ncapabilities further. Based on the documented success of Adam, learning rate\ndecay, and weight decay, we hypothesize that the pre-training loss landscape\nfeatures a narrowing valley structure. Through experiments with synthetic loss\nfunctions, we discover that when gradient query noise is high relative to the\nvalley's sharpness, Adam's performance falls behind that of Signum because Adam\nreduces the effective step size too drastically. This observation led us to\ndevelop FOCUS, an optimizer that enhances Signum by incorporating attraction\ntoward moving averaged parameters, allowing it to handle noise better while\nmaintaining larger step sizes. In training GPT-2, FOCUS proves to be more\nstable than Signum and faster than Adam. These results suggest that gradient\nnoise may be an underappreciated limiting factor in LLM training, and FOCUS\noffers promising solutions.",
        "Chiral phonons, characterized by nonzero angular momenta and magnetic\nmoments, have attracted extensive attention. However, a long-standing critical\nissue in this field is the lack of ab initio methods to accurately calculate\nphonon magnetic moments resulting from electron-phonon coupling (EPC). Here, we\nresolve this challenge by developing an ab initio theory for calculating\nEPC-induced phonon magnetic properties, applicable to both insulating and\nmetallic materials. Based on this theory, we demonstrate EPC-induced giant\nphonon magnetic moments and resulting phonon Zeeman splittings in magnetic\nmetals, which are orders of magnitude larger than classical predictions from\npoint-charge models. Interestingly, these splittings could open observable\ntopological gaps in phonon spectra of magnets, generating intrinsic phonon\nChern states. Through first-principles calculations, we first propose candidate\nmaterials with such intrinsic phonon Chern states hosting robust edge phonon\ncurrents which may be applied to detecting neutral particles such as dark\nmatter particles. Our work not only establishes a theoretical foundation for\nEPC-induced phonon magnetic properties, but also enables the ab initio\ncalculation of long-sought TRS-breaking phonon spectra throughout the Brillouin\nzone in realistic materials.",
        "We formalize a rigorous connection between barren plateaus (BP) in\nvariational quantum algorithms and exponential concentration of quantum kernels\nfor machine learning. Our results imply that recently proposed strategies to\nbuild BP-free quantum circuits can be utilized to construct useful quantum\nkernels for machine learning. This is illustrated by a numerical example\nemploying a provably BP-free quantum neural network to construct kernel\nmatrices for classification datasets of increasing dimensionality without\nexponential concentration.",
        "This paper introduces a differentiable ray-tracing based model that\nincorporates aberrations and distortions to render realistic coded\nhyperspectral acquisitions using Coded-Aperture Spectral Snapshot Imagers\n(CASSI). CASSI systems can now be optimized in order to fulfill simultaneously\nseveral optical design constraints as well as processing constraints. Four\ncomparable CASSI systems with varying degree of optical aberrations have been\ndesigned and modeled. The resulting rendered hyperspectral acquisitions from\neach of these systems are combined with five state-of-the-art hyperspectral\ncube reconstruction processes. These reconstruction processes encompass a\nmapping function created from each system's propagation model to account for\ndistortions and aberrations during the reconstruction process. Our analyses\nshow that if properly modeled, the effects of geometric distortions of the\nsystem and misalignments of the dispersive elements have a marginal impact on\nthe overall quality of the reconstructed hyperspectral data cubes. Therefore,\nrelaxing traditional constraints on measurement conformity and fidelity to the\nscene enables the development of novel imaging instruments, guided by\nperformance metrics applied to the design or the processing of acquisitions. By\nproviding a complete framework for design, simulation and evaluation, this work\ncontributes to the optimization and exploration of new CASSI systems, and more\ngenerally to the computational imaging community.",
        "Our recent numerical studies on cryogenic liquid helium-4 strongly indicate\nthe features of multiscale physics that can be identified using the Landau's\ntwo-fluid model. This study presents the possibility that two-fluid models\nbased on classical and quantum hydrodynamics have a relationship between the\nscale transformation using filtering in large eddy simulations (LES) and the\ninverse scale transformation using smoothed particle hydrodynamics (SPH). We\nshow that the spin angular momentum conservation term, which we previously\nintroduced into the two-fluid model as a quantum mechanical correction,\nformally corresponds to the subgrid-scale (SGS) model, which can be derived\nfrom the scale transformation of the two-fluid model from quantum to classical\nhydrodynamics. Our theoretical analysis shows that solving the two-fluid model\nbased on classical hydrodynamics using SPH can reproduce the microscopic\nfluctuations even at the macroscopic scale because the truncation errors owing\nto the smoothing kernel approximation can substitute the microscopic\nfluctuations. In particular, the fluctuations can be amplified according to the\nsize of the kernel radius at the macroscopic scale. Our further theoretical\nanalysis shows that the Condiff viscosity model can serve as an SGS model and\nincorporate the quantum vortex interactions into the two-fluid model. Our\nresults and discussion provide new insights into the microscopic composition of\nthe cryogenic liquid helium-4 within a multiscale framework. First, a normal\nfluid can be a mixture of inviscid and viscous fluid particles. Second, a flow\nidentified as a normal fluid on the microscopic scale because of the presence\nof molecular viscosity is still classified as an inviscid fluid on the\nhydrodynamic scale because its viscosity is insufficient to produce eddy\nviscosity.",
        "A general system of difference equations is presented for multispecies\ncommunities with density dependent population growth and delayed maturity.\nInterspecific competition, mutualism, predation, commensalism, and amensalism\nare accommodated. A sufficient condition for the local asymptotic stability of\na coexistence equilibrium in this system is then proven. Using this system, the\ngeneralisation of the Beverton-Holt and Leslie-Gower models of competition to\nmultispecies systems with possible maturation delays is presented and shown to\nyield interesting stability properties. The stability of coexistence depends on\nthe relative abundances of the species at the unique interior equilibrium. A\nsufficient condition for local stability is derived that only requires\nintraspecific competition to outweigh interspecific competition. The condition\ndoes not depend on maturation delays. The derived stability properties are used\nto develop a novel estimation approach for the coefficients of interspecific\ncompetition. This approach finds an optimal configuration given the conjecture\nthat coexisting species strive to outcompete competitors and are most likely\nobserved near the stable interior equilibrium with the greatest dampening of\nperturbations. The optimal solution is compared to estimates of niche overlap\nusing an empirical example of malaria mosquito vectors with delayed maturity in\nthe Anopheles gambiae sensu lato species complex.",
        "We introduce the light-sheet confocal quantum diamond microscope (LC-QDM) for\nwidefield 3D quantum sensing with efficient confocal readout. The LC-QDM\nleverages light-sheet illumination and laser scanning confocal methods to\nenable high-resolution, high-speed 3D measurements with nitrogen-vacancy (NV)\ndefects in diamond, combining the best of widefield and confocal modalities in\na single device and eliminating the need for thin-NV-layer diamond chips. We\nperform simulations and measurements of NV initialization and readout times to\nmodel the anticipated performance of the LC-QDM compared to existing QDM\ndesigns. Our findings show that the LC-QDM will provide significant advantages\nfor applications requiring limited laser power.",
        "We report the X-ray spectral and timing analysis of the high mass X-ray\nbinary EXO 2030+375 during the 2021 type-II outburst based on the Insight-HXMT\nobservations. Pulsations can be detected in the energy band of 1-150 keV. The\npulse profile shows energy and luminosity dependence and variability. We\nobserved transitions in the pulse profile shape during the rising and the\ndecaying phase of the outburst. The pulse fraction exhibits an anti-correlation\nwith luminosity and a non-monotonic energy dependence, with a possible dip near\n30 keV during the outburst peak. The hardness-intensity diagrams (7-10 keV\/4-7\nkeV) suggest state transitions during the early and late phases of the\noutburst. These transitions are consistent with the luminosity at which the\npulse profile shape changes occur, revealing the source reaching the critical\nluminosity and transitioning between super-critical and sub-critical accretion\nregimes. We performed the average and phase-resolved spectral analysis, where\nthe flux-resolved average spectra show a stable spectral evolution with\nluminosity. The phase-resolved spectral analysis reveals that the dependence of\nspectral parameters on the pulse phase varies with different luminosities.",
        "Superheated steam is widely employed in various energy systems, particularly\nin power plants, chemical industries, and other applications where\nhigh-temperature and high-pressure steam is essential for efficient energy\nconversion and process control. In these systems, regulation valves are crucial\ncomponents that control the flow of steam, adjusting its pressure and\ntemperature to ensure safe and efficient operation. Accurate understanding and\nprediction of temperature variations within regulation valves are essential for\noptimizing their performance and improving the overall system efficiency. This\nstudy investigates the temperature variations of superheated steam flowing\nthrough a regulation valve using computational fluid dynamics (CFD) simulations\ncombined with Proper Orthogonal Decomposition (POD) techniques. The analysis\nbegins with an examination of the internal flow field parameters, including\ntemperature and pressure, to understand the overall fluid dynamics within the\nvalve. POD is applied to reduce the dimensionality of the CFD results. Singular\nValue Decomposition (SVD) is employed to extract the dominant modes that\ncapture the key flow structures responsible for heat transfer and temperature\nfluctuations. The POD analysis reveals that the most influential modes are\nassociated with regions of high turbulence intensity and significant\ntemperature gradients, which are critical to the thermal performance of the\nsteam flow through the regulation valve. The application of POD to 3D CFD\nresults represents a novel approach, particularly for complex fluid flow models\nsuch as steam flow through regulation valves. The insights gained from this\nstudy have practical implications for the design and optimization of\ntemperature and pressure regulation valves in energy systems, providing a\ntheoretical foundation for enhancing the efficiency and reliability of these\nsystems.",
        "Ring polymer self-consistent field theory is used to calculate the critical\ntemperatures and heat capacities of an ideal Bose gas for an order of magnitude\nmore particles than previously reported. A lambda-transition indicative of\nBose-Einstein condensation is observed as expected. Using a known proof of\nspatial mode entanglement in Bose-Einstein condensates, a relationship between\nboson exchange and quantum entanglement is established. This is done without\nthe use of state vectors, since ring polymer quantum theory uses instead a\nthermal degree of freedom, sometimes called the \"imaginary time\", to map\nclassical statistical mechanics onto non-relativistic quantum mechanics through\nthe theorems of density functional theory. It is shown that quantum phenomena,\nsuch as Bose-Einstein condensation, boson exchange, entanglement and\ncontextuality, can be visualized in terms of merging and separating ring\npolymer threads in thermal-space. A possible extension to fermions is\nmentioned.",
        "This study utilizes a hybrid Finite Element Method (FEM) and Material Point\nMethod (MPM) to investigate the runout of liquefaction-induced flow slide\nfailures. The key inputs to this analysis are the earthquake ground motion,\nwhich induces liquefaction, and the post-liquefaction residual strength. The\ninfluence of these factors on runout is evaluated by subjecting a model of a\ntailings dam to thirty different earthquake motions and by assigning different\nvalues of post-liquefaction residual strength. Ground motions with larger peak\nground accelerations (PGA) generate liquefaction to larger depths, thus\nmobilizing a greater mass of material and resulting in a flow slide with\ngreater runout. However, different ground motions with the same PGA yield\nsignificant variations in the depth of liquefaction, indicating that other\nground motion characteristics (e.g., frequency content) also exert significant\ninfluence over the initiation of liquefaction. Ground motion characteristics of\npeak ground velocity (PGV) and Modified Acceleration Spectrum Intensity (MASI)\nshow a strong correlation to the induced depth of liquefaction because they\ncapture both the intensity and frequency content of the earthquake motion. The\ncomputed runout is directly related to the depth of liquefaction induced by the\nearthquake motion. For dam geometry analyzed, measurable runout occurs when\nliquefaction extends to 10 m depth and the runout is maximized when\nliquefaction extends to about 18 m. Strain-softening of the residual strength\nof the liquefied tailings during runout is shown to substantially increase the\nrunout distance of the flow slide, highlighting the need for additional\nresearch to better characterize the appropriate strength of liquefied materials\nduring flow failures.",
        "Altermagnetism has emerged as a third fundamental category of collinear\nmagnetism, characterized by spin-splitting in symmetry-compensated collinear\nantiferromagnets, opening new frontiers in spintronics and condensed matter\nphysics. Here, based on first-principles calculations, we propose a novel\naltermagnetic semiconductor, the asymmetric Janus V$_2$AsBrO monolayer, which\nexhibits a magnetic easy axis favoring the out-of-plane direction and a\nN\\'{e}el temperature ($T_N$) exceeding room temperature. The system exhibits a\nstrain-tunable piezovalley effect, generating valley polarization under\nuniaxial strain. Notably, hole doping under uniaxial strain generates a net\nmagnetization ($M$) through a piezomagnetic mechanism. Additionally, the broken\ninversion symmetry endows the monolayer with a substantial out-of-plane\npiezoelectric coefficient $d_{31}$ (2.19 pm\/V), presenting broad prospects for\nthe development and design of novel piezoelectric devices. Our findings provide\na promising candidate material for the advancement of 2D multifunctional\ndevices in nanoelectronics, spintronics, valleytronics, and piezoelectrics.",
        "A commutative monoid $M$ is called a linearly orderable monoid if there\nexists a total order on $M$ that is compatible with the monoid operation. The\nfinitary power monoid of a commutative monoid $M$ is the monoid consisting of\nall nonempty finite subsets of $M$ under the so-called sumset. In this paper,\nwe investigate whether certain atomic and divisibility properties ascend from\nlinearly orderable monoids to their corresponding finitary power monoids.",
        "We study the dynamics of the $(\\alpha+\\beta)$ Fermi-Pasta-Ulam-Tsingou\nlattice (FPUT lattice, for short) for an arbitrary number $N$ of interacting\nparticles, in regimes of small enough nonlinearity so that a Birkhoff-Gustavson\ntype of normal form can be found using tools from wave-turbulence theory.\nSpecifically, we obtain the so-called Zakharov equation for $4$-wave resonant\ninteractions and its extension to $5$-wave resonant interactions by Krasitskii,\nbut we introduce an important new feature: even the generic terms in these\nnormal forms contain $resonant$ $interactions$ $only$, via a $unique$ canonical\ntransformation. The resulting normal forms provide an approximation to the\noriginal FPUT lattice that possesses a significant number of exact quadratic\nconservation laws, beyond the quadratic part of the Hamiltonian. We call the\nnew equations \"exact-resonance evolution equations\" and examine their\nproperties: (i) Heisenberg representation's slow evolution allows us to\nimplement numerical methods with large time steps to obtain relevant dynamical\ninformation, such as Lyapunov exponents. (ii) We introduce tests, such as\nconvergence of the normal form transformation and truncation error\nverification, to successfully validate our exact-resonance evolution equations.\n(iii) The systematic construction of new quadratic invariants (via the resonant\ncluster matrix) allows us to use finite-time Lyapunov exponent calculations to\nquantify the level of nonlinearity at which the original FPUT lattice is well\napproximated by the exact-resonance evolution equations. We show numerical\nexperiments in the case $N=9$, but the theory and numerical methods are valid\nfor arbitrary values of $N$. We conclude that, when $3$ divides $N$, at small\nenough nonlinearity the FPUT lattice's dynamics and nontrivial hyperchaos are\ngoverned by $5$-wave resonant interactions.",
        "The characteristics and images of Einstein-Maxwell-Dilaton (EMD) black holes\nare examined in this paper, focusing on their effective potential, photon\ntrajectories, and images with thin and thick accretion disks. We found that the\nshadow and photon sphere radii decrease with increasing dilaton charge. As the\nobservation inclination increases, direct and secondary images become separate,\nwith the direct image appearing hat-shaped. Simulations indicate that the\nbrightness of the shadow and photon ring is higher in static spherical\naccretion flows compared to infalling ones. The study also shows that in thin\ndisk accretion flows, the direct emission predominantly influences observed\nluminosity, with photon ring emission being less significant. Additionally, the\nappearance of black hole images varies with the observer's inclination angle.",
        "Every symmetric polynomial $h(x) = h_0 + h_1\\,x + \\cdots + h_n\\,x^n$, where\n$h_i = h_{n-i}$ for each $i$, can be expressed as a linear combination in the\nbasis $\\{x^i(1+x)^{n-2i}\\}_{i=0}^{\\lfloor n\/2\\rfloor}$. The polynomial\n$\\gamma_h(x) = \\gamma_0 + \\gamma_1 \\,x+ \\cdots + \\gamma_{\\lfloor n\/2\\rfloor}\\,\nx^{\\lfloor n\/2\\rfloor}$, commonly referred to as the $\\gamma$-polynomial of\n$h(x)$, records the coefficients of the aforementioned linear combination. Two\ndecades ago, Br\\\"and\\'en and Gal independently showed that if $\\gamma_h(x)$ has\nnonpositive real roots only, then so does $h(x)$. More recently, Br\\\"and\\'en,\nFerroni, and Jochemko proved using Lorentzian polynomials that if $\\gamma_h(x)$\nis ultra log-concave, then so is $h(x)$, and they raised the question of\nwhether a similar statement can be proved for the usual notion of\nlog-concavity. The purpose of this article is to show that the answer to the\nquestion of Br\\\"anden, Ferroni, and Jochemko is affirmative. One of the crucial\ningredients of the proof is an inequality involving binomial numbers that we\nestablish via a path-counting argument.",
        "In the present paper, we present an adaptive mesh refinement(AMR) approach\ndesigned for the discontinuous Galerkin method for conservation laws. The\nblock-based AMR is adopted to ensure the local data structure simplicity and\nthe efficiency, while the unstructured topology of the initial blocks is\nsupported by the forest concept such that the complex geometry of the\ncomputational domain can be easily treated. The inter-block communication\nthrough guardcells is introduced to avoid the direct treatment of flux\ncomputing between cells at different refinement levels. The sharp corners and\ncreases generated during direct refinement can be avoided by projecting the\nboundary nodes to either the user-defined boundary surface function or the\nauto-generated NURBs. High-level MPI parallelization is implemented with\ndynamic load balancing through a space curve filling procedure. Some test cases\nare presented. As a result, ideal accuracy order and versatility in tracing and\ncontrolling the dynamic refinement are observed. Also, good parallelization\nefficiency is demonstrated.",
        "This paper presents a comprehensive system model for goodput maximization\nwith quantized feedback in Ultra-Reliable Low-Latency Communication (URLLC),\nfocusing on dynamic channel conditions and feedback schemes. The study\ninvestigates a communication system, where the receiver provides quantized\nchannel state information to the transmitter. The system adapts its feedback\nscheme based on reinforcement learning, aiming to maximize goodput while\naccommodating varying channel statistics. We introduce a novel Rician-$K$\nfactor estimation technique to enable the communication system to optimize the\nfeedback scheme. This dynamic approach increases the overall performance,\nmaking it well-suited for practical URLLC applications where channel statistics\nvary over time.",
        "We investigate an uplink MIMO-OFDM localization scenario where a legitimate\nbase station (BS) aims to localize a user equipment (UE) using pilot signals\ntransmitted by the UE, while an unauthorized BS attempts to localize the UE by\neavesdropping on these pilots, posing a risk to the UE's location privacy. To\nenhance legitimate localization performance while protecting the UE's privacy,\nwe formulate an optimization problem regarding the beamformers at the UE,\naiming to minimize the Cram\\'er-Rao bound (CRB) for legitimate localization\nwhile constraining the CRB for unauthorized localization above a threshold. A\npenalty dual decomposition optimization framework is employed to solve the\nproblem, leading to a novel beamforming approach for location privacy\npreservation. Numerical results confirm the effectiveness of the proposed\napproach and demonstrate its superiority over existing benchmarks.",
        "We construct a tropical analogue of the Poincar\\'e bundle and prove a\n(cohomological) Fourier-Mukai transform for real tori with integral structures.\nWe then prove a tropical analogue of Beauville's generalized Poincar\\'e formula\nfor polarized abelian varieties. Some consequences include a geometric\nRiemann-Roch theorem for tropical abelian varieties, as well as a tropical\nPoincar\\'e-Prym formula which was recently conjectured by R\\\"ohrle and\nZakharov.",
        "This paper studies generic surface defects for multiscalar critical models\nusing a perturbative $\\epsilon$ expansion in $4-\\epsilon$ dimensions. The beta\nfunctions of the defect couplings for a generic multiscalar bulk with quartic\ninteractions are computed at first non-trivial order in $\\epsilon$. Specific\nbulks of interest are then considered: $O(N)$, hypercubic, hypertetrahdral, and\nbiconical $O(m)\\times O(n)$. In each case, we compute fixed points for the\ndefect couplings and determine the remaining bulk symmetry. Expanding beyond\nthe $O(N)$ model, we find a greater variety of patterns of symmetry breaking.",
        "Recent years have witnessed a growing interest in understating the\nlimitations imposed by quantum noise in precision measurements and devising\ntechniques to reduce it. The attention is currently turning to the\nsimultaneously estimation of several parameters of interest, driven by its\npromising potential across a wide range of sensing applications as well as\nfueled by experimental progress in various optical and atomic platforms. Here,\nwe provide a comprehensive overview of key research directions in\nmultiparameter quantum sensing and metrology, highlighting both opportunities\nand challenges. We introduce the basic framework, discuss ultimate sensitivity\nbounds, optimal measurement strategies, and the role of quantum\nincompatibility, showing important differences with respect to single-parameter\nestimation. Additionally, we discuss emerging experimental implementations in\ndistributed quantum sensing, including cutting-edge optimization techniques.\nThis review aims to bridge the gap between theory and experiments, paving the\nway for the next-generation of quantum sensors and their integration with other\nquantum technologies.",
        "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research.",
        "Advances in deep learning and representation learning have transformed item\nfactor analysis (IFA) in the item response theory (IRT) literature by enabling\nmore efficient and accurate parameter estimation. Variational Autoencoders\n(VAEs) have been one of the most impactful techniques in modeling\nhigh-dimensional latent variables in this context. However, the limited\nexpressiveness of the inference model based on traditional VAEs can still\nhinder the estimation performance. We introduce Adversarial Variational Bayes\n(AVB) algorithms as an improvement to VAEs for IFA with improved flexibility\nand accuracy. By bridging the strengths of VAEs and Generative Adversarial\nNetworks (GANs), AVB incorporates an auxiliary discriminator network to reframe\nthe estimation process as a two-player adversarial game and removes the\nrestrictive assumption of standard normal distributions in the inference model.\nTheoretically, AVB can achieve similar or higher likelihood compared to VAEs. A\nfurther enhanced algorithm, Importance-weighted Adversarial Variational Bayes\n(IWAVB) is proposed and compared with Importance-weighted Autoencoders (IWAE).\nIn an exploratory analysis of empirical data, IWAVB demonstrated superior\nexpressiveness by achieving a higher likelihood compared to IWAE. In\nconfirmatory analysis with simulated data, IWAVB achieved similar mean-square\nerror results to IWAE while consistently achieving higher likelihoods. When\nlatent variables followed a multimodal distribution, IWAVB outperformed IWAE.\nWith its innovative use of GANs, IWAVB is shown to have the potential to extend\nIFA to handle large-scale data, facilitating the potential integration of\npsychometrics and multimodal data analysis.",
        "The recent major developments in information technologies have opened\ninteresting possibilities for the effective management of multi-agent systems.\nIn many cases, the important role of central control nodes can now be\nundertaken by several controllers in a distributed topology that suits better\nthe structure of the system. This opens as well the possibility to promote\ncooperation between control agents in competitive environments, establishing\nlinks between controllers in order to adapt the exchange of critical\ninformation to the degree of subsystems' interactions. In this paper a\nbottom-up approach to coalitional control is presented, where the structure of\neach agent's model predictive controller is adapted to the time-variant\ncoupling conditions, promoting the formation of coalitions - clusters of\ncontrol agents where communication is essential to ensure the cooperation -\nwhenever it can bring benefit to the overall system performance.",
        "One crucial and basic method for disclosing a secret to every participant in\nquantum cryptography is quantum secret sharing. Numerous intricate protocols,\nincluding secure multiparty summation, multiplication, sorting, voting, and\nmore, can be designed with it. A quantum secret sharing protocol with a $(t,n)$\nthreshold approach and modulo d, where t and n represent the threshold number\nof participants and the total number of participants, respectively was recently\ndiscussed by Song et al. Kao et al. notes that without the information of other\nparticipants, the secret in Song {\\em et al.'s}protocol cannot be\nreconstructed. We address a protocol that solves this issue in this paper.",
        "A periodically loaded waveguide composed of periodic discrete nonlinear gain\nand radiating elements supports a stable oscillation regime related to the\npresence of an exceptional point of degeneracy (EPD). After reaching\nsaturation, the EPD in the system establishes the oscillation frequency. We\ndemonstrate a synchronization regime at a stable oscillation frequency,\nresulting in uniform saturated gain across the array and uniform radiating\npower. Unlike conventional one-dimensional cavity resonances, the oscillation\nfrequency is independent of the array length. Our investigations further show\nthat when small-signal gain is non-uniformly distributed across the array, the\nsaturated gain results in having a uniform distribution at a gain value that\ngenerates an EPD. Experimental validation using the measured board confirmed\nthat the system saturates at an EPD, with a measured spectrum exhibiting very\nlow phase noise. This low noise allows for operation at a clean oscillation\nfrequency. Additionally, the measured uniform power across the array\ncorresponds to the simulation results. The proposed scheme can pave the way for\na new generation of high-power radiating arrays with distributed active\nelements.",
        "Let $\\gf_{p^n}$ denote the finite field containing $p^n$ elements, where $n$\nis a positive integer and $p$ is a prime. The function\n$f_u(x)=x^{\\frac{p^n+3}{2}}+ux^2$ over $\\gf_{p^n}[x]$ with\n$u\\in\\gf_{p^n}\\setminus\\{0,\\pm1\\}$ was recently studied by Budaghyan and Pal in\n\\cite{Budaghyan2024ArithmetizationorientedAP}, whose differential uniformity is\nat most $5$ when $p^n\\equiv3~(mod~4)$. In this paper, we study the differential\nuniformity and the differential spectrum of $f_u$ for $u=\\pm1$. We first give\nsome properties of the differential spectrum of any cryptographic function.\nMoreover, by solving some systems of equations over finite fields, we express\nthe differential spectrum of $f_{\\pm1}$ in terms of the quadratic character\nsums.",
        "The maintenance of primary mirror segment co-phasing is a critical aspect to\nthe operation of segmented telescopes. However, speckle-based measurements of\nthe phasing of the Keck primary have estimated semi-static surface aberrations\nof approximately 65 nm rms, which were not sensed by the current phasing\ncontrol system. We propose directly sensing and controlling the primary via the\nadaptive optics (AO) system, as a Controllable Segmented Primary (CSP), to\nactively correct its phasing. We develop a methodology for separating the\nindependent controllable signal of the CSP from the rest of the AO system and\nshow estimations of the achievable measurement precision from models of the\nKeck-II AO system."
      ]
    }
  },
  {
    "id":2411.18767,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer",
    "start_abstract":"Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction"
      ],
      "abstract":[
        "Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World\n  Multimodal Mathematical Error Detection",
        "Improved Rates of Differentially Private Nonconvex-Strongly-Concave\n  Minimax Optimization",
        "Online Learning of Danger Avoidance for Complex Structures of\n  Musculoskeletal Humanoids and Its Applications",
        "Orientation-dependent transport in junctions formed by $d$-wave\n  altermagnets and $d$-wave superconductors",
        "Assessing the impacts of tradable credit schemes through agent-based\n  simulation",
        "Mitigating Hallucinations in Multimodal Spatial Relations through\n  Constraint-Aware Prompting",
        "Quantum Entanglement and Measurement Noise: A Novel Approach to\n  Satellite Node Authentication",
        "CallNavi: A Study and Challenge on Function Calling Routing and\n  Invocation in Large Language Models",
        "Global existence of weak solutions to a cell migration and\n  (de)differentiation model with double haptotaxis in the context of tissue\n  regeneration",
        "An Energy-Aware RIoT System: Analysis, Modeling and Prediction in the\n  SUPERIOT Framework",
        "Decentralized Personalization for Federated Medical Image Segmentation\n  via Gossip Contrastive Mutual Learning",
        "Using cyclic $(f,\\sigma)$-codes over finite chain rings to construct\n  $\\mathbb{Z}_p$- and $\\mathbb{F}_q[\\![t]\\!]$-lattices",
        "MAPN: Enhancing Heterogeneous Sparse Graph Representation by Mamba-based\n  Asynchronous Aggregation",
        "Transparent Decompilation for Timing Side-Channel Analyses",
        "Utilizing API Response for Test Refinement",
        "ScNeuGM: Scalable Neural Graph Modeling for Coloring-Based Contention\n  and Interference Management in Wi-Fi 7",
        "Journalists Knowledge and Utilisation of Google Translate Application in\n  South East, Nigeria",
        "Cove-edged Chiral Graphene Nanoribbons with Chirality-Dependent Bandgap\n  and Carrier Mobility",
        "Interplay of $d$- and $p$-States in RbTi$_3$Bi$_5$ and CsTi$_3$Bi$_5$\n  Flat-Band Kagome Metals",
        "Stellar Ages: A Code to Infer Properties of Stellar Populations",
        "Federated Distributed Key Generation",
        "Zooming In On The Multi-Phase Structure of Magnetically-Dominated Quasar\n  Disks: Radiation From Torus to ISCO Across Accretion Rates",
        "Is there Kibble-Zurek scaling of topological defects in first-order\n  phase transitions?",
        "Symplectic Optimization for Cross Subcarrier Precoder Design with\n  Channel Smoothing in Massive MIMO-OFDM System",
        "Small noise limits of Markov chains and the PageRank",
        "De Sitter quantum gravity within the covariant Lorentzian approach to\n  asymptotic safety",
        "TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified\n  Flow Models",
        "NFTs as a Data-Rich Test Bed: Conspicuous Consumption and its\n  Determinants",
        "Fundamental Limits of Hierarchical Secure Aggregation with Cyclic User\n  Association"
      ],
      "abstract":[
        "Mathematical error detection in educational settings presents a significant\nchallenge for Multimodal Large Language Models (MLLMs), requiring a\nsophisticated understanding of both visual and textual mathematical content\nalong with complex reasoning capabilities. Though effective in mathematical\nproblem-solving, MLLMs often struggle with the nuanced task of identifying and\ncategorizing student errors in multimodal mathematical contexts. Therefore, we\nintroduce MathAgent, a novel Mixture-of-Math-Agent framework designed\nspecifically to address these challenges. Our approach decomposes error\ndetection into three phases, each handled by a specialized agent: an image-text\nconsistency validator, a visual semantic interpreter, and an integrative error\nanalyzer. This architecture enables more accurate processing of mathematical\ncontent by explicitly modeling relationships between multimodal problems and\nstudent solution steps. We evaluate MathAgent on real-world educational data,\ndemonstrating approximately 5% higher accuracy in error step identification and\n3% improvement in error categorization compared to baseline models. Besides,\nMathAgent has been successfully deployed in an educational platform that has\nserved over one million K-12 students, achieving nearly 90% student\nsatisfaction while generating significant cost savings by reducing manual error\ndetection.",
        "In this paper, we study the problem of (finite sum) minimax optimization in\nthe Differential Privacy (DP) model. Unlike most of the previous studies on the\n(strongly) convex-concave settings or loss functions satisfying the\nPolyak-Lojasiewicz condition, here we mainly focus on the\nnonconvex-strongly-concave one, which encapsulates many models in deep learning\nsuch as deep AUC maximization. Specifically, we first analyze a DP version of\nStochastic Gradient Descent Ascent (SGDA) and show that it is possible to get a\nDP estimator whose $l_2$-norm of the gradient for the empirical risk function\nis upper bounded by $\\tilde{O}(\\frac{d^{1\/4}}{({n\\epsilon})^{1\/2}})$, where $d$\nis the model dimension and $n$ is the sample size. We then propose a new method\nwith less gradient noise variance and improve the upper bound to\n$\\tilde{O}(\\frac{d^{1\/3}}{(n\\epsilon)^{2\/3}})$, which matches the best-known\nresult for DP Empirical Risk Minimization with non-convex loss. We also\ndiscussed several lower bounds of private minimax optimization. Finally,\nexperiments on AUC maximization, generative adversarial networks, and temporal\ndifference learning with real-world data support our theoretical analysis.",
        "The complex structure of musculoskeletal humanoids makes it difficult to\nmodel them, and the inter-body interference and high internal muscle force are\nunavoidable. Although various safety mechanisms have been developed to solve\nthis problem, it is important not only to deal with the dangers when they occur\nbut also to prevent them from happening. In this study, we propose a method to\nlearn a network outputting danger probability corresponding to the muscle\nlength online so that the robot can gradually prevent dangers from occurring.\nApplications of this network for control are also described. The method is\napplied to the musculoskeletal humanoid, Musashi, and its effectiveness is\nverified.",
        "We investigate de Gennes-Saint-James states and Josephson effect in hybrid\njunctions based on $d$-wave altermagnet and $d$-wave superconductor. Even\nthough these states are associated to long junctions, we find that the\n$d_{x^{2}-y^{2}}$-altermagnet in a normal metal\/altermagnet\/$d$-wave\nsuperconductor junction forms de Gennes-Saint-James states in a short junction\ndue to an enhanced mismatch between electron and hole wave vectors. As a\nresult, the zero-bias conductance peak vanishes and pronounced resonance spikes\nemerge in the subgap conductance spectra. By contrast, the $d_{xy}$-altermagnet\nonly features de Gennes-Saint-James states in the long junction. Moreover, the\nwell-known features such as V-shape conductance for $d_{x^2-y^2}$ pairings and\nzero-biased conductance peak for $d_{xy}$ pairings are not affected by the\nstrength of $d_{xy}$-altermagnetism in the short junction. We also study the\nJosephson current-phase relation $I(\\varphi)$ of $d$-wave\nsuperconductor\/altermagnet\/$d$-wave superconductor hybrids, where $\\varphi$ is\nthe macroscopic phase difference between two $d$-wave superconductors. In\nsymmetric junctions, we obtain anomalous current phase relation such as a\n$0$-$\\pi$ transition by changing either the orientation or the magnitude of the\naltermagnetic order parameter and dominant higher Josephson harmonics.\nInterestingly, we find the first-order Josephson coupling in an asymmetric\n$d_{x^{2}-y^{2}}$-superconductor\/altermagnet\/$d_{xy}$-superconductor junction\nwhen the symmetry of altermagnetic order parameter is neither\n$d_{x^{2}-y^{2}}$- nor $d_{xy}$-wave. We present the symmetry analysis and\nconclude that the anomalous orientation-dependent current-phase relations are\nascribed to the peculiar feature of the altermagnetic spin-splitting field.",
        "Tradable credit schemes (TCS) have been attracting interest from the\ntransportation research community as an appealing alternative to congestion\npricing, due to the advantages of revenue neutrality and equity. Nonetheless,\nexisting research has largely employed network and market equilibrium\napproaches with simplistic characterizations of transportation demand, supply,\ncredit market operations, and market behavior. Agent- and activity-based\nsimulation affords a natural means to comprehensively assess TCS by more\nrealistically modeling demand, supply, and individual market interactions. We\npropose an integrated simulation framework for modeling a TCS, and implements\nit within the state-of-the-art open-source urban simulation platform\nSimMobility, including: (a) a flexible TCS design that considers multiple trips\nand explicitly accounts for individual trading behaviors; (b) a simulation\nframework that captures the complex interactions between a TCS regulator, the\ntraveler, and the TCS market itself, with the flexibility to test future TCS\ndesigns and relevant mobility models; and (c) a set of simulation experiments\non a large mesoscopic multimodal network combined with a Bayesian Optimization\napproach for TCS optimal design. The experiment results indicate network and\nmarket performance to stabilize over the day-to-day process, showing the\nalignment of our agent-based simulation with the known theoretical properties\nof TCS. We confirm the efficiency of TCS in reducing congestion under the\nadopted market behavioral assumptions and open the door for simulating\ndifferent individual behaviors. We measure how TCS impacts differently the\nlocal network, heterogeneous users, the different travel behaviors, and how\ntesting different TCS designs can avoid negative market trading behaviors.",
        "Spatial relation hallucinations pose a persistent challenge in large\nvision-language models (LVLMs), leading to generate incorrect predictions about\nobject positions and spatial configurations within an image. To address this\nissue, we propose a constraint-aware prompting framework designed to reduce\nspatial relation hallucinations. Specifically, we introduce two types of\nconstraints: (1) bidirectional constraint, which ensures consistency in\npairwise object relations, and (2) transitivity constraint, which enforces\nrelational dependence across multiple objects. By incorporating these\nconstraints, LVLMs can produce more spatially coherent and consistent outputs.\nWe evaluate our method on three widely-used spatial relation datasets,\ndemonstrating performance improvements over existing approaches. Additionally,\na systematic analysis of various bidirectional relation analysis choices and\ntransitivity reference selections highlights greater possibilities of our\nmethods in incorporating constraints to mitigate spatial relation\nhallucinations.",
        "In this paper, we introduce a novel authentication scheme for satellite nodes\nbased on quantum entanglement and measurement noise profiles. Our approach\nleverages the unique noise characteristics exhibited by each satellite's\nquantum optical communication system to create a distinctive \"quantum noise\nfingerprint.\" This fingerprint is used for node authentication within a\nsatellite constellation, offering a quantum-safe alternative to traditional\ncryptographic methods. The proposed scheme consists of a training phase, where\neach satellite engages in a training exercise with its neighbors to compile\nnoise profiles, and an online authentication phase, where these profiles are\nused for real-time authentication. Our method addresses the inherent challenges\nof implementing cryptographic-based schemes in space, such as key management\nand distribution, by exploiting the fundamental properties of quantum mechanics\nand the unavoidable imperfections in quantum systems. This approach enhances\nthe security and reliability of satellite communication networks, providing a\nrobust solution to the authentication challenges in satellite constellations.\nWe validated and tested several hypotheses for this approach using IBM System\nOne quantum computers.",
        "Interacting with a software system via a chatbot can be challenging,\nespecially when the chatbot needs to generate API calls, in the right order and\nwith the right parameters, to communicate with the system. API calling in\nchatbot systems poses significant challenges, particularly in complex,\nmulti-step tasks requiring accurate API selection and execution. We contribute\nto this domain in three ways: first, by introducing a novel dataset designed to\nassess models on API function selection, parameter generation, and nested API\ncalls; second, by benchmarking state-of-the-art language models across varying\nlevels of complexity to evaluate their performance in API function generation\nand parameter accuracy; and third, by proposing an enhanced API routing method\nthat combines general-purpose large language models for API selection with\nfine-tuned models for parameter generation and some prompt engineering\napproach. These approaches lead to substantial improvements in handling complex\nAPI tasks, offering practical advancements for real-world API-driven chatbot\nsystems.",
        "We study a model for the spread and (de)differentiation of mesenchymal stem\ncells and chondrocytes in a scaffold whose fibers are coated with hyaluron. The\nchondrocytes produce new extracellular matrix, which, together with hyaluron,\nserves as haptotactic cue for the stem cell migration. We prove global\nexistence of weak solutions of the corresponding cross-diffusion system with\ndouble haptotaxis.",
        "This paper presents a comprehensive analysis of the energy consumption\ncharacteristics of a Silicon (Si)-based Reconfigurable IoT (RIoT) node\ndeveloped in the initial phase of the SUPERIOT project, focusing on key\noperating states, including Bluetooth Low Energy (BLE) communication,\nNarrow-Band Visible Light Communication (NBVLC), sensing, and E-ink display.\nExtensive measurements were conducted to establish a detailed energy profile,\nwhich serves as a benchmark for evaluating the effectiveness of subsequent\noptimizations and future node iterations. To minimize the energy consumption,\nmultiple optimizations were implemented at both the software and hardware\nlevels, achieving a reduction of over 60% in total energy usage through\nsoftware modifications alone. Further improvements were realized by optimizing\nthe E-ink display driving waveform and implementing a very low-power mode for\nnon-communication activities. Based on the measured data, three\nmeasurement-based energy consumption models were developed to characterize the\nenergy behavior of the node under: (i) normal, unoptimized operation, (ii)\nlow-power, software-optimized operation, and (iii) very low-power,\nhardware-optimized operation. These models, validated with new measurement\ndata, achieved an accuracy exceeding 97%, confirming their reliability for\npredicting energy consumption in diverse configurations.",
        "Federated Learning (FL) presents a promising avenue for collaborative model\ntraining among medical centers, facilitating knowledge exchange without\ncompromising data privacy. However, vanilla FL is prone to server failures and\nrarely achieves optimal performance on all participating sites due to\nheterogeneous data distributions among them. To overcome these challenges, we\npropose Gossip Contrastive Mutual Learning (GCML), a unified framework to\noptimize personalized models in a decentralized environment, where Gossip\nProtocol is employed for flexible and robust peer-to-peer communication. To\nmake efficient and reliable knowledge exchange in each communication without\nthe global knowledge across all the sites, we introduce deep contrast mutual\nlearning (DCML), a simple yet effective scheme to encourage knowledge transfer\nbetween the incoming and local models through collaborative training on local\ndata. By integrating DCML with other efforts to optimize site-specific models\nby leveraging useful information from peers, we evaluated the performance and\nefficiency of the proposed method on three publicly available datasets with\ndifferent segmentation tasks. Our extensive experimental results show that the\nproposed GCML framework outperformed both centralized and decentralized FL\nmethods with significantly reduced communication overhead, indicating its\npotential for real-world deployment.",
        "We construct $\\mathbb{Z}_p$-lattices and $\\mathbb{F}_q[\\![t]\\!]$-lattices\nfrom cyclic $(f,\\sigma)$-codes over finite chain rings, employing quotients of\nnatural nonassociative orders and principal left ideals in carefully chosen\nnonassociative algebras. This approach generalizes the classical Construction A\nthat obtains $\\mathbb{Z}$-lattices from linear codes over finite fields or\ncommutative rings to the nonassociative setting. We mostly use proper\nnonassociative cyclic algebras that are defined over field extensions of\n$p$-adic fields. This means we focus on $\\sigma$-constacyclic codes to obtain\n$\\mathbb{Z}_p$-lattices, hence $\\mathbb{Z}_p$-lattice codes. We construct\nlinear maximum rank distance (MRD) codes that are $\\mathbb{Z}_p$-lattice codes\nemploying the left multiplication of a nonassociative algebra over a finite\nchain ring.\n  Possible applications of our constructions include post-quantum cryptography\ninvolving $p$-adic lattices, e.g. learning with errors, building rank-metric\ncodes like MRD-codes, or $p$-adic coset coding, in particular wire-tap coding.",
        "Graph neural networks (GNNs) have become the state of the art for various\ngraph-related tasks and are particularly prominent in heterogeneous graphs\n(HetGs). However, several issues plague this paradigm: first, the difficulty in\nfully utilizing long-range information, known as over-squashing; second, the\ntendency for excessive message-passing layers to produce indistinguishable\nrepresentations, referred to as over-smoothing; and finally, the inadequacy of\nconventional MPNNs to train effectively on large sparse graphs. To address\nthese challenges in deep neural networks for large-scale heterogeneous graphs,\nthis paper introduces the Mamba-based Asynchronous Propagation Network (MAPN),\nwhich enhances the representation of heterogeneous sparse graphs. MAPN consists\nof two primary components: node sequence generation and semantic information\naggregation. Node sequences are initially generated based on meta-paths through\nrandom walks, which serve as the foundation for a spatial state model that\nextracts essential information from nodes at various distances. It then\nasynchronously aggregates semantic information across multiple hops and layers,\neffectively preserving unique node characteristics and mitigating issues\nrelated to deep network degradation. Extensive experiments across diverse\ndatasets demonstrate the effectiveness of MAPN in graph embeddings for various\ndownstream tasks underscoring its substantial benefits for graph representation\nin large sparse heterogeneous graphs.",
        "This paper considers the problem of analyzing the timing side-channel\nsecurity of binary programs through decompilation and source-level analysis. We\nfocus on two popular policies, namely constant-time and speculative\nconstant-time, (S)CT for short, used to protect cryptographic libraries.\n  First, we observe that popular decompilers remove (S)CT violations, i.e.,\ntransform non-(S)CT programs into (S)CT programs; it follows that analyzing\ndecompiled programs is not sound. Second, we develop techniques to prove that\ndecompilers are transparent, i.e., neither introduce nor remove (S)CT\nviolations. Third, we apply our techniques to \\refleCT{}, a core but\nnon-trivial decompiler. As a contribution of independent interest, we find that\nconstant-time verification tools may not be sound, due to their use of\npreprocessors (e.g.\\, binary lifters or IR converters) that eliminate CT\nviolations.",
        "Most of the web services are offered in the form of RESTful APIs. This has\nled to an active research interest in API testing to ensure the reliability of\nthese services. While most of the testing techniques proposed in the past rely\non the API specification to generate the test cases, a major limitation of such\nan approach is that in the case of an incomplete or inconsistent specification,\nthe test cases may not be realistic in nature and would result in a lot of 4xx\nresponse due to invalid input. This is indicative of poor test quality.\nLearning-based approaches may learn about valid inputs but often require a\nlarge number of request-response pairs to learn the constraints, making it\ninfeasible to be readily used in the industry. To address this limitation, this\npaper proposes a dynamic test refinement approach that leverages the response\nmessage. The response is used to infer the point in the API testing flow where\na test scenario fix is required. Using an intelligent agent, the approach adds\nconstraints to the API specification that are further used to generate a test\nscenario accounting for the learned constraint from the response. Following a\ngreedy approach, the iterative learning and refinement of test scenarios are\nobtained from the API testing system. The proposed approach led to a decrease\nin the number of 4xx responses, taking a step closer to generating more\nrealistic test cases with high coverage that would aid in functional testing. A\nhigh coverage was obtained from a lesser number of API requests, as compared\nwith the state-of-the-art search-based API Testing tools.",
        "Carrier-sense multiple access with collision avoidance in Wi-Fi often leads\nto contention and interference, thereby increasing packet losses. These\nchallenges have traditionally been modeled as a graph, with stations (STAs)\nrepresented as vertices and contention or interference as edges. Graph coloring\nassigns orthogonal transmission slots to STAs, managing contention and\ninterference, e.g., using the restricted target wake time (RTWT) mechanism\nintroduced in Wi-Fi 7 standards. However, legacy graph models lack flexibility\nin optimizing these assignments, often failing to minimize slot usage while\nmaintaining reliable transmissions. To address this issue, we propose ScNeuGM,\na neural graph modeling (NGM) framework that flexibly trains a neural network\n(NN) to construct optimal graph models whose coloring corresponds to optimal\nslot assignments. ScNeuGM is highly scalable to large Wi-Fi networks with\nmassive STA pairs: 1) it utilizes an evolution strategy (ES) to directly\noptimize the NN parameters based on one network-wise reward signal, avoiding\nexhaustive edge-wise feedback estimations in all STA pairs; 2) ScNeuGM also\nleverages a deep hashing function (DHF) to group contending or interfering STA\npairs and restricts NGM NN training and inference to pairs within these groups,\nsignificantly reducing complexity. Simulations show that the ES-trained NN in\nScNeuGM returns near-optimal graphs 4-10 times more often than algorithms\nrequiring edge-wise feedback and reduces 25\\% slots than legacy graph\nconstructions. Furthermore, the DHF in ScNeuGM reduces the training and the\ninference time of NGM by 4 and 8 times, respectively, and the online slot\nassignment time by 3 times in large networks, and up to 30\\% fewer packet\nlosses in dynamic scenarios due to the timely assignments.",
        "This study was aimed at finding out if journalists in South East Nigeria have\nknowledge of Google Translate Application and also utilise it. It adopted a\nsurvey design with a sample size of 320 which was determined using Krejcie &\nMorgan (1970). Its objectives were to ascertain the extent journalists in South\nEast Nigeria know about Google Translate Application, assess the utilisation of\nGoogle Translate Application among journalists in South East Nigeria, and\nidentify the challenges affecting the journalists in South East Nigeria while\nusing Google Translate Application. The theoretical underpin was Knowledge\nAttitude and Practise Model (KAP). The findings showed that journalists in\nSouth East Nigeria have knowledge of Google Translate Application but apply it\nmostly outside the region. It concludes that journalists in South East Nigeria\nhave the knowledge of the App. but apply it outside the zone. The study\nrecommends increased usage of the App. within South East Nigeria.",
        "Graphene nanoribbons (GNRs) have garnered significant interest due to their\nhighly customizable physicochemical properties and potential utility in\nnanoelectronics. Besides controlling widths and edge structures, the inclusion\nof chirality in GNRs brings another dimension for fine-tuning their\noptoelectronic properties, but related studies remain elusive owing to the\nabsence of feasible synthetic strategies. Here, we demonstrate a novel class of\ncove-edged chiral GNRs (CcGNRs) with a tunable chiral vector (n,m). Notably,\nthe bandgap and effective mass of (n,2)- CcGNR show a distinct positive\ncorrelation with the increasing value of n, as indicated by theory. Within this\nGNR family, two representative members, namely, (4,2)- CcGNR and (6,2)-CcGNR,\nare successfully synthesized. Both CcGNRs exhibit prominently curved geometries\narising from the incorporated [4]helicene motifs along their peripheries, as\nalso evidenced by the single-crystal structures of the two respective model\ncompounds (1 and 2). The chemical identities and optoelectronic properties of\n(4,2)- and (6,2)-CcGNRs are comprehensively investigated via a combination of\nIR, Raman, solid-state NMR, UV-vis, and THz spectroscopies as well as\ntheoretical calculations. In line with theoretical expectation, the obtained\n(6,2)-CcGNR possesses a low optical bandgap of 1.37 eV along with charge\ncarrier mobility of 8 cm2\/Vs, whereas (4,2)-CcGNR exhibits a narrower bandgap\nof 1.26 eV with increased mobility of 14 cm2\/Vs. This work opens up a new\navenue to precisely engineer the bandgap and carrier mobility of GNRs by\nmanipulating their chiral vector.",
        "Shifting the Fermi level of the celebrated $AM_3X_5$ (135) compounds into\nproximity of flat bands strongly enhances electronic correlations and severely\naffects the formation of density waves and superconductivity. Our broadband\ninfrared spectroscopy measurements of RbTi$_3$Bi$_5$ and CsTi$_3$Bi$_5$\ncombined with density-functional band-structure calculations reveal that the\ncorrelated Ti $d$-states are intricately coupled with the Bi $p$-states that\nform a tilted Dirac crossing. Electron-phonon coupling manifests itself in the\nstrong damping of itinerant carriers and in the anomalous shape of the phonon\nline in RbTi$_3$Bi$_5$. An anomaly in these spectral features around 150 K can\nbe paralleled to the onset of nematicity detected by low-temperature probes.\nOur findings show that the materials with low band filling open unexplored\ndirections in the physics of kagome metals and involve electronic states of\ndifferent nature strongly coupled with lattice dynamics.",
        "We present a novel statistical algorithm, Stellar Ages, which currently\ninfers the age, metallicity, and extinction posterior distributions of stellar\npopulations from their magnitudes. While this paper focuses on these\nparameters, the framework is readily adaptable to include additional\nproperties, such as rotation, in future work. Historical age-dating techniques\neither model individual stars or populations of stars, often sacrificing\npopulation context or precision for individual estimates. Stellar Ages does\nboth, combining the strengths of these approaches to provide precise individual\nages for stars while leveraging population-level constraints. We verify the\nalgorithm's capabilities by determining the age of synthetic stellar\npopulations and actual stellar populations surrounding a nearby supernova, SN\n2004dj. In addition to inferring an age, we infer a progenitor mass consistent\nwith direct observations of the precursor star. The median age inferred from\nthe brightest nearby stars is $\\log_{10}$(Age\/yr) = $7.19^{+0.10}_{-0.13}$, and\nits corresponding progenitor mass is $13.95^{+3.33}_{-1.96}$\n$\\text{M}_{\\odot}$.",
        "Distributed Key Generation (DKG) is vital to threshold-based cryptographic\nprotocols such as threshold signatures, secure multiparty computation, and\ni-voting. Yet, standard $(n,t)$-DKG requires a known set of $n$ participants\nand a fixed threshold $t$, making it impractical for public or decentralized\nsettings where membership and availability can change.\n  We introduce Federated Distributed Key Generation (FDKG), which relaxes these\nconstraints by allowing each participant to select its own guardian set, with a\nlocal threshold to reconstruct that participant's partial key. FDKG generalizes\nDKG and draws inspiration from Federated Byzantine Agreement, enabling dynamic\ntrust delegation with minimal message complexity (two rounds). The protocol's\nliveness can tolerate adversary that controls up to $k - t + 1$ nodes in every\nguardian set. The paper presents a detailed protocol, a formal description of\nliveness, privacy, and integrity properties, and a simulation-based evaluation\nshowcasing the efficacy of FDKG in mitigating node unreliability.\n  In a setting of 100 parties, a 50% participation rate, 80% retention, and 40\nguardians, the distribution phase incurred a total message size of 332.7 kB\n($O(n\\,k)$), and reconstruction phase 416.56 kB ($O(n\\,k)$. Groth16 client-side\nproving took about 5 s in the distribution phase and ranged from 0.619 s up to\n29.619 s in the reconstruction phase.\n  Our work advances distributed cryptography by enabling flexible trust models\nfor dynamic networks, with applications ranging from ad-hoc collaboration to\nblockchain governance.",
        "Recent radiation-thermochemical-magnetohydrodynamic simulations resolved\nformation of quasar accretion disks from cosmological scales down to ~300\ngravitational radii $R_{g}$, arguing they were 'hyper-magnetized' (plasma\n$\\beta\\ll1$ supported by toroidal magnetic fields) and distinct from\ntraditional $\\alpha$-disks. We extend these, refining to $\\approx 3\\,R_{g}$\naround a $10^{7}\\,{\\rm M_{\\odot}}$ BH with multi-channel radiation and\nthermochemistry, and exploring a factor of 1000 range of accretion rates\n($\\dot{m}\\sim0.01-20$). At smaller scales, we see the disks maintain steady\naccretion, thermalize and self-ionize, and radiation pressure grows in\nimportance, but large deviations from local thermodynamic equilibrium and\nsingle-phase equations of state are always present. Trans-Alfvenic and\nhighly-supersonic turbulence persists in all cases, and leads to efficient\nvertical mixing, so radiation pressure saturates at levels comparable to\nfluctuating magnetic and turbulent pressures even for $\\dot{m}\\gg1$. The disks\nalso become radiatively inefficient in the inner regions at high $\\dot{m}$. The\nmidplane magnetic field remains primarily toroidal at large radii, but at\nsuper-Eddington $\\dot{m}$ we see occasional transitions to a poloidal-field\ndominated state associated with outflows and flares. Large-scale\nmagnetocentrifugal and continuum radiation-pressure-driven outflows are weak at\n$\\dot{m}<1$, but can be strong at $\\dot{m}\\gtrsim1$. In all cases there is a\nscattering photosphere above the disk extending to $\\gtrsim 1000\\,R_{g}$ at\nlarge $\\dot{m}$, and the disk is thick and flared owing to magnetic support\n(with $H\/R$ nearly independent of $\\dot{m}$), so the outer disk is strongly\nilluminated by the inner disk and most of the inner disk continuum scatters or\nis reprocessed at larger scales, giving apparent emission region sizes as large\nas $\\gtrsim 10^{16}\\,{\\rm cm}$.",
        "Kibble-Zurek scaling is the scaling of the density of the topological defects\nformed via the Kibble-Zurek mechanism with respect to the rate at which a\nsystem is cooled across a continuous phase transition. Recently, the density of\nthe topological defects formed via the Kibble-Zurek mechanism was computed for\na system cooled through a first-order phase transition instead of the usual\ncontinuous transitions. Here we address the problem of whether such defects\ngenerated across a first-order phase transition exhibit Kibble-Zurek scaling\nsimilar to the case in continuous phase transitions. We show that any possible\nKibble-Zurek scaling for the topological defects can only be a very rough\napproximation due to an intrinsic field for the scaling. However, complete\nuniversal scaling for other properties does exist.",
        "In this paper, we propose a cross subcarrier precoder design (CSPD) for\nmassive multiple-input multiple-output (MIMO) orthogonal frequency division\nmultiplexing (OFDM) systems. The aim is to maximize the weighted sum-rate (WSR)\nperformance while considering the smoothness of the frequency domain effective\nchannel. To quantify the smoothness of the effective channel, we introduce a\ndelay indicator function to measure the large delay components of the effective\nchannel. An optimization problem is then formulated to balance the WSR\nperformance and the delay indicator function. By appropriately selecting the\nweight factors in the objective function and the parameters in the delay\nindicator function, the delay spread of the effective channel can be reduced,\nthereby enhancing the smoothness of the effective channel. To solve the\noptimization problem, we apply the symplectic optimization, which achieves\nfaster convergence compared to the gradient descent methods. Simulation results\nindicate that the proposed algorithm achieves satisfying WSR performance while\nmaintaining the smoothness of the effective channel.",
        "We recall the classical formulation of PageRank as the stationary\ndistribution of a singularly perturbed irreducible Markov chain that is not\nirreducible when the perturbation parameter goes to zero. Specifically, we use\nthe Markov chain tree theorem to derive explicit expressions for the PageRank.\nThis analysis leads to some surprising results. These results are then extended\nto a much more general class of perturbations that subsume personalized\nPageRank. We also give examples where even simpler formulas for PageRank are\npossible.",
        "Recent technical and conceptual advancements in the asymptotic safety\napproach to quantum gravity have enabled studies of the UV completion of\nLorentzian Einstein gravity, emphasizing the role of the state dependence. We\npresent here the first complete investigation of the flow equations of the\nEinstein-Hilbert action within a cosmological spacetime, namely de Sitter\nspacetime. Using the newly derived graviton propagator for general gauges and\nmasses in de Sitter spacetime, we analyze the dependence on the gauge and on\nfinite renormalization parameters. Our results provide evidence of a UV fixed\npoint for the most commonly used gauges.",
        "Recent advancements in diffusion techniques have propelled image and video\ngeneration to unprecedented levels of quality, significantly accelerating the\ndeployment and application of generative AI. However, 3D shape generation\ntechnology has so far lagged behind, constrained by limitations in 3D data\nscale, complexity of 3D data processing, and insufficient exploration of\nadvanced techniques in the 3D domain. Current approaches to 3D shape generation\nface substantial challenges in terms of output quality, generalization\ncapability, and alignment with input conditions. We present TripoSG, a new\nstreamlined shape diffusion paradigm capable of generating high-fidelity 3D\nmeshes with precise correspondence to input images. Specifically, we propose:\n1) A large-scale rectified flow transformer for 3D shape generation, achieving\nstate-of-the-art fidelity through training on extensive, high-quality data. 2)\nA hybrid supervised training strategy combining SDF, normal, and eikonal losses\nfor 3D VAE, achieving high-quality 3D reconstruction performance. 3) A data\nprocessing pipeline to generate 2 million high-quality 3D samples, highlighting\nthe crucial rules for data quality and quantity in training 3D generative\nmodels. Through comprehensive experiments, we have validated the effectiveness\nof each component in our new framework. The seamless integration of these parts\nhas enabled TripoSG to achieve state-of-the-art performance in 3D shape\ngeneration. The resulting 3D shapes exhibit enhanced detail due to\nhigh-resolution capabilities and demonstrate exceptional fidelity to input\nimages. Moreover, TripoSG demonstrates improved versatility in generating 3D\nmodels from diverse image styles and contents, showcasing strong generalization\ncapabilities. To foster progress and innovation in the field of 3D generation,\nwe will make our model publicly available.",
        "Conspicuous consumption occurs when a consumer derives value from a good\nbased on its social meaning as a signal of wealth, taste, and\/or community\naffiliation. Common conspicuous goods include designer footwear, country club\nmemberships, and artwork; conspicuous goods also exist in the digital sphere,\nwith non-fungible tokens (NFTs) as a prominent example. The NFT market merits\ndeeper study for two key reasons: first, it is poorly understood relative to\nits economic scale; and second, it is unusually amenable to analysis because\nNFT transactions are publicly available on the blockchain, making them useful\nas a test bed for conspicuous consumption dynamics. This paper introduces a\nmodel that incorporates two previously identified elements of conspicuous\nconsumption: the \\emph{bandwagon effect} (goods increase in value as they\nbecome more popular) and the \\emph{snob effect} (goods increase in value as\nthey become rarer). Our model resolves the apparent tension between these two\neffects, exhibiting net complementarity between others' and one's own\nconspicuous consumption. We also introduce a novel dataset combining NFT\ntransactions with embeddings of the corresponding NFT images computed using an\noff-the-shelf vision transformer architecture. We use our dataset to validate\nthe model, showing that the bandwagon effect raises an NFT collection's value\nas more consumers join, while the snob effect drives consumers to seek rarer\nNFTs within a given collection.",
        "Secure aggregation is motivated by federated learning (FL) where a cloud\nserver aims to compute an averaged model (i.e., weights of deep neural\nnetworks) of the locally-trained models of numerous clients, while adhering to\ndata security requirements. Hierarchical secure aggregation (HSA) extends this\nconcept to a three-layer network, where clustered users communicate with the\nserver through an intermediate layer of relays. In HSA, beyond conventional\nserver security, relay security is also enforced to ensure that the relays\nremain oblivious to the users' inputs (an abstraction of the local models in\nFL). Existing study on HSA assumes that each user is associated with only one\nrelay, limiting opportunities for coding across inter-cluster users to achieve\nefficient communication and key generation. In this paper, we consider HSA with\na cyclic association pattern where each user is connected to $B$ consecutive\nrelays in a wrap-around manner. We propose an efficient aggregation scheme\nwhich includes a message design for the inputs inspired by gradient coding-a\nwell-known technique for efficient communication in distributed computing-along\nwith a highly nontrivial security key design. We also derive novel converse\nbounds on the minimum achievable communication and key rates using\ninformation-theoretic arguments."
      ]
    }
  },
  {
    "id":2411.10004,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Artificial Intelligence for Pediatric Ophthalmology",
    "start_abstract":"PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm"
      ],
      "abstract":[
        "In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Solutions of first passage times problems: a biscaling approach",
        "Homeostatic Kinematic Growth Model for Arteries -- Residual Stresses and\n  Active Response",
        "Incident beam optics optimization for the single crystal neutron\n  diffractometer Pioneer with a polarized beam option",
        "Enhanced Field-Free Perpendicular Magnetization Switching via spin\n  splitting torque in Altermagnetic RuO2-based Heterostructures",
        "Electromagnetic Radiation from High-Energy Nuclear Collisions",
        "First X-ray polarimetric view of a Low-Luminosity Active Galactic\n  Nucleus: the case of NGC 2110",
        "Classifier Weighted Mixture models",
        "Formation of condensations for non-radial solutions to 3-wave kinetic\n  equations",
        "Shifting Attention to You: Personalized Brain-Inspired AI Models",
        "Can wormholes mirror the quasi-normal mode spectrum of Schwarzschild\n  black holes?",
        "The influence of the Hardy potential and a Convection Term on a\n  Nonlinear Degenerate Elliptic Equations",
        "Transit Timing Variations of the Sub-Saturn Exoplanet HAT-P-12b",
        "Nuclear matter in relativistic Brueckner-Hartree-Fock theory with local\n  and nonlocal covariant chiral interactions at leading order",
        "Bayesian Computation in Deep Learning",
        "Anomalies in the electronic, magnetic and thermal behavior near the\n  Invar compositions of Fe-Ni alloys",
        "Copula methods for modeling pair densities in density functional theory",
        "Visualizing beat phenomenon between two amplitude-modulated (AM) light\n  beams on a solar cell using smartphones",
        "Proportion of Nilpotent Subgroups in Finite Groups and Their Properties",
        "Striped Spin Density Wave in a Graphene\/Black Phosphorous\n  Heterostructure",
        "Breast Lump Detection and Localization with a Tactile Glove Using Deep\n  Learning",
        "Sampling Theory for Function Approximation with Numerical Redundancy",
        "The role of finite value of strange quark mass $(m_{s}\\neq0)$ and baryon\n  number density $(n)$ on the stability and maximum mass of strange stars",
        "A classification of van der Waerden complexes with linear resolution",
        "An assessment of observational coverage and gaps for robust Sun to\n  heliosphere integrated science",
        "Intermediate band analysis in Green's functions calculations of\n  quasiparticle interference",
        "Kinematically-enhanced interpolating operators for boosted hadrons",
        "Secondary ionisation in hot atmospheres and interactions between\n  planetary and stellar winds",
        "Systematic calculation on alpha decay and cluster radioactivity of\n  superheavy nuclei",
        "Processing the 2D and 3D Fresnel experimental databases via topological\n  derivative methods"
      ],
      "abstract":[
        "We study the first-passage time (FPT) problem for widespread recurrent\nprocesses in confined though large systems and present a comprehensive\nframework for characterizing the FPT distribution over many time scales. We\nfind that the FPT statistics can be described by two scaling functions: one\ncorresponds to the solution for an infinite system, and the other describes a\nscaling that depends on system size. We find a universal scaling relationship\nfor the FPT moments $\\langle t^q \\rangle$ with respect to the domain size and\nthe source-target distance. This scaling exhibits a transition at $q_c=\\theta$,\nwhere $\\theta$ is the persistence exponent. For low-order moments with $q<q_c$,\nconvergence occurs towards the moments of an infinite system. In contrast, the\nhigh-order moments, $q>q_c$, can be derived from an infinite density function.\nThe presented uniform approximation, connecting the two scaling functions,\nprovides a description of the first-passage time statistics across all time\nscales. We extend the results to include diffusion in a confining potential in\nthe high-temperature limit, where the potential strength takes the place of the\nsystem's size as the relevant scale. This study has been applied to various\nmediums, including a particle in a box, two-dimensional wedge, fractal\ngeometries, non-Markovian processes and the non-equilibrium process of\nresetting.",
        "A simple kinematic growth model for muscular arteries is presented which\nallows the incorporation of residual stresses such that a homeostatic in-vivo\nstress state under physiological loading is obtained. To this end, new\nevolution equations for growth are proposed, which avoid issues with\ninstability of final growth states known from other kinematric growth models.\nThese evolution equations are connected to a new set of growth driving forces.\nBy introducing a formulation using the principle Cauchy stresses, reasonable in\nvivo stress distributions can be obtained while ensuring realistic geometries\nof arteries after growth. To incorporate realistic Cauchy stresses for muscular\narteries under varying loading conditions, which appear in vivo, e.g., due to\nphysical activity, the growth model is combined with a newly proposed\nstretch-dependent model for smooth muscle activation. To account for the\nchanges of geometry and mechanical behavior during the growth process, an\noptimization procedure is described which leads to a more accurate\nrepresentation of an arterial ring and its mechanical behavior after the\ngrowth-related homogenization of the stresses is reached. Based thereon,\nparameters are adjusted to experimental data of a healthy middle cerebral\nartery of a rat showing that the proposed model accurately describes real\nmaterial behavior. The successful combination of growth and active response\nindicates that the new growth model can be used without problems for modeling\nactive tissues under various conditions.",
        "Pioneer, a next-generation single-crystal neutron diffractometer, is under\ndevelopment for Oak Ridge National Laboratory's Second Target Station (STS).\nDesigned to address a wide range of scientific questions, Pioneer will deliver\nhomogeneous neutron beams with customizable size and divergence, and provide a\npolarized beam option. This article introduces its incident beam optics,\nhighlighting the optimization methodology and the simulated performance.\nPioneer will utilize a modified elliptical-straight guide for neutron transport\nand deploy slit packages and insertable apertures to control beam size and\ndivergence. The optimized guide geometry matches the\noptimal-and-full-sample-illumination condition, and the beam control system\neffectively filters out unwanted neutrons while preserving the desired ones.\nAdditionally, we have found that polygon-approximated guides provide\nsatisfactory transport efficiency and beam homogeneity, eliminating the need of\ntruly curved guides. To enhance neutronics performance and reduce cost, the\ncoatings of supermirror elements are individually optimized to the lowest\nhalf-integer $m$-values that are sufficient to deliver the desired neutrons.\nAfter evaluating polarizing V-cavities and $^3$He spin filters over the default\npolarized wavelength band of 1.2-5.5~\\AA, we selected a translatable\nmultichannel polarizing V-cavity as the incident beam polarizer. Strategically\nplaced at a location where the beam divergence is low and a large in-guide gap\nhas negligible impact on transport efficiency, the optimized V-cavity achieves\nan average $P^2T$ of approximately 35\\%.",
        "Current-induced spin-orbit torque (SOT) has emerged as a promising method for\nachieving energy-efficient magnetization switching in advanced spintronic\ndevices. However, technological advancement has been inadequate because an\nexternal in-plane magnetic field is required to attain deterministic switching.\nSeveral approaches have been explored to address these challenges. In this\nwork, we explored the potential of a newly emerged altermagnetic material RuO2\nin combination with a Pt layer to achieve both field-free and low-power\nswitching concurrently. We leveraged out-of-plane (OOP) spin polarization via\nthe spin-splitter effect (SSE) in RuO2 for field-free switching (FFS) and\nin-plane spin polarization combined with spin Hall effect (SHE) in Pt for\nenhanced SOT efficiency. We revealed that the effective OOP magnetic field and\nFFS can be maximized by tuning the nominal thickness of the Pt underlayer and\nthe direction of the applied current. We observed a significant enhancement in\nFFS at an optimized Pt thickness of 1.5 nm for an applied current density as\nlow as 2.56e11 A\/m2 at a crystal angle of 90 deg. Our study paves the way for\nenergy-efficient spintronics devices for non-volatile memory, logic circuits,\nand neuromorphic computing.",
        "We highlight some of the developments in the theory and the observation of\nthe electromagnetic radiation, thermal and otherwise, emitted in relativistic\nheavy-ion collisions.",
        "Low-Luminosity Active Galactic Nuclei (LLAGN) provides a unique view of\nComptonization and non-thermal emission from accreting black holes in the\nlow-accretion rate regime. However, to decipher the exact nature of the\nComptonizing corona in LLAGN, its geometry and emission mechanism must be\nunderstood beyond the limits of spectro-timing techniques. Spectro-polarimetry\noffers the potential to break the degeneracies between different coronal\nemission models. Compton-thin LLAGN provide an opportunity for such\nspectro-polarimetric exploration in the 2-8 keV energy range using IXPE. In\nthis work, we carry out a spectro-polarimetric analysis of the first IXPE\nobservation, in synergy with a contemporaneous NuSTAR observation, of an LLAGN:\nNGC 2110. Using 554.4 ks of IXPE data from October 2024, we constrain the 99%\nupper limit on the Polarization Degree (PD) to be less than 8.3% assuming the\ncorresponding Polarization Angle (PA) to be aligned with the radio jet, and\nless than 3.6% if in the perpendicular direction. In the absence of a\nsignificant PD detection, the PA remains formally unconstrained, yet the\npolarization significance contours appear to be aligned with the radio jet,\ntentatively supporting models in which the corona is radially extended in the\nplane of the disk. We also carry out detailed Monte Carlo simulations using\nMONK and STOKES codes to test different coronal models against our results and\ncompare the polarization properties between NGC 2110 and brighter Seyferts.",
        "This paper proposes an extension of standard mixture stochastic models, by\nreplacing the constant mixture weights with functional weights defined using a\nclassifier. Classifier Weighted Mixtures enable straightforward density\nevaluation, explicit sampling, and enhanced expressivity in variational\nestimation problems, without increasing the number of components nor the\ncomplexity of the mixture components.",
        "We consider in this work a $2$-dimensional $3$-wave kinetic equation\ndescribing the dynamics of the thermal cloud outside a Bose-Einstein\nCondensate. We construct global non-radial mild solutions for the equation.\nThose mild solutions are the summation of Dirac masses on circles. We prove\nthat in each spatial direction, either Dirac masses at the origin, which are\nthe so-called Bose-Einstein condensates, can be formed in finite time or the\nsolutions converge to Bose-Einstein condensates as time evolves to infinity. We\nalso describe a dynamics of the formation of the Bose-Einstein condensates\nlatter case. In this case, on each direction, the solutions accumulate around\ncircles close to the origin at growth rates at least linearly in time.",
        "The integration of human and artificial intelligence represents a scientific\nopportunity to advance our understanding of information processing, as each\nsystem offers unique computational insights that can enhance and inform the\nother. The synthesis of human cognitive principles with artificial intelligence\nhas the potential to produce more interpretable and functionally aligned\ncomputational models, while simultaneously providing a formal framework for\ninvestigating the neural mechanisms underlying perception, learning, and\ndecision-making through systematic model comparisons and representational\nanalyses. In this study, we introduce personalized brain-inspired modeling that\nintegrates human behavioral embeddings and neural data to align with cognitive\nprocesses. We took a stepwise approach, fine-tuning the Contrastive\nLanguage-Image Pre-training (CLIP) model with large-scale behavioral decisions,\ngroup-level neural data, and finally, participant-level neural data within a\nbroader framework that we have named CLIP-Human-Based Analysis (CLIP-HBA). We\nfound that fine-tuning on behavioral data enhances its ability to predict human\nsimilarity judgments while indirectly aligning it with dynamic representations\ncaptured via MEG. To further gain mechanistic insights into the temporal\nevolution of cognitive processes, we introduced a model specifically fine-tuned\non millisecond-level MEG neural dynamics (CLIP-HBA-MEG). This model resulted in\nenhanced temporal alignment with human neural processing while still showing\nimprovement on behavioral alignment. Finally, we trained individualized models\non participant-specific neural data, effectively capturing individualized\nneural dynamics and highlighting the potential for personalized AI systems.\nThese personalized systems have far-reaching implications for the fields of\nmedicine, cognitive research, human-computer interfaces, and AI development.",
        "Wormholes are exotic compact objects characterized by the absence of\nessential singularities and horizons, acting as slender bridges linking two\ndistinct regions of spacetime. Despite their theoretical significance, they\nremain however undetected, possibly due to their ability to closely mimic the\nobservational properties of black holes. This study explores whether a static\nand spherically symmetric wormhole within General Relativity can reproduce the\nquasi-normal mode spectrum of a Schwarzschild black hole under scalar,\nelectromagnetic, and axial gravitational perturbations, both individually and\nin combination. To address this, we reformulate the wormhole metric components\nusing a near-throat parametrization. Our analysis concentrates on the\nfundamental mode and first overtone, estimated via the\nWentzel-Kramers-Brillouin method. By employing a customized minimization\nstrategy, we demonstrate that within a specific region of the parameter space,\na wormhole can successfully replicate a subset of the black hole quasi-normal\nmode spectrum.",
        "This paper is devoted to prove existence of renormalized solutions for a\nclass of non--linear degenerate elliptic equations involving a non--linear\nconvection term, which satisfies a growth properties, and a Hardy potential.\nAdditionally, we assume that the right-hand side is an $L^m$ function, with\n$m\\geq 1$.",
        "We present Transit Timing Variations (TTVs) of HAT-P-12b, a low-density\nsub-Saturn mass planet orbiting a metal-poor K4 dwarf star. Using 14 years of\nobservational data (2009-2022), our study incorporates 7 new ground-based\nphotometric transit observations, three sectors of Transiting Exoplanet Survey\nSatellite (TESS) data, and 23 previously published light curves. A total of 46\nlight curves were analyzed using various analytical models, such as linear,\norbital decay, apsidal precession, and sinusoidal models to investigate the\npresence of additional planets. The stellar tidal quality factor ($Q_\\star'\n\\sim$ 28.4) is lower than the theoretical predictions, making the orbital decay\nmodel an unlikely explanation. The apsidal precession model with a $\\chi_r^2$\nof 4.2 revealed a slight orbital eccentricity (e = 0.0013) and a precession\nrate of 0.0045 rad\/epoch. Frequency analysis using the Generalized Lomb-Scargle\n(GLS) periodogram identified a significant periodic signal at 0.00415\ncycles\/day (FAP = 5.1$\\times$10$^{-6}$ %), suggesting the influence of an\nadditional planetary companion. The sinusoidal model provides the lowest\nreduced chi-squared value ($\\chi_r^2$) of 3.2. Sinusoidal fitting of the timing\nresiduals estimated this companion to have a mass of approximately 0.02 $M_J$ ,\nassuming it is in a 2:1 Mean-Motion Resonance (MMR) with HAT-P-12b.\nAdditionally, the Applegate mechanism, with an amplitude much smaller than the\nobserved TTV amplitude of 156 s, confirms that stellar activity is not\nresponsible for the observed variations.",
        "The simultaneous description for nuclear matter and finite nuclei has been a\nlong-standing challenge in nuclear ab initio theory. With the success for\nnuclear matter, the relativistic Brueckner-Hartree-Fock (RBHF) theory with\ncovariant chiral interactions is a promising ab initio approach to describe\nboth nuclear matter and finite nuclei. In the description of the finite nuclei\nwith the current RBHF theory, the covariant chiral interactions have to be\nlocalized to make calculations feasible. In order to examine the reliability\nand validity, in this letter, the RBHF theory with local and nonlocal covariant\nchiral interactions at leading order are applied for nuclear matter. The\nlow-energy constants in the covariant chiral interactions determined with the\nlocal regularization are close to those with the nonlocal regularization.\nMoreover, the RBHF theory with local and nonlocal covariant chiral interactions\nprovide equally well description of the saturation properties of nuclear\nmatter. The present work paves the way for the implementation of covariant\nchiral interactions in RBHF theory for finite nuclei.",
        "This review paper is intended for the 2nd edition of the Handbook of Markov\nchain Monte Carlo. We provide an introduction to approximate inference\ntechniques as Bayesian computation methods applied to deep learning models. We\norganize the chapter by presenting popular computational methods for Bayesian\nneural networks and deep generative models, explaining their unique challenges\nin posterior inference as well as the solutions.",
        "The structural and magnetic properties of Fe$_{1-x}$Ni$_x$~($x$ = 0.32, 0.36,\n0.40, 0.50) alloys have been investigated using synchrotron based x-ray\ndiffraction (XRD) technique with x-rays of wavelength 0.63658 \\AA\\ down to 50 K\ntemperature, magnetic measurement using superconducting quantum interference\ndevice (SQUID) magnetometer and high resolution x-ray photoelectron\nspectroscopy (XPS) with monochromatic AlK$_\\alpha$ radiation. The XRD studies\nsuggest a single phase with fcc structure for $x$ = 0.36, 0.40, and 0.50\n~alloys and a mixed phase for $x$ = 0.32 alloy containing both bcc and fcc\nstructures. The lattice parameter of the alloys exhibits a linear dependence on\ntemperature giving rise to a temperature independent coefficient of thermal\nexpansion (CTE). The lowest CTE is observed for $x$ = 0.36 Invar alloy as\nexpected while $x$ = 0.50 alloy exhibits the highest CTE among the alloys\nstudied. The CTE of the fcc component of mixed phase alloy is close to that of\nInvar alloy. The temperature dependence of magnetization of the alloys down to\n2 K reveals an overall antiferromagnetic interactions within the ferromagnetic\nphase causing the magnetization decreasing with cooling. The field cooled and\nzero field cooled data show larger differences for the Invar compositions; this\nis also manifested in the magnetic hysteresis data at 2 K and 300 K.",
        "We propose a new approach towards approximating the density-to-pair-density\nmap based on copula theory from statistics. We extend the copula theory to\nmulti-dimensional marginals, and deduce that one can describe any (exact or\napproximate) pair density by the single-particle density and a copula. We\npresent analytical formulas for the exact copula in scaling limits, numerically\ncompute the copula for dissociating systems with two to four particles in one\ndimension, and propose accurate approximations of the copula between\nequilibrium and dissociation for two-particle systems.",
        "We present a new simple experimental setup for demonstrating beat phenomenon.\nWe have combined two amplitude-modulated light beams on a solar cell using two\nsmartphones as signal generators and a third smartphone as an oscilloscope to\nvisualize the resulting wave beats. A very good agreement is obtained between\nthe theoretical model and the experimental result. It is an innovative approach\nto bring physics experimentation to the students and discover the potential\npossibilities of smartphones in basic physics courses.",
        "This work introduces and investigates the function $J(G) =\n\\frac{\\text{Nil}(G)}{L(G)}$, where $\\text{Nil}(G)$ denotes the number of\nnilpotent subgroups and $L(G)$ the total number of subgroups of a finite group\n$G$. The function $J(G)$, defined over the interval $(0,1]$, serves as a tool\nto analyze structural patterns in finite groups, particularly within\nnon-nilpotent families such as supersolvable and dihedral groups. Analytical\nresults demonstrate the product density of $J(G)$ values in $(0,1]$,\nhighlighting its distribution across products of dihedral groups. Additionally,\na probabilistic analysis was conducted, and based on extensive computational\nsimulations, it was conjectured that the sample mean of $J(G)$ values converges\nin distribution to the standard normal distribution, in accordance with the\nCentral Limit Theorem, as the sample size increases. These findings expand the\nunderstanding of multiplicative functions in group theory, offering novel\ninsights into the structural and probabilistic behavior of finite groups.",
        "A bilayer formed by stacking two distinct materials creates a moir\\'e\nlattice, which can serve as a platform for novel electronic phases. In this\nwork we study a unique example of such a system: the graphene-black phosphorus\nheterostructure (G\/BP), which has been suggested to have an intricate band\nstructure. Most notably, the valence band hosts a quasi-one-dimensional region\nin the Brillouin zone of high density of states, suggesting that various\nmany-body electronic phases are likely to emerge. We derive an effective\ntight-binding model that reproduces this band structure, and explore the\nemergent broken-symmetry phases when interactions are introduced. Employing a\nmean-field analysis, we find that the favored ground-state exhibits a striped\nspin density wave (SDW) order, characterized by either one of two-fold\ndegenerate wave-vectors that are tunable by gating. Further exploring the\nphase-diagram controlled by gate voltage and the interaction strength, we find\nthat the SDW-ordered state undergoes a metal to insulator transition via an\nintermediate metallic phase which supports striped SDW correlations. Possible\nexperimental signatures are discussed, in particular a highly anisotropic\ndispersion of the collective excitations which should be manifested in electric\nand thermal transport.",
        "Breast cancer is the leading cause of mortality among women. Inspection of\nbreasts by palpation is the key to early detection. We aim to create a wearable\ntactile glove that could localize the lump in breasts using deep learning (DL).\nIn this work, we present our flexible fabric-based and soft wearable tactile\nglove for detecting the lumps within custom-made silicone breast prototypes\n(SBPs). SBPs are made of soft silicone that imitates the human skin and the\ninner part of the breast. Ball-shaped silicone tumors of 1.5-, 1.75- and 2.0-cm\ndiameters are embedded inside to create another set with lumps. Our approach is\nbased on the InceptionTime DL architecture with transfer learning between\nexperienced and non-experienced users. We collected a dataset from 10 naive\nparticipants and one oncologist-mammologist palpating SBPs. We demonstrated\nthat the DL model can classify lump presence, size and location with an\naccuracy of 82.22%, 67.08% and 62.63%, respectively. In addition, we showed\nthat the model adapted to unseen experienced users with an accuracy of 95.01%,\n88.54% and 82.98% for lump presence, size and location classification,\nrespectively. This technology can assist inexperienced users or healthcare\nproviders, thus facilitating more frequent routine checks.",
        "The study of numerical rounding errors is often greatly simplified in the\nanalytical treatment of mathematical problems, or even entirely separated from\nit. In sampling theory, for instance, it is standard to assume the availability\nof an orthonormal basis for computations, ensuring that numerical errors are\nnegligible. In reality, however, this assumption is often unmet. In this paper,\nwe discard it and demonstrate the advantages of integrating numerical insights\nmore deeply into sampling theory. To clearly pinpoint when the numerical\nphenomena play a significant role, we introduce the concept of numerical\nredundancy. A set of functions is numerically redundant if it spans a\nlower-dimensional space when analysed numerically rather than analytically.\nThis property makes it generally impossible to compute the best approximation\nof a function in its span using finite precision. In contrast,\n$\\ell^2$-regularized approximations are computable and, therefore, form the\nfoundation of many practical methods. Regularization generally reduces accuracy\ncompared to the best approximation, but our analysis shows that there is a\nbenefit: it also significantly reduces the amount of data needed for accurate\napproximation. Furthermore, we present a constructive method for optimally\nselecting data points for $L^2$-approximations, explicitly accounting for the\neffects of regularization. The results are illustrated for two common scenarios\nthat lead to numerical redundancy: (1) approximations on irregular domains and\n(2) approximations that incorporate specific features of the function to be\napproximated. In doing so, we obtain new results on random sampling for Fourier\nextension frames. Finally, we establish that regularization is implicit in\nnumerical orthogonalization of a numerically redundant set, indicating that its\nanalysis cannot be bypassed in a much broader range of methods.",
        "This study describes the impact of non-zero value of strange quark mass\n$(m_{s})$ and number density of baryons $(n)$ on the structure, stability and\nmaximum mass of strange stars. We derive an exact relativistic solution of the\nEinstein field equation using the Tolman-IV metric potential and modified MIT\nbag model EoS, $p_{r}=\\frac{1}{3}(\\rho-4B')$, where $B'$ is a function of bag\nconstant $B$, $m_{s}$ and baryon number density $(n)$. Following CERN's\nfindings, transition of phase from hadronic matter to Quark-Gluon Plasma (QGP)\nmay occur at high densities in presence of favourable conditions. The standard\nMIT bag model, with a constant $B$, fails to explain such transition properly.\nIntroducing a finite $m_{s}$ and Wood-Saxon parametrisation for $B$, dependent\non baryon number density $(n)$, provides a more realistic EoS to address such\nphase transition. Both $m_{s}$ and $n$ constrain the EoS, making it softer as\n$m_{s}$ increases. Solutions to the TOV equations reveal that for massless\nstrange quarks, maximum mass is 2.01 $M_{\\odot}$ and corresponding radius is\n10.96 Km when $n=0.66~fm^{-3}$. These values decrease to 1.99 $M_{\\odot}$ and\n1.96 $M_{\\odot}$, with corresponding radii of 10.88 Km and 10.69 Km for\n$m_{s}=50$ and $100~MeV$ respectively having same $n$ value. It is interesting\nto note that a corelation exists between $n$ and $m_{s}$. The hadronic to quark\nmatter transition occurs at higher values of $n$, when $m_{s}$ increases such\nas $n\\geq0.484,~0.489$ and $0.51~fm^{-3}$ for $m_{s}=50$ and $100~MeV$\nrespectively. Beyond these values, the energy per baryon $(\\mathcal{E_{B}})$\ndrops below $930.4~MeV$, indicating a complete transition to quark matter. For\nphysical analysis, we have considered $n~(=0.578~fm^{-3})$ which lies in the\nstable region with $B(n)=70~MeV\/fm^{3}$. The model provides a viable\ndescription of strange stars, satisfying all necessary physical requirements.",
        "In 2017, Ehrenborg, Govindaiah, Park, and Readdy defined the van der Waerden\ncomplex ${\\tt vdW}(n,k)$ to be the simplicial complex whose facets correspond\nto all the arithmetic sequences on the set $\\{1,\\ldots,n\\}$ of a fixed length\n$k$. To complement a classification of the Cohen--Macaulay van der Waerden\ncomplexes obtained by Hooper and Van Tuyl in 2019, a classification of van der\nWaerden complexes with linear resolution is presented. Furthermore, we show\nthat the Stanley--Reisner ring of a Cohen--Macaulay van der Waerden complex is\nlevel.",
        "Understanding the generation and development of the continuous outflow from\nthe Sun requires tracing the physical conditions from deep in the corona to the\nheliosphere. Detailed global observations of plasma state variables and the\nmagnetic field are needed to provide critical constraints to the underlying\nphysics driving models of the corona and solar wind. Key diagnostics of the\nsolar wind require measurements at its formation site and during its outflow to\ncontinuously track it across rapidly changing regions of space. A unified view\nof the solar wind is only possible through coordinated remote and in situ\nobservations that probe these different regions. Here, we discuss current\nobservational coverage and gaps of different plasma properties and review\nrecent coordinated studies. We highlight how these efforts may become more\nroutine with the launch of upcoming and planned missions.",
        "The measurement of quasiparticle scattering patterns on material surfaces\nusing scanning tunneling microscopy (STM) is now an established technique for\naccessing the momentum-resolved electronic band structure of solids. However,\nsince these quasiparticle interference (QPI) patterns reflect spatial\nvariations related to differences in the band momenta rather than the momenta\nthemselves, their interpretation often relies on comparisons with simple\ngeometrical models such as the joint density of states (JDOS) or with the\nconvolution of Green's functions. In this paper, we highlight non-intuitive\ndifferences between Green's function and JDOS results. To understand the origin\nof these discrepancies, we analyze the convolution of Green's functions using\nthe Feynman parametrization technique and introduce a framework that we call\nthe intermediate band analysis. This approach allows us to derive simple\nselection rules for interband QPI, based on electron group velocities.\nConnecting the intermediate band analysis with the experiment, we consider\nexperimental Bogoliubov QPI patterns measured for FeSe1-xSx, which were\nrecently used to demonstrate a highly anisotropic superconducting gap,\nindicating superconductivity mediated by nematic fluctuations [1]. The\ncalculated Green's functions convolutions reproduce the particle-hole asymmetry\nin the intensity of QPI patterns across the Fermi level observed in\nexperiments. Finally, we demonstrate the utility of intermediate band analysis\nin tracing the origin of this asymmetry to a coherence factor effect of the\nsuperconducting state.",
        "We propose to use interpolating operators for lattice quantum chromodyanmics\n(QCD) calculations of highly-boosted pions and nucleons with\nkinematically-enhanced ground-state overlap factors at large momentum. Because\nthis kinematic enhancement applies to the signal but not the variance of the\ncorrelation function, these interpolating operators can achieve better\nsignal-to-noise ratios at large momentum. We perform proof-of-principle\ncalculations with boosted pions and nucleons using close-to-physical and larger\nquark masses to explore the utility of our proposal. Results for effective\nenergies and matrix elements, as well as Lanczos ground-state energy\nestimators, are consistent with theoretical expectations for signal-to-noise\nimprovement at large momenta.",
        "The loss of close-in planetary atmospheres is influenced by various physical\nprocesses, such as photoionisation, which could potentially affect the\natmosphere survivability on a secular timescale. The amount of stellar\nradiation converted into heat depends on the energy of the primary electrons\nproduced by photoionisation and the local ionisation fraction. The Lyman-alpha\nline is an excellent probe for atmospheric escape. We study the interaction\nbetween the planetary and the stellar wind, the difference of the predicted\nmass-loss rates between 1D and 2D models, the signal of Ly-a and the impact of\nstellar flares. Using the PLUTO code, we perform 2D hydrodynamics simulations\nfor four different planets. We consider planets in the size range from Neptune\nto Jupiter. We produce synthetic Ly-a profiles to comprehend the origin of the\nsignal, and in particular its high velocity Doppler shift. Our results indicate\na trend similar to the 1D models, with a decrease in the planetary mass-loss\nrate for all systems when secondary ionisation is taken into account. The\nmass-loss rates are found to decrease by 48% for the least massive planet when\nsecondary ionisation is accounted for. We find nevertheless a decrease that is\nless pronounced in 2D than in 1D. We observe differences in the Ly-a profile\nbetween the different cases and significant asymmetries in all of them,\nespecially for the lower mass planets. Finally, we observe that stellar flares\ndo not affect the mass-loss rate because they act, in general, on a timescale\nthat is too short. We find velocities in the escaping atmosphere up to 100\nkm\/s, with the gas moving away from the star, which could be the result of the\ninteraction with the stellar wind. Furthermore, we find that stellar flares\ngenerally occur on a timescale that is too short to have a visible impact on\nthe mass-loss rate of the atmosphere.",
        "In the Coulomb and Proximity Potential Model (CPPM) framework, we have\ninvestigated the cluster radioactivity and alpha decay half-lives of superheavy\nnuclei. We study 22 different versions of proximity potential forms that have\nbeen proposed to describe proton radioactivity, two-proton radioactivity,\nheavy-ion radioactivity, quasi-elastic scattering, fusion reactions, and other\napplications. The half-lives of cluster radioactivity and alpha decay of 41\natomic nuclei ranging from 221Fr to 244Cm were calculated, and the results\nindicate that the refined nuclear potential named BW91 is the most suitable\nproximity potential form for the cluster radioactivity and alpha decay of\nsuperheavy nuclei since the root-mean-square (RMS) deviation between the\nexperimental data and the relevant theoretical calculation results is the\nsmallest ({\\sigma}= 0.841). By using CPPM, we predicted the half-lives of 20\npotential cluster radioactivity and alpha decay candidates. These cluster\nradioactivities and alpha decays are energetically allowed or observable but\nnot yet quantified in NUBASE2020.",
        "This paper presents reconstructions of homogeneous targets from the 2D and 3D\nFresnel databases by one-step imaging methods based on the computation of\ntopological derivative and topological energy fields. The electromagnetic\ninverse scattering problem is recast as a constrained optimization problem, in\nwhich we seek to minimize the error when comparing experimental microwave\nmeasurements with computer-generated synthetic data for arbitrary targets by\napproximating a Maxwell forward model. The true targets are then characterized\nby combining the topological derivatives or energies of such shape functionals\nfor all available receivers and emitters at different frequencies. Our\napproximations are comparable to the best approximations already obtained by\nother methods. However, these topological fields admit easy to evaluate\nclosed-form expressions, which speeds up the process."
      ]
    }
  },
  {
    "id":2411.10004,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm",
    "start_abstract":"In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Artificial Intelligence for Pediatric Ophthalmology"
      ],
      "abstract":[
        "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Compressed Image Generation with Denoising Diffusion Codebook Models",
        "Juggling with Tensor Bases in Functional Approaches",
        "A Comprehensive Search for Leptoquarks Decaying into Top-$\\tau$ Final\n  States at the Future LHC",
        "MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D\n  CT Image Synthesis",
        "Measuring Diversity in Synthetic Datasets",
        "Quantitative Analysis of Deeply Quantized Tiny Neural Networks Robust to\n  Adversarial Attacks",
        "A Parareal in time numerical method for the collisional Vlasov equation\n  in the hyperbolic scaling",
        "POSMAC: Powering Up In-Network AR\/CG Traffic Classification with Online\n  Learning",
        "Evaluating Stenosis Detection with Grounding DINO, YOLO, and DINO-DETR",
        "Quantum superposition of boundary condition in $\\mathrm{PAdS}_2$",
        "Towards Automated Fact-Checking of Real-World Claims: Exploring Task\n  Formulation and Assessment with LLMs",
        "AUTOFRAME -- A Software-driven Integration Framework for Automotive\n  Systems",
        "Multi-Fidelity Policy Gradient Algorithms",
        "Structural Damping Identification Sensitivity in Flutter Speed\n  Estimation",
        "Comment on \"QCD factorization with multihadron fragmentation functions\"",
        "Funzac at CoMeDi Shared Task: Modeling Annotator Disagreement from\n  Word-In-Context Perspectives",
        "Elliptic Relaxation Strategies to Support Numerical Stability of\n  Segregated Continuous Adjoint Flow Solvers",
        "Graph-Guided Scene Reconstruction from Images with 3D Gaussian Splatting",
        "Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for\n  Furniture Assembly Using Vision-Language Models",
        "On the Underlying Nonrelativistic Nature of Relativistic Holography",
        "A model for dynamical systems with strange attractors",
        "BiRating -- Iterative averaging on a bipartite graph of Beat Saber\n  scores, player skills, and map difficulties",
        "Revisiting Ensemble Methods for Stock Trading and Crypto Trading Tasks\n  at ACM ICAIF FinRL Contest 2023-2024",
        "Anisotropic Raman scattering and lattice orientation identification of\n  2M-WS2",
        "Self-Taught Agentic Long Context Understanding",
        "Advancing Language Model Reasoning through Reinforcement Learning and\n  Inference Scaling",
        "A Self-Supervised Reinforcement Learning Approach for Fine-Tuning Large\n  Language Models Using Cross-Attention Signals",
        "The disrupting and growing open cluster spiral arm patterns of the Milky\n  Way",
        "Unveiling Biases while Embracing Sustainability: Assessing the Dual\n  Challenges of Automatic Speech Recognition Systems"
      ],
      "abstract":[
        "We present a novel generative approach based on Denoising Diffusion Models\n(DDMs), which produces high-quality image samples along with their losslessly\ncompressed bit-stream representations. This is obtained by replacing the\nstandard Gaussian noise sampling in the reverse diffusion with a selection of\nnoise samples from pre-defined codebooks of fixed iid Gaussian vectors.\nSurprisingly, we find that our method, termed Denoising Diffusion Codebook\nModel (DDCM), retains sample quality and diversity of standard DDMs, even for\nextremely small codebooks. We leverage DDCM and pick the noises from the\ncodebooks that best match a given image, converting our generative model into a\nhighly effective lossy image codec achieving state-of-the-art perceptual image\ncompression results. More generally, by setting other noise selections rules,\nwe extend our compression method to any conditional image generation task\n(e.g., image restoration), where the generated images are produced jointly with\ntheir condensed bit-stream representations. Our work is accompanied by a\nmathematical interpretation of the proposed compressed conditional generation\nschemes, establishing a connection with score-based approximations of posterior\nsamplers for the tasks considered.",
        "Systematic expansion schemes in functional approaches require the inclusion\nof higher order vertices. These vertices are expanded in independent tensor\nbases with a rapidly increasing number of basis elements. Amongst the related\ntasks are the construction of bases and projection operators, the importance\nordering of their elements, and the optimisation of such tensor bases, as well\nas an analysis of their regularity in momentum space. We present progress in\nall these directions and introduce the Mathematica package TensorBases designed\nfor the aforementioned tasks.",
        "We studied the collider phenomenology of third-generation scalar leptoquarks\nat the Large Hadron Collider (LHC) with a 14 TeV center-of-mass energy. The\nanalysis focuses on leptoquarks decaying exclusively into top quarks and tau\nleptons, employing machine learning-based tagging techniques for identifying\nhadronically decaying boosted top quarks, W\/Z, and Higgs bosons, as well as a\nmultivariate classifier to distinguish signal events from Standard Model (SM)\nbackgrounds. The expected 95% confidence level (CL) upper limits on the\nleptoquark production cross-section are computed assuming integrated\nluminosities of 200 and 500 inverse femtobarns at the 14 TeV LHC. The results\ndemonstrate significant sensitivity improvements for detecting leptoquarks at\nmasses beyond the current experimental limits.",
        "Advancements in AI for medical imaging offer significant potential. However,\ntheir applications are constrained by the limited availability of data and the\nreluctance of medical centers to share it due to patient privacy concerns.\nGenerative models present a promising solution by creating synthetic data as a\nsubstitute for real patient data. However, medical images are typically\nhigh-dimensional, and current state-of-the-art methods are often impractical\nfor computational resource-constrained healthcare environments. These models\nrely on data sub-sampling, raising doubts about their feasibility and\nreal-world applicability. Furthermore, many of these models are evaluated on\nquantitative metrics that alone can be misleading in assessing the image\nquality and clinical meaningfulness of the generated images. To address this,\nwe introduce MedLoRD, a generative diffusion model designed for computational\nresource-constrained environments. MedLoRD is capable of generating\nhigh-dimensional medical volumes with resolutions up to\n512$\\times$512$\\times$256, utilizing GPUs with only 24GB VRAM, which are\ncommonly found in standard desktop workstations. MedLoRD is evaluated across\nmultiple modalities, including Coronary Computed Tomography Angiography and\nLung Computed Tomography datasets. Extensive evaluations through radiological\nevaluation, relative regional volume analysis, adherence to conditional masks,\nand downstream tasks show that MedLoRD generates high-fidelity images closely\nadhering to segmentation mask conditions, surpassing the capabilities of\ncurrent state-of-the-art generative models for medical image synthesis in\ncomputational resource-constrained environments.",
        "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing approaches. Code\nis available at: https:\/\/github.com\/BlueWhaleLab\/DCScore.",
        "Reducing the memory footprint of Machine Learning (ML) models, especially\nDeep Neural Networks (DNNs), is imperative to facilitate their deployment on\nresource-constrained edge devices. However, a notable drawback of DNN models\nlies in their susceptibility to adversarial attacks, wherein minor input\nperturbations can deceive them. A primary challenge revolves around the\ndevelopment of accurate, resilient, and compact DNN models suitable for\ndeployment on resource-constrained edge devices. This paper presents the\noutcomes of a compact DNN model that exhibits resilience against both black-box\nand white-box adversarial attacks. This work has achieved this resilience\nthrough training with the QKeras quantization-aware training framework. The\nstudy explores the potential of QKeras and an adversarial robustness technique,\nJacobian Regularization (JR), to co-optimize the DNN architecture through\nper-layer JR methodology. As a result, this paper has devised a DNN model\nemploying this co-optimization strategy based on Stochastic Ternary\nQuantization (STQ). Its performance was compared against existing DNN models in\nthe face of various white-box and black-box attacks. The experimental findings\nrevealed that, the proposed DNN model had small footprint and on average, it\nexhibited better performance than Quanos and DS-CNN MLCommons\/TinyML (MLC\/T)\nbenchmarks when challenged with white-box and black-box attacks, respectively,\non the CIFAR-10 image and Google Speech Commands audio datasets.",
        "We present the design of a multiscale parareal method for kinetic equations\nin the fluid dynamic regime. The goal is to reduce the cost of a fully kinetic\nsimulation using a parallel in time procedure. Using the multiscale property of\nkinetic models, the cheap, coarse propagator consists in a fluid solver and the\nfine (expensive) propagation is achieved through a kinetic solver for a\ncollisional Vlasov equation. To validate our approach, we present simulations\nin the 1D in space, 3D in velocity settings over a wide range of initial data\nand kinetic regimes, showcasing the accuracy, efficiency, and the speedup\ncapabilities of our method.",
        "In this demonstration, we showcase POSMAC1, a platform designed to deploy\nDecision Tree (DT) and Random Forest (RF) models on the NVIDIA DOCA DPU,\nequipped with an ARM processor, for real-time network traffic classification.\nDeveloped specifically for Augmented Reality (AR) and Cloud Gaming (CG) traffic\nclassification, POSMAC streamlines model evaluation, and generalization while\noptimizing throughput to closely match line rates.",
        "Detecting stenosis in coronary angiography is vital for diagnosing and\nmanaging cardiovascular diseases. This study evaluates the performance of\nstate-of-the-art object detection models on the ARCADE dataset using the\nMMDetection framework. The models are assessed using COCO evaluation metrics,\nincluding Intersection over Union (IoU), Average Precision (AP), and Average\nRecall (AR). Results indicate variations in detection accuracy across different\nmodels, attributed to differences in algorithmic design, transformer-based vs.\nconvolutional architectures. Additionally, several challenges were encountered\nduring implementation, such as compatibility issues between PyTorch, CUDA, and\nMMDetection, as well as dataset inconsistencies in ARCADE. The findings provide\ninsights into model selection for stenosis detection and highlight areas for\nfurther improvement in deep learning-based coronary artery disease diagnosis.",
        "We explore the quantum superposition of boundary conditions in the context of\nthe Poincar\\'e patch of the two-dimensional Anti-de Sitter space\n($\\mathrm{PAdS}_2$). Focusing on Robin (mixed) boundary conditions (RBC), we\ninvestigate the response function of the Unruh-DeWitt (UDW) detector\ninteracting with two or more scalar fields, each respecting a different\nboundary condition. The role of this quantum superposition is two-fold: i) it\nmay represent different fields propagating on the same spacetime and\ninteracting with an UDW detector or ii) it may describe an UDW detector on a\nsuperposition of spacetimes, each one with an inequivalent propagating field.",
        "Fact-checking is necessary to address the increasing volume of\nmisinformation. Traditional fact-checking relies on manual analysis to verify\nclaims, but it is slow and resource-intensive. This study establishes baseline\ncomparisons for Automated Fact-Checking (AFC) using Large Language Models\n(LLMs) across multiple labeling schemes (binary, three-class, five-class) and\nextends traditional claim verification by incorporating analysis, verdict\nclassification, and explanation in a structured setup to provide comprehensive\njustifications for real-world claims. We evaluate Llama-3 models of varying\nsizes (3B, 8B, 70B) on 17,856 claims collected from PolitiFact (2007-2024)\nusing evidence retrieved via restricted web searches. We utilize TIGERScore as\na reference-free evaluation metric to score the justifications. Our results\nshow that larger LLMs consistently outperform smaller LLMs in classification\naccuracy and justification quality without fine-tuning. We find that smaller\nLLMs in a one-shot scenario provide comparable task performance to fine-tuned\nSmall Language Models (SLMs) with large context sizes, while larger LLMs\nconsistently surpass them. Evidence integration improves performance across all\nmodels, with larger LLMs benefiting most. Distinguishing between nuanced labels\nremains challenging, emphasizing the need for further exploration of labeling\nschemes and alignment with evidences. Our findings demonstrate the potential of\nretrieval-augmented AFC with LLMs.",
        "The evolution of automotive technologies towards more integrated and\nsophisticated systems requires a shift from traditional distributed\narchitectures to centralized vehicle architectures. This work presents a novel\nframework that addresses the increasing complexity of Software Defined Vehicles\n(SDV) through a centralized approach that optimizes software and hardware\nintegration. Our approach introduces a scalable, modular, and secure automotive\ndeployment framework that leverages a hardware abstraction layer and dynamic\nsoftware deployment capabilities to meet the growing demands of the industry.\nThe framework supports centralized computing of vehicle functions, making\nsoftware development more dynamic and easier to update and upgrade. We\ndemonstrate the capabilities of our framework by implementing it in a simulated\nenvironment where it effectively handles several automotive operations such as\nlane detection, motion planning, and vehicle control. Our results highlight the\nframework's potential to facilitate the development and maintenance of future\nvehicles, emphasizing its adaptability to different hardware configurations and\nits readiness for real-world applications. This work lays the foundation for\nfurther exploration of robust, scalable, and secure SDV systems, setting a new\nstandard for future automotive architectures.",
        "Many reinforcement learning (RL) algorithms require large amounts of data,\nprohibiting their use in applications where frequent interactions with\noperational systems are infeasible, or high-fidelity simulations are expensive\nor unavailable. Meanwhile, low-fidelity simulators--such as reduced-order\nmodels, heuristic reward functions, or generative world models--can cheaply\nprovide useful data for RL training, even if they are too coarse for direct\nsim-to-real transfer. We propose multi-fidelity policy gradients (MFPGs), an RL\nframework that mixes a small amount of data from the target environment with a\nlarge volume of low-fidelity simulation data to form unbiased, reduced-variance\nestimators (control variates) for on-policy policy gradients. We instantiate\nthe framework by developing multi-fidelity variants of two policy gradient\nalgorithms: REINFORCE and proximal policy optimization. Experimental results\nacross a suite of simulated robotics benchmark problems demonstrate that when\ntarget-environment samples are limited, MFPG achieves up to 3.9x higher reward\nand improves training stability when compared to baselines that only use\nhigh-fidelity data. Moreover, even when the baselines are given more\nhigh-fidelity samples--up to 10x as many interactions with the target\nenvironment--MFPG continues to match or outperform them. Finally, we observe\nthat MFPG is capable of training effective policies even when the low-fidelity\nenvironment is drastically different from the target environment. MFPG thus not\nonly offers a novel paradigm for efficient sim-to-real transfer but also\nprovides a principled approach to managing the trade-off between policy\nperformance and data collection costs.",
        "Predicting flutter remains a key challenge in aeroelastic research, with\ncertain models relying on modal parameters, such as natural frequencies and\ndamping ratios. These models are particularly useful in early design stages or\nfor the development of small UAVs (maximum take-off mass below 7 kg). This\nstudy evaluates two frequency-domain system identification methods, Fast\nRelaxed Vector Fitting (FRVF) and the Loewner Framework (LF), for predicting\nthe flutter onset speed of a flexible wing model. Both methods are applied to\nextract modal parameters from Ground Vibration Testing data, which are\nsubsequently used to develop a reduced-order model with two degrees of freedom.\nResults indicate that FRVF and LFinformed models provide reliable flutter\nspeed, with predictions deviating by no more than 3% (FRVF) and 5% (LF) from\nthe N4SID-informed benchmark. The findings highlight the sensitivity of flutter\nspeed predictions to damping ratio identification accuracy and demonstrate the\npotential of these methods as computationally efficient alternatives for\npreliminary aeroelastic assessments.",
        "We make several comments on the recent work in Ref.~\\cite{Rogers:2024nhb}\nwhile also reaffirming and adding to the work in Ref.~\\cite{Pitonyak:2023gjx}.\nWe show that the factorization formula for $e^+e^-\\to (h_1\\cdots h_n)\\, X$ in\nRef.~\\cite{Rogers:2024nhb} is equivalent to a version one can derive using the\ndefinition of a $n$-hadron fragmentation function (FF) introduced in\nRef.~\\cite{Pitonyak:2023gjx}. In addition, we scrutinize how to generalize the\nnumber density definition of a single-hadron FF to a $n$-hadron FF, arguing\nthat the definition given in Ref.~\\cite{Pitonyak:2023gjx} should be considered\nthe standard one. We also emphasize that the evolution equations for dihadron\nFFs~(DiFFs) in Ref.~\\cite{Pitonyak:2023gjx} have the same splitting functions\nas those for single-hadron FFs. Therefore, the DiFF (and $n$-hadron FF)\ndefinitions in Ref.~\\cite{Pitonyak:2023gjx} have a natural number density\ninterpretation and are consistent with collinear factorization using the\nstandard hard factors and evolution kernels. Moreover, we make clear that the\noperator definition for the DiFF $D_1^{h_1h_2}(\\xi,M_h)$ written down in\nRef.~\\cite{Rogers:2024nhb} agrees exactly with the one in\nRef.~\\cite{Pitonyak:2023gjx}. Contrary to what is implied in\nRef.~\\cite{Rogers:2024nhb}, this definition did not appear in the literature\nprior to the work in Ref.~\\cite{Pitonyak:2023gjx}. There also seem to be\ninconsistencies in how $D_1^{h_1h_2}(\\xi,M_h)$ appears in previous unpolarized\ncross section formulas in the literature.",
        "In this work, we evaluate annotator disagreement in Word-in-Context (WiC)\ntasks exploring the relationship between contextual meaning and disagreement as\npart of the CoMeDi shared task competition. While prior studies have modeled\ndisagreement by analyzing annotator attributes with single-sentence inputs,\nthis shared task incorporates WiC to bridge the gap between sentence-level\nsemantic representation and annotator judgment variability. We describe three\ndifferent methods that we developed for the shared task, including a feature\nenrichment approach that combines concatenation, element-wise differences,\nproducts, and cosine similarity, Euclidean and Manhattan distances to extend\ncontextual embedding representations, a transformation by Adapter blocks to\nobtain task-specific representations of contextual embeddings, and classifiers\nof varying complexities, including ensembles. The comparison of our methods\ndemonstrates improved performance for methods that include enriched and\ntask-specfic features. While the performance of our method falls short in\ncomparison to the best system in subtask 1 (OGWiC), it is competitive to the\nofficial evaluation results in subtask 2 (DisWiC).",
        "This paper introduces a novel method for numerically stabilizing sequential\ncontinuous adjoint flow solvers utilizing an elliptic relaxation strategy. The\nproposed approach is formulated as a Partial Differential Equation (PDE)\ncontaining a single user-defined parameter, which analytical investigations\nreveal to represent the filter width of a probabilistic density function or\nGaussian kernel. Key properties of the approach include (a) smoothing features\nwith redistribution capabilities while (b) preserving integral properties. The\ntechnique targets explicit adjoint cross-coupling terms, such as the Adjoint\nTranspose Convection (ATC) term, which frequently causes numerical\ninstabilities, especially on unstructured grids common in industrial\napplications. A trade-off is made by sacrificing sensitivity consistency to\nachieve enhanced numerical robustness.\n  The method is validated on a two-phase, laminar, two-dimensional cylinder\nflow test case at Re=20 and Fn=0.75, focusing on minimizing resistance or\nmaximizing lift. A range of homogeneous and inhomogeneous filter widths is\nevaluated. Subsequently, the relaxation method is employed to stabilize adjoint\nsimulations during shape optimizations that aim at drag reduction of ship\nhulls. Two case studies are considered: A model-scale bulk carrier traveling at\nRe=7.246E+06 and Fn=0.142 as well as a harbor ferry cruising at Re=2.43E+08 and\nFn=0.4 in full-scale conditions. Both cases, characterized by unstructured\ngrids prone to adjoint divergence, demonstrate the effectiveness of the\nproposed method in overcoming stability challenges. The resulting optimizations\nachieve superior outcomes compared to approaches that omit problematic coupling\nterms.",
        "This paper investigates an open research challenge of reconstructing\nhigh-quality, large 3D open scenes from images. It is observed existing methods\nhave various limitations, such as requiring precise camera poses for input and\ndense viewpoints for supervision. To perform effective and efficient 3D scene\nreconstruction, we propose a novel graph-guided 3D scene reconstruction\nframework, GraphGS. Specifically, given a set of images captured by RGB cameras\non a scene, we first design a spatial prior-based scene structure estimation\nmethod. This is then used to create a camera graph that includes information\nabout the camera topology. Further, we propose to apply the graph-guided\nmulti-view consistency constraint and adaptive sampling strategy to the 3D\nGaussian Splatting optimization process. This greatly alleviates the issue of\nGaussian points overfitting to specific sparse viewpoints and expedites the 3D\nreconstruction process. We demonstrate GraphGS achieves high-fidelity 3D\nreconstruction from images, which presents state-of-the-art performance through\nquantitative and qualitative evaluation across multiple datasets. Project Page:\nhttps:\/\/3dagentworld.github.io\/graphgs.",
        "Humans possess an extraordinary ability to understand and execute complex\nmanipulation tasks by interpreting abstract instruction manuals. For robots,\nhowever, this capability remains a substantial challenge, as they cannot\ninterpret abstract instructions and translate them into executable actions. In\nthis paper, we present Manual2Skill, a novel framework that enables robots to\nperform complex assembly tasks guided by high-level manual instructions. Our\napproach leverages a Vision-Language Model (VLM) to extract structured\ninformation from instructional images and then uses this information to\nconstruct hierarchical assembly graphs. These graphs represent parts,\nsubassemblies, and the relationships between them. To facilitate task\nexecution, a pose estimation model predicts the relative 6D poses of components\nat each assembly step. At the same time, a motion planning module generates\nactionable sequences for real-world robotic implementation. We demonstrate the\neffectiveness of Manual2Skill by successfully assembling several real-world\nIKEA furniture items. This application highlights its ability to manage\nlong-horizon manipulation tasks with both efficiency and precision,\nsignificantly enhancing the practicality of robot learning from instruction\nmanuals. This work marks a step forward in advancing robotic systems capable of\nunderstanding and executing complex manipulation tasks in a manner akin to\nhuman capabilities.",
        "Over the past quarter century, considerable effort has been invested in the\nstudy of nonrelativistic (NR) string theory, its U-dual NR brane theories, and\ntheir geometric foundations in (generalized) Newton-Cartan geometry. Many\ninteresting results have been obtained, both for their intrinsic value and in\nthe hope that they hold useful lessons for relativistic string\/M theory. By\nsynthesizing two strands of recent developments (especially, arXiv:2312.13243\nand arXiv:2410.03591), we argue that this hope has already come to fruition,\nbecause standard, relativistic holography can now be recognized as a statement\nwithin a corresponding nonrelativistic brane theory. Our main conclusions are\ngeneral, but within the familiar example of D3-brane based holography, they\nread as follows: (i) N=4 SYM is exactly the worldvolume theory of D3-branes\nwithin `NR D3-brane theory'; (ii) AdS_5*S^5 is exactly the corresponding RR\nblack 3-brane, and includes an asymptotically-flat-Newton-Cartan region; (iii)\nAdS\/CFT duality is precisely synonymous with black-brane\/D-brane (i.e.,\nclosed-string\/open-string) duality within NR D3-brane theory; (iv)\nNewton-Cartan geometry is the underlying structure upon which entanglement of\nthe D3-brane degrees of freedom builds relativistic spacetime.",
        "We derive a system with one degree of freedom that models a class of\ndynamical systems with strange attractors in three dimensions. This system\nretains all the characteristics of chaotic attractors and is expressed by a\nsecond-order integro-differential equation which mimics a spring-like problem.\nWe determine the potential energy, the rate of change of the kinetic energy of\nthis system, and show that is self-oscillating.",
        "Difficulty estimation of Beat Saber maps is an interesting data analysis\nproblem and valuable to the Beat Saber competitive scene. We present a simple\nalgorithm that iteratively averages player skill and map difficulty estimations\nin a bipartite graph of players and maps, connected by scores, using scores\nonly as input. This approach simultaneously estimates player skills and map\ndifficulties, exploiting each of them to improve the estimation of the other,\nexploitng the relation of multiple scores by different players on the same map,\nor on different maps by the same player. While we have been unable to prove or\ncharacterize theoretical convergence, the implementation exhibits convergent\nbehaviour to low estimation error in all instances, producing accurate results.\nAn informal qualitative evaluation involving experienced Beat Saber community\nmembers was carried out, comparing the difficulty estimations output by our\nalgorithm with their personal perspectives on the difficulties of different\nmaps. There was a significant alignment with player perceived perceptions of\ndifficulty and with other existing methods for estimating difficulty. Our\napproach showed significant improvement over existing methods in certain known\nproblematic maps that are not typically accurately estimated, but also produces\nproblematic estimations for certain families of maps where the assumptions on\nthe meaning of scores were inadequate (e.g. not enough scores, or scores over\noptimized by players). The algorithm has important limitations, related to data\nquality and meaningfulness, assumptions on the domain problem, and theoretical\nconvergence of the algorithm. Future work would significantly benefit from a\nbetter understanding of adequate ways to quantify map difficulty in Beat Saber,\nincluding multidimensionality of skill and difficulty, and the systematic\nbiases present in score data.",
        "Reinforcement learning has demonstrated great potential for performing\nfinancial tasks. However, it faces two major challenges: policy instability and\nsampling bottlenecks. In this paper, we revisit ensemble methods with massively\nparallel simulations on graphics processing units (GPUs), significantly\nenhancing the computational efficiency and robustness of trained models in\nvolatile financial markets. Our approach leverages the parallel processing\ncapability of GPUs to significantly improve the sampling speed for training\nensemble models. The ensemble models combine the strengths of component agents\nto improve the robustness of financial decision-making strategies. We conduct\nexperiments in both stock and cryptocurrency trading tasks to evaluate the\neffectiveness of our approach. Massively parallel simulation on a single GPU\nimproves the sampling speed by up to $1,746\\times$ using $2,048$ parallel\nenvironments compared to a single environment. The ensemble models have high\ncumulative returns and outperform some individual agents, reducing maximum\ndrawdown by up to $4.17\\%$ and improving the Sharpe ratio by up to $0.21$.\n  This paper describes trading tasks at ACM ICAIF FinRL Contests in 2023 and\n2024.",
        "Anisotropic materials with low symmetries hold significant promise for\nnext-generation electronic and quantum devices. 2M-WS2, a candidate for\ntopological superconductivity, has garnered considerable interest. However, a\ncomprehensive understanding of how its anisotropic features contribute to\nunconventional superconductivity, along with a simple, reliable method to\nidentify its crystal orientation, remains elusive. Here, we combine theoretical\nand experimental approaches to investigate angle- and polarization-dependent\nanisotropic Raman modes of 2M-WS2. Through first-principles calculations, we\npredict and analyze phonon dispersion and lattice vibrations of all Raman modes\nin 2M-WS2. We establish a direct correlation between their anisotropic Raman\nspectra and high-resolution transmission electron microscopy images. Finally,\nwe demonstrate that anisotropic Raman spectroscopy can accurately determine the\ncrystal orientation and twist angle between two stacked 2M-WS2 layers. Our\nfindings provide insights into the electron-phonon coupling and anisotropic\nproperties of 2M-WS2, paving the way for the use of anisotropic materials in\nadvanced electronic and quantum devices.",
        "Answering complex, long-context questions remains a major challenge for large\nlanguage models (LLMs) as it requires effective question clarifications and\ncontext retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a\nframework designed to enhance an LLM's understanding of such queries by\nintegrating targeted self-clarification with contextual grounding within an\nagentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC),\nwhere models refine their understanding through self-generated clarification\nquestions and corresponding contextual groundings. By scaling inference as a\ntree search where each node represents a CoC step, we achieve 97.8% answer\nrecall on NarrativeQA with a search depth of up to three and a branching factor\nof eight. To amortize the high cost of this search process to training, we\nleverage the preference pairs for each step obtained by the CoC workflow and\nperform two-stage model finetuning: (1) supervised finetuning to learn\neffective decomposition strategies, and (2) direct preference optimization to\nenhance reasoning quality. This enables AgenticLU models to generate\nclarifications and retrieve relevant context effectively and efficiently in a\nsingle inference pass. Extensive experiments across seven long-context tasks\ndemonstrate that AgenticLU significantly outperforms state-of-the-art prompting\nmethods and specialized long-context LLMs, achieving robust multi-hop reasoning\nwhile sustaining consistent performance as context length grows.",
        "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks. However, existing approaches mainly rely on imitation\nlearning and struggle to achieve effective test-time scaling. While\nreinforcement learning (RL) holds promise for enabling self-exploration and\nlearning from feedback, recent attempts yield only modest improvements in\ncomplex reasoning. In this paper, we present T1 to scale RL by encouraging\nexploration and understand inference scaling. We first initialize the LLM using\nsynthesized chain-of-thought data that integrates trial-and-error and\nself-verification. To scale RL training, we promote increased sampling\ndiversity through oversampling. We further employ an entropy bonus as an\nauxiliary loss, alongside a dynamic anchor for regularization to facilitate\nreward optimization. We demonstrate that T1 with open LLMs as its base exhibits\ninference scaling behavior and achieves superior performance on challenging\nmath reasoning benchmarks. For example, T1 with Qwen2.5-32B as the base model\noutperforms the recent Qwen QwQ-32B-Preview model on MATH500, AIME2024, and\nOmni-math-500. More importantly, we present a simple strategy to examine\ninference scaling, where increased inference budgets directly lead to T1's\nbetter performance without any additional verification. We will open-source the\nT1 models and the data used to train them at \\url{https:\/\/github.com\/THUDM\/T1}.",
        "We propose a novel reinforcement learning framework for post training large\nlanguage models that does not rely on human in the loop feedback. Instead, our\napproach uses cross attention signals within the model itself to derive a self\nsupervised reward, thereby guiding iterative fine tuning of the model policy.\nBy analyzing how the model attends to the input prompt during generation, we\nconstruct measures of prompt coverage, focus, and coherence. We then use these\nmeasures to rank or score candidate responses, providing a reward signal that\nencourages the model to produce well aligned, on topic text. In empirical\ncomparisons against standard policy gradient methods and RL fine tuning with\nsynthetic preference models, our method shows significant gains in prompt\nrelevance and consistency over a non RL baseline. While it does not yet match\nthe performance of fully human supervised RLHF systems, it highlights an\nimportant direction for scaling alignment with minimal human labeling. We\nprovide a detailed analysis, discuss potential limitations, and outline future\nwork for combining cross-attention based signals with smaller amounts of human\nfeedback.",
        "Star clusters provide unique advantages for investigating Galactic spiral\narms, particularly due to their precise ages, positions, and kinematic\nproperties, which are further enhanced by ongoing updates from the astrometric\ndata. In this study, we employ the latest extensive catalogue of open clusters\nfrom Gaia DR3 to examine the positional deviations of clusters belonging to\ndifferent age groups. Additionally, we employ dynamical simulations to probe\nthe evolutionary behavior of spiral arm positions. Our analysis reveals an\nabsence of a theoretical age pattern in the spiral arms traced by open\nclusters, and the pattern speeds of the spiral arms are consistent with the\nrotation curve. Both of these results do not align with the predictions of\nquasi-stationary density wave theory, suggesting a more dynamic or transient\narm scenario for the Milky Way. From this perspective, combined with vertex\ndeviation estimates, it appears that the Local arm is in a state of growth. In\ncontrast, the Sagittarius-Carina arm and the Perseus arm exhibit opposing\ntrends. Consequently, we speculate that the Galactic stellar disk does not\nexhibit a grand-design spiral pattern with a fixed pattern speed, but rather\nmanifests as a multi-armed structure with arms that continuously emerge and\ndissipate.",
        "In this paper, we present a bias and sustainability focused investigation of\nAutomatic Speech Recognition (ASR) systems, namely Whisper and Massively\nMultilingual Speech (MMS), which have achieved state-of-the-art (SOTA)\nperformances. Despite their improved performance in controlled settings, there\nremains a critical gap in understanding their efficacy and equity in real-world\nscenarios. We analyze ASR biases w.r.t. gender, accent, and age group, as well\nas their effect on downstream tasks. In addition, we examine the environmental\nimpact of ASR systems, scrutinizing the use of large acoustic models on carbon\nemission and energy consumption. We also provide insights into our empirical\nanalyses, offering a valuable contribution to the claims surrounding bias and\nsustainability in ASR systems."
      ]
    }
  },
  {
    "id":2411.01375,
    "research_type":"basic",
    "start_id":"b15",
    "start_title":"Learning Parities with Neural Networks",
    "start_abstract":"In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Testing conditional independence of discrete distributions"
      ],
      "abstract":[
        "We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary."
      ],
      "categories":[
        "stat.CO"
      ]
    },
    "list":{
      "title":[
        "CoHiRF: A Scalable and Interpretable Clustering Framework for\n  High-Dimensional Data",
        "A look-up table algorithm to model radiation damage effects in Monte\n  Carlo events for HL-LHC experiments",
        "Consistent crust-core interpolation and its effect on non-radial neutron\n  star oscillations",
        "Some Bohr-type inequalities with two parameters for bounded analytic\n  functions",
        "Spatially resolved dust properties over 50 kpc in a hyperluminous galaxy\n  merger at $z = 4.6$",
        "Inductive Construction of Variational Quantum Circuit for Constrained\n  Combinatorial Optimization",
        "Safety in safe Bayesian optimization and its ramifications for control",
        "Sample and Map from a Single Convex Potential: Generation using\n  Conjugate Moment Measures",
        "Improving Student Self-Efficacy in Quantum Computing with the Qubit\n  Touchdown Board Game",
        "A linearly-implicit energy preserving scheme for geometrically nonlinear\n  mechanics based on non-canonical Hamiltonian formulations",
        "Colimits of internal categories",
        "A Bayesian Multivariate Spatial Point Pattern Model: Application to Oral\n  Microbiome FISH Image Data",
        "Chiral Magnetic Effect enhancement at lower collision energies",
        "Rotational beta expansions and Schmidt games",
        "Constructing the low-temperature phase diagram for the $2+p$-quantum\n  spin glass using the nonperturbative renormalization group",
        "Integrating Product Coefficients for Improved 3D LiDAR Data\n  Classification",
        "A note on rank $\\frac{3}{2}$ Liouville irregular block",
        "Rational points on the non-split Cartan modular curve of level 27 and\n  quadratic Chabauty over number fields",
        "Generalized Uncertainty Principle mimicking dynamical Dark Energy:\n  matter perturbations and gravitational wave data analysis",
        "Statistical analysis of Multipath Entanglement Purification in Quantum\n  Networks",
        "Singleshot Multispectral Imaging via a Chromatic Metalens Array",
        "Wrapped Floer homology and hyperbolic sets",
        "Momentum and Matter Matter for Axion Dark Matter Matters on Earth",
        "Magneto-$\\nu$: Heavy neutral lepton search using $^{241}$Pu $\\beta^-$\n  decays",
        "Charged rotating black holes in scalar multipolar universes",
        "Impact of Optic Nerve Tortuosity, Globe Proptosis, and Size on Retinal\n  Ganglion Cell Thickness Across General, Glaucoma, and Myopic Populations:\n  Insights from the UK Biobank",
        "Affine diffractive beam dividers",
        "Building Machine Learning Challenges for Anomaly Detection in Science",
        "Maxwell-Vlasov-Uehling-Uhlenbeck (VUU) Simulation for Coupled\n  Laser-Electron Dynamics in a Metal Irradiated by Ultrashort Intense Laser\n  Pulses"
      ],
      "abstract":[
        "Clustering high-dimensional data poses significant challenges due to the\ncurse of dimensionality, scalability issues, and the presence of noisy and\nirrelevant features. We propose Consensus Hierarchical Random Feature (CoHiRF),\na novel clustering method designed to address these challenges effectively.\nCoHiRF leverages random feature selection to mitigate noise and dimensionality\neffects, repeatedly applies K-Means clustering in reduced feature spaces, and\ncombines results through a unanimous consensus criterion. This iterative\napproach constructs a cluster assignment matrix, where each row records the\ncluster assignments of a sample across repetitions, enabling the identification\nof stable clusters by comparing identical rows. Clusters are organized\nhierarchically, enabling the interpretation of the hierarchy to gain insights\ninto the dataset. CoHiRF is computationally efficient with a running time\ncomparable to K-Means, scalable to massive datasets, and exhibits robust\nperformance against state-of-the-art methods such as SC-SRGF, HDBSCAN, and\nOPTICS. Experimental results on synthetic and real-world datasets confirm the\nmethod's ability to reveal meaningful patterns while maintaining scalability,\nmaking it a powerful tool for high-dimensional data analysis.",
        "Radiation damage significantly impacts the performance of silicon tracking\ndetectors in Large Hadron Collider (LHC) experiments such as ATLAS and CMS,\nwith signal reduction being the most critical effect. Adjusting sensor bias\nvoltage and detection thresholds can help mitigate these effects, but\ngenerating simulated data that accurately mirror the performance evolution with\nthe accumulation of luminosity, hence fluence, is crucial. The ATLAS\ncollaboration has developed and implemented algorithms to correct simulated\nMonte Carlo (MC) events for radiation damage effects, achieving impressive\nagreement between collision data and simulated events. In preparation for the\nhigh-luminosity phase (HL-LHC), the demand for a faster ATLAS MC production\nalgorithm becomes imperative due to escalating collision, events, tracks, and\nparticle hit rates, imposing stringent constraints on available computing\nresources. This article outlines the philosophy behind the new algorithm, its\nimplementation strategy, and the essential components involved. The results\nfrom closure tests indicate that the events simulated using the new algorithm\nagree with fully simulated events at the level of few \\%. The first tests on\ncomputing performance show that the new algorithm is as fast as it is when no\nradiation damage corrections are applied.",
        "To model the structure of neutron stars (NSs) theoretically,it is common to\nconsider layers with different density regimes. Matching the equation of state\n(EoS) for the crust and core and obtaining a suitable description of these\nextreme conditions are crucial for understanding the properties of these\ncompact objects. In this work, we construct ten different NS EoSs incorporating\nthree distinct crust models, which are connected to the core using a\nthermodynamically and causally consistent formalism. For cold NSs, we propose a\nlinear relationship between pressure and energy density in a narrow region\nbetween the crust and core, effectively establishing an interpolation function\nin the pressure-baryonic chemical potential plane. We then compare this EoS\nmatching method with the classical approach, which neglects causal and\nthermodynamic consistency. We solve the Tolman-Oppenheimer-Volkoff equation to\nobtain the mass-radius relationship and compare our results with observational\nconstraints on NSs. Furthermore, we investigate the influence of the new\nmatching formalism on non-radial oscillation frequencies and damping times. Our\nfindings suggest that the method used to glue the crust and core EoS impacts NS\nobservables, such as the radius, oscillation frequencies, and damping times of\nnon-radial modes, which may be crucial for interpreting future gravitational\nwave observations from neutron star mergers or isolated pulsars. The effects\nare particularly noticeable for low-mass NSs, regardless of the specific EoS\nmodel chosen. In particular, we find that the $p_1$ oscillation mode exhibits\nsignificant differences in frequencies among alternative matching methods,\nwhereas the fundamental $f$-mode remains unaffected by changes in crust models\nor interpolation schemes.",
        "In this article, some Bohr inequalities for analytical functions on the unit\ndisk are generalized to the forms with two parameters. One of our results is\nsharp.",
        "We present spatially resolved dust-continuum ALMA observations from\nrest-frame $\\sim$60 to $\\sim$600 $\\mu$m (bands 3-10) of the hyperluminous hot\ndust-obscured galaxy (hot DOG) WISE J224607.6-052634.9 (W2246-0526), at\nredshift $z=4.6$. W2246-0526 is interacting with at least three companion\ngalaxies, forming a system connected by tidal streams. We model the\nmultiwavelength ALMA observations of the dust continuum using a modified\nblackbody, from which we derive the dust properties (mass, emissivity index,\narea of the emitting region, and temperature) in the hot DOG and resolved\nstructures across a region of nearly $\\sim$50 kpc. The peak temperature at the\nlocation of the hot DOG, $\\sim$110 K, is likely the consequence of heating by\nthe central quasar. The dust temperature drops to $\\sim$40 K at a radius of\n$\\sim$8 kpc, suggesting that heating by the quasar beyond that distance is\nnondominant. The dust in the connecting streams between the host and companion\ngalaxies is at temperatures between 30-40 K, typical of starburst galaxies,\nsuggesting it is most likely heated by recent, in-situ star formation. This is\nthe first time dust properties are spatially resolved over several tens of kpc\nin a galaxy system beyond Cosmic Noon --this is more than six times the scales\npreviously probed in galaxies at those redshifts.",
        "In this study, we propose a new method for constrained combinatorial\noptimization using variational quantum circuits. Quantum computers are\nconsidered to have the potential to solve large combinatorial optimization\nproblems faster than classical computers. Variational quantum algorithms, such\nas Variational Quantum Eigensolver (VQE), have been studied extensively because\nthey are expected to work on noisy intermediate scale devices. Unfortunately,\nmany optimization problems have constraints, which induces infeasible solutions\nduring VQE process. Recently, several methods for efficiently solving\nconstrained combinatorial optimization problems have been proposed by designing\na quantum circuit so as to output only the states that satisfy the constraints.\nHowever, the types of available constraints are still limited. Therefore, we\nhave started to develop variational quantum circuits that can handle a wider\nrange of constraints. The proposed method utilizes a forwarding operation that\nmaps from feasible states for subproblems to those for larger subproblems. As\nlong as appropriate forwarding operations can be defined, iteration of this\nprocess can inductively construct variational circuits outputting feasible\nstates even in the case of multiple and complex constraints. In this paper, the\nproposed method was applied to facility location problem and was found to\nincrease the probability for measuring feasible solutions or optimal solutions.\nIn addition, the cost of the obtained circuit was comparable to that of\nconventional variational circuits.",
        "A recurring and important task in control engineering is parameter tuning\nunder constraints, which conceptually amounts to optimization of a blackbox\nfunction accessible only through noisy evaluations. For example, in control\npractice parameters of a pre-designed controller are often tuned online in\nfeedback with a plant, and only safe parameter values should be tried, avoiding\nfor example instability. Recently, machine learning methods have been deployed\nfor this important problem, in particular, Bayesian optimization (BO). To\nhandle safety constraints, algorithms from safe BO have been utilized,\nespecially SafeOpt-type algorithms, which enjoy considerable popularity in\nlearning-based control, robotics, and adjacent fields. However, we identify two\nsignificant obstacles to practical safety. First, SafeOpt-type algorithms rely\non quantitative uncertainty bounds, and most implementations replace these by\ntheoretically unsupported heuristics. Second, the theoretically valid\nuncertainty bounds crucially depend on a quantity - the reproducing kernel\nHilbert space norm of the target function - that at present is impossible to\nreliably bound using established prior engineering knowledge. By careful\nnumerical experiments we show that these issues can indeed cause safety\nviolations. To overcome these problems, we propose Lipschitz-only Safe Bayesian\nOptimization (LoSBO), a safe BO algorithm that relies only on a known Lipschitz\nbound for its safety. Furthermore, we propose a variant (LoS-GP-UCB) that\navoids gridding of the search space and is therefore applicable even for\nmoderately high-dimensional problems.",
        "A common approach to generative modeling is to split model-fitting into two\nblocks: define first how to sample noise (e.g. Gaussian) and choose next what\nto do with it (e.g. using a single map or flows). We explore in this work an\nalternative route that ties sampling and mapping. We find inspiration in moment\nmeasures, a result that states that for any measure $\\rho$ supported on a\ncompact convex set of $\\mathbb{R}^d$, there exists a unique convex potential\n$u$ such that $\\rho=\\nabla u\\,\\sharp\\,e^{-u}$. While this does seem to tie\neffectively sampling (from log-concave distribution $e^{-u}$) and action\n(pushing particles through $\\nabla u$), we observe on simple examples (e.g.,\nGaussians or 1D distributions) that this choice is ill-suited for practical\ntasks. We study an alternative factorization, where $\\rho$ is factorized as\n$\\nabla w^*\\,\\sharp\\,e^{-w}$, where $w^*$ is the convex conjugate of $w$. We\ncall this approach conjugate moment measures, and show far more intuitive\nresults on these examples. Because $\\nabla w^*$ is the Monge map between the\nlog-concave distribution $e^{-w}$ and $\\rho$, we rely on optimal transport\nsolvers to propose an algorithm to recover $w$ from samples of $\\rho$, and\nparameterize $w$ as an input-convex neural network.",
        "Qubit Touchdown is a two-player, competitive board game that was developed to\nintroduce students to quantum computing. A quantum computer is a new kind of\ncomputer that is based on the laws of quantum physics, and it can solve certain\nproblems faster than normal computers because it follows a different set of\nrules. Qubit Touchdown's game play mirrors the rules of (American) football,\nwith players taking turns moving the football to score the most touchdowns, and\nno knowledge of quantum computing is needed to play the game. We evaluated the\ngame with 107 public high school students in Precalculus, Advanced Placement\n(AP) Statistics, and\/or AP Physics 1 courses, assessing whether their interest\nin and self-efficacy toward quantum computing changed as a result of playing\nthe game and learning about its connections to quantum computing. We also\nassessed whether the game was easy to learn and enjoyable. We found that\nstudents' self-efficacy was improved by 33.4%, and they widely considered the\ngame accessible and fun. Thus, Qubit Touchdown could be an effective resource\nto introduce students to Quantum Computing and boost their confidence in\nlearning about the field. Free printables of the game are available, and\nprofessionally produced copies can be purchased on demand.",
        "This work presents a novel formulation and numerical strategy for the\nsimulation of geometrically nonlinear structures. First, a non-canonical\nHamiltonian (Poisson) formulation is introduced by including the dynamics of\nthe stress tensor. This framework is developed for von-K\\'arm\\'an\nnonlinearities in beams and plates, as well as finite strain elasticity with\nSaint-Venant material behavior. In the case of plates, both negligible and\nnon-negligible membrane inertia are considered. For the former case the\ntwo-dimensional elasticity complex is leveraged to express the dynamics in\nterms of the Airy stress function. The finite element discretization employs a\nmixed approach, combining a conforming approximation for displacement and\nvelocity fields with a discontinuous stress tensor representation. A staggered,\nlinear implicit time integration scheme is proposed, establishing connections\nwith existing explicit-implicit energy-preserving methods. The stress degrees\nof freedom are statically condensed, reducing the computational complexity to\nsolving a system with a positive definite matrix. The methodology is validated\nthrough numerical experiments on the Duffing oscillator, a von-K\\'arm\\'an beam,\nand a column undergoing finite strain elasticity. Comparisons with fully\nimplicit energy-preserving method and the explicit Newmark scheme demonstrate\nthat the proposed approach achieves superior accuracy while maintaining energy\nstability. Additionally, it enables larger time steps compared to explicit\nschemes and exhibits computational efficiency comparable to the leapfrog\nmethod.",
        "We show that for a list-arithmetic pretopos $\\mathcal{E}$ with pullback\nstable coequalisers, the $2$-category $\\mathbf{Cat}(\\mathcal{E})$ of internal\ncategories, functors and natural transformations has finite $2$-colimits.",
        "Advances in cellular imaging technologies, especially those based on\nfluorescence in situ hybridization (FISH) now allow detailed visualization of\nthe spatial organization of human or bacterial cells. Quantifying this spatial\norganization is crucial for understanding the function of multicellular tissues\nor biofilms, with implications for human health and disease. To address the\nneed for better methods to achieve such quantification, we propose a flexible\nmultivariate point process model that characterizes and estimates complex\nspatial interactions among multiple cell types. The proposed Bayesian framework\nis appealing due to its unified estimation process and the ability to directly\nquantify uncertainty in key estimates of interest, such as those of inter-type\ncorrelation and the proportion of variance due to inter-type relationships. To\nensure stable and interpretable estimation, we consider shrinkage priors for\ncoefficients associated with latent processes. Model selection and comparison\nare conducted by using a deviance information criterion designed for models\nwith latent variables, effectively balancing the risk of overfitting with that\nof oversimplifying key quantities. Furthermore, we develop a hierarchical\nmodeling approach to integrate multiple image-specific estimates from a given\nsubject, allowing inference at both the global and subject-specific levels. We\napply the proposed method to microbial biofilm image data from the human tongue\ndorsum and find that specific taxon pairs, such as Streptococcus\nmitis-Streptococcus salivarius and Streptococcus mitis-Veillonella, exhibit\nstrong positive spatial correlations, while others, such as Actinomyces-Rothia,\nshow slight negative correlations. For most of the taxa, a substantial portion\nof spatial variance can be attributed to inter-taxon relationships.",
        "We extend previous holographic studies of the Chiral Magnetic Effect (CME) by\nincorporating a time-dependent magnetic field. Various magnetic field profiles\nproposed in the literature are implemented, and their impact on the CME signal\nis analyzed in both static and expanding backgrounds. Interestingly, the\nintegrated chiral magnetic current can exhibit a non-monotonic dependence on\nthe collision energy. Our results suggest that the CME signal is enhanced at\ncollision energies below $\\sqrt{s}=200$ GeV. In addition, we derive a\nquasi-equilibrium formula for the chiral magnetic effect in the expanding\nbackground that is valid at late times.",
        "We consider rotational beta expansions in dimensions 1, 2 and 4 and view them\nas expansions on real numbers, complex numbers, and quaternions, respectively.\nWe give sufficient conditions on the parameters $\\alpha, \\beta \\in (0,1)$ so\nthat particular cylinder sets arising from the expansions are winning or losing\nSchmidt $(\\alpha,\\beta)$-game.",
        "In this paper, we use a nonperturbative renormalization group approach to\nconstruct the dynamical phase space of a quantum spin glass in the large $N$\nlimit. The disordered Hamiltonian is of ``$2 + p$\" type, and we perform a\ncoarse-graining procedure over the Wigner spectrum for the matrix-like\ndisorder. The phase space reconstruction relies on phase transitions derived\nfrom the Luttinger-Ward functional, which accounts for interactions that are\nforbidden by perturbation theory. Various phases are identified, characterized\nby large correlations between replicas and\/or the breaking of time translation\nsymmetry.",
        "In this paper, we address the enhancement of classification accuracy for 3D\npoint cloud Lidar data, an optical remote sensing technique that estimates the\nthree-dimensional coordinates of a given terrain. Our approach introduces\nproduct coefficients, theoretical quantities derived from measure theory, as\nadditional features in the classification process. We define and present the\nformulation of these product coefficients and conduct a comparative study,\nusing them alongside principal component analysis (PCA) as feature inputs.\nResults demonstrate that incorporating product coefficients into the feature\nset significantly improves classification accuracy within this new framework.",
        "This paper focuses on a conformal block with rank $\\frac{3}{2}$ irregular\nsingularity which corresponds to the prepotential of the ${\\cal H}_1$\nArgyres-Douglas theory in $\\Omega$ background. We derive this irregular\nconformal block using generalized holomorphic anomaly recursion relation. This\nresults is an expression which is a power series in $\\Omega$-background\nparameters $\\epsilon_{1,2}$ and exact in coupling. We have verified that in\nsmall coupling regime our result is consistent with previously known\nexpressions.\n  Furthermore we derive the Deformed Seiberg-Witten curve which provides an\nalternative tool to explore above mentioned theory in Nekrasov-Shatashvili\nlimit of $\\Omega$-background. We checked that the results are in complete\nagreement with the holomorphic anomaly approach.",
        "Thanks to work of Rouse, Sutherland, and Zureick-Brown, it is known exactly\nwhich subgroups of GL$_2(\\mathbf{Z}_3)$ can occur as the image of the $3$-adic\nGalois representation attached to a non-CM elliptic curve over $\\mathbf{Q}$,\nwith a single exception: the normaliser of the non-split Cartan subgroup of\nlevel 27. In this paper, we complete the classification of 3-adic Galois images\nby showing that the normaliser of the non-split Cartan subgroup of level 27\ncannot occur as a 3-adic Galois image of a non-CM elliptic curve.\n  Our proof proceeds via computing the $\\mathbf{Q}(\\zeta_3)$-rational points on\na certain smooth plane quartic curve $X'_H$ (arising as a quotient of the\nmodular curve $X_{ns}^+(27)$) defined over $\\mathbf{Q}(\\zeta_3)$ whose Jacobian\nhas Mordell--Weil rank 6. To this end, we describe how to carry out the\nquadratic Chabauty method for a modular curve $X$ defined over a number field\n$F$, which, when applicable, determines a finite subset of\n$X(F\\otimes\\mathbf{Q}_p)$ in certain situations of larger Mordell--Weil rank\nthan previously considered. Together with an analysis of local heights above 3,\nwe apply this quadratic Chabauty method to determine\n$X'_H(\\mathbf{Q}(\\zeta_3))$. This allows us to compute the set\n$X_{ns}^+(27)(\\mathbf{Q})$, finishing the classification of 3-adic images of\nGalois.",
        "The Generalized Uncertainty Principle (GUP) stands out as a nearly ubiquitous\nfeature in quantum gravity modeling, predicting the emergence of a minimum\nlength at the Planck scale. Recently, it has been shown to modify the area-law\nscaling of the Bekenstein-Hawking entropy, giving rise to deformed Friedmann\nequations within Jacobson's approach. The ensuing model incorporates the GUP\ncorrection as a quintessence-like dark energy that supplements the cosmological\nconstant, influencing the dynamics of the early Universe while aligning with\nthe $\\Lambda$CDM paradigm in the current epoch. In this extended scenario, we\nexamine the growth of matter perturbations and structure formation employing\nthe Top-Hat Spherical Collapse approach. Our analysis reveals that the profile\nof the density contrast is sensitive to the GUP parameter $\\beta$, resulting in\na slower gravitational evolution of primordial fluctuations in the matter\ndensity. We also discuss implications for the relic density of Primordial\nGravitational Waves (PGWs), identifying the parameter space that enhances the\nPGW spectrum. Using the sensitivity of the next-generation GW observatories in\nthe frequency range below $10^3\\,\\mathrm{Hz}$, we constrain\n$\\beta\\lesssim10^{39}$, which is more stringent than most other\ncosmological\/astrophysical limits. This finding highlights the potential role\nof GWs in the pursuit of understanding quantum gravity phenomenology.",
        "In quantum networks, a set of entangled states distributed over multiple,\nalternative, distinct paths between a pair of source-destination nodes can be\npurified to obtain a higher fidelity entangled state between the nodes. This\nmultipath entanglement purification (MP-EP) strategy can exploit the network's\ncomplex structure to strengthen the entanglement connection between node pairs\nseparated by appropriate graph distances. We investigate the network scenarios\nin which MP-EP outperforms entanglement distribution over single network paths\nutilising a statistical model of a quantum network and find that MP-EP can be\nan effective entanglement distribution strategy over a range of node\nseparations determined by the average edge fidelities and probabilities of the\nnetwork. We find that MP-EP can bost the entanglement connection between\nsuitably separated node pairs to reach fidelities sufficient for a given\nquantum task thereby increasing the functionality of a quantum network.\nFurther, we provide statistical criteria in terms of network parameters that\ncan determine the regions of the network where MP-EP can be a useful\nentanglement distribution strategy.",
        "Real time, singleshot multispectral imaging systems are crucial for\nenvironment monitoring and biomedical imaging. Most singleshot multispectral\nimagers rely on complex computational backends, which precludes real time\noperations. In this work, we leverage the spectral selectivity afforded by\nengineered photonic materials to perform bulk of the multispectral data\nextraction in the optical domain, thereby circumventing the need for heavy\nbackend computation. We use our imager to extract multispectral data for two\nreal world objects at 8 predefined spectral channels in the 400 to 900 nm\nwavelength range. For both objects, an RGB image constructed using extracted\nmultispectral data shows good agreement with an image taken using a phone\ncamera, thereby validating our imaging approach. We believe that the proposed\nsystem can provide new avenues for the development of highly compact and low\nlatency multispectral imaging technologies.",
        "In this paper, we continue the quest to understand the interplay between\nwrapped Floer homology barcode and topological entropy. Wrapped Floer homology\nbarcode entropy is defined as the exponential growth, with respect to the left\nendpoints, of the number of not-too-short bars in its barcode. We prove that,\nin the presence of a topologically transitive, locally maximal hyperbolic set\nfor the Reeb flow on the boundary of a Liouville domain, the barcode entropy is\nbounded from below by the topological entropy restricted to the hyperbolic set.",
        "We investigate the implications of matter effects to searches for axion Dark\nMatter on Earth. The finite momentum of axion Dark Matter is crucial to\nelucidating the effects of Earth on both the axion Dark Matter field value and\nits gradient. We find that experiments targeting axion couplings compatible\nwith canonical solutions of the strong CP puzzle are likely not affected by\nEarth's matter effects. However, experiments sensitive to lighter axions with\nstronger couplings can be significantly affected, with a significant part of\nthe parameter space suffering from a reduced axion field value, and therefore\ndecreased experimental sensitivity. In contrast, the spatial gradient of the\naxion field can be enhanced along Earth's radial direction, with important\nimplications for ongoing and planned experiments searching for axion Dark\nMatter.",
        "We present experimental beta spectra of $^{241}$Pu as part of the\nMagneto-$\\nu$ experiment aimed at searching for keV heavy neutral leptons\n(HNLs). Total 200 million beta decays are measured by metallic magnetic\ncalorimeters (MMCs), representing the highest statistical precision achieved\nfor $^{241}$Pu to date. The end-point energy of $^{241}$Pu beta decay is\nmeasured via in-situ alpha calibration, yielding a value of 21.52(2) keV. The\ndifferential decay rate of the first forbidden non-unique transition of\n$^{241}$Pu is described with a quadratic shape correction factor $C(w) = 1 -\n1.931w + 0.940w^2$. Using these high-statistics spectra, we set an upper limit\non the admixture of a 10.5\\,keV sterile neutrino to the electron neutrino at\n$|U_{e4}|^2 < 2 \\times 10^{-3}$ for HNL mass of 10.5\\,keV.",
        "Recently, Cardoso and Nat\\'ario [6] constructed an exact solution of\nEinstein-scalar field equations that describes a scalar counterpart of the\nSchwarzschild-Melvin Universe. In fact, this solution belongs to a more general\nclass of solutions described by Herdeiro in [7]. In this work we show how to\nfurther generalize these solutions in presence of rotation and various charges.\nMore specifically, we describe the general charged rotating black holes with\nNUT charge and present some of their properties.",
        "Purpose: To investigate the impact of optic nerve tortuosity (ONT), and the\ninteraction of globe proptosis and globe size on retinal ganglion cell (RGC)\nthickness, using Retinal Nerve Fiber Layer (RNFL) thickness, across general,\nglaucoma, and myopic populations. Methods: We analyzed 17,970 eyes from the\nUKBiobank cohort (ID 76442), including 371 glaucoma and 2481 myopic eyes. AI\nmodels segmented structures from 3D optical coherence tomography (OCT) scans\nand magnetic resonance images (MRI). RNFL thickness was derived from OCT scans\nand corrected for ocular magnification, was derived from OCT. From MRIs, we\nextracted: ONT, globe proptosis, axial length, and a novel interzygomatic\nline-to-posterior pole (ILPP) distance, a composite marker of globe proptosis\nand size. GEE models assessed associations between orbital and retinal features\nacross all populations. Results: Segmentation models achieved Dice coefficients\nover 0.94 for both MRI and OCT. RNFL thickness was positively correlated with\nboth ONT and ILPP distance (r = 0.065, p < 0.001, and r = 0.206, p < 0.001\nrespectively). The same was true for glaucoma (r = 0.140, p < 0.01, and r =\n0.256, p < 0.01), and for myopia (r = 0.071, p < 0.001, and r = 0.100, p <\n0.0001). GEE models revealed straighter optic nerves and shorter ILPP distance\nas predictive of thinner RNFL in all populations. Conclusions: This study\nemphasizes the impact of ONT, globe size, and proptosis on retinal health,\nsuggesting RNFL thinning may arise from biomechanical stress due to straighter\noptic nerves or reduced ILPP distance, particularly in glaucoma or myopia. The\nnovel ILPP metric, integrating globe size and position, shows potential as a\nbiomarker for axonal health. These findings highlight the role of orbit\nstructures in RGC axonal health and warrant further exploration of the\nbiomechanical relationship between the orbit and optic nerve.",
        "Diffractive optical elements that divide an input beam into a set of replicas\nare used in many optical applications ranging from image processing to\ncommunications. Their design requires time-consuming optimization processes,\nwhich, for a given number of generated beams, are to be separately treated for\none-dimensional and two-dimensional cases because the corresponding optimal\nefficiencies may be different. After generalizing their Fourier treatment, we\nprove that, once a particular divider has been designed, its transmission\nfunction can be used to generate numberless other dividers through affine\ntransforms that preserve the efficiency of the original element without\nrequiring any further optimization.",
        "Scientific discoveries are often made by finding a pattern or object that was\nnot predicted by the known rules of science. Oftentimes, these anomalous events\nor objects that do not conform to the norms are an indication that the rules of\nscience governing the data are incomplete, and something new needs to be\npresent to explain these unexpected outliers. The challenge of finding\nanomalies can be confounding since it requires codifying a complete knowledge\nof the known scientific behaviors and then projecting these known behaviors on\nthe data to look for deviations. When utilizing machine learning, this presents\na particular challenge since we require that the model not only understands\nscientific data perfectly but also recognizes when the data is inconsistent and\nout of the scope of its trained behavior. In this paper, we present three\ndatasets aimed at developing machine learning-based anomaly detection for\ndisparate scientific domains covering astrophysics, genomics, and polar\nscience. We present the different datasets along with a scheme to make machine\nlearning challenges around the three datasets findable, accessible,\ninteroperable, and reusable (FAIR). Furthermore, we present an approach that\ngeneralizes to future machine learning challenges, enabling the possibility of\nlarge, more compute-intensive challenges that can ultimately lead to scientific\ndiscovery.",
        "The description of electron-electron scattering presents challenges in the\nmicroscopic modeling of the interaction of ultrashort intense laser pulses with\nsolids. We extend the semiclassical approach based on the Vlasov equation\n[Phys. Rev. B 104, 075157(2021)] to account for dynamic electron-electron\nscattering by introducing the Vlasov-Uehling-Uhlenbeck (VUU) equation. We\nfurther couple the VUU equation with Maxwell's equations to describe the laser\npulse propagation. We apply the present approach to simulate laser-electron\ninteractions in bulk and thin-film aluminum, focusing on energy absorption and\ntransport. Our calculation results reveal that electron-electron scattering\naffects energy absorption more significantly under p-polarization than under\ns-polarization, highlighting the role of the non-uniform surface potential. Our\nsimulations also show that the energy transport extends beyond the optical\npenetration depth, which is consistent with observations in previous laser\nablation experiments. The developed Maxwell-VUU approach is expected to advance\nthe understanding of intense laser-material interactions not only as a\ncost-effective alternative to the time-dependent density functional theory\n(TDDFT), but also by incorporating fermionic two-body collisions whose\ndescription is limited in TDDFT."
      ]
    }
  },
  {
    "id":2411.01375,
    "research_type":"basic",
    "start_id":"b11",
    "start_title":"Testing conditional independence of discrete distributions",
    "start_abstract":"We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary.",
    "start_categories":[
      "stat.CO"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Learning Parities with Neural Networks"
      ],
      "abstract":[
        "In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by"
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Strain Problems got you in a Twist? Try StrainRelief: A Quantum-Accurate\n  Tool for Ligand Strain Calculations",
        "Towards Fair and Robust Face Parsing for Generative AI: A\n  Multi-Objective Approach",
        "On the generalized eigenvalue problem in subspace-based excited state\n  methods for quantum computers",
        "Measuring Similarity in Causal Graphs: A Framework for Semantic and\n  Structural Analysis",
        "Hyperparameters in Score-Based Membership Inference Attacks",
        "Tensor renormalization group study of the two-dimensional lattice U(1)\n  gauge-Higgs model with a topological $\\theta$ term under L\\\"uscher's\n  admissibility condition",
        "Nonflat bands and chiral symmetry in magic-angle twisted bilayer\n  graphene",
        "Effect of imaginary gauge on wave transport in driven-dissipative\n  systems",
        "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
        "Explainable Reinforcement Learning via Temporal Policy Decomposition",
        "Towards Change Impact Analysis in Microservices-based System Evolution",
        "RIO EPICS device support application case study on an ion source control\n  system (ISHP)",
        "Migration of phthalate plasticisers in heritage objects made of\n  poly(vinyl chloride): mechanical and environmental aspects",
        "Near-extremal dumb holes and some aspects of the Hawking effect",
        "Can Safety Fine-Tuning Be More Principled? Lessons Learned from\n  Cybersecurity",
        "Fusion of Indirect Methods and Iterative Learning for Persistent\n  Velocity Trajectory Optimization of a Sustainably Powered Autonomous Surface\n  Vessel",
        "Removing Atmospheric Carbon Dioxide Using Large Land Or Ocean Areas Will\n  Change Earth Albedo And Force Climate",
        "Efficient Quantum Frequency Conversion of Ultra-Violet Single Photons\n  from a Trapped Ytterbium Ion",
        "Construction of Simultaneously Good Polar Codes and Polar Lattices",
        "Convexification With the Viscocity Term for Electrical Impedance\n  Tomography",
        "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them",
        "Transfer Learning for EDFA Gain Modeling: A Semi-Supervised Approach\n  Using Internal Amplifier Features",
        "Rutherford Backscattering Spectrometry analysis of the formation of\n  superconducting V$_3$Si thin films",
        "Large Language Model-based Nonnegative Matrix Factorization For\n  Cardiorespiratory Sound Separation",
        "Foundation Models for CPS-IoT: Opportunities and Challenges",
        "Federated Digital Twin Construction via Distributed Sensing: A\n  Game-Theoretic Online Optimization with Overlapping Coalitions",
        "Enhanced Beampattern Synthesis Using Electromagnetically Reconfigurable\n  Antennas",
        "NAS-PINNv2: Improved neural architecture search framework for\n  physics-informed neural networks in low-temperature plasma simulation",
        "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache"
      ],
      "abstract":[
        "Ligand strain energy, the energy difference between the bound and unbound\nconformations of a ligand, is an important component of structure-based small\nmolecule drug design. A large majority of observed ligands in protein-small\nmolecule co-crystal structures bind in low-strain conformations, making strain\nenergy a useful filter for structure-based drug design. In this work we present\na tool for calculating ligand strain with a high accuracy. StrainRelief uses a\nMACE Neural Network Potential (NNP), trained on a large database of Density\nFunctional Theory (DFT) calculations to estimate ligand strain of neutral\nmolecules with quantum accuracy. We show that this tool estimates strain energy\ndifferences relative to DFT to within 1.4 kcal\/mol, more accurately than\nalternative NNPs. These results highlight the utility of NNPs in drug\ndiscovery, and provide a useful tool for drug discovery teams.",
        "Face parsing is a fundamental task in computer vision, enabling applications\nsuch as identity verification, facial editing, and controllable image\nsynthesis. However, existing face parsing models often lack fairness and\nrobustness, leading to biased segmentation across demographic groups and errors\nunder occlusions, noise, and domain shifts. These limitations affect downstream\nface synthesis, where segmentation biases can degrade generative model outputs.\nWe propose a multi-objective learning framework that optimizes accuracy,\nfairness, and robustness in face parsing. Our approach introduces a\nhomotopy-based loss function that dynamically adjusts the importance of these\nobjectives during training. To evaluate its impact, we compare multi-objective\nand single-objective U-Net models in a GAN-based face synthesis pipeline\n(Pix2PixHD). Our results show that fairness-aware and robust segmentation\nimproves photorealism and consistency in face generation. Additionally, we\nconduct preliminary experiments using ControlNet, a structured conditioning\nmodel for diffusion-based synthesis, to explore how segmentation quality\ninfluences guided image generation. Our findings demonstrate that\nmulti-objective face parsing improves demographic consistency and robustness,\nleading to higher-quality GAN-based synthesis.",
        "Solving challenging problems in quantum chemistry is one of the leading\npromised applications of quantum computers. Within the quantum algorithms\nproposed for problems in excited state quantum chemistry, subspace-based\nquantum algorithms, including quantum subspace expansion (QSE), quantum\nequation of motion (qEOM) and quantum self-consistent equation-of-motion\n(q-sc-EOM), are promising for pre-fault-tolerant quantum devices. The working\nequation of QSE and qEOM requires solving a generalized eigenvalue equation\nwith associated matrix elements measured on a quantum computer. Our\nquantitative analysis of the QSE method shows that the errors in eigenvalues\nincrease drastically with an increase in the condition number of the overlap\nmatrix when a generalized eigenvalue equation is solved in the presence of\nstatistical sampling errors. This makes such methods unstable to errors that\nare unavoidable when using quantum computers. Further, at very high condition\nnumbers of overlap matrix, the QSE's working equation could not be solved\nwithout any additional steps in the presence of sampling errors as it becomes\nill-conditioned. It was possible to use the thresholding technique in this case\nto solve the equation, but the solutions achieved had missing excited states,\nwhich may be a problem for future chemical studies. We also show that\nexcited-state methods that have an eigenvalue equation as the working equation,\nsuch as q-sc-EOM, do not have such problems and could be suitable candidates\nfor excited-state quantum chemistry calculations using quantum computers.",
        "Causal graphs are commonly used to understand and model complex systems.\nResearchers often construct these graphs from different perspectives, leading\nto significant variations for the same problem. Comparing causal graphs is,\ntherefore, essential for evaluating assumptions, integrating insights, and\nresolving disagreements. The rise of AI tools has further amplified this need,\nas they are increasingly used to generate hypothesized causal graphs by\nsynthesizing information from various sources such as prior research and\ncommunity inputs, providing the potential for automating and scaling causal\nmodeling for complex systems. Similar to humans, these tools also produce\ninconsistent results across platforms, versions, and iterations. Despite its\nimportance, research on causal graph comparison remains scarce. Existing\nmethods often focus solely on structural similarities, assuming identical\nvariable names, and fail to capture nuanced semantic relationships, which is\nessential for causal graph comparison. We address these gaps by investigating\nmethods for comparing causal graphs from both semantic and structural\nperspectives. First, we reviewed over 40 existing metrics and, based on\npredefined criteria, selected nine for evaluation from two threads of machine\nlearning: four semantic similarity metrics and five learning graph kernels. We\ndiscuss the usability of these metrics in simple examples to illustrate their\nstrengths and limitations. We then generated a synthetic dataset of 2,000\ncausal graphs using generative AI based on a reference diagram. Our findings\nreveal that each metric captures a different aspect of similarity, highlighting\nthe need to use multiple metrics.",
        "Membership Inference Attacks (MIAs) have emerged as a valuable framework for\nevaluating privacy leakage by machine learning models. Score-based MIAs are\ndistinguished, in particular, by their ability to exploit the confidence scores\nthat the model generates for particular inputs. Existing score-based MIAs\nimplicitly assume that the adversary has access to the target model's\nhyperparameters, which can be used to train the shadow models for the attack.\nIn this work, we demonstrate that the knowledge of target hyperparameters is\nnot a prerequisite for MIA in the transfer learning setting. Based on this, we\npropose a novel approach to select the hyperparameters for training the shadow\nmodels for MIA when the attacker has no prior knowledge about them by matching\nthe output distributions of target and shadow models. We demonstrate that using\nthe new approach yields hyperparameters that lead to an attack near\nindistinguishable in performance from an attack that uses target\nhyperparameters to train the shadow models. Furthermore, we study the empirical\nprivacy risk of unaccounted use of training data for hyperparameter\noptimization (HPO) in differentially private (DP) transfer learning. We find no\nstatistically significant evidence that performing HPO using training data\nwould increase vulnerability to MIA.",
        "We investigate the two-dimensional lattice U(1) gauge-Higgs model with a\ntopological term, employing L\\\"uscher's admissibility condition. The standard\nMonte Carlo simulation for this model is hindered not only by the complex\naction problem due to the topological term but also by the topological freezing\nproblem originating from the admissibility condition. Resolving both obstacles\nsimultaneously with the tensor renormalization group approach, we show the\nadvantage of the admissibility condition in dealing with the topological term\ndiscretized with the so-called field-theoretical definition.",
        "In this work, we study an interacting tight-binding model of magic-angle\ntwisted bilayer graphene (MATBG), with a twist angle of $1.05^\\circ$. We derive\neffective theories based on a mean-field normal state at charge neutrality,\nthereby including the renormalizations coming from integrating out high-energy\nmodes. In these theories, the flat bands display a sizable increase of the\nbandwidth, suggesting a renormalization of the magic angle. Additionally, the\ncorresponding wavefunctions flow towards the limit of perfect particle-hole\nsymmetry and sublattice polarization (the 'chiral' limit). We further represent\nthe flat bands in the 'vortex Chern' basis and discuss the implications on the\ndynamics regarding the 'flat' and 'chiral' symmetries of MATBG, as manifested\nin the symmetry-broken states at neutrality.",
        "Wave transport in disordered media is a fundamental problem with direct\nimplications in condensed matter, materials science, optics, atomic physics,\nand even biology. The majority of studies are focused on Hermitian systems to\nunderstand disorder-induced localization. However, recent studies of\nnon-Hermitian disordered media have revealed unique behaviors, with a universal\nprinciple emerging that links the eigenvalue spectrum of the disordered\nHamiltonian and its statistics with its transport properties. In this work we\nshow that the situation can be very different in driven-dissipative lattices of\ncavities, where a uniform gain applied equally to all the components of the\nsystem can act as a knob for controlling the wave transport properties without\naltering the eigenvalue statistics of the underlying Hamiltonian. Our results\nopen a new avenue for developing a deeper insight into the transport properties\nin disordered media and will aid in building new devices as well. Our work\nwhich is presented in the context of optics generalizes to any physical\nplatforms where gain can be implemented. These include acoustics, electronics,\nand coupled quantum oscillators such as atoms, diamond centers and\nsuperconducting qubits.",
        "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
        "We investigate the explainability of Reinforcement Learning (RL) policies\nfrom a temporal perspective, focusing on the sequence of future outcomes\nassociated with individual actions. In RL, value functions compress information\nabout rewards collected across multiple trajectories and over an infinite\nhorizon, allowing a compact form of knowledge representation. However, this\ncompression obscures the temporal details inherent in sequential\ndecision-making, presenting a key challenge for interpretability. We present\nTemporal Policy Decomposition (TPD), a novel explainability approach that\nexplains individual RL actions in terms of their Expected Future Outcome (EFO).\nThese explanations decompose generalized value functions into a sequence of\nEFOs, one for each time step up to a prediction horizon of interest, revealing\ninsights into when specific outcomes are expected to occur. We leverage\nfixed-horizon temporal difference learning to devise an off-policy method for\nlearning EFOs for both optimal and suboptimal actions, enabling contrastive\nexplanations consisting of EFOs for different state-action pairs. Our\nexperiments demonstrate that TPD generates accurate explanations that (i)\nclarify the policy's future strategy and anticipated trajectory for a given\naction and (ii) improve understanding of the reward composition, facilitating\nfine-tuning of the reward function to align with human expectations.",
        "Cloud-native systems are the mainstream for enterprise solutions, given their\nscalability, resilience, and other benefits. While the benefits of cloud-native\nsystems fueled by microservices are known, less guidance exists on their\nevolution. One could assume that since microservices encapsulate their code,\ncode changes remain encapsulated as well; however, the community is becoming\nmore aware of the possible consequences of code change propagation across\nmicroservices. Moreover, an active mitigation instrument for negative\nconsequences of change propagation across microservices (i.e., ripple effect)\nis yet missing, but the microservice community would greatly benefit from it.\nThis paper introduces what it could look like to have an infrastructure to\nassist with change impact analysis across the entire microservice system and\nintends to facilitate advancements in laying out the foundations and building\nguidelines on microservice system evolution. It shares a new direction for\nincremental software architecture reconstruction that could serve as the\ninfrastructure concept and demonstrates early results from prototyping to\nillustrate the potential impact.",
        "Experimental Physics and Industrial Control System (EPICS) is a software tool\nthat during last years has become relevant as a main framework to deploy\ndistributed control systems in large scientific environments. At the moment,\nESS Bilbao uses this middleware to perform the control of their Ion Source\nHydrogen Positive (ISHP) project. The implementation of the control system was\nbased on: PXI Real Time controllers using the LabVIEW-RT and LabVIEW-EPICS\ntools; and RIO devices based on Field-Programmable Gate Array (FPGA)\ntechnology. Intended to provide a full compliant EPICS IOCs for RIO devices and\nto avoid additional efforts on the system maintainability, a migration of the\ncurrent system to a derivative Red Hat Linux (CentOS) environment has been\nconducted. This paper presents a real application case study for using the\nNIRIO EPICS device support (NIRIO-EDS) to give support to the ISHP. Although\nRIO FPGA configurations are particular solutions for ISHP performance, the\nNIRIO-EDS has permitted the control and monitoring of devices by applying a\nwell-defined design methodology into the previous FPGA configuration for\nRIO\/FlexRIO devices. This methodology has permitted a fast and easy deployment\nfor the new robust, scalable and maintainable software to support RIO devices\ninto the ISHP control architecture.",
        "To clean or not to clean? The solution to this dilemma is related to\nunderstanding the plasticiser migration which has a few practical implications\nfor the state of museum artefacts made of plasticised poly(vinyl chloride) -\nPVC and objects stored in their vicinity. The consequences of this process\nencompass aesthetic changes due to the presence of exudates and dust\ndeposition, an increase in air pollution and the development of mechanical\nstresses. Therefore, this paper discusses the plasticiser migration in PVC to\nprovide evidence and support the development of recommendations and guidelines\nfor conservators, collection managers and heritage scientists. Particularly,\nthe investigation is focused on the migration of the ortho-phthalates\nrepresenting the group of the most abundant plasticisers in PVC collections.\nThe predominance of inner diffusion or surface emission (evaporation)\ndetermining the rate-limiting step of the overall migration process is\nconsidered a fundament for understanding the potential environmental and\nmechanical risk. According to this concept, general correlations for various\northo-phthalates are proposed depending on their molar mass with the support of\nmolecular dynamics simulations and NMR diffusometry. The study reveals that for\nthe majority of the PVC objects in collections, the risk of accelerated\nmigration upon mild removal of surface plasticiser exudate is low. Thus,\nsurface cleaning would allow for diminishing dust deposition and air pollution\nby phthalate-emitting objects in a museum environment. Bearing in mind\nsimplicity and the need for fast decision-supporting solutions, the\nstep-by-step protocol for non-destructive identification and quantification of\nplasticisers in objects made of or containing plasticised PVC, determination of\nthe physical state of investigated artefacts and rate-limiting process of\nplasticiser migration is proposed.",
        "We propose novel non-relativistic fluid analogue models, that is dumb hole\nmodels, for extremal and near-extremal black holes. Further we study the\nback-reaction effects of analogue Hawking radiation emitted from these dumb\nholes. We discuss and quantify the reduction in the background fluid velocity\ncaused by radiation of Hawking phonons. In doing so, we speculate on the\nexistence of an emergent Hawking force which leads to the reduction in the\nbackground fluid velocity and which is produced as a consequence of phonon\nemission. In addition to the analogue gravity literature, our results might be\nof relevance to black hole pedagogy.",
        "As LLMs develop increasingly advanced capabilities, there is an increased\nneed to minimize the harm that could be caused to society by certain model\noutputs; hence, most LLMs have safety guardrails added, for example via\nfine-tuning. In this paper, we argue the position that current safety\nfine-tuning is very similar to a traditional cat-and-mouse game (or arms race)\nbetween attackers and defenders in cybersecurity. Model jailbreaks and attacks\nare patched with bandaids to target the specific attack mechanism, but many\nsimilar attack vectors might remain. When defenders are not proactively coming\nup with principled mechanisms, it becomes very easy for attackers to sidestep\nany new defenses. We show how current defenses are insufficient to prevent new\nadversarial jailbreak attacks, reward hacking, and loss of control problems. In\norder to learn from past mistakes in cybersecurity, we draw analogies with\nhistorical examples and develop lessons learned that can be applied to LLM\nsafety. These arguments support the need for new and more principled approaches\nto designing safe models, which are architected for security from the\nbeginning. We describe several such approaches from the AI literature.",
        "In this paper, we present the methodology and results for a real-time\nvelocity trajectory optimization for a solar-powered autonomous surface vessel\n(ASV), where we combine indirect optimal control techniques with iterative\nlearning. The ASV exhibits cyclic operation due to the nature of the solar\nprofile, but weather patterns create inevitable disturbances in this profile.\nThe nature of the problem results in a formulation where the satisfaction of\npointwise-in-time state of charge constraints does not generally guarantee\npersistent feasibility, and the goal is to maximize information gathered over a\nvery long (ultimately persistent) time duration. To address these challenges,\nwe first use barrier functions to tighten pointwise-in-time state of charge\nconstraints by the minimal amount necessary to achieve persistent feasibility.\nWe then use indirect methods to derive a simple switching control law, where\nthe optimal velocity is shown to be an undetermined constant value during each\nconstraint-inactive time segment. To identify this optimal constant velocity\n(which can vary from one segment to the next), we employ an iterative learning\napproach. The result is a simple closed-form control law that does not require\na solar forecast. We present simulation-based validation results, based on a\nmodel of the SeaTrac SP-48 ASV and solar data from the North Carolina coast.\nThese simulation results show that the proposed methodology, which amounts to a\nclosed-form controller and simple iterative learning update law, performs\nnearly as well as a model predictive control approach that requires an accurate\nfuture solar forecast and significantly greater computational capability.",
        "When large surface areas of the Earth are altered, radiative forcing due to\nchanges in surface reflectance can drive climate change. Yet to achieve the\nnecessary scale to remove the substantial amounts of carbon dioxide from the\natmosphere relevant for ameliorating climate change, enhanced rock weathering\n(ERW) will need to be applied to very large land areas. Likewise, marine carbon\ndioxide removal (mCDR) must alter a large fraction of the ocean surface waters\nto have a significant impact upon climate. We show that surface albedo\nmodification (SAM) associated with ERW or mCDR can easily overwhelm the\nradiative forcing from the decrease of atmospheric CO2 over years or even\ndecades. A change in albedo as small as parts per thousand has a radiative\nimpact comparable to the removal of 10 tons of carbon per hectare. SAM via ERW\ncan be either cooling or warming. We identify some of the many questions raised\nby radiative forcing due to these forms of CDR.",
        "Ion trap system is a leading candidate for quantum network privileged by its\nlong coherence time, high-fidelity gate operations, and the ion-photon\nentanglement that generates an ideal pair of a stationary memory qubit and a\nflying communication qubit. Rapid developments in nonlinear quantum frequency\nconversion techniques have enhanced the potential for constructing a trapped\nion quantum network via optical fiber connections. The generation of\nlong-distance entanglement has been demonstrated with ions such as Ca$^{+}$ and\nBa$^{+}$, which emit photons in visible or near-infrared range naturally. On\nthe other hand, as the qubit-native photons reside in ultra-violet (UV)\nspectrum, the Yb$^{+}$ ion has not been considered as a strong competitor for\ntelecommunication qubits despite extensive research on it. Here, we demonstrate\nan efficient difference-frequency conversion of UV photons, emitted from a\ntrapped Yb$^{+}$ ion, into a visible range. We provide experimental evidence\nthat confirms the converted photons are radiated from the Yb$^{+}$ ion. Our\nresults provide a crucial step toward realizing a long-distance trapped ion\nquantum network based on Yb$^{+}$ ions through quantum frequency conversion.",
        "In this work, we investigate the simultaneous goodness of polar codes and\npolar lattices. The simultaneous goodness of a lattice or a code means that it\nis optimal for both channel coding and source coding simultaneously. The\nexistence of such kind of lattices was proven by using random lattice\nensembles. Our work provides an explicit construction based on the polarization\ntechnique.",
        "A version of the globally convergent convexification numerical method is\nconstructed for the problem of Electrical Impedance Tomography in the 2D case.\nAn important element of this version is the presence of the viscosity term.\nGlobal convergence analysis is carried out. Results of numerical experiments\nare presented.",
        "Despite their impressive ability to generate high-quality and fluent text,\ngenerative large language models (LLMs) also produce hallucinations: statements\nthat are misaligned with established world knowledge or provided input context.\nHowever, measuring hallucination can be challenging, as having humans verify\nmodel generations on-the-fly is both expensive and time-consuming. In this\nwork, we release HALoGEN, a comprehensive hallucination benchmark consisting\nof: (1) 10,923 prompts for generative models spanning nine domains including\nprogramming, scientific attribution, and summarization, and (2) automatic\nhigh-precision verifiers for each use case that decompose LLM generations into\natomic units, and verify each unit against a high-quality knowledge source. We\nuse this framework to evaluate ~150,000 generations from 14 language models,\nfinding that even the best-performing models are riddled with hallucinations\n(sometimes up to 86% of generated atomic facts depending on the domain). We\nfurther define a novel error classification for LLM hallucinations based on\nwhether they likely stem from incorrect recollection of training data (Type A\nerrors), or incorrect knowledge in training data (Type B errors), or are\nfabrication (Type C errors). We hope our framework provides a foundation to\nenable the principled study of why generative models hallucinate, and advances\nthe development of trustworthy large language models.",
        "The gain spectrum of an Erbium-Doped Fiber Amplifier (EDFA) has a complex\ndependence on channel loading, pump power, and operating mode, making accurate\nmodeling difficult to achieve. Machine Learning (ML) based modeling methods can\nachieve high accuracy, but they require comprehensive data collection. We\npresent a novel ML-based Semi-Supervised, Self-Normalizing Neural Network\n(SS-NN) framework to model the wavelength dependent gain of EDFAs using minimal\ndata, which achieve a Mean Absolute Error (MAE) of 0.07\/0.08 dB for\nbooster\/pre-amplifier gain prediction. We further perform Transfer Learning\n(TL) using a single additional measurement per target-gain setting to transfer\nthis model among 22 EDFAs in Open Ireland and COSMOS testbeds, which achieves a\nMAE of less than 0.19 dB even when operated across different amplifier types.\nWe show that the SS-NN model achieves high accuracy for gain spectrum\nprediction with minimal data requirement when compared with current benchmark\nmethods.",
        "Vanadium silicide, V$_3$Si, is a promising superconductor for silicon-based\nsuperconducting (SC) devices due to its compatibility with silicon substrates\nand its potential for integration into existing semiconductor technologies.\nHowever, to date there have been only a limited number of studies of the\nformation of SC V$_3$Si thin films and the associated structural and\nsuperconducting properties. This work aims to explore the structural\ncharacteristics and SC properties of V$_3$Si films, paving the way for the\ndevelopment of functional SC devices for quantum technology applications. We\nhave investigated the formation of V$_3$Si films by directly depositing\nvanadium (V) onto thermally grown SiO$_2$ on Si, followed by high-vacuum\nannealing to induce the phase transformation into V$_3$Si. Rutherford\nBackscattering Spectrometry(RBS) was employed throughout the sample growth\nprocess to analyze the material composition as a function of depth using a\n$^4He^+$ ion beam. Analysis of the RBS data confirmed that the V layer fully\nreacted with the SiO$_2$ substrate to form V$_3$Si at the interface, in\naddition to a vanadium oxide (VO$_x$) layer forming atop the V$_3$Si film. The\nthickness of the V$_3$Si layer ranges from 63 to 130 nm, with annealing\ntemperatures between 750$^\\circ$C and 800$^\\circ$C. A sharp SC transition was\nobserved at T$_c$ = 13 K in the sample annealed at 750$^\\circ$C, with a narrow\ntransition width ($\\Delta T_c$) of 0.6 K. Initial reactive ion etching (RIE)\nstudies yielded promising results for local removal of the (VO$_x$) to\nfacilitate electrical contact formation to the SC layer.",
        "This study represents the first integration of large language models (LLMs)\nwith non-negative matrix factorization (NMF), marking a novel advancement in\nthe source separation field. The LLM is employed in two unique ways: enhancing\nthe separation results by providing detailed insights for disease prediction\nand operating in a feedback loop to optimize a fundamental frequency penalty\nadded to the NMF cost function. We tested the algorithm on two datasets: 100\nsynthesized mixtures of real measurements, and 210 recordings of heart and lung\nsounds from a clinical manikin including both individual and mixed sounds,\ncaptured using a digital stethoscope. The approach consistently outperformed\nexisting methods, demonstrating its potential to significantly enhance medical\nsound analysis for disease diagnostics.",
        "Methods from machine learning (ML) have transformed the implementation of\nPerception-Cognition-Communication-Action loops in Cyber-Physical Systems (CPS)\nand the Internet of Things (IoT), replacing mechanistic and basic statistical\nmodels with those derived from data. However, the first generation of ML\napproaches, which depend on supervised learning with annotated data to create\ntask-specific models, faces significant limitations in scaling to the diverse\nsensor modalities, deployment configurations, application tasks, and operating\ndynamics characterizing real-world CPS-IoT systems. The success of\ntask-agnostic foundation models (FMs), including multimodal large language\nmodels (LLMs), in addressing similar challenges across natural language,\ncomputer vision, and human speech has generated considerable enthusiasm for and\nexploration of FMs and LLMs as flexible building blocks in CPS-IoT analytics\npipelines, promising to reduce the need for costly task-specific engineering.\n  Nonetheless, a significant gap persists between the current capabilities of\nFMs and LLMs in the CPS-IoT domain and the requirements they must meet to be\nviable for CPS-IoT applications. In this paper, we analyze and characterize\nthis gap through a thorough examination of the state of the art and our\nresearch, which extends beyond it in various dimensions. Based on the results\nof our analysis and research, we identify essential desiderata that CPS-IoT\ndomain-specific FMs and LLMs must satisfy to bridge this gap. We also propose\nactions by CPS-IoT researchers to collaborate in developing key community\nresources necessary for establishing FMs and LLMs as foundational tools for the\nnext generation of CPS-IoT systems.",
        "In this paper, we propose a novel federated framework for constructing the\ndigital twin (DT) model, referring to a living and self-evolving visualization\nmodel empowered by artificial intelligence, enabled by distributed sensing\nunder edge-cloud collaboration. In this framework, the DT model to be built at\nthe cloud is regarded as a global one being split into and integrating from\nmultiple functional components, i.e., partial-DTs, created at various edge\nservers (ESs) using feature data collected by associated sensors. Considering\ntime-varying DT evolutions and heterogeneities among partial-DTs, we formulate\nan online problem that jointly and dynamically optimizes partial-DT assignments\nfrom the cloud to ESs, ES-sensor associations for partial-DT creation, and as\nwell as computation and communication resource allocations for global-DT\nintegration. The problem aims to maximize the constructed DT's model quality\nwhile minimizing all induced costs, including energy consumption and\nconfiguration costs, in long runs. To this end, we first transform the original\nproblem into an equivalent hierarchical game with an upper-layer two-sided\nmatching game and a lower-layer overlapping coalition formation game. After\nanalyzing these games in detail, we apply the Gale-Shapley algorithm and\nparticularly develop a switch rules-based overlapping coalition formation\nalgorithm to obtain short-term equilibria of upper-layer and lower-layer\nsubgames, respectively. Then, we design a deep reinforcement learning-based\nsolution, called DMO, to extend the result into a long-term equilibrium of the\nhierarchical game, thereby producing the solution to the original problem.\nSimulations show the effectiveness of the introduced framework, and demonstrate\nthe superiority of the proposed solution over counterparts.",
        "Beampattern synthesis seeks to optimize array weights to shape radiation\npatterns, playing a critical role in various wireless applications. In addition\nto theoretical advancements, recent hardware innovations have facilitated new\navenues to enhance beampattern synthesis performance. This paper studies the\nbeampattern synthesis problem using newly proposed electromagnetically\nreconfigurable antennas (ERAs). By utilizing spherical harmonics decomposition,\nwe simultaneously optimize each antenna's radiation pattern and phase shift to\nmatch a desired beampattern of the entire array. The problem is formulated for\nboth far-field and near-field scenarios, with the optimization solved using\nRiemannian manifold techniques. The simulation results validate the\neffectiveness of the proposed solution and illustrate that ERAs exhibit\nsuperior beampattern synthesis capabilities compared to conventional fixed\nradiation pattern antennas. This advantage becomes increasingly significant as\nthe array size grows.",
        "Limited by the operation and measurement conditions, numerical simulation is\noften the only feasible approach for studying plasma behavior and mechanisms.\nAlthough artificial intelligence methods, especially physics-informed neural\nnetwork (PINN), have been widely applied in plasma simulation, the design of\nthe neural network structures still largely relies on the experience of\nresearchers. Meanwhile, existing neural architecture search methods tailored\nfor PINN have encountered failures when dealing with complex plasma governing\nequations characterized by variable coefficients and strong nonlinearity.\nTherefore, we propose an improved neural architecture search-guided method,\nnamely NAS-PINNv2, to address the limitations of existing methods. By analyzing\nthe causes of failure, the sigmoid function is applied to calculate the\narchitecture-related weights, and a new loss term is introduced. The\nperformance of NAS-PINNv2 is verified in several numerical experiments\nincluding the Elenbaas-Heller equation without and with radial velocity, the\ndrift-diffusion-Poisson equation and the Boltzmann equation. The results again\nemphasize that larger neural networks do not necessarily perform better, and\nthe discovered neural architecture with multiple neuron numbers in a single\nhidden layer imply a more flexible and sophisticated design rule for fully\nconnected networks.",
        "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https:\/\/github.com\/DD-DuDa\/BitDecoding."
      ]
    }
  },
  {
    "id":2412.00544,
    "research_type":"applied",
    "start_id":"b10",
    "start_title":"Attention Is All You Need",
    "start_abstract":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data.",
    "start_categories":[
      "cond-mat.dis-nn"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning"
      ],
      "abstract":[
        "Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process."
      ],
      "categories":[
        "astro-ph.HE"
      ]
    },
    "list":{
      "title":[
        "Optimizing Model Selection for Compound AI Systems",
        "Leveraging Intermediate Representations for Better Out-of-Distribution\n  Detection",
        "Generative Modeling for Mathematical Discovery",
        "Binary Diffusion Probabilistic Model",
        "Identification of non-causal systems with random switching modes\n  (Extended Version)",
        "Convergence rates for the vanishing viscosity approximation of\n  Hamilton-Jacobi equations: the convex case",
        "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views",
        "Provably Safeguarding a Classifier from OOD and Adversarial Samples: an\n  Extreme Value Theory Approach",
        "Interleaved Gibbs Diffusion for Constrained Generation",
        "ATLaS: Agent Tuning via Learning Critical Steps",
        "Multi-dataset synergistic in supervised learning to pre-label structural\n  components in point clouds from shell construction scenes",
        "Asymptotic lengths of permutahedra and associahedra",
        "Cross-Domain Continual Learning for Edge Intelligence in Wireless ISAC\n  Networks",
        "Slamming: Training a Speech Language Model on One GPU in a Day",
        "Deep Change Monitoring: A Hyperbolic Representative Learning Framework\n  and a Dataset for Long-term Fine-grained Tree Change Detection",
        "The effect of minimum wages on employment in the presence of\n  productivity fluctuations",
        "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
        "An Analytical Study of the Min-Sum Approximation for Polar Codes",
        "Modulo arithmetic of function spaces: Subset hyperspaces as quotients of\n  function spaces",
        "Finding all solutions of qKZ equations in characteristic $p$",
        "A characterization of Generalized functions of Bounded Deformation",
        "Feature-EndoGaussian: Feature Distilled Gaussian Splatting in Surgical\n  Deformable Scene Reconstruction",
        "Recommendations to OSCE\/ODIHR (on how to give better recommendations for\n  Internet voting)",
        "On the Khovanov homology of 3-braids",
        "Concerns and Values in Human-Robot Interactions: A Focus on Social\n  Robotics",
        "Improving Scientific Document Retrieval with Concept Coverage-based\n  Query Set Generation",
        "The Reconstruction of Theaetetus' Theory of Ratios of Magnitudes",
        "How Can Video Generative AI Transform K-12 Education? Examining\n  Teachers' Perspectives through TPACK and TAM",
        "AGAV-Rater: Adapting Large Multimodal Model for AI-Generated\n  Audio-Visual Quality Assessment"
      ],
      "abstract":[
        "Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules.",
        "In real-world applications, machine learning models must reliably detect\nOut-of-Distribution (OoD) samples to prevent unsafe decisions. Current OoD\ndetection methods often rely on analyzing the logits or the embeddings of the\npenultimate layer of a neural network. However, little work has been conducted\non the exploitation of the rich information encoded in intermediate layers. To\naddress this, we analyze the discriminative power of intermediate layers and\nshow that they can positively be used for OoD detection. Therefore, we propose\nto regularize intermediate layers with an energy-based contrastive loss, and by\ngrouping multiple layers in a single aggregated response. We demonstrate that\nintermediate layer activations improves OoD detection performance by running a\ncomprehensive evaluation across multiple datasets.",
        "We present a new implementation of the LLM-driven genetic algorithm {\\it\nfunsearch}, whose aim is to generate examples of interest to mathematicians and\nwhich has already had some success in problems in extremal combinatorics. Our\nimplementation is designed to be useful in practice for working mathematicians;\nit does not require expertise in machine learning or access to high-performance\ncomputing resources. Applying {\\it funsearch} to a new problem involves\nmodifying a small segment of Python code and selecting a large language model\n(LLM) from one of many third-party providers. We benchmarked our implementation\non three different problems, obtaining metrics that may inform applications of\n{\\it funsearch} to new problems. Our results demonstrate that {\\it funsearch}\nsuccessfully learns in a variety of combinatorial and number-theoretic\nsettings, and in some contexts learns principles that generalize beyond the\nproblem originally trained on.",
        "We introduce the Binary Diffusion Probabilistic Model (BDPM), a novel\ngenerative model optimized for binary data representations. While denoising\ndiffusion probabilistic models (DDPMs) have demonstrated notable success in\ntasks like image synthesis and restoration, traditional DDPMs rely on\ncontinuous data representations and mean squared error (MSE) loss for training,\napplying Gaussian noise models that may not be optimal for discrete or binary\ndata structures. BDPM addresses this by decomposing images into bitplanes and\nemploying XOR-based noise transformations, with a denoising model trained using\nbinary cross-entropy loss. This approach enables precise noise control and\ncomputationally efficient inference, significantly lowering computational costs\nand improving model convergence. When evaluated on image restoration tasks such\nas image super-resolution, inpainting, and blind image restoration, BDPM\noutperforms state-of-the-art methods on the FFHQ, CelebA, and CelebA-HQ\ndatasets. Notably, BDPM requires fewer inference steps than traditional DDPM\nmodels to reach optimal results, showcasing enhanced inference efficiency.",
        "We consider the identification of non-causal systems with random switching\nmodes (NCSRSM), a class of models essential for describing typical power load\nmanagement and department store inventory dynamics. The simultaneous\nidentification of causal-andanticausal subsystems, along with the presence of\nrandom switching sequences, however, make the overall identification problem\nparticularly challenging. To this end, we develop an expectation-maximization\n(EM) based system identification technique, where the E-step proposes a\nmodified Kalman filter (KF) to estimate the states and switching sequences of\ncausal-and-anticausal subsystems, while the M-step consists in a switching\nleast-squares algorithm to estimate the parameters of individual subsystems. We\nestablish the main convergence features of the proposed identification\nprocedure, also providing bounds on the parameter estimation errors under mild\nconditions. Finally, the effectiveness of our identification method is\nvalidated through two numerical simulations.",
        "We study the speed of convergence in $L^\\infty$ norm of the vanishing\nviscosity process for Hamilton-Jacobi equations with uniformly or strictly\nconvex Hamiltonian terms with superquadratic behavior. Our analysis boosts\nprevious findings on the rate of convergence for this procedure in $L^p$ norms,\nshowing rates in sup-norm of order $\\mathcal{O}(\\epsilon^\\beta)$,\n$\\beta\\in(1\/2,1)$, or $\\mathcal{O}(\\epsilon|\\log\\epsilon|)$ with respect to the\nvanishing viscosity parameter $\\epsilon$, depending on the regularity of the\ninitial datum of the problem and convexity properties of the Hamiltonian. Our\nproofs are based on integral methods and avoid the use of techniques based on\nstochastic control or the maximum principle.",
        "We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https:\/\/zhanghe3z.github.io\/FLARE\/",
        "This paper introduces a novel method, Sample-efficient Probabilistic\nDetection using Extreme Value Theory (SPADE), which transforms a classifier\ninto an abstaining classifier, offering provable protection against\nout-of-distribution and adversarial samples. The approach is based on a\nGeneralized Extreme Value (GEV) model of the training distribution in the\nclassifier's latent space, enabling the formal characterization of OOD samples.\nInterestingly, under mild assumptions, the GEV model also allows for formally\ncharacterizing adversarial samples. The abstaining classifier, which rejects\nsamples based on their assessment by the GEV model, provably avoids OOD and\nadversarial samples. The empirical validation of the approach, conducted on\nvarious neural architectures (ResNet, VGG, and Vision Transformer) and medium\nand large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its\nfrugality, stability, and efficiency compared to the state of the art.",
        "We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling\nframework for mixed continuous-discrete data, focusing on constrained\ngeneration problems. Prior works on discrete and continuous-discrete diffusion\nmodels assume factorized denoising distribution for fast generation, which can\nhinder the modeling of strong dependencies between random variables encountered\nin constrained generation. IGD moves beyond this by interleaving continuous and\ndiscrete denoising algorithms via a discrete time Gibbs sampling type Markov\nchain. IGD provides flexibility in the choice of denoisers, allows conditional\ngeneration via state-space doubling and inference time scaling via the\nReDeNoise method. Empirical evaluations on three challenging tasks-solving\n3-SAT, generating molecule structures, and generating layouts-demonstrate\nstate-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT\nout of the box and achieves state-of-the-art results in molecule generation\nwithout relying on equivariant diffusion or domain-specific architectures. We\nexplore a wide range of modeling, and interleaving strategies along with\nhyperparameters in each of these problems.",
        "Large Language Model (LLM) agents have demonstrated remarkable generalization\ncapabilities across multi-domain tasks. Existing agent tuning approaches\ntypically employ supervised finetuning on entire expert trajectories. However,\nbehavior-cloning of full trajectories can introduce expert bias and weaken\ngeneralization to states not covered by the expert data. Additionally, critical\nsteps, such as planning, complex reasoning for intermediate subtasks, and\nstrategic decision-making, are essential to success in agent tasks, so learning\nthese steps is the key to improving LLM agents. For more effective and\nefficient agent tuning, we propose ATLaS that identifies the critical steps in\nexpert trajectories and finetunes LLMs solely on these steps with reduced\ncosts. By steering the training's focus to a few critical steps, our method\nmitigates the risk of overfitting entire trajectories and promotes\ngeneralization across different environments and tasks. In extensive\nexperiments, an LLM finetuned on only 30% critical steps selected by ATLaS\noutperforms the LLM finetuned on all steps and recent open-source LLM agents.\nATLaS maintains and improves base LLM skills as generalist agents interacting\nwith diverse environments.",
        "The significant effort required to annotate data for new training datasets\nhinders computer vision research and machine learning in the construction\nindustry. This work explores adapting standard datasets and the latest\ntransformer model architectures for point cloud semantic segmentation in the\ncontext of shell construction sites. Unlike common approaches focused on object\nsegmentation of building interiors and furniture, this study addressed the\nchallenges of segmenting complex structural components in Architecture,\nEngineering, and Construction (AEC). We establish a baseline through supervised\ntraining and a custom validation dataset, evaluate the cross-domain inference\nwith large-scale indoor datasets, and utilize transfer learning to maximize\nsegmentation performance with minimal new data. The findings indicate that with\nminimal fine-tuning, pre-trained transformer architectures offer an effective\nstrategy for building component segmentation. Our results are promising for\nautomating the annotation of new, previously unseen data when creating larger\ntraining resources and for the segmentation of frequently recurring objects.",
        "We define asymptotic lengths for families of oriented polytopes. We show that\npermutahedra with weak order orientations have asymptotic total length 1 and\nassociahedra with Tamari order orientations have asymptotic total length 1\/2.",
        "In wireless networks with integrated sensing and communications (ISAC), edge\nintelligence (EI) is expected to be developed at edge devices (ED) for sensing\nuser activities based on channel state information (CSI). However, due to the\nCSI being highly specific to users' characteristics, the CSI-activity\nrelationship is notoriously domain dependent, essentially demanding EI to learn\nsufficient datasets from various domains in order to gain cross-domain sensing\ncapability. This poses a crucial challenge owing to the EDs' limited resources,\nfor which storing datasets across all domains will be a significant burden. In\nthis paper, we propose the EdgeCL framework, enabling the EI to continually\nlearn-then-discard each incoming dataset, while remaining resilient to\ncatastrophic forgetting. We design a transformer-based discriminator for\nhandling sequences of noisy and nonequispaced CSI samples. Besides, we propose\na distilled core-set based knowledge retention method with robustness-enhanced\noptimization to train the discriminator, preserving its performance for\nprevious domains while preventing future forgetting. Experimental evaluations\nshow that EdgeCL achieves 89% of performance compared to cumulative training\nwhile consuming only 3% of its memory, mitigating forgetting by 79%.",
        "We introduce Slam, a recipe for training high-quality Speech Language Models\n(SLMs) on a single academic GPU in 24 hours. We do so through empirical\nanalysis of model initialisation and architecture, synthetic training data,\npreference optimisation with synthetic data and tweaking all other components.\nWe empirically demonstrate that this training recipe also scales well with more\ncompute getting results on par with leading SLMs in a fraction of the compute\ncost. We hope these insights will make SLM training and research more\naccessible. In the context of SLM scaling laws, our results far outperform\npredicted compute optimal performance, giving an optimistic view to SLM\nfeasibility. See code, data, models, samples at -\nhttps:\/\/pages.cs.huji.ac.il\/adiyoss-lab\/slamming .",
        "In environmental protection, tree monitoring plays an essential role in\nmaintaining and improving ecosystem health. However, precise monitoring is\nchallenging because existing datasets fail to capture continuous fine-grained\nchanges in trees due to low-resolution images and high acquisition costs. In\nthis paper, we introduce UAVTC, a large-scale, long-term, high-resolution\ndataset collected using UAVs equipped with cameras, specifically designed to\ndetect individual Tree Changes (TCs). UAVTC includes rich annotations and\nstatistics based on biological knowledge, offering a fine-grained view for tree\nmonitoring. To address environmental influences and effectively model the\nhierarchical diversity of physiological TCs, we propose a novel Hyperbolic\nSiamese Network (HSN) for TC detection, enabling compact and hierarchical\nrepresentations of dynamic tree changes.\n  Extensive experiments show that HSN can effectively capture complex\nhierarchical changes and provide a robust solution for fine-grained TC\ndetection. In addition, HSN generalizes well to cross-domain face anti-spoofing\ntask, highlighting its broader significance in AI. We believe our work,\ncombining ecological insights and interdisciplinary expertise, will benefit the\ncommunity by offering a new benchmark and innovative AI technologies.",
        "Traditionally, the impact of minimum wages on employment has been studied,\nand it is generally believed to have a negative effect. Yet, some recent\nstudies have shown that the impact of minimum wages on employment can sometimes\nbe positive. In addition, certain recent proposals set a higher minimum wage\nthan the wage earned by some high-productivity workers. However, the impact of\nminimum wages on employment has been primarily studied on low-skilled workers,\nwhereas there is limited research on high-skilled workers. To address this gap\nand examine the effects of minimum wages on high-productivity workers'\nemployment, I construct a macroeconomic model incorporating productivity\nfluctuations, incomplete markets, directed search, and on-the-job search and\ncompare the steady-state distributions between the baseline model and the model\nwith a minimum wage. As a result, binding minimum wages increase the\nunemployment rate of both low and high-productivity workers.",
        "Despite notable advancements in Multimodal Large Language Models (MLLMs),\nmost state-of-the-art models have not undergone thorough alignment with human\npreferences. This gap exists because current alignment research has primarily\nachieved progress in specific areas (e.g., hallucination reduction), while the\nbroader question of whether aligning models with human preferences can\nsystematically enhance MLLM capability remains largely unexplored. To this end,\nwe introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained,\nhuman-annotated preference comparison pairs. This dataset represents a\nsubstantial advancement over existing resources, offering superior size,\ndiversity, annotation granularity, and quality. Leveraging this dataset, we\npropose several key innovations to improve both the quality of reward models\nand the efficiency of alignment algorithms. Notably, we introduce a\nCritique-Based Reward Model, which generates critiques of model outputs before\nassigning scores, offering enhanced interpretability and more informative\nfeedback compared to traditional scalar reward mechanisms. Additionally, we\npropose Dynamic Reward Scaling, a method that adjusts the loss weight of each\nsample according to the reward signal, thereby optimizing the use of\nhigh-quality comparison pairs. Our approach is rigorously evaluated across\n$\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results\ndemonstrating significant and consistent improvements in model performance.\nSpecifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm\nleads to a $\\mathbf{19.5}$% increase in conversational abilities and a\n$\\mathbf{60}$% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and\nevaluation code, as well as reward modeling and safety benchmarks. For more\ndetails, please visit our project page: https:\/\/mm-rlhf.github.io.",
        "The min-sum approximation is widely used in the decoding of polar codes.\nAlthough it is a numerical approximation, hardly any penalties are incurred in\npractice. We give a theoretical justification for this. We consider the common\ncase of a binary-input, memoryless, and symmetric channel, decoded using\nsuccessive cancellation and the min-sum approximation. Under mild assumptions,\nwe show the following. For the finite length case, we show how to exactly\ncalculate the error probabilities of all synthetic (bit) channels in time\n$O(N^{1.585})$, where $N$ is the codeword length. This implies a code\nconstruction algorithm with the above complexity. For the asymptotic case, we\ndevelop two rate thresholds, denoted $R_{\\mathrm{L}} = R_{\\mathrm{L}}(\\lambda)$\nand $R_{\\mathrm{U}} =R_{\\mathrm{U}}(\\lambda)$, where $\\lambda(\\cdot)$ is the\nlabeler of the channel outputs (essentially, a quantizer). For any $0 < \\beta <\n\\frac{1}{2}$ and any code rate $R < R_{\\mathrm{L}}$, there exists a family of\npolar codes with growing lengths such that their rates are at least $R$ and\ntheir error probabilities are at most $2^{-N^\\beta}$. That is, strong\npolarization continues to hold under the min-sum approximation. Conversely, for\ncode rates exceeding $R_{\\mathrm{U}}$, the error probability approaches $1$ as\nthe code-length increases, irrespective of which bits are frozen. We show that\n$0 < R_{\\mathrm{L}} \\leq R_{\\mathrm{U}} \\leq C$, where $C$ is the channel\ncapacity. The last inequality is often strict, in which case the ramification\nof using the min-sum approximation is that we can no longer achieve capacity.",
        "Let $X$ be a (topological) space and $Cl(X)$ the collection of nonempty\nclosed subsets of $X$. Given a topology on $Cl(X)$, making $Cl(X)$ a space, a\n\\emph{(subset) hyperspace} of $X$ is a subspace $\\mathcal{J}\\subset Cl(X)$ with\nan embedding $X\\hookrightarrow\\mathcal{J}$, $x\\mapsto\\{x\\}$ (which thus\nrequires $X$ to be $T_1$). In this note, we characterize certain hyperspaces\n$\\mathcal{J}\\subset Cl(X)$ as explicit quotient spaces of function spaces\n$\\mathcal{F}\\subset X^Y$ and discuss metrization of associated compact-subset\nhyperspaces in this setting.",
        "In [MV] the difference qKZ equations were considered modulo a prime number\n$p$ and a family of polynomial solutions of the qKZ equations modulo $p$ was\nconstructed by an elementary procedure as suitable $p$-approximations of the\nhypergeometric integrals. In this paper, we study in detail the first family of\nnontrivial example of the qKZ equations in characteristic $p$. We describe all\nsolutions of these qKZ equations in characteristic $p$ by demonstrating that\nthey all stem from the $p$-hypergeometric solutions. We also prove a Lagrangian\nproperty (called the orthogonality property) of the subbundle of the qKZ bundle\nspanned by the $p$-hypergeometric sections. This paper extends the results of\n[VV1] on the differential KZ equations to the difference qKZ equations.",
        "We show that Dal Maso's GBD space, introduced for tackling crack growth in\nlinearized elasticity, can be defined by simple conditions in a finite number\nof directions of slicing.",
        "Minimally invasive surgery (MIS) has transformed clinical practice by\nreducing recovery times, minimizing complications, and enhancing precision.\nNonetheless, MIS inherently relies on indirect visualization and precise\ninstrument control, posing unique challenges. Recent advances in artificial\nintelligence have enabled real-time surgical scene understanding through\ntechniques such as image classification, object detection, and segmentation,\nwith scene reconstruction emerging as a key element for enhanced intraoperative\nguidance. Although neural radiance fields (NeRFs) have been explored for this\npurpose, their substantial data requirements and slow rendering inhibit\nreal-time performance. In contrast, 3D Gaussian Splatting (3DGS) offers a more\nefficient alternative, achieving state-of-the-art performance in dynamic\nsurgical scene reconstruction. In this work, we introduce Feature-EndoGaussian\n(FEG), an extension of 3DGS that integrates 2D segmentation cues into 3D\nrendering to enable real-time semantic and scene reconstruction. By leveraging\npretrained segmentation foundation models, FEG incorporates semantic feature\ndistillation within the Gaussian deformation framework, thereby enhancing both\nreconstruction fidelity and segmentation accuracy. On the EndoNeRF dataset, FEG\nachieves superior performance (SSIM of 0.97, PSNR of 39.08, and LPIPS of 0.03)\ncompared to leading methods. Additionally, on the EndoVis18 dataset, FEG\ndemonstrates competitive class-wise segmentation metrics while balancing model\nsize and real-time performance.",
        "This paper takes a critical look at the recommendations OSCE\/ODIHR has given\nfor the Estonian Internet voting over the 20 years it has been running. We\npresent examples of recommendations that can not be fulfilled at all, but also\nexamples where fulfilling a recommendation requires a non-trivial trade-off,\npotentially weakening the system in some other respect. In such cases\nOSCE\/ODIHR should take an explicit position which trade-off it recommends. We\nalso look at the development of the recommendation to introduce end-to-end\nverifiability. In this case we expect OSCE\/ODIHR to define what it exactly\nmeans by this property, as well as to give explicit criteria to determine\nwhether and to which extent end-to-end verifiability has been achieved.",
        "We prove the conjecture of Przytycki and Sazdanovic that the Khovanov\nhomology of the closure of a 3-stranded braid only contains torsion of order 2.\nThis conjecture has been known for six out of seven classes in the\nMurasugi-classification of 3-braids and we show it for the remaining class. Our\nproof also works for the other classes and relies on Bar-Natan's version of\nKhovanov homology for tangles as well as his delooping and cancellation\ntechniques, and the reduced integral Bar-Natan--Lee--Turner spectral sequence.\nWe also show that the Knight-move conjecture holds for 3-braids.",
        "Robots, as AI with physical instantiation, inhabit our social and physical\nworld, where their actions have both social and physical consequences, posing\nchallenges for researchers when designing social robots. This study starts with\na scoping review to identify discussions and potential concerns arising from\ninteractions with robotic systems. Two focus groups of technology ethics\nexperts then validated a comprehensive list of key topics and values in\nhuman-robot interaction (HRI) literature. These insights were integrated into\nthe HRI Value Compass web tool, to help HRI researchers identify ethical values\nin robot design. The tool was evaluated in a pilot study. This work benefits\nthe HRI community by highlighting key concerns in human-robot interactions and\nproviding an instrument to help researchers design robots that align with human\nvalues, ensuring future robotic systems adhere to these values in social\napplications.",
        "In specialized fields like the scientific domain, constructing large-scale\nhuman-annotated datasets poses a significant challenge due to the need for\ndomain expertise. Recent methods have employed large language models to\ngenerate synthetic queries, which serve as proxies for actual user queries.\nHowever, they lack control over the content generated, often resulting in\nincomplete coverage of academic concepts in documents. We introduce Concept\nCoverage-based Query set Generation (CCQGen) framework, designed to generate a\nset of queries with comprehensive coverage of the document's concepts. A key\ndistinction of CCQGen is that it adaptively adjusts the generation process\nbased on the previously generated queries. We identify concepts not\nsufficiently covered by previous queries, and leverage them as conditions for\nsubsequent query generation. This approach guides each new query to complement\nthe previous ones, aiding in a thorough understanding of the document.\nExtensive experiments demonstrate that CCQGen significantly enhances query\nquality and retrieval performance.",
        "In the present chapter, we obtain the reconstruction of Theaetetus' theory of\nratios of magnitudes based, according to Aristotle's Topics 158b, on the\ndefinition of proportion in terms of equal anthyphairesis. Our reconstruction\nis built on the anthyphairetic interpretation of the notoriously difficult\nTheaetetus 147d6-e1 passage on Theaetetus' mathematical discovery of quadratic\nincommensurabilities, itself based on the traces it has left on Plato's\nphilosophical definition of Knowledge in his dialogues Theaetetus, Sophist and\nMeno. Contrary to earlier reconstructions by Becker, van der Waerden and Knorr,\nour reconstruction reveals a theory that (a) applies only to the restricted\nclass of pairs of magnitudes whose anthyphairesis is finite or eventually\nperiodic, and (b) avoids the problematic use of Eudoxus' definition 4 of Book V\nof Euclid's Elements.\n  The final version of this paper will appear as a chapter in the book Essays\non Topology: Dedicated to Valentin Po\\'enaru, ed. L. Funar and A. Papadopoulos,\nSpringer, 2025.",
        "The rapid advancement of generative AI technology, particularly video\ngenerative AI (Video GenAI), has opened new possibilities for K-12 education by\nenabling the creation of dynamic, customized, and high-quality visual content.\nDespite its potential, there is limited research on how this emerging\ntechnology can be effectively integrated into educational practices. This study\nexplores the perspectives of leading K-12 teachers on the educational\napplications of Video GenAI, using the TPACK (Technological Pedagogical Content\nKnowledge) and TAM (Technology Acceptance Model) frameworks as analytical\nlenses. Through interviews and hands-on experimentation with video generation\ntools, the research identifies opportunities for enhancing teaching strategies,\nfostering student engagement, and supporting authentic task design. It also\nhighlights challenges such as technical limitations, ethical considerations,\nand the need for institutional support. The findings provide actionable\ninsights into how Video GenAI can transform teaching and learning, offering\npractical implications for policy, teacher training, and the future development\nof educational technology.",
        "Many video-to-audio (VTA) methods have been proposed for dubbing silent\nAI-generated videos. An efficient quality assessment method for AI-generated\naudio-visual content (AGAV) is crucial for ensuring audio-visual quality.\nExisting audio-visual quality assessment methods struggle with unique\ndistortions in AGAVs, such as unrealistic and inconsistent elements. To address\nthis, we introduce AGAVQA, the first large-scale AGAV quality assessment\ndataset, comprising 3,382 AGAVs from 16 VTA methods. AGAVQA includes two\nsubsets: AGAVQA-MOS, which provides multi-dimensional scores for audio quality,\ncontent consistency, and overall quality, and AGAVQA-Pair, designed for optimal\nAGAV pair selection. We further propose AGAV-Rater, a LMM-based model that can\nscore AGAVs, as well as audio and music generated from text, across multiple\ndimensions, and selects the best AGAV generated by VTA methods to present to\nthe user. AGAV-Rater achieves state-of-the-art performance on AGAVQA,\nText-to-Audio, and Text-to-Music datasets. Subjective tests also confirm that\nAGAV-Rater enhances VTA performance and user experience. The project page is\navailable at https:\/\/agav-rater.github.io."
      ]
    }
  },
  {
    "id":2412.00544,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning",
    "start_abstract":"Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process.",
    "start_categories":[
      "astro-ph.HE"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b10"
      ],
      "title":[
        "Attention Is All You Need"
      ],
      "abstract":[
        "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
      ],
      "categories":[
        "cond-mat.dis-nn"
      ]
    },
    "list":{
      "title":[
        "Optimizing Model Selection for Compound AI Systems",
        "Leveraging Intermediate Representations for Better Out-of-Distribution\n  Detection",
        "Generative Modeling for Mathematical Discovery",
        "Binary Diffusion Probabilistic Model",
        "Identification of non-causal systems with random switching modes\n  (Extended Version)",
        "Convergence rates for the vanishing viscosity approximation of\n  Hamilton-Jacobi equations: the convex case",
        "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views",
        "Provably Safeguarding a Classifier from OOD and Adversarial Samples: an\n  Extreme Value Theory Approach",
        "Interleaved Gibbs Diffusion for Constrained Generation",
        "ATLaS: Agent Tuning via Learning Critical Steps",
        "Multi-dataset synergistic in supervised learning to pre-label structural\n  components in point clouds from shell construction scenes",
        "Asymptotic lengths of permutahedra and associahedra",
        "Cross-Domain Continual Learning for Edge Intelligence in Wireless ISAC\n  Networks",
        "Slamming: Training a Speech Language Model on One GPU in a Day",
        "Deep Change Monitoring: A Hyperbolic Representative Learning Framework\n  and a Dataset for Long-term Fine-grained Tree Change Detection",
        "The effect of minimum wages on employment in the presence of\n  productivity fluctuations",
        "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
        "An Analytical Study of the Min-Sum Approximation for Polar Codes",
        "Modulo arithmetic of function spaces: Subset hyperspaces as quotients of\n  function spaces",
        "Finding all solutions of qKZ equations in characteristic $p$",
        "A characterization of Generalized functions of Bounded Deformation",
        "Feature-EndoGaussian: Feature Distilled Gaussian Splatting in Surgical\n  Deformable Scene Reconstruction",
        "Recommendations to OSCE\/ODIHR (on how to give better recommendations for\n  Internet voting)",
        "On the Khovanov homology of 3-braids",
        "Concerns and Values in Human-Robot Interactions: A Focus on Social\n  Robotics",
        "Improving Scientific Document Retrieval with Concept Coverage-based\n  Query Set Generation",
        "The Reconstruction of Theaetetus' Theory of Ratios of Magnitudes",
        "How Can Video Generative AI Transform K-12 Education? Examining\n  Teachers' Perspectives through TPACK and TAM",
        "AGAV-Rater: Adapting Large Multimodal Model for AI-Generated\n  Audio-Visual Quality Assessment"
      ],
      "abstract":[
        "Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules.",
        "In real-world applications, machine learning models must reliably detect\nOut-of-Distribution (OoD) samples to prevent unsafe decisions. Current OoD\ndetection methods often rely on analyzing the logits or the embeddings of the\npenultimate layer of a neural network. However, little work has been conducted\non the exploitation of the rich information encoded in intermediate layers. To\naddress this, we analyze the discriminative power of intermediate layers and\nshow that they can positively be used for OoD detection. Therefore, we propose\nto regularize intermediate layers with an energy-based contrastive loss, and by\ngrouping multiple layers in a single aggregated response. We demonstrate that\nintermediate layer activations improves OoD detection performance by running a\ncomprehensive evaluation across multiple datasets.",
        "We present a new implementation of the LLM-driven genetic algorithm {\\it\nfunsearch}, whose aim is to generate examples of interest to mathematicians and\nwhich has already had some success in problems in extremal combinatorics. Our\nimplementation is designed to be useful in practice for working mathematicians;\nit does not require expertise in machine learning or access to high-performance\ncomputing resources. Applying {\\it funsearch} to a new problem involves\nmodifying a small segment of Python code and selecting a large language model\n(LLM) from one of many third-party providers. We benchmarked our implementation\non three different problems, obtaining metrics that may inform applications of\n{\\it funsearch} to new problems. Our results demonstrate that {\\it funsearch}\nsuccessfully learns in a variety of combinatorial and number-theoretic\nsettings, and in some contexts learns principles that generalize beyond the\nproblem originally trained on.",
        "We introduce the Binary Diffusion Probabilistic Model (BDPM), a novel\ngenerative model optimized for binary data representations. While denoising\ndiffusion probabilistic models (DDPMs) have demonstrated notable success in\ntasks like image synthesis and restoration, traditional DDPMs rely on\ncontinuous data representations and mean squared error (MSE) loss for training,\napplying Gaussian noise models that may not be optimal for discrete or binary\ndata structures. BDPM addresses this by decomposing images into bitplanes and\nemploying XOR-based noise transformations, with a denoising model trained using\nbinary cross-entropy loss. This approach enables precise noise control and\ncomputationally efficient inference, significantly lowering computational costs\nand improving model convergence. When evaluated on image restoration tasks such\nas image super-resolution, inpainting, and blind image restoration, BDPM\noutperforms state-of-the-art methods on the FFHQ, CelebA, and CelebA-HQ\ndatasets. Notably, BDPM requires fewer inference steps than traditional DDPM\nmodels to reach optimal results, showcasing enhanced inference efficiency.",
        "We consider the identification of non-causal systems with random switching\nmodes (NCSRSM), a class of models essential for describing typical power load\nmanagement and department store inventory dynamics. The simultaneous\nidentification of causal-andanticausal subsystems, along with the presence of\nrandom switching sequences, however, make the overall identification problem\nparticularly challenging. To this end, we develop an expectation-maximization\n(EM) based system identification technique, where the E-step proposes a\nmodified Kalman filter (KF) to estimate the states and switching sequences of\ncausal-and-anticausal subsystems, while the M-step consists in a switching\nleast-squares algorithm to estimate the parameters of individual subsystems. We\nestablish the main convergence features of the proposed identification\nprocedure, also providing bounds on the parameter estimation errors under mild\nconditions. Finally, the effectiveness of our identification method is\nvalidated through two numerical simulations.",
        "We study the speed of convergence in $L^\\infty$ norm of the vanishing\nviscosity process for Hamilton-Jacobi equations with uniformly or strictly\nconvex Hamiltonian terms with superquadratic behavior. Our analysis boosts\nprevious findings on the rate of convergence for this procedure in $L^p$ norms,\nshowing rates in sup-norm of order $\\mathcal{O}(\\epsilon^\\beta)$,\n$\\beta\\in(1\/2,1)$, or $\\mathcal{O}(\\epsilon|\\log\\epsilon|)$ with respect to the\nvanishing viscosity parameter $\\epsilon$, depending on the regularity of the\ninitial datum of the problem and convexity properties of the Hamiltonian. Our\nproofs are based on integral methods and avoid the use of techniques based on\nstochastic control or the maximum principle.",
        "We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https:\/\/zhanghe3z.github.io\/FLARE\/",
        "This paper introduces a novel method, Sample-efficient Probabilistic\nDetection using Extreme Value Theory (SPADE), which transforms a classifier\ninto an abstaining classifier, offering provable protection against\nout-of-distribution and adversarial samples. The approach is based on a\nGeneralized Extreme Value (GEV) model of the training distribution in the\nclassifier's latent space, enabling the formal characterization of OOD samples.\nInterestingly, under mild assumptions, the GEV model also allows for formally\ncharacterizing adversarial samples. The abstaining classifier, which rejects\nsamples based on their assessment by the GEV model, provably avoids OOD and\nadversarial samples. The empirical validation of the approach, conducted on\nvarious neural architectures (ResNet, VGG, and Vision Transformer) and medium\nand large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its\nfrugality, stability, and efficiency compared to the state of the art.",
        "We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling\nframework for mixed continuous-discrete data, focusing on constrained\ngeneration problems. Prior works on discrete and continuous-discrete diffusion\nmodels assume factorized denoising distribution for fast generation, which can\nhinder the modeling of strong dependencies between random variables encountered\nin constrained generation. IGD moves beyond this by interleaving continuous and\ndiscrete denoising algorithms via a discrete time Gibbs sampling type Markov\nchain. IGD provides flexibility in the choice of denoisers, allows conditional\ngeneration via state-space doubling and inference time scaling via the\nReDeNoise method. Empirical evaluations on three challenging tasks-solving\n3-SAT, generating molecule structures, and generating layouts-demonstrate\nstate-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT\nout of the box and achieves state-of-the-art results in molecule generation\nwithout relying on equivariant diffusion or domain-specific architectures. We\nexplore a wide range of modeling, and interleaving strategies along with\nhyperparameters in each of these problems.",
        "Large Language Model (LLM) agents have demonstrated remarkable generalization\ncapabilities across multi-domain tasks. Existing agent tuning approaches\ntypically employ supervised finetuning on entire expert trajectories. However,\nbehavior-cloning of full trajectories can introduce expert bias and weaken\ngeneralization to states not covered by the expert data. Additionally, critical\nsteps, such as planning, complex reasoning for intermediate subtasks, and\nstrategic decision-making, are essential to success in agent tasks, so learning\nthese steps is the key to improving LLM agents. For more effective and\nefficient agent tuning, we propose ATLaS that identifies the critical steps in\nexpert trajectories and finetunes LLMs solely on these steps with reduced\ncosts. By steering the training's focus to a few critical steps, our method\nmitigates the risk of overfitting entire trajectories and promotes\ngeneralization across different environments and tasks. In extensive\nexperiments, an LLM finetuned on only 30% critical steps selected by ATLaS\noutperforms the LLM finetuned on all steps and recent open-source LLM agents.\nATLaS maintains and improves base LLM skills as generalist agents interacting\nwith diverse environments.",
        "The significant effort required to annotate data for new training datasets\nhinders computer vision research and machine learning in the construction\nindustry. This work explores adapting standard datasets and the latest\ntransformer model architectures for point cloud semantic segmentation in the\ncontext of shell construction sites. Unlike common approaches focused on object\nsegmentation of building interiors and furniture, this study addressed the\nchallenges of segmenting complex structural components in Architecture,\nEngineering, and Construction (AEC). We establish a baseline through supervised\ntraining and a custom validation dataset, evaluate the cross-domain inference\nwith large-scale indoor datasets, and utilize transfer learning to maximize\nsegmentation performance with minimal new data. The findings indicate that with\nminimal fine-tuning, pre-trained transformer architectures offer an effective\nstrategy for building component segmentation. Our results are promising for\nautomating the annotation of new, previously unseen data when creating larger\ntraining resources and for the segmentation of frequently recurring objects.",
        "We define asymptotic lengths for families of oriented polytopes. We show that\npermutahedra with weak order orientations have asymptotic total length 1 and\nassociahedra with Tamari order orientations have asymptotic total length 1\/2.",
        "In wireless networks with integrated sensing and communications (ISAC), edge\nintelligence (EI) is expected to be developed at edge devices (ED) for sensing\nuser activities based on channel state information (CSI). However, due to the\nCSI being highly specific to users' characteristics, the CSI-activity\nrelationship is notoriously domain dependent, essentially demanding EI to learn\nsufficient datasets from various domains in order to gain cross-domain sensing\ncapability. This poses a crucial challenge owing to the EDs' limited resources,\nfor which storing datasets across all domains will be a significant burden. In\nthis paper, we propose the EdgeCL framework, enabling the EI to continually\nlearn-then-discard each incoming dataset, while remaining resilient to\ncatastrophic forgetting. We design a transformer-based discriminator for\nhandling sequences of noisy and nonequispaced CSI samples. Besides, we propose\na distilled core-set based knowledge retention method with robustness-enhanced\noptimization to train the discriminator, preserving its performance for\nprevious domains while preventing future forgetting. Experimental evaluations\nshow that EdgeCL achieves 89% of performance compared to cumulative training\nwhile consuming only 3% of its memory, mitigating forgetting by 79%.",
        "We introduce Slam, a recipe for training high-quality Speech Language Models\n(SLMs) on a single academic GPU in 24 hours. We do so through empirical\nanalysis of model initialisation and architecture, synthetic training data,\npreference optimisation with synthetic data and tweaking all other components.\nWe empirically demonstrate that this training recipe also scales well with more\ncompute getting results on par with leading SLMs in a fraction of the compute\ncost. We hope these insights will make SLM training and research more\naccessible. In the context of SLM scaling laws, our results far outperform\npredicted compute optimal performance, giving an optimistic view to SLM\nfeasibility. See code, data, models, samples at -\nhttps:\/\/pages.cs.huji.ac.il\/adiyoss-lab\/slamming .",
        "In environmental protection, tree monitoring plays an essential role in\nmaintaining and improving ecosystem health. However, precise monitoring is\nchallenging because existing datasets fail to capture continuous fine-grained\nchanges in trees due to low-resolution images and high acquisition costs. In\nthis paper, we introduce UAVTC, a large-scale, long-term, high-resolution\ndataset collected using UAVs equipped with cameras, specifically designed to\ndetect individual Tree Changes (TCs). UAVTC includes rich annotations and\nstatistics based on biological knowledge, offering a fine-grained view for tree\nmonitoring. To address environmental influences and effectively model the\nhierarchical diversity of physiological TCs, we propose a novel Hyperbolic\nSiamese Network (HSN) for TC detection, enabling compact and hierarchical\nrepresentations of dynamic tree changes.\n  Extensive experiments show that HSN can effectively capture complex\nhierarchical changes and provide a robust solution for fine-grained TC\ndetection. In addition, HSN generalizes well to cross-domain face anti-spoofing\ntask, highlighting its broader significance in AI. We believe our work,\ncombining ecological insights and interdisciplinary expertise, will benefit the\ncommunity by offering a new benchmark and innovative AI technologies.",
        "Traditionally, the impact of minimum wages on employment has been studied,\nand it is generally believed to have a negative effect. Yet, some recent\nstudies have shown that the impact of minimum wages on employment can sometimes\nbe positive. In addition, certain recent proposals set a higher minimum wage\nthan the wage earned by some high-productivity workers. However, the impact of\nminimum wages on employment has been primarily studied on low-skilled workers,\nwhereas there is limited research on high-skilled workers. To address this gap\nand examine the effects of minimum wages on high-productivity workers'\nemployment, I construct a macroeconomic model incorporating productivity\nfluctuations, incomplete markets, directed search, and on-the-job search and\ncompare the steady-state distributions between the baseline model and the model\nwith a minimum wage. As a result, binding minimum wages increase the\nunemployment rate of both low and high-productivity workers.",
        "Despite notable advancements in Multimodal Large Language Models (MLLMs),\nmost state-of-the-art models have not undergone thorough alignment with human\npreferences. This gap exists because current alignment research has primarily\nachieved progress in specific areas (e.g., hallucination reduction), while the\nbroader question of whether aligning models with human preferences can\nsystematically enhance MLLM capability remains largely unexplored. To this end,\nwe introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained,\nhuman-annotated preference comparison pairs. This dataset represents a\nsubstantial advancement over existing resources, offering superior size,\ndiversity, annotation granularity, and quality. Leveraging this dataset, we\npropose several key innovations to improve both the quality of reward models\nand the efficiency of alignment algorithms. Notably, we introduce a\nCritique-Based Reward Model, which generates critiques of model outputs before\nassigning scores, offering enhanced interpretability and more informative\nfeedback compared to traditional scalar reward mechanisms. Additionally, we\npropose Dynamic Reward Scaling, a method that adjusts the loss weight of each\nsample according to the reward signal, thereby optimizing the use of\nhigh-quality comparison pairs. Our approach is rigorously evaluated across\n$\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results\ndemonstrating significant and consistent improvements in model performance.\nSpecifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm\nleads to a $\\mathbf{19.5}$% increase in conversational abilities and a\n$\\mathbf{60}$% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and\nevaluation code, as well as reward modeling and safety benchmarks. For more\ndetails, please visit our project page: https:\/\/mm-rlhf.github.io.",
        "The min-sum approximation is widely used in the decoding of polar codes.\nAlthough it is a numerical approximation, hardly any penalties are incurred in\npractice. We give a theoretical justification for this. We consider the common\ncase of a binary-input, memoryless, and symmetric channel, decoded using\nsuccessive cancellation and the min-sum approximation. Under mild assumptions,\nwe show the following. For the finite length case, we show how to exactly\ncalculate the error probabilities of all synthetic (bit) channels in time\n$O(N^{1.585})$, where $N$ is the codeword length. This implies a code\nconstruction algorithm with the above complexity. For the asymptotic case, we\ndevelop two rate thresholds, denoted $R_{\\mathrm{L}} = R_{\\mathrm{L}}(\\lambda)$\nand $R_{\\mathrm{U}} =R_{\\mathrm{U}}(\\lambda)$, where $\\lambda(\\cdot)$ is the\nlabeler of the channel outputs (essentially, a quantizer). For any $0 < \\beta <\n\\frac{1}{2}$ and any code rate $R < R_{\\mathrm{L}}$, there exists a family of\npolar codes with growing lengths such that their rates are at least $R$ and\ntheir error probabilities are at most $2^{-N^\\beta}$. That is, strong\npolarization continues to hold under the min-sum approximation. Conversely, for\ncode rates exceeding $R_{\\mathrm{U}}$, the error probability approaches $1$ as\nthe code-length increases, irrespective of which bits are frozen. We show that\n$0 < R_{\\mathrm{L}} \\leq R_{\\mathrm{U}} \\leq C$, where $C$ is the channel\ncapacity. The last inequality is often strict, in which case the ramification\nof using the min-sum approximation is that we can no longer achieve capacity.",
        "Let $X$ be a (topological) space and $Cl(X)$ the collection of nonempty\nclosed subsets of $X$. Given a topology on $Cl(X)$, making $Cl(X)$ a space, a\n\\emph{(subset) hyperspace} of $X$ is a subspace $\\mathcal{J}\\subset Cl(X)$ with\nan embedding $X\\hookrightarrow\\mathcal{J}$, $x\\mapsto\\{x\\}$ (which thus\nrequires $X$ to be $T_1$). In this note, we characterize certain hyperspaces\n$\\mathcal{J}\\subset Cl(X)$ as explicit quotient spaces of function spaces\n$\\mathcal{F}\\subset X^Y$ and discuss metrization of associated compact-subset\nhyperspaces in this setting.",
        "In [MV] the difference qKZ equations were considered modulo a prime number\n$p$ and a family of polynomial solutions of the qKZ equations modulo $p$ was\nconstructed by an elementary procedure as suitable $p$-approximations of the\nhypergeometric integrals. In this paper, we study in detail the first family of\nnontrivial example of the qKZ equations in characteristic $p$. We describe all\nsolutions of these qKZ equations in characteristic $p$ by demonstrating that\nthey all stem from the $p$-hypergeometric solutions. We also prove a Lagrangian\nproperty (called the orthogonality property) of the subbundle of the qKZ bundle\nspanned by the $p$-hypergeometric sections. This paper extends the results of\n[VV1] on the differential KZ equations to the difference qKZ equations.",
        "We show that Dal Maso's GBD space, introduced for tackling crack growth in\nlinearized elasticity, can be defined by simple conditions in a finite number\nof directions of slicing.",
        "Minimally invasive surgery (MIS) has transformed clinical practice by\nreducing recovery times, minimizing complications, and enhancing precision.\nNonetheless, MIS inherently relies on indirect visualization and precise\ninstrument control, posing unique challenges. Recent advances in artificial\nintelligence have enabled real-time surgical scene understanding through\ntechniques such as image classification, object detection, and segmentation,\nwith scene reconstruction emerging as a key element for enhanced intraoperative\nguidance. Although neural radiance fields (NeRFs) have been explored for this\npurpose, their substantial data requirements and slow rendering inhibit\nreal-time performance. In contrast, 3D Gaussian Splatting (3DGS) offers a more\nefficient alternative, achieving state-of-the-art performance in dynamic\nsurgical scene reconstruction. In this work, we introduce Feature-EndoGaussian\n(FEG), an extension of 3DGS that integrates 2D segmentation cues into 3D\nrendering to enable real-time semantic and scene reconstruction. By leveraging\npretrained segmentation foundation models, FEG incorporates semantic feature\ndistillation within the Gaussian deformation framework, thereby enhancing both\nreconstruction fidelity and segmentation accuracy. On the EndoNeRF dataset, FEG\nachieves superior performance (SSIM of 0.97, PSNR of 39.08, and LPIPS of 0.03)\ncompared to leading methods. Additionally, on the EndoVis18 dataset, FEG\ndemonstrates competitive class-wise segmentation metrics while balancing model\nsize and real-time performance.",
        "This paper takes a critical look at the recommendations OSCE\/ODIHR has given\nfor the Estonian Internet voting over the 20 years it has been running. We\npresent examples of recommendations that can not be fulfilled at all, but also\nexamples where fulfilling a recommendation requires a non-trivial trade-off,\npotentially weakening the system in some other respect. In such cases\nOSCE\/ODIHR should take an explicit position which trade-off it recommends. We\nalso look at the development of the recommendation to introduce end-to-end\nverifiability. In this case we expect OSCE\/ODIHR to define what it exactly\nmeans by this property, as well as to give explicit criteria to determine\nwhether and to which extent end-to-end verifiability has been achieved.",
        "We prove the conjecture of Przytycki and Sazdanovic that the Khovanov\nhomology of the closure of a 3-stranded braid only contains torsion of order 2.\nThis conjecture has been known for six out of seven classes in the\nMurasugi-classification of 3-braids and we show it for the remaining class. Our\nproof also works for the other classes and relies on Bar-Natan's version of\nKhovanov homology for tangles as well as his delooping and cancellation\ntechniques, and the reduced integral Bar-Natan--Lee--Turner spectral sequence.\nWe also show that the Knight-move conjecture holds for 3-braids.",
        "Robots, as AI with physical instantiation, inhabit our social and physical\nworld, where their actions have both social and physical consequences, posing\nchallenges for researchers when designing social robots. This study starts with\na scoping review to identify discussions and potential concerns arising from\ninteractions with robotic systems. Two focus groups of technology ethics\nexperts then validated a comprehensive list of key topics and values in\nhuman-robot interaction (HRI) literature. These insights were integrated into\nthe HRI Value Compass web tool, to help HRI researchers identify ethical values\nin robot design. The tool was evaluated in a pilot study. This work benefits\nthe HRI community by highlighting key concerns in human-robot interactions and\nproviding an instrument to help researchers design robots that align with human\nvalues, ensuring future robotic systems adhere to these values in social\napplications.",
        "In specialized fields like the scientific domain, constructing large-scale\nhuman-annotated datasets poses a significant challenge due to the need for\ndomain expertise. Recent methods have employed large language models to\ngenerate synthetic queries, which serve as proxies for actual user queries.\nHowever, they lack control over the content generated, often resulting in\nincomplete coverage of academic concepts in documents. We introduce Concept\nCoverage-based Query set Generation (CCQGen) framework, designed to generate a\nset of queries with comprehensive coverage of the document's concepts. A key\ndistinction of CCQGen is that it adaptively adjusts the generation process\nbased on the previously generated queries. We identify concepts not\nsufficiently covered by previous queries, and leverage them as conditions for\nsubsequent query generation. This approach guides each new query to complement\nthe previous ones, aiding in a thorough understanding of the document.\nExtensive experiments demonstrate that CCQGen significantly enhances query\nquality and retrieval performance.",
        "In the present chapter, we obtain the reconstruction of Theaetetus' theory of\nratios of magnitudes based, according to Aristotle's Topics 158b, on the\ndefinition of proportion in terms of equal anthyphairesis. Our reconstruction\nis built on the anthyphairetic interpretation of the notoriously difficult\nTheaetetus 147d6-e1 passage on Theaetetus' mathematical discovery of quadratic\nincommensurabilities, itself based on the traces it has left on Plato's\nphilosophical definition of Knowledge in his dialogues Theaetetus, Sophist and\nMeno. Contrary to earlier reconstructions by Becker, van der Waerden and Knorr,\nour reconstruction reveals a theory that (a) applies only to the restricted\nclass of pairs of magnitudes whose anthyphairesis is finite or eventually\nperiodic, and (b) avoids the problematic use of Eudoxus' definition 4 of Book V\nof Euclid's Elements.\n  The final version of this paper will appear as a chapter in the book Essays\non Topology: Dedicated to Valentin Po\\'enaru, ed. L. Funar and A. Papadopoulos,\nSpringer, 2025.",
        "The rapid advancement of generative AI technology, particularly video\ngenerative AI (Video GenAI), has opened new possibilities for K-12 education by\nenabling the creation of dynamic, customized, and high-quality visual content.\nDespite its potential, there is limited research on how this emerging\ntechnology can be effectively integrated into educational practices. This study\nexplores the perspectives of leading K-12 teachers on the educational\napplications of Video GenAI, using the TPACK (Technological Pedagogical Content\nKnowledge) and TAM (Technology Acceptance Model) frameworks as analytical\nlenses. Through interviews and hands-on experimentation with video generation\ntools, the research identifies opportunities for enhancing teaching strategies,\nfostering student engagement, and supporting authentic task design. It also\nhighlights challenges such as technical limitations, ethical considerations,\nand the need for institutional support. The findings provide actionable\ninsights into how Video GenAI can transform teaching and learning, offering\npractical implications for policy, teacher training, and the future development\nof educational technology.",
        "Many video-to-audio (VTA) methods have been proposed for dubbing silent\nAI-generated videos. An efficient quality assessment method for AI-generated\naudio-visual content (AGAV) is crucial for ensuring audio-visual quality.\nExisting audio-visual quality assessment methods struggle with unique\ndistortions in AGAVs, such as unrealistic and inconsistent elements. To address\nthis, we introduce AGAVQA, the first large-scale AGAV quality assessment\ndataset, comprising 3,382 AGAVs from 16 VTA methods. AGAVQA includes two\nsubsets: AGAVQA-MOS, which provides multi-dimensional scores for audio quality,\ncontent consistency, and overall quality, and AGAVQA-Pair, designed for optimal\nAGAV pair selection. We further propose AGAV-Rater, a LMM-based model that can\nscore AGAVs, as well as audio and music generated from text, across multiple\ndimensions, and selects the best AGAV generated by VTA methods to present to\nthe user. AGAV-Rater achieves state-of-the-art performance on AGAVQA,\nText-to-Audio, and Text-to-Music datasets. Subjective tests also confirm that\nAGAV-Rater enhances VTA performance and user experience. The project page is\navailable at https:\/\/agav-rater.github.io."
      ]
    }
  },
  {
    "id":2411.0484,
    "research_type":"basic",
    "start_id":"b9",
    "start_title":"Kinetic description and convergence analysis of genetic algorithms for global optimization",
    "start_abstract":"Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "Genetic Algorithms + Data Structures = Evolution Programs"
      ],
      "abstract":[
        "Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation."
      ],
      "categories":[
        "cs.DS"
      ]
    },
    "list":{
      "title":[
        "Neuromorphic Circuits with Spiking Astrocytes for Increased Energy\n  Efficiency, Fault Tolerance, and Memory Capacitance",
        "Bargaining with transfers",
        "Constraints on extended axion structures from the lensing of fast radio\n  bursts",
        "OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes",
        "Bangla Fake News Detection Based On Multichannel Combined CNN-LSTM",
        "MAIA: A new detector concept for a 10 TeV muon collider",
        "High-pressure growth effect on the properties of high-Tc iron-based\n  superconductors: A short review",
        "The Dead Internet Theory: A Survey on Artificial Interactions and the\n  Future of Social Media",
        "Talking to GDELT Through Knowledge Graphs",
        "The First Chemical Census the Milky Way's Nuclear Star Cluster",
        "Seismic wavefield solutions via physics-guided generative neural\n  operator",
        "Drop size distribution from laboratory experiments based on single-drop\n  fragmentation and comparison with aerial in-situ measurements",
        "Quantitative study on anomalous Nernst effect in Co thin films by laser\n  irradiation",
        "Temperature-Distance Relations in Casimir Physics",
        "Formation of Singularity and Apparent Horizon for Dissipative Collapse\n  in $f(R,T)$ Theory of Gravity",
        "Adapting Video Diffusion Models for Time-Lapse Microscopy",
        "Generalized black-bounces solutions in f(R) gravity and their field\n  sources",
        "The angular momentum spiral of the Milky Way disc in Gaia",
        "Near-Term Fermionic Simulation with Subspace Noise Tailored Quantum\n  Error Mitigation",
        "Extremal eigenvectors of sparse random matrices",
        "Long-term evolution of Sco X-1: implications for the current spin\n  frequency and ellipticity of the neutron star",
        "Shedding Infrared Light on QCD Axion and ALP Dark Matter with JWST",
        "Ancilla-free Quantum Adder with Sublinear Depth",
        "What Does a Software Engineer Look Like? Exploring Societal Stereotypes\n  in LLMs",
        "Lifelong Evolution of Swarms",
        "Metastable Cosmic Strings and Gravitational Waves from Flavour Symmetry\n  Breaking",
        "Towards Lightweight, Adaptive and Attribute-Aware Multi-Aspect\n  Controllable Text Generation with Large Language Models",
        "FedSDP: Explainable Differential Privacy in Federated Learning via\n  Shapley Values",
        "Decision Making in Changing Environments: Robustness, Query-Based\n  Learning, and Differential Privacy"
      ],
      "abstract":[
        "In the rapidly advancing field of neuromorphic computing, integrating\nbiologically-inspired models like the Leaky Integrate-and-Fire Astrocyte (LIFA)\ninto spiking neural networks (SNNs) enhances system robustness and performance.\nThis paper introduces the LIFA model in SNNs, addressing energy efficiency,\nmemory management, routing mechanisms, and fault tolerance. Our core\narchitecture consists of neurons, synapses, and astrocyte circuits, with each\nastrocyte supporting multiple neurons for self-repair. This clustered model\nimproves fault tolerance and operational efficiency, especially under adverse\nconditions. We developed a routing methodology to map the LIFA model onto a\nfault-tolerant, many-core design, optimizing network functionality and\nefficiency. Our model features a fault tolerance rate of 81.10\\% and a\nresilience improvement rate of 18.90\\%, significantly surpassing other\nimplementations. The results validate our approach in memory management,\nhighlighting its potential as a robust solution for advanced neuromorphic\ncomputing applications. The integration of astrocytes represents a significant\nadvancement, setting the stage for more resilient and adaptable neuromorphic\nsystems.",
        "We propose a solution to the problem of bargaining with transfers, along with\nan axiomatisation of the solution. Inefficient allocations in the bargaining\nset can influence the solution, but are discounted relative to more efficient\nones. The key axioms are additivity and a property we call \\emph{inverse\nmonotonicity}, which states that adding an allocation to the bargaining set\nthat is worse for a given player than the initial solution cannot benefit that\nplayer.",
        "Axions are hypothetical pseudoscalar particles that have been regarded as\npromising dark matter (DM) candidates. On the other hand, extended compact\nobjects such as axion stars, which are supported by gravity and axion self\ninteractions, may have also been formed in the early Universe and comprise part\nof DM. In this work, we consider the lensing of electromagnetic signals from\ndistant sources by axion stars, as a way to constrain the properties of axion\nstars and fundamental axion parameters. Accounting for the effect of the finite\nsize of the axion star, we study the lensing effect induced by gravity and the\naxion-photon coupling. The latter effect is frequency dependent, and is\nrelevant in the low frequency band, which motivates the use of fast radio burst\n(FRB) signals as a probe. We calculate the predicted number of lensed FRB\nevents by specifying the fundamental axion parameters, axion star radial\nprofile, fraction of DM residing in axion stars, and imposing lensing criteria\nbased on the flux ratio and time delay between the brightest images from\nlensing. Assuming an optimistic case of $10^4$ observed FRB events, and a\ntiming resolution of $1~\\mu{\\rm s}$, the lack of observed FRB lensing events in\nCHIME allows us to probe axion stars with mass $ \\gtrsim 2 \\times 10^{-2}\nM_\\odot$, corresponding to axion masses $\\lesssim 10^{-10}{\\rm eV}$. We obtain\nconstraints for even lighter axion stars up to $\\sim 10^{-3} M_\\odot$, when the\naxion-photon interactions are taken into account. Our results indicate that FRB\nlensing lead to constraints that are competitive with conventional microlensing\nsearches operating in the optical band.",
        "Images generated by text-to-image (T2I) models often exhibit visual biases\nand stereotypes of concepts such as culture and profession. Existing\nquantitative measures of stereotypes are based on statistical parity that does\nnot align with the sociological definition of stereotypes and, therefore,\nincorrectly categorizes biases as stereotypes. Instead of oversimplifying\nstereotypes as biases, we propose a quantitative measure of stereotypes that\naligns with its sociological definition. We then propose OASIS to measure the\nstereotypes in a generated dataset and understand their origins within the T2I\nmodel. OASIS includes two scores to measure stereotypes from a generated image\ndataset: (M1) Stereotype Score to measure the distributional violation of\nstereotypical attributes, and (M2) WALS to measure spectral variance in the\nimages along a stereotypical attribute. OASIS also includes two methods to\nunderstand the origins of stereotypes in T2I models: (U1) StOP to discover\nattributes that the T2I model internally associates with a given concept, and\n(U2) SPI to quantify the emergence of stereotypical attributes in the latent\nspace of the T2I model during image generation. Despite the considerable\nprogress in image fidelity, using OASIS, we conclude that newer T2I models such\nas FLUX.1 and SDv3 contain strong stereotypical predispositions about concepts\nand still generate images with widespread stereotypical attributes.\nAdditionally, the quantity of stereotypes worsens for nationalities with lower\nInternet footprints.",
        "There have recently been many cases of unverified or misleading information\ncirculating quickly over bogus web networks and news portals. This false news\ncreates big damage to society and misleads people. For Example, in 2019, there\nwas a rumor that the Padma Bridge of Bangladesh needed 100,000 human heads for\nsacrifice. This rumor turns into a deadly position and this misleading\ninformation takes the lives of innocent people. There is a lot of work in\nEnglish but a few works in Bangla. In this study, we are going to identify the\nfake news from the unconsidered news source to provide the newsreader with\nnatural news or real news. The paper is based on the combination of\nconvolutional neural network (CNN) and long short-term memory (LSTM), where CNN\nis used for deep feature extraction and LSTM is used for detection using the\nextracted feature. The first thing we did to deploy this piece of work was data\ncollection. We compiled a data set from websites and attempted to deploy it\nusing the methodology of deep learning which contains about 50k of news. With\nthe proposed model of Multichannel combined CNN-LSTM architecture, our model\ngained an accuracy of 75.05%, which is a good sign for detecting fake news in\nBangla.",
        "Muon colliders offer a compelling opportunity to explore the TeV scale and\nconduct precision tests of the Standard Model, all within a relatively compact\ngeographical footprint. This paper introduces a new detector concept, MAIA\n(Muon Accelerator Instrumented Apparatus), optimized for $\\sqrt{s}=10$ TeV\n$\\mu\\mu$ collisions. The detector features an all-silicon tracker immersed in a\n5T solenoid field. High-granularity silicon-tungsten and iron-scintillator\ncalorimeters surrounding the solenoid capture high-energy electronic and\nhadronic showers, respectively, and support particle-flow reconstruction. The\noutermost subsystem comprises an air-gap muon spectrometer, which enables\nstandalone track reconstruction for high-momentum muons. The performance of the\nMAIA detector is evaluated in terms of differential particle reconstruction\nefficiencies and resolutions. Beam-induced background (BIB) simulations\ngenerated in FLUKA are overlaid with single particle gun samples to assess\ndetector reconstruction capabilities under realistic experimental conditions.\nEven with BIB, reconstruction efficiencies exceed 95% for energetic tracks,\nphotons, and neutrons in the central region of the detector. This paper\noutlines promising avenues of future work, including forward region\noptimization and opportunities for enhanced flavor\/boosted object tagging, and\naddresses the technological assumptions needed to achieve the desired detector\nperformance.",
        "The high-pressure growth technique is a vital approach that facilitates the\nstabilization of new phases and allows for meticulous control of structural\nparameters, which significantly impact electronic and magnetic properties. We\npresent a short review of our ongoing investigations into various families of\niron-based superconductors (IBS), employing the high-gas pressure and\nhigh-temperature synthesis (HP-HTS) method. This technique is capable of\nproducing the gas pressures up to 1.8 GPa and a heating temperature of up to\n1700 {\\deg}C through a three-zone furnace within a cylindrical chamber.\nDifferent kinds of IBS samples are prepared using HPHTS and characterized\nthrough various measurements to reach the final conclusions. The results\ndemonstrate that the high-pressure growth technique significantly enhances the\nproperties of IBS, including the transition temperature, critical current\ndensity, and pinning force. In addition, the quality of the samples and their\ndensity are improved through the intergrain connections. Furthermore, the\ncomprehensive evaluations and investigations prove that a growth pressure of\n0.5 GPa is sufficient for producing high-quality IBS bulks under the optimized\nsynthesis conditions.",
        "The Dead Internet Theory (DIT) suggests that much of today's internet,\nparticularly social media, is dominated by non-human activity, AI-generated\ncontent, and corporate agendas, leading to a decline in authentic human\ninteraction. This study explores the origins, core claims, and implications of\nDIT, emphasizing its relevance in the context of social media platforms. The\ntheory emerged as a response to the perceived homogenization of online spaces,\nhighlighting issues like the proliferation of bots, algorithmically generated\ncontent, and the prioritization of engagement metrics over genuine user\ninteraction. AI technologies play a central role in this phenomenon, as social\nmedia platforms increasingly use algorithms and machine learning to curate\ncontent, drive engagement, and maximize advertising revenue. While these tools\nenhance scalability and personalization, they also prioritize virality and\nconsumption over authentic communication, contributing to the erosion of trust,\nthe loss of content diversity, and a dehumanized internet experience. This\nstudy redefines DIT in the context of social media, proposing that the\ncommodification of content consumption for revenue has taken precedence over\nmeaningful human connectivity. By focusing on engagement metrics, platforms\nfoster a sense of artificiality and disconnection, underscoring the need for\nhuman-centric approaches to revive authentic online interaction and community\nbuilding.",
        "In this work we study various Retrieval Augmented Regeneration (RAG)\napproaches to gain an understanding of the strengths and weaknesses of each\napproach in a question-answering analysis. To gain this understanding we use a\ncase-study subset of the Global Database of Events, Language, and Tone (GDELT)\ndataset as well as a corpus of raw text scraped from the online news articles.\nTo retrieve information from the text corpus we implement a traditional vector\nstore RAG as well as state-of-the-art large language model (LLM) based\napproaches for automatically constructing KGs and retrieving the relevant\nsubgraphs. In addition to these corpus approaches, we develop a novel\nontology-based framework for constructing knowledge graphs (KGs) from GDELT\ndirectly which leverages the underlying schema of GDELT to create structured\nrepresentations of global events. For retrieving relevant information from the\nontology-based KGs we implement both direct graph queries and state-of-the-art\ngraph retrieval approaches. We compare the performance of each method in a\nquestion-answering task. We find that while our ontology-based KGs are valuable\nfor question-answering, automated extraction of the relevant subgraphs is\nchallenging. Conversely, LLM-generated KGs, while capturing event summaries,\noften lack consistency and interpretability. Our findings suggest benefits of a\nsynergistic approach between ontology and LLM-based KG construction, with\nproposed avenues toward that end.",
        "An important step in understanding the formation and evolution of the Nuclear\nStar Cluster (NSC) is to investigate its chemistry and chemical evolution.\nAdditionally, exploring the relationship of the NSC to the other structures in\nthe Galactic Center and the Milky Way disks is of great interest. Extreme\noptical extinction has previously prevented optical studies, but near-IR\nhigh-resolution spectroscopy is now possible. Here, we present a detailed\nchemical abundance analysis of 19 elements - more than four times as many as\npreviously published - for 9 stars in the NSC of the Milky Way, observed with\nthe IGRINS spectrometer on the Gemini South telescope. This study provides new,\ncrucial observational evidence to shed light on the origin of the NSC. We\ndemonstrate that it is possible to probe a variety of nucleosynthetic channels,\nreflecting different chemical evolution timescales. Our findings reveal that\nthe NSC trends for the elements F, Mg, Al, Si, S, K, Ca, Ti, Cr, Mn, Co, Ni,\nCu, and Zn, as well as the s-process elements Ba, Ce, Nd, and Yb, generally\nfollow the inner bulge trends within uncertainties. This suggests a likely\nshared evolutionary history and our results indicate that the NSC population is\nconsistent with the chemical sequence observed in the inner Galaxy (the\ninner-disk sequence). However, we identify a significant and unexplained\ndifference in the form of higher Na abundances in the NSC compared to the\ninner-bulge. This is also observed in few Galactic globular clusters, and may\nsuggest a common enrichment process at work in all these systems.",
        "Current neural operators often struggle to generalize to complex,\nout-of-distribution conditions, limiting their ability in seismic wavefield\nrepresentation. To address this, we propose a generative neural operator (GNO)\nthat leverages generative diffusion models (GDMs) to learn the underlying\nstatistical distribution of scattered wavefields while incorporating a\nphysics-guided sampling process at each inference step. This physics guidance\nenforces wave equation-based constraints corresponding to specific velocity\nmodels, driving the iteratively generated wavefields toward physically\nconsistent solutions. By training the diffusion model on wavefields\ncorresponding to a diverse dataset of velocity models, frequencies, and source\npositions, our GNO enables to rapidly synthesize high-fidelity wavefields at\ninference time. Numerical experiments demonstrate that our GNO not only\nproduces accurate wavefields matching numerical reference solutions, but also\ngeneralizes effectively to previously unseen velocity models and frequencies.",
        "Laboratory experiments and theoretical modelling are conducted to determine\nthe raindrop size distribution (DSD) resulting from distinct fragmentation\nprocesses under various upward airstreams. Since weather radar echoes are\nproportional to the sixth power of the average droplet diameter, understanding\nthe fragmentation mechanisms that lead to different breakup sizes is crucial\nfor accurate rainfall predictions. We utilize a two-parameter gamma\ndistribution for theoretical modelling and estimate the average droplet\ndiameter from the theoretically obtained characteristic sizes, often treated as\nassumed input parameters for different rain conditions in rainfall modelling.\nOur experimental and theoretical findings demonstrate a close agreement with\nthe DSD predicted by the Marshall and Palmer relationship for steady rain\nconditions. Additionally, in situ DSD measurements at different altitudes were\nobtained through research flights equipped with advanced sensors, further\nvalidating our rainfall model. This study underscores the effectiveness of\nlaboratory-scale experiments and the critical importance of accurately\ncharacterizing DSD to enhance rainfall predictions.",
        "The anomalous Nernst effect (ANE) generates electromotive forces transverse\nto temperature gradients and has attracted much attention for potential\napplications into novel thermoelectric power generators. ANE efficiency is\ngenerally characterized by uniform temperature gradients in a steady state\nprepared by heaters. However, although focusing laser beams on a magnetic film\ncan form much larger temperature gradients, the laser irradiation method has\nnot been sufficiently considered for quantifying the ANE coefficient due to the\ndifficulty in estimating the localized in-homogeneous temperature gradients. In\nthis study, we present a quantitative study of ANE in Ru(5\nnm)\/Co($t_{\\mathrm{Co}}$) ($t_{\\mathrm{Co}}$ = 3, 5, 7, 10, 20, 40, and 60 nm)\nbilayers on sapphire (0001) substrates by combining a laser irradiation\napproach with finite element analysis of temperature gradients under laser\nexcitation. We find that the estimated ANE coefficients are consistent with\npreviously reported values and one independently characterized using a heater.\nOur results also reveal the advantages of the laser irradiation method over the\nconventional method using heaters. Intensity-modulated laser beams can create\nac temperature gradients as large as approximately 10$^3$ K\/mm at a frequency\nof tens of kilohertz in a micrometer-scale region.",
        "The Casimir-Lifshitz force arises from thermal and quantum mechanical\nfluctuations between classical bodies and becomes significant below the micron\nscale. We explore temperature-distance relations based on the concepts of Wick\nand Bohr arising from energy-time uncertainty relations. We show that\ntemperature-distance relations similar to those arising from the uncertainty\nprinciple are found in various Casimir interactions, with an exact relation\noccurring in the low-temperature regime when the zero point energy contribution\ncancels the thermal radiation pressure contribution between two plates.",
        "In this paper, we consider the spherically symmetric gravitational collapse\nof isotropic matter undergoing dissipation in the form of heat flux, with a\ngeneralized Vaidya exterior, in the context of $f(R, T)$ gravity. Choosing\n$f(R, T)=R+2\\lambda T$, and applying the $f(R, T)$ junction conditions on the\nfield equations for the interior and exterior regions, we have obtained\nmatching conditions of the matter-Lagrangian and its derivatives across the\nboundary. The time of formation of singularity and the time of formation of\napparent horizon have been determined and constraints on the integration\nconstants are examined for which the final singularity is hidden behind the\nhorizon.",
        "We present a domain adaptation of video diffusion models to generate highly\nrealistic time-lapse microscopy videos of cell division in HeLa cells. Although\nstate-of-the-art generative video models have advanced significantly for\nnatural videos, they remain underexplored in microscopy domains. To address\nthis gap, we fine-tune a pretrained video diffusion model on\nmicroscopy-specific sequences, exploring three conditioning strategies: (1)\ntext prompts derived from numeric phenotypic measurements (e.g., proliferation\nrates, migration speeds, cell-death frequencies), (2) direct numeric embeddings\nof phenotype scores, and (3) image-conditioned generation, where an initial\nmicroscopy frame is extended into a complete video sequence. Evaluation using\nbiologically meaningful morphological, proliferation, and migration metrics\ndemonstrates that fine-tuning substantially improves realism and accurately\ncaptures critical cellular behaviors such as mitosis and migration. Notably,\nthe fine-tuned model also generalizes beyond the training horizon, generating\ncoherent cell dynamics even in extended sequences. However, precisely\ncontrolling specific phenotypic characteristics remains challenging,\nhighlighting opportunities for future work to enhance conditioning methods. Our\nresults demonstrate the potential for domain-specific fine-tuning of generative\nvideo models to produce biologically plausible synthetic microscopy data,\nsupporting applications such as in-silico hypothesis testing and data\naugmentation.",
        "In this work, following our recent findings in [1], we extend our analysis to\nexplore the generalization of spherically symmetric and static black-bounce\nsolutions, known from General Relativity, within the framework of the $f(R)$\ntheory in the metric formalism. We develop a general approach to determine the\nsources for any model where $f(R) = R + H(R)$, provided that the corresponding\nsource for the bounce metric in General Relativity is known. As a result, we\ndemonstrate that black-bounce solutions can emerge from this theory when\nconsidering the coupling of $f(R)$ gravity with nonlinear electrodynamics and a\npartially phantom scalar field. We also analyzed the energy conditions of these\nsolutions and found that, unlike in General Relativity, it is possible to\nsatisfy all energy conditions in certain regions of space-time.",
        "Data from the Gaia mission shows prominent phase-space spirals that are the\nsignatures of disequilibrium in the Milky Way (MW) disc. In this work, we\npresent a novel perspective on the phase-space spiral in angular momentum (AM)\nspace. Using Gaia DR3, we detect a prominent AM spiral in the solar\nneighbourhood. We demonstrate the relation of AM to the $z-v_z$ spiral and show\nthat we can map to this space from angular momentum through simplifying\nassumptions. By modelling the orbit of stars in AM, we develop a generative\nmodel for the spiral where the disc is perturbed by a bulk tilt at an earlier\ntime. Our model successfully describes the salient features of the AM spiral in\nthe data. Modelling the phase spiral in AM is a promising method to constrain\nboth perturbation and MW potential parameters. Our AM framework simplifies the\ninterpretation of the spiral and offers a robust approach to modelling\ndisequilibrium in the MW disc using all six dimensions of phase space\nsimultaneously.",
        "Quantum error mitigation (QEM) has emerged as a powerful tool for the\nextraction of useful quantum information from quantum devices. Here, we\nintroduce the Subspace Noise Tailoring (SNT) algorithm, which efficiently\ncombines the cheap cost of Symmetry Verification (SV) and low bias of\nProbabilistic Error Cancellation (PEC) QEM techniques. We study the performance\nof our method by simulating the Trotterized time evolution of the spin-1\/2\nFermi-Hubbard model (FHM) using a variety of local fermion-to-qubit encodings,\nwhich define a computational subspace through a set of stabilizers, the\nmeasurement of which can be used to post-select noisy quantum data. We study\ndifferent combinations of QEM and encodings and uncover a rich phase diagram of\noptimal combinations, depending on the hardware performance, system size and\navailable shot budget. We then demonstrate how SNT extends the reach of current\nnoisy quantum computers in terms of the number of fermionic lattice sites and\nthe number of Trotter steps, and quantify the required hardware performance\nbeyond which a noisy device may outperform classical computational methods.",
        "We consider a class of sparse random matrices, which includes the adjacency\nmatrix of Erd\\H{o}s-R\\'enyi graph ${\\bf G}(N,p)$. For $N^{-1+o(1)}\\leq p\\leq\n1\/2$, we show that the non-trivial edge eigenvectors are asymptotically jointly\nnormal. The main ingredient of the proof is an algorithm that directly computes\nthe joint eigenvector distributions, without comparisons with GOE. The method\nis applicable in general. As an illustration, we also use it to prove the\nnormal fluctuation in quantum ergodicity at the edge for Wigner matrices.\nAnother ingredient of the proof is the isotropic local law for sparse matrices,\nwhich at the same time improves several existing results.",
        "Sco X-1 is the brightest observed extra-solar X-ray source, which is a\nneutron star (NS) low-mass X-ray binary (LMXB), and is thought to have a strong\npotential for continuous gravitational waves (CW) detection due to its high\naccretion rate and relative proximity. Here, we compute the long-term evolution\nof its parameters, particularly the NS spin frequency ($\\nu$) and the surface\nmagnetic field ($B$), to probe its nature and its potential for CW detection.\nWe find that Sco X-1 is an unusually young ($\\sim7\\times10^6$ yr) LMXB and\nconstrain the current NS mass to $\\sim 1.4-1.6~{\\rm M}_\\odot$. Our computations\nreveal a rapid $B$ decay, with the maximum current value of $\\sim\n1.8\\times10^8$ G, which can be useful to constrain the decay models. Note that\nthe maximum current $\\nu$ value is $\\sim 550$ Hz, implying that, unlike what is\ngenerally believed, a CW emission is not required to explain the current source\nproperties. However, $\\nu$ will exceed an observed cut-off frequency of $\\sim\n730$ Hz, and perhaps even the NS break-up frequency, in the future, without a\nCW emission. The minimum NS mass quadrupole moment ($Q$) to avoid this is $\\sim\n(2-3)\\times10^{37}$ g cm$^2$, corresponding to a CW strain of $\\sim 10^{-26}$.\nOur estimation of current $\\nu$ values can improve the CW search sensitivity.",
        "James Webb Space Telescope (JWST) has opened up a new chapter in infrared\nastronomy. Besides the discovery and a deeper understanding of various\nastrophysical sources, JWST can also uncover the non-gravitational nature of\ndark matter (DM). If DM is QCD axion or an eV-scale Axion-like particle (ALP),\nit can decay into two photons in the infrared band. This will produce a\ndistinct line signature in the spectroscopic observations made by JWST. Using\nthe latest NIRSpec IFU spectroscopic observations from JWST, we put the\nstrongest bound on the photon coupling for QCD axion\/ ALP DM in the mass range\nbetween 0.47 and 2.55 eV. In particular, we are able to probe a new mass range\nfor ALP DM between $\\sim$ 0.47 eV to 0.78 eV beyond what can be probed by\nglobular cluster observations. We constrain well-motivated and UV complete\nmodels of QCD axion and ALP DM, including predictions from some models derived\nfrom string theory and\/ or various Grand Unification scenarios. Future JWST\nobservations of DM-rich systems with a better understanding of the\nastrophysical and instrumental backgrounds can thus enable us to potentially\ndiscover QCD axion and ALP DM. The datasets used in this work are available at:\nhttps:\/\/dx.doi.org\/10.17909\/3e5f-nv69",
        "We present the first exact quantum adder with sublinear depth and no ancilla\nqubits. Our construction is based on classical reversible logic only and\nemploys low-depth implementations for the CNOT ladder operator and the Toffoli\nladder operator, two key components to perform ripple-carry addition. Namely,\nwe demonstrate that any ladder of $n$ CNOT gates can be replaced by a\nCNOT-circuit with $O(\\log n)$ depth, while maintaining a linear number of\ngates. We then generalize this construction to Toffoli gates and demonstrate\nthat any ladder of $n$ Toffoli gates can be substituted with a circuit with\n$O(\\log^2 n)$ depth while utilizing a linearithmic number of gates. This builds\non the recent works of Nie et al. and Khattar and Gidney on the technique of\nconditionally clean ancillae. By combining these two key elements, we present a\nnovel approach to design quantum adders that can perform the addition of two\n$n$-bit numbers in depth $O(\\log^2 n)$ without the use of any ancilla and using\nclassical reversible logic only (Toffoli, CNOT and X gates).",
        "Large language models (LLMs) have rapidly gained popularity and are being\nembedded into professional applications due to their capabilities in generating\nhuman-like content. However, unquestioned reliance on their outputs and\nrecommendations can be problematic as LLMs can reinforce societal biases and\nstereotypes. This study investigates how LLMs, specifically OpenAI's GPT-4 and\nMicrosoft Copilot, can reinforce gender and racial stereotypes within the\nsoftware engineering (SE) profession through both textual and graphical\noutputs. We used each LLM to generate 300 profiles, consisting of 100\ngender-based and 50 gender-neutral profiles, for a recruitment scenario in SE\nroles. Recommendations were generated for each profile and evaluated against\nthe job requirements for four distinct SE positions. Each LLM was asked to\nselect the top 5 candidates and subsequently the best candidate for each role.\nEach LLM was also asked to generate images for the top 5 candidates, providing\na dataset for analysing potential biases in both text-based selections and\nvisual representations. Our analysis reveals that both models preferred male\nand Caucasian profiles, particularly for senior roles, and favoured images\nfeaturing traits such as lighter skin tones, slimmer body types, and younger\nappearances. These findings highlight underlying societal biases influence the\noutputs of LLMs, contributing to narrow, exclusionary stereotypes that can\nfurther limit diversity and perpetuate inequities in the SE field. As LLMs are\nincreasingly adopted within SE research and professional practices, awareness\nof these biases is crucial to prevent the reinforcement of discriminatory norms\nand to ensure that AI tools are leveraged to promote an inclusive and equitable\nengineering culture rather than hinder it.",
        "Adapting to task changes without forgetting previous knowledge is a key skill\nfor intelligent systems, and a crucial aspect of lifelong learning. Swarm\ncontrollers, however, are typically designed for specific tasks, lacking the\nability to retain knowledge across changing tasks. Lifelong learning, on the\nother hand, focuses on individual agents with limited insights into the\nemergent abilities of a collective like a swarm. To address this gap, we\nintroduce a lifelong evolutionary framework for swarms, where a population of\nswarm controllers is evolved in a dynamic environment that incrementally\npresents novel tasks. This requires evolution to find controllers that quickly\nadapt to new tasks while retaining knowledge of previous ones, as they may\nreappear in the future. We discover that the population inherently preserves\ninformation about previous tasks, and it can reuse it to foster adaptation and\nmitigate forgetting. In contrast, the top-performing individual for a given\ntask catastrophically forgets previous tasks. To mitigate this phenomenon, we\ndesign a regularization process for the evolutionary algorithm, reducing\nforgetting in top-performing individuals. Evolving swarms in a lifelong fashion\nraises fundamental questions on the current state of deep lifelong learning and\non the robustness of swarm controllers in dynamic environments.",
        "Metastable cosmic strings (MSCSs) are among the best-fitting explanations of\nthe 2023 pulsar timing array (PTA) signal for gravitational waves at nanohertz\nfrequencies. We propose the novel possibility that a network of MSCSs\ngenerating this signal originates from the multi-step spontaneous breaking of a\ngauged flavour symmetry. As a specific example, we construct a model of $SU(2)$\nflavour symmetry in the context of $SU(5)$ grand unification, where the $SU(2)$\nacts exclusively on the first two generations of the matter 10-plet, such that\nit is ``right for leptons'' and allows for large lepton mixing. The model\nexplains the mass hierarchies of the Standard Model fermions, and predicts the\nstring scale of the MSCSs in a range compatible with the 2023 PTA signal.\nCosmic inflation is associated with the latter step of (two-step) family\nsymmetry breaking, and the phase transition ending inflation generates the\ncosmic string network.",
        "Multi-aspect controllable text generation aims to control text generation in\nattributes from multiple aspects, making it a complex but powerful task in\nnatural language processing. Supervised fine-tuning methods are often employed\nfor this task due to their simplicity and effectiveness. However, they still\nhave some limitations: low rank adaptation (LoRA) only fine-tunes a few\nparameters and has suboptimal control effects, while full fine-tuning (FFT)\nrequires significant computational resources and is susceptible to overfitting,\nparticularly when data is limited. Moreover, existing works typically train\nmulti-aspect controllable text generation models using only single-aspect\nannotated data, which results in discrepancies in data distribution; at the\nsame time, accurately generating text with specific attributes is a challenge\nthat requires strong attribute-aware capabilities. To address these\nlimitations, we propose a lightweight, adaptive and attribute-aware framework\nfor multi-aspect controllable text generation. Our framework can dynamically\nadjust model parameters according to different aspects of data to achieve\ncontrollable text generation, aiming to optimize performance across multiple\naspects. Experimental results show that our framework outperforms other strong\nbaselines, achieves state-of-the-art performance, adapts well to data\ndiscrepancies, and is more accurate in attribute perception.",
        "Federated learning (FL) enables participants to store data locally while\ncollaborating in training, yet it remains vulnerable to privacy attacks, such\nas data reconstruction. Existing differential privacy (DP) technologies inject\nnoise dynamically into the training process to mitigate the impact of excessive\nnoise. However, this dynamic scheduling is often grounded in factors indirectly\nrelated to privacy, making it difficult to clearly explain the intricate\nrelationship between dynamic noise adjustments and privacy requirements. To\naddress this issue, we propose FedSDP, a novel and explainable DP-based privacy\nprotection mechanism that guides noise injection based on privacy contribution.\nSpecifically, FedSDP leverages Shapley values to assess the contribution of\nprivate attributes to local model training and dynamically adjusts the amount\nof noise injected accordingly. By providing theoretical insights into the\ninjection of varying scales of noise into local training, FedSDP enhances\ninterpretability. Extensive experiments demonstrate that FedSDP can achieve a\nsuperior balance between privacy preservation and model performance, surpassing\nstate-of-the-art (SOTA) solutions.",
        "We study the problem of interactive decision making in which the underlying\nenvironment changes over time subject to given constraints. We propose a\nframework, which we call \\textit{hybrid Decision Making with Structured\nObservations} (hybrid DMSO), that provides an interpolation between the\nstochastic and adversarial settings of decision making. Within this framework,\nwe can analyze local differentially private (LDP) decision making, query-based\nlearning (in particular, SQ learning), and robust and smooth decision making\nunder the same umbrella, deriving upper and lower bounds based on variants of\nthe Decision-Estimation Coefficient (DEC). We further establish strong\nconnections between the DEC's behavior, the SQ dimension, local minimax\ncomplexity, learnability, and joint differential privacy. To showcase the\nframework's power, we provide new results for contextual bandits under the LDP\nconstraint."
      ]
    }
  },
  {
    "id":2411.0484,
    "research_type":"basic",
    "start_id":"b25",
    "start_title":"Genetic Algorithms + Data Structures = Evolution Programs",
    "start_abstract":"Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation.",
    "start_categories":[
      "cs.DS"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Kinetic description and convergence analysis of genetic algorithms for global optimization"
      ],
      "abstract":[
        "Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Strongly nonlinear age structured equation,time-elapsed model and large\n  delays",
        "Benchmarking nuclear matrix elements of $0\\nu\\beta\\beta$ decay with\n  high-energy nuclear collisions",
        "Periodic orbits and their gravitational wave radiations around the\n  Schwarzschild-MOG black hole",
        "A positive product formula of integral kernels of $k$-Hankel transforms",
        "Floquet geometric squeezing in fast-rotating condensates",
        "Clarkson-McCarthy inequality on a locally compact group",
        "The BAD Paradox: A Critical Assessment of the Belin\/Ambr\\'osio Deviation\n  Model",
        "Low-rank variance reduction for uncertain radiative transfer with\n  control variates",
        "Lyapunov exponent for quantum graphs that are elements of a subshift of\n  finite type",
        "Multiple Populations of the Large Magellanic Cloud Globular Cluster NGC\n  2257: No Major Environmental Effect on the Formation of Multiple Populations\n  of the Old Globular Clusters in Large Magellanic Cloud",
        "Nonlinear Spectroscopy as a Magnon Breakdown Diagnosis and its Efficient\n  Simulation",
        "Empirical Thermophotovoltaic Performance Predictions and Limits",
        "The waves-in-space Purcell effect for superconducting qubits",
        "Anomalous nuclear spin coherence in superconducting Nb$_3$Sn",
        "Effects of Initial Nucleon-Nucleon Correlations on Light Nuclei\n  Production in Au+Au Collisions at $\\sqrt{s_\\mathrm{NN}} = 3\\ $ GeV",
        "Prospection and dispersal in metapopulations: a perspective from opinion\n  dynamics models",
        "An assessment of observational coverage and gaps for robust Sun to\n  heliosphere integrated science",
        "Crosstalk analysis in single hole-spin qubits within highly anisotropic\n  g-tensors",
        "Global and local approaches for the minimization of a sum of pointwise\n  minima of convex functions",
        "Mortality simulations for insured and general populations",
        "Catalytic Nanoparticles: An Introduction",
        "Temperature-redshift relation in energy-momentum-powered gravity models",
        "iLOCO: Distribution-Free Inference for Feature Interactions",
        "On the structure of modular lattices -- Unique gluing and dissection",
        "On solvmanifolds with complex commutator and constant holomorphic\n  sectional curvature",
        "Unravelling The potential of Hybrid Borocarbonitride Biphenylene 2D\n  Network for Thermoelectric Applications: A First Principles Study",
        "Formation of Anomalously Energetic Ions in Hollow Cathode Plume by\n  Charge Separation Instability",
        "Asset pre-selection for a cardinality constrained index tracking\n  portfolio with optional enhancement",
        "Strichartz's conjecture for the spinor bundle over the real hyperbolic\n  space"
      ],
      "abstract":[
        "The time-elapsed model for neural networks is a nonlinear age structured\nequationwhere the renewal term describes the network activity and influences\nthe dischargerate, possibly with a delay due to the length of connections.We\nsolve a long standing question, namely that an inhibitory network withoutdelay\nwill converge to a steady state and thus the network is desynchonised.\nOurapproach is based on the observation that a non-expansion property holds\ntrue.However a non-degeneracy condition is needed and, besides the standard\none, weintroduce a new condition based on strict nonlinearity.When a delay is\nincluded, and following previous works for Fokker-Planck models,we prove that\nthe network may generate periodic solutions. We introduce a newformalism to\nestablish rigorously this property for large delays.The fundamental contraction\nproperty also holds for some other age structuredequations and systems.",
        "Reducing uncertainties in the nuclear matrix element (NME) remains a critical\nchallenge in designing and interpreting experiments aimed at discovering\nneutrinoless double beta ($0\\nu\\beta\\beta$) decay. Here, we identify a class of\nobservables, distinct from those employed in low-energy nuclear structure\napplications, that are strongly correlated with the NME: momentum correlations\namong hadrons produced in high-energy nuclear collisions. Focusing on the\n$^{150}$Nd$\\rightarrow$$^{150}$Sm transition, we combine a Bayesian analysis of\nthe structure of $^{150}$Nd with simulations of high-energy\n$^{150}$Nd+$^{150}$Nd collisions. We reveal prominent correlations between the\nNME and features of the quark-gluon plasma (QGP) formed in these processes,\nsuch as spatial gradients and anisotropies, which are accessible via collective\nflow measurements. Our findings demonstrate collider experiments involving\n$0\\nu\\beta\\beta$ decay candidates as a platform for benchmarking theoretical\npredictions of the NME.",
        "This article explores the motion of massive particles in the gravitational\nfield of a modified gravity (MOG) black hole (BH), characterized by the\nparameter $\\alpha$. Using the Hamiltonian formalism, the geodesic equations and\nthe effective potential governing particle trajectories are derived. Key\nfeatures, including the innermost stable circular orbit (ISCO) and the\ninnermost bound circular orbit (IBCO), are analyzed, revealing their dependence\non the particle's energy, angular momentum, and the MOG parameter. In the\nextremal case, where $\\alpha=-1$, the event horizon merges with the Cauchy\nhorizon, forming a distinctive BH configuration. Numerical methods are employed\nto compute periodic orbits in this spacetime, with a comparison drawn to the\nSchwarzschild BH. The findings indicate that for $\\alpha>0$, periodic orbits\naround Schwarzschild-MOG BH exhibit lower energy requirements than those in\nSchwarzschild spacetime, whereas for $-1<\\alpha<0$, the energy requirements are\nhigher. Precessing orbits near periodic trajectories are also examined,\noffering insights into their complex dynamical behavior. Finally, the\ngravitational wave (GW) radiation from the periodic orbits of a test particle\naround the Schwarzschild-MOG BH is examined, generating intricate waveforms\nthat provide insights into the gravitational structure of the system.",
        "Let $R$ be a root system in $\\mathbb R^N$ and $G$ be the finite subgroup\ngenerated by the reflections associated to the root system. We establish a\npositive radial product formula for the integral kernels $B_{k,1}(x,y)$ of\n$(k,1)$-generalized Fourier transforms (or the $k$-Hankel transforms) $F_{k,1}$\n$$B_{k,1}(x,z)j_{2\\left\\langle\nk\\right\\rangle+N-2}\\left(2\\sqrt{t\\left|z\\right|}\\right)=\\int_{\\mathbb R^N}\nB_{k,1}(\\xi,z)\\,d\\sigma_{x,t}^{k,1}(\\xi),$$ where $j_{\\lambda}$ is the\nnormalized Bessel function, and $\\sigma_{x,t}^{k,1}(\\xi)$ is the unique\nprobability measure. Such a product formula is equivalent to the following\nrepresentation of the generalized spherical mean operator $f\\mapsto M_f,\\;f\\in\nC_b(\\mathbb{R}^N)$ in $(k,1)$-generalized Fourier analysis \\begin{align*}\nM_f(x,t)=\\int_{\\mathbb{R}^N}f\\,d\\sigma_{x,t}^{k,1},\\;(x,t)\\in\\mathbb{R}^N\\times{\\mathbb{R}}_+.\\end{align*}\nWe will then analyze the representing measure $\\sigma_{x,t}^{k,1}(\\xi)$ and\nshow that the support of the measure is contained in\n$$\\left\\{\\xi\\in\\mathbb{R}^N:\\sqrt{\\vert\\xi\\vert}\\geq\\vert\\sqrt{\\vert\nx\\vert}-\\sqrt t\\vert\\right\\}\\cap\\left(\\bigcup_{g\\in\nG}\\{\\xi\\in\\mathbb{R}^N:d(\\xi,gx)\\leq\\sqrt t\\}\\right),$$ where\n$d\\left(x,y\\right)=\\sqrt{\\left|x\\right|+\\left|y\\right|-\\sqrt{2\\left(\\left|x\\right|\\left|y\\right|+\\left\\langle\nx,y\\right\\rangle\\right)}}$. Based on the support of the representing measure\n$\\sigma_{x,t}^{k,1}$, we will get a weak Huygens's principle for the deformed\nwave equation in $(k,1)$-generalized Fourier analysis. Moreover, for $N\\geq 2$,\nif we assume that $F_{k,1}\\left(\\mathcal S(\\mathbb{R}^N)\\right)$ consists of\nrapidly decreasing functions at infinity, then we get two different results on\n$\\text{supp}\\sigma_{x,t}^{k,1}$, which indirectly denies such assumption.",
        "Constructing and manipulating quantum states in fast-rotating Bose-Einstein\ncondensates (BEC) has long stood as a significant challenge as the rotating\nspeed approaching the critical velocity. Although the recent experiment\n[Science, 372, 1318 (2021)] has realized the geometrically squeezed state of\nthe guiding-center mode, the remaining degree of freedom, the cyclotron mode,\nremains unsqueezed due to the large energy gap of Landau levels. To overcome\nthis limitation, in this paper, we propose a Floquet-based state-preparation\nprotocol by periodically driving an anisotropic potential. This protocol not\nonly facilitates the single cyclotron-mode squeezing, but also enables a\ntwo-mode squeezing. Such two-mode squeezing offers a richer set of dynamics\ncompared to single-mode squeezing and can achieve wavepacket width well below\nthe lowest Landau level limit. Our work provides a highly controllable knob for\nrealizing diverse geometrically squeezed states in ultracold quantum gases\nwithin the quantum Hall regime.",
        "Let $G$ be a locally compact group, $\\mu$ its Haar measure, $\\hat G$ its\nPontryagin dual and $\\nu$ the dual measure. For any $A_\\theta\\in L^1(G;\\mathcal\nC_p)\\cap L^2(G;\\mathcal C_p)$, ($\\mathcal C_p$ is Schatten ideal), and\n$1<p\\le2$ we prove $$\\int_{\\hat\nG}\\left\\|\\int_GA_\\theta\\overline{\\xi(\\theta)}\\,\\mathrm\nd\\mu(\\theta)\\right\\|_p^q\\,\\mathrm d\\nu(\\xi)\\le\n  \\left(\\int_G\\|A_\\theta\\|_p^p\\,\\mathrm d\\mu(\\theta)\\right)^{q\/p}, $$ where\n$q=p\/(p-1)$. This appears to be a generalization of some earlier obtained\ninequalities, including Clarkson-McCarthy inequalities (in the case $G=\\mathbf\nZ_2$), and Hausdorff-Young inequality. Some corollaries are also given.",
        "The Belin\/Ambr\\'osio Deviation (BAD) model is a widely used diagnostic tool\nfor detecting keratoconus and corneal ectasia. The input to the model is a set\nof z-score normalized $D$ indices that represent physical characteristics of\nthe cornea. Paradoxically, the output of the model, Total Deviation Value\n($D_{\\text{final}}$), is reported in standard deviations from the mean, but\n$D_{\\text{final}}$ does not behave like a z-score normalized value. Although\nthresholds like $D_{\\text{final}} \\ge 1.6$ for \"suspicious\" and\n$D_{\\text{final}} \\ge 3.0$ for \"abnormal\" are commonly cited, there is little\nexplanation on how to interpret values outside of those thresholds or to\nunderstand how they relate to physical characteristics of the cornea. This\nstudy explores the reasons for $D_{\\text{final}}$'s apparent inconsistency\nthrough a meta-analysis of published data and a more detailed statistical\nanalysis of over 1,600 Pentacam exams. The results reveal that systematic bias\nin the BAD regression model, multicollinearity among predictors, and\ninconsistencies in normative datasets contribute to the non-zero mean of\n$D_{\\text{final}}$, complicating its clinical interpretation. These findings\nhighlight critical limitations in the model's design and underscore the need\nfor recalibration to enhance its transparency and diagnostic reliability.",
        "The radiative transfer equation models various physical processes ranging\nfrom plasma simulations to radiation therapy. In practice, these phenomena are\noften subject to uncertainties. Modeling and propagating these uncertainties\nrequires accurate and efficient solvers for the radiative transfer equations.\nDue to the equation's high-dimensional phase space, fine-grid solutions of the\nradiative transfer equation are computationally expensive and memory-intensive.\nIn recent years, dynamical low-rank approximation has become a popular method\nfor solving kinetic equations due to the development of computationally\ninexpensive, memory-efficient and robust algorithms like the augmented basis\nupdate \\& Galerkin integrator. In this work, we propose a low-rank Monte Carlo\nestimator and combine it with a control variate strategy based on\nmulti-fidelity low-rank approximations for variance reduction. We investigate\nthe error analytically and numerically and find that a joint approach to\nbalance rank and grid size is necessary. Numerical experiments further show\nthat the efficiency of estimators can be improved using dynamical low-rank\napproximation, especially in the context of control variates.",
        "We consider the Schr\\\"odinger operator on the quantum graph whose edges\nconnect the points of ${\\Bbb Z}$. The numbers of the edges connecting two\nconsecutive points $n$ and $n+1$ are read along the orbits of a shift of finite\ntype. We prove that the Lyapunov exponent is potitive for energies $E$ that do\nnot belong to a discrete subset of $[0,\\infty)$. The number of points $E$ of\nthis subset in $[(\\pi (j-1))^2, (\\pi j)^2]$ is the same for all $j\\in {\\Bbb\nN}$.",
        "How the environment of the host galaxy affects the formation of multiple\npopulations (MPs) in globular clusters (GCs) is one of the outstanding\nquestions in the near-field cosmology. To understand the true nature of the old\nGC MPs in the Large Magellanic Cloud (LMC), we study the Ca--CN--CH photometry\nof the old metal-poor LMC GC NGC 2257. We find the predominantly FG-dominated\npopulational number ratio of $n$(FG):$n$(SG) = 61:39($\\pm$4), where the FG and\nSG denote the first and second generations. Both the FG and SG have similar\ncumulative radial distributions, consistent with the idea that NGC 2257 is\ndynamically old. We obtain [Fe\/H] = $-$1.78$\\pm$0.00 dex($\\sigma$=0.05 dex) and\nour metallicity is $\\sim$0.2 dex larger than that from the high-resolution\nspectroscopy by other, due to their significantly lower temperatures by $\\sim$\n$-$200 K. The NGC 2257 FG shows a somewhat larger metallicity variation than\nthe SG, the first detection of such phenomenon in an old LMC GC, similar to\nGalactic GCs with MPs, strongly suggesting that it is a general characteristic\nof GCs with MPs. Interestingly, the NGC 2257 SG does not show a helium\nenhancement compared to the FG. Our results for the Galactic normal GCs exhibit\nthat the degree of carbon and nitrogen variations are tightly correlated with\nthe GC mass, while NGC 2257 exhibits slightly smaller variations for its mass.\nWe show that old LMC GCs follow the same trends as the Galactic normal GCs in\nthe $\\Delta$W$_{\\rm CF336W,F438W,F814W}$, $N_{\\rm FG}\/N_{\\rm tot}$, and $\\log\nM\/M_{\\rm \\odot}$ domains. Our result indicates that the environment of the host\ngalaxy did not play a major role in the formation and evolution of GC MPs.",
        "Identifying quantum spin liquids, magnon breakdown, or fractionalized\nexcitations in quantum magnets is an ongoing challenge due to the ambiguity of\npossible origins of excitation continua occurring in linear response probes.\nRecently, it was proposed that techniques measuring higher-order response, such\nas two-dimensional coherent spectroscopy (2DCS), could resolve such\nambiguities. Numerically simulating nonlinear response functions can, however,\nbe computationally very demanding. We present an efficient Lanczos-based method\nto compute second-order susceptibilities $\\chi^{2}\\omega_t,\\omega_\\tau)$\ndirectly in the frequency domain. Applying this to extended Kitaev models\ndescribing $\\alpha$-RuCl$_3$, we find qualitatively different nonlinear\nresponses between intermediate magnetic field strengths and the high-field\nregime. To put these results into context, we derive the general 2DCS response\nof partially-polarized magnets within the linear spin-wave approximation,\nestablishing that $\\chi^2(\\omega_t,\\omega_\\tau)$ is restricted to a distinct\nuniversal form if the excitations are conventional magnons. Deviations from\nthis form, as predicted in our (Lanczos-based) simulations for\n$\\alpha$-RuCl$_3$, can hence serve in 2DCS experiments as direct criteria to\ndetermine whether an observed excitation continuum is of conventional\ntwo-magnon type or of different nature.",
        "Significant progress has been made in the field of thermophotovoltaics, with\nefficiency recently rising to over 40% due to improvements in cell design and\nmaterial quality, higher emitter temperatures, and better spectral management.\nHowever, inconsistencies in trends for efficiency with semiconductor bandgap\nenergy across various temperatures pose challenges in predicting optimal\nbandgaps or expected performance for different applications. To address these\nissues, here we present realistic performance predictions for various types of\nsingle-junction cells over a broad range of emitter temperatures using an\nempirical model based on past cell measurements. Our model is validated using\ndata from different authors with various bandgaps and emitter temperatures, and\nan excellent agreement is seen between the model and the experimental data.\nUsing our model, we show that in addition to spectral losses, it is important\nto consider practical electrical losses associated with series resistance and\ncell quality to avoid overestimation of system efficiency. We also show the\neffect of modifying various system parameters such as bandgap, above and\nbelow-bandgap reflectance, saturation current, and series resistance on the\nefficiency and power density of thermophotovoltaics at different temperatures.\nFinally, we predict the bandgap energies for best performance over a range of\nemitter temperatures for different cell material qualities.",
        "Quantum information processing, especially with quantum error correction,\nrequires both long-lived qubits and fast, quantum non-demolition readout. In\nsuperconducting circuits this leads to the requirement to both strongly couple\nqubits, such as transmons, to readout modes while also protecting them from\nassociated Purcell decay through the readout port. So-called Purcell filters\ncan provide this protection, at the cost of significant increases in circuit\ncomponents and complexity. However, as we demonstrate in this work, visualizing\nthe qubit fields in space reveals locations where the qubit fields are strong\nand cavity fields weak; simply placing ports at these locations provides\nintrinsic Purcell protection. For a $\\lambda\/2$ readout mode in the\n`chip-in-tube' geometry, we show both millisecond level Purcell protection and,\nconversely, greatly enhanced Purcell decay (qubit lifetime of 1~$\\mu$s) simply\nby relocating the readout port. This method of integrating the Purcell\nprotection into the qubit-cavity geometry can be generalized to other 3D\nimplementations, such as post-cavities, as well as planar geometries. For qubit\nfrequencies below the readout mode this effect is quite distinct from the\nmulti-mode Purcell effect, which we demonstrate in a 3D-post geometry where we\nshow both Purcell protection of the qubit while spoiling the quality factor of\nhigher cavity harmonics to protect against dephasing due to stray photons in\nthese modes.",
        "We have investigated the normal and superconducting states of the\ntechnologically important compound Nb$_3$Sn using $^{93}$Nb nuclear magnetic\nresonance. From spin-lattice relaxation we find strong suppression of the\nzero-temperature superconducting order parameter by magnetic field. We have\nidentified an anomalously large electron-nuclear exchange interaction from\nspin-spin relaxation measurements, an order of magnitude beyond that of the\ndipole-dipole interaction, and thereby sensitive to vortex dynamics and vortex\npinning.",
        "Light nuclei production in heavy-ion collisions serves as a sensitive probe\nof the QCD phase structure. In coalescence models, triton ($N_t$) and deuteron\n($N_d$) yields depend on the spatial separation of nucleon pairs ($\\Delta r$)\nin Wigner functions, yet the impact of initial two-nucleon correlations\n$\\rho(\\Delta r)$ remains underexplored. We develop a method to sample nucleons\nin $^{197}$Au nuclei that simultaneously satisfies both the single-particle\ndistribution $f(r)$ and the two-nucleon correlation $\\rho(\\Delta r)$. Using\nthese nuclei, we simulate Au+Au collisions at $\\sqrt{s_\\mathrm{NN}}=3$ GeV via\nthe SMASH transport model (mean-field mode) to calculate proton, deuteron, and\ntriton yields. Simulations reveal a 36% enhancement in mid-rapidity deuteron\nyields across all centrality ranges and a 33% rise in mid-rapidity triton\nproduction for 0-10% central collisions. Calculated transverse momentum of\nlight nuclei aligns with STAR data. We further analyze impacts of baryon\nconservation, spectator exclusion, and centrality determination via charged\nmultiplicity. Notably, observed discrepancies in the double yield ratio suggest\nunaccounted physical mechanisms, such as critical fluctuations or inaccuracies\nin coalescence parameters or light nuclei cross-sections. This underscores the\ncritical role of initial nucleon-nucleon correlations, linking microscopic\nnuclear structure to intermediate-energy collision dynamics.",
        "Dispersal is often used by living beings to gather information from\nconspecifics, integrating it with personal experience to guide decision-making.\nThis mechanism has only recently been studied experimentally, facilitated by\nadvancements in tracking animal groups over extended periods. Such studies\nenable the analysis of the adaptive dynamics underlying sequential decisions\nand collective choices. Here, we present a theoretical framework based on the\nVoter Model to investigate these processes. The model, originally designed to\nstudy opinion or behavioral consensus within groups through imitation, is\nadapted to include the prospection of others' decisions as a mechanism for\nupdating personal criteria. We demonstrate that several properties of our model\n(such as average consensus times and polarization dynamic) can be analytically\nmapped onto those of the classical Voter Model under simplifying assumptions.\nFinally, we discuss the potential of this framework for studying more complex\nscenarios.",
        "Understanding the generation and development of the continuous outflow from\nthe Sun requires tracing the physical conditions from deep in the corona to the\nheliosphere. Detailed global observations of plasma state variables and the\nmagnetic field are needed to provide critical constraints to the underlying\nphysics driving models of the corona and solar wind. Key diagnostics of the\nsolar wind require measurements at its formation site and during its outflow to\ncontinuously track it across rapidly changing regions of space. A unified view\nof the solar wind is only possible through coordinated remote and in situ\nobservations that probe these different regions. Here, we discuss current\nobservational coverage and gaps of different plasma properties and review\nrecent coordinated studies. We highlight how these efforts may become more\nroutine with the launch of upcoming and planned missions.",
        "Spin qubits based on valence band hole states are highly promising for\nquantum information processing due to their strong spin-orbit coupling and\nultrafast operation speed. As these systems scale up, achieving high-fidelity\nsingle-qubit operations becomes essential. However, mitigating crosstalk\neffects from neighboring qubits in larger arrays, particularly for anisotropic\nqubits with strong spin-orbit coupling, presents a significant challenge. We\ninvestigate the impact of crosstalk on qubit fidelities during single-qubit\noperations and derive an analytical equation that serves as a synchronization\ncondition to eliminate crosstalk in anisotropic media. Our analysis proposes\noptimized driving field conditions that can robustly synchronize Rabi\noscillations and minimize crosstalk, showing a strong dependence on qubit\nanisotropy and the orientation of the external magnetic field. Taking\nexperimental data into our analysis, we identify a set of parameter values that\nenable nearly crosstalk-free single-qubit gates, thereby paving the way for\nscalable quantum computing architectures.",
        "Numerous machine learning and industrial problems can be modeled as the\nminimization of a sum of $N$ so-called clipped convex functions (SCC), i.e.\neach term of the sum stems as the pointwise minimum between a constant and a\nconvex function. In this work, we extend this framework to capture more\nproblems of interest. Specifically, we allow each term of the sum to be a\npointwise minimum of an arbitrary number of convex functions, called\ncomponents, turning the objective into a sum of pointwise minima of convex\nfunctions (SMC).\n  Problem (SCC) is NP-hard, highlighting an appeal for scalable local\nheuristics. In this spirit, one can express (SMC) objectives as the difference\nbetween two convex functions to leverage the possibility to apply (DC)\nalgorithms to compute critical points of the problem. Our approach relies on a\nbi-convex reformulation of the problem. From there, we derive a family of local\nmethods, dubbed as relaxed alternating minimization (r-AM) methods, that\ninclude classical alternating minimization (AM) as a special case. We prove\nthat every accumulation point of r-AM is critical. In addition, we show the\nempirical superiority of r-AM, compared to traditional AM and (DC) approaches,\non piecewise-linear regression and restricted facility location problems.\n  Under mild assumptions, (SCC) can be cast as a mixed-integer convex program\n(MICP) using perspective functions. This approach can be generalized to (SMC)\nbut introduces many copies of the primal variable. In contrast, we suggest a\ncompact big-M based (MICP) equivalent formulation of (SMC), free of these extra\nvariables. Finally, we showcase practical examples where solving our (MICP),\nrestricted to a neighbourhood of a given candidate (i.e. output iterate of a\nlocal method), will either certify the candidate's optimality on that\nneighbourhood or providing a new point, strictly better, to restart the local\nmethod.",
        "This study presents a framework for high-resolution mortality simulations\ntailored to insured and general populations. Due to the scarcity of detailed\ndemographic-specific mortality data, we leverage Iterative Proportional Fitting\n(IPF) and Monte Carlo simulations to generate refined mortality tables that\nincorporate age, gender, smoker status, and regional distributions. This\nmethodology enhances public health planning and actuarial analysis by providing\nenriched datasets for improved life expectancy projections and insurance\nproduct development.",
        "This study explores the transformative potential of nanocatalysts,\nemphasizing their pivotal role in catalysis and material science. Key synthesis\ntechniques, including chemical reduction and hybrid methods, are highlighted\nfor their ability to control particle size and enhance stability. Applications\nin environmental remediation, fuel quality improvement, and renewable energy\nshowcase the broad impact of nanocatalysts. Despite challenges in scalability\nand stabilization, advancements in bimetallic configurations and electro-steric\napproaches demonstrate significant progress. This research underscores\nnanocatalysts' promise for sustainable industrial processes and global\nchallenges.",
        "There has been recent interest in the cosmological consequences of\nenergy-momentum-powered gravity models, in which the matter side of Einstein's\nequations includes a term proportional to some power, $n$, of the\nenergy-momentum tensor, in addition to the canonical linear term. Previous\nworks have suggested that these models can lead to a recent accelerating\nuniverse without a cosmological constant, but they can also be seen as\nphenomenological extensions of the standard $\\Lambda$CDM, which are\nobservationally constrained to be close to the $\\Lambda$CDM limit. Here we show\nthat these models violate the temperature-redshift relation, and are therefore\nfurther constrained by astrophysical measurements of the cosmic microwave\nbackground temperature. We provide joint constraints on these models from the\ncombination of astrophysical and background cosmological data, showing that\nthis power is constrained to be about $|n|<0.01$ and $|n|<0.1$, respectively in\nmodels without and with a cosmological constant, and improving previous\nconstraints on this parameter by more than a factor of three. By breaking\ndegeneracies between this parameter and the matter density, constraints on the\nlatter are also improved by a factor of about two.",
        "Feature importance measures are widely studied and are essential for\nunderstanding model behavior, guiding feature selection, and enhancing\ninterpretability. However, many machine learning fitted models involve complex,\nhigher-order interactions between features. Existing feature importance metrics\nfail to capture these higher-order effects while existing interaction metrics\noften suffer from limited applicability or excessive computation; no methods\nexist to conduct statistical inference for feature interactions. To bridge this\ngap, we first propose a new model-agnostic metric, interaction\nLeave-One-Covariate-Out iLOCO, for measuring the importance of higher-order\nfeature interactions. Next, we leverage recent advances in LOCO inference to\ndevelop distribution-free and assumption-light confidence intervals for our\niLOCO metric. To address computational challenges, we also introduce an\nensemble learning method for calculating the iLOCO metric and confidence\nintervals that we show is both computationally and statistically efficient. We\nvalidate our iLOCO metric and our confidence intervals on both synthetic and\nreal data sets, showing that our approach outperforms existing methods and\nprovides the first inferential approach to detecting feature interactions.",
        "This work proves that the process of gluing finite lattices to form a larger\nlattice is bijective, that is each lattice is the glued sum of a unique system\nof finite lattices, provided the class of lattices is constrained to modular,\nlocally-finite lattices with finite covers. The results of this work are not\nsurprising given the prior literature, but this seems to be the first proof\nthat the processes of gluing and dissection can be made inverses, and hence\nthat gluing is bijective.",
        "An old open question in non-K\\\"ahler geometry predicts that any compact\nHermitian manifold with constant holomorphic sectional curvature must be\nK\\\"ahler or Chern flat. The conjecture is known to be true in dimension $2$ due\nto the work by Balas-Gauduchon and Apostolov-Davidov-Muskarov in the 1980s and\n1990s, but is still open in dimensions $3$ or higher, except in several special\ncases. The difficulty in this quest for `Hermitian space forms' is largely due\nto the algebraic complicity or lack of symmetry for the curvature tensor of a\ngeneral Hermitian metric. In this article, we confirm the conjecture for all\nsolvmanifolds with complex commutator, extending earlier result on nilmanifolds\nby Li and the second named author.",
        "In this study, we investigate a novel hybrid borocarbonitrides (bpn-BCN) 2D\nmaterial inspired by recent advances in carbon biphenylene synthesis, using\nfirst-principles calculations and semi-classical Boltzmann transport theory.\nOur analysis confirms the structural stability of bpn-BCN through formation\nenergy, elastic coefficients, phonon dispersion, and molecular dynamics\nsimulations at 300 K and 800 K. The material exhibits an indirect band gap of\n0.19 eV (PBE) between the X and Y points and a direct band gap of 0.58 eV (HSE)\nat the X point. Thermoelectric properties reveal a high Seebeck coefficient,\npeaking at for n-type carriers at 200K along the x-axis, while n-type has a\nmaximum of The electrical conductivity is for hole carriers, surpassing that of\nconventional 2D materials. The consequences of the high Seebeck coefficient and\nconductivity reflect a high-power factor with a peak value of at 1000K for\np-type carriers along the y-axis whereas, for n-type. Moreover, the highest\nobserved values were 0.78 (0.72) along the x (y) direction at 750 K for p-type\nand 0.57 (0.53) at 750 K along the x (y) axis for n-type. Our findings suggest\nthat the bpn-BCN 2D network holds significant potential for thermoelectric\napplications due to its exceptional performance.",
        "Hollow cathodes are becoming the bottleneck of many electric propulsion\nsystems, because of the sputtering and erosion on both cathodes and thrusters\nfrom the generation of anomalously energetic ions. So far, it is believed that\nenergetic ions are formed by waves and instabilities always accompanied in\ncathode discharge, but there is no evidence yet that those proposed\ninstabilities can lead to such high ion energies measured in experiments. In\nthis work, a new mechanism of charge separation instability in hollow cathode\nplume is found via fully kinetic PIC simulations, which can easily produce\nenergetic ions to the same level as measured in experiments.",
        "An index tracker is a passive investment reproducing the return and risk of a\nmarket index, an enhanced index tracker offers a return greater than the index.\nWe consider the selection of a portfolio of given cardinality to track an\nindex, both without and with enhancement. We divide the problem into two steps\n- (1) pre-selection of assets; (2) estimation of weights on the assets chosen.\nThe eight pre-selection procedures considered use: forward selection (FS) or\nbackward elimination (BE); implemented using ordinary least squares (OLS) or\nleast absolute deviation (LAD) regression; with a regression constant (c) or\nwithout (n). The two-step approach avoids the NP-hard problem arising when\nasset selection and asset weight computation are combined, leading to the\nselection of a cardinality constrained index tracking portfolio by computer\nintensive heuristic procedures with many examples in the literature solving for\nportfolios of 10 or fewer assets. Avoiding these restrictions, we show that\nout-of-sample tracking errors are roughly proportional to 1\/sqrt(cardinality).\n  We find OLS more effective than LAD; BE marginally more effective than FS;\n(n) marginally more effective than (c). For index tracking, both without and\nwith enhancement, we use BE-OLS(n) in sensitivity analyses on the periods used\nfor selection and evaluation. For a S&P 500 index tracker, we find that\nout-of-sample tracking error, transaction volume and return-risk ratios all\nimprove as cardinality increases. By contrast for enhanced returns,\ncardinalities of the order 10 to 20 are most effective. The S&P 500 data used\nfrom 3\/1\/2005 to 29\/12\/2023 is available to researchers.",
        "Let $H^n(\\mathbb R)$ denote the real hyperbolic space realized as the\nsymmetric space $Spin_0(1,n)\/Spin(n)$. In this paper, we provide a\ncharacterization for the image of the Poisson transform for $L^2$-sections of\nthe spinor bundle over the boundary ${\\partial H}^n(\\mathbb R)$. As a\nconsequence, we obtain an $L^2$ uniform estimate for the generalized spectral\nprojections associated to the spinor bundle over $H^n(\\mathbb R)$, thereby\nextending Strichartz's conjecture from the scalar case to the spinor setting."
      ]
    }
  },
  {
    "id":2411.17342,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques",
    "start_abstract":"Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b35"
      ],
      "title":[
        "Deep learning-based framework for automatic cranial defect reconstruction and implant modeling"
      ],
      "abstract":[
        "This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "RL + Transformer = A General-Purpose Problem Solver",
        "Matter creation, adiabaticity and phantom behavior",
        "Could AI Leapfrog the Web? Evidence from Teachers in Sierra Leone",
        "Optimizing Distributed Deployment of Mixture-of-Experts Model Inference\n  in Serverless Computing",
        "Tempo: Helping Data Scientists and Domain Experts Collaboratively\n  Specify Predictive Modeling Tasks",
        "Ideal MHD. Part II: Rigidity from infinity for ideal Alfv\\'en waves in\n  3D thin domains",
        "Ultrafast Proton Delivery with Pin Ridge Filters (pRFs): A Novel\n  Approach for Motion Management in Proton Therapy",
        "Upper limits on the gamma-ray emission from the microquasar V4641 Sgr",
        "First-principles Hubbard parameters with automated and reproducible\n  workflows",
        "The bright, dusty aftermath of giant eruptions & H-rich supernovae. Late\n  interaction of supernova shocks & dusty circumstellar shells",
        "Evolution and Pathogenicity of SARS-CoVs: A Microcanonical Analysis of\n  Receptor-Binding Motifs",
        "PVTree: Realistic and Controllable Palm Vein Generation for Recognition\n  Tasks",
        "Unsupervised CP-UNet Framework for Denoising DAS Data with Decay Noise",
        "Generative Learning of Densities on Manifolds",
        "scDD: Latent Codes Based scRNA-seq Dataset Distillation with Foundation\n  Model Knowledge",
        "Learning Cascade Ranking as One Network",
        "Ball Lightning as a profound manifestation of the Dark Matter physics",
        "Covariate Adjusted Response Adaptive Design with Delayed Outcomes",
        "Core-radius approximation of singular minimizers in nonlinear elasticity",
        "Synthesizing Grid Data with Cyber Resilience and Privacy Guarantees",
        "Tensor Product in the Category of Effect Algebras",
        "Performance-driven Constrained Optimal Auto-Tuner for MPC",
        "Developing a Network Discovery Protocol for the Constellation Control\n  and Data Acquisition Framework",
        "Prediction-Powered Inference with Imputed Covariates and Nonuniform\n  Sampling",
        "Computational Safety for Generative AI: A Signal Processing Perspective",
        "Uniqueness of Weak Solutions to One-Dimensional Doubly Degenerate\n  Cross-Diffusion System",
        "A Modal-Based Approach for System Frequency Response and Frequency Nadir\n  Prediction",
        "A Non-Relativistic Limit for Heterotic Supergravity and its Gauge\n  Lagrangian",
        "A Faster Algorithm for Constrained Correlation Clustering"
      ],
      "abstract":[
        "What if artificial intelligence could not only solve problems for which it\nwas trained but also learn to teach itself to solve new problems (i.e.,\nmeta-learn)? In this study, we demonstrate that a pre-trained transformer\nfine-tuned with reinforcement learning over multiple episodes develops the\nability to solve problems that it has never encountered before - an emergent\nability called In-Context Reinforcement Learning (ICRL). This powerful\nmeta-learner not only excels in solving unseen in-distribution environments\nwith remarkable sample efficiency, but also shows strong performance in\nout-of-distribution environments. In addition, we show that it exhibits\nrobustness to the quality of its training data, seamlessly stitches together\nbehaviors from its context, and adapts to non-stationary environments. These\nbehaviors demonstrate that an RL-trained transformer can iteratively improve\nupon its own solutions, making it an excellent general-purpose problem solver.",
        "We present a novel cosmological framework that unifies matter creation\ndynamics with thermodynamic principles. Starting with a single-component fluid\ncharacterized by a constant equation of state parameter, $\\omega$, we introduce\na generalized second law of thermodynamics by considering the entropy\nassociated with the cosmic horizon. Imposing an adiabatic expansion condition\nuniquely determines the particle creation rate, $\\Gamma$, a feature\nunprecedented in previous matter creation models. This mechanism yields a\ncosmology featuring phantom-like expansion while relying solely on a single\nconstituent, which can be either a quintessence-like fluid or a non-exotic,\nnon-relativistic dark matter component. Remarkably, this framework avoids the\nneed for exotic physics while providing a consistent explanation for the\naccelerated expansion of the universe. Our results open new pathways for\nunderstanding the interplay between horizon thermodynamics, particle creation,\nand cosmic evolution, offering fresh insights into the nature of dark energy\nand its potential thermodynamic origins.",
        "Although 85% of sub-Saharan Africa's population is covered by mobile\nbroadband signal, only 37% use the internet, and those who do seldom use the\nweb. The most frequently cited reason for low internet usage is the cost of\ndata. We investigate whether AI can bridge this gap by analyzing 40,350 queries\nsubmitted to an AI chatbot by 469 teachers in Sierra Leone over 17 months.\nTeachers use AI for teaching assistance more frequently than web search. We\ncompare the AI responses to the corresponding top search results for the same\nqueries from the most popular local web search engine, google.com.sl. Only 2%\nof results for corresponding web searches contain content from in country.\nAdditionally, the average web search result consumes 3,107 times more data than\nan AI response. Bandwidth alone costs \\$2.41 per thousand web search results\nloaded, while the total cost of AI is \\$0.30 per thousand responses. As a\nresult, AI is 87% less expensive than web search. In blinded evaluations, an\nindependent sample of teachers rate AI responses as more relevant, helpful, and\ncorrect than web search results. These findings suggest that AI-driven\nsolutions can cost-effectively bridge information gaps in low-connectivity\nregions.",
        "With the advancement of serverless computing, running machine learning (ML)\ninference services over a serverless platform has been advocated, given its\nlabor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models\nhave been a dominant type of model architectures to enable large models\nnowadays, with parallel expert networks. Serving large MoE models on serverless\ncomputing is potentially beneficial, but has been underexplored due to\nsubstantial challenges in handling the skewed expert popularity and\nscatter-gather communication bottleneck in MoE model execution, for\ncost-efficient serverless MoE deployment and performance guarantee. We study\noptimized MoE model deployment and distributed inference serving on a\nserverless platform, that effectively predict expert selection, pipeline\ncommunication with model execution, and minimize the overall billed cost of\nserving MoE models. Especially, we propose a Bayesian optimization framework\nwith multi-dimensional epsilon-greedy search to learn expert selections and\noptimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian\ndecision-making method for predicting expert popularity; 2) flexibly pipelined\nscatter-gather communication; and 3) an optimal model deployment algorithm for\ndistributed MoE serving. Extensive experiments on AWS Lambda show that our\ndesigns reduce the billed cost of all MoE layers by at least 75.67% compared to\nCPU clusters while maintaining satisfactory inference throughput. As compared\nto LambdaML in serverless computing, our designs achieves 43.41% lower cost\nwith a throughput decrease of at most 18.76%.",
        "Temporal predictive models have the potential to improve decisions in health\ncare, public services, and other domains, yet they often fail to effectively\nsupport decision-makers. Prior literature shows that many misalignments between\nmodel behavior and decision-makers' expectations stem from issues of model\nspecification, namely how, when, and for whom predictions are made. However,\nmodel specifications for predictive tasks are highly technical and difficult\nfor non-data-scientist stakeholders to interpret and critique. To address this\nchallenge we developed Tempo, an interactive system that helps data scientists\nand domain experts collaboratively iterate on model specifications. Using\nTempo's simple yet precise temporal query language, data scientists can quickly\nprototype specifications with greater transparency about pre-processing\nchoices. Moreover, domain experts can assess performance within data subgroups\nto validate that models behave as expected. Through three case studies, we\ndemonstrate how Tempo helps multidisciplinary teams quickly prune infeasible\nspecifications and identify more promising directions to explore.",
        "This paper concerns the rigidity from infinity for Alfv\\'en waves governed by\nideal incompressible magnetohydrodynamic equations subjected to strong\nbackground magnetic fields along the $x_1$-axis in 3D thin domains\n$\\Omega_\\delta=\\mathbb{R}^2\\times(-\\delta,\\delta)$ with $\\delta\\in(0,1]$ and\nslip boundary conditions. We show that in any thin domain $\\Omega_\\delta$,\nAlfv\\'en waves must vanish identically if their scattering fields vanish at\ninfinities. As an application, the rigidity of Alfv\\'en waves in\n$\\Omega_{\\delta}$, propagating along the horizontal direction, can be\napproximated by the rigidity of Alfv\\'en waves in $\\mathbb{R}^2$ when $\\delta$\nis sufficiently small. Our proof relies on the uniform (with respect to\n$\\delta$) weighted energy estimates with a position parameter in weights to\ntrack the center of Alfv\\'en waves. The key issues in the analysis include\ndealing with the nonlinear nature of Alfv\\'en waves and the geometry of thin\ndomains.",
        "Active breath-hold techniques effectively mitigate respiratory motion but\npose challenges for patients who are ineligible for the procedure. Conventional\ntreatment planning relies on multiple energy layers, extending delivery time\ndue to slow layer switching. We propose to use pin ridge filters (pRFs),\ninitially developed for FLASH radiotherapy, to construct a single energy beam\nplan and minimize dose delivery time. The conventional ITV--based\nfree--breathing treatment plan served as the reference. A GTV--based IMPT--DS\nplan with a downstream energy modulation strategy was developed based on a new\nbeam model that was commissioned using the maximum energy of the IMPT plan.\nConsequently, a nested iterative pencil beam direction (PBD) spot reduction\nprocess removed low--weighted spots along each PBD, generating pRFs with\ncoarser resolution. Finally, the IMPT--DS plan was then converted into an\nIMPT--pRF plan, using a monoenergetic beam with optimized spot positions and\nweights. This approach was validated on lung and liver SBRT cases (10 Gy RBE x\n5). For the lung case, the mean lung--GTV dose decreased from 10.3 Gy to 6.9\nGy, with delivery time reduced from 188.79 to 36.16 seconds. The largest time\nreduction was at 150{\\deg}, from 47.4 to 3.99 seconds. For the liver case, the\nmean liver--GTV dose decreased from 5.7 Gy to 3.8 Gy, with delivery time\nreduced from 111.13 to 30.54 seconds. The largest time reduction was at\n180{\\deg}, from 38.57 to 3.94 seconds. This method significantly reduces dose\ndelivery time and organ at risk dose. Further analysis is needed to validate\nits clinical feasibility.",
        "Following a recent detection of TeV radiation by the Large High Altitude Air\nShower Observatory (LHAASO) and the High-Altitude Water Cherenkov Observatory\n(HAWC), coincident with the direction of the microquasar V4641 Sgr, we search\nfor possible GeV emission from this source. We explored the morphology and\ntemporal features of the source as well as two nearby unassociated point\nsources which could be a part of extended structure of V4641 Sgr, and compared\nresults with corresponding X-ray and TeV emissions. The 95% confidence level\nupper limits for the flux from the source, assuming both point and extended\nsource models were 5.38$\\times$ 10$^{-13}$ erg cm$^{-2}$ s$^{-1}$ and\n1.12$\\times$ 10$^{-12}$ erg cm$^{-2}$ s$^{-1}$, respectively. Additionally, no\ncorrelation between gamma-ray light curve and X-ray outbursts was observed.",
        "We introduce an automated, flexible framework (aiida-hubbard) to\nself-consistently calculate Hubbard $U$ and $V$ parameters from\nfirst-principles. By leveraging density-functional perturbation theory, the\ncomputation of the Hubbard parameters is efficiently parallelized using\nmultiple concurrent and inexpensive primitive cell calculations. Furthermore,\nthe intersite $V$ parameters are defined on-the-fly during the iterative\nprocedure to account for atomic relaxations and diverse coordination\nenvironments. We demonstrate the scalability and reliability of the framework\nby computing in high-throughput fashion the self-consistent onsite $U$ and\nintersite $V$ parameters for 115 Li-containing bulk solids. Our analysis of the\nHubbard parameters calculated reveals a significant correlation of the onsite\n$U$ values on the oxidation state and coordination environment of the atom on\nwhich the Hubbard manifold is centered, while intersite $V$ values exhibit a\ngeneral decay with increasing interatomic distance. We find, e.g., that the\nnumerical values of $U$ for Fe and Mn 3d orbitals can vary up to 3 eV and 6 eV,\nrespectively; their distribution is characterized by typical shifts of about\n0.5 eV and 1.0 eV upon change in oxidation state, or local coordination\nenvironment. For the intersite $V$ a narrower spread is found, with values\nranging between 0.2 eV and 1.6 eV when considering transition metal and oxygen\ninteractions. This framework paves the way for the exploration of redox\nmaterials chemistry and high-throughput screening of $d$ and $f$ compounds\nacross diverse research areas, including the discovery and design of novel\nenergy storage materials, as well as other technologically-relevant\napplications.",
        "The late-stage evolution of massive stars is marked by intense instability as\nthey approach core-collapse. During these phases, giant stellar eruptions lead\nto exceptionally high mass-loss rates, forming significant amounts of dust.\nHowever, the survival of these dust grains is challenged by the powerful shock\nwaves generated when the progenitor explodes as a supernova (SN). We explore\nthe impact of hydrogen-rich SN explosions from 45, 50, and 60 M$_\\odot$\nprogenitors on dust formed after these eruptions, focusing on interactions with\ncircumstellar shells occurring from a few years to centuries after the event.\nUsing 3D hydrodynamical simulations, we track the evolution of dust particles\nin a scenario that includes the progenitor's stellar wind, a giant eruption,\nand the subsequent SN explosion, following the mass budgets predicted by\nstellar evolution models. For a standard SN ejecta mass of 10 M$_\\odot$ and\nkinetic energy of $10^{51}$ erg, only 25% of the dust mass survives 250 years\npost-explosion in a spherical circumstellar medium (CSM), while merely 2%\nremains a century after the explosion in a bipolar CSM. If the SN follows the\neruption within a dozen years, 75% of the dust survives for a standard\nexplosion, dropping to 20% for more massive ejecta (15-20 M$_\\odot$) with\nkinetic energy of $5 \\times 10^{51}$ erg. The geometry of the CSM and the early\ntransition of the SN remnant into a radiative phase significantly influence\ndust survival. As the shock wave weakens and efficiently converts kinetic\nenergy into thermal radiation (up to half of the injected kinetic energy) the\nlikelihood of dust survival increases, affecting not only pre-existing dust in\nthe CSM but also SN-condensed dust and ambient interstellar dust. Contrary to\nexpectations, a larger fraction of the dust mass can survive if the SN occurs\nonly a few years after the eruption.",
        "The rapid evolution and global impact of coronaviruses, notably SARS-CoV-1\nand SARS-CoV-2, underscore the importance of understanding their molecular\nmechanisms in detail. This study focuses on the receptor-binding motif (RBM)\nwithin the Spike protein of these viruses, a critical element for viral entry\nthrough interaction with the ACE2 receptor. We investigate the sequence\nvariations in the RBM across SARS-CoV-1, SARS-CoV-2 and its early variants of\nconcern (VOCs). Utilizing multicanonical simulations and microcanonical\nanalysis, we examine how these variations influence the folding dynamics,\nthermostability, and solubility of the RBMs. Our methodology includes\ncalculating the density of states (DoS) to identify structural phase\ntransitions and assess thermodynamic properties. Furthermore, we solve the\nPoisson-Boltzmann equation to model the solubility of the RBMs in aqueous\nenvironments. This methodology is expected to elucidate structural and\nfunctional differences in viral evolution and pathogenicity, likely improving\ntargeted treatments and vaccines.",
        "Palm vein recognition is an emerging biometric technology that offers\nenhanced security and privacy. However, acquiring sufficient palm vein data for\ntraining deep learning-based recognition models is challenging due to the high\ncosts of data collection and privacy protection constraints. This has led to a\ngrowing interest in generating pseudo-palm vein data using generative models.\nExisting methods, however, often produce unrealistic palm vein patterns or\nstruggle with controlling identity and style attributes. To address these\nissues, we propose a novel palm vein generation framework named PVTree. First,\nthe palm vein identity is defined by a complex and authentic 3D palm vascular\ntree, created using an improved Constrained Constructive Optimization (CCO)\nalgorithm. Second, palm vein patterns of the same identity are generated by\nprojecting the same 3D vascular tree into 2D images from different views and\nconverting them into realistic images using a generative model. As a result,\nPVTree satisfies the need for both identity consistency and intra-class\ndiversity. Extensive experiments conducted on several publicly available\ndatasets demonstrate that our proposed palm vein generation method surpasses\nexisting methods and achieves a higher TAR@FAR=1e-4 under the 1:1 Open-set\nprotocol. To the best of our knowledge, this is the first time that the\nperformance of a recognition model trained on synthetic palm vein data exceeds\nthat of the recognition model trained on real data, which indicates that palm\nvein image generation research has a promising future.",
        "Distributed acoustic sensor (DAS) technology leverages optical fiber cables\nto detect acoustic signals, providing cost-effective and dense monitoring\ncapabilities. It offers several advantages including resistance to extreme\nconditions, immunity to electromagnetic interference, and accurate detection.\nHowever, DAS typically exhibits a lower signal-to-noise ratio (S\/N) compared to\ngeophones and is susceptible to various noise types, such as random noise,\nerratic noise, level noise, and long-period noise. This reduced S\/N can\nnegatively impact data analyses containing inversion and interpretation. While\nartificial intelligence has demonstrated excellent denoising capabilities, most\nexisting methods rely on supervised learning with labeled data, which imposes\nstringent requirements on the quality of the labels. To address this issue, we\ndevelop a label-free unsupervised learning (UL) network model based on\nContext-Pyramid-UNet (CP-UNet) to suppress erratic and random noises in DAS\ndata. The CP-UNet utilizes the Context Pyramid Module in the encoding and\ndecoding process to extract features and reconstruct the DAS data. To enhance\nthe connectivity between shallow and deep features, we add a Connected Module\n(CM) to both encoding and decoding section. Layer Normalization (LN) is\nutilized to replace the commonly employed Batch Normalization (BN),\naccelerating the convergence of the model and preventing gradient explosion\nduring training. Huber-loss is adopted as our loss function whose parameters\nare experimentally determined. We apply the network to both the 2-D synthetic\nand filed data. Comparing to traditional denoising methods and the latest UL\nframework, our proposed method demonstrates superior noise reduction\nperformance.",
        "A generative modeling framework is proposed that combines diffusion models\nand manifold learning to efficiently sample data densities on manifolds. The\napproach utilizes Diffusion Maps to uncover possible low-dimensional underlying\n(latent) spaces in the high-dimensional data (ambient) space. Two approaches\nfor sampling from the latent data density are described. The first is a\nscore-based diffusion model, which is trained to map a standard normal\ndistribution to the latent data distribution using a neural network. The second\none involves solving an It\\^o stochastic differential equation in the latent\nspace. Additional realizations of the data are generated by lifting the samples\nback to the ambient space using Double Diffusion Maps, a recently introduced\ntechnique typically employed in studying dynamical system reduction; here the\nfocus lies in sampling densities rather than system dynamics. The proposed\napproaches enable sampling high dimensional data densities restricted to\nlow-dimensional, a priori unknown manifolds. The efficacy of the proposed\nframework is demonstrated through a benchmark problem and a material with\nmultiscale structure.",
        "Single-cell RNA sequencing (scRNA-seq) technology has profiled hundreds of\nmillions of human cells across organs, diseases, development and perturbations\nto date. However, the high-dimensional sparsity, batch effect noise, category\nimbalance, and ever-increasing data scale of the original sequencing data pose\nsignificant challenges for multi-center knowledge transfer, data fusion, and\ncross-validation between scRNA-seq datasets. To address these barriers, (1) we\nfirst propose a latent codes-based scRNA-seq dataset distillation framework\nnamed scDD, which transfers and distills foundation model knowledge and\noriginal dataset information into a compact latent space and generates\nsynthetic scRNA-seq dataset by a generator to replace the original dataset.\nThen, (2) we propose a single-step conditional diffusion generator named SCDG,\nwhich perform single-step gradient back-propagation to help scDD optimize\ndistillation quality and avoid gradient decay caused by multi-step\nback-propagation. Meanwhile, SCDG ensures the scRNA-seq data characteristics\nand inter-class discriminability of the synthetic dataset through flexible\nconditional control and generation quality assurance. Finally, we propose a\ncomprehensive benchmark to evaluate the performance of scRNA-seq dataset\ndistillation in different data analysis tasks. It is validated that our\nproposed method can achieve 7.61% absolute and 15.70% relative improvement over\nprevious state-of-the-art methods on average task.",
        "Cascade Ranking is a prevalent architecture in large-scale top-k selection\nsystems like recommendation and advertising platforms. Traditional training\nmethods focus on single-stage optimization, neglecting interactions between\nstages. Recent advances such as RankFlow and FS-LTR have introduced\ninteraction-aware training paradigms but still struggle to 1) align training\nobjectives with the goal of the entire cascade ranking (i.e., end-to-end\nrecall) and 2) learn effective collaboration patterns for different stages. To\naddress these challenges, we propose LCRON, which introduces a novel surrogate\nloss function derived from the lower bound probability that ground truth items\nare selected by cascade ranking, ensuring alignment with the overall objective\nof the system. According to the properties of the derived bound, we further\ndesign an auxiliary loss for each stage to drive the reduction of this bound,\nleading to a more robust and effective top-k selection. LCRON enables\nend-to-end training of the entire cascade ranking system as a unified network.\nExperimental results demonstrate that LCRON achieves significant improvement\nover existing methods on public benchmarks and industrial applications,\naddressing key limitations in cascade ranking training and significantly\nenhancing system performance.",
        "Ball lighting (BL) has been observed for centuries. There are large number of\nbooks, review articles, and original scientific papers devoted to different\naspects of BL phenomenon. Yet, the basic features of this phenomenon have never\nbeen explained by known physics. The main problem is the source which could\npower the dynamics of the BL. We advocate an idea that the dark matter in form\nof the axion quark nuggets (AQN) made of standard model quarks and gluons\n(similar to the old idea of the Witten's strangelets) could internally generate\nthe required power. The corresponding macroscopically large object in form of\nthe AQN behaves as {\\it chameleon}: it does not interact with the surrounding\nmaterial in dilute environment and serves as perfect cold DM candidate.\nHowever, AQN becomes strongly interacting object in sufficiently dense\nenvironment. The AQN model was invented long ago without any relation to the BL\nphysics. It was invented with a single motivation to explain the observed\nsimilarity $\\Omega_{\\rm DM}\\sim \\Omega_{\\rm visible}$ between visible and DM\ncomponents. This relation represents a very generic feature of this framework,\nnot sensitive to any parameters of the construction. However, with the same set\nof parameters being fixed long ago this model is capable to address the key\nelements of the BL phenomenology, including the source of the energy powering\nthe BL events. In particular, we argue that the visible size of BL, its typical\nlife time, the frequency of appearance , etc are all consistent with suggested\nproposal when BL represents a profound manifestation of the DM physics\nrepresented by the AQN objects. We also formulate a unique possible test which\ncan refute or unambiguously substantiate this unorthodox proposal on nature of\nBL.",
        "Covariate-adjusted response adaptive (CARA) designs have gained widespread\nadoption for their clear benefits in enhancing experimental efficiency and\nparticipant welfare. These designs dynamically adjust treatment allocations\nduring interim analyses based on participant responses and covariates collected\nduring the experiment. However, delayed responses can significantly compromise\nthe effectiveness of CARA designs, as they hinder timely adjustments to\ntreatment assignments when certain participant outcomes are not immediately\nobserved. In this manuscript, we propose a fully forward-looking CARA design\nthat dynamically updates treatment assignments throughout the experiment as\nresponse delay mechanisms are progressively estimated. Our design strategy is\ninformed by novel semiparametric efficiency calculations that explicitly\naccount for outcome delays in a multi-stage adaptive experiment. Through both\ntheoretical investigations and simulation studies, we demonstrate that our\nproposed design offers a robust solution for handling delayed outcomes in CARA\ndesigns, yielding significant improvements in both statistical power and\nparticipant welfare.",
        "We study a variational model in nonlinear elasticity allowing for cavitation\nwhich penalizes both the volume and the perimeter of the cavities.\nSpecifically, we investigate the approximation (in the sense of\n{\\Gamma}-convergence) of the energy by means of functionals defined on\nperforated domains. Perforations are introduced at flaw points where\nsingularities are expected and, hence, the corresponding deformations do not\nexhibit cavitation. Notably, those points are not prescribed but rather\nselected by the variational principle. Our analysis is motivated by the\nnumerical simulation of cavitation and extends previous results on models which\nsolely accounted for elastic energy but neglected contributions related to the\nformation of cavities.",
        "Differential privacy (DP) provides a principled approach to synthesizing data\n(e.g., loads) from real-world power systems while limiting the exposure of\nsensitive information. However, adversaries may exploit synthetic data to\ncalibrate cyberattacks on the source grids. To control these risks, we propose\nnew DP algorithms for synthesizing data that provide the source grids with both\ncyber resilience and privacy guarantees. The algorithms incorporate both normal\noperation and attack optimization models to balance the fidelity of synthesized\ndata and cyber resilience. The resulting post-processing optimization is\nreformulated as a robust optimization problem, which is compatible with the\nexponential mechanism of DP to moderate its computational burden.",
        "We study a tensor product in the category of effect algebras and in the\ncategory of partially ordered Abelian groups with order unit. We show that the\ntensor product preserves all the constructions that are essentially colimits\nover a connected diagram. Further, we prove the construction of a universal\ngroup for an effect algebra preserves all tensor products. We establish the\ncorresponding functor from the category of effect algebras to the category of\nunital Abelian po-groups as a strong monoidal functor. We note that the\ntechnique we use in establishing the result could be used in various similar\nsituations. Finally, we show that the tensor product of effect algebras does\nnot preserve the Riesz decomposition property, which was an open question for a\nwhile.",
        "A key challenge in tuning Model Predictive Control (MPC) cost function\nparameters is to ensure that the system performance stays consistently above a\ncertain threshold. To address this challenge, we propose a novel method,\nCOAT-MPC, Constrained Optimal Auto-Tuner for MPC. With every tuning iteration,\nCOAT-MPC gathers performance data and learns by updating its posterior belief.\nIt explores the tuning parameters' domain towards optimistic parameters in a\ngoal-directed fashion, which is key to its sample efficiency. We theoretically\nanalyze COAT-MPC, showing that it satisfies performance constraints with\narbitrarily high probability at all times and provably converges to the optimum\nperformance within finite time. Through comprehensive simulations and\ncomparative analyses with a hardware platform, we demonstrate the effectiveness\nof COAT-MPC in comparison to classical Bayesian Optimization (BO) and other\nstate-of-the-art methods. When applied to autonomous racing, our approach\noutperforms baselines in terms of constraint violations and cumulative regret\nover time.",
        "Qualifying new detectors in test beam environments presents a challenging\nsetting that requires stable operation of diverse devices, often employing\nmultiple data acquisition systems. Changes to these setups are frequent, such\nas using different reference detectors depending on the facility. Managing this\ncomplexity necessitates a system capable of controlling the data taking,\nmonitoring the experimental setup, facilitating seamless configuration, and\neasy integration of new devices. One aspect of such systems is network\nconfiguration. Many systems require fixed IP addresses for all machines\nparticipating in the data acquisition, which adds complexity for users. In this\npaper, a network protocol for network discovery tailored towards\nnetwork-distributed control and data acquisition systems is described.",
        "Machine learning models are increasingly used to produce predictions that\nserve as input data in subsequent statistical analyses. For example, computer\nvision predictions of economic and environmental indicators based on satellite\nimagery are used in downstream regressions; similarly, language models are\nwidely used to approximate human ratings and opinions in social science\nresearch. However, failure to properly account for errors in the machine\nlearning predictions renders standard statistical procedures invalid. Prior\nwork uses what we call the Predict-Then-Debias estimator to give valid\nconfidence intervals when machine learning algorithms impute missing variables,\nassuming a small complete sample from the population of interest. We expand the\nscope by introducing bootstrap confidence intervals that apply when the\ncomplete data is a nonuniform (i.e., weighted, stratified, or clustered) sample\nand to settings where an arbitrary subset of features is imputed. Importantly,\nthe method can be applied to many settings without requiring additional\ncalculations. We prove that these confidence intervals are valid under no\nassumptions on the quality of the machine learning model and are no wider than\nthe intervals obtained by methods that do not use machine learning predictions.",
        "AI safety is a rapidly growing area of research that seeks to prevent the\nharm and misuse of frontier AI technology, particularly with respect to\ngenerative AI (GenAI) tools that are capable of creating realistic and\nhigh-quality content through text prompts. Examples of such tools include large\nlanguage models (LLMs) and text-to-image (T2I) diffusion models. As the\nperformance of various leading GenAI models approaches saturation due to\nsimilar training data sources and neural network architecture designs, the\ndevelopment of reliable safety guardrails has become a key differentiator for\nresponsibility and sustainability. This paper presents a formalization of the\nconcept of computational safety, which is a mathematical framework that enables\nthe quantitative assessment, formulation, and study of safety challenges in\nGenAI through the lens of signal processing theory and methods. In particular,\nwe explore two exemplary categories of computational safety challenges in GenAI\nthat can be formulated as hypothesis testing problems. For the safety of model\ninput, we show how sensitivity analysis and loss landscape analysis can be used\nto detect malicious prompts with jailbreak attempts. For the safety of model\noutput, we elucidate how statistical signal processing and adversarial learning\ncan be used to detect AI-generated content. Finally, we discuss key open\nresearch challenges, opportunities, and the essential role of signal processing\nin computational AI safety.",
        "The uniqueness of global weak solutions to one-dimensional doubly degenerate\ncross-diffusion system is shown. The equations model the evolution of feeding\nbacterial populations in a malnourished environment. The key idea of the proof\nis applying anti-derivative of the sum of weak solutions to the system.",
        "This letter introduces a novel approach for predicting system frequency\nresponse and frequency nadir by leveraging modal information. It significantly\ndifferentiates from traditional methods rooted in the average system frequency\nmodel. The proposed methodology targets system modes associated with the slower\ndynamics of the grid, enabling precise predictions through modal decomposition\napplied to the full system model. This decomposition facilitates an analytical\nsolution for the frequency at the center of inertia, resulting in highly\naccurate predictions of both frequency response and nadir. Numerical results\nfrom a 39-bus, 10-machine test system verify the method's effectiveness and\naccuracy. This methodology represents a shift from observing a simplified\naverage system frequency response to a more detailed analysis focusing on\nsystem dynamics.",
        "We show that the non-relativistic (NR) limit of $D=10$ heterotic supergravity\nhas a finite gauge Lagrangian due to non-trivial cancellations of divergent\nparts coming from the Chern-Simons terms in the curvature of the $\\hat B$-field\nand the Yang-Mills Lagrangian. This is similar to what happens in bosonic\nsupergravity between the Ricci scalar, $\\hat R$, and the $- \\frac{1}{12} \\hat\nH^2$ term after taking the same limit. In this work we present the explicit\nform of the gauge transformations and curvatures after considering the NR limit\nand we compute the finite gauge Lagrangian in its covariant form. As an\ninteresting property, the Green-Schwarz mechanism for the two-form can be\ntrivialized in this limit. Terms equivalent to Chern-Simons contributions\nnaturally arise from the previous property.",
        "In the Correlation Clustering problem we are given $n$ nodes, and a\npreference for each pair of nodes indicating whether we prefer the two\nendpoints to be in the same cluster or not. The output is a clustering inducing\nthe minimum number of violated preferences. In certain cases, however, the\npreference between some pairs may be too important to be violated. The\nconstrained version of this problem specifies pairs of nodes that must be in\nthe same cluster as well as pairs that must not be in the same cluster (hard\nconstraints). The output clustering has to satisfy all hard constraints while\nminimizing the number of violated preferences.\n  Constrained Correlation Clustering is APX-Hard and has been approximated\nwithin a factor 3 by van Zuylen et al. [SODA '07] using $\\Omega(n^{3\\omega})$\ntime. In this work, using a more combinatorial approach, we show how to\napproximate this problem significantly faster at the cost of a slightly weaker\napproximation factor. In particular, our algorithm runs in $\\widetilde{O}(n^3)$\ntime and approximates Constrained Correlation Clustering within a factor 16.\n  To achieve our result we need properties guaranteed by a particular\ninfluential algorithm for (unconstrained) Correlation Clustering, the CC-PIVOT\nalgorithm. This algorithm chooses a pivot node $u$, creates a cluster\ncontaining $u$ and all its preferred nodes, and recursively solves the rest of\nthe problem. As a byproduct of our work, we provide a derandomization of the\nCC-PIVOT algorithm that still achieves the 3-approximation; furthermore, we\nshow that there exist instances where no ordering of the pivots can give a\n$(3-\\varepsilon)$-approximation, for any constant $\\varepsilon$.\n  Finally, we introduce a node-weighted version of Correlation Clustering,\nwhich can be approximated within factor 3 using our insights on Constrained\nCorrelation Clustering."
      ]
    }
  },
  {
    "id":2411.17342,
    "research_type":"applied",
    "start_id":"b35",
    "start_title":"Deep learning-based framework for automatic cranial defect reconstruction and implant modeling",
    "start_abstract":"This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques"
      ],
      "abstract":[
        "Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives"
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Constraining the curvature-induced quantum gravity scales via gamma-ray\n  bursts",
        "Neural Chaos: A Spectral Stochastic Neural Operator",
        "Constructing balanced datasets for predicting failure modes in\n  structural systems under seismic hazards",
        "On rigid regular graphs and a problem of Babai and Pultr",
        "Groupoids, equivalence bibundles and bimodules for noncommutative\n  solenoids",
        "The Solo Revolution: A Theory of AI-Enabled Individual Entrepreneurship",
        "Moduli of curves and moduli of sheaves",
        "Time-dependent global sensitivity analysis of the Doyle-Fuller-Newman\n  model",
        "Calculating the Hawking Temperature of Black Holes in $f(Q)$ Gravity\n  Using the RVB Method: A Residue-Based Approach",
        "Human fields and their impact on brain waves A pilot study",
        "Highly efficient field-free switching by orbital Hall torque in a\n  MoS2-based device operating at room temperature",
        "Ergodic optimization for beta-transformations",
        "A non-homogeneous, non-stationary and path-dependent Markov anomalous\n  diffusion model",
        "Computational Complexity of Covering Colored Mixed Multigraphs with\n  Simple Degree Partitions",
        "Multi-scale Energy Release Events in the Quiet Sun: A Possible Source\n  for Coronal Heating",
        "Notes on a special order on $\\mathbb{Z}^\\infty$",
        "Wasserstein-based Kernels for Clustering: Application to Power\n  Distribution Graphs",
        "Non-stop Variability of Sgr A* using JWST at 2.1 and 4.8 micron\n  Wavelengths: Evidence for Distinct Populations of Faint and Bright Variable\n  Emission",
        "A tropical approach to rigidity: counting realisations of frameworks",
        "Spectral heat content for non-isotropic L\\'evy processes with weak lower\n  scaling condition",
        "Differentially Private Sequential Learning",
        "Local doping of an oxide semiconductor by voltage-driven splitting of\n  anti-Frenkel defects",
        "Revisiting Projection-Free Online Learning with Time-Varying Constraints",
        "Generic jet evaluation transversality of contact instantons against\n  contact distribution",
        "A 1.8 m class pathfinder Raman LIDAR for the Northern Site of the\n  Cherenkov Telescope Array Observatory -- Technical Design",
        "Benefits of Mutual Coupling in Dynamic Metasurface Antennas for\n  Optimizing Wireless Communications -- Theory and Experimental Validation",
        "On the nature of rubber wear",
        "Trotter error mitigation by error profiling with shallow quantum circuit",
        "Physics Informed Neural Networks for Learning the Horizon Size in\n  Bond-Based Peridynamic Models"
      ],
      "abstract":[
        "We constrain the parameters that govern curvature-induced quantum gravity\ntime-of-flight (TOF) effects. These TOF delays, which occur due to modified\ndispersion relations of particles in a vacuum, could be a phenomenological\nsignature of quantum gravity. Gamma-ray bursts (GRBs), short, high-energy\nevents from distant galaxies, offer a unique opportunity to impose\nobservational limits on TOF delays and, by extension, on the energy scales of\nquantum gravity. Using the standard Jacob-Piran relation, which assumes a\nlocally-flat spacetime, the analysis of quantum gravity-induced TOF effects\nestablishes a lower limit of approximately 10 Planck energies on the energy\nscale of these effects. However, curvature-induced quantum gravity effects may\nintroduce additional contributions. From current GRB observations, we find\nthat, at a 95% credibility level, in the symmetry-deformed scenario,\ncurvature-induced TOF effects may only arise at energies above 0.04 Planck\nenergy. If we consider only curvature-induced effects, this limit is an order\nof magnitude stronger. Observing more GRBs at different redshifts could improve\nthe constraints on the curvature-induced QG phenomena. However, given the\ncapabilities of current telescopes and the current understanding of GRBs, it is\nunlikely that these constraints will be significantly extended beyond the\npresent level.",
        "Building surrogate models with uncertainty quantification capabilities is\nessential for many engineering applications where randomness, such as\nvariability in material properties, is unavoidable. Polynomial Chaos Expansion\n(PCE) is widely recognized as a to-go method for constructing stochastic\nsolutions in both intrusive and non-intrusive ways. Its application becomes\nchallenging, however, with complex or high-dimensional processes, as achieving\naccuracy requires higher-order polynomials, which can increase computational\ndemands and or the risk of overfitting. Furthermore, PCE requires specialized\ntreatments to manage random variables that are not independent, and these\ntreatments may be problem-dependent or may fail with increasing complexity. In\nthis work, we adopt the spectral expansion formalism used in PCE; however, we\nreplace the classical polynomial basis functions with neural network (NN) basis\nfunctions to leverage their expressivity. To achieve this, we propose an\nalgorithm that identifies NN-parameterized basis functions in a purely\ndata-driven manner, without any prior assumptions about the joint distribution\nof the random variables involved, whether independent or dependent. The\nproposed algorithm identifies each NN-parameterized basis function\nsequentially, ensuring they are orthogonal with respect to the data\ndistribution. The basis functions are constructed directly on the joint\nstochastic variables without requiring a tensor product structure. This\napproach may offer greater flexibility for complex stochastic models, while\nsimplifying implementation compared to the tensor product structures typically\nused in PCE to handle random vectors. We demonstrate the effectiveness of the\nproposed scheme through several numerical examples of varying complexity and\nprovide comparisons with classical PCE.",
        "Accurate prediction of structural failure modes under seismic excitations is\nessential for seismic risk and resilience assessment. Traditional\nsimulation-based approaches often result in imbalanced datasets dominated by\nnon-failure or frequently observed failure scenarios, limiting the\neffectiveness in machine learning-based prediction. To address this challenge,\nthis study proposes a framework for constructing balanced datasets that include\ndistinct failure modes. The framework consists of three key steps. First,\ncritical ground motion features (GMFs) are identified to effectively represent\nground motion time histories. Second, an adaptive algorithm is employed to\nestimate the probability densities of various failure domains in the space of\ncritical GMFs and structural parameters. Third, samples generated from these\nprobability densities are transformed into ground motion time histories by\nusing a scaling factor optimization process. A balanced dataset is constructed\nby performing nonlinear response history analyses on structural systems with\nparameters matching the generated samples, subjected to corresponding\ntransformed ground motion time histories. Deep neural network models are\ntrained on balanced and imbalanced datasets to highlight the importance of\ndataset balancing. To further evaluate the framework's applicability, numerical\ninvestigations are conducted using two different structural models subjected to\nrecorded and synthetic ground motions. The results demonstrate the framework's\nrobustness and effectiveness in addressing dataset imbalance and improving\nmachine learning performance in seismic failure mode prediction.",
        "A graph is \\textit{rigid} if it only admits the identity endomorphism. We\nshow that for every $d\\ge 3$ there exist infinitely many mutually rigid\n$d$-regular graphs of arbitrary odd girth $g\\geq 7$. Moreover, we determine the\nminimum order of a rigid $d$-regular graph for every $d\\ge 3$. This provides\nstrong positive answers to a question of van der Zypen\n[https:\/\/mathoverflow.net\/q\/296483, https:\/\/mathoverflow.net\/q\/321108].\nFurther, we use our construction to show that every finite monoid is isomorphic\nto the endomorphism monoid of a regular graph. This solves a problem of Babai\nand Pultr [J. Comb.~Theory, Ser.~B, 1980].",
        "Let $p$ be a prime number and $\\mathcal{S}_p$ the $p$-solenoid. For\n$\\alpha\\in \\mathbb{R}\\times \\mathbb{Q}_p$ we consider in this paper a naturally\nassociated action groupoid $S_\\alpha:=\\mathbb{Z} [1\/p]\\ltimes_\\alpha\n\\mathcal{S}_p \\rightrightarrows \\mathcal{S}_p$ whose $C^*-$algebra is a model\nfor the noncommutative solenoid $\\mathcal{A}_\\alpha^\\mathscr{S}$ studied by\nLatremoli\\`ere and Packer. Following the geometric ideas of Connes and Rieffel\nto describe the Morita equivalences of noncommutative torus using the Kronecker\nfoliation on the torus, we give an explicit description of the\ngeometric\/topologic equivalence bibundle for groupoids $S_\\alpha$ and $S_\\beta$\nwhenever $\\alpha,\\beta\\in \\mathbb{R}\\times \\mathbb{Q}_p$ are in the same orbit\nof the $GL_2(\\mathbb{Z}[1\/p])$ action by linear fractional transformations. As\na corollary, for $\\alpha,\\beta\\in \\mathbb{R}\\times \\mathbb{Q}_p$ as above we\nget an explicit description of the imprimitivity bimodules for the associated\nnoncommutative solenoids.",
        "This paper presents the AI Enabled Individual Entrepreneurship Theory (AIET),\na theoretical framework explaining how artificial intelligence technologies\ntransform individual entrepreneurial capability. The theory identifies two\nfoundational premises: knowledge democratization and resource requirements\nevolution. Through three core mechanisms skill augmentation, capital structure\ntransformation, and risk profile modification AIET explains how individuals can\nnow undertake entrepreneurial activities at scales previously requiring\nsignificant organizational infrastructure. The theory presents five testable\npropositions addressing the changing relationship between organizational size\nand competitive advantage, the expansion of individual entrepreneurial\ncapacity, the transformation of market entry barriers, the evolution of\ntraditional firm advantages, and the modification of entrepreneurial risk\nprofiles. Boundary conditions related to task characteristics and market\nconditions define the theory's scope and applicability. The framework suggests\nsignificant implications for entrepreneurship theory, organizational design,\nand market structure as AI capabilities continue to advance. This theory\nprovides a foundation for understanding the evolving landscape of\nentrepreneurship in an AI-enabled world.",
        "Relationships between moduli spaces of curves and sheaves on 3-folds are\npresented starting with the Gromov-Witten\/Donaldson-Thomas correspondence\nproposed more than 20 years ago with D. Maulik, N. Nekrasov, and A. Okounkov.\nThe descendent and relative correspondences as developed with A. Pixton in the\ncontext of stable pairs led to the proof of the correspondence for the\nCalabi-Yau quintic 3-fold. More recently, the study of correspondences in\nfamilies has played an important role in connection with other basic moduli\nproblems in algebraic geometry. The full conjectural framework is presented\nhere in the context of families of 3-folds. This article accompanies my lecture\nat the ICBS in July 2024.",
        "The Doyle-Fuller-Newman model is arguably the most ubiquitous electrochemical\nmodel in lithium-ion battery research. Since it is a highly nonlinear model,\nits input-output relations are still poorly understood. Researchers therefore\noften employ sensitivity analyses to elucidate relative parametric importance\nfor certain use cases. However, some methods are ill-suited for the complexity\nof the model and appropriate methods often face the downside of only being\napplicable to scalar quantities of interest. We implement a novel framework for\nglobal sensitivity analysis of time-dependent model outputs and apply it to a\ndrive cycle simulation. We conduct a full and a subgroup sensitivity analysis\nto resolve lowly sensitive parameters and explore the model error when\nunimportant parameters are set to arbitrary values. Our findings suggest that\nthe method identifies insensitive parameters whose variations cause only small\ndeviations in the voltage response of the model. By providing the methodology,\nwe hope research questions related to parametric sensitivity for time-dependent\nquantities of interest, such as voltage responses, can be addressed more easily\nand adequately in simulative battery research and beyond.",
        "This paper investigates the computation of Hawking temperatures for black\nholes within various $f(Q)$ gravity models using the RVB method. This\ntopological approach uncovers an additional term in the temperature\ncalculation, which we propose originates from the residue of a specific contour\nintegral related to the metric or curvature. By examining several specific\n$f(Q)$ models including quadratic, logarithmic, square root, and power law\nmodifications as well as well known black hole solutions such as RN, Kerr, and\nKN black holes, we demonstrate that the correction term can consistently be\ninterpreted as this residue. Our findings provide new insights into black hole\nthermodynamics within modified gravity frameworks.",
        "During brain function, groups of neurons fire synchronously. When these\ngroups are large enough, the resulting electrical signals can be measured on\nthe scalp using Electroencephalography (EEG). The amplitude of these signals\ncan be significant depending on the size and synchronization of the neural\nactivity. EEG waves exhibit distinct patterns based on the brain's state, such\nas whether it is asleep, awake, engaged in mental calculations, or performing\nother cognitive functions. Additionally, these patterns can be modified by\nexternal factors, such as transcranial magnetic stimulation (TMS). TMS involves\nbringing an antenna that generates variable electromagnetic fields close to\nspecific areas of the skull to treat certain pathologies. Given that the human\nbody naturally generates magnetic fields, a question arises: Can these fields\ninfluence the EEG by modulating neuronal function, causing a resonance effect,\nor through some unknown interaction? This study investigated whether\napproaching the palm of the hand to the top of the head (Intervention) could\ninduce effects in the EEG. Power Spectral Density (PSD) was obtained for the 30\nseconds preceding the intervention (PSD_pre) and the final 30 seconds of the\nintervention (PSD_last). The exact Wilcoxon signed-rank test suggests that the\nmedian of PSD_pre is greater than the median of PSD_last at the 95% confidence\nlevel (p-value = 0.004353). In contrast, in the control group, the test\nindicates that at the 95% confidence level (p-value = 0.7667), the median of\nPSD_pre is not greater than the median of PSD_last.",
        "Charge-to-spin and spin-to-charge conversion mechanisms in high spin-orbit\nmaterials are the new frontier of memory devices. They operate via spin-orbit\ntorque (SOT) switching of a magnetic electrode, driven by an applied charge\ncurrent. In this work, we propose a novel memory device based on the\nsemiconducting two-dimensional centrosymmetric transition metal dichalcogenide\n(TMD) MoS2, that operates as a SOT device in the writing process and a spin\nvalve in the reading process. We demonstrate that stable voltage states at room\ntemperature can be deterministically controlled by a switching current density\nas low as 3.2x10^4 A\/cm^2 even in zero field. An applied field 50-100 Oe can be\nused as a further or alternative control parameter for the state switching. Ab\ninitio calculations of spin Hall effect (SHE) and orbital Hall effect (OHE)\nindicate that the latter is the only one responsible for the generation of the\nSOT in the magnetic electrode. The large value of OHC in bulk MoS2 makes our\ndevice competitive in terms of energetic efficiency and could be integrated in\nTMD heterostructures to design memory devices with multiple magnetization\nstates for non-Boolean computation.",
        "Ergodic optimization for beta-transformations $T_\\beta(x)= \\beta x \\pmod 1$\nis developed. If $\\beta>1$ is a beta-number, or such that the orbit-closure of\n$1$ is not minimal, we show that the Typically Periodic Optimization Conjecture\nholds, establishing that there exists an open dense set of H\\\"{o}lder\ncontinuous functions such that for each function in this set, there exists a\nunique maximizing measure, this measure is supported on a periodic orbit, and\nthe periodic locking property holds. It follows that typical periodic\noptimization is typical among the class of beta-transformations: it holds for a\nset of parameters $\\beta>1$ that is residual, and has full Lebesgue measure.",
        "A novel probabilistic framework for modelling anomalous diffusion is\npresented. The resulting process is Markovian, non-homogeneous, non-stationary,\nnon-ergodic, and state-dependent. The fundamental law governing this process is\ndriven by two opposing forces: one proportional to the current state,\nrepresenting the intensity of autocorrelation or contagion, and another\ninversely proportional to the elapsed time, acting as a damping function. The\ninterplay between these forces determines the diffusion regime, characterized\nby the ratio of their proportionality coefficients. This framework encompasses\nvarious regimes, including subdiffusion, Brownian non-Gaussian, superdiffusion,\nballistic, and hyperballistic behaviours. The hyperballistic regime emerges\nwhen the correlation force dominates over damping, whereas a balance between\nthese mechanisms results in a ballistic regime, which is also stationary.\nCrucially, non-stationarity is shown to be necessary for regimes other than\nballistic. The model's ability to describe hyperballistic phenomena has been\ndemonstrated in applications such as epidemics, software reliability, and\nnetwork traffic. Furthermore, deviations from Gaussianity are explored and\nviolations of the Central Limit Theorem are highlighted, supported by\ntheoretical analysis and simulations. It will also be shown that the model\nexhibits a strong autocorrelation structure due to a position dependent jump\nprobability.",
        "The notion of graph covers (also referred to as locally bijective\nhomomorphisms) plays an important role in topological graph theory and has\nfound its computer science applications in models of local computation. For a\nfixed target graph $H$, the {\\sc $H$-Cover} problem asks if an input graph $G$\nallows a graph covering projection onto $H$. Despite the fact that the quest\nfor characterizing the computational complexity of {\\sc $H$-Cover} had been\nstarted more than 30 years ago, only a handful of general results have been\nknown so far.\n  In this paper, we present a complete characterization of the computational\ncomplexity of covering coloured graphs for the case that every equivalence\nclass in the degree partition of the target graph has at most two vertices. We\nprove this result in a very general form. Following the lines of current\ndevelopment of topological graph theory, we study graphs in the most relaxed\nsense of the definition. In particular, we consider graphs that are mixed (they\nmay have both directed and undirected edges), may have multiple edges, loops,\nand semi-edges. We show that a strong P\/NP-complete dichotomy holds true in the\nsense that for each such fixed target graph $H$, the {\\sc $H$-Cover} problem is\neither polynomial-time solvable for arbitrary inputs, or NP-complete even for\nsimple input graphs.",
        "The coronal heating problem remains one of the most challenging questions in\nsolar physics. The energy driving coronal heating is widely understood to be\nassociated with convective motions below the photosphere. Recent\nhigh-resolution observations reveal that photospheric magnetic fields in the\nquiet Sun undergo complex and rapid evolution. These photospheric dynamics are\nexpected to be reflected in the coronal magnetic field. Motivated by these\ninsights, our research aims to explore the relationship between magnetic energy\nand coronal heating. By combining observations from Solar Orbiter and SDO with\na magnetic field extrapolation technique, we estimate the magnetic free energy\nof multi-scale energy release events in the quiet Sun. Interestingly, our\nresults reveal a strong correlation between the evolution of free energy and\nthe integrated intensity of extreme ultraviolet emission at 171 \\AA~in these\nevents. We quantitatively assess the potential energy flux budget of these\nevents to evaluate their contribution to coronal heating. Our study implies a\nlink between photospheric magnetic field evolution and coronal temperature\nvariations, paving the way for further research into similar phenomena.",
        "In 1958, Helson and Lowdenslager extended the theory of analytic functions to\na general class of groups with ordered duals. In this context, analytic\nfunctions on such a group $G$ are defined as the integrable functions whose\nFourier coefficients lie in the positive semigroup of the dual of $G$. In this\npaper, we found some applications of their theory to infinite-dimensional\ncomplex analysis. Specifically, we considered a special order on\n$\\mathbb{Z}^\\infty$ and corresponding analytic continuous functions on\n$\\mathbb{T}^\\omega$, which serves as the counterpart of the disk algebra in\ninfinitely many variables setting. By characterizing its maximal ideals, we\nhave generalized the following theorem to the infinite-dimensional case: For a\npositive function $w$ that is integrable and log-integrable on $\\mathbb{T}^d$,\nthere exists an outer function $g$ such that $w=|g|^2$ if and only if the\nsupport of $\\hat{\\log w}$ is a subset of $\\mathbb{N}^d\\cap (-\\mathbb{N})^d$.\nFurthermore, we have found the counterpart of the above function algebra in the\nclosed right half-plane, and the representing measures of each point in the\nright half-plane for this algebra. As an application of the order, we provided\na new proof of the infinite-dimensional Szeg\\\"{o}'s theorem.",
        "Many data clustering applications must handle objects that cannot be\nrepresented as vector data. In this context, the bag-of-vectors representation\ncan be leveraged to describe complex objects through discrete distributions,\nand the Wasserstein distance can effectively measure the dissimilarity between\nthem. Additionally, kernel methods can be used to embed data into feature\nspaces that are easier to analyze. Despite significant progress in data\nclustering, a method that simultaneously accounts for distributional and\nvectorial dissimilarity measures is still lacking. To tackle this gap, this\nwork explores kernel methods and Wasserstein distance metrics to develop a\ncomputationally tractable clustering framework. The compositional properties of\nkernels allow the simultaneous handling of different metrics, enabling the\nintegration of both vectors and discrete distributions for object\nrepresentation. This approach is flexible enough to be applied in various\ndomains, such as graph analysis and image processing. The framework consists of\nthree main components. First, we efficiently approximate pairwise Wasserstein\ndistances using multiple reference distributions. Second, we employ kernel\nfunctions based on Wasserstein distances and present ways of composing kernels\nto express different types of information. Finally, we use the kernels to\ncluster data and evaluate the quality of the results using scalable and\ndistance-agnostic validity indices. A case study involving two datasets of 879\nand 34,920 power distribution graphs demonstrates the framework's effectiveness\nand efficiency.",
        "We present first results of JWST Cycle 1 and 2 observations of Sgr A* using\nNIRCam taken simultaneously at 2.1 and 4.8 micron for a total of ~48 hours over\nseven different epochs in 2023 and 2024. We find correlated variability at 2.1\nand 4.8 micron in all epochs, continual short-time scale (a few seconds)\nvariability and epoch-to-epoch variable emission implying long-term ( ~days to\nmonths) variability of Sgr A*. A highlight of this analysis is the evidence for\nsub-minute, horizon-scale time variability of Sgr A*, probing inner accretion\ndisk size scales. The power spectra of the light curves in each observing epoch\nalso indicate long-term variable emission. With continuous observations, JWST\ndata suggest that the flux of Sgr A* is fluctuating constantly. The flux\ndensity correlation exhibits a distinct break in the slope at ~3 mJy at 2.1\nmicron. The analysis indicates two different processes contributing to the\nvariability of Sgr A*. Brighter emission trends towards shallower spectral\nindices than the fainter emission. Cross correlation of the light curves\nindicates for the first time, a time delay of 3 - 40 sec in the 4.8 micron\nvariability with respect to 2.1 micron. This phase shift leads to loops in\nplots of flux density vs spectral index as the emission rises and falls.\nModeling suggests that the synchrotron emission from the evolving,\nage-stratified electron population reproduces the shape of the observed light\ncurves with a direct estimate of the magnetic field strengths in the range\nbetween 40-90 G, and upper cutoff energy, E_c, between 420 and 720 MeV.",
        "A realisation of a graph in the plane as a bar-joint framework is rigid if\nthere are finitely many other realisations, up to isometries, with the same\nedge lengths. Each of these finitely-many realisations can be seen as a\nsolution to a system of quadratic equations prescribing the distances between\npairs of points. For generic realisations, the size of the solution set depends\nonly on the underlying graph so long as we allow for complex solutions. We\nprovide a characterisation of the realisation number - that is the cardinality\nof this complex solution set - of a minimally rigid graph. Our characterisation\nuses tropical geometry to express the realisation number as an intersection of\nBergman fans of the graphic matroid. As a consequence, we derive a\ncombinatorial upper bound on the realisation number involving the Tutte\npolynomial. Moreover, we provide computational evidence that our upper bound is\nusually an improvement on the mixed volume bound.",
        "In this paper, we study the small-time asymptotic behavior of symmetric, but\nnot necessarily isotropic, L\\'evy processes with weak lower scaling condition\nnear zero on its L\\'evy density. Our main result, Theorem 2.1, extends and\ngeneralizes key findings in \\cite{KP24} and \\cite{PS22} by encompassing\nnon-isotropic L\\'evy processes and providing a unified proof that includes the\ncritical case in which the one-dimensional projection of the underlying\nprocesses is non-integrable. In particular, the main result recovers\n\\cite[Theorem 1.1]{PS22} for both $\\alpha\\in (1,2)$ and $\\alpha=1$ cases and\nprovide a robust proof that can be applied to study the small-time asymptotic\nbehavior of the spectral heat content for other interesting examples discussed\nin Section 4.",
        "In a differentially private sequential learning setting, agents introduce\nendogenous noise into their actions to maintain privacy. Applying this to a\nstandard sequential learning model leads to different outcomes for continuous\nvs. binary signals. For continuous signals with a nonzero privacy budget, we\nintroduce a novel smoothed randomized response mechanism that adapts noise\nbased on distance to a threshold, unlike traditional randomized response, which\napplies uniform noise. This enables agents' actions to better reflect both\nprivate signals and observed history, accelerating asymptotic learning speed to\n$\\Theta_{\\epsilon}(\\log(n))$, compared to $\\Theta(\\sqrt{\\log(n)})$ in the\nnon-private regime where privacy budget is infinite. Moreover, in the\nnon-private setting, the expected stopping time for the first correct decision\nand the number of incorrect actions diverge, meaning early agents may make\nmistakes for an unreasonably long period. In contrast, under a finite privacy\nbudget $\\epsilon \\in (0,1)$, both remain finite, highlighting a stark contrast\nbetween private and non-private learning. Learning with continuous signals in\nthe private regime is more efficient, as smooth randomized response enhances\nthe log-likelihood ratio over time, improving information aggregation.\nConversely, for binary signals, differential privacy noise hinders learning, as\nagents tend to use a constant randomized response strategy before an\ninformation cascade forms, reducing action informativeness and hampering the\noverall process.",
        "Layered oxides exhibit high ionic mobility and chemical flexibility,\nattracting interest as cathode materials for lithium-ion batteries and the\npairing of hydrogen production and carbon capture. Recently, layered oxides\nemerged as highly tunable semiconductors. For example, by introducing\nanti-Frenkel defects, the electronic hopping conductance in hexagonal\nmanganites was increased locally by orders of magnitude. Here, we demonstrate\nlocal acceptor and donor doping in Er(Mn,Ti)O$_3$, facilitated by the splitting\nof such anti-Frenkel defects under applied d.c. voltage. By combining density\nfunctional theory calculations, scanning probe microscopy, atom probe\ntomography, and scanning transmission electron microscopy, we show that the\noxygen defects readily move through the layered crystal structure, leading to\nnano-sized interstitial-rich (p-type) and vacancy-rich (n-type) regions. The\nresulting pattern is comparable to dipolar npn-junctions and stable on the\ntimescale of days. Our findings reveal the possibility of temporarily\nfunctionalizing oxide semiconductors at the nanoscale, giving additional\nopportunities for the field of oxide electronics and the development of\ntransient electronics in general.",
        "We investigate constrained online convex optimization, in which decisions\nmust belong to a fixed and typically complicated domain, and are required to\napproximately satisfy additional time-varying constraints over the long term.\nIn this setting, the commonly used projection operations are often\ncomputationally expensive or even intractable. To avoid the time-consuming\noperation, several projection-free methods have been proposed with an\n$\\mathcal{O}(T^{3\/4} \\sqrt{\\log T})$ regret bound and an $\\mathcal{O}(T^{7\/8})$\ncumulative constraint violation (CCV) bound for general convex losses. In this\npaper, we improve this result and further establish \\textit{novel} regret and\nCCV bounds when loss functions are strongly convex. The primary idea is to\nfirst construct a composite surrogate loss, involving the original loss and\nconstraint functions, by utilizing the Lyapunov-based technique. Then, we\npropose a parameter-free variant of the classical projection-free method,\nnamely online Frank-Wolfe (OFW), and run this new extension over the\nonline-generated surrogate loss. Theoretically, for general convex losses, we\nachieve an $\\mathcal{O}(T^{3\/4})$ regret bound and an $\\mathcal{O}(T^{3\/4} \\log\nT)$ CCV bound, both of which are order-wise tighter than existing results. For\nstrongly convex losses, we establish new guarantees of an\n$\\mathcal{O}(T^{2\/3})$ regret bound and an $\\mathcal{O}(T^{5\/6})$ CCV bound.\nMoreover, we also extend our methods to a more challenging setting with bandit\nfeedback, obtaining similar theoretical findings. Empirically, experiments on\nreal-world datasets have demonstrated the effectiveness of our methods.",
        "For a given coorientable contact manifold $(M,\\Xi)$ with contact distribution\n$\\Xi$, we consider its contact forms $\\lambda$ with $\\ker \\lambda = \\Xi$, and\nthe associated contact triads $(M,\\lambda, J)$. For a generic choice of contact\nform $\\lambda$, we prove the (0-jet) the interior and boundary evaluation maps,\nand the 1-jet transversality of contact instantons (against contact\ndistribution, for example).",
        "This paper presents the technical design of the pathfinder Barcelona Raman\nLIDAR (pBRL) for the northern site of the Cherenkov Telescope Array Observatory\n(CTAO-N) located at the Roque de los Muchachos Observatory (ORM). The pBRL is\ndeveloped for continuous atmospheric characterization, essential for correcting\nhigh-energy gamma-ray observations captured by Imaging Atmospheric Cherenkov\nTelescopes (IACTs). The LIDAR consists of a steerable telescope with a 1.8 m\nparabolic mirror and a pulsed Nd:YAG laser with frequency doubling and\ntripling. It emits at wavelengths of 355 nm and 532 nm to measure aerosol\nscattering and extinction through two elastic and Raman channels. Built upon a\nformer Cherenkov Light Ultraviolet Experiment (CLUE) telescope, the pBRL's\ndesign includes a Newtonian mirror configuration, a coaxial laser beam, a\nnear-range system, a liquid light guide and a custom-made polychromator. During\na one-year test at the ORM, the stability of the LIDAR and\nsemi-remote-controlled operations were tested. This pathfinder leads the way to\ndesigning a final version of a CTAO Raman LIDAR which will provide real-time\natmospheric monitoring and, as such, ensure the necessary accuracy of\nscientific data collected by the CTAO-N telescope array.",
        "Dynamic metasurface antennas (DMAs) are a promising embodiment of\nnext-generation reconfigurable antenna technology to realize base stations and\naccess points with reduced cost and power consumption. A DMA is a thin\nstructure patterned on its front with reconfigurable radiating metamaterial\nelements (meta-atoms) that are excited by waveguides or cavities. Mutual\ncoupling between the meta-atoms can result in a strongly non-linear dependence\nof the DMA's radiation pattern on the configuration of its meta-atoms. However,\nbesides the obvious algorithmic challenges of working with physics-compliant\nDMA models, it remains unclear how mutual coupling in DMAs influences the\nability to achieve a desired wireless functionality. In this paper, we provide\ntheoretical, numerical and experimental evidence that strong mutual coupling in\nDMAs increases the radiation pattern sensitivity to the DMA configuration and\nthereby boosts the available control over the radiation pattern, improving the\nability to tailor the radiation pattern to the requirements of a desired\nwireless functionality. Counterintuitively, we hence encourage next-generation\nDMA implementations to enhance (rather than suppress) mutual coupling, in\ncombination with suitable physics-compliant modeling and optimization. We\nexpect the unveiled mechanism by which mutual coupling boosts the radiation\npattern control to also apply to other reconfigurable antenna systems based on\ntunable lumped elements.",
        "Rubber wear results from the removal of small (micrometer-sized) rubber\nparticles through crack propagation. In this study, we investigate the wear\nbehavior of Styrene-Butadiene Rubber (SBR) and Natural Rubber (NR) sliding on\ntwo different concrete surfaces under dry and wet conditions. Experiments were\nconducted at low sliding speeds ($\\approx 1 \\ {\\rm mm\/s}$) to minimize\nfrictional heating and hydrodynamic effects. For two SBR compounds, we observe\nsignificantly higher wear rates in water compared to the dry state, with\nenhancement factors of $1.5-2.5$ for a low-glass-transition-temperature SBR\ncompound ($T_{\\rm g} = -50^\\circ {\\rm C}$) and approximately $4$ for a\nhigher-glass-transition compound ($T_{\\rm g} = -7^\\circ {\\rm C}$). In contrast,\nthe NR compound showed no wear in water at low nominal contact pressures\n($\\sigma_0 \\approx 0.12$, $0.16$, and $0.25 \\ {\\rm MPa}$), while at higher\npressures ($\\sigma_0 \\approx 0.36$ and $0.49 \\ {\\rm MPa}$), the wear rates in\ndry and wet states were similar. The experimental results are analyzed using a\nrecently developed rubber wear theory. The findings provide insights into the\nmechanisms of rubber wear under varying environmental and mechanical\nconditions, highlighting the influence of material properties, interfacial\neffects, and applied pressures on wear behavior.",
        "Understanding the dynamics of quantum systems is crucial in many areas of\nphysics, but simulating many-body systems presents significant challenges due\nto the large Hilbert space to navigate and the exponential growth of\ncomputational overhead. Quantum computers offer a promising platform to\novercome these challenges, particularly for simulating the time evolution with\nHamiltonians. Trotterization is a widely used approach among available\nalgorithms in this regard, and well suited for near-term quantum devices.\nHowever, it introduces algorithmic Trotter errors due to the non-commutativity\nof Hamiltonian components. Several techniques such as multi-product formulas\nhave been developed to mitigate Trotter errors, but often require deep quantum\ncircuits, which can introduce additional physical errors. In this work, we\npropose a resource-efficient scheme to reduce the algorithmic Trotter error\nwith relatively shallow circuit depth. We develop a profiling method by\nintroducing an auxiliary parameter to estimate the error effects in expectation\nvalues, enabling significant error suppression with a fixed number of Trotter\nsteps. Our approach offers an efficient way of quantum simulation on near-term\nquantum processors with shallow circuits.",
        "This paper broaches the peridynamic inverse problem of determining the\nhorizon size of the kernel function in a one-dimensional model of a linear\nmicroelastic material. We explore different kernel functions, including\nV-shaped, distributed, and tent kernels. The paper presents numerical\nexperiments using PINNs to learn the horizon parameter for problems in one and\ntwo spatial dimensions. The results demonstrate the effectiveness of PINNs in\nsolving the peridynamic inverse problem, even in the presence of challenging\nkernel functions. We observe and prove a one-sided convergence behavior of the\nStochastic Gradient Descent method towards a global minimum of the loss\nfunction, suggesting that the true value of the horizon parameter is an\nunstable equilibrium point for the PINN's gradient flow dynamics."
      ]
    }
  },
  {
    "id":2411.04747,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Trends in Phase II Trials for Cancer Therapies",
    "start_abstract":"Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "E(n) Equivariant Graph Neural Networks"
      ],
      "abstract":[
        "This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "OpeNLGauge: An Explainable Metric for NLG Evaluation with Open-Weights\n  LLMs",
        "Contextual bandits with entropy-based human feedback",
        "Rational Functions on the Projective Line from a Computational Viewpoint",
        "EarthScape: A Multimodal Dataset for Surficial Geologic Mapping and\n  Earth Surface Analysis",
        "Solar flares as electron accelerators: toward a resolution of the\n  acceleration efficiency issue",
        "Connectivity of Coxeter group Morse boundaries",
        "Passive Heart Rate Monitoring During Smartphone Use in Everyday Life",
        "Privacy Bills of Materials: A Transparent Privacy Information Inventory\n  for Collaborative Privacy Notice Generation in Mobile App Development",
        "Enhancing Transformers for Generalizable First-Order Logical Entailment",
        "Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm\n  Perception in Large Vision-Language Models",
        "Monotonicity results in half spaces for quasilinear elliptic equations\n  involving a singular term",
        "On Fair Ordering and Differential Privacy",
        "Comparative Analysis of Efficient Adapter-Based Fine-Tuning of\n  State-of-the-Art Transformer Models",
        "Feedback cooling of fermionic atoms in optical lattices",
        "A Constraint-Preserving Neural Network Approach for Solving Mean-Field\n  Games Equilibrium",
        "Adaptive Mixture of Experts Learning for Robust Audio Spoofing Detection",
        "Model Fusion via Neuron Transplantation",
        "Ringworlds and Dyson spheres can be stable",
        "Parametrization Framework for the Deceleration Parameter in Scalar Field\n  Dark Energy Model",
        "KeBaB: $k$-mer based breaking for finding super-maximal exact matches",
        "The Transition from Centralized Machine Learning to Federated Learning\n  for Mental Health in Education: A Survey of Current Methods and Future\n  Directions",
        "Holes in silicon are heavier than expected: transport properties of\n  extremely high mobility electrons and holes in silicon MOSFETs",
        "Uniform Lyndon interpolation for the pure logic of necessitation with a\n  modal reduction principle",
        "Computing the $p$-Laplacian eigenpairs of signed graphs",
        "Global Lipschitz and Sobolev estimates for the Monge-Amp\\`ere\n  eigenfunctions of general bounded convex domains",
        "Academic Literature Recommendation in Large-scale Citation Networks\n  Enhanced by Large Language Models",
        "OminiControl2: Efficient Conditioning for Diffusion Transformers",
        "Differential inclusion systems with double phase competing operators,\n  convection, and mixed boundary conditions",
        "Supervised contrastive learning for cell stage classification of animal\n  embryos"
      ],
      "abstract":[
        "Large Language Models (LLMs) have demonstrated great potential as evaluators\nof NLG systems, allowing for high-quality, reference-free, and multi-aspect\nassessments. However, existing LLM-based metrics suffer from two major\ndrawbacks: reliance on proprietary models to generate training data or perform\nevaluations, and a lack of fine-grained, explanatory feedback. In this paper,\nwe introduce OpeNLGauge, a fully open-source, reference-free NLG evaluation\nmetric that provides accurate explanations based on error spans. OpeNLGauge is\navailable as a two-stage ensemble of larger open-weight LLMs, or as a small\nfine-tuned evaluation model, with confirmed generalizability to unseen tasks,\ndomains and aspects. Our extensive meta-evaluation shows that OpeNLGauge\nachieves competitive correlation with human judgments, outperforming\nstate-of-the-art models on certain tasks while maintaining full reproducibility\nand providing explanations more than twice as accurate.",
        "In recent years, preference-based human feedback mechanisms have become\nessential for enhancing model performance across diverse applications,\nincluding conversational AI systems such as ChatGPT. However, existing\napproaches often neglect critical aspects, such as model uncertainty and the\nvariability in feedback quality. To address these challenges, we introduce an\nentropy-based human feedback framework for contextual bandits, which\ndynamically balances exploration and exploitation by soliciting expert feedback\nonly when model entropy exceeds a predefined threshold. Our method is\nmodel-agnostic and can be seamlessly integrated with any contextual bandit\nagent employing stochastic policies. Through comprehensive experiments, we show\nthat our approach achieves significant performance improvements while requiring\nminimal human feedback, even under conditions of suboptimal feedback quality.\nThis work not only presents a novel strategy for feedback solicitation but also\nhighlights the robustness and efficacy of incorporating human guidance into\nmachine learning systems. Our code is publicly available:\nhttps:\/\/github.com\/BorealisAI\/CBHF",
        "We explore the moduli space $\\mathcal{M}_d^1$ of degree $d \\geq 3$ rational\nfunctions on the projective line using a machine learning approach, focusing on\nautomorphism group classification. For $d = 3$, where $\\mathcal{M}_3^1 =\n{\\mathbb P}_{\\mathbf{w}}^5 ({\\mathbb Q})$ with weights $\\mathbf{w} = (2, 2, 3,\n3, 4, 6)$, we generate a dataset of 2,078,697 rational functions over $\\Q$ with\nnaive height $\\leq 4$. Initial coefficient-based models achieved high overall\naccuracy but struggled with minority classes due to extreme class imbalance. By\nusing invariants $\\xi_0, \\ldots, \\xi_5$ as features in a Random Forest\nclassifier, we achieved approximately 99.992\\% accuracy, mirroring successes in\ngenus 2 curves \\cite{2024-03}. This highlights the transformative role of\ninvariants in arithmetic dynamics, yet for $d > 3$, unknown generators of\n$\\mathcal{R}_{(d+1, d-1)}$ pose scalability challenges. Our framework bridges\ndata-driven and algebraic methods, with potential extensions to higher degrees\nand $\\mathcal{M}_d^2$.",
        "Surficial geologic mapping is essential for understanding Earth surface\nprocesses, addressing modern challenges such as climate change and national\nsecurity, and supporting common applications in engineering and resource\nmanagement. However, traditional mapping methods are labor-intensive, limiting\nspatial coverage and introducing potential biases. To address these\nlimitations, we introduce EarthScape, a novel, AI-ready multimodal dataset\nspecifically designed for surficial geologic mapping and Earth surface\nanalysis. EarthScape integrates high-resolution aerial RGB and near-infrared\n(NIR) imagery, digital elevation models (DEM), multi-scale DEM-derived terrain\nfeatures, and hydrologic and infrastructure vector data. The dataset provides\ndetailed annotations for seven distinct surficial geologic classes encompassing\nvarious geological processes. We present a comprehensive data processing\npipeline using open-sourced raw data and establish baseline benchmarks using\ndifferent spatial modalities to demonstrate the utility of EarthScape. As a\nliving dataset with a vision for expansion, EarthScape bridges the gap between\ncomputer vision and Earth sciences, offering a valuable resource for advancing\nresearch in multimodal learning, geospatial analysis, and geological mapping.\nOur code is available at https:\/\/github.com\/masseygeo\/earthscape.",
        "A major open issue concerning the active Sun is the effectiveness with which\nmagnetic reconnection accelerates electrons in flares. A paper published by\n{\\em{Nature}} in 2022 used microwave observations to conclude that the Sun is\nan almost ideal accelerator, energizing nearly all electrons within a coronal\nvolume to nonthermal energies. Shortly thereafter, a paper published in\n{\\em{Astrophysical Journal Letters}} used hard X-ray measurements \\emph{of the\nsame event} to reach the contradictory conclusion that less than 1\\% of the\navailable electrons were accelerated. Here we address this controversy by using\nspatially resolved observations of hard X-ray emission and a spectral inversion\nmethod to determine the evolution of the electron spectrum throughout the\nflare. So we estimated the density of the medium where electrons accelerate\nand, from this, the ratio of accelerated to ambient electron densities. Results\nshow that this ratio never exceeds a percent or so in the cases analyzed.",
        "We study the connectivity of Morse boundaries of Coxeter groups. We define\ntwo conditions on the defining graph of a Coxeter group: wide-avoidant and\nwide-spherical-avoidant. We show that wide-spherical-avoidant, one-ended,\naffine-free Coxeter groups have connected and locally connected Morse\nboundaries. On the other hand, one-ended Coxeter groups that are not\nwide-avoidant and not wide have disconnected Morse boundary. For the\nright-angled case, we get a full characterization: a one-ended right-angled\nCoxeter group has connected, non-empty Morse boundary if and only if it is\nwide-avoidant. Along the way we characterize Morse geodesic rays in affine-free\nCoxeter groups as those that spend uniformly bounded time in cosets of wide\nspecial subgroups.",
        "Resting heart rate (RHR) is an important biomarker of cardiovascular health\nand mortality, but tracking it longitudinally generally requires a wearable\ndevice, limiting its availability. We present PHRM, a deep learning system for\npassive heart rate (HR) and RHR measurements during everyday smartphone use,\nusing facial video-based photoplethysmography. Our system was developed using\n225,773 videos from 495 participants and validated on 185,970 videos from 205\nparticipants in laboratory and free-living conditions, representing the largest\nvalidation study of its kind. Compared to reference electrocardiogram, PHRM\nachieved a mean absolute percentage error (MAPE) < 10% for HR measurements\nacross three skin tone groups of light, medium and dark pigmentation; MAPE for\neach skin tone group was non-inferior versus the others. Daily RHR measured by\nPHRM had a mean absolute error < 5 bpm compared to a wearable HR tracker, and\nwas associated with known risk factors. These results highlight the potential\nof smartphones to enable passive and equitable heart health monitoring.",
        "Privacy regulations mandate that developers must provide authentic and\ncomprehensive privacy notices, e.g., privacy policies or labels, to inform\nusers of their apps' privacy practices. However, due to a lack of knowledge of\nprivacy requirements, developers often struggle to create accurate privacy\nnotices, especially for sophisticated mobile apps with complex features and in\ncrowded development teams. To address these challenges, we introduce Privacy\nBills of Materials (PriBOM), a systematic software engineering approach that\nleverages different development team roles to better capture and coordinate\nmobile app privacy information. PriBOM facilitates transparency-centric privacy\ndocumentation and specific privacy notice creation, enabling traceability and\ntrackability of privacy practices. We present a pre-fill of PriBOM based on\nstatic analysis and privacy notice analysis techniques. We demonstrate the\nperceived usefulness of PriBOM through a human evaluation with 150 diverse\nparticipants. Our findings suggest that PriBOM could serve as a significant\nsolution for providing privacy support in DevOps for mobile apps.",
        "Transformers, as a fundamental deep learning architecture, have demonstrated\nremarkable capabilities in reasoning. This paper investigates the generalizable\nfirst-order logical reasoning ability of transformers with their parameterized\nknowledge and explores ways to improve it. The first-order reasoning capability\nof transformers is assessed through their ability to perform first-order\nlogical entailment, which is quantitatively measured by their performance in\nanswering knowledge graph queries. We establish connections between (1) two\ntypes of distribution shifts studied in out-of-distribution generalization and\n(2) the unseen knowledge and query settings discussed in the task of knowledge\ngraph query answering, enabling a characterization of fine-grained\ngeneralizability. Results on our comprehensive dataset show that transformers\noutperform previous methods specifically designed for this task and provide\ndetailed empirical evidence on the impact of input query syntax, token\nembedding, and transformer architectures on the reasoning capability of\ntransformers. Interestingly, our findings reveal a mismatch between positional\nencoding and other design choices in transformer architectures employed in\nprior practices. This discovery motivates us to propose a more sophisticated,\nlogic-aware architecture, TEGA, to enhance the capability for generalizable\nfirst-order logical entailment in transformers.",
        "With the advent of large vision-language models (LVLMs) demonstrating\nincreasingly human-like abilities, a pivotal question emerges: do different\nLVLMs interpret multimodal sarcasm differently, and can a single model grasp\nsarcasm from multiple perspectives like humans? To explore this, we introduce\nan analytical framework using systematically designed prompts on existing\nmultimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2,409\nsamples, we examine interpretive variations within and across models, focusing\non confidence levels, alignment with dataset labels, and recognition of\nambiguous \"neutral\" cases. Our findings reveal notable discrepancies -- across\nLVLMs and within the same model under varied prompts. While\nclassification-oriented prompts yield higher internal consistency, models\ndiverge markedly when tasked with interpretive reasoning. These results\nchallenge binary labeling paradigms by highlighting sarcasm's subjectivity. We\nadvocate moving beyond rigid annotation schemes toward multi-perspective,\nuncertainty-aware modeling, offering deeper insights into multimodal sarcasm\ncomprehension. Our code and data are available at:\nhttps:\/\/github.com\/CoderChen01\/LVLMSarcasmAnalysis",
        "We consider positive solutions to $\\displaystyle -\\Delta_p\nu=\\frac{1}{u^\\gamma}+f(u)$ under zero Dirichlet condition in the half space.\nExploiting a prio-ri estimates and the moving plane technique, we prove that\nany solution is monotone increasing in the direction orthogonal to the\nboundary.",
        "In blockchain systems, fair transaction ordering is crucial for a trusted and\nregulation-compliant economic ecosystem. Unlike traditional State Machine\nReplication (SMR) systems, which focus solely on liveness and safety,\nblockchain systems also require a fairness property. This paper examines these\nproperties and aims to eliminate algorithmic bias in transaction ordering\nservices.\n  We build on the notion of equal opportunity. We characterize transactions in\nterms of relevant and irrelevant features, requiring that the order be\ndetermined solely by the relevant ones. Specifically, transactions with\nidentical relevant features should have an equal chance of being ordered before\none another. We extend this framework to define a property where the greater\nthe distance in relevant features between transactions, the higher the\nprobability of prioritizing one over the other.\n  We reveal a surprising link between equal opportunity in SMR and Differential\nPrivacy (DP), showing that any DP mechanism can be used to ensure fairness in\nSMR. This connection not only enhances our understanding of the interplay\nbetween privacy and fairness in distributed computing but also opens up new\nopportunities for designing fair distributed protocols using well-established\nDP techniques.",
        "In this work, we investigate the efficacy of various adapter architectures on\nsupervised binary classification tasks from the SuperGLUE benchmark as well as\na supervised multi-class news category classification task from Kaggle.\nSpecifically, we compare classification performance and time complexity of\nthree transformer models, namely DistilBERT, ELECTRA, and BART, using\nconventional fine-tuning as well as nine state-of-the-art (SoTA) adapter\narchitectures. Our analysis reveals performance differences across adapter\narchitectures, highlighting their ability to achieve comparable or better\nperformance relative to fine-tuning at a fraction of the training time. Similar\nresults are observed on the new classification task, further supporting our\nfindings and demonstrating adapters as efficient and flexible alternatives to\nfine-tuning. This study provides valuable insights and guidelines for selecting\nand implementing adapters in diverse natural language processing (NLP)\napplications.",
        "We discuss the preparation of topological insulator states with fermionic\nultracold atoms in optical lattices by means of measurement-based Markovian\nfeedback control. The designed measurement and feedback operators induce an\neffective dissipative channel that stabilizes the desired insulator state,\neither in an exact way or approximately in the case where additional\nexperimental constraints are assumed. Successful state preparation is\ndemonstrated in one-dimensional insulators as well as for Haldane's Chern\ninsulator, by calculating the fidelity between the target ground state and the\nsteady state of the feedback-modified master equation. The fidelity is obtained\nvia time evolution of the system with moderate sizes. For larger 2D systems, we\ncompare the mean occupation of the single-particle eigenstates for the ground\nand steady state computed through mean-field kinetic equations.",
        "Neural network-based methods have demonstrated effectiveness in solving\nhigh-dimensional Mean-Field Games (MFG) equilibria, yet ensuring mathematically\nconsistent density-coupled evolution remains a major challenge. This paper\nproposes the NF-MKV Net, a neural network approach that integrates\nprocess-regularized normalizing flow (NF) with state-policy-connected\ntime-series neural networks to solve MKV FBSDEs and their associated\nfixed-point formulations of MFG equilibria. The method first reformulates MFG\nequilibria as MKV FBSDEs, embedding density evolution into equation\ncoefficients within a probabilistic framework. Neural networks are then\nemployed to approximate value functions and their gradients. To enforce\nvolumetric invariance and temporal continuity, NF architectures impose loss\nconstraints on each density transfer function.",
        "In audio spoofing detection, most studies rely on clean datasets, making\nmodels susceptible to real-world post-processing attacks, such as channel\ncompression and noise. To overcome this challenge, we propose the Adaptive\nMixture of Experts Learning (AMEL) framework, which enhances resilience by\nleveraging attack-specific knowledge and adapting dynamically to varied attack\nconditions. Specifically, AMEL utilizes Attack-Specific Experts (ASE)\nfine-tuned with Low-Rank Adaptation (LoRA), enabling each expert to target\nspecific post-processing patterns while requiring only 1.12\\% of the parameters\nneeded for full fine-tuning. Furthermore, we introduce Dynamic Expert\nAggregation (DEA), which adaptively selects and integrates expert knowledge to\nenhance the robustness of spoofing detection. Experimental results demonstrate\nthat AMEL significantly enhances robustness by improving noise resilience and\nexhibiting greater adaptability to previously unseen post-processing methods\ncompared to models relying on full fine-tuning. Additionally, our framework\noutperforms both single expert and simple average ensemble under various mixed\nattacks, demonstrating its superior robustness and adaptability in managing\ncomplex, real-world conditions.",
        "Ensemble learning is a widespread technique to improve the prediction\nperformance of neural networks. However, it comes at the price of increased\nmemory and inference time. In this work we propose a novel model fusion\ntechnique called \\emph{Neuron Transplantation (NT)} in which we fuse an\nensemble of models by transplanting important neurons from all ensemble members\ninto the vacant space obtained by pruning insignificant neurons. An initial\nloss in performance post-transplantation can be quickly recovered via\nfine-tuning, consistently outperforming individual ensemble members of the same\nmodel capacity and architecture. Furthermore, NT enables all the ensemble\nmembers to be jointly pruned and jointly trained in a combined model. Comparing\nit to alignment-based averaging (like Optimal-Transport-fusion), it requires\nless fine-tuning than the corresponding OT-fused model, the fusion itself is\nfaster and requires less memory, while the resulting model performance is\ncomparable or better. The code is available under the following link:\nhttps:\/\/github.com\/masterbaer\/neuron-transplantation.",
        "In his 1856 Adams Prize essay, James Clark Maxwell demonstrated that Saturn's\nrings cannot be comprised of a uniform rigid body. This is a consequence of the\ntwo-body gravitational interaction between a ring and planet resulting in\ninstability. Similarly, it is also known that a so-called Dyson sphere\nencompassing a single star would be unstable due to Newton's shell theorem. A\nsurprising finding is reported here that both a ring and a sphere (shell) can\nbe stable in the restricted three-body problem. First, if two primary masses\nare considered in orbit about their common centre of mass, a large, uniform,\ninfinitesimal ring enclosing the smaller of the masses can in principle be\nstable under certain conditions. Similarly, a Dyson sphere can, be stable, if\nthe sphere encloses the smaller of the two primary masses, again under certain\nconditions. These findings extend Maxwell's results on the dynamics of rings\nand have an interesting bearing on so-called Ringworlds and Dyson spheres from\nfiction. Moreover, the existence of passively stable orbits for such\nlarge-scale structures may have implications for so-called techno-signatures in\nsearch for extra-terrestrial intelligence studies.",
        "We propose a Friedmann-Lemaitre-Robertson-Walker cosmological model with a\nscalar field that represents dark energy. A new parametrization of the\ndeceleration parameter is introduced of the form $q = 1 + \\eta (1 + \\mu\na^{\\eta})$ where $\\eta$ and $\\mu$ are model parameters. and the compatibility\nof the model is constrained by recent observational datasets, including cosmic\nchronometers, Pantheon+ and Baryon Acoustic Observations. By considering a\nvariable deceleration parameter, we address the expansion history of the\nuniverse, providing a viable description of the transition from deceleration to\nacceleration. Using the Markov Chain Monte Carlo method, the parameters of the\nmodel are constrained and we examine the cosmological parameters. A comparison\nis then made with the $\\Lambda$CDM model using the latest observations. We\nexamine the history of the main cosmological parameters, such as the\ndeceleration parameter, jerk parameter, snap parameter, density parameter, and\nequation-of-state parameter, by constraining and interpreting them to reveal\ninsights into what has been dubbed \"dynamical dark energy\" under the\nassumptions made above. Our method provides a framework that is independent of\nthe model to explore dark energy, leading to a deeper and more subtle\nunderstanding of the mechanisms driving late-time cosmic acceleration.",
        "Suppose we have a tool for finding super-maximal exact matches (SMEMs) and we\nwant to use it to find all the long SMEMs between a noisy long read $P$ and a\nhighly repetitive pangenomic reference $T$. Notice that if $L \\geq k$ and the\n$k$-mer $P [i..i + k - 1]$ does not occur in $T$ then no SMEM of length at\nleast $L$ contains $P [i..i + k - 1]$. Therefore, if we have a Bloom filter for\nthe distinct $k$-mers in $T$ and we want to find only SMEMs of length $L \\geq\nk$, then when given $P$ we can break it into maximal substrings consisting only\nof $k$-mers the filter says occur in $T$ -- which we call pseudo-SMEMs -- and\nsearch only the ones of length at least $L$. If $L$ is reasonably large and we\ncan choose $k$ well then the Bloom filter should be small (because $T$ is\nhighly repetitive) but the total length of the pseudo-SMEMs we search should\nalso be small (because $P$ is noisy). Now suppose we are interested only in the\nlongest $t$ SMEMs of length at least $L$ between $P$ and $T$. Notice that once\nwe have found $t$ SMEMs of length at least $\\ell$ then we need only search for\nSMEMs of length greater than $\\ell$. Therefore, if we sort the pseudo-SMEMs\ninto non-increasing order by length, then we can stop searching once we have\nfound $t$ SMEMs at least as long as the next pseudo-SMEM we would search. Our\npreliminary experiments indicate that these two admissible heuristics may\nsignificantly speed up SMEM-finding in practice.",
        "Research has increasingly explored the application of artificial intelligence\n(AI) and machine learning (ML) within the mental health domain to enhance both\npatient care and healthcare provider efficiency. Given that mental health\nchallenges frequently emerge during early adolescence -- the critical years of\nhigh school and college -- investigating AI\/ML-driven mental health solutions\nwithin the education domain is of paramount importance. Nevertheless,\nconventional AI\/ML techniques follow a centralized model training architecture,\nwhich poses privacy risks due to the need for transferring students' sensitive\ndata from institutions, universities, and clinics to central servers. Federated\nlearning (FL) has emerged as a solution to address these risks by enabling\ndistributed model training while maintaining data privacy. Despite its\npotential, research on applying FL to analyze students' mental health remains\nlimited. In this paper, we aim to address this limitation by proposing a\nroadmap for integrating FL into mental health data analysis within educational\nsettings. We begin by providing an overview of mental health issues among\nstudents and reviewing existing studies where ML has been applied to address\nthese challenges. Next, we examine broader applications of FL in the mental\nhealth domain to emphasize the lack of focus on educational contexts. Finally,\nwe propose promising research directions focused on using FL to address mental\nhealth issues in the education sector, which entails discussing the synergies\nbetween the proposed directions with broader human-centered domains. By\ncategorizing the proposed research directions into short- and long-term\nstrategies and highlighting the unique challenges at each stage, we aim to\nencourage the development of privacy-conscious AI\/ML-driven mental health\nsolutions.",
        "The quality of the silicon-oxide interface plays a crucial role in\nfabricating reproducible silicon spin qubits. In this work we characterize\ninterface quality by performing mobility measurements on silicon Hall bars. We\nfind a peak electron mobility of nearly $40,000\\,\\text{cm}^2\/\\text{Vs}$ in a\ndevice with a $21\\,\\text{nm}$ oxide layer, and a peak hole mobility of about\n$2,000\\,\\text{cm}^2\/\\text{Vs}$ in a device with $8\\,\\text{nm}$ oxide, the\nlatter being the highest recorded mobility for a p-type silicon MOSFET. Despite\nthe high device quality, we note an order-of-magnitude difference in mobility\nbetween electrons and holes. By studying additional n-type and p-type devices\nwith identical oxides, and fitting to transport theory, we show that this\nmobility discrepancy is due to valence band nonparabolicity. The\nnonparabolicity endows holes with a density-dependent transverse effective mass\nranging from $0.6m_0$ to $0.7m_0$, significantly larger than the usually quoted\nbend-edge mass of $0.22m_0$. Finally, we perform magnetotransport measurements\nto extract momentum and quantum scattering lifetimes.",
        "We prove the uniform Lyndon interpolation property (ULIP) of some extensions\nof the pure logic of necessitation $\\mathbf{N}$. For any $m, n \\in \\mathbb{N}$,\n$\\mathbf{N}^+\\mathbf{A}_{m,n}$ is the logic obtained from $\\mathbf{N}$ by\nadding a single axiom $\\Box^n \\varphi \\to \\Box^m \\varphi$, which is a\n$\\Diamond$-free modal reduction principle, and a rule $\\frac{\\neg \\Box\n\\varphi}{\\neg \\Box \\Box \\varphi}$, which is required to make the logic complete\nwith respect to its Kripke-like semantics. We first introduce a sequent\ncalculus $\\mathbf{GN}^+\\mathbf{A}_{m,n}$ for $\\mathbf{N}^+\\mathbf{A}_{m,n}$ and\nshow that it enjoys cut elimination, proving Craig and Lyndon interpolation\nproperties as a consequence. We then construct an embedding of\n$\\mathbf{N}^+\\mathbf{A}_{m,n}$ into classical propositional logic\n$\\mathbf{Cl}$, which is then used to prove ULIP of\n$\\mathbf{N}^+\\mathbf{A}_{m,n}$ by reducing it to that of $\\mathbf{Cl}$. We also\nprove ULIP of $\\mathbf{NRA}_{m,n} = \\mathbf{N} + \\Box^n \\varphi \\to \\Box^m\n\\varphi + \\frac{\\neg \\varphi}{\\neg \\Box \\varphi}$ with a similar method.",
        "As a nonlinear extension of the graph Laplacian, the graph $p$-Laplacian has\nvarious applications in many fields. Due to the nonlinearity, it is very\ndifficult to compute the eigenvalues and eigenfunctions of graph $p$-Laplacian.\nIn this paper, we establish the equivalence between the graph $p$-Laplacian\neigenproblem and the tensor eigenproblem when $p$ is even. Building on this\nresult, algorithms designed for tensor eigenproblems can be adapted to compute\nthe eigenpairs of the graph $p$-Laplacian. For general $p>1$, we give a fast\nand convergent algorithm to compute the largest eigenvalue and the\ncorresponding eigenfunction of the signless graph $p$-Laplacian. As an\napplication, we provide a new criterion to determine when a graph is not a\nsubgraph of another one, which outperforms existing criteria based on the\nlinear Laplacian and adjacency matrices. Our work highlights the deep\nconnections and numerous similarities between the spectral theories of tensors\nand graph $p$-Laplacians.",
        "We show that the Monge-Amp\\`ere eigenfunctions of general bounded convex\ndomains are globally Lipschitz. The same result holds for convex solutions to\ndegenerate Monge-Amp\\`ere equations of the form $\\det D^2 u =M|u|^p$ with zero\nboundary condition on general bounded convex domains in ${\\mathbb R}^n$ within\nthe sharp threshold $p>n-2$. As a consequence, we obtain global $W^{2, 1}$\nestimates for these solutions.",
        "Literature recommendation is essential for researchers to find relevant\narticles in an ever-growing academic field. However, traditional methods often\nstruggle due to data limitations and methodological challenges. In this work,\nwe construct a large citation network and propose a hybrid recommendation\nframework for scientific article recommendation. Specifically, the citation\nnetwork contains 190,381 articles from 70 journals, covering statistics,\neconometrics, and computer science, spanning from 1981 to 2022. The\nrecommendation mechanism integrates network-based citation patterns with\ncontent-based semantic similarities. To enhance content-based recommendations,\nwe employ text-embedding-3-small model of OpenAI to generate an embedding\nvector for the abstract of each article. The model has two key advantages:\ncomputational efficiency and embedding stability during incremental updates,\nwhich is crucial for handling dynamic academic databases. Additionally, the\nrecommendation mechanism is designed to allow users to adjust weights according\nto their preferences, providing flexibility and personalization. Extensive\nexperiments have been conducted to verify the effectiveness of our approach. In\nsummary, our work not only provides a complete data system for building and\nanalyzing citation networks, but also introduces a practical recommendation\nmethod that helps researchers navigate the growing volume of academic\nliterature, making it easier to find the most relevant and influential articles\nin the era of information overload.",
        "Fine-grained control of text-to-image diffusion transformer models (DiT)\nremains a critical challenge for practical deployment. While recent advances\nsuch as OminiControl and others have enabled a controllable generation of\ndiverse control signals, these methods face significant computational\ninefficiency when handling long conditional inputs. We present OminiControl2,\nan efficient framework that achieves efficient image-conditional image\ngeneration. OminiControl2 introduces two key innovations: (1) a dynamic\ncompression strategy that streamlines conditional inputs by preserving only the\nmost semantically relevant tokens during generation, and (2) a conditional\nfeature reuse mechanism that computes condition token features only once and\nreuses them across denoising steps. These architectural improvements preserve\nthe original framework's parameter efficiency and multi-modal versatility while\ndramatically reducing computational costs. Our experiments demonstrate that\nOminiControl2 reduces conditional processing overhead by over 90% compared to\nits predecessor, achieving an overall 5.9$\\times$ speedup in multi-conditional\ngeneration scenarios. This efficiency enables the practical implementation of\ncomplex, multi-modal control for high-quality image synthesis with DiT models.",
        "In this paper, a new framework for studying the existence of generalized or\nstrongly generalized solutions to a wide class of inclusion systems involving\ndouble-phase, possibly competing differential operators, convection, and mixed\nboundary conditions is introduced. The technical approach exploits Galerkin's\nmethod and a surjective theorem for multifunctions in finite dimensional\nspaces.",
        "Video microscopy, when combined with machine learning, offers a promising\napproach for studying the early development of in vitro produced (IVP) embryos.\nHowever, manually annotating developmental events, and more specifically cell\ndivisions, is time-consuming for a biologist and cannot scale up for practical\napplications. We aim to automatically classify the cell stages of embryos from\n2D time-lapse microscopy videos with a deep learning approach. We focus on the\nanalysis of bovine embryonic development using video microscopy, as we are\nprimarily interested in the application of cattle breeding, and we have created\na Bovine Embryos Cell Stages (ECS) dataset. The challenges are three-fold: (1)\nlow-quality images and bovine dark cells that make the identification of cell\nstages difficult, (2) class ambiguity at the boundaries of developmental\nstages, and (3) imbalanced data distribution. To address these challenges, we\nintroduce CLEmbryo, a novel method that leverages supervised contrastive\nlearning combined with focal loss for training, and the lightweight 3D neural\nnetwork CSN-50 as an encoder. We also show that our method generalizes well.\nCLEmbryo outperforms state-of-the-art methods on both our Bovine ECS dataset\nand the publicly available NYU Mouse Embryos dataset."
      ]
    }
  },
  {
    "id":2411.04747,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"E(n) Equivariant Graph Neural Networks",
    "start_abstract":"This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Trends in Phase II Trials for Cancer Therapies"
      ],
      "abstract":[
        "Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "On the eternal non-Markovianity of qubit maps",
        "Micro Blossom: Accelerated Minimum-Weight Perfect Matching Decoding for\n  Quantum Error Correction",
        "Search for continuous gravitational wave signals from luminous dark\n  photon superradiance clouds with LVK O3 observations",
        "Asymptotics for multiple $q$-orthogonal polynomials from the RHP",
        "Scalable First-order Method for Certifying Optimal k-Sparse GLMs",
        "Towards Transparent and Accurate Plasma State Monitoring at JET",
        "Quark number susceptibility and conserved charge fluctuation for\n  (2+1)-flavor QCD with M\\\"obius domain wall fermions",
        "Galaxy-cluster-stacked Fermi-LAT III: substructure and radio-relic\n  counterparts",
        "Erosion of a dense molecular core by a strong outflow from a massive\n  protostar",
        "Reporting on pTP sublimation during evaporation deposition",
        "Parameter Invariance Analysis of Moment Equations Using\n  Dulmage-Mendelsohn Decomposition",
        "Singularity-Based Consistent QML Estimation of Multiple Breakpoints in\n  High-Dimensional Factor Models",
        "Spectral Analysis and Invariant Measure in Studies of the Dynamics of\n  the Hemostasis of a Blood Vessel",
        "The Evolution of Hypervelocity Supernova Survivors and the Outcomes of\n  Interacting Double White Dwarf Binaries",
        "$K$-theoretic pullbacks for Lagrangians on derived critical loci",
        "Flux-tunable parity-protected qubit based on a single full-shell\n  nanowire Josephson junction",
        "Dollarized Economies in Latin America. An Inflationary Analysis of Pre,\n  During and Post Pandemic",
        "van der Waals epitaxy of $\\alpha$-MoO$_3$ films on f-mica by pulsed\n  sputter deposition",
        "Uniqueness of Dirac-harmonic maps from a compact surface with boundary",
        "Multi-Hazard Bayesian Hierarchical Model for Damage Prediction",
        "Symmetrical bipolar electrobending deformation in acceptor-doped\n  piezoceramics",
        "Smoothing surfaces on fourfolds",
        "The Atiyah-Schmid formula for reductive groups",
        "Orthogonal Delay-Doppler Division Multiplexing Modulation with\n  Hierarchical Mode-Based Index Modulation",
        "Computable $K$-theory for $\\mathrm{C}^*$-algebras: UHF algebras",
        "Topological flow data analysis for transient flow patterns: a\n  graph-based approach",
        "Transport equations for Osgood velocity fields",
        "Fault-Tolerant Qudit Gate Optimization in Solid-State Quantum Memory",
        "Lattice stitching by eigenvector continuation for Holstein polaron"
      ],
      "abstract":[
        "As is well known, unital Pauli maps can be eternally non-CP-divisible. In\ncontrast, here we show that in the case of non-unital maps, eternal\nnon-Markovianity in the non-unital part is ruled out. In the unital case, the\neternal non-Markovianity can be obtained by a convex combination of two\ndephasing semigroups, but not all three of them. We study these results and the\nramifications arising from them.",
        "Minimum-Weight Perfect Matching (MWPM) decoding is important to quantum error\ncorrection decoding because of its accuracy. However, many believe that it is\ndifficult, if possible at all, to achieve the microsecond latency requirement\nposed by superconducting qubits. This work presents the first publicly known\nMWPM decoder, called Micro Blossom, that achieves sub-microsecond decoding\nlatency. Micro Blossom employs a heterogeneous architecture that carefully\npartitions a state-of-the-art MWPM decoder between software and a programmable\naccelerator with parallel processing units, one of each vertex\/edge of the\ndecoding graph. On a surface code with code distance $d$ and a circuit-level\nnoise model with physical error rate $p$, Micro Blossom's accelerator employs\n$O(d^3)$ parallel processing units to reduce the worst-case latency from\n$O(d^{12})$ to $O(d^9)$ and reduce the average latency from $O(p d^3+1)$ to\n$O(p^2 d^2+1)$ when $p \\ll 1$.\n  We report a prototype implementation of Micro Blossom using FPGA. Measured at\n$d=13$ and $p=0.1\\%$, the prototype achieves an average decoding latency of\n$0.8 \\mu s$ at a moderate clock frequency of $62 MHz$. Micro Blossom is the\nfirst publicly known hardware-accelerated exact MWPM decoder, and the decoding\nlatency of $0.8 \\mu s$ is 8 times shorter than the best latency of MWPM decoder\nimplementations reported in the literature.",
        "Superradiance clouds of kinetically-mixed dark photons around spinning black\nholes can produce observable multi-messenger electromagnetic and gravitational\nwave signals. The cloud generates electric fields of up to a\nTeravolt-per-meter, which lead to a cascade production of charged particles,\nyielding a turbulent quasi-equilibrium plasma around the black hole, and\nresulting in electromagnetic fluxes ranging from supernova to pulsar-like\nluminosities. For stellar mass black holes, such systems resemble millisecond\npulsars and are expected to emit pulsating radio waves and continuous\ngravitational waves (CWs) within the LIGO-Virgo-KAGRA (LVK) sensitivity band.\nWe select 44 sources with approximately coincident frequencies or positive\nfrequency drifts from existing pulsar catalogs as potential candidates of\nlong-lasting superradiance clouds around old galactic black holes. For a subset\nof 34 sources that are well measured and have not been previously targeted, we\nperform the first search for CW emission in LVK data from the third observing\nrun. We find no evidence of a CW signal and place 95% confidence level upper\nlimits on the emitted strain amplitude. We interpret these results, together\nwith limits from previous searches, in terms of the underlying dark photon\ntheory by performing an analysis of the expected signals from superradiance\nclouds from galactic black holes. We find that, even for moderately spinning\nblack holes, the absence of an observed CW signal disfavors a discrete set of\ndark photon masses between about $10^{-13}$ $\\rm{eV}\/c^2$ and $10^{-12}$\n$\\rm{eV}\/c^2$ and kinetic mixing couplings in the range of $10^{-9}$-$10^{-7}$,\nsubject to assumptions about the properties of the black hole population and\nthe cloud's electromagnetic emission.",
        "We deduce the asymptotic behaviour of a broad class of multiple\n$q$-orthogonal polynomials as their degree tends to infinity. We achieve this\nby rephrasing multiple $q$-orthogonal polynomials as part of a solution to a\nRiemann Hilbert Problem (RHP). In particular, we study multiple $q$-orthogonal\npolynomials of the first kind (see [12]), which are Type II orthogonal\npolynomials with weights given by\n  \\begin{equation}\n  w_1(x) = x^\\alpha \\omega(x)d_qx,\\qquad w_2(x) = x^\\beta \\omega(x)d_qx,\n\\nonumber \\end{equation} which satisfy the constraint \\begin{equation}\\nonumber\n  |\\omega(q^{2n})-1| = \\mathcal{O}(q^{2n}), \\end{equation} as $n\\to \\infty$.\nUsing $q$-calculus we obtain detailed asymptotics for these polynomials from\nthe RHP. This class of polynomials studied was chosen in part to their\nconnection to the work of [11,12], concerning the irrationality of $\\zeta_q(1)$\nand $\\zeta_q(2)$. To conduct our asymptotic analysis we will require the\nfollowing added restrictions on $w_1(x)$ and $w_2(x)$: $\\alpha \\notin\n\\mathbb{Z}$, $\\beta \\notin \\mathbb{Z}$ and $\\alpha \\neq \\beta \\mod \\mathbb{Z}$.\nThese restrictions are necessary for the asymptotic analysis but not the\nstatement of multiple $q$-orthogonal polynomials as solutions to a RHP.\n  The author wishes to extend special thanks to Prof. Walter Van Assche, who\nmotivated this studied and provided valuable discussion.",
        "This paper investigates the problem of certifying optimality for sparse\ngeneralized linear models (GLMs), where sparsity is enforced through an\n$\\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can\ncertify optimality by pruning nodes using dual bounds, existing methods for\ncomputing these bounds are either computationally intensive or exhibit slow\nconvergence, limiting their scalability to large-scale problems. To address\nthis challenge, we propose a first-order proximal gradient algorithm designed\nto solve the perspective relaxation of the problem within a BnB framework.\nSpecifically, we formulate the relaxed problem as a composite optimization\nproblem and demonstrate that the proximal operator of the non-smooth component\ncan be computed exactly in log-linear time complexity, eliminating the need to\nsolve a computationally expensive second-order cone program. Furthermore, we\nintroduce a simple restart strategy that enhances convergence speed while\nmaintaining low per-iteration complexity. Extensive experiments on synthetic\nand real-world datasets show that our approach significantly accelerates dual\nbound computations and is highly effective in providing optimality certificates\nfor large-scale problems.",
        "Controlling and monitoring plasma within a tokamak device is complex and\nchallenging. Plasma off-normal events, such as disruptions, are hindering\nsteady-state operation. For large devices, they can even endanger the machine's\nintegrity and it represents in general one of the most serious concerns for the\nexploitation of the tokamak concept for future power plants. Effective plasma\nstate monitoring carries the potential to enable an understanding of such\nphenomena and their evolution which is crucial for the successful operation of\ntokamaks. This paper presents the application of a transparent and data-driven\nmethodology to monitor the plasma state in a tokamak. Compared to previous\nstudies in the field, supervised and unsupervised learning techniques are\ncombined. The dataset consisted of 520 expert-validated discharges from JET.\nThe goal was to provide an interpretable plasma state representation for the\nJET operational space by leveraging multi-task learning for the first time in\nthe context of plasma state monitoring. When evaluated as disruption\npredictors, a sequence-based approach showed significant improvements compared\nto the state-based models. The best resulting network achieved a promising\ncross-validated success rate when combined with a physical indicator and\naccounting for nearby instabilities. Qualitative evaluations of the learned\nlatent space uncovered operational and disruptive regions as well as patterns\nrelated to learned dynamics and global feature importance. The applied\nmethodology provides novel possibilities for the definition of triggers to\nswitch between different control scenarios, data analysis, and learning as well\nas exploring latent dynamics for plasma state monitoring. It also showed\npromising quantitative and qualitative results with warning times suitable for\navoidance purposes and distributions that are consistent with known physical\nmechanisms.",
        "We present quark number susceptibilities and conserved charge fluctuations\nfor (2+1)-flavor QCD using M\\\"obius Domain Wall fermions with a pion mass of\n\\(135~\\rm{MeV}\\). Our results are compared with hadron resonance gas models\nbelow the QCD transition temperature and with \\(\\mathcal{O}(g^2)\\) perturbation\ntheory at high temperatures. Additionally, we compare our findings with results\nfrom staggered fermion discretizations. Furthermore, we also present results of\nleading order Kurtosis of electric charge and strangeness fluctuations.",
        "Faint $\\gamma$-ray signatures emerge in Fermi-LAT data stacked scaled to the\ncharacteristic $R_{500}$ radii of MCXC galaxy clusters. This third paper in a\nseries shows a $4.3\\sigma$ excess of discrete 4FGL-DR4 catalog $\\gamma$-ray\nsources at the $r<1.5R_{500}$ radii of 205 clusters, coincident with an $r\\sim\nR_{500}$ diffuse $2.6\\sigma$ excess of 1-100 GeV emission from 75 high-latitude\nclusters. The source excess becomes highly ($>5\\sigma$) significant when\nconsidering the substantial ($3.4\\sigma$) and unexpectedly rapid quenching of\n$\\gamma$-ray sources just inside the virial shock. The excess sources show\nradial, spectral, and luminosity distributions better matching radio-relic\ncounterparts or substructure than present tentative classifications as\nblazar-candidates. Their spectral distribution is bimodal: flat-spectrum\nsources are consistent with enhanced hadronic emission behind weak, Mach\n$\\sim2$ shocks, while softer sources may be phoenix counterparts.",
        "We present Atacama Large Millimeter\/submillimeter Array Band 3 observations\nof N$_2$H$^+$ (1-0) and CH$_3$CN (5-4), as well as Band 7 observations of the\nH$_2$CO molecular line emissions from the protostellar system GGD 27-MM2(E).\nThrough position-velocity diagrams along and across the outflow axis, we study\nthe kinematics and structure of the outflow. We also fit extracted spectra of\nthe CH$_3$CN emission to obtain the physical conditions of the gas. We use the\nresults to discuss the impact of the outflow on its surroundings. We find that\nN$_2$H$^+$ emission traces a dense molecular cloud surrounding GGD 27-MM2(E).\nWe estimate that the mass of this cloud is $\\sim$13.3-26.5 M$_\\odot$. The\nmolecular cloud contains an internal cavity aligned with the H$_2$CO-traced\nmolecular outflow. The outflow, also traced by $\\mathrm{CH_3 CN}$, shows\nevidence of a collision with a molecular core (MC), as indicated by the\ndistinctive increases in the distinct physical properties of the gas such as\nexcitation temperature, column density, line width, and velocity. This\ncollision results in an X-shape structure in the northern part of the outflow\naround the position of the MC, which produces spray-shocked material downstream\nin the north of MC as observed in position-velocity diagrams both along and\nacross of the outflow axis. The outflow has a mass of 1.7-2.1 M$_\\odot$, a\nmomentum of 7.8-10.1 M$_\\odot$ km s$^{-1}$, a kinetic energy of 5.0-6.6$\\times\n10^{44}$ erg, and a mass loss rate of 4.9--6.0$\\times10^{-4}$ M$_\\odot$\nyr$^{-1}$. The molecular outflow from GGD 27-MM2(E) significantly perturbs and\nerodes its parent cloud, compressing the gas of sources such as MC and ALMA 12.\nThe feedback from this powerful protostellar outflow contributes to maintain\nthe turbulence in the surrounding area.",
        "Noble liquid detectors rely on wavelength shifter materials, such as\np-terphenyl (pTP) and Tetraphenyl-butadiene (TPB), which are widely used in\nneutrino and dark matter experiments. Given their importance, a thorough\nunderstanding and characterization of these compounds are essential for\noptimizing experimental techniques and enhancing detector performance. In this\nstudy, we report a novel phenomenon in which commonly used wavelength shifters\nundergo spontaneous sublimation under high vacuum conditions. We quantify the\nsublimation rates of pTP and TPB as a function of pressure and temperature,\nassessing their impact on material growth and physical properties.\nAdditionally, we investigate how variations in film thickness and growth rate\ninfluence the sublimation process. These findings provide critical insights\ninto improving the handling and preparation of wavelength shifters during the\nfabrication of light detectors for these experiments, ensuring their stability\nand performance in low-background photodetection systems.",
        "Living organisms maintain stable functioning amid environmental fluctuations\nthrough homeostasis, a mechanism that preserves a system's behavior despite\nchanges in environmental conditions. To elucidate homeostasis in stochastic\nbiochemical reactions, theoretical tools for assessing population-level\ninvariance under parameter perturbations are crucial. In this paper, we propose\na systematic method for identifying the stationary moments that remain\ninvariant under parameter perturbations by leveraging the structural properties\nof the stationary moment equations. A key step in this development is\naddressing the underdetermined nature of moment equations, which has\ntraditionally made it difficult to characterize how stationary moments depend\non system parameters. To overcome this, we utilize the Dulmage-Mendelsohn (DM)\ndecomposition of the coefficient matrix to extract welldetermined subequations\nand reveal their hierarchical structure. Leveraging this structure, we identify\nstationary moments whose partial derivatives with respect to parameters are\nstructurally zero, facilitating the exploration of fundamental constraints that\ngovern homeostatic behavior in stochastic biochemical systems.",
        "This paper investigates the estimation of high-dimensional factor models in\nwhich factor loadings undergo an unknown number of structural changes over\ntime. Given that a model with multiple changes in factor loadings can be\nobservationally indistinguishable from one with constant loadings but varying\nfactor variances, this reduces the high-dimensional structural change problem\nto a lower-dimensional one. Due to the presence of multiple breakpoints, the\nfactor space may expand, potentially causing the pseudo factor covariance\nmatrix within some regimes to be singular. We define two types of breakpoints:\n{\\bf a singular change}, where the number of factors in the combined regime\nexceeds the minimum number of factors in the two separate regimes, and {\\bf a\nrotational change}, where the number of factors in the combined regime equals\nthat in each separate regime. Under a singular change, we derive the properties\nof the small eigenvalues and establish the consistency of the QML estimators.\nUnder a rotational change, unlike in the single-breakpoint case, the pseudo\nfactor covariance matrix within each regime can be either full rank or\nsingular, yet the QML estimation error for the breakpoints remains stably\nbounded. We further propose an information criterion (IC) to estimate the\nnumber of breakpoints and show that, with probability approaching one, it\naccurately identifies the true number of structural changes. Monte Carlo\nsimulations confirm strong finite-sample performance. Finally, we apply our\nmethod to the FRED-MD dataset, identifying five structural breaks in factor\nloadings between 1959 and 2024.",
        "A mathematical model of atherosclerosis of a blood vessel is advanced with\nregard for the entry of low-density lipoproteins (LDLs) into blood. For the\nfirst time, the influence of cytokines on the inflammation of a blood vessel at\nthe formation of atherosclerotic plaques is taken into account. With the help\nof the expansion in a Fourier series and the calculation of an invariant\nmeasure, the scenario of the appearance of strange attractors depending on a\nchange in the parameter of the dissipation of cholesterol is studied. The\nconclusion is made about the interconnection of the dynamics of the metabolic\nprocess in a blood vascular system and its physical state.",
        "The recent prediction and discovery of hypervelocity supernova survivors has\nprovided strong evidence that the \"dynamically driven double-degenerate\ndouble-detonation\" (D6) Type Ia supernova scenario occurs in Nature. In this\nmodel, the accretion stream from the secondary white dwarf in a double white\ndwarf binary strikes the primary white dwarf violently enough to trigger a\nhelium shell detonation, which in turn triggers a carbon\/oxygen core\ndetonation. If the secondary white dwarf survives the primary's explosion, it\nwill be flung away as a hypervelocity star. While previous work has shown that\nthe hotter observed D6 stars can be broadly understood as secondaries whose\nouter layers have been heated by their primaries' explosions, the properties of\nthe cooler D6 stars have proven difficult to reproduce. In this paper, we show\nthat the cool D6 stars can be explained by the Kelvin-Helmholtz contraction of\nhelium or carbon\/oxygen white dwarfs that underwent significant mass loss and\ncore heating prior to and during the explosion of their white dwarf companions.\nWe find that the current population of known D6 candidates is consistent with\n~2% of Type Ia supernovae leaving behind a hypervelocity surviving companion.\nWe also calculate the evolution of hot, low-mass oxygen\/neon stars and find\nreasonable agreement with the properties of the LP 40-365 class of\nhypervelocity survivors, suggesting that these stars are the kicked remnants of\nnear-Chandrasekhar-mass oxygen\/neon white dwarfs that were partially disrupted\nby oxygen deflagrations. We use these results as motivation for schematic\ndiagrams showing speculative outcomes of interacting double white dwarf\nbinaries, including long-lived merger remnants, Type Ia supernovae, and several\nkinds of peculiar transients.",
        "Given a regular function $\\phi$ on a smooth stack, and a $(-1)$-shifted\nLagrangian $M$ on the derived critical locus of $\\phi$, under fairly general\nhypotheses, we construct a pullback map from the Grothendieck group of coherent\nmatrix factorizations of $\\phi$ to that of coherent sheaves on $M$. This map\nsatisfies a functoriality property with respect to the composition of\nLagrangian correspondences, as well as the usual bivariance and base-change\nproperties.\n  We provide three applications of the construction, one in the definition of\nquantum $K$-theory of critical loci (Landau-Ginzburg models), paving the way to\ngeneralize works of Okounkov school from Nakajima quiver varieties to quivers\nwith potentials, one in establishing a degeneration formula for $K$-theoretic\nDonaldson-Thomas theory of local Calabi-Yau 4-folds, the other in confirming a\n$K$-theoretic version of Joyce-Safronov conjecture.",
        "Leveraging the higher harmonics content of the Josephson potential in a\nsuperconducting circuit offers a promising route in the search for new qubits\nwith increased protection against decoherence. In this work, we demonstrate how\nthe flux tunability of a hybrid semiconductor-superconductor Josephson junction\nbased on a single full-shell nanowire enables this possibility. Near one flux\nquantum, $\\Phi\\approx \\Phi_0=h\/2e$, we find that the qubit system can be tuned\nfrom a gatemon regime to a parity-protected regime with qubit eigenstates\nlocalized in phase space in the $0$ and $\\pi$ minima of the Josephson potential\n($\\cos 2\\varphi_0$). Estimates of qubit coherence and relaxation times due to\ndifferent noise sources are presented.",
        "Given the hyperinflation that most of the Latin American countries suffered\nin the 90 and their decision towards adopting dollarization and in most cases\nkeeping their own currency, this paper analyzes the effectiveness of\ndollarization as a protective mechanism against economic disruptions in Latin\nAmerican countries. It assesses the context that led Latin American dollarized\ncountries to dollarize and analyzes CPI, GDP, and the poverty rates pre,\nduring, and postpandemic in Latin American countries, considering those that\nare dollarized and those that are not, and evaluating its relation to the US.\nInterviews were carried out with experts in the field. It assesses the\nadvantages and disadvantages of dollarization regarding global crises. The data\nwas compared and analyzed to check if there were patterns that support the\npaper objective which is that dollarization might serve as a protective\nmechanism against economic disruption. It was found that dollarization protects\nthe economy against inflation, however, it does not fully protect the economy\nwhen considering economic performance and poverty. In conclusion, this research\nconcludes that dollarization does not completely serve as a protective\nmechanism against economic disruptions nonetheless, it found that a bigger role\nis played by domestic policies and government action.",
        "This study examines the growth characteristics and structural properties of\n$\\alpha$-MoO$_3$ thin films with thicknesses ranging from 2.5 to 160 nm,\ndeposited on f-mica and c-sapphire substrates at 400 {\\deg}C. X-ray diffraction\nanalysis reveals that the films are predominantly orthorhombic $\\alpha$-MoO$_3$\nwith a preferred 0k0 orientation along the out-of-plane direction on both\nsubstrates. The d-spacing for the 060 reflection shows a slight reduction with\nincreased thickness, particularly on f-mica, which suggests minimal\nout-of-plane strain in the film and a stabilization of lattice parametes over\nlarger thicknesses. Furthermore, full-width at half maximum measurements\nindicate improved stacking and crystal quality on f-mica compared to\nc-sapphire. The films on f-mica exhibit epitaxial growth with specific\norientation relationships, while films on c-sapphire display a fiber texture.\nThe near-thickness-independent nature of the peak positions on f-mica suggests\nstable lattice parameters and reduced strain accumulation, could be attributed\nto the van der Waals epitaxy. These results highlight the role of substrate\nchoice in $\\alpha$-MoO$_3$ film growth and minimizing strain, providing\nvaluable insights into the tuning of thin-film properties.",
        "As a commutative version of the supersymmetric nonlinear sigma model,\nDirac-harmonic maps from Riemann surfaces were introduced fifteen years ago.\nThey are critical points of an unbounded conformally invariant functional\ninvolving two fields, a map from a Riemann surface into a Riemannian manifold\nand a section of a Dirac bundle which is the usual spinor bundle twisted with\nthe pull-back of the tangent bundle of the target by the map. As solutions to a\ncoupled nonlinear elliptic system, the existence and regularity theory of\nDirac-harmonic maps has already received much attention, while the general\nuniqueness theory has not been established yet. For uncoupled Dirac-harmonic\nmaps, the map components are harmonic maps. Since the uniqueness theory of\nharmonic maps from a compact surface with boundary is known, it is sufficient\nto consider the uniqueness of the spinor components, which are solutions to the\ncorresponding boundary value problems for a nonlinear Dirac equation. In\nparticular, when the map components belong to $W^{1,p}$ with $p>2$, the spinor\ncomponents are uniquely determined by boundary values and map components. For\ncoupled Dirac-harmonic maps, the map components are not harmonic maps. So the\nuniqueness problem is more difficult to solve. In this paper, we study the\nuniqueness problem on a compact surface with boundary. More precisely, we prove\nthe energy convexity for weakly Dirac-harmonic maps from the unit disk with\nsmall energy. This yields the first uniqueness result about Dirac-harmonic maps\nfrom a surface conformal to the unit disk with small energy and arbitrary\nboundary values.",
        "A fundamental theoretical limitation undermines current disaster risk models:\nexisting approaches suffer from two critical constraints. First, conventional\ndamage prediction models remain predominantly deterministic, relying on fixed\nparameters established through expert judgment rather than learned from data.\nSecond, probabilistic frameworks are fundamentally restricted by their\nunderlying assumption of hazard independence, which directly contradicts the\nobserved reality of cascading and compound disasters. By relying on fixed\nexpert parameters and treating hazards as independent phenomena, these models\ndangerously misrepresent the true risk landscape. This work addresses this\nchallenge by developing the Multi-Hazard Bayesian Hierarchical Model (MH-BHM),\nwhich reconceptualizes the classical risk equation beyond its deterministic\norigins. The model's core theoretical contribution lies in reformulating a\nclassical risk formula as a fully probabilistic model that naturally\naccommodates hazard interactions through its hierarchical structure while\npreserving the traditional hazard-exposure-vulnerability framework. Using\ntropical cyclone damage data (1952-2020) from the Philippines as a test case,\nwith out-of-sample validation on recent events (2020-2022), the model\ndemonstrates significant empirical advantages. Key findings include a reduction\nin damage prediction error by 61% compared to a single-hazard model, and 80%\ncompared to a benchmark deterministic model. This corresponds to an improvement\nin damage estimation accuracy of USD 0.8 billion and USD 2 billion,\nrespectively. The improved accuracy enables more effective disaster risk\nmanagement across multiple domains, from optimized insurance pricing and\nnational resource allocation to local adaptation strategies, fundamentally\nimproving society's capacity to prepare for and respond to disasters.",
        "Since 2022, large apparent strains (>1%) with highly asymmetrical\nstrain-electric field (S-E) curves have been reported in various thin\npiezoceramic materials, attributed to a bidirectional electric-field-induced\nbending (electrobending) deformation, which consistently produces convex\nbending along the negative electric field direction. In this study, we report a\nnovel unidirectional electrobending behavior in acceptor-doped K0.5Na0.5NbO3\nceramics, where convex bending always occurs along the pre-poling direction\nregardless of the direction of the applied electric field. This unique\ndeformation is related to the reorientation of the defect dipoles in one\nsurface layer during the pre-poling process, resulting in an asymmetrical\ndistribution of defect dipoles in the two surface layers. The synergistic\ninteraction between ferroelectric domains and defect dipoles in the surface\nlayers induces this unidirectional electrobending, as evidenced by a\nbutterfly-like symmetrical bipolar S-E curve with a giant apparent strain of\n3.2%. These findings provide new insights into defect engineering strategies\nfor developing advanced piezoelectric materials with large electroinduced\ndisplacements.",
        "If $\\mathcal E, \\mathcal F$ are vector bundles of ranks $r-1,r$ on a smooth\nfourfold $X$ and $\\mathcal{Hom}(\\mathcal E,\\mathcal F)$ is globally generated,\nit is well known that the general map $\\phi: \\mathcal E \\to \\mathcal F$ is\ninjective and drops rank along a smooth surface. Chang improved on this with a\nfiltered Bertini theorem. We strengthen these results by proving variants in\nwhich (a) $\\mathcal F$ is not a vector bundle and (b) $\\mathcal{Hom}(\\mathcal\nE,\\mathcal F)$ is not globally generated. As an application, we give examples\nof even linkage classes of surfaces on $\\mathbb P^4$ in which all integral\nsurfaces are smoothable, including the linkage classes associated with the\nHorrocks-Mumford surface.",
        "We prove the Atiyah-Schmid formula for tempered and projective tempered\nrepresentations of reductive groups.",
        "The orthogonal time frequency space with index modulation (OTFS-IM) offers\nflexible tradeoffs between spectral efficiency (SE) and bit error rate (BER) in\ndoubly selective fading channels. While OTFS-IM schemes demonstrated such\npotential, a persistent challenge lies in the detection complexity. To address\nthis problem, we propose the hierarchical mode-based index modulation (HMIM).\nHMIM introduces a novel approach to modulate information bits by IM patterns,\nsignificantly simplifying the complexity of maximum a posteriori (MAP)\nestimation with Gaussian noise. Further, we incorporate HMIM with the recently\nproposed orthogonal delay-Doppler division multiplexing (ODDM) modulation,\nnamely ODDM-HMIM, to exploit the full diversity of the delay-Doppler (DD)\nchannel. The BER performance of ODDM-HMIM is analyzed considering a maximum\nlikelihood (ML) detector. Our numerical results reveal that, with the same SE,\nHMIM can outperform conventional IM in terms of both BER and computational\ncomplexity. In addition, we propose a successive interference\ncancellation-based minimum mean square error (SIC-MMSE) detector for ODDM-HMIM,\nwhich enables low-complexity detection with large frame sizes.",
        "We initiate the study of the effective content of $K$-theory for\n$\\mathrm{C}^*$-algebras. We prove that there are computable functors which\nassociate, to a computably enumerable presentation of a $\\mathrm{C}^*$-algebra\n$\\boldA$, computably enumerable presentations of the abelian groups\n$K_0(\\boldA)$ and $K_1(\\boldA)$. When $\\boldA$ is stably finite, we show that\nthe positive cone of $K_0(\\boldA)$ is computably enumerable. We strengthen the\nresults in the case that $\\boldA$ is a UHF algebra by showing that the\naforementioned presentation of $K_0(\\boldA)$ is actually computable. In the UHF\ncase, we also show that $\\boldA$ has a computable presentation precisely when\n$K_0(\\boldA)$ has a computable presentation, which in turn is equivalent to the\nsupernatural number of $\\boldA$ being lower semicomputable; we give an example\nthat shows that this latter equivalence cannot be improved to requiring that\nthe supernatural number of $\\boldA$ is computable. Finally, we prove that every\nUHF algebra is computably categorical.",
        "We introduce a time-series analysis method for transient two-dimensional flow\npatterns based on Topological Flow Data Analysis (TFDA), a new approach to\ntopological data analysis. TFDA identifies local topological flow structures\nfrom an instantaneous streamline pattern and describes their global connections\nas a unique planar tree and its string representation. With TFDA, the evolution\nof two-dimensional flow patterns is reduced to a discrete dynamical system\nrepresented as a transition graph between topologically equivalent streamline\npatterns. We apply this method to study the lid-driven cavity flow at Reynolds\nnumbers ranging from $Re=14000$ to $Re=16000$, a benchmark problem in fluid\ndynamics data analysis. Our approach reveals the transition from periodic to\nchaotic flow at a critical Reynolds number when the reduced dynamical system is\nmodelled as a Markov process on the transition graph. Additionally, we perform\nan observational causal inference to analyse changes in local flow patterns at\nthe cavity corners and discuss differences with a standard interventional\nsensitivity analysis. This work demonstrates the potential of TFDA-based\ntime-series analysis for uncovering complex dynamical behaviours in fluid flow\ndata.",
        "We consider the transport equation with a velocity field satisfying the\nOsgood condition. The weak formulation is not meaningful in the usual Lebesgue\nsense, meaning that the usual DiPerna--Lions treatment of the problem is not\napplicable {(in particular, the divergence of the velocity might be\nunbounded)}. Instead, we use Riemann--Stieltjes integration to interpret the\nweak formulation, leading to a well-posedness theory in regimes not covered by\nexisting works. The most general results are for the one-dimensional problem,\nwith generalisations to multiple dimensions in the particular case of\nlog-Lipschitz velocities.",
        "Achieving scalable, fault-tolerant quantum computation requires quantum\nmemory architectures that minimize error correction overhead while preserving\ncoherence. This work presents a framework for high-dimensional qudit memory in\n153Eu:Y2SiO5, integrating three core mechanisms: (i) non-destructive syndrome\nextraction, using spin-echo sequences to encode error syndromes without direct\nmeasurement; (ii) adaptive quantum Fourier transform (QFT) for error\nidentification, leveraging frequency-space transformations to reduce gate\ncomplexity; and (iii) coset-based fault-tolerant correction, factorizing large\nstabilizer-like unitaries into modular operations to confine error propagation.\nBy combining generalized stabilizer formalism, Weyl-Heisenberg operators, and\nfinite-group coset decompositions, we develop a qudit error correction scheme\noptimized for solid-state quantum memory. This approach circumvents\nresource-intensive multi-qubit concatenation, enabling scalable, long-lived\nquantum storage with efficient state retrieval and computational redundancy.\nThese results provide a pathway toward practical fault-tolerant architectures\nfor rare-earth-ion-doped quantum memories.",
        "Simulations of lattice particle - phonon systems are fundamentally restricted\nby the exponential growth of the number of quantum states with the lattice\nsize. Here, we demonstrate an algorithm that constructs the lowest eigenvalue\nand eigenvector for the Holstein model in extended lattices from eigenvalue\nproblems for small, independent lattice segments. This leads to exponential\nreduction of the computational Hilbert space and allows applications of\nvariational quantum algorithms to particle - phonon interactions in large\nlattices. We illustrate that the ground state of the Holstein polaron in the\nentire range of electron - phonon coupling, from weak to strong, and the lowest\nphonon frequency ($\\omega\/t = 0.1$) considered by numerical calculations to\ndate can be obtained from a sequence of up to four-site problems. When combined\nwith quantum algorithms, the present approach leads to a dramatic reduction of\nrequired quantum resources. We show that the ground state of the Holstein\npolaron in a lattice with 100 sites and 32 site phonons can be computed by a\nvariational quantum eigensolver with 11 qubits."
      ]
    }
  },
  {
    "id":2411.11513,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"The Genome Analysis Toolkit: A MapReduce framework for analyzing next-generation DNA sequencing data",
    "start_abstract":"Next-generation DNA sequencing (NGS) projects, such as the 1000 Genomes Project, are already revolutionizing our understanding of genetic variation among individuals. However, massive data sets generated by NGS\u2014the Genome pilot alone includes nearly five terabases\u2014make writing feature-rich, efficient, and robust analysis tools difficult for even computationally sophisticated Indeed, many professionals limited in scope ease with which they can answer scientific questions complexity accessing manipulating produced these machines. Here, we discuss Analysis Toolkit (GATK), a structured programming framework designed to development efficient next-generation sequencers using functional philosophy MapReduce. The GATK provides small but rich set access patterns that encompass majority tool needs. Separating specific calculations from common management infrastructure enables us optimize correctness, stability, CPU memory efficiency enable distributed shared parallelization. We highlight capabilities describing implementation application robust, scale-tolerant like coverage calculators single nucleotide polymorphism (SNP) calling. conclude developers analysts quickly easily write NGS tools, have been incorporated into large-scale projects Project Cancer Atlas.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Rethinking the Inception Architecture for Computer Vision"
      ],
      "abstract":[
        "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety tasks. Since 2014 very deep convolutional started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend translate immediate quality tasks (as long as enough labeled data is provided training), efficiency low parameter count still enabling factors use cases such mobile big-data scenarios. Here we exploring ways scale up that aim utilizing added computation efficiently possible by suitably factorized convolutions aggressive regularization. We benchmark our methods on ILSVRC 2012 classification challenge validation set demonstrate over art: 21:2% top-1 5:6% top-5 error single frame evaluation using network with 5 billion multiply-adds per inference less than 25 million parameters. With an ensemble 4 models multi-crop evaluation, report 3:5% 17:3% 3:6% official test set."
      ],
      "categories":[
        "BioInformatics"
      ]
    },
    "list":{
      "title":[
        "Modeling and stability analysis of live systems with time-varying\n  dimension",
        "Physics-Informed Neural Network Surrogate Models for River Stage\n  Prediction",
        "Robustness tests for biomedical foundation models should tailor to\n  specification",
        "Differentiable Information Enhanced Model-Based Reinforcement Learning",
        "Quantum induced superradiance",
        "Computerized Assessment of Motor Imitation for Distinguishing Autism in\n  Video (CAMI-2DNet)",
        "Social Media for Activists: Reimagining Safety, Content Presentation,\n  and Workflows",
        "OODD: Test-time Out-of-Distribution Detection with Dynamic Dictionary",
        "Moving Plasma Structures and Possible Driving Mechanisms of Solar\n  Microflares Observed with High-Resolution Coronal Imaging",
        "Grid-level impacts of renewable energy on thermal generation:\n  efficiency, emissions and flexibility",
        "Stable Hypergraph Matching in Unimodular Hypergraphs",
        "The rise of stochasticity in physics",
        "Inspecting the Representation Manifold of Differentially-Private Text",
        "Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large\n  Language Models",
        "Hyperspectral image reconstruction by deep learning with super-Rayleigh\n  speckles",
        "A Steerable Deep Network for Model-Free Diffusion MRI Registration",
        "Short-time Variational Mode Decomposition",
        "Spherically symmetric electrically counterpoised dust either collapses\n  or disperses",
        "Backcasting the Optimal Decisions in Transport Systems: An Example with\n  Electric Vehicle Purchase Incentives",
        "Revisiting Convolution Architecture in the Realm of DNA Foundation\n  Models",
        "Thermal and baryon density modifications to the $\\sigma$-boson\n  propagator: A road to describe the transfer of vorticity to spin in a nuclear\n  environment in relativistic heavy-ion collisions",
        "Iterative Motion Planning in Multi-agent Systems with Opportunistic\n  Communication under Disturbance",
        "REdiSplats: Ray Tracing for Editable Gaussian Splatting",
        "A Hierarchical Region-Based Approach for Efficient Multi-Robot\n  Exploration",
        "A note on Ordered Ruzsa-Szemer\\'edi graphs",
        "CMamba: Learned Image Compression with State Space Models",
        "Regularized higher-order Taylor approximation methods for nonlinear\n  least-squares",
        "Maximum likelihood estimation in the sparse Rasch model",
        "Benchmarking Classical, Deep, and Generative Models for Human Activity\n  Recognition"
      ],
      "abstract":[
        "A major limitation of the classical control theory is the assumption that the\nstate space and its dimension do not change with time. This prevents analyzing\nand even formalizing the stability and control problems for open multi-agent\nsystems whose agents may enter or leave the network, industrial processes where\nthe sensors or actuators may be exchanged frequently, smart grids, etc. In this\nwork, we propose a framework of live systems that covers a rather general class\nof systems with a time-varying state space. We argue that input-to-state\nstability is a proper stability notion for this class of systems, and many of\nthe classic tools and results, such as Lyapunov methods and superposition\ntheorems, can be extended to this setting.",
        "This work investigates the feasibility of using Physics-Informed Neural\nNetworks (PINNs) as surrogate models for river stage prediction, aiming to\nreduce computational cost while maintaining predictive accuracy. Our primary\ncontribution demonstrates that PINNs can successfully approximate HEC-RAS\nnumerical solutions when trained on a single river, achieving strong predictive\naccuracy with generally low relative errors, though some river segments exhibit\nhigher deviations.\n  By integrating the governing Saint-Venant equations into the learning\nprocess, the proposed PINN-based surrogate model enforces physical consistency\nand significantly improves computational efficiency compared to HEC-RAS. We\nevaluate the model's performance in terms of accuracy and computational speed,\ndemonstrating that it closely approximates HEC-RAS predictions while enabling\nreal-time inference.\n  These results highlight the potential of PINNs as effective surrogate models\nfor single-river hydrodynamics, offering a promising alternative for\ncomputationally efficient river stage forecasting. Future work will explore\ntechniques to enhance PINN training stability and robustness across a more\ngeneralized multi-river model.",
        "Existing regulatory frameworks for biomedical AI include robustness as a key\ncomponent but lack detailed implementational guidance. The recent rise of\nbiomedical foundation models creates new hurdles in testing and certification\ngiven their broad capabilities and susceptibility to complex distribution\nshifts. To balance test feasibility and effectiveness, we suggest a\npriority-based, task-oriented approach to tailor robustness evaluation\nobjectives to a predefined specification. We urge concrete policies to adopt a\ngranular categorization of robustness concepts in the specification. Our\napproach promotes the standardization of risk assessment and monitoring, which\nguides technical developments and mitigation efforts.",
        "Differentiable environments have heralded new possibilities for learning\ncontrol policies by offering rich differentiable information that facilitates\ngradient-based methods. In comparison to prevailing model-free reinforcement\nlearning approaches, model-based reinforcement learning (MBRL) methods exhibit\nthe potential to effectively harness the power of differentiable information\nfor recovering the underlying physical dynamics. However, this presents two\nprimary challenges: effectively utilizing differentiable information to 1)\nconstruct models with more accurate dynamic prediction and 2) enhance the\nstability of policy training. In this paper, we propose a Differentiable\nInformation Enhanced MBRL method, MB-MIX, to address both challenges. Firstly,\nwe adopt a Sobolev model training approach that penalizes incorrect model\ngradient outputs, enhancing prediction accuracy and yielding more precise\nmodels that faithfully capture system dynamics. Secondly, we introduce mixing\nlengths of truncated learning windows to reduce the variance in policy gradient\nestimation, resulting in improved stability during policy learning. To validate\nthe effectiveness of our approach in differentiable environments, we provide\ntheoretical analysis and empirical results. Notably, our approach outperforms\nprevious model-based and model-free methods, in multiple challenging tasks\ninvolving controllable rigid robots such as humanoid robots' motion control and\ndeformable object manipulation.",
        "Superradiance, the phenomenon enabling energy extraction through radiation\namplification, is not universal to all black holes. We show that semi-classical\nbackreaction can induce superradiance, even when absent at the classical level.\nSpecifically, we compute the quasinormal modes of a massless scalar field\nprobing a family of rotating `quantum' black holes in three-dimensional anti-de\nSitter space, accounting for all orders of backreaction due to quantum\nconformal matter. A subset of these modes is identified as superradiant,\nleading to the formation of quantum black hole `bombs'. All such quantum black\nholes have curvature singularities shrouded by horizons. Thus, while\nbackreaction enforces cosmic censorship, it also renders the black holes\ndynamically unstable. Further, we find all thermally unstable black holes are\ndynamically unstable, though the converse does not hold generally. Our findings\nthus suggest a semiclassical version of the Gubser-Mitra conjecture on black\nhole stability. This motivates us to propose a stability criterion for quantum\nblack holes.",
        "Motor imitation impairments are commonly reported in individuals with autism\nspectrum conditions (ASCs), suggesting that motor imitation could be used as a\nphenotype for addressing autism heterogeneity. Traditional methods for\nassessing motor imitation are subjective, labor-intensive, and require\nextensive human training. Modern Computerized Assessment of Motor Imitation\n(CAMI) methods, such as CAMI-3D for motion capture data and CAMI-2D for video\ndata, are less subjective. However, they rely on labor-intensive data\nnormalization and cleaning techniques, and human annotations for algorithm\ntraining. To address these challenges, we propose CAMI-2DNet, a scalable and\ninterpretable deep learning-based approach to motor imitation assessment in\nvideo data, which eliminates the need for data normalization, cleaning and\nannotation. CAMI-2DNet uses an encoder-decoder architecture to map a video to a\nmotion encoding that is disentangled from nuisance factors such as body shape\nand camera views. To learn a disentangled representation, we employ synthetic\ndata generated by motion retargeting of virtual characters through the\nreshuffling of motion, body shape, and camera views, as well as real\nparticipant data. To automatically assess how well an individual imitates an\nactor, we compute a similarity score between their motion encodings, and use it\nto discriminate individuals with ASCs from neurotypical (NT) individuals. Our\ncomparative analysis demonstrates that CAMI-2DNet has a strong correlation with\nhuman scores while outperforming CAMI-2D in discriminating ASC vs NT children.\nMoreover, CAMI-2DNet performs comparably to CAMI-3D while offering greater\npracticality by operating directly on video data and without the need for\nad-hoc data normalization and human annotations.",
        "Social media is central to activists, who use it internally for coordination\nand externally to reach supporters and the public. To date, the HCI community\nhas not explored activists' perspectives on future social media platforms. In\ninterviews with 14 activists from an environmental and a queer-feminist\nmovement in Germany, we identify activists' needs and feature requests for\nfuture social media platforms. The key finding is that on- and offline safety\nis their main need. Based on this, we make concrete proposals to improve safety\nmeasures. Increased control over content presentation and tools to streamline\nactivist workflows are also central to activists. We make concrete design and\nresearch recommendations on how social media platforms and the HCI community\ncan contribute to improved safety and content presentation, and how activists\nthemselves can reduce their workload.",
        "Out-of-distribution (OOD) detection remains challenging for deep learning\nmodels, particularly when test-time OOD samples differ significantly from\ntraining outliers. We propose OODD, a novel test-time OOD detection method that\ndynamically maintains and updates an OOD dictionary without fine-tuning. Our\napproach leverages a priority queue-based dictionary that accumulates\nrepresentative OOD features during testing, combined with an informative inlier\nsampling strategy for in-distribution (ID) samples. To ensure stable\nperformance during early testing, we propose a dual OOD stabilization mechanism\nthat leverages strategically generated outliers derived from ID data. To our\nbest knowledge, extensive experiments on the OpenOOD benchmark demonstrate that\nOODD significantly outperforms existing methods, achieving a 26.0% improvement\nin FPR95 on CIFAR-100 Far OOD detection compared to the state-of-the-art\napproach. Furthermore, we present an optimized variant of the KNN-based OOD\ndetection framework that achieves a 3x speedup while maintaining detection\nperformance.",
        "Solar microflares are ubiquitous in the solar corona, yet their driving\nmechanisms remain a subject of ongoing debate. Using high-resolution coronal\nobservations from the Solar Orbiter's Extreme Ultraviolet Imager (EUI), we\nidentified about a dozen distinct moving plasma structures (hereafter, `` tiny\nejections'') originating from the centers of three homologous microflares out\nof four successive events. These tiny ejections propagate roughly perpendicular\nto the flaring loops. They often originate as dot-like structures with a length\nscale of approximately $10^{3}$ km. While these initial dot-like shapes are\nobservable in EUI images, they remain undetectable in the images captured by\nthe Atmospheric Imaging Assembly onboard the Solar Dynamics Observatory. As\nthey propagate, these dot-like structures consistently evolve into loop-like\nformations, possibly due to the heating of the surrounding magnetic field.\nRather than being generated by a series of flux rope eruptions, the tiny\nejections appear to result from small-angle magnetic reconnections within a\nbipolar field. Thus, the microflares associated with these ejections may be\ndriven by magnetic reconnection within braided fields, a process similar to the\nproposed nanoflare mechanism and distinct from the standard large-scale flare\nmodel.",
        "Wind and solar generation constitute an increasing share of electricity\nsupply globally. We find that this leads to shifts in the operational dynamics\nof thermal power plants. Using fixed effects panel regression across seven\nmajor U.S. balancing authorities, we analyze the impact of renewable generation\non coal, natural gas combined cycle plants, and natural gas combustion\nturbines. Wind generation consistently displaces thermal output, while effects\nfrom solar vary significantly by region, achieving substantial displacement in\nareas with high solar penetration such as the California Independent System\nOperator but limited impacts in coal reliant grids such as the Midcontinent\nIndependent System Operator. Renewable energy sources effectively reduce carbon\ndioxide emissions in regions with flexible thermal plants, achieving\ndisplacement effectiveness as high as one hundred and two percent in the\nCalifornia Independent System Operator and the Electric Reliability Council of\nTexas. However, in coal heavy areas such as the Midcontinent Independent System\nOperator and the Pennsylvania New Jersey Maryland Interconnection,\ninefficiencies from ramping and cycling reduce carbon dioxide displacement to\nas low as seventeen percent and often lead to elevated nitrogen oxides and\nsulfur dioxide emissions. These findings underscore the critical role of grid\ndesign, fuel mix, and operational flexibility in shaping the emissions benefits\nof renewables. Targeted interventions, including retrofitting high emitting\nplants and deploying energy storage, are essential to maximize emissions\nreductions and support the decarbonization of electricity systems.",
        "We study the NP-hard Stable Hypergraph Matching (SHM) problem and its\ngeneralization allowing capacities, the Stable Hypergraph $b$-Matching (SH$b$M)\nproblem, and investigate their computational properties under various\nstructural constraints. Our study is motivated by the fact that Scarf's Lemma\n(Scarf, 1967) together with a result of Lov\\'asz (1972) guarantees the\nexistence of a stable matching whenever the underlying hypergraph is normal.\nFurthermore, if the hypergraph is unimodular (i.e., its incidence matrix is\ntotally unimodular), then even a stable $b$-matching is guaranteed to exist.\nHowever, no polynomial-time algorithm is known for finding a stable matching or\n$b$-matching in unimodular hypergraphs.\n  We identify subclasses of unimodular hypergraphs where SHM and SH$b$M are\ntractable such as laminar hypergraphs or so-called subpath hypergraphs with\nbounded-size hyperedges; for the latter case, even a maximum-weight stable\n$b$-matching can be found efficiently. We complement our algorithms by showing\nthat optimizing over stable matchings is NP-hard even in laminar hypergraphs.\nAs a practically important special case of SH$b$M for unimodular hypergraphs,\nwe investigate a tripartite stable matching problem with students, schools, and\ncompanies as agents, called the University Dual Admission problem, which models\nreal-world scenarios in higher education admissions.\n  Finally, we examine a superclass of subpath hypergraphs that are normal but\nnecessarily not unimodular, namely subtree hypergraphs where hyperedges\ncorrespond to subtrees of a tree. We establish that for such hypergraphs,\nstable matchings can be found in polynomial time but, in the setting with\ncapacities, finding a stable $b$-matching is NP-hard.",
        "In the last 175 years, the physical understanding of\n  nature has seen a revolutionary change. Until about 1850, Newton's\n  theory and the mechanical world view derived from it provided the\n  dominant view of the physical world, later supplemented by Maxwell's\n  theory of the electromagnetic field. That approach was entirely\n  deterministic and free of probabilistic concepts. In contrast to\n  that conceptual edifice, today many fields of physics are governed\n  by probabilistic concepts. Statistical mechanics in its classical or\n  quantum version and random-matrix theory are obvious\n  examples. Quantum mechanics is an intrinsically statistical\n  theory. Classical chaos and its quantum manifestations also require\n  a stochastic approach. The paper describes how a combination of\n  discoveries and conceptual problems undermined the mechanical world\n  view, led to novel concepts, and shaped the modern understanding of\n  physics.",
        "Differential Privacy (DP) for text has recently taken the form of text\nparaphrasing using language models and temperature sampling to better balance\nprivacy and utility. However, the geometric distortion of DP regarding the\nstructure and complexity in the representation space remains unexplored. By\nestimating the intrinsic dimension of paraphrased text across varying privacy\nbudgets, we find that word-level methods severely raise the representation\nmanifold, while sentence-level methods produce paraphrases whose manifolds are\ntopologically more consistent with human-written paraphrases. Among\nsentence-level methods, masked paraphrasing, compared to causal paraphrasing,\ndemonstrates superior preservation of structural complexity, suggesting that\nautoregressive generation propagates distortions from unnatural word choices\nthat cascade and inflate the representation space.",
        "Unsafe prompts pose significant safety risks to large language models (LLMs).\nExisting methods for detecting unsafe prompts rely on data-driven fine-tuning\nto train guardrail models, necessitating significant data and computational\nresources. In contrast, recent few-shot gradient-based methods emerge,\nrequiring only few safe and unsafe reference prompts. A gradient-based approach\nidentifies unsafe prompts by analyzing consistent patterns of the gradients of\nsafety-critical parameters in LLMs. Although effective, its restriction to\ndirectional similarity (cosine similarity) introduces ``directional bias'',\nlimiting its capability to identify unsafe prompts. To overcome this\nlimitation, we introduce GradCoo, a novel gradient co-occurrence analysis\nmethod that expands the scope of safety-critical parameter identification to\ninclude unsigned gradient similarity, thereby reducing the impact of\n``directional bias'' and enhancing the accuracy of unsafe prompt detection.\nComprehensive experiments on the widely-used benchmark datasets ToxicChat and\nXStest demonstrate that our proposed method can achieve state-of-the-art (SOTA)\nperformance compared to existing methods. Moreover, we confirm the\ngeneralizability of GradCoo in detecting unsafe prompts across a range of LLM\nbase models with various sizes and origins.",
        "Ghost imaging via sparsity constraints (GISC) spectral camera modulates the\nthree-dimensional (3D) hyperspectral image into a two-dimensional (2D)\ncompressive image with speckles in a single shot. It obtains a 3D hyperspectral\nimage (HSI) by reconstruction algorithms. The rapid development of deep\nlearning has provided a new method for 3D HSI reconstruction. Moreover, the\nimaging performance of the GISC spectral camera can be improved by optimizing\nthe speckle modulation. In this paper, we propose an end-to-end GISCnet with\nsuper-Rayleigh speckle modulation to improve the imaging quality of the GISC\nspectral camera. The structure of GISCnet is very simple but effective, and we\ncan easily adjust the network structure parameters to improve the image\nreconstruction quality. Relative to Rayleigh speckles, our super-Rayleigh\nspeckles modulation exhibits a wealth of detail in reconstructing 3D HSIs.\nAfter evaluating 648 3D HSIs, it was found that the average peak\nsignal-to-noise ratio increased from 27 dB to 31 dB. Overall, the proposed\nGISCnet with super-Rayleigh speckle modulation can effectively improve the\nimaging quality of the GISC spectral camera by taking advantage of both\noptimized super-Rayleigh modulation and deep-learning image reconstruction,\ninspiring joint optimization of light-field modulation and image reconstruction\nto improve ghost imaging performance.",
        "Nonrigid registration is vital to medical image analysis but remains\nchallenging for diffusion MRI (dMRI) due to its high-dimensional,\norientation-dependent nature. While classical methods are accurate, they are\ncomputationally demanding, and deep neural networks, though efficient, have\nbeen underexplored for nonrigid dMRI registration compared to structural\nimaging. We present a novel, deep learning framework for model-free, nonrigid\nregistration of raw diffusion MRI data that does not require explicit\nreorientation. Unlike previous methods relying on derived representations such\nas diffusion tensors or fiber orientation distribution functions, in our\napproach, we formulate the registration as an equivariant diffeomorphism of\nposition-and-orientation space. Central to our method is an\n$\\mathsf{SE}(3)$-equivariant UNet that generates velocity fields while\npreserving the geometric properties of a raw dMRI's domain. We introduce a new\nloss function based on the maximum mean discrepancy in Fourier space,\nimplicitly matching ensemble average propagators across images. Experimental\nresults on Human Connectome Project dMRI data demonstrate competitive\nperformance compared to state-of-the-art approaches, with the added advantage\nof bypassing the overhead for estimating derived representations. This work\nestablishes a foundation for data-driven, geometry-aware dMRI registration\ndirectly in the acquisition space.",
        "Variational mode decomposition (VMD) and its extensions like Multivariate VMD\n(MVMD) decompose signals into ensembles of band-limited modes with narrow\ncentral frequencies. These methods utilize Fourier transformations to shift\nsignals between time and frequency domains. However, since Fourier\ntransformations span the entire time-domain signal, they are suboptimal for\nnon-stationary time series.\n  We introduce Short-Time Variational Mode Decomposition (STVMD), an innovative\nextension of the VMD algorithm that incorporates the Short-Time Fourier\ntransform (STFT) to minimize the impact of local disturbances. STVMD segments\nsignals into short time windows, converting these segments into the frequency\ndomain. It then formulates a variational optimization problem to extract\nband-limited modes representing the windowed data. The optimization aims to\nminimize the sum of the bandwidths of these modes across the windowed data,\nextending the cost functions used in VMD and MVMD. Solutions are derived using\nthe alternating direction method of multipliers, ensuring the extraction of\nmodes with narrow bandwidths.\n  STVMD is divided into dynamic and non-dynamic types, depending on whether the\ncentral frequencies vary with time. Our experiments show that non-dynamic STVMD\nis comparable to VMD with properly sized time windows, while dynamic STVMD\nbetter accommodates non-stationary signals, evidenced by reduced mode function\nerrors and tracking of dynamic central frequencies. This effectiveness is\nvalidated by steady-state visual-evoked potentials in electroencephalogram\nsignals.",
        "We explore the dynamical evolution of spherically symmetric objects made of\nelectrically counterpoised dust in general relativity. It has been claimed that\nthese objects are in neutral equilibrium and, therefore, that black hole\nmimickers made of electrically counterpoised dust are feasible. Here we show\nthat if a velocity is imparted to the fluid elements, no matter how small, the\nevolution leads either to a black hole or to the dispersion of the fluid.\nFurthermore, in the case of collapse, the resulting object is necessarily an\nextremal Reissner-Nordstr\\\"om black hole.",
        "This study represents a first attempt to build a backcasting methodology to\nidentify the optimal policy roadmaps in transport systems. In this methodology,\ndesired objectives are set by decision makers at a given time horizon, and then\nthe optimal combinations of policies to achieve these objectives are computed\nas a function of time (i.e., ``backcasted''). This approach is illustrated on\nthe transportation sector by considering a specific subsystem with a single\npolicy decision. The subsystem describes the evolution of the passenger car\nfleet within a given region and its impact on greenhouse gas emissions. The\noptimized policy is a monetary incentive for the purchase of electric vehicles\nwhile minimizing the total budget of the state and achieving a desired CO$_2$\ntarget. A case study applied to Metropolitan France is presented to illustrate\nthe approach. Additionally, alternative policy scenarios are also analyzed to\nprovide further insights.",
        "In recent years, a variety of methods based on Transformer and state space\nmodel (SSM) architectures have been proposed, advancing foundational DNA\nlanguage models. However, there is a lack of comparison between these recent\napproaches and the classical architecture convolutional networks (CNNs) on\nfoundation model benchmarks. This raises the question: are CNNs truly being\nsurpassed by these recent approaches based on transformer and SSM\narchitectures? In this paper, we develop a simple but well-designed CNN-based\nmethod termed ConvNova. ConvNova identifies and proposes three effective\ndesigns: 1) dilated convolutions, 2) gated convolutions, and 3) a dual-branch\nframework for gating mechanisms. Through extensive empirical experiments, we\ndemonstrate that ConvNova significantly outperforms recent methods on more than\nhalf of the tasks across several foundation model benchmarks. For example, in\nhistone-related tasks, ConvNova exceeds the second-best method by an average of\n5.8%, while generally utilizing fewer parameters and enabling faster\ncomputation. In addition, the experiments observed findings that may be related\nto biological characteristics. This indicates that CNNs are still a strong\ncompetitor compared to Transformers and SSMs. We anticipate that this work will\nspark renewed interest in CNN-based methods for DNA foundation models.",
        "In the context of the description of how the vortical motion, produced in\nperipheral heavy-ion collisions, is transferred to the spin of hadrons, we\ncompute the $\\sigma$-meson propagator at finite temperature and baryon density.\nThis propagator encodes the properties of a medium consisting mainly of\nnucleons{, and can be used to model the main interactions between hadrons} in\nthe corona region of the reaction. We compute the one-loop $\\sigma$ self-energy\nin an approximation that accounts for the large nucleon mass. From the real\npart of the self-energy, we find the dispersion relation and show that the\n$\\sigma$-mass receives a non-negligible thermal and baryon chemical dependent\n{contribution}. From the imaginary part, we also compute the spectral density,\nwhich we show to contain a piece coming from the branch cut associated with\nLandau damping. We also present approximations for the dispersion relation and\nthe residue at the pole in the small- and large-momentum regimes and complement\nthe calculation, providing the sum rules satisfied by the propagator. This\nstudy aims to determine one of the elements needed to compute how the vortical\nmotion in the corona region of the reaction is transferred to the spin of\n$\\Lambda$ hyperons that can interact with nucleons by $\\sigma$-meson exchange.",
        "In complex multi-agent systems involving heterogeneous teams, uncertainty\narises from numerous sources like environmental disturbances, model\ninaccuracies, and changing tasks. This causes planned trajectories to become\ninfeasible, requiring replanning. Further, different communication\narchitectures used in multi-agent systems give rise to asymmetric knowledge of\nplanned trajectories across the agents. In such systems, replanning must be\ndone in a communication-aware fashion. This paper establishes the conditions\nfor synchronization and feasibility in epistemic planning scenarios introduced\nby opportunistic communication architectures. We also establish conditions on\ntask satisfaction based on quantified recoverability of disturbances in an\niterative planning scheme. We further validate these theoretical results\nexperimentally in a UAV--UGV task assignment problem.",
        "Gaussian Splatting (GS) has become one of the most important neural rendering\nalgorithms. GS represents 3D scenes using Gaussian components with trainable\ncolor and opacity. This representation achieves high-quality renderings with\nfast inference. Regrettably, it is challenging to integrate such a solution\nwith varying light conditions, including shadows and light reflections, manual\nadjustments, and a physical engine. Recently, a few approaches have appeared\nthat incorporate ray-tracing or mesh primitives into GS to address some of\nthese caveats. However, no such solution can simultaneously solve all the\nexisting limitations of the classical GS. Consequently, we introduce\nREdiSplats, which employs ray tracing and a mesh-based representation of flat\n3D Gaussians. In practice, we model the scene using flat Gaussian distributions\nparameterized by the mesh. We can leverage fast ray tracing and control\nGaussian modification by adjusting the mesh vertices. Moreover, REdiSplats\nallows modeling of light conditions, manual adjustments, and physical\nsimulation. Furthermore, we can render our models using 3D tools such as\nBlender or Nvdiffrast, which opens the possibility of integrating them with all\nexisting 3D graphics techniques dedicated to mesh representations.",
        "Multi-robot autonomous exploration in an unknown environment is an important\napplication in robotics.Traditional exploration methods only use information\naround frontier points or viewpoints, ignoring spatial information of unknown\nareas. Moreover, finding the exact optimal solution for multi-robot task\nallocation is NP-hard, resulting in significant computational time consumption.\nTo address these issues, we present a hierarchical multi-robot exploration\nframework using a new modeling method called RegionGraph. The proposed approach\nmakes two main contributions: 1) A new modeling method for unexplored areas\nthat preserves their spatial information across the entire space in a weighted\ngraph called RegionGraph. 2) A hierarchical multi-robot exploration framework\nthat decomposes the global exploration task into smaller subtasks, reducing the\nfrequency of global planning and enabling asynchronous exploration. The\nproposed method is validated through both simulation and real-world\nexperiments, demonstrating a 20% improvement in efficiency compared to existing\nmethods.",
        "A recent breakthrough of Behnezhad and Ghafari [FOCS 2024] and subsequent\nwork of Assadi, Khanna, and Kiss [SODA 2025] gave algorithms for the fully\ndynamic $(1-\\varepsilon)$-approximate maximum matching problem whose runtimes\nare determined by a purely combinatorial quantity: the maximum density of\nOrdered Ruzsa-Szemer\\'edi (ORS) graphs. We say a graph $G$ is an $(r,t)$-ORS\ngraph if its edges can be partitioned into $t$ matchings $M_1,M_2, \\ldots, M_t$\neach of size $r$, such that for every $i$, $M_i$ is an induced matching in the\nsubgraph $M_{i} \\cup M_{i+1} \\cup \\cdots \\cup M_t$. This is a relaxation of the\nextensively-studied notion of a Ruzsa-Szemer\\'edi (RS) graph, the difference\nbeing that in an RS graph each $M_i$ must be an induced matching in $G$.\n  In this note, we show that these two notions are roughly equivalent.\nSpecifically, let $\\mathrm{ORS}(n)$ be the largest $t$ such that there exists\nan $n$-vertex ORS-$(\\Omega(n), t)$ graph, and define $\\mathrm{RS}(n)$\nanalogously. We show that if $\\mathrm{ORS}(n) \\ge \\Omega(n^c)$, then for any\nfixed $\\delta > 0$, $\\mathrm{RS}(n) \\ge \\Omega(n^{c(1-\\delta)})$. This resolves\na question of Behnezhad and Ghafari.",
        "Learned Image Compression (LIC) has explored various architectures, such as\nConvolutional Neural Networks (CNNs) and transformers, in modeling image\ncontent distributions in order to achieve compression effectiveness. However,\nachieving high rate-distortion performance while maintaining low computational\ncomplexity (\\ie, parameters, FLOPs, and latency) remains challenging. In this\npaper, we propose a hybrid Convolution and State Space Models (SSMs) based\nimage compression framework, termed \\textit{CMamba}, to achieve superior\nrate-distortion performance with low computational complexity. Specifically,\nCMamba introduces two key components: a Content-Adaptive SSM (CA-SSM) module\nand a Context-Aware Entropy (CAE) module. First, we observed that SSMs excel in\nmodeling overall content but tend to lose high-frequency details. In contrast,\nCNNs are proficient at capturing local details. Motivated by this, we propose\nthe CA-SSM module that can dynamically fuse global content extracted by SSM\nblocks and local details captured by CNN blocks in both encoding and decoding\nstages. As a result, important image content is well preserved during\ncompression. Second, our proposed CAE module is designed to reduce spatial and\nchannel redundancies in latent representations after encoding. Specifically,\nour CAE leverages SSMs to parameterize the spatial content in latent\nrepresentations. Benefiting from SSMs, CAE significantly improves spatial\ncompression efficiency while reducing spatial content redundancies. Moreover,\nalong the channel dimension, CAE reduces inter-channel redundancies of latent\nrepresentations via an autoregressive manner, which can fully exploit prior\nknowledge from previous channels without sacrificing efficiency. Experimental\nresults demonstrate that CMamba achieves superior rate-distortion performance.",
        "In this paper, we develop a regularized higher-order Taylor based method for\nsolving composite (e.g., nonlinear least-squares) problems. At each iteration,\nwe replace each smooth component of the objective function by a higher-order\nTaylor approximation with an appropriate regularization, leading to a\nregularized higher-order Taylor approximation (RHOTA) algorithm. We derive\nglobal convergence guarantees for RHOTA algorithm. In particular, we prove\nstationary point convergence guarantees for the iterates generated by RHOTA,\nand leveraging a Kurdyka-{\\L}ojasiewicz (KL) type property of the objective\nfunction, we derive improved rates depending on the KL parameter. When the\nTaylor approximation is of order $2$, we present an efficient implementation of\nRHOTA algorithm, demonstrating that the resulting nonconvex subproblem can be\neffectively solved utilizing standard convex programming tools. Furthermore, we\nextend the scope of our investigation to include the behavior and efficacy of\nRHOTA algorithm in handling systems of nonlinear equations and optimization\nproblems with nonlinear equality constraints deriving new rates under improved\nconstraint qualifications conditions. Finally, we consider solving the phase\nretrieval problem with a higher-order proximal point algorithm, showcasing its\nrapid convergence rate for this particular application. Numerical simulations\non phase retrieval and output feedback control problems also demonstrate the\nefficacy and performance of the proposed methods when compared to some\nstate-of-the-art optimization methods and software.",
        "The Rasch model has been widely used to analyse item response data in\npsychometrics and educational assessments. When the number of individuals and\nitems are large, it may be impractical to provide all possible responses. It is\ndesirable to study sparse item response experiments. Here, we propose to use\nthe Erd\\H{o}s\\textendash R\\'enyi random sampling design, where an individual\nresponds to an item with low probability $p$. We prove the uniform consistency\nof the maximum likelihood estimator %by developing a leave-one-out method for\nthe Rasch model when both the number of individuals, $r$, and the number of\nitems, $t$, approach infinity. Sampling probability $p$ can be as small as\n$\\max\\{\\log r\/r, \\log t\/t\\}$ up to a constant factor, which is a fundamental\nrequirement to guarantee the connection of the sampling graph by the theory of\nthe Erd\\H{o}s\\textendash R\\'enyi graph. The key technique behind this\nsignificant advancement is a powerful leave-one-out method for the Rasch model.\nWe further establish the asymptotical normality of the MLE by using a simple\nmatrix to approximate the inverse of the Fisher information matrix. The\ntheoretical results are corroborated by simulation studies and an analysis of a\nlarge item-response dataset.",
        "Human Activity Recognition (HAR) has gained significant importance with the\ngrowing use of sensor-equipped devices and large datasets. This paper evaluates\nthe performance of three categories of models : classical machine learning,\ndeep learning architectures, and Restricted Boltzmann Machines (RBMs) using\nfive key benchmark datasets of HAR (UCI-HAR, OPPORTUNITY, PAMAP2, WISDM, and\nBerkeley MHAD). We assess various models, including Decision Trees, Random\nForests, Convolutional Neural Networks (CNN), and Deep Belief Networks (DBNs),\nusing metrics such as accuracy, precision, recall, and F1-score for a\ncomprehensive comparison. The results show that CNN models offer superior\nperformance across all datasets, especially on the Berkeley MHAD. Classical\nmodels like Random Forest do well on smaller datasets but face challenges with\nlarger, more complex data. RBM-based models also show notable potential,\nparticularly for feature learning. This paper offers a detailed comparison to\nhelp researchers choose the most suitable model for HAR tasks."
      ]
    }
  },
  {
    "id":2411.11513,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Rethinking the Inception Architecture for Computer Vision",
    "start_abstract":"Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety tasks. Since 2014 very deep convolutional started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend translate immediate quality tasks (as long as enough labeled data is provided training), efficiency low parameter count still enabling factors use cases such mobile big-data scenarios. Here we exploring ways scale up that aim utilizing added computation efficiently possible by suitably factorized convolutions aggressive regularization. We benchmark our methods on ILSVRC 2012 classification challenge validation set demonstrate over art: 21:2% top-1 5:6% top-5 error single frame evaluation using network with 5 billion multiply-adds per inference less than 25 million parameters. With an ensemble 4 models multi-crop evaluation, report 3:5% 17:3% 3:6% official test set.",
    "start_categories":[
      "BioInformatics"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "The Genome Analysis Toolkit: A MapReduce framework for analyzing next-generation DNA sequencing data"
      ],
      "abstract":[
        "Next-generation DNA sequencing (NGS) projects, such as the 1000 Genomes Project, are already revolutionizing our understanding of genetic variation among individuals. However, massive data sets generated by NGS\u2014the Genome pilot alone includes nearly five terabases\u2014make writing feature-rich, efficient, and robust analysis tools difficult for even computationally sophisticated Indeed, many professionals limited in scope ease with which they can answer scientific questions complexity accessing manipulating produced these machines. Here, we discuss Analysis Toolkit (GATK), a structured programming framework designed to development efficient next-generation sequencers using functional philosophy MapReduce. The GATK provides small but rich set access patterns that encompass majority tool needs. Separating specific calculations from common management infrastructure enables us optimize correctness, stability, CPU memory efficiency enable distributed shared parallelization. We highlight capabilities describing implementation application robust, scale-tolerant like coverage calculators single nucleotide polymorphism (SNP) calling. conclude developers analysts quickly easily write NGS tools, have been incorporated into large-scale projects Project Cancer Atlas."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Canonical Energy-Momentum Tensor of Abelian Fields",
        "Split Gibbs Discrete Diffusion Posterior Sampling",
        "Toward Generalized Image Quality Assessment: Relaxing the Perfect\n  Reference Quality Assumption",
        "A note on multiplicative roots of multivariable formal power series",
        "On total transitivity of graphs",
        "Effects of non-parallelism on standard and magnetorheological\n  measurements",
        "It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate\n  Mutually via Selective Rationale Optimisation",
        "VidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human\n  Videos for Zero-Shot Robotic Manipulation",
        "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving",
        "Measurement of the Quantum Efficiency of Electrode Materials for VUV\n  Photons in Liquid Xenon",
        "General Coded Computing: Adversarial Settings",
        "Status and prospect of weak radiative hyperon decays",
        "LONGCODEU: Benchmarking Long-Context Language Models on Long Code\n  Understanding",
        "Higher Axion Strings",
        "MultiResolution Low-Rank Regularization of Dynamic Imaging Problems",
        "Multi-column Compton Camera of stacked Si pixel sensors for sub-degree\n  angular resolution",
        "Self-similar Features in Sub-secondary Breakup of a Droplet and Ligament\n  Mediated Fragmentation under Extreme Conditions",
        "Statistical Distributions for Transient Transport",
        "A Thermodynamic Theory of Proximity Ferroelectricity",
        "Federated Learning for Diffusion Models",
        "Layer-Resolved Quantum Transport in Twisted Bilayer Graphene:\n  Counterflow and Machine Learning Predictions",
        "Separation Axioms Among US",
        "Dominance Regions of Pursuit-evasion Games in Non-anticipative\n  Information Patterns",
        "Rate distortion dimension and ergodic decomposition for\n  $\\mathbb{R}^d$-actions",
        "The Fertile Steppe: Computability Logic and the decidability of one of\n  its fragments",
        "Multivariate spatial models for small area estimation of\n  species-specific forest inventory parameters",
        "LLM Embeddings for Deep Learning on Tabular Data",
        "Energy-efficient Merging of Connected and Automated Vehicles using\n  Control Barrier Functions",
        "Moderate deviations in first-passage percolation for bounded weights"
      ],
      "abstract":[
        "In this tutorial, we provide the natural derivation of symmetrical,\ngauge-invariant canonical energy-momentum tensor for the abelian gauge field,\ni.e., the electromagnetic field.",
        "We study the problem of posterior sampling in discrete-state spaces using\ndiscrete diffusion models. While posterior sampling methods for continuous\ndiffusion models have achieved remarkable progress, analogous methods for\ndiscrete diffusion models remain challenging. In this work, we introduce a\nprincipled plug-and-play discrete diffusion posterior sampling algorithm based\non split Gibbs sampling, which we call SG-DPS. Our algorithm enables\nreward-guided generation and solving inverse problems in discrete-state spaces.\nWe demonstrate that SG-DPS converges to the true posterior distribution on\nsynthetic benchmarks, and enjoys state-of-the-art posterior sampling\nperformance on a range of benchmarks for discrete data, achieving up to 2x\nimproved performance compared to existing baselines.",
        "Full-reference image quality assessment (FR-IQA) generally assumes that\nreference images are of perfect quality. However, this assumption is flawed due\nto the sensor and optical limitations of modern imaging systems. Moreover,\nrecent generative enhancement methods are capable of producing images of higher\nquality than their original. All of these challenge the effectiveness and\napplicability of current FR-IQA models. To relax the assumption of perfect\nreference image quality, we build a large-scale IQA database, namely DiffIQA,\ncontaining approximately 180,000 images generated by a diffusion-based image\nenhancer with adjustable hyper-parameters. Each image is annotated by human\nsubjects as either worse, similar, or better quality compared to its reference.\nBuilding on this, we present a generalized FR-IQA model, namely Adaptive\nFidelity-Naturalness Evaluator (A-FINE), to accurately assess and adaptively\ncombine the fidelity and naturalness of a test image. A-FINE aligns well with\nstandard FR-IQA when the reference image is much more natural than the test\nimage. We demonstrate by extensive experiments that A-FINE surpasses standard\nFR-IQA models on well-established IQA datasets and our newly created DiffIQA.\nTo further validate A-FINE, we additionally construct a super-resolution IQA\nbenchmark (SRIQA-Bench), encompassing test images derived from ten\nstate-of-the-art SR methods with reliable human quality annotations. Tests on\nSRIQA-Bench re-affirm the advantages of A-FINE. The code and dataset are\navailable at https:\/\/tianhewu.github.io\/A-FINE-page.github.io\/.",
        "Suppose that we are given a formal power series of many variables with\ncoefficients in $\\mathbb{R}$ (or $\\mathbb{C}$) and we want to compute its\n$n$-th (multiplicative) root. As can be expected coefficients of the root have\nto satisfy a system of infinitely many equations. We present such a system of\nequations that strictly corresponds with the system for $n$-th of a formal\npower series of one variable. With help of an example we show that the case of\nformal power series of many variables is very different from the one variable\ncase with respect to the existence of roots.",
        "Let $G=(V, E)$ be a graph where $V$ and $E$ are the vertex and edge sets,\nrespectively. For two disjoint subsets $A$ and $B$ of $V$, we say $A$\n\\emph{dominates} $B$ if every vertex of $B$ is adjacent to at least one vertex\nof $A$. A vertex partition $\\pi = \\{V_1, V_2, \\ldots, V_k\\}$ of $G$ is called a\n\\emph{transitive partition} of size $k$ if $V_i$ dominates $V_j$ for all $1\\leq\ni<j\\leq k$. In this article, we study a variation of transitive partition,\nnamely \\emph{total transitive partition}. The total transitivity $Tr_t(G)$ is\nequal to the maximum order of a vertex partition $\\pi = \\{V_1, V_2, \\ldots,\nV_k\\}$ of $G$ obtained by repeatedly removing a total dominating set from $G$,\nuntil no vertices remain. Thus, $V_1$ is a total dominating set of $G$, $V_2$\nis a total dominating set of the graph $G_1=G-V_1$, and, in general, $V_{i+1}$\nis a total dominating set in the graph $G_i=G-\\bigcup_{j=1}^i V_i$. A vertex\npartition of order $Tr_t(G)$ is called $Tr_t$-partition. The \\textsc{Maximum\nTotal Transitivity Problem} is to find a total transitive partition of a given\ngraph with the maximum number of parts. First, we characterize split graphs\nwith total transitivity equal to $1$ and $\\omega(G)-1$. Moreover, for the split\ngraph $G$ and $1\\leq p\\leq \\omega(G)-1$, we give some necessary conditions for\n$Tr_t(G)=p$. Furthermore, we show that the decision version of this problem is\nNP-complete for bipartite graphs. On the positive side, we design a\npolynomial-time algorithm to solve \\textsc{Maximum Total Transitivity Problem}\nin trees.",
        "Human blood has a complex composition and unique rheological properties,\nmaking it challenging to measure accurately. In addition to this, its\nmechanical properties may be influenced by external magnetic fields, which,\ndespite being a characteristic of significant interest in the development of\nnew treatment therapies, remains relatively unexplored. To achieve an accurate\nmagnetorheological description of blood, the employed equipment must achieve\naccurate results taking into account its low viscous and elastic character.\nHowever, low and inconsistent apparent-viscosity values were observed\nsystematically in a rotational rheometer equipped with a magnetorheological\ncell, without the applied magnetic field. In this work, a parametric study was\nconducted, experimentally and numerically, to evaluate this error source.\nSteady shear measurements were carried out with low-viscosity Newtonian fluids\nwith two geometries: a parallel-plate, at different gap heights, and a\ncone-plate. An additional standard bottom plate for non-magnetic testing was\nalso employed for comparison. The standard bottom plate returned constant\nviscosities near the expected values, whereas the plate attached to the\nmagnetorheological cell showed a clear decrease of measured viscosity with\nparallel-plate gap reduction and an increase in cone-plate-measured viscosity.\nNumerical results corroborated the experimental observations, pointing towards\nan inclination of the bottom magnetic plate which can significantly affect the\nflow. Additional experimental and numerical work was conducted to evaluate the\neffects of the setup imperfection on magnetorheological measurements, unveiling\nmagnetorheology's deep dependence on the geometric characteristics.",
        "Very large language models (LLMs) such as GPT-4 have shown the ability to\nhandle complex tasks by generating and self-refining step-by-step rationales.\nSmaller language models (SLMs), typically with < 13B parameters, have been\nimproved by using the data generated from very-large LMs through knowledge\ndistillation. However, various practical constraints such as API costs,\ncopyright, legal and ethical policies restrict using large (often opaque)\nmodels to train smaller models for commercial use. Limited success has been\nachieved at improving the ability of an SLM to explore the space of possible\nrationales and evaluate them by itself through self-deliberation. To address\nthis, we propose COALITION, a trainable framework that facilitates interaction\nbetween two variants of the same SLM and trains them to generate and refine\nrationales optimized for the end-task. The variants exhibit different behaviors\nto produce a set of diverse candidate rationales during the generation and\nrefinement steps. The model is then trained via Selective Rationale\nOptimization (SRO) to prefer generating rationale candidates that maximize the\nlikelihood of producing the ground-truth answer. During inference, COALITION\nemploys a controller to select the suitable variant for generating and refining\nthe rationales. On five different datasets covering mathematical problems,\ncommonsense reasoning, and natural language inference, COALITION outperforms\nseveral baselines by up to 5%. Our ablation studies reveal that\ncross-communication between the two variants performs better than using the\nsingle model to self-refine the rationales. We also demonstrate the\napplicability of COALITION for LMs of varying scales (4B to 14B parameters) and\nmodel families (Mistral, Llama, Qwen, Phi). We release the code for this work\nat https:\/\/github.com\/Sohanpatnaik106\/coalition.",
        "Future robots are envisioned as versatile systems capable of performing a\nvariety of household tasks. The big question remains, how can we bridge the\nembodiment gap while minimizing physical robot learning, which fundamentally\ndoes not scale well. We argue that learning from in-the-wild human videos\noffers a promising solution for robotic manipulation tasks, as vast amounts of\nrelevant data already exist on the internet. In this work, we present VidBot, a\nframework enabling zero-shot robotic manipulation using learned 3D affordance\nfrom in-the-wild monocular RGB-only human videos. VidBot leverages a pipeline\nto extract explicit representations from them, namely 3D hand trajectories from\nvideos, combining a depth foundation model with structure-from-motion\ntechniques to reconstruct temporally consistent, metric-scale 3D affordance\nrepresentations agnostic to embodiments. We introduce a coarse-to-fine\naffordance learning model that first identifies coarse actions from the pixel\nspace and then generates fine-grained interaction trajectories with a diffusion\nmodel, conditioned on coarse actions and guided by test-time constraints for\ncontext-aware interaction planning, enabling substantial generalization to\nnovel scenes and embodiments. Extensive experiments demonstrate the efficacy of\nVidBot, which significantly outperforms counterparts across 13 manipulation\ntasks in zero-shot settings and can be seamlessly deployed across robot systems\nin real-world environments. VidBot paves the way for leveraging everyday human\nvideos to make robot learning more scalable.",
        "We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. Despite using only supervised\nfine-tuning, our final prover significantly outperforms the previous best\nopen-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning.\nOn the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32),\nsurpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover\nsuccessfully solves 7 problems (Pass@512), ranking first on the leaderboard.\nFurthermore, it generates 29.7K formal proofs for Lean Workbook problems,\nnearly doubling the 15.7K produced by earlier works.",
        "Light dark matter searches using ionization signals in dual-phase liquid\nxenon (LXe) time projection chambers (TPCs) are limited by low-energy\nionization backgrounds, including those from the photoelectric effect on the\nelectrodes. To address this, we measured the quantum efficiency (QE) of various\nelectrode materials for vacuum ultraviolet (VUV) photons in LXe, including\nplatinum (Pt), stainless steel (SUS304), and magnesium fluoride\n(MgF$_{2}$)-coated aluminum (Al). Our results show that MgF$_{2}$-coated Al\nexhibits the lowest QE among the tested materials. The QE for VUV photons with\na mean wavelength of 179.5~nm was measured to be $(7.2 \\pm 2.3) \\times\n10^{-5}$, corresponding to a reduction by a factor of 4.4 compared to SUS304, a\ncommonly used electrode material in direct dark matter experiments with LXe.\nThese findings suggest that employing low-QE electrodes can help mitigate\nphotoelectric-induced backgrounds, potentially improving the sensitivity of LXe\nTPCs in light dark matter searches.",
        "Conventional coded computing frameworks are predominantly tailored for\nstructured computations, such as matrix multiplication and polynomial\nevaluation. Such tasks allow the reuse of tools and techniques from algebraic\ncoding theory to improve the reliability of distributed systems in the presence\nof stragglers and adversarial servers.\n  This paper lays the foundation for general coded computing, which extends the\napplicability of coded computing to handle a wide class of computations. In\naddition, it particularly addresses the challenging problem of managing\nadversarial servers. We demonstrate that, in the proposed scheme, for a system\nwith $N$ servers, where $\\mathcal{O}(N^a)$, $a \\in [0,1)$, are adversarial, the\nsupremum of the average approximation error over all adversarial strategies\ndecays at a rate of $N^{\\frac{6}{5}(a-1)}$, under minimal assumptions on the\ncomputing tasks. Furthermore, we show that within a general framework, the\nproposed scheme achieves optimal adversarial robustness, in terms of maximum\nnumber of adversarial servers it can tolerate. This marks a significant step\ntoward practical and reliable general coded computing. Implementation results\nfurther validate the effectiveness of the proposed method in handling various\ncomputations, including inference in deep neural networks.",
        "Weak radiative hyperon decays represent a rich interplay between weak\ninteractions and the internal structure of baryons, offering profound insights\ninto Quantum Chromodynamics and weak interactions. Recent experimental\nobservations, particularly from BESIII, have revealed deviations from\ntheoretical predictions. These deviations could signal new physics or the need\nfor refined theoretical models incorporating intermediate resonance effects.\nThis review discusses recent theoretical advancements and key experimental\nfindings, focusing on recent measurements from BESIII and their implications\nfor strong interactions and baryon structure.",
        "Current advanced long-context language models offer great potential for\nreal-world software engineering applications. However, progress in this\ncritical domain remains hampered by a fundamental limitation: the absence of a\nrigorous evaluation framework for long code understanding. To gap this\nobstacle, we propose a long code understanding benchmark LONGCODEU from four\naspects (8 tasks) to evaluate LCLMs' long code understanding ability required\nfor practical applications, including code unit perception, intra-code unit\nunderstanding, inter-code unit relation understanding, and long code\ndocumentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6\ngeneral models and 3 code models). Our experimental results reveal key\nlimitations in current LCLMs' capabilities for long code understanding.\nParticularly, the performance of LCLMs drops dramatically when the long code\nlength is greater than 32K, falling far short of their claimed 128K-1M context\nwindows. In the four aspects, inter-code unit relation understanding is the\nmost challenging for LCLMs. Our study provides valuable insights for optimizing\nLCLMs and driving advancements in software engineering.",
        "We study the minimal requirements to obtain axion strings for axions with\nexponentially good quality. These ingredients appear in theories where an axion\ncoming from a higher-form gauge field mixes with the phase of a complex scalar\nfield in a situation that resembles higher-groups. The resulting axion is\nperturbatively massless and inherits a high-quality shift symmetry from the\nglobal higher-form symmetry while being compatible with a post-inflationary\naxion scenario. Due to differences and resemblances with both,\nextra-dimensional and field theory axions, we call this field the higher axion.\nTo this end, we study a toy model on a 5-dimensional manifold with boundary.\nThe boundary hosts the complex scalar that provides axion strings through\nstandard mechanisms. In addition, we study how these scenarios may arise in\nheterotic string theory and type II string compactifications.",
        "MultiResolution Low-Rank decomposition is formulated for regularization of\ndynamic image sequences. The decomposition applies a local low-rank\ndecomposition on a sequence of discrete wavelet transforms. Its effective\nformulation as a regularization functional is discussed and numerically tested\nfor dynamic X-ray tomography in comparison to other low-rank methods. The\nresults suggest it is similar to traditional locally low-rank decomposition but\nproduces less severe block artifacts.",
        "The Compton camera is a sensitive imaging detector for soft gamma-rays.\nCompton Reconstruction can not only give imaging capability but also remove\nbackground events to achieve good sensitivity. However, the angular resolution\nis in principle limited to several degrees. In this paper, we propose a novel\nconcept of Compton camera incorporating shadow effects. We consider\nmulti-column Compton camera (MCCC), consisting of stacked Si pixel sensors.\nEach of columns is separated each other to create shadow effects. This design\nachieves an angular resolution of less than 1 degree within around 1 degree\nfrom the center of the field-of-view by just modifying a conventional\nSi-stacked Compton camera and keeping advantages (wide field-of-view and good\nsensitivity) of conventional Compton camera. Here we validated the concept of\nproposed Compton camera through Monte-Carlo simulation. MCCC with 1 m column\nheight, 0.5 mm pixel size, 100 layers, and 10 columns for the 1-D direction can\ndistinguish two sources separated by 0.1 degree with 0.6M Compton-reconstructed\nevents.",
        "Droplet formation is relevant in many applications spanning natural and\nartificial settings. Comprehending droplet aerobreakup or air-assisted\nsecondary atomization is challenging, especially in high-speed flow scenarios.\nThis entails multi-scale interface deformations with intricate wave dynamics\nthat conform to a non-linear cascade. In the present study, we look into\nshockwave-induced breakups and associated intermediate processes happening at\nsmaller spatiotemporal scales across the disintegrating droplet interface at\ndifferent Weber numbers ($We \\sim 10^3$). We observe the undulations to follow\nbreakup patterns that resemble a scaled-down version of a secondary atomization\nevent. These sub-secondary breakup processes end with corrugated ligaments that\ngenerate the final daughter droplets. The size distribution of these droplets\nis estimated using a Depth from Defocus (DFD) technique. These illustrate the\ntransient nature of aerobreakup, where the normalized statistics in subsequent\ntime periods and different $We$ are observed to follow a universal\ndistribution. This conforms to a gamma distribution where the associated fit\nparameters agree well with the coefficients determined from ligament shape\nfactors, corresponding to the limit associated with most extreme corrugations.\nScaling laws based on $We$ are deduced for the averaged statistics using a high\nenergy chaotic breakup mechanism. These observations reinforce the idea of a\nself-similar mechanism for catastrophic aerobreakup of a droplet.",
        "This paper introduces the use of statistical distributions based on transport\ndifferential equations for clear distinction of transport modes within\ntransient kinetic experiments. More specifically,novel techniques are developed\nfor the transient data obtained through the Temporal Analysis of Products (TAP)\nreactor. The methodology allows distinguishing between two domains of diffusion\ntransport in heterogeneous catalytic systems, i.e., Knudsen and non-Knudsen\ndiffusion, using statistical fingerprints, and finding the transition domain.\nTwo distribution parameters were obtained that directly result in coefficients\nthat correspond to the concentration and the rate of transport. Using a linear\nrelationship between the rate and concentration coefficients, Knudsen diffusion\nis revealed when the rate of transport is constant and non-Knudsen diffusion is\nconfirmed when the rate of transport coefficient is a function of the\nconcentration coefficient. As a result, accurate transport information is\nobtained while in the presence of instrument drift or noise while investigating\nhigher pressure pulse responses. As such, experiments where the influence of\ngas phase reactions can be more directly studied.",
        "Proximity ferroelectricity has recently been reported as a new design\nparadigm for inducing ferroelectricity, where a non-ferroelectric polar\nmaterial becomes a ferroelectric by interfacing with a thin ferroelectric\nlayer. Strongly polar materials, such as AlN and ZnO, which were previously\nunswitchable with an external field below their dielectric breakdown fields,\ncan now be switched with practical coercive fields when they are in intimate\nproximity to a switchable ferroelectric. Here, we develop a general\nLandau-Ginzburg theory of proximity ferroelectricity in multilayers of\nnon-ferroelectrics and ferroelectrics to analyze their switchability and\ncoercive fields. The theory predicts regimes of both \"proximity switching\"\nwhere the multilayers collectively switch, as well as \"proximity suppression\"\nwhere they collectively do not switch. The mechanism of the proximity\nferroelectricity is an internal electric field determined by the polarization\nof the layers and their relative thickness in a self-consistent manner that\nrenormalizes the double-well ferroelectric potential to lower the steepness of\nthe switching barrier. Further reduction in the coercive field emerges from\ncharged defects in the bulk that act as nucleation centers. The application of\nthe theory to proximity ferroelectricity in Alx-1ScxN\/AlN and Zn1-xMgxO\/ZnO\nbilayers is demonstrated. The theory further predicts that multilayers of\ndielectric\/ferroelectric and paraelectric\/ferroelectric layers can potentially\nresult in induced ferroelectricity in the dielectric or paraelectric layers,\nresulting in the entire stack being switched, an exciting avenue for new\ndiscoveries. This thawing of \"frozen ferroelectrics\", paraelectrics and\npotentially dielectrics, promises a large class of new ferroelectrics with\nexciting prospects for previously unrealizable domain-patterned optoelectronic\nand memory technologies.",
        "Diffusion models are powerful generative models that can produce highly\nrealistic samples for various tasks. Typically, these models are constructed\nusing centralized, independently and identically distributed (IID) training\ndata. However, in practical scenarios, data is often distributed across\nmultiple clients and frequently manifests non-IID characteristics. Federated\nLearning (FL) can leverage this distributed data to train diffusion models, but\nthe performance of existing FL methods is unsatisfactory in non-IID scenarios.\nTo address this, we propose FedDDPM-Federated Learning with Denoising Diffusion\nProbabilistic Models, which leverages the data generative capability of\ndiffusion models to facilitate model training. In particular, the server uses\nwell-trained local diffusion models uploaded by each client before FL training\nto generate auxiliary data that can approximately represent the global data\ndistribution. Following each round of model aggregation, the server further\noptimizes the global model using the auxiliary dataset to alleviate the impact\nof heterogeneous data on model performance. We provide a rigorous convergence\nanalysis of FedDDPM and propose an enhanced algorithm, FedDDPM+, to reduce\ntraining overheads. FedDDPM+ detects instances of slow model learning and\nperforms a one-shot correction using the auxiliary dataset. Experimental\nresults validate that our proposed algorithms outperform the state-of-the-art\nFL algorithms on the MNIST, CIFAR10 and CIFAR100 datasets.",
        "The layer-resolved quantum transport response of a twisted bilayer graphene\ndevice is investigated by driving a current through the bottom layer and\nmeasuring the induced voltage in the top layer. Devices with four- and\neight-layer differentiated contacts were analyzed, revealing that in a\nnanoribbon geometry (four contacts), a counterflow current emerges in the top\nlayer, while in a square-junction configuration (eight contacts), this\ncounterflow is accompanied by a transverse, or Hall, component. These effects\npersist despite weak coupling to contacts, onsite disorder, and variations in\ndevice size. The observed counterflow response indicates a circulating\ninterlayer current, which generates an in-plane magnetic moment excited by the\ninjected current. Finally, due to the intricate relationship between the\nelectrical layer response, energy, and twist angle, a clusterized machine\nlearning model was trained, validated, and tested to predict various\nconductances.",
        "A standard introductory result is that Hausdorff spaces have the property US,\nthat is, each convergent sequence has a unique limit. This paper explores\nseveral existing and new characterizations of separation axioms that are\nstrictly weaker than $T_2$ but strictly stronger than US.",
        "The evader's dominance region is an important concept and the foundation of\ngeometric methods for pursuit-evasion games. This article mainly reveals the\nrelevant properties of the evader's dominance region, especially in\nnon-anticipative information patterns. We can use these properties to research\npursuit-evasion games in non-anticipative information patterns. The core\nproblem is under what condition the pursuer has a non-anticipative strategy to\nprevent the evader leaving its initial dominance region before being captured\nregardless of the evader's strategy. We first define the evader's dominance\nregion by the shortest path distance, and we rigorously prove for the first\ntime that the initial dominance region of the evader is the reachable region of\nthe evader in the open-loop sense. Subsequently, we prove that there exists a\nnon-anticipative strategy by which the pursuer can capture the evader before\nthe evader leaves its initial dominance region's closure in the absence of\nobstacles. For cases with obstacles, we provide a counter example to illustrate\nthat such a non-anticipative strategy does not always exist, and provide a\nnecessary condition for the existence of such strategy. Finally, we consider a\nscenario with a single corner obstacle and provide a sufficient condition for\nthe existence of such a non-anticipative strategy. At the end of this article,\nwe discuss the application of the evader's dominance region in target defense\ngames. This article has important reference significance for the design of\nnon-anticipative strategies in pursuit-evasion games with obstacles.",
        "Rate distortion dimension describes the theoretical limit of lossy data\ncompression methods as the distortion bound goes to zero. It was originally\nintroduced in the context of information theory, and recently it was discovered\nthat it has an intimate connection to Gromov's theory of mean dimension of\ndynamical systems. This paper studies the behavior of rate distortion dimension\nof $\\mathbb{R}^d$-actions under ergodic decomposition. Our main theorems\nprovide natural convexity and concavity of upper and lower rate distortion\ndimensions under convex combination of invariant probability measures. We also\npresent examples which clarify the validity and limitations of the theorems.",
        "The present work is devoted to Computability Logic (CoL), the young and\nvolcanic research-project developed by Giorgi Japaridze. Our main goal is to\nprovide the reader with a clear panoramic view of this vast new land, starting\nfrom its core knots and making our way towards the outer threads, in a somewhat\nthree-dimensional, spacial gait. Furthermore, through the present work, we\nprovide a tentative proof for the decidability of one of CoL's numerous\naxiomatisations, namely CL15. Thus, our expedition initially takes off for an\naerial, perusal overview of this fertile steppe. The first chapter introduces\nCoL in a philosophical fashion, exposing and arguing its main key points. We\nthen move over to unfold its semantics and syntax profiles, allowing the reader\nto become increasingly more familiar with this new environment. Landing on to\nthe second chapter, we thoroughly introduce Cirquent Calculus, the new\ndeductive system Japaridze has developed in order to axiomatise Computability\nLogic. Indeed, this new proof-system can also be a useful tool for many other\nlogics. We then review each of the 17 axiomatisations found so far. The third\nchapter zooms-in on CL15, in order to come up with a possible solution to its\nopen problem. We outline its soundness and completeness proofs; then provide\nsome few deductive examples; and, finally, build a tentative proof of its\ndecidability. Lastly, the fourth chapter focuses on the potential and actual\napplications of Computability Logic, both in arithmetic (clarithmetic) and in\nArtificial Intelligence systems (meaning knowledgebase and planning-and-action\nones). We close our journey with some final remarks on the richness of this\nframework and, hence, the research-worthiness it entails.",
        "National Forest Inventories (NFIs) provide statistically reliable information\non forest resources at national and other large spatial scales. As forest\nmanagement and conservation needs become increasingly complex, NFIs are being\ncalled upon to provide forest parameter estimates at spatial scales smaller\nthan current design-based estimation procedures can provide. This is\nparticularly true when estimates are desired by species or species groups. Here\nwe propose a multivariate spatial model for small area estimation of\nspecies-specific forest inventory parameters. The hierarchical Bayesian\nmodeling framework accounts for key complexities in species-specific forest\ninventory data, such as zero-inflation, correlations among species, and\nresidual spatial autocorrelation. Importantly, by fitting the model directly to\nthe individual plot-level data, the framework enables estimates of\nspecies-level forest parameters, with associated uncertainty, across any\nuser-defined small area of interest. A simulation study revealed minimal bias\nand higher accuracy of the proposed model-based approach compared to the\ndesign-based estimator and a non-parametric k-nearest neighbor (kNN) estimator.\nWe applied the model to estimate species-specific county-level aboveground\nbiomass for the 20 most abundant tree species in the southern United States\nusing Forest Inventory and Analysis (FIA) data. Biomass estimates from the\nproposed model had high correlations with design-based estimates and kNN\nestimates. Importantly, the proposed model provided large gains in precision\nacross all 20 species. On average across species, 91.5% of county-level biomass\nestimates had higher precision compared to the design-based estimates. The\nproposed framework improves the ability of NFI data users to generate\nspecies-level forest parameter estimates with reasonable precision at\nmanagement-relevant spatial scales.",
        "Tabular deep-learning methods require embedding numerical and categorical\ninput features into high-dimensional spaces before processing them. Existing\nmethods deal with this heterogeneous nature of tabular data by employing\nseparate type-specific encoding approaches. This limits the cross-table\ntransfer potential and the exploitation of pre-trained knowledge. We propose a\nnovel approach that first transforms tabular data into text, and then leverages\npre-trained representations from LLMs to encode this data, resulting in a\nplug-and-play solution to improv ing deep-learning tabular methods. We\ndemonstrate that our approach improves accuracy over competitive models, such\nas MLP, ResNet and FT-Transformer, by validating on seven classification\ndatasets.",
        "Highway merges present difficulties for human drivers and automated vehicles\ndue to incomplete situational awareness and a need for a structured\n(precedence, order) environment, respectively. In this paper, an unstructured\nmerge algorithm is presented for connected and automated vehicles. There is\nneither precedence nor established passing order through the merge point. The\nalgorithm relies on Control Barrier Functions for safety (collision avoidance)\nand for coordination that arises from exponential instability of\nstall-equilibria in the inter-agent space. A Monte Carlo simulation comparison\nto a first-in-first-out approach shows improvement in traffic flow and a\nsignificant energy efficiency benefit.",
        "We investigate the moderate and large deviations in first-passage percolation\n(FPP) with bounded weights on $\\mathbb{Z}^d$ for $d \\geq 2$. Write\n$T(\\mathbf{x}, \\mathbf{y})$ for the first-passage time and denote by\n$\\mu(\\mathbf{u})$ the time constant in direction $\\mathbf{u}$. In this paper,\nwe establish that, if one assumes that the sublinear error term $T(\\mathbf{0},\nN\\mathbf{u}) - N\\mu(\\mathbf{u})$ is of order $N^\\chi$, then under some\nunverified (but widely believed) assumptions, for $\\chi < a < 1$,\n\\begin{align*}\n  &\\mathbb{P}\\bigl(T(\\mathbf{0}, N\\mathbf{u}) > N\\mu(\\mathbf{u}) + N^a\\bigr) =\n\\exp{\\Big(-\\,N^{\\frac{d(1+o(1))}{1-\\chi}(a-\\chi)}\\Big)},\n  &\\mathbb{P}\\bigl(T(\\mathbf{0}, N\\mathbf{u}) < N\\mu(\\mathbf{u}) - N^a\\bigr) =\n\\exp{\\Big(-\\,N^{\\frac{1+o(1)}{1-\\chi}(a-\\chi)}\\Big)}, \\end{align*} with\naccompanying estimates in the borderline case $a=1$. Moreover, the exponents\n$\\frac{d}{1-\\chi}$ and $\\frac{1}{1-\\chi}$ also appear in the asymptotic\nbehavior near $0$ of the rate functions for upper and lower tail large\ndeviations. Notably, some of our estimates are established rigorously without\nrelying on any unverified assumptions. Our main results highlight the interplay\nbetween fluctuations and the decay rates of large deviations, and bridge the\ngap between these two regimes.\n  A key ingredient of our proof is an improved concentration via multi-scale\nanalysis for several moderate deviation estimates, a phenomenon that has\npreviously appeared in the contexts of two-dimensional last-passage percolation\nand two-dimensional rotationally invariant FPP."
      ]
    }
  },
  {
    "id":2412.00319,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Human-Centric Interfaces for Ambient Intelligence",
    "start_abstract":"To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence.",
    "start_categories":[
      "physics.app-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Speaker Diarization with LSTM"
      ],
      "abstract":[
        "For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Multi-modal Speech Enhancement with Limited Electromyography Channels",
        "Spline Quantile Regression",
        "Custom Loss Functions in Fuel Moisture Modeling",
        "Interior control for surfaces with positive scalar curvature and its\n  application",
        "Estimates for short character sums evaluated at homogeneous polynomials",
        "An Explainable Pipeline for Machine Learning with Functional Data",
        "Mamba-Shedder: Post-Transformer Compression for Efficient Selective\n  Structured State Space Models",
        "EHCTNet: Enhanced Hybrid of CNN and Transformer Network for Remote\n  Sensing Image Change Detection",
        "Data-driven geometric parameter optimization for PD-GMRES",
        "Diagrammatic Categories which arise from Representation Graphs",
        "Detecting Heel Strike and toe off Events Using Kinematic Methods and\n  LSTM Models",
        "Approximation properties of neural ODEs",
        "Objective Metrics for Human-Subjects Evaluation in Explainable\n  Reinforcement Learning",
        "Unraveling Pedestrian Fatality Patterns: A Comparative Study with\n  Explainable AI",
        "Duoidal R-Matrices",
        "Dark Deceptions in DHCP: Dismantling Network Defenses",
        "Numerical Schemes for Signature Kernels",
        "TokenSkip: Controllable Chain-of-Thought Compression in LLMs",
        "A Unifying Framework for Causal Imitation Learning with Hidden\n  Confounders",
        "Nonabelian Yang-Mills-Higgs and Plateau's problem in codimension three",
        "Multi-layer RIS on Edge: Communication, Computation and Wireless Power\n  Transfer",
        "Causal Learning for Heterogeneous Subgroups Based on Nonlinear Causal\n  Kernel Clustering",
        "Spiking Neural Network Accelerator Architecture for Differential-Time\n  Representation using Learned Encoding",
        "Fast and Cheap Covariance Smoothing",
        "Phase variation and angular momentum of the Riemann, and, Dirichlet Xi\n  functions",
        "On the ADM mass of critical area-normalized capacitors",
        "Assortative Marriage and Geographic Sorting",
        "CPVis: Evidence-based Multimodal Learning Analytics for Evaluation in\n  Collaborative Programming",
        "Measuring Political Preferences in AI Systems: An Integrative Approach"
      ],
      "abstract":[
        "Speech enhancement (SE) aims to improve the clarity, intelligibility, and\nquality of speech signals for various speech enabled applications. However,\nair-conducted (AC) speech is highly susceptible to ambient noise, particularly\nin low signal-to-noise ratio (SNR) and non-stationary noise environments.\nIncorporating multi-modal information has shown promise in enhancing speech in\nsuch challenging scenarios. Electromyography (EMG) signals, which capture\nmuscle activity during speech production, offer noise-resistant properties\nbeneficial for SE in adverse conditions. Most previous EMG-based SE methods\nrequired 35 EMG channels, limiting their practicality. To address this, we\npropose a novel method that considers only 8-channel EMG signals with acoustic\nsignals using a modified SEMamba network with added cross-modality modules. Our\nexperiments demonstrate substantial improvements in speech quality and\nintelligibility over traditional approaches, especially in extremely low SNR\nsettings. Notably, compared to the SE (AC) approach, our method achieves a\nsignificant PESQ gain of 0.235 under matched low SNR conditions and 0.527 under\nmismatched conditions, highlighting its robustness.",
        "Quantile regression is a powerful tool capable of offering a richer view of\nthe data as compared to linear-squares regression. Quantile regression is\ntypically performed individually on a few quantiles or a grid of quantiles\nwithout considering the similarity of the underlying regression coefficients at\nnearby quantiles. When needed, an ad hoc post-processing procedure such as\nkernel smoothing is employed to smooth the estimated coefficients across\nquantiles and thereby improve the performance of these estimates. This paper\nintroduces a new method, called spline quantile regression (SQR), that unifies\nquantile regression with quantile smoothing and jointly estimates the\nregression coefficients across quantiles as smoothing splines. We discuss the\ncomputation of the SQR solution as a linear program (LP) using an\ninterior-point algorithm. We also experiment with some gradient algorithms that\nrequire less memory than the LP algorithm. The performance of the SQR method\nand these algorithms is evaluated using simulated and real-world data.",
        "Fuel moisture content (FMC) is a key predictor for wildfire rate of spread\n(ROS). Machine learning models of FMC are being used more in recent years,\naugmenting or replacing traditional physics-based approaches. Wildfire rate of\nspread (ROS) has a highly nonlinear relationship with FMC, where small\ndifferences in dry fuels lead to large differences in ROS. In this study,\ncustom loss functions that place more weight on dry fuels were examined with a\nvariety of machine learning models of FMC. The models were evaluated with a\nspatiotemporal cross-validation procedure to examine whether the custom loss\nfunctions led to more accurate forecasts of ROS. Results show that the custom\nloss functions improved accuracy for ROS forecasts by a small amount. Further\nresearch would be needed to establish whether the improvement in ROS forecasts\nleads to more accurate real-time wildfire simulations.",
        "Let $M^n$, $n\\in\\{3,4,5\\}$, be a closed aspherical $n$-manifold and $S\\subset\nM$ a subset consisting of disjoint incompressible embedded closed aspherical\nsubmanifolds (possibly with different dimensions). When $n =3,4$, we show that\n$M\\setminus S$ cannot admit any complete metric with positive scalar curvature.\nWhen $n=5$, we obtain the same result either when $S$ contains a submanifold of\ncodimension 1 or 2, or when $S$ itself is a connected submaifold of codimension\n$\\ge 3.$ The key ingredient is a new interior control for the extrinsic\ndiameter of surfaces with positive scalar curvature.",
        "Let $p$ be a prime. We prove bounds on short Dirichlet character sums\nevaluated at a class of homogeneous polynomials in arbitrary dimensions. In\nevery dimension, this bound is nontrivial for sums over boxes with side lengths\nas short as $p^{1\/4 + \\kappa}$ for any $\\kappa>0$. Our methods capitalize on\nthe relationship between characters mod $p$ and characters over finite field\nextensions as well as bounds on the multiplicative energy of sets in products\nof finite fields.",
        "Machine learning (ML) models have shown success in applications with an\nobjective of prediction, but the algorithmic complexity of some models makes\nthem difficult to interpret. Methods have been proposed to provide insight into\nthese \"black-box\" models, but there is little research that focuses on\nsupervised ML when the model inputs are functional data. In this work, we\nconsider two applications from high-consequence spaces with objectives of\nmaking predictions using functional data inputs. One application aims to\nclassify material types to identify explosive materials given hyperspectral\ncomputed tomography scans of the materials. The other application considers the\nforensics science task of connecting an inkjet printed document to the source\nprinter using color signatures extracted by Raman spectroscopy. An instinctive\nroute to consider for analyzing these data is a data driven ML model for\nclassification, but due to the high consequence nature of the applications, we\nargue it is important to appropriately account for the nature of the data in\nthe analysis to not obscure or misrepresent patterns. As such, we propose the\nVariable importance Explainable Elastic Shape Analysis (VEESA) pipeline for\ntraining ML models with functional data that (1) accounts for the vertical and\nhorizontal variability in the functional data and (2) provides an explanation\nin the original data space of how the model uses variability in the functional\ndata for prediction. The pipeline makes use of elastic functional principal\ncomponents analysis (efPCA) to generate uncorrelated model inputs and\npermutation feature importance (PFI) to identify the principal components\nimportant for prediction. The variability captured by the important principal\ncomponents in visualized the original data space. We ultimately discuss ideas\nfor natural extensions of the VEESA pipeline and challenges for future\nresearch.",
        "Large pre-trained models have achieved outstanding results in sequence\nmodeling. The Transformer block and its attention mechanism have been the main\ndrivers of the success of these models. Recently, alternative architectures,\nsuch as Selective Structured State Space Models (SSMs), have been proposed to\naddress the inefficiencies of Transformers. This paper explores the compression\nof SSM-based models, particularly Mamba and its hybrids. We study the\nsensitivity of these models to the removal of selected components at different\ngranularities to reduce the model size and computational overhead, thus\nimproving their efficiency while maintaining accuracy. The proposed solutions,\ncollectively referred to as Mamba-Shedder, achieve a speedup of up to 1.4x\nduring inference, demonstrating that model efficiency can be improved by\neliminating several redundancies with minimal impact on the overall model\nperformance. The code is available at\nhttps:\/\/github.com\/IntelLabs\/Hardware-Aware-Automated-Machine-Learning.",
        "Remote sensing (RS) change detection incurs a high cost because of false\nnegatives, which are more costly than false positives. Existing frameworks,\nstruggling to improve the Precision metric to reduce the cost of false\npositive, still have limitations in focusing on the change of interest, which\nleads to missed detections and discontinuity issues. This work tackles these\nissues by enhancing feature learning capabilities and integrating the frequency\ncomponents of feature information, with a strategy to incrementally boost the\nRecall value. We propose an enhanced hybrid of CNN and Transformer network\n(EHCTNet) for effectively mining the change information of interest. Firstly, a\ndual branch feature extraction module is used to extract the multi scale\nfeatures of RS images. Secondly, the frequency component of these features is\nexploited by a refined module I. Thirdly, an enhanced token mining module based\non the Kolmogorov Arnold Network is utilized to derive semantic information.\nFinally, the semantic change information's frequency component, beneficial for\nfinal detection, is mined from the refined module II. Extensive experiments\nvalidate the effectiveness of EHCTNet in comprehending complex changes of\ninterest. The visualization outcomes show that EHCTNet detects more intact and\ncontinuous changed areas and perceives more accurate neighboring distinction\nthan state of the art models.",
        "Restarted GMRES is a robust and widely used iterative solver for linear\nsystems. The control of the restart parameter is a key task to accelerate\nconvergence and to prevent the well-known stagnation phenomenon. We focus on\nthe Proportional-Derivative GMRES (PD-GMRES), which has been derived using\ncontrol-theoretic ideas in [Cuevas N\\'u\\~nez, Schaerer, and Bhaya (2018)] as a\nversatile method for modifying the restart parameter. Several variants of a\nquadtree-based geometric optimization approach are proposed to find a best\nchoice of PD-GMRES parameters. We show that the optimized PD-GMRES performs\nwell across a large number of matrix types and we observe superior performance\nas compared to major other GMRES-based iterative solvers. Moreover, we propose\nan extension of the PD-GMRES algorithm to further improve performance by\ncontrolling the range of values for the restart parameter.",
        "The main result of this paper utilizes the representation graph of a group\n$G$, $R(V,G)$, and gives a general construction of a diagrammatic category\n$\\mathbf{Dgrams}_{R(V,G)}$. The proof of the main theorem shows that, given\nexplicit criteria, there is an equivalence of categories between a quotient\ncategory of $\\mathbf{Dgrams}_{R(V,G)}$ and a full subcategory of\n$G-\\textbf{mod}$ with objects being the tensor products of finitely many\nirreducible $G$-modules.",
        "Accurate gait event detection is crucial for gait analysis, rehabilitation,\nand assistive technology, particularly in exoskeleton control, where precise\nidentification of stance and swing phases is essential. This study evaluated\nthe performance of seven kinematics-based methods and a Long Short-Term Memory\n(LSTM) model for detecting heel strike and toe-off events across 4363 gait\ncycles from 588 able-bodied subjects. The results indicated that while the Zeni\net al. method achieved the highest accuracy among kinematics-based approaches,\nother methods exhibited systematic biases or required dataset-specific tuning.\nThe LSTM model performed comparably to Zeni et al., providing a data-driven\nalternative without systematic bias. These findings highlight the potential of\ndeep learning-based approaches for gait event detection while emphasizing the\nneed for further validation in clinical populations and across diverse gait\nconditions. Future research will explore the generalizability of these methods\nin pathological populations, such as individuals with post-stroke conditions\nand knee osteoarthritis, as well as their robustness across varied gait\nconditions and data collection settings to enhance their applicability in\nrehabilitation and exoskeleton control.",
        "We study the approximation properties of shallow neural networks whose\nactivation function is defined as the flow of a neural ordinary differential\nequation (neural ODE) at the final time of the integration interval. We prove\nthe universal approximation property (UAP) of such shallow neural networks in\nthe space of continuous functions. Furthermore, we investigate the\napproximation properties of shallow neural networks whose parameters are\nrequired to satisfy some constraints. In particular, we constrain the Lipschitz\nconstant of the flow of the neural ODE to increase the stability of the shallow\nneural network, and we restrict the norm of the weight matrices of the linear\nlayers to one to make sure that the restricted expansivity of the flow is not\ncompensated by the increased expansivity of the linear layers. For this\nsetting, we prove approximation bounds that tell us the accuracy to which we\ncan approximate a continuous function with a shallow neural network with such\nconstraints. We prove that the UAP holds if we consider only the constraint on\nthe Lipschitz constant of the flow or the unit norm constraint on the weight\nmatrices of the linear layers.",
        "Explanation is a fundamentally human process. Understanding the goal and\naudience of the explanation is vital, yet existing work on explainable\nreinforcement learning (XRL) routinely does not consult humans in their\nevaluations. Even when they do, they routinely resort to subjective metrics,\nsuch as confidence or understanding, that can only inform researchers of users'\nopinions, not their practical effectiveness for a given problem. This paper\ncalls on researchers to use objective human metrics for explanation evaluations\nbased on observable and actionable behaviour to build more reproducible,\ncomparable, and epistemically grounded research. To this end, we curate,\ndescribe, and compare several objective evaluation methodologies for applying\nexplanations to debugging agent behaviour and supporting human-agent teaming,\nillustrating our proposed methods using a novel grid-based environment. We\ndiscuss how subjective and objective metrics complement each other to provide\nholistic validation and how future work needs to utilise standardised\nbenchmarks for testing to enable greater comparisons between research.",
        "Road fatalities pose significant public safety and health challenges\nworldwide, with pedestrians being particularly vulnerable in vehicle-pedestrian\ncrashes due to disparities in physical and performance characteristics. This\nstudy employs explainable artificial intelligence (XAI) to identify key factors\ncontributing to pedestrian fatalities across the five U.S. states with the\nhighest crash rates (2018-2022). It compares them to the five states with the\nlowest fatality rates. Using data from the Fatality Analysis Reporting System\n(FARS), the study applies machine learning techniques-including Decision Trees,\nGradient Boosting Trees, Random Forests, and XGBoost-to predict contributing\nfactors to pedestrian fatalities. To address data imbalance, the Synthetic\nMinority Over-sampling Technique (SMOTE) is utilized, while SHapley Additive\nExplanations (SHAP) values enhance model interpretability. The results indicate\nthat age, alcohol and drug use, location, and environmental conditions are\nsignificant predictors of pedestrian fatalities. The XGBoost model outperformed\nothers, achieving a balanced accuracy of 98 %, accuracy of 90 %, precision of\n92 %, recall of 90 %, and an F1 score of 91 %. Findings reveal that pedestrian\nfatalities are more common in mid-block locations and areas with poor\nvisibility, with older adults and substance-impaired individuals at higher\nrisk. These insights can inform policymakers and urban planners in implementing\ntargeted safety measures, such as improved lighting, enhanced pedestrian\ninfrastructure, and stricter traffic law enforcement, to reduce fatalities and\nimprove public safety.",
        "In this note, we define an analogue of R-matrices for bialgebras in the\nsetting of a monad that is opmonoidal over two tensor products. Analogous to\nthe classical case, such structures bijectively correspond to duoidal\nstructures on the Eilenberg--Moore category of the monad. Further, we\ninvestigate how a cocommutative version of this lifts the linearly distributive\nstructure of a normal duoidal category.",
        "This paper explores vulnerabilities in the Dynamic Host Configuration\nProtocol (DHCP) and their implications on the Confidentiality, Integrity, and\nAvailability (CIA) Triad. Through an analysis of various attacks, including\nDHCP Starvation, Rogue DHCP Servers, Replay Attacks, and TunnelVision exploits,\nthe paper provides a taxonomic classification of threats, assesses risks, and\nproposes appropriate controls. The discussion also highlights the dangers of\nVPN decloaking through DHCP exploits and underscores the importance of\nsafeguarding network infrastructures. By bringing awareness to the TunnelVision\nexploit, this paper aims to mitigate risks associated with these prevalent\nvulnerabilities.",
        "Signature kernels have emerged as a powerful tool within kernel methods for\nsequential data. In the paper \"The Signature Kernel is the solution of a\nGoursat PDE\", the authors identify a kernel trick that demonstrates that, for\ncontinuously differentiable paths, the signature kernel satisfies a Goursat\nproblem for a hyperbolic partial differential equation (PDE) in two independent\ntime variables. While finite difference methods have been explored for this\nPDE, they face limitations in accuracy and stability when handling highly\noscillatory inputs. In this work, we introduce two advanced numerical schemes\nthat leverage polynomial representations of boundary conditions through either\napproximation or interpolation techniques, and rigorously establish the\ntheoretical convergence of the polynomial approximation scheme. Experimental\nevaluations reveal that our approaches yield improvements of several orders of\nmagnitude in mean absolute percentage error (MAPE) compared to traditional\nfinite difference schemes, without increasing computational complexity.\nFurthermore, like finite difference methods, our algorithms can be\nGPU-parallelized to reduce computational complexity from quadratic to linear in\nthe length of the input sequences, thereby improving scalability for\nhigh-frequency data. We have implemented these algorithms in a dedicated Python\nlibrary, which is publicly available at:\nhttps:\/\/github.com\/FrancescoPiatti\/polysigkernel.",
        "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop.",
        "We propose a general and unifying framework for causal Imitation Learning\n(IL) with hidden confounders that subsumes several existing confounded IL\nsettings from the literature. Our framework accounts for two types of hidden\nconfounders: (a) those observed by the expert, which thus influence the\nexpert's policy, and (b) confounding noise hidden to both the expert and the IL\nalgorithm. For additional flexibility, we also introduce a confounding noise\nhorizon and time-varying expert-observable hidden variables. We show that\ncausal IL in our framework can be reduced to a set of Conditional Moment\nRestrictions (CMRs) by leveraging trajectory histories as instruments to learn\na history-dependent policy. We propose DML-IL, a novel algorithm that uses\ninstrumental variable regression to solve these CMRs and learn a policy. We\nprovide a bound on the imitation gap for DML-IL, which recovers prior results\nas special cases. Empirical evaluation on a toy environment with continues\nstate-action spaces and multiple Mujoco tasks demonstrate that DML-IL\noutperforms state-of-the-art causal IL algorithms.",
        "We investigate the asymptotic behavior of the\n$\\mathrm{SU}(2)$-Yang-Mills-Higgs energy $E(\\Phi,A)=\\int_M|d_A\\Phi|^2+|F_A|^2$\nin the large mass limit, proving convergence to the codimension-three area\nfunctional in the sense of De Giorgi's $\\Gamma$-convergence. More precisely,\nfor a compact manifold with boundary $M$ and any family of pairs\n$\\Phi_m\\in\\Omega^0(M;\\mathfrak{su}(2))$ and $A_m\\in\n\\Omega^1(M;\\mathfrak{su}(2))$ indexed by a mass parameter $m\\to\\infty$,\nsatisfying $$E(\\Phi_m,A_m)\\leq\nCm\\quad\\text{and}\\quad\\lim_{m\\to\\infty}\\frac{1}{m}\\int_M(m-|\\Phi_m|)^2=0,$$ we\nprove that the $(n-3)$-currents dual to $\\frac{1}{2\\pi\nm}\\mathrm{tr}(d_{A_m}\\Phi_m\\wedge F_{A_m})$ converge subsequentially to a\nrelative integral $(n-3)$-cycle $T$ of mass \\begin{equation}\n  \\mathbb{M}(T)\\leq \\liminf_{m\\to\\infty}\\frac{1}{4\\pi m}E(\\Phi_m,A_m),\n\\end{equation} and show conversely that any integral $(n-3)$-current $T$ with\n$[T]=0\\in H_{n-3}(M,\\partial M;\\mathbb{Z})$ admits such an approximation, with\nequality in the above inequality. In the special case of pairs $(\\Phi_m,A_m)$\nsatisfying the generalized monopole equation $*d_{A_m}\\Phi_m=F_{A_m}\\wedge\n\\Theta$ for a calibration form $\\Theta\\in \\Omega^{n-3}(M)$, we deduce that the\nlimit $\\nu=\\lim_{m\\to\\infty}\\frac{1}{2\\pi m}|d_{A_m}\\Phi_m|^2$ of the Dirichlet\nenergy measures satisfies $\\nu\\leq |T|$, with equality if and only if $T$ is\ncalibrated by $\\Theta$, giving evidence for predictions of Donaldson-Segal in\nthe settings of $G_2$-manifolds and Calabi-Yau $3$-folds.",
        "The rapid expansion of Internet of Things (IoT) and its integration into\nvarious applications highlight the need for advanced communication,\ncomputation, and energy transfer techniques. However, the traditional\nhardware-based evolution of communication systems faces challenges due to\nexcessive power consumption and prohibitive hardware cost. With the rapid\nadvancement of reconfigurable intelligent surface (RIS), a new approach by\nparallel stacking a series of RIS, i.e., multi-layer RIS, has been proposed.\nBenefiting from the characteristics of scalability, passivity, low cost, and\nenhanced computation capability, multi-layer RIS is a promising technology for\nfuture massive IoT scenarios. Thus, this article proposes a multi-layer\nRIS-based universal paradigm at the network edge, enabling three functions,\ni.e., multiple-input multiple-output (MIMO) communication, computation, and\nwireless power transfer (WPT). Starting by picturing the possible applications\nof multi-layer RIS, we explore the potential signal transmission links, energy\ntransmission links, and computation processes in IoT scenarios, showing its\nability to handle on-edge IoT tasks and associated green challenges. Then,\nthese three key functions are analyzed respectively in detail, showing the\nadvantages of the proposed scheme, compared with the traditional hardware-based\nscheme. To facilitate the implementation of this new paradigm into reality, we\nlist the dominant future research directions at last, such as inter-layer\nchannel modeling, resource allocation and scheduling, channel estimation, and\nedge training. It is anticipated that multi-layer RIS will contribute to more\nenergy-efficient wireless networks in the future by introducing a revolutionary\nparadigm shift to an all-wave-based approach.",
        "Due to the challenge posed by multi-source and heterogeneous data collected\nfrom diverse environments, causal relationships among features can exhibit\nvariations influenced by different time spans, regions, or strategies. This\ndiversity makes a single causal model inadequate for accurately representing\ncomplex causal relationships in all observational data, a crucial consideration\nin causal learning. To address this challenge, the nonlinear Causal Kernel\nClustering method is introduced for heterogeneous subgroup causal learning,\nhighlighting variations in causal relationships across diverse subgroups. The\nmain component for clustering heterogeneous subgroups lies in the construction\nof the $u$-centered sample mapping function with the property of unbiased\nestimation, which assesses the differences in potential nonlinear causal\nrelationships in various samples and supported by causal identifiability\ntheory. Experimental results indicate that the method performs well in\nidentifying heterogeneous subgroups and enhancing causal learning, leading to a\nreduction in prediction error.",
        "Spiking Neural Networks (SNNs) have garnered attention over recent years due\nto their increased energy efficiency and advantages in terms of operational\ncomplexity compared to traditional Artificial Neural Networks (ANNs). Two\nimportant questions when implementing SNNs are how to best encode existing data\ninto spike trains and how to efficiently process these spike trains in\nhardware. This paper addresses both of these problems by incorporating the\nencoding into the learning process, thus allowing the network to learn the\nspike encoding alongside the weights. Furthermore, this paper proposes a\nhardware architecture based on a recently introduced differential-time\nrepresentation for spike trains allowing decoupling of spike time and\nprocessing time. Together these contributions lead to a feedforward SNN using\nonly Leaky-Integrate and Fire (LIF) neurons that surpasses 99% accuracy on the\nMNIST dataset while still being implementable on medium-sized FPGAs with\ninference times of less than 295us.",
        "We introduce the Tensorized-and-Restricted Krylov (TReK) method, a simple and\nefficient algorithm for estimating covariance tensors with large observational\nsizes. TReK extends the conjugate gradient method to incorporate range\nrestrictions, enabling its use in a variety of covariance smoothing\napplications. By leveraging matrix-level operations, it achieves significant\nimprovements in both computational speed and memory cost, improving over\nexisting methods by an order of magnitude. TReK ensures finite-step convergence\nin the absence of rounding errors and converges fast in practice, making it\nwell-suited for large-scale problems. The algorithm is also highly flexible,\nsupporting a wide range of forward and projection tensors.",
        "The concept of angular momentum is used to find new RH equivalence\nstatements, and, generalize some known results from Riemann to Dirichlet\nprimitive Xi functions",
        "In this note, we prove mass-capacity inequalities for asymptotically flat\nmanifolds whose boundary capacity potential satisfies an overdetermined\nproblem, referred to as critical area-normalized capacitors. As a consequence,\nwe obtain uniqueness results for the Schwarzschild metric, from which\nimprovements in the uniqueness theorems for spin asymptotically flat spacetimes\ncontaining a connected photon surface, as well as for spin asymptotically flat\nstatic manifolds with boundary are obtained.",
        "Between 1980 and 2000, the U.S. experienced a significant rise in geographic\nsorting and educational homogamy, with college graduates increasingly\nconcentrating in high-skill cities and marrying similarly educated spouses. We\ndevelop and estimate a spatial equilibrium model with local labor, housing, and\nmarriage markets, incorporating a marriage matching framework with transferable\nutility. Using the model, we estimate trends in assortative preferences,\nquantify the interplay between marital and geographic sorting, and assess their\ncombined impact on household inequality. Welfare analyses show that after\naccounting for marriage, the college well-being gap grew substantially more\nthan the college wage gap.",
        "As programming education becomes more widespread, many college students from\nnon-computer science backgrounds begin learning programming. Collaborative\nprogramming emerges as an effective method for instructors to support novice\nstudents in developing coding and teamwork abilities. However, due to limited\nclass time and attention, instructors face challenges in monitoring and\nevaluating the progress and performance of groups or individuals. To address\nthis issue, we collect multimodal data from real-world settings and develop\nCPVis, an interactive visual analytics system designed to assess student\ncollaboration dynamically. Specifically, CPVis enables instructors to evaluate\nboth group and individual performance efficiently. CPVis employs a novel\nflower-based visual encoding to represent performance and provides time-based\nviews to capture the evolution of collaborative behaviors. A within-subject\nexperiment (N=22), comparing CPVis with two baseline systems, reveals that\nusers gain more insights, find the visualization more intuitive, and report\nincreased confidence in their assessments of collaboration.",
        "Political biases in Large Language Model (LLM)-based artificial intelligence\n(AI) systems, such as OpenAI's ChatGPT or Google's Gemini, have been previously\nreported. While several prior studies have attempted to quantify these biases\nusing political orientation tests, such approaches are limited by potential\ntests' calibration biases and constrained response formats that do not reflect\nreal-world human-AI interactions. This study employs a multi-method approach to\nassess political bias in leading AI systems, integrating four complementary\nmethodologies: (1) linguistic comparison of AI-generated text with the language\nused by Republican and Democratic U.S. Congress members, (2) analysis of\npolitical viewpoints embedded in AI-generated policy recommendations, (3)\nsentiment analysis of AI-generated text toward politically affiliated public\nfigures, and (4) standardized political orientation testing. Results indicate a\nconsistent left-leaning bias across most contemporary AI systems, with arguably\nvarying degrees of intensity. However, this bias is not an inherent feature of\nLLMs; prior research demonstrates that fine-tuning with politically skewed data\ncan realign these models across the ideological spectrum. The presence of\nsystematic political bias in AI systems poses risks, including reduced\nviewpoint diversity, increased societal polarization, and the potential for\npublic mistrust in AI technologies. To mitigate these risks, AI systems should\nbe designed to prioritize factual accuracy while maintaining neutrality on most\nlawful normative issues. Furthermore, independent monitoring platforms are\nnecessary to ensure transparency, accountability, and responsible AI\ndevelopment."
      ]
    }
  },
  {
    "id":2412.00319,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Speaker Diarization with LSTM",
    "start_abstract":"For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Human-Centric Interfaces for Ambient Intelligence"
      ],
      "abstract":[
        "To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence."
      ],
      "categories":[
        "physics.app-ph"
      ]
    },
    "list":{
      "title":[
        "Einstein-Maxwell-Dilaton Wormholes that meet the Energy Conditions",
        "On the comparison principle for a nonlocal infinity Laplacian",
        "Freeze-and-release direct optimization method for variational\n  calculations of excited electronic states",
        "$e$-product of distributions, with applications",
        "A nonlinear model of shearable elastic rod from an origami-like\n  microstructure displaying folding and faulting",
        "Galaxy infall models for arbitrary velocity directions",
        "Analysis of $q_\\mathrm{rec}^2$-distribution for $B\\to K M_X$ and $B\\to\n  K^* M_X$ decays in a scalar-mediator dark-matter scenario",
        "Are Large Language Models Good In-context Learners for Financial\n  Sentiment Analysis?",
        "Geometric Gauss Sums and Gross-Koblitz Formulas over Function Fields",
        "Solar irradiance statistical analysis in Mexico City from 2018 to 2021",
        "On the axially symmetric solutions to the spatially homogeneous Landau\n  equation",
        "Electroweak baryogenesis from charged current anomalies in $B$ meson\n  decays",
        "Ferri- and Ferro-Electric Switching in Spontaneously Chiral Polar Liquid\n  Crystals",
        "A general quasilinear elliptic problem with variable exponents and\n  Neumann boundary conditions for image processing",
        "Limit theorems for the fluctuation of the mixed elephant random walk in\n  the superdiffusive case",
        "Sequential Change Point Detection via Denoising Score Matching",
        "Emergence of the polydeterminant in QCD",
        "Infinitely many solutions for a boundary Yamabe problem",
        "Euclid: Early Release Observations -- Interplay between dwarf galaxies\n  and their globular clusters in the Perseus galaxy cluster",
        "The Spectroscopy of Kerr-Einstein-Maxwell-Dilaton-Axion: Exact\n  Quasibound States, Scalar Cloud, Horizon's Boson Statistics and Superradiance",
        "Skew shapes, Ehrhart positivity and beyond",
        "Multiplicative chaos measure for multiplicative functions: the\n  $L^1$-regime",
        "Non-linear Quantum Monte Carlo",
        "Differential topology of the spaces of asymptotically stable vector\n  fields and Lyapunov functions",
        "Orbital Depot Location Optimization for Satellite Constellation\n  Servicing with Low-Thrust Transfers",
        "Multigrid Preconditioning for FD-DLM Method in Elliptic Interface\n  Problems",
        "Dirichlet Spaces In Balls And Half-spaces of $\\R^n$",
        "Disjointly non-singular operators and various topologies on Banach\n  lattices",
        "Exact Parent Hamiltonians for All Landau Level States in a Half-flux\n  Lattice"
      ],
      "abstract":[
        "One of the latest predictions of Einstein's theory is the existence of\nWormholes (WH). In this work, we present exact solutions of the\nEinstein-Maxwell-Dilaton equations representing traversable Wormholes. These\nsolutions satisfy the energy conditions and have a ring singularity satisfying\nthe cosmic censorship of WHs, i.e. we show that, as in previous solutions,\ngeodesics cannot touch the singularity. We find that the most optimal input\nregions for the first class of solutions traversing these wormholes are near\nthe poles and near the equatorial plane for the second class. We also find that\nthe solution associated with the first class is physically feasible, while for\nthe second class it presents the problem of not being asymptotically flat when\nconsidering a dilatonic-type scalar field. Finally, we give examples of\nrealistic astrophysical objects that could fulfill these conditions.",
        "In this article, we prove the uniqueness of viscosity solutions to\n$\\mathcal{L}_{\\infty} u =f$ in $\\Omega$, where $\\mathcal{L}_{\\infty}$ denotes\nthe nonlocal infinity Laplace operator, $\\Omega$ a bounded domain, and $f$ a\ncontinuous functions such that $f \\leq 0$. Uniqueness is established through a\ncomparison principle.",
        "Time-independent, orbital-optimized density functional approaches outperform\ntime-dependent density functional theory (TDDFT) in calculations of excited\nelectronic states involving a large rearrangement of the electron density, such\nas charge transfer excitations. However, optimizing orbitals for excited states\nremains challenging, as the latter typically correspond to saddle points on the\nelectronic energy surface. A simple and robust strategy for variational orbital\noptimization of excited states is presented. The approach involves two steps:\n(1) a constrained energy minimization, where a subset of orbitals changed by\nthe excitation are frozen, followed by (2) a fully unconstrained saddle point\noptimization. The constrained minimization step makes it possible to identify\nthe electronic degrees of freedom along which the energy needs to be maximized,\npreventing variational collapse. Both steps of this freeze-and-release strategy\nare carried out using direct optimization algorithms with a computational\nscaling comparable to ground state calculations. Numerical tests using a\nsemilocal functional are performed on intramolecular charge transfer states of\norganic molecules and intermolecular charge transfer states of molecular\ndimers. It is shown that the freeze-and-release direct optimization (FR-DO)\napproach can successfully converge challenging charge transfer states,\novercoming limitations of conventional algorithms based on the maximum overlap\nmethod, which either collapse to lower energy, charge-delocalized solutions or\nfail to converge. While FR-DO requires more iterations on average, the overall\nincrease in computational cost is small. For the NH3-F2 dimer, it is found that\nunlike TDDFT, orbital-optimized calculations reproduce the correct long-range\ndependency of the energy with respect to the donor-acceptor separation without\nthe need to include exact exchange in the long range.",
        "We consider and reformulate a recent definition of multiplication between\ndistributions. We show that this definition can be adopted, in particular, to\nprove biorthonormality of some distributions arising when looking to the\n(generalized) eigenvalues of a specific non self-adjoint number-like operator,\nconsidered in connection with the recently introduced {\\em weak pseudo-bosons}.\nSeveral examples are discussed in details.",
        "A new continuous model of shearable rod, subject to large elastic\ndeformation, is derived from nonlinear homogenization of a one-dimensional\nperiodic microstructured chain. As particular cases, the governing equations\nreduce to the Euler elastica and to the shearable elastica known as 'Engesser',\nthat has been scarcely analysed so far. The microstructure that is homogenized\nis made up of elastic hinges and four-bar linkages, which may be realized in\npractice using origami joints. The equivalent continuous rod is governed by a\nDifferential-Algebraic system of nonlinear Equations (DAE), containing an\ninternal length ratio, and showing a surprisingly rich mechanical landscape,\nwhich involves a twin sequence of bifurcation loads, separated by a\n'transition' mode. The latter occurs, for simply supported and cantilever rods\nin a 'bookshelf-like' mode and in a mode involving faulting (formation of a\nstep in displacement), respectively. The postcritical response of the simply\nsupported rod exhibits the emergence of folding, an infinite curvature\noccurring at a point of the rod axis, developing into a curvature jump at\nincreasing load. Faulting and folding, excluded for both Euler and Reissner\nmodels and so far unknown in the rod theory, represent 'signatures' revealing\nthe origami design of the microstructure. These two features are shown to be\nassociated with bifurcations and, in particular folding, with a secondary\nbifurcation of the corresponding discrete chain when the number of elements is\nodd. Beside the intrinsic theoretical relevance to the field of structural\nmechanics, our results can be applied to various technological contexts\ninvolving highly compliant mechanisms, such as the achievement of objective\ntrajectories with soft robot arms through folding and localized displacement of\norigami-inspired or multi-material mechanisms.",
        "For most galaxies in the cosmos, our knowledge of their motion is limited to\nline-of-sight velocities from redshift observations. Peculiar motions on the\nsky are only measured for a few cases. With increasingly detailed observations,\nthe assumption that line-of-sight velocities suffice for an accurate and\nprecise reconstruction of galaxy kinematics needs to be re-investigated and the\nimpact of perpendicular velocities to be quantified. We analyse the motion of\ntwo galaxies with arbitrary velocities, determine their mutual velocity on an\narbitrary background, and compare this general relative velocity to the one\nfrom line-of-sight components only. The latter are known as ``minor and major\ninfall models'' established by Karachentsev and Kashibadze (2006). Our\nderivations reveal that the infall models approximate the radial velocity\nbetween two galaxies by two different projections employing different\ninformation about the system. For galaxies with small angular separations, all\ninfall models agree that the radial velocity is the difference of their\nline-of-sight velocities. For larger angles, the minor infall model is mostly\nsuitable when perpendicular velocity components are negligible and there is no\ninformation about the tangential velocity of the binary. The major infall model\nis best suitable when the motion is mainly radial and symmetry assumptions\ncancel the tangential and one perpendicular component. The latter often\nrequires to transition from galaxy binaries to groups or clusters, as we show\nquantitatively. We give an encompassing overview how the infall models over-\nand under-estimate general binary or $N$-body motions. We quantify the impact\nof perpendicular velocity components, sparse sampling, and deviations of the\ntracer-galaxies from the motion in an embedding gravitational potential which\nare related to the angular momentum of the structure. (abridged)",
        "We demonstrate that the scalar-mediator dark-matter scenario is consistent\nwith the experimental data on the decay $B\\to K M_X$ and provides a good\ndescription of the shape of the observed excess. Within this scenario, the\ninteraction with dark-matter particles leads to approximately the same excess\nin $\\Gamma(B\\to K^* M_X)$ and $\\Gamma(B\\to K M_X)$ compared to the Standard\nModel; also the differential distributions of the excess events are similar in\nshape in the variable $q_\\mathrm{rec}^2$ measured by experiment.",
        "Recently, large language models (LLMs) with hundreds of billions of\nparameters have demonstrated the emergent ability, surpassing traditional\nmethods in various domains even without fine-tuning over domain-specific data.\nHowever, when it comes to financial sentiment analysis (FSA)$\\unicode{x2013}$a\nfundamental task in financial AI$\\unicode{x2013}$these models often encounter\nvarious challenges, such as complex financial terminology, subjective human\nemotions, and ambiguous inclination expressions. In this paper, we aim to\nanswer the fundamental question: whether LLMs are good in-context learners for\nFSA? Unveiling this question can yield informative insights on whether LLMs can\nlearn to address the challenges by generalizing in-context demonstrations of\nfinancial document-sentiment pairs to the sentiment analysis of new documents,\ngiven that finetuning these models on finance-specific data is difficult, if\nnot impossible at all. To the best of our knowledge, this is the first paper\nexploring in-context learning for FSA that covers most modern LLMs (recently\nreleased DeepSeek V3 included) and multiple in-context sample selection\nmethods. Comprehensive experiments validate the in-context learning capability\nof LLMs for FSA.",
        "In this paper, we prove the Gross-Koblitz-Thakur formulas relating special\n$v$-adic gamma values to the newly introduced geometric Gauss sums in the\nfunction field setting. These are analogous to those for the $p$-adic gamma\nfunction in the classical setting due to Gross-Koblitz and the $v$-adic\narithmetic gamma function over function fields due to Thakur. For these new\nGauss sums, we establish their key arithmetic properties, including the\nuniformity of absolute values and prime factorizations. We also determine their\nsigns at infinite places, and derive two analogs of the Hasse-Davenport\nrelations.",
        "Solar radiation is made up of three components of electromagnetic waves:\ninfrared, visible and ultraviolet. The infrared component is the cause of\nthermal energy, the visible spectrum allows to see through the eyes and the\nultraviolet component is the most energetic and damaging. Solar radiation has\nseveral benefits, such as helping to synthesize vitamin D in the skin, favors\nblood circulation, among others benefits for the human body. In the Earth, it\nis the main source of energy for agriculture, also used as an alternative\nsource of energy to hydrocarbons, through solar cells. The solar irradiance\nrepresents the surface power density with units W\/m$^2$ in SI. Too much\nexposure can cause damage and an increase in value over the time can be can be\nalso damaging. In this work it was used an open data base provided by\nSecretar\\'ia del Medio Ambiente, from which a statistical analysis was\nperformed of the solar irradiance values measured at various meteorological\nstations in Mexico City and the so-called metropolitan area, from 2018 to 2021.\nThis analysis was carried out per years, months and days. From the solar\nirradiance values distributions, it was obtained the averages, maximums and\nmeans were it was found there was no variation in the solar irradiance values\nover this period of years.",
        "In this paper, we consider the spatially homogeneous Landau equation, which\nis a variation of the Boltzmann equation in the grazing collision limit. For\nthe Landau equation for hard potentials in the style of Desvillettes-Villani\n(Comm. Partial Differential Equations, 2000), we provide the proof of the\nexistence of axisymmetric measure-valued solution for any axisymmetric\n$\\mathcal{P}_p(\\mathbb{R}^3)$ initial profile for any $p\\ge 2$. Moreover, we\nprove that if the initial data is not a single Dirac mass, then the solution\ninstantaneously becomes analytic for any time $t>0$ in the hard potential case.\nIn the soft potential and the Maxwellian molecule cases, we show that there are\nno solutions whose support is contained in a fixed line even for any given\nline-concentrated data.",
        "We demonstrate for the first time that new physics explaining the long\nstanding charged $B$ meson anomalies, $R(D^{(*)})$, can be the source of CP\nviolation that explains the observed baryon asymmetry of the universe (BAU). We\nconsider the general two Higgs doublet model with complex Yukawa couplings and\ncompute the BAU in the semiclassical formalism, using a novel analytic\napproximation for the latter. After imposing constraints from both flavor\nobservables and the electron electric dipole moment (eEDM), we find that a\nsignificant BAU can still be generated for a variety of benchmark points in the\nparameter space, assuming the occurrence of a sufficiently strong first order\nelectroweak phase transition. These scenarios, which explain both the\n$R(D^{(*)})$ flavor anomalies and the BAU, can be probed with future eEDM\nexperiments and Higgs factories measurements.",
        "The recent discovery of spontaneous chiral symmetry breaking has demonstrated\nthe possibility of discovering the exotic textures of ferromagnetic systems in\nliquid crystalline fluid ferro-electrics. We show that the polar smectic\nmesophase exhibited by the first molecule discovered to exhibit a spontaneously\nchiral ferroelectric nematic phase is also helical has a strongly varied\ntextural morphology depending in its thermal history and phase ordering.\nElectro-optic studies demonstrate that the two spontaneously chiral phases\nexhibit field induced phase transitions. For the nematic variant, this process\nis threshold-less and has no hysteresis while for the smectic it has a clear\nthreshold and shows hysteresis meaning this phase exhibits pseudo-ferrielectric\nswitching, the first of its kind for ferroelectric nematic like phases. We show\nthat helix formation can be both 1st and 2nd order but when it is 1st it is\naccompanied by pre-transitional helix formation in the preceding ferroelectric\nnematic phase.",
        "The aim of this paper is to state and prove existence and uniqueness results\nfor a general elliptic problem with homogeneous Neumann boundary conditions,\noften associated with image processing tasks like denoising. The novelty is\nthat we surpass the lack of coercivity of the Euler-Lagrange functional with an\ninnovative technique that has at its core the idea of showing that the minimum\nof the energy functional over a subset of the space $W^{1,p(x)}(\\Omega)$\ncoincides with the global minimum. The obtained existence result applies to\nmultiple-phase elliptic problems under remarkably weak assumptions.",
        "Motivated by the previous results by Coletti--de Lima--Gava--Luiz (2020) and\nShiozawa (2022), we study the fluctuation of the mixed elephant random walk in\nthe superdiffusive case, and prove the Central Limit Theorem and the Law of\nIterated Logarithm with subtracting a random drift.",
        "Sequential change-point detection plays a critical role in numerous\nreal-world applications, where timely identification of distributional shifts\ncan greatly mitigate adverse outcomes. Classical methods commonly rely on\nparametric density assumptions of pre- and post-change distributions, limiting\ntheir effectiveness for high-dimensional, complex data streams. This paper\nproposes a score-based CUSUM change-point detection, in which the score\nfunctions of the data distribution are estimated by injecting noise and\napplying denoising score matching. We consider both offline and online versions\nof score estimation. Through theoretical analysis, we demonstrate that\ndenoising score matching can enhance detection power by effectively controlling\nthe injected noise scale. Finally, we validate the practical efficacy of our\nmethod through numerical experiments on two synthetic datasets and a real-world\nearthquake precursor detection task, demonstrating its effectiveness in\nchallenging scenarios.",
        "A generalization of the determinant appears in particle physics in effective\nLagrangian interaction terms that model the chiral anomaly in Quantum\nChromodynamics (PRD 97 (2018) 9, 091901 PRD 109 (2024) 7, L071502), in\nparticular in connection to mesons. This \\textit{polydeterminant function},\nknown in the mathematical literature as a mixed discriminant, associates $N$\ndistinct $N\\times N$ complex matrices into a complex number and reduces to the\nusual determinant when all matrices are taken as equal. Here, we explore the\nmain properties of the polydeterminant applied to (quantum) fields by using a\nformalism and a language close to high-energy physics approaches. We discuss\nits use as a tool to write down novel Lagrangian terms and present an explicit\nillustrative model for mesons. Finally, the extension of the polydeterminant as\na function of tensors is shown.",
        "We consider the classical geometric problem of prescribing the scalar and the\nboundary mean curvature in the unit ball endowed with the standard Euclidean\nmetric. We will deal with the case of negative scalar curvature showing the\nexistence of infinitely many non-radial positive solutions when the dimension\nis larger or equal to 5. This is the first result of existence of solutions in\nthe case of negative prescribed scalar curvature problem in higher dimensions.",
        "We present an analysis of globular clusters (GCs) of dwarf galaxies in the\nPerseus galaxy cluster to explore the relationship between dwarf galaxy\nproperties and their GCs. Our focus is on GC numbers ($N_{\\rm GC}$) and GC\nhalf-number radii ($R_{\\rm GC}$) around dwarf galaxies, and their relations\nwith host galaxy stellar masses ($M_*$), central surface brightnesses\n($\\mu_0$), and effective radii ($R_{\\rm e}$). Interestingly, we find that at a\ngiven stellar mass, $R_{\\rm GC}$ is almost independent of the host galaxy\n$\\mu_0$ and $R_{\\rm e}$, while $R_{\\rm GC}\/R_{\\rm e}$ depends on $\\mu_0$ and\n$R_{\\rm e}$; lower surface brightness and diffuse dwarf galaxies show $R_{\\rm\nGC}\/R_{\\rm e}\\approx 1$ while higher surface brightness and compact dwarf\ngalaxies show $R_{\\rm GC}\/R_{\\rm e}\\approx 1.5$-$2$. This means that for dwarf\ngalaxies of similar stellar mass, the GCs have a similar median extent;\nhowever, their distribution is different from the field stars of their host.\nAdditionally, low surface brightness and diffuse dwarf galaxies on average have\na higher $N_{\\rm GC}$ than high surface brightness and compact dwarf galaxies\nat any given stellar mass. We also find that UDGs (ultra-diffuse galaxies) and\nnon-UDGs have similar $R_{\\rm GC}$, while UDGs have smaller $R_{\\rm GC}\/R_{\\rm\ne}$ (typically less than 1) and 3-4 times higher $N_{\\rm GC}$ than non-UDGs.\nExamining nucleated and not-nucleated dwarf galaxies, we find that for\n$M_*>10^8M_{\\odot}$, nucleated dwarf galaxies seem to have smaller $R_{\\rm GC}$\nand $R_{\\rm GC}\/R_{\\rm e}$, with no significant differences between their\n$N_{\\rm GC}$, except at $M_*<10^8M_{\\odot}$ where the nucleated dwarf galaxies\ntend to have a higher $N_{\\rm GC}$. Lastly, we explore the stellar-to-halo mass\nratio (SHMR) of dwarf galaxies and conclude that the Perseus cluster dwarf\ngalaxies follow the expected SHMR at $z=0$ extrapolated down to\n$M_*=10^6M_{\\odot}$.",
        "In the present study, we investigate the quasibound states, scalar cloud and\nsuperradiance of relativistic scalar fields bound to a rotating black hole in\nEinstein-Maxwell-Dilaton-Axion theory (Kerr-EMDA). We present the exact\neigensolutions of the governing Klein-Gordon equation in the black hole\nbackground. By imposing boundary conditions on the quasibound states, we are\nable to find the exact complex quasibound state frequencies of the\ncorresponding radial wave functions in terms of the confluent Heun polynomial.\nConsidering light scalar field limit of the obtained solution, we investigate\nthe scalar-black hole resonance configuration known as the scalar cloud. In\naddition, we obtain analytic relation between light scalar mass and black hole\nspin for scalar cloud. We explore a boson distribution function by linearly\nexpanding the radial wave function near the black hole's event horizon.\nMoreover, by applying the Damour-Ruffini method, this allows us to calculate\nthe Hawking radiation flux. In the final section, we consider propagating wave\nin a slowly rotating Kerr-EMDA black hole for bosons having much larger Compton\nwavelength comparing to the size of rotating black hole. This condition allows\nus to use the asymptotic matching technique to calculate the amplification\nfactor for scalar fields in the Kerr-EMDA black hole. We present the dependence\nof amplification factor on black hole parameters by graphical analysis.",
        "A classical result by Kreweras (1965) allows one to compute the number of\nplane partitions of a given skew shape and bounded parts as certain\ndeterminants. We prove that these determinants expand as polynomials with\nnonnegative coefficients. This result can be reformulated in terms of order\npolynomials of cell posets of skew shapes, and explains important positivity\nphenomena about the Ehrhart polynomials of shard polytopes, matroids, and order\npolytopes. Among other applications, we generalize a positivity statement from\nSchubert calculus by Fomin and Kirillov (1997) from straight shapes to skew\nshapes. We show that all shard polytopes are Ehrhart positive and, stronger,\nthat all fence posets, including the zig-zag poset, have order polynomials with\nnonnegative coefficients. We present a more general method for proving\npositivity which reduces to showing positivity of the linear terms of the order\npolynomials and we state conjectures on other positive classes of posets.",
        "Let $\\alpha$ be a Steinhaus random multiplicative function. For a wide class\nof multiplicative functions $f$ we construct a multiplicative chaos measure\narising from the Dirichlet series of $\\alpha f$, in the whole $L^1$-regime. Our\nmethod does not rely on the thick point approach or Gaussian approximation, and\nuses a modified second moment method with the help of an approximate Girsanov\ntheorem. We also employ the idea of weak convergence in $L^r$ to show that the\nlimiting measure is independent of the choice of the approximation schemes, and\nthis may be seen as a non-Gaussian analogue of Shamov's characterisation of\nmultiplicative chaos.\n  Our class of $f$-s consists of those for which the mean value of $|f(p)|^2$\nlies in $(0,1)$. In particular, it includes the indicator of sums of two\nsquares. As an application of our construction, we establish a generalised\ncentral limit theorem for the (normalised) sums of $\\alpha f$, with random\nvariance determined by the total mass of our measure.",
        "The mean of a random variable can be understood as a $\\textit{linear}$\nfunctional on the space of probability distributions. Quantum computing is\nknown to provide a quadratic speedup over classical Monte Carlo methods for\nmean estimation. In this paper, we investigate whether a similar quadratic\nspeedup is achievable for estimating $\\textit{non-linear}$ functionals of\nprobability distributions. We propose a quantum-inside-quantum Monte Carlo\nalgorithm that achieves such a speedup for a broad class of non-linear\nestimation problems, including nested conditional expectations and stochastic\noptimization. Our algorithm improves upon the direct application of the quantum\nmultilevel Monte Carlo algorithm introduced by An et al.. The existing lower\nbound indicates that our algorithm is optimal up polylogarithmic factors. A key\ninnovation of our approach is a new sequence of multilevel Monte Carlo\napproximations specifically designed for quantum computing, which is central to\nthe algorithm's improved performance.",
        "We study the topology of the space of all smooth asymptotically stable vector\nfields on $\\mathbb{R}^n$, as well as the space of all proper smooth Lyapunov\nfunctions for such vector fields. We prove that both spaces are path-connected\nand simply connected when $n\\neq 4,5$ and weakly contractible when $n\\leq 3$.\nMoreover, both spaces have the weak homotopy type of the nonlinear Grassmannian\nof submanifolds of $\\mathbb{R}^n$ diffeomorphic to the $n$-disc.\n  The proofs rely on Lyapunov theory and differential topology, such as the\nwork of Smale and Perelman on the generalized Poincar\\'{e} conjecture and\nresults of Smale, Cerf, and Hatcher on the topology of diffeomorphism groups of\ndiscs. Applications include a partial answer to a question of Conley, a\nparametric Hartman-Grobman theorem for nonyperbolic but asymptotically stable\nequilibria, and a parametric Morse lemma for degenerate minima. We also study\nthe related topics of hyperbolic equilibria, Morse minima, and relative\nhomotopy groups of the space of asymptotically stable vector fields inside the\nspace of those vanishing at a single point.",
        "This paper addresses the critical problem of co-optimizing the optimal\nlocations for orbital depots and the sequence of in-space servicing for a\nsatellite constellation. While most traditional studies used network\noptimization for this problem, assuming a fixed set of discretized nodes in the\nnetwork (i.e., a limited number of depot location candidates), this work is\nunique in that it develops a method to optimize the depot location in\ncontinuous space. The problem is formulated as mixed-integer nonlinear\nprogramming, and we propose a solution methodology that iteratively solves two\ndecoupled problems: one using mixed-integer linear programming and the other\nusing nonlinear programming with an analytic transfer solution. To demonstrate\nthe effectiveness of our approach, we apply this methodology to a case study\ninvolving a GPS satellite constellation. Numerical experiments confirm the\nstability of our proposed solutions.",
        "We investigate the performance of multigrid preconditioners for solving\nlinear systems arising from finite element discretizations of elliptic\ninterface problems using the Fictitious Domain with Distributed Lagrange\nMultipliers (FD-DLM) formulation. Numerical experiments are conducted using\ncontinuous and discontinuous finite element spaces for the Lagrange multiplier.\nResults indicate that multigrid is a promising preconditioner for problems in\nthe FD-DLM formulation.",
        "The present paper studies the Dirichlet spaces in balls and upper-half\nEuclidean spaces. As main results, we give identical characterizations of the\nDirichlet norms in the respective contexts as for the classical 2-D disc case\nproved by Douglas and Ahlfors.",
        "We continue the study of dispersed subspaces and disjointly non-singular\n(DNS) operators on Banach lattices using topological methods. In particular, we\nprovide a simple proof of the fact that in an order continuous Banach lattice\nan operator is DNS if and only if it is $n$-DNS, for some $n\\in\\mathbb{N}$. We\ncharacterize Banach lattices with order continuous dual in terms of dispersed\nsubspaces and absolute weak topology. We also connect these topics with the\nrecently launched study of phase retrieval in Banach lattices.",
        "Realizing topological flat bands with tailored single-particle Hilbert spaces\nis a critical step toward exploring many-body phases, such as those featuring\nanyonic excitations. One prominent example is the Kapit-Mueller model, a\nvariant of the Harper-Hofstadter model that stabilizes lattice analogs of the\nlowest Landau level states. The Kapit-Mueller model is constructed based on the\nPoisson summation rule, an exact lattice sum rule for coherent states. In this\nwork, we consider higher Landau-level generalizations of the Poisson summation\nrule, from which we derive families of parent Hamiltonians on a half-flux\nlattice which have exact flat bands whose flatband wavefunctions are lattice\nversion of higher Landau level states. Focusing on generic Bravais lattices\nwith only translation and inversion symmetries, we discuss how these symmetries\nenforced gaplessness and singular points for odd Landau level series, and how\nto achieve fully gapped parent Hamiltonians by mixing even and odd series. Our\nmodel points to a large class of tight-binding models with suitable energetic\nand quantum geometries that are potentially useful for realizing non-Abelian\nfractionalized states when interactions are included. The model exhibits fast\ndecay hopping amplitudes, making it potentially realizable with neutral atoms\nin optical lattices."
      ]
    }
  },
  {
    "id":2412.00173,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Turning single-molecule localization microscopy into a quantitative bioanalytical tool",
    "start_abstract":"Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b22"
      ],
      "title":[
        "A framework for evaluating the performance of SMLM cluster analysis algorithms"
      ],
      "abstract":[
        "This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation."
      ],
      "categories":[
        "eess.IV"
      ]
    },
    "list":{
      "title":[
        "MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length\n  Video Generation",
        "Out-of-distribution generalisation for learning quantum channels with\n  low-energy coherent states",
        "Formalising the intentional stance 2: a coinductive approach",
        "UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep\n  Reinforcement Learning",
        "MastermindEval: A Simple But Scalable Reasoning Benchmark",
        "Critical Unstable Qubits: an Application to $B^0\\bar{B}^0$-Meson System",
        "MobileSteward: Integrating Multiple App-Oriented Agents with\n  Self-Evolution to Automate Cross-App Instructions",
        "Simple Hamiltonians for Matrix Product State models",
        "A Reinforcement Learning Approach to Quiet and Safe UAM Traffic\n  Management",
        "Learning Visual Proxy for Compositional Zero-Shot Learning",
        "k-LLMmeans: Summaries as Centroids for Interpretable and Scalable\n  LLM-Based Text Clustering",
        "Non-homogeneous problem for the fractional wave equation with irregular\n  coefficients and data",
        "Multi-messenger detection of black hole binaries in dark matter spikes",
        "An overview of regularity results for the Laplacian and $p$-Laplacian in\n  metric spaces",
        "Temporal Feature Weaving for Neonatal Echocardiographic Viewpoint Video\n  Classification",
        "Detectability, Riccati Equations, and the Game-Based Control of\n  Discrete-Time MJLSs with the Markov Chain on a Borel Space",
        "An Open Source Validation System for Continuous Arterial Blood Pressure\n  Measuring Sensors",
        "Distribution Matching for Self-Supervised Transfer Learning",
        "Neural Reflectance Fields for Radio-Frequency Ray Tracing",
        "PolyPath: Adapting a Large Multimodal Model for Multi-slide Pathology\n  Report Generation",
        "Q-NL Verifier: Leveraging Synthetic Data for Robust Knowledge Graph\n  Question Answering",
        "Optimal-Reference Excited State Methods: Static Correlation at\n  Polynomial Cost with Single-Reference Coupled-Cluster Approaches",
        "Solving the Traveling Salesman Problem via Different Quantum Computing\n  Architectures",
        "Logarithmic Regret for Online KL-Regularized Reinforcement Learning",
        "Impact of photoevaporative winds in chemical models of externally\n  irradiated protoplanetary disks",
        "Gravitational waves from the E-model inflation with Gauss-Bonnet\n  correction",
        "Vector-Quantized Vision Foundation Models for Object-Centric Learning",
        "On the positivity of light-ray operators",
        "LiteChain: A Lightweight Blockchain for Verifiable and Scalable\n  Federated Learning in Massive Edge Networks"
      ],
      "abstract":[
        "Diffusion models are successful for synthesizing high-quality videos but are\nlimited to generating short clips (e.g., 2-10 seconds). Synthesizing sustained\nfootage (e.g. over minutes) still remains an open research question. In this\npaper, we propose MALT Diffusion (using Memory-Augmented Latent Transformers),\na new diffusion model specialized for long video generation. MALT Diffusion (or\njust MALT) handles long videos by subdividing them into short segments and\ndoing segment-level autoregressive generation. To achieve this, we first\npropose recurrent attention layers that encode multiple segments into a compact\nmemory latent vector; by maintaining this memory vector over time, MALT is able\nto condition on it and continuously generate new footage based on a long\ntemporal context. We also present several training techniques that enable the\nmodel to generate frames over a long horizon with consistent quality and\nminimal degradation. We validate the effectiveness of MALT through experiments\non long video benchmarks. We first perform extensive analysis of MALT in\nlong-contextual understanding capability and stability using popular long video\nbenchmarks. For example, MALT achieves an FVD score of 220.4 on 128-frame video\ngeneration on UCF-101, outperforming the previous state-of-the-art of 648.4.\nFinally, we explore MALT's capabilities in a text-to-video generation setting\nand show that it can produce long videos compared with recent techniques for\nlong text-to-video generation.",
        "When experimentally learning the action of a continuous variable quantum\nprocess by probing it with inputs, there will often be some restriction on the\ninput states used. One experimentally simple way to probe the channel is using\nlow-energy coherent states. Learning a quantum channel in this way presents\ndifficulties, due to the fact that two channels may act similarly on low energy\ninputs but very differently for high energy inputs. They may also act similarly\non coherent state inputs but differently on non-classical inputs. Extrapolating\nthe behaviour of a channel for more general input states from its action on the\nfar more limited set of low energy coherent states is a case of\nout-of-distribution generalisation. To be sure that such generalisation gives\nmeaningful results, one needs to relate error bounds for the training set to\nbounds that are valid for all inputs. We show that for any pair of channels\nthat act sufficiently similarly on low energy coherent state inputs, one can\nbound how different the input-output relations are for any (high energy or\nhighly non-classical) input. This proves out-of-distribution generalisation is\nalways possible for learning quantum channels using low energy coherent states.",
        "Given a stochastic process with inputs and outputs, how might its behaviour\nbe related to pursuit of a goal? We model this using 'transducers', objects\nthat capture only the external behaviour of a system and not its internal\nstate. A companion paper summarises our results for cognitive scientists; the\ncurrent paper gives formal definitions and proofs.\n  To formalise the concept of a system that behaves as if it were pursuing a\ngoal, we consider what happens when a transducer (a 'policy') is coupled to\nanother transducer that comes equipped with a success condition (a\n'teleo-environment'). An optimal policy is identified with a transducer that\nbehaves as if it were perfectly rational in the pursuit of a goal; our\nframework also allows us to model constrained rationality.\n  Optimal policies obey a version of Bellman's principle: a policy that's\noptimal in one time step will again be optimal in the next time step, but with\nrespect to a different teleo-environment (obtained from the original one by a\nmodified version of Bayesian filtering). This property sometimes also applies\nto the bounded-rational case; we give a sufficient condition.\n  A policy is deterministic if and only if there exists a teleo-environment for\nwhich it is uniquely optimal among the set of all policies; we relate this to\nclassical representation theorems from decision theory. This result need not\nhold in the bounded-rational case; we give an example related to the\nabsent-minded driver problem. The formalism is defined using coinduction,\nfollowing the style proposed by Czajka.",
        "Deep reinforcement learning (DRL) is widely applied to safety-critical\ndecision-making scenarios. However, DRL is vulnerable to backdoor attacks,\nespecially action-level backdoors, which pose significant threats through\nprecise manipulation and flexible activation, risking outcomes like vehicle\ncollisions or drone crashes. The key distinction of action-level backdoors lies\nin the utilization of the backdoor reward function to associate triggers with\ntarget actions. Nevertheless, existing studies typically rely on backdoor\nreward functions with fixed values or conditional flipping, which lack\nuniversality across diverse DRL tasks and backdoor designs, resulting in\nfluctuations or even failure in practice.\n  This paper proposes the first universal action-level backdoor attack\nframework, called UNIDOOR, which enables adaptive exploration of backdoor\nreward functions through performance monitoring, eliminating the reliance on\nexpert knowledge and grid search. We highlight that action tampering serves as\na crucial component of action-level backdoor attacks in continuous action\nscenarios, as it addresses attack failures caused by low-frequency target\nactions. Extensive evaluations demonstrate that UNIDOOR significantly enhances\nthe attack performance of action-level backdoors, showcasing its universality\nacross diverse attack scenarios, including single\/multiple agents,\nsingle\/multiple backdoors, discrete\/continuous action spaces, and sparse\/dense\nreward signals. Furthermore, visualization results encompassing state\ndistribution, neuron activation, and animations demonstrate the stealthiness of\nUNIDOOR. The source code of UNIDOOR can be found at\nhttps:\/\/github.com\/maoubo\/UNIDOOR.",
        "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing.",
        "We extend our previous work on a novel class of unstable qubits which we have\nidentified recently and called them Critical Unstable Qubits (CUQs). The\ncharacteristic property of CUQs is that the energy-level and decay-width\nvectors, ${\\bf E}$ and ${\\bf \\Gamma}$, are orthogonal to one another, and the\nkey parameter $r = |{\\bf \\Gamma}|\/|2{\\bf E}|$ is less than 1. Most remarkably,\nCUQs exhibit two atypical behaviours: (i) they display coherence-decoherence\noscillations in a co-decaying frame of the system described by a unit Bloch\nvector ${\\bf b}$, and (ii) the unit Bloch vector ${\\bf b}$ describing a pure\nCUQ sweeps out unequal areas during equal intervals of time, while rotating\nabout the vector ${\\bf E}$. The latter anharmonic phenomenon emerges beyond the\nusual oscillatory pattern due to the energy-level difference of the two-level\nquantum system, which governs an ordinary qubit. By making use of a Fourier\nseries decomposition, we define anharmonicity observables that quantify the\ndegree of non-sinusoidal oscillation of a CUQ. We apply the results of our\nformalism to the $B^0\\bar{B}^0$-meson system and derive, for the first time,\ngeneric upper limits on these new observables.",
        "Mobile phone agents can assist people in automating daily tasks on their\nphones, which have emerged as a pivotal research spotlight. However, existing\nprocedure-oriented agents struggle with cross-app instructions, due to the\nfollowing challenges: (1) complex task relationships, (2) diverse app\nenvironment, and (3) error propagation and information loss in multi-step\nexecution. Drawing inspiration from object-oriented programming principles, we\nrecognize that object-oriented solutions is more suitable for cross-app\ninstruction. To address these challenges, we propose a self-evolving\nmulti-agent framework named MobileSteward, which integrates multiple\napp-oriented StaffAgents coordinated by a centralized StewardAgent. We design\nthree specialized modules in MobileSteward: (1) Dynamic Recruitment generates a\nscheduling graph guided by information flow to explicitly associate tasks among\napps. (2) Assigned Execution assigns the task to app-oriented StaffAgents, each\nequipped with app-specialized expertise to address the diversity between apps.\n(3) Adjusted Evaluation conducts evaluation to provide reflection tips or\ndeliver key information, which alleviates error propagation and information\nloss during multi-step execution. To continuously improve the performance of\nMobileSteward, we develop a Memory-based Self-evolution mechanism, which\nsummarizes the experience from successful execution, to improve the performance\nof MobileSteward. We establish the first English Cross-APP Benchmark (CAPBench)\nin the real-world environment to evaluate the agents' capabilities of solving\ncomplex cross-app instructions. Experimental results demonstrate that\nMobileSteward achieves the best performance compared to both single-agent and\nmulti-agent frameworks, highlighting the superiority of MobileSteward in better\nhandling user instructions with diverse complexity.",
        "Matrix Product States (MPS) and Tensor Networks provide a general framework\nfor the construction of solvable models. The best-known example is the\nAffleck-Kennedy-Lieb-Tasaki (AKLT) model, which is the ground state of a 2-body\nnearest-neighbor parent Hamiltonian. We show that such simple parent\nHamiltonians for MPS models are, in fact, much more prevalent than hitherto\nknown: The existence of a single example with a simple Hamiltonian for a given\nchoice of dimensions already implies that any generic MPS with those dimensions\npossesses an equally simple Hamiltonian. We illustrate our finding by\ndiscussing a number of models with nearest-neighbor parent Hamiltonians, which\ngeneralize the AKLT model on various levels.",
        "Urban air mobility (UAM) is a transformative system that operates various\nsmall aerial vehicles in urban environments to reshape urban transportation.\nHowever, integrating UAM into existing urban environments presents a variety of\ncomplex challenges. Recent analyses of UAM's operational constraints highlight\naircraft noise and system safety as key hurdles to UAM system implementation.\nFuture UAM air traffic management schemes must ensure that the system is both\nquiet and safe. We propose a multi-agent reinforcement learning approach to\nmanage UAM traffic, aiming at both vertical separation assurance and noise\nmitigation. Through extensive training, the reinforcement learning agent learns\nto balance the two primary objectives by employing altitude adjustments in a\nmulti-layer UAM network. The results reveal the tradeoffs among noise impact,\ntraffic congestion, and separation. Overall, our findings demonstrate the\npotential of reinforcement learning in mitigating UAM's noise impact while\nmaintaining safe separation using altitude adjustments",
        "Compositional Zero-Shot Learning (CZSL) aims to recognize novel\nattribute-object compositions by leveraging knowledge from seen compositions.\nExisting methods align textual prototypes with visual features through\nVision-Language Models (VLMs), but they face two key limitations: (1) modality\ngaps hinder the discrimination of semantically similar composition pairs, and\n(2) single-modal textual prototypes lack fine-grained visual cues, creating\nbottlenecks in VLM-based CZSL. In this paper, we introduce Visual Proxy\nLearning, a novel approach that facilitates the learning of distinct visual\ndistributions, effectively reducing the modality gap and improving\ncompositional generalization performance. Specifically, we initialize visual\nproxies for various attributes, objects, and their compositions using text\nrepresentations. By optimizing the visual space, we capture fine-grained visual\ncues and guide the learning of more discriminative visual representations for\nattributes, objects and compositions. Furthermore, we propose an effective\nCross-Modal Joint Learning (CMJL) strategy that imposes cross-modal constraints\nbetween the original text-image space and the fine-grained visual space. This\napproach not only boosts generalization for previously unseen composition pairs\nbut also sharpens the discrimination of similar pairs, fostering more robust\nand precise learning. Extensive experiments demonstrate state-of-the-art\nperformance in closed-world scenarios and competitive open-world results across\nfour established CZSL benchmarks, validating the effectiveness of our approach\nin advancing compositional generalization.",
        "We introduce k-LLMmeans, a novel modification of the k-means clustering\nalgorithm that utilizes LLMs to generate textual summaries as cluster\ncentroids, thereby capturing contextual and semantic nuances often lost when\nrelying on purely numerical means of document embeddings. This modification\npreserves the properties of k-means while offering greater interpretability:\nthe cluster centroid is represented by an LLM-generated summary, whose\nembedding guides cluster assignments. We also propose a mini-batch variant,\nenabling efficient online clustering for streaming text data and providing\nreal-time interpretability of evolving cluster centroids. Through extensive\nsimulations, we show that our methods outperform vanilla k-means on multiple\nmetrics while incurring only modest LLM usage that does not scale with dataset\nsize. Finally, We present a case study showcasing the interpretability of\nevolving cluster centroids in sequential text streams. As part of our\nevaluation, we compile a new dataset from StackExchange, offering a benchmark\nfor text-stream clustering.",
        "In this paper, we consider the Cauchy problem for a non-homogeneous wave\nequation generated by the fractional Laplacian and involving different kinds of\nlower order terms. We allow the equation coefficients and data to be of\ndistributional type or less regular, having in mind the Dirac delta function\nand its powers, and we prove that the problem is well-posed in the sense of the\nconcept of very weak solutions. Moreover, we prove the uniqueness in an\nappropriate sense and the coherence of the very weak solution concept with\nclassical theory.",
        "We investigate the inspiral of a high mass-ratio black hole binary located in\nthe nucleus of a galaxy, where the primary central black hole is surrounded by\na dense dark matter spike formed through accretion during the black hole growth\nphase. Within this spike, dark matter undergoes strong self-annihilation,\nproducing a compact source of $\\gamma$-ray radiation that is highly sensitive\nto spike density, while the binary emits gravitational waves at frequencies\ndetectable by LISA. As the inspiralling binary interacts with the surrounding\ndark matter particles, it alters the density of the spike, thereby influencing\nthe $\\gamma$-ray flux from dark matter annihilation. We demonstrate that the\nspike self-annihilation luminosity decreases by $10\\%$ to $90\\%$ of its initial\nvalue, depending on the initial density profile and binary mass ratio, as the\nbinary sweeps through the LISA band. This presents a new opportunity to\nindirectly probe dark matter through multi-messenger observations of galactic\nnuclei.",
        "We review some regularity results for the Laplacian and $p$-Laplacian in\nmetric measure spaces. The focus is mainly on interior H\\\"older, Lipschitz and\nsecond-regularity estimates and on spaces supporting a Poincar\\'e inequality or\nhaving Ricci curvature bounded below.",
        "Automated viewpoint classification in echocardiograms can help\nunder-resourced clinics and hospitals in providing faster diagnosis and\nscreening when expert technicians may not be available. We propose a novel\napproach towards echocardiographic viewpoint classification. We show that\ntreating viewpoint classification as video classification rather than image\nclassification yields advantage. We propose a CNN-GRU architecture with a novel\ntemporal feature weaving method, which leverages both spatial and temporal\ninformation to yield a 4.33\\% increase in accuracy over baseline image\nclassification while using only four consecutive frames. The proposed approach\nincurs minimal computational overhead. Additionally, we publish the Neonatal\nEchocardiogram Dataset (NED), a professionally-annotated dataset providing\nsixteen viewpoints and associated echocardipgraphy videos to encourage future\nwork and development in this field. Code available at:\nhttps:\/\/github.com\/satchelfrench\/NED",
        "In this paper, detectability is first put forward for discrete-time Markov\njump linear systems with the Markov chain on a Borel space ($\\Theta$,\n$\\mathcal{B}(\\Theta)$). Under the assumption that the unforced system is\ndetectable, a stability criterion is established relying on the existence of\nthe positive semi-definite solution to the generalized Lyapunov equation. It\nplays a key role in seeking the conditions that guarantee the existence and\nuniqueness of the maximal solution and the stabilizing solution for a class of\ngeneral coupled algebraic Riccati equations (coupled-AREs). Then the\nnonzero-sum game-based control problem is tackled, and Nash equilibrium\nstrategies are achieved by solving four integral coupled-AREs. As an\napplication of the Nash game approach, the infinite horizon mixed\n$H_{2}\/H_{\\infty}$ control problem is studied, along with its solvability\nconditions. These works unify and generalize those set up in the case where the\nstate space of the Markov chain is restricted to a finite or countably infinite\nset. Finally, some examples are included to validate the developed results,\ninvolving a practical example of the solar thermal receiver.",
        "Measuring the blood pressure waveform is becoming a more frequently studied\narea. The development of sensor technologies opens many new ways to be able to\nmeasure high-quality signals. The development of such an aim-specific sensor\ncan be time-consuming, expensive, and difficult to test or validate with known\nand consistent waveforms. In this paper, we present an open source blood\npressure waveform simulator with an open source Python validation package to\nreduce development costs for early-stage sensor development and research. The\nsimulator mainly consists of 3D printed parts which technology has become a\nwidely available and cheap solution. The core part of the simulator is a 3D\nprinted cam that can be generated based on real blood pressure waveforms. The\nvalidation framework can create a detailed comparison between the signal\nwaveform used to design the cam and the measured time series from the sensor\nbeing validated. The presented simulator proved to be robust and accurate in\nshort- and long-term use, as it produced the signal waveform consistently and\naccurately. To validate this solution, a 3D force sensor was used, which was\nproven earlier to be able to measure high-quality blood pressure waveforms on\nthe radial artery at the wrist. The results showed high similarity between the\nmeasured and the nominal waveforms, meaning that comparing the normalized\nsignals, the RMSE value ranged from $0.0276 \\pm 0.0047$ to $0.0212 \\pm 0.0023$,\nand the Pearson correlation ranged from $0.9933 \\pm 0.0027$ to $0.9978 \\pm\n0.0005$. Our validation framework is available at\nhttps:\/\/github.com\/repat8\/cam-bpw-sim. Our hardware framework, which allows\nreproduction of the presented solution, is available at\nhttps:\/\/github.com\/repat8\/cam-bpw-sim-hardware. The entire design is an open\nsource project and was developed using free software.",
        "In this paper, we propose a novel self-supervised transfer learning method\ncalled Distribution Matching (DM), which drives the representation distribution\ntoward a predefined reference distribution while preserving augmentation\ninvariance. The design of DM results in a learned representation space that is\nintuitively structured and offers easily interpretable hyperparameters.\nExperimental results across multiple real-world datasets and evaluation metrics\ndemonstrate that DM performs competitively on target classification tasks\ncompared to existing self-supervised transfer learning methods. Additionally,\nwe provide robust theoretical guarantees for DM, including a population theorem\nand an end-to-end sample theorem. The population theorem bridges the gap\nbetween the self-supervised learning task and target classification accuracy,\nwhile the sample theorem shows that, even with a limited number of samples from\nthe target domain, DM can deliver exceptional classification performance,\nprovided the unlabeled sample size is sufficiently large.",
        "Ray tracing is widely employed to model the propagation of radio-frequency\n(RF) signal in complex environment. The modelling performance greatly depends\non how accurately the target scene can be depicted, including the scene\ngeometry and surface material properties. The advances in computer vision and\nLiDAR make scene geometry estimation increasingly accurate, but there still\nlacks scalable and efficient approaches to estimate the material reflectivity\nin real-world environment. In this work, we tackle this problem by learning the\nmaterial reflectivity efficiently from the path loss of the RF signal from the\ntransmitters to receivers. Specifically, we want the learned material\nreflection coefficients to minimize the gap between the predicted and measured\npowers of the receivers. We achieve this by translating the neural reflectance\nfield from optics to RF domain by modelling both the amplitude and phase of RF\nsignals to account for the multipath effects. We further propose a\ndifferentiable RF ray tracing framework that optimizes the neural reflectance\nfield to match the signal strength measurements. We simulate a complex\nreal-world environment for experiments and our simulation results show that the\nneural reflectance field can successfully learn the reflection coefficients for\nall incident angles. As a result, our approach achieves better accuracy in\npredicting the powers of receivers with significantly less training data\ncompared to existing approaches.",
        "The interpretation of histopathology cases underlies many important\ndiagnostic and treatment decisions in medicine. Notably, this process typically\nrequires pathologists to integrate and summarize findings across multiple\nslides per case. Existing vision-language capabilities in computational\npathology have so far been largely limited to small regions of interest, larger\nregions at low magnification, or single whole-slide images (WSIs). This limits\ninterpretation of findings that span multiple high-magnification regions across\nmultiple WSIs. By making use of Gemini 1.5 Flash, a large multimodal model\n(LMM) with a 1-million token context window, we demonstrate the ability to\ngenerate bottom-line diagnoses from up to 40,000 768x768 pixel image patches\nfrom multiple WSIs at 10X magnification. This is the equivalent of up to 11\nhours of video at 1 fps. Expert pathologist evaluations demonstrate that the\ngenerated report text is clinically accurate and equivalent to or preferred\nover the original reporting for 68% (95% CI: [60%, 76%]) of multi-slide\nexamples with up to 5 slides. While performance decreased for examples with 6\nor more slides, this study demonstrates the promise of leveraging the\nlong-context capabilities of modern LMMs for the uniquely challenging task of\nmedical report generation where each case can contain thousands of image\npatches.",
        "Question answering (QA) requires accurately aligning user questions with\nstructured queries, a process often limited by the scarcity of high-quality\nquery-natural language (Q-NL) pairs. To overcome this, we present Q-NL\nVerifier, an approach to generating high-quality synthetic pairs of queries and\nNL translations. Our approach relies on large language models (LLMs) to\ngenerate semantically precise natural language paraphrases of structured\nqueries. Building on these synthetic Q-NL pairs, we introduce a learned\nverifier component that automatically determines whether a generated paraphrase\nis semantically equivalent to the original query. Our experiments with the\nwell-known LC-QuAD 2.0 benchmark show that Q-NL Verifier generalizes well to\nparaphrases from other models and even human-authored translations. Our\napproach strongly aligns with human judgments across varying query complexities\nand outperforms existing NLP metrics in assessing semantic correctness. We also\nintegrate the verifier into QA pipelines, showing that verifier-filtered\nsynthetic data has significantly higher quality in terms of translation\ncorrectness and enhances NL to Q translation accuracy. Lastly, we release an\nupdated version of the LC-QuAD 2.0 benchmark containing our synthetic Q-NL\npairs and verifier scores, offering a new resource for robust and scalable QA.",
        "Accurate yet efficient modeling of chemical systems with pronounced static\ncorrelation in their excited states remains a significant challenge in quantum\nchemistry, as most electronic structure methods that can adequately capture\nstatic correlation scale factorially with system size. Researchers are often\nleft with no option but to use more affordable methods that may lack the\naccuracy required to model critical processes in photochemistry such as\nphotolysis, photocatalysis, and non-adiabatic relaxation. A great deal of work\nhas been dedicated to refining single-reference descriptions of static\ncorrelation in the ground state via ``addition-by-subtraction'' coupled cluster\nmethods such as pair coupled cluster with double substitutions (pCCD),\nsinglet-paired CCD (CCD0), triplet-paired CCD (CCD1), and CCD with frozen\nsinglet- or triplet-paired amplitudes (CCDf0\/CCDf1). By combining wave\nfunctions derived from these methods with the intermediate state representation\n(ISR), we gain insights into the extensibility of single-reference coupled\ncluster theory's coverage of static correlation to the excited state problem.\nOur CCDf1-ISR(2) approach is robust in the face of static correlation and\nprovides enough dynamical correlation to accurately predict excitation energies\nto within about 0.2~eV in small organic molecules. We also highlight distinct\nadvantages of the Hermitian ISR construction, such as the avoidance of\npathological failures of equation-of-motion methods for excited state potential\nenergy surface topology. Our results prompt us to continue exploring optimal\nsingle-reference theories (excited state approaches that leverage dependence on\nthe initial reference wave function) as a potentially economical approach to\nthe excited state static correlation problem.",
        "We study the application of emerging photonic and quantum computing\narchitectures to solving the Traveling Salesman Problem (TSP), a well-known\nNP-hard optimization problem. We investigate several approaches: Simulated\nAnnealing (SA), Quadratic Unconstrained Binary Optimization (QUBO-Ising)\nmethods implemented on quantum annealers and Optical Coherent Ising Machines,\nas well as the Quantum Approximate Optimization Algorithm (QAOA) and the\nQuantum Phase Estimation (QPE) algorithm on gate-based quantum computers.\n  QAOA and QPE were tested on the IBM Quantum platform. The QUBO-Ising method\nwas explored using the D-Wave quantum annealer, which operates on\nsuperconducting Josephson junctions, and the QCI Dirac machine, a nonlinear\noptoelectronic Ising machine. Gate-based quantum computers demonstrated\naccurate results for small TSP instances in simulation. However, real quantum\ndevices are hindered by noise and limited scalability. Circuit complexity grows\nwith problem size, restricting performance to TSP instances with a maximum of 6\nnodes.\n  In contrast, Ising-based architectures show improved scalability for larger\nproblem sizes. SQUID-based Ising machines can handle TSP instances with up to\n12 nodes, while nonlinear optoelectronic Ising machines extend this capability\nto 18 nodes. Nevertheless, the solutions tend to be suboptimal due to hardware\nlimitations and challenges in achieving ground state convergence as the problem\nsize increases. Despite these limitations, Ising machines demonstrate\nsignificant time advantages over classical methods, making them a promising\ncandidate for solving larger-scale TSPs efficiently.",
        "Recent advances in Reinforcement Learning from Human Feedback (RLHF) have\nshown that KL-regularization plays a pivotal role in improving the efficiency\nof RL fine-tuning for large language models (LLMs). Despite its empirical\nadvantage, the theoretical difference between KL-regularized RL and standard RL\nremains largely under-explored. While there is a recent line of work on the\ntheoretical analysis of KL-regularized objective in decision making\n\\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses\neither reduce to the traditional RL setting or rely on strong coverage\nassumptions. In this paper, we propose an optimism-based KL-regularized online\ncontextual bandit algorithm, and provide a novel analysis of its regret. By\ncarefully leveraging the benign optimization landscape induced by the\nKL-regularization and the optimistic reward estimation, our algorithm achieves\nan $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$\nlogarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote\nthe KL-regularization parameter, the cardinality of the reward function class,\nnumber of rounds, and the complexity of the reward function class. Furthermore,\nwe extend our algorithm and analysis to reinforcement learning by developing a\nnovel decomposition over transition steps and also obtain a similar logarithmic\nregret bound.",
        "Most stars form in dense clusters within high-mass star-forming regions,\nwhere protoplanetary disks may be exposed to intense UV radiation from nearby\nmassive stars. While previous studies have typically focused on isolated\nsources in low-mass regions, recent observational campaigns have started to\nprobe the chemistry of irradiated disks in unprecedented detail. Interpreting\nthis data requires complex chemical models, yet few studies have examined these\ndisks' chemistry, and none have incorporated the photoevaporative wind launched\nby external UV fields into their physical structure. In this study, we\npost-process radiation hydrodynamics simulations of externally irradiated\nprotoplanetary disks using the thermochemical code DALI, comparing models with\nand without the wind to assess its impact on disk chemistry. Results show that\nUV radiation is rapidly attenuated by the disk in both cases. However, thermal\nre-emission from the wind at longer wavelengths enhances disk heating,\nincreasing the gas-phase abundances of some key volatiles. Synthetic line\nfluxes vary by orders of magnitude between wind and windless models, primarily\ndue to emission from the wind itself rather than abundance variations within\nthe disk. Our findings demonstrate that the photoevaporative wind significantly\ninfluences the physical and chemical structure, and observational\ncharacteristics, of externally irradiated disks. We conclude that incorporating\nthe wind into chemical models is essential for accurately predicting chemical\nabundances, interpreting observations, and ultimately understanding planet\nformation in these common yet complex environments.",
        "In this work, we study the generation of gravitational waves in the E-model\ninflation with the scalar field non-minimally coupled to the Gauss-Bonnet term.\nConsidering a wall-crossing behavior in the moduli space, we parameterize the\ncoupling coefficient $\\xi$ as a step-like function, then if\n$V_{,\\phi}\\xi_{,\\phi}>0$, the Gauss-Bonnet term dominate the inflation\ndynamics, causing a short rapid-decline phase of the inflaton, and for\nappropriate parameter spaces, the mode equation of tensor perturbations\ndevelops a transient growing solution. This process generates a peak in the\ntensor perturbation power spectrum, corresponding to a peak in the\ngravitational wave energy spectrum around the nanohertz frequency band. Further\nmore, we investigate the feasibility of generating double peaks in the\ngravitational wave spectrum using a double-step coupling, For certain parameter\nchoices, one peak lies near nanohertz frequencies, while the other is around\nmillihertz frequencies. Consequently, these gravitational waves can be observed\nby the pulsar timing array and the space-based gravitational wave detectors\nsuch as LISA, simultaneously.",
        "Decomposing visual scenes into objects, as humans do, facilitates modeling\nobject relations and dynamics. Object-Centric Learning (OCL) achieves this by\naggregating image or video feature maps into object-level feature vectors,\nknown as \\textit{slots}. OCL's self-supervision via reconstructing the input\nfrom slots struggles with complex textures, thus many methods employ Vision\nFoundation Models (VFMs) to extract feature maps with better objectness.\nHowever, using VFMs merely as feature extractors does not fully unlock their\npotential. We propose Vector-Quantized VFMs for OCL (VQ-VFM-OCL, or VVO), where\nVFM features are extracted to facilitate object-level information aggregation\nand further quantized to strengthen supervision in reconstruction. Our VVO\nunifies OCL representatives into a concise architecture. Experiments\ndemonstrate that VVO not only outperforms mainstream methods on object\ndiscovery tasks but also benefits downstream tasks like visual prediction and\nreasoning. The source code is available in the supplement.",
        "We consider light-ray operators $\\mathcal{L}_{2n} = \\int\\mathrm{d} x^+\n(x^+)^{2n}T_{++}$, where $x^+$ is a null coordinate and $n$ a positive integer,\nin QFT in Minkowski spacetime in arbitrary dimensions. These operators are\ngeneralizations of the average null energy operator, which is positive. We give\na proof that the light-ray operators are positive in a non-minimally coupled\nbut otherwise free scalar field theory, and we present various arguments that\nshow that $\\mathcal{L}_2$ is positive semi-definite in two-dimensional\nconformal field theories. However, we are also able to construct reasonable\nstates which contradict these results by exploiting an infrared loophole in our\nproof. To resolve the resulting tension, we conjecture that the light-ray\noperators are positive in a more restrictive set of states. These states\nsatisfy stronger conditions than the Hadamard condition, and have the\ninterpretation of states that can be physically prepared. Our proposal is\nnontrivial even in two-dimensional CFT.",
        "Leveraging blockchain in Federated Learning (FL) emerges as a new paradigm\nfor secure collaborative learning on Massive Edge Networks (MENs). As the scale\nof MENs increases, it becomes more difficult to implement and manage a\nblockchain among edge devices due to complex communication topologies,\nheterogeneous computation capabilities, and limited storage capacities.\nMoreover, the lack of a standard metric for blockchain security becomes a\nsignificant issue. To address these challenges, we propose a lightweight\nblockchain for verifiable and scalable FL, namely LiteChain, to provide\nefficient and secure services in MENs. Specifically, we develop a distributed\nclustering algorithm to reorganize MENs into a two-level structure to improve\ncommunication and computing efficiency under security requirements. Moreover,\nwe introduce a Comprehensive Byzantine Fault Tolerance (CBFT) consensus\nmechanism and a secure update mechanism to ensure the security of model\ntransactions through LiteChain. Our experiments based on Hyperledger Fabric\ndemonstrate that LiteChain presents the lowest end-to-end latency and on-chain\nstorage overheads across various network scales, outperforming the other two\nbenchmarks. In addition, LiteChain exhibits a high level of robustness against\nreplay and data poisoning attacks."
      ]
    }
  },
  {
    "id":2412.00173,
    "research_type":"applied",
    "start_id":"b22",
    "start_title":"A framework for evaluating the performance of SMLM cluster analysis algorithms",
    "start_abstract":"This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation.",
    "start_categories":[
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Turning single-molecule localization microscopy into a quantitative bioanalytical tool"
      ],
      "abstract":[
        "Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Mechanism of Shape Symmetry Breaking in Surfactant Mediated Crystal\n  Growth",
        "A New Interpretation for the Hot Corona in Active Galactic Nuclei",
        "Dynamics of Quantum Correlations within the double Caldeira-Leggett\n  formalism",
        "Comparative Time-Series Analysis of Hip and Shoulder Rotation in\n  Baseball Bat Swings",
        "Nearly tight weighted 2-designs in complex projective spaces of every\n  dimension",
        "On the viability of higher order theories",
        "On the number of cofinalities of cuts in ultraproducts of linear orders",
        "Exceptional-point-controlled mode interaction in three-dimensional\n  microcavities represented by generalized Husimi functions",
        "Proxy Control Barrier Functions: Integrating Barrier-Based and\n  Lyapunov-Based Safety-Critical Control Design",
        "New exact spatially localized solutions of the (3 + 1) -dimensional\n  nonlinear non-dissipative quasi-geostrophic potential vorticity equation for\n  an exponential atmosphere",
        "Hierarchies from deterministic non-locality in theory space Anderson\n  localisation",
        "Efficient detection of entanglement by stimulated disentanglement",
        "Fractional Sobolev spaces related to an ultraparabolic operator",
        "Geography of irreducible 4-manifolds with order two fundamental group",
        "Evaluation codes arising from symmetric polynomials",
        "Intrinsic vs. Extrinsic Magnetic Transitions in Sr3Ru2O7 films",
        "Risk-Adjusted learning curve assessment using comparative probability\n  metrics",
        "High-Frequency Market Manipulation Detection with a Markov-modulated\n  Hawkes process",
        "Optimizer-Dependent Generalization Bound for Quantum Neural Networks",
        "First-ish Order Methods: Hessian-aware Scalings of Gradient Descent",
        "Comparative study of small-scale magnetic fields on $\\xi$ Boo A using\n  optical and near-infrared spectroscopy",
        "Remarks on ghost resonances",
        "Ecosystem Evolution and Drivers across the Tibetan Plateau and\n  Surrounding Regions",
        "Approximation of High-Dimensional Gibbs Distributions with Functional\n  Hierarchical Tensors",
        "Impermanent loss and Loss-vs-Rebalancing II",
        "Redefining $Q$ for multi-component discs of stars and gas",
        "Entropic optimal transport with congestion aversion Application to\n  relocation of drones",
        "A Million Three-body Binaries Caught by Gaia",
        "On the $\\mathcal{F}$-multicolor Tur\\'{a}n number of hypergraph graphs"
      ],
      "abstract":[
        "We present a dynamical model of crystal growth, in which it is possible to\nreliably achieve asymmetric products, beginning from symmetric initial\nconditions and growing within an isotropic environment. The asymmetric growth\nis the result of a positive feedback mechanism that amplifies the effect of\nthermal fluctuations in the coverage of surfactants on the growing crystalline\nfacets. Within our simple model, we are able to understand the kinetic and\nthermodynamic factors involved in both the onset of symmetry breaking and the\npersistence of anisotropic growth. We demonstrate that the mechanism is general\nby studying models with increasing complexity. We argue that this mechanism of\nsymmetry breaking underpins observations of colloidal, seed-mediated syntheses\nof single crystalline metal nanorods capped with strongly interacting\nsurfactants. The parameters within our model are related to experimental\nobservables such as the concentration, hydrophobicity, and binding strength of\nthe surfactants, which suggests a potential route to optimize the yield of\nasymmetric products in colloidal nanoparticle syntheses.",
        "This work attempts to provide a new interpretation for the hot corona in\nactive galactic nuclei (AGNs). A thin\n  parabolic magnetic reconnection layer, anchored at the innermost disk and\nextending along the boundary of the\n  magnetic tower for a few tens of gravitational radii, serves as a hard-X-ray\nsource above the disk. Within this\n  reconnection layer, the tearing instability leads to the formation of a chain\nof plasmoids, which contain relativistic\n  electrons that generate X-ray radiation through inverse-Compton (IC)\nscattering of soft photons emitted by the\n  accretion disk. Based on previous theoretical works and numerical\nsimulations, we develop a heuristic framework\n  to parameterize the geometry and magnetization of the reconnection layer, as\nwell as to compute both the power of\n  the IC-scattering radiation and the height of the reconnection layer. Our\nmodel allows for a quantitative\n  investigation of the relation between the height of the corona and the X-ray\nradiation luminosity, which can be\n  directly compared against the observed relation from X-ray reverberation\nmapping of individual AGNs. The\n  theoretical results are in good agreement with the observations of IRAS\n13224-3809, indicating the validation of\n  our model.",
        "This study investigates the effects of decoherence and squeezing on the\ndynamics of various kinds of quantum features--local quantum coherence, local\nentropy, EPR correlations, and entanglement--in the high-temperature limit of\nthe double Caldeira-Leggett model, focusing on initially squeezed states. We\ncompare two scenarios: (1) particles interacting with distinct environments and\n(2) particles coupled to a common environment. Our analysis reveals that common\nenvironments better preserve local coherence over time, whereas distinct\nenvironments accelerate decoherence. Temperature enhances decoherence and\nsuppresses coherence revivals, while squeezing affects transient dynamics but\nnot long-term coherence saturation. Local entropy increases with temperature\nand squeezing, though their underlying physical mechanisms differ. EPR\ncorrelations degrade due to environmental interactions, with squeezing\ninitially enhancing them but failing to prevent their eventual loss.\nEntanglement exhibits distinct behaviors: in separate environments, it\nundergoes sudden death, whereas in common environments, it experiences a dark\nperiod whose duration shortens with stronger squeezing. These findings provide\na comprehensive understanding of how decoherence and squeezing influence\nquantum correlations in open quantum systems.",
        "This study focuses on the rotation of the hips and shoulders during a\nbaseball bat swing, analyzing the time-series changes in rotational angles,\nrotational velocities, and axes using marker position data obtained from a\nmotion capture system with 12 infrared cameras. Previous studies have examined\nfactors such as ground reaction forces, muscle activation patterns, rotational\nenergy, angular velocity, and angles during a swing. However, to the best of\nour knowledge, the hip and shoulder rotational motions have not been adequately\nvisualized or compared. In particular, there is a lack of analysis regarding\nthe coordination and timing differences between hip and shoulder movements\nduring the swing. Therefore, this study aims to quantitatively compare the hip\nand shoulder rotational movements during the swing between skilled and\nunskilled players and visualizes the differences between them. Based on the\nobtained data, the study aims to improve the understanding of bat swing\nmechanics by visualizing the coordinated body movements during the swing.",
        "We use dense Sidon sets to construct small weighted projective 2-designs.\nThis represents quantitative progress on Zauner's conjecture.",
        "In physics, all dynamical equations that describe fundamental interactions\nare second order ordinary differential equations in the time derivatives. In\nthe literature, this property is traced back to a result obtained by\nOstrogradski in the mid 19th century, which is the technical basis of a 'no-go'\ntheorem for higher order theories. In this work, we review the connection of\nsymmetry properties with the order of dynamical equations, before reconsidering\nOstrogradski's result. Then, we show how Ostrogradski's conclusion is reached\nby applying to higher order theories concepts and method that have been\nspecifically developed for second order theories. We discuss a potential lack\nof consistency in this approach, to support the claim that Ostrogradski's\nresult applies to a class of higher order theories that is nowhere\nrepresentative of generic ones: we support this claim by giving an example of a\nhigher-order Lagrangian that is asymptotically stable, but that would be\nunstable under Ostrogradski's criterion. We also conclude that, when\nconsidering higher order theories as fundamental, we may need to reconsider and\nextend the conceptual framework on which our standard treatment of second order\ntheories is based.",
        "Suppose $\\kappa$ is a regular cardinal and $\\bar a=\\langle \\mu_i: i<\\kappa\n\\rangle$ is a non-decreasing sequence of regular cardinals. We study the set of\npossible cofinalities of cuts Pcut$(\\bar a)=\\{(\\lambda_1, \\lambda_2):$ for some\nultrafilter $D$ on $\\kappa$, $(\\lambda_1, \\lambda_2)$ is the cofinality of a\ncut of $\\prod\\limits_{i<\\kappa} \\mu_i \/ D \\}$.",
        "Non-Hermitian photonics has attracted significant interest and influences\nseveral key areas such as optical metamaterials, laser physics, and nonlinear\noptics. While non-Hermitian effects have been widely addressed in\ntwo-dimensional systems, we focus on realistic three-dimensional devices. To\nthis end we generalize established phase space methods from mesoscopic optics\nand introduce Husimi functions for three-dimensional systems that deepen the\ninsight and access to the mode morphology and their dynamics. We illustrate\nthat four-dimensional Husimi functions can be represented using a specific\nprojection in two dimensions and illustrate it for (conical) cylindrical\ncavities. The non-Hermitian character of the intrinsically open photonic\nsystems is in particular revealed when examining the TE and TM polarization\ncharacter of the resonance modes. Unlike the 2D case, polarization is not\nconserved in three-dimensional cavities, and we use generalized Husimi function\nto represent the interaction of polarization modes. We find their dynamics to\nbe ruled by a network of exceptional points in the parameter space spanned by\nthe refractive index and the cavity geometry tilt angle. This approach not only\nenhances our understanding of cavity modes but also aids in the design of more\nefficient photonic devices and systems.",
        "This work introduces a novel Proxy Control Barrier Function (PCBF) scheme\nthat integrates barrier-based and Lyapunov-based safety-critical control\nstrategies for strict-feedback systems with potentially unknown dynamics. The\nproposed method employs a modular design procedure, decomposing the original\nsystem into a proxy subsystem and a virtual tracking subsystem that are\ncontrolled by the control barrier function (CBF)-based and Lyapunov-based\ncontrollers, respectively. By integrating these separately designed\ncontrollers, the overall system's safety is ensured. Moreover, a new\nfilter-based disturbance observer is utilized to design a PCBF-based safe\ncontroller for strict-feedback systems subject to mismatched disturbances. This\napproach broadens the class of systems to which CBF-based methods can be\napplied and significantly simplifies CBF construction by requiring only the\nmodel of the proxy subsystem. The effectiveness of the proposed method is\ndemonstrated through numerical simulations.",
        "New exact spatially localized stationary solutions against the background of\na zonal flow are found for the (3+1)-dimensional nonlinear non-dissipative\nquasi-geostrophic potential vorticity equation, which describes Rossby waves\nand vortices in an exponential atmosphere. In total, three solutions are\npresented. The nonlinear boundary conditions with a flat bottom and a rigid lid\ngenerate an infinite discrete set of baroclinic modes for each solution. The\nsolutions show the possibility of existence of baroclinic dipoles in the\nexponential atmosphere, similar to baroclinic dipoles in the ocean. It is shown\nthat: a) a pair of vortices in the baroclinic dipole appears and disappears\nwhen the velocity of stationary motion changes; b) the baroclinic dipoles show\nthe ability to transfer warm or cold air depending on the polarity of the\nvortices in the dipole.",
        "The nearest-neighbour or local mass terms in theory space among quantum\nfields, with their generic disordered values, are known to lead to the\nlocalisation of mass eigenstates, analogous to Anderson localisation in a\none-dimensional lattice. This mechanism can be used to create an exponential\nhierarchy in the coupling between two fields by placing them at opposite ends\nof the lattice chain. Extending this mechanism, we show that when copies of\nsuch fields are appropriately attached to the lattice chain, it leads to the\nemergence of multiple massless modes. These vanishing masses are a direct\nconsequence of the locality of interactions in theory space. The latter may\nbreak down in an ordered and deterministic manner through quantum effects if\nadditional interactions exist among the chain fields. Such non-locality can\ninduce small masses for the otherwise massless modes without necessarily\ndelocalising the mass eigenstates. We provide examples of interactions that\npreserve or even enhance localisation. Applications to flavour hierarchies,\nneutrino mass, and the $\\mu$-problem in supersymmetric theories are discussed.",
        "Standard detection of entanglement relies on local measurements of the\nindividual particles, evaluating their correlations in post-processing. For\ntime-energy entangled photons, either times $(t_{1},t_{2})$ or energies\n$(E_{1},E_{2})$ are measured, but not both due to the mutual quantum\nuncertainty, providing only partial information of the entanglement. In\nprinciple, a global detector could recover the complete information of\nentanglement in a single shot if it could measure the combined correlated\nvariables $(t_{1}-t_{2})$ and $(E_{1}+E_{2})$ without measuring the individual\nenergies or times. Such a global measurement is possible using the reverse\ndisentangling interaction, like sum-frequency generation (SFG), but nonlinear\ninteractions at the single-photon level are exceedingly inefficient. We\novercome this barrier by stimulating the nonlinear SFG interaction with a\nstrong pump, thereby measuring both the energy-sum (SFG spectrum) and the\ntime-difference (response to group delay\/dispersion) simultaneously and\nefficiently. We generate bi-photons with extreme time-energy entanglement\n(octave-spanning spectrum of 113THz) and measure a relative uncertainty between\ntime-difference and energy-sum of\n$\\Delta(t_1-t_2)\\Delta(E_1+E_2)\\approx\\!2\\!\\cdot\\!10^{-13}h$, violating the\nclassical bound by more than 12 orders of magnitude. The presented coherent SFG\ndramatically enhances the detection SNR compared to standard methods since it\nideally rejects erroneous coincidences in both time and energy, paving the way\nfor sensing applications, such as quantum illumination (radar) and more.",
        "We propose a functional framework of fractional Sobolev spaces for a class of\nultra-parabolic Kolmogorov type operators satisfying the weak H\\\"ormander\ncondition. We characterize these spaces as real interpolation of natural order\nintrinic Sobolev spaces recently introduced in [27], and prove continuous\nembeddings into $L^p$ and intrinsic H\\\"older spaces from [24]. These embeddings\nnaturally extend the standard Euclidean ones, coherently with the homogeneous\nstructure of the associated Kolmogorov group. Our approach to interpolation is\nbased on approximation of intrinsically regular functions, the latter heavily\nrelying on integral estimates of the intrinsic Taylor remainder. The embeddings\nexploit the aforementioned interpolation property and the corresponding\nembeddings of natural order intrinsic spaces.",
        "Let $R$ be a closed, oriented topological 4-manifold whose Euler\ncharacteristic and signature are denoted by $e$ and $\\sigma$. We show that if\n$R$ has order two $\\pi_1$, odd intersection form, and $2e + 3\\sigma \\geq 0$,\nthen for all but seven $(e, \\sigma)$ coordinates, $R$ admits an irreducible\nsmooth structure. We accomplish this by performing a variety of operations on\nirreducible simply-connected 4-manifolds to build 4-manifolds with order two\n$\\pi_1$. These techniques include torus surgeries, symplectic fiber sums,\nrational blow-downs, and numerous constructions of Lefschetz fibrations,\nincluding a new approach to equivariant fiber summing.",
        "Datta and Johnsen (Des. Codes and Cryptogr., {\\bf{91}} (2023), 747-761)\nintroduced a new family of evalutation codes in an affine space of dimension\n$\\ge 2$ over a finite field $\\mathbb{F}_q$ where linear combinations of\nelementary symmetric polynomials are evaluated on the set of all points with\npairwise distinct coordinates. In this paper, we propose a generalization by\ntaking low dimensional linear systems of symmetric polynomials. Computation for\nsmall values of $q=7,9$ shows that carefully chosen generalized Datta-Johnsen\ncodes $\\left[\\frac{1}{2}q(q-1),3,d\\right]$ have minimum distance $d$ equal to\nthe optimal value minus 1.",
        "In scientific research, both positive and negative results play crucial role\nin advancing the field. Negative results provide valuable insights that can\nguide future experiments and prevent repeated failures. Here we present our\ngrowth attempts of Sr3Ru2O7 thin films using the hybrid molecular beam epitaxy.\nX-ray diffraction suggests nominally phase-pure films. A combination of\nmagnetoresistance and magnetization measurements exhibits an onset of\nferromagnetism at 170 K and 100 K, along with a metamagnetic-like transition at\n40 K. These results could initially be interpreted as intrinsic behavior of\nstrain-engineered Sr3Ru2O7 films. However, detailed microstructural analysis\nreveals intergrowths of Sr2RuO4, Sr4Ru3O10, and SrRuO3 phases, dispersed\nthroughout the film. Our findings suggest that the Sr3Ru2O7 films are likely\nparamagnetic, with the observed ferromagnetism arising from the Sr4Ru3O10 and\nSrRuO3 phases. Our results highlight the need for detailed microstructural\nanalysis when interpreting new material properties influenced by strain and\nmicrostructure.",
        "Surgical learning curves are graphical tools used to evaluate a trainee's\nprogress in the early stages of their career and determine whether they have\nachieved proficiency after completing a specified number of surgeries.\nCumulative sum (CUSUM) techniques are commonly used to assess learning curves\ndue to their simplicity, but they face criticism for relying on fixed\nperformance thresholds and lacking interpretability. This paper introduces a\nrisk-adjusted surgical learning curve assessment (SLCA) method that focuses on\nestimation rather than hypothesis testing, as seen in CUSUM methods. The method\nis designed to accommodate right-skewed outcomes, such as surgery durations,\ncharacterized by the Weibull distribution. To evaluate the learning process,\nthe SLCA approach estimates comparative probability metrics that assess the\nlikelihood of a clinically important difference between the trainee's\nperformance and a standard. Expecting improvement over time, we use weighted\nestimating equations to give greater weight to recent outcomes. Compared to\nCUSUM methods, SLCA offers enhanced interpretability, avoids reliance on\nexternally defined performance levels, and emphasizes assessing clinical\nequivalence or noninferiority. We demonstrate the method's effectiveness\nthrough a colorectal surgery dataset case study and a numerical study.",
        "This work focuses on a self-exciting point process defined by a Hawkes-like\nintensity and a switching mechanism based on a hidden Markov chain. Previous\nworks in such a setting assume constant intensities between consecutive events.\nWe extend the model to general Hawkes excitation kernels that are piecewise\nconstant between events. We develop an expectation-maximization algorithm for\nthe statistical inference of the Hawkes intensities parameters as well as the\nstate transition probabilities. The numerical convergence of the estimators is\nextensively tested on simulated data. Using high-frequency cryptocurrency data\non a top centralized exchange, we apply the model to the detection of anomalous\nbursts of trades. We benchmark the goodness-of-fit of the model with the\nMarkov-modulated Poisson process and demonstrate the relevance of the model in\ndetecting suspicious activities.",
        "Quantum neural networks (QNNs) play a pivotal role in addressing complex\ntasks within quantum machine learning, analogous to classical neural networks\nin deep learning. Ensuring consistent performance across diverse datasets is\ncrucial for understanding and optimizing QNNs in both classical and quantum\nmachine learning tasks, but remains a challenge as QNN's generalization\nproperties have not been fully explored. In this paper, we investigate the\ngeneralization properties of QNNs through the lens of learning algorithm\nstability, circumventing the need to explore the entire hypothesis space and\nproviding insights into how classical optimizers influence QNN performance. By\nestablishing a connection between QNNs and quantum combs, we examine the\ngeneral behaviors of QNN models from a quantum information theory perspective.\nLeveraging the uniform stability of the stochastic gradient descent algorithm,\nwe propose a generalization error bound determined by the number of trainable\nparameters, data uploading times, dataset dimension, and classical optimizer\nhyperparameters. Numerical experiments validate this comprehensive\nunderstanding of QNNs and align with our theoretical conclusions. As the first\nexploration into understanding the generalization capability of QNNs from a\nunified perspective of design and training, our work offers practical insights\nfor applying QNNs in quantum machine learning.",
        "Gradient descent is the primary workhorse for optimizing large-scale problems\nin machine learning. However, its performance is highly sensitive to the choice\nof the learning rate. A key limitation of gradient descent is its lack of\nnatural scaling, which often necessitates expensive line searches or heuristic\ntuning to determine an appropriate step size. In this paper, we address this\nlimitation by incorporating Hessian information to scale the gradient\ndirection. By accounting for the curvature of the function along the gradient,\nour adaptive, Hessian-aware scaling method ensures a local unit step size\nguarantee, even in nonconvex settings. Near a local minimum that satisfies the\nsecond-order sufficient conditions, our approach achieves linear convergence\nwith a unit step size. We show that our method converges globally under a\nsignificantly weaker version of the standard Lipschitz gradient smoothness\nassumption. Even when Hessian information is inexact, the local unit step size\nguarantee and global convergence properties remain valid under mild conditions.\nFinally, we validate our theoretical results empirically on a range of convex\nand nonconvex machine learning tasks, showcasing the effectiveness of the\napproach.",
        "Magnetic field investigations of Sun-like stars, using Zeeman splitting of\nnon-polarised spectra, in the optical and H-band have found significantly\ndifferent magnetic field strengths for the same stars, the cause of which is\ncurrently unknown. We aim to further investigate this issue by systematically\nanalysing the magnetic field of $\\xi$ Boo A, a magnetically active G7 dwarf,\nusing spectral lines at different wavelengths. We used polarised radiative\ntransfer accounting for the departures from local thermodynamic equilibrium to\ngenerate synthetic spectra. To find the magnetic field strengths in the\noptical, H-band, and K-band, we employed MCMC sampling analysis of\nhigh-resolution spectra observed with the spectrographs CRIRES$^+$, ESPaDOnS,\nNARVAL, and UVES. We also determine the formation depth of different lines by\ncalculating the contribution functions for each line employed in the analysis.\nWe find that the magnetic field strength discrepancy between lines in the\noptical and H-band persists even when treating the different wavelength regions\nconsistently. In addition, the magnetic measurements derived from the K-band\nappear to more closely align with the optical. The H-band appears to yield\nmagnetic field strengths $\\sim$ 0.4 kG with a statistically significant\nvariation while the optical and K-band is stable at $\\sim$ 0.6 kG for\nobservations spanning about two decades. The contribution functions reveal that\nthe optical lines form at a significantly higher altitude in the photosphere\ncompared to those in the H- and K-band. While we find that the discrepancy\nremains, the variation of formation depths could indicate that the disagreement\nbetween magnetic field measurements obtained at different wavelengths is linked\nto the variation of the magnetic field along the line of sight and between\ndifferent structures, such as star spots and faculae, in the stellar\nphotosphere.",
        "In this paper we study various aspects of ghost resonances: the resummation\nthat leads to the dressed propagator, the poles locations, the analytic\ncontinuation into the second Riemann sheet and the spectral representations in\nboth first and second sheets. In particular, we show that for real masses above\nthe multiparticle threshold the ghost propagator has a pair of complex\nconjugate poles in the first sheet, unlike the case of an ordinary unstable\nresonance which has no pole in the first sheet but a complex conjugate pair in\nthe second sheet. Mathematical and physical implications of this feature are\ndiscussed. We also clarify an important point regarding the two absorptive\ncontributions of a ghost propagator in the narrow-width approximation.\nFurthermore, we argue that finite-time quantum field theories are needed to\nconsistently derive the dressed ghost propagator and capture the true physical\nproperties of ghost resonances. Throughout the work, different prescriptions to\ndefine the ghost propagator on the real axis are considered: Feynman,\nanti-Feynman and fakeon prescriptions.",
        "The Tibetan Plateau (TP) and surrounding regions, vital to global energy and\nwater cycles, are profoundly influenced by climate change and anthropogenic\nactivities. Despite widespread attention to vegetation greening across the\nregion since the 1980s, its underlying mechanisms remain poorly understood.\nThis study employs the eigen microstates method to quantify vegetation greening\ndynamics using long-term remote sensing and reanalysis data. We identify two\ndominant modes that collectively explain more than 61% of the vegetation\ndynamics. The strong seasonal heterogeneity in the southern TP, primarily\ndriven by radiation and agricultural activities, is reflected in the first\nmode, which accounts for 46.34% of the variance. The second mode, which\nexplains 15% of the variance, is closely linked to deep soil moisture (SM3, 28\ncm to 1 m). Compared to precipitation and surface soil moisture (SM1 and SM2, 0\nto 28 cm), our results show that deep soil moisture exerts a stronger and more\nimmediate influence on vegetation growth, with a one-month response time. This\nstudy provides a complexity theory-based framework to quantify vegetation\ndynamics and underscores the critical influence of deep soil moisture on\ngreening patterns in the TP.",
        "The numerical representation of high-dimensional Gibbs distributions is\nchallenging due to the curse of dimensionality manifesting through the\nintractable normalization constant calculations. This work addresses this\nchallenge by performing a particle-based high-dimensional parametric density\nestimation subroutine, and the input to the subroutine is Gibbs samples\ngenerated by leveraging advanced sampling techniques. Specifically, to generate\nGibbs samples, we employ ensemble-based annealed importance sampling, a\npopulation-based approach for sampling multimodal distributions. These samples\nare then processed using functional hierarchical tensor sketching, a\ntensor-network-based density estimation method for high-dimensional\ndistributions, to obtain the numerical representation of the Gibbs\ndistribution. We successfully apply the proposed approach to complex\nGinzburg-Landau models with hundreds of variables. In particular, we show that\nthe approach proposed is successful at addressing the metastability issue under\ndifficult numerical cases.",
        "This paper examines the relationship between impermanent loss (IL) and\nloss-versus-rebalancing (LVR) in automated market makers (AMMs). Our main focus\nis on statistical properties, the impact of fees, the role of block times, and,\nrelated to the latter, the continuous time limit. We find there are three\nrelevant regimes: (i) very short times where LVR and IL are identical; (ii)\nintermediate time where LVR and IL show distinct distribution functions but are\nconnected via the central limit theorem exhibiting the same expectation value;\n(iii) long time behavior where both the distribution functions and averages are\ndistinct. Subsequently, we study how fees change this dynamics with a special\nfocus on competing time scales like block times and 'arbitrage times'.",
        "We point out a fundamental mismatch in the $Q$ stability parameter for\nGalactic discs: Toomre's $Q = 1$ defines the boundary between axisymmetric\nstability\/instability, while simulations, observations, and theoretical\nexpectations apply $Q$ in the region $Q > 1$ as a measure for spiral activity\n(e.g. swing amplification), for which $Q$ has not been designed. We suggest to\nredefine $Q$ to keep $Q = 1$ as the stability boundary, but to equally yield a\nconsistent map between $Q$ and the maximum swing amplification factor. Using\nthe Goldreich-Lynden-Bell formalism, we find that particularly the $Q$ for gas\ndiscs has been mismatched, and should be redefined to close to the square of\nthe traditional definition. We provide new formulations of $Q$ for simple,\ntwo-component, and multi-component discs, including a discussion of vertically\nextended discs, providing a simple iterative formula for which we also provide\ncode. We find $Q \\approx 1.58$ for the Solar Neighbourhood under our\ndefinition, closer to results from simulations. We compare the Milky Way and\nM74, showing that, consistent with observations, the theory suggests a higher\n$m$ number for the Milky Way (arguing against a 2-arm pattern) for\nstellar-dominated patterns. Gas instability arises at much smaller scales ($m\n\\gtrsim 10$), and we link both M74's gas pattern and local spurs in the Milky\nWay to this gas instability rather than stellar spiral arms.",
        "We present a mathematical framework for tempo-spatial entropic optimal\ntransport, motivated by the problem of efficiently routing drones back to\nlogistics centers. To address collision risk, we incorporate a convex penalty\nterm into the transport model. We propose the Sinkhorn-Frank-Wolfe algorithm, a\nnumerically efficient method with theoretical convergence guarantees, and\ndemonstrate its effectiveness through experiments on synthetic datasets. Our\napproach provides a foundation for optimizing large-scale autonomous drone\nlogistics while ensuring safe and efficient transportation.",
        "Gaia observations have revealed over a million stellar binary candidates\nwithin ~1 kpc of the Sun, predominantly characterized by orbital separations\n>10^3 AU and eccentricities >0.7. The prevalence of such wide, eccentric\nbinaries has proven challenging to explain through canonical binary formation\nchannels. However, recent advances in our understanding of three-body binary\nformation (3BBF) -- new binary assembly by the gravitational scattering of\nthree unbound bodies -- have shown that 3BBF in star clusters can efficiently\ngenerate wide, highly eccentric binaries. We further explore this possibility\nby constructing a semi-analytic model of the Galactic binary population in the\nsolar neighborhood, originating from 3BBF in star clusters. The model relies on\n3BBF scattering experiments to determine how the 3BBF rate and resulting binary\nproperties scale with local stellar density and velocity dispersion. We then\nmodel the Galactic star cluster population, incorporating up-to-date\nprescriptions for the Galaxy's star formation history as well as the birth\nproperties and internal evolution of its star clusters. Finally, we account for\nbinary destruction induced by perturbations from stellar interactions before\ncluster escape and and for subsequent changes to binary orbital elements by\ndynamical interactions in the Galactic field. Without any explicit fine-tuning,\nour model closely reproduces both the total number of Gaia's wide binaries and\ntheir separation distribution, and qualitatively matches the eccentricity\ndistribution, suggesting that 3BBF may be an important formation channel for\nthese enigmatic systems.",
        "Recently, Imolay, Karl, Nazy and V\\'{a}li explored a generalization of\nTur\\'{a}n's forbidden subgraph problem and Ruzsa-Szrmer\\'{e}di $(6,3)$-problem.\nThey specifically studied the following question: for two graphs $F$ and $G$,\ndetermine the maximum number of edge-disjoint copies of $F$ in a set of $n$\nvertices such that there is no copies of $G$ whose edges come from different\n$F$-copies. The maximum number is denoted by $ex_F(n,G)$ and is called the {\\em\n$F$-multicolor Tur\\'{a}n number} of $G$. One of their main results is that\n$ex_F(n,G)=o(n^2)$ if and only if there exists a homomorphism from $G$ to $F$.\nWe generalize the result to uniformly hypergraph using main tools of Hypergraph\nRegularity Lemma and Counting Lemma."
      ]
    }
  },
  {
    "id":2411.1945,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "start_abstract":"It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products.",
    "start_categories":[
      "physics.gen-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Auto-Encoding Variational Bayes"
      ],
      "abstract":[
        "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "ALPET: Active Few-shot Learning for Citation Worthiness Detection in\n  Low-Resource Wikipedia Languages",
        "Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline\n  Comparison for Semantic Similarity",
        "ResBench: Benchmarking LLM-Generated FPGA Designs with Resource\n  Awareness",
        "D3PO: Preference-Based Alignment of Discrete Diffusion Models",
        "Pre-Equalization Aided Grant-Free Massive Access in Massive MIMO System",
        "MSV-Mamba: A Multiscale Vision Mamba Network for Echocardiography\n  Segmentation",
        "A stochastic programming approach for the scheduling of medical\n  interpreting service under uncertainty",
        "Extinction and Extirpation Conditions in Coalescent and Ecotonal\n  Metacommunities",
        "A dynamical proof of Matui's absorption theorem",
        "Semantic Communications Services within Generalist Operated Networks",
        "Unified Approaches in Self-Supervised Event Stream Modeling: Progress\n  and Prospects",
        "A note on finiteness properties of vertex stabilisers",
        "Universality of Packing Dimension Estimates for Spectral Measures of\n  Quasiperiodic Operators: Monotone Potentials",
        "GameFactory: Creating New Games with Generative Interactive Videos",
        "Handling Uncertainty in Health Data using Generative Algorithms",
        "Graph Neural Networks for Efficient AC Power Flow Prediction in Power\n  Grids",
        "First Token Probability Guided RAG for Telecom Question Answering",
        "Optimal Traffic Allocation for Multi-Slot Sponsored Search: Balance of\n  Efficiency and Fairness",
        "Local time-integration for Friedrichs' systems",
        "On the Resolution of Partial Differential Equations for Lattice\n  Structures on Smooth Manifolds",
        "Grothendieck rings of localizations of the integers as ordered abelian\n  groups",
        "Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models\n  for Wireless Networks",
        "I2V3D: Controllable image-to-video generation with 3D guidance",
        "Spin-$s$ $Q$-systems: Twist and Open Boundaries",
        "Counterexamples to a conjecture of Adams",
        "Serenade: A Singing Style Conversion Framework Based On Audio Infilling",
        "Comparison of CNN-based deep learning architectures for unsteady CFD\n  acceleration on small datasets",
        "Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented\n  Manipulation",
        "Radial symmetry and sharp asymptotic behaviors of nonnegative solutions\n  to weighted doubly $D^{1,p}$-critical quasi-linear nonlocal elliptic\n  equations with Hardy potential"
      ],
      "abstract":[
        "Citation Worthiness Detection (CWD) consists in determining which sentences,\nwithin an article or collection, should be backed up with a citation to\nvalidate the information it provides. This study, introduces ALPET, a framework\ncombining Active Learning (AL) and Pattern-Exploiting Training (PET), to\nenhance CWD for languages with limited data resources. Applied to Catalan,\nBasque, and Albanian Wikipedia datasets, ALPET outperforms the existing CCW\nbaseline while reducing the amount of labeled data in some cases above 80\\%.\nALPET's performance plateaus after 300 labeled samples, showing it suitability\nfor low-resource scenarios where large, labeled datasets are not common. While\nspecific active learning query strategies, like those employing K-Means\nclustering, can offer advantages, their effectiveness is not universal and\noften yields marginal gains over random sampling, particularly with smaller\ndatasets. This suggests that random sampling, despite its simplicity, remains a\nstrong baseline for CWD in constraint resource environments. Overall, ALPET's\nability to achieve high performance with fewer labeled samples makes it a\npromising tool for enhancing the verifiability of online content in\nlow-resource language settings.",
        "This paper investigates the efficacy of RWKV, a novel language model\narchitecture known for its linear attention mechanism, for generating sentence\nembeddings in a zero-shot setting. I conduct a layer-wise analysis to evaluate\nthe semantic similarity captured by embeddings from different hidden layers of\na pre-trained RWKV model. The performance is assessed on the Microsoft Research\nParaphrase Corpus (MRPC) dataset using Spearman correlation and compared\nagainst a GloVe-based baseline. My results indicate that while RWKV embeddings\ncapture some semantic relatedness, they underperform compared to the GloVe\nbaseline in terms of Spearman correlation. I also analyze the inference time\nand GPU memory usage, highlighting the computational trade-offs associated with\nRWKV embeddings. The findings suggest that while RWKV offers potential\nadvantages in terms of linear scaling, its zero-shot sentence embedding quality\nfor semantic similarity tasks requires further investigation and potential\ntask-specific fine-tuning to match or exceed simpler baselines.",
        "Field-Programmable Gate Arrays (FPGAs) are widely used in modern hardware\ndesign, yet writing Hardware Description Language (HDL) code for FPGA\nimplementation remains a complex and time-consuming task. Large Language Models\n(LLMs) have emerged as a promising tool for HDL generation, but existing\nbenchmarks for LLM-based code generation primarily focus on functional\ncorrectness while overlooking hardware resource usage. Furthermore, current\nbenchmarks offer limited diversity and do not fully represent the wide range of\nreal-world FPGA applications. To address these shortcomings, we introduce\nResBench, the first resource-focused benchmark explicitly designed to\ndistinguish between resource-optimized and inefficient LLM-generated HDL code.\nResBench consists of 56 problems across 12 categories, covering applications\nfrom finite state machines to financial computing. Our open-source evaluation\nframework automatically tests LLMs by generating Verilog code, verifying\ncorrectness, and measuring resource usage. The experiments, which primarily\nanalyze Lookup Table (LUT) usage, reveal significant differences among LLMs,\ndemonstrating ResBench's capability to identify models that generate more\nresource-optimized FPGA designs.",
        "Diffusion models have achieved state-of-the-art performance across multiple\ndomains, with recent advancements extending their applicability to discrete\ndata. However, aligning discrete diffusion models with task-specific\npreferences remains challenging, particularly in scenarios where explicit\nreward functions are unavailable. In this work, we introduce Discrete Diffusion\nDPO (D3PO), the first adaptation of Direct Preference Optimization (DPO) to\ndiscrete diffusion models formulated as continuous-time Markov chains. Our\napproach derives a novel loss function that directly fine-tunes the generative\nprocess using preference data while preserving fidelity to a reference\ndistribution. We validate D3PO on a structured binary sequence generation task,\ndemonstrating that the method effectively aligns model outputs with preferences\nwhile maintaining structural validity. Our results highlight that D3PO enables\ncontrolled fine-tuning without requiring explicit reward models, making it a\npractical alternative to reinforcement learning-based approaches. Future\nresearch will explore extending D3PO to more complex generative tasks,\nincluding language modeling and protein sequence generation, as well as\ninvestigating alternative noise schedules, such as uniform noising, to enhance\nflexibility across different applications.",
        "The spatial diversity and multiplexing advantages of massive\nmulti-input-multi-output (mMIMO) can significantly improve the capacity of\nmassive non-orthogonal multiple access (NOMA) in machine type communications.\nHowever, state-of-the-art grant-free massive NOMA schemes for mMIMO systems\nrequire accurate estimation of random access channels to perform activity\ndetection and the following coherent data demodulation, which suffers from\nexcessive pilot overhead and access latency. To address this, we propose a\npre-equalization aided grant-free massive access scheme for mMIMO systems,\nwhere an iterative detection scheme is conceived. Specifically, the base\nstation (BS) firstly activates one of its antennas (i.e., beacon antenna) to\nbroadcast a beacon signal, which facilitates the user equipment (UEs) to\nperform downlink channel estimation and pre-equalize the uplink random access\nsignal with respect to the channels associated with the beacon antenna. During\nthe uplink transmission stage, the BS detects UEs' activity and data by using\nthe proposed iterative detection algorithm, which consists of three modules:\ncoarse data detection (DD), data-aided channel estimation (CE), and fine DD. In\nthe proposed algorithm, the joint activity and DD is firstly performed based on\nthe signals received by the beacon antenna. Subsequently, the DD is further\nrefined by iteratively performing data-aided CE module and fine DD module using\nsignals received by all BS antennas. Our simulation results demonstrate that\nthe proposed scheme outperforms state-of-the-art mMIMO-based grant-free massive\nNOMA schemes with the same access latency. Simulation codes are provided to\nreproduce the results in this article: https:\/\/github.com\/owenwang517\/tvt-2025.",
        "Ultrasound imaging frequently encounters challenges, such as those related to\nelevated noise levels, diminished spatiotemporal resolution, and the complexity\nof anatomical structures. These factors significantly hinder the model's\nability to accurately capture and analyze structural relationships and dynamic\npatterns across various regions of the heart. Mamba, an emerging model, is one\nof the most cutting-edge approaches that is widely applied to diverse vision\nand language tasks. To this end, this paper introduces a U-shaped deep learning\nmodel incorporating a large-window Mamba scale (LMS) module and a hierarchical\nfeature fusion approach for echocardiographic segmentation. First, a cascaded\nresidual block serves as an encoder and is employed to incrementally extract\nmultiscale detailed features. Second, a large-window multiscale mamba module is\nintegrated into the decoder to capture global dependencies across regions and\nenhance the segmentation capability for complex anatomical structures.\nFurthermore, our model introduces auxiliary losses at each decoder layer and\nemploys a dual attention mechanism to fuse multilayer features both spatially\nand across channels. This approach enhances segmentation performance and\naccuracy in delineating complex anatomical structures. Finally, the\nexperimental results using the EchoNet-Dynamic and CAMUS datasets demonstrate\nthat the model outperforms other methods in terms of both accuracy and\nrobustness. For the segmentation of the left ventricular endocardium\n(${LV}_{endo}$), the model achieved optimal values of 95.01 and 93.36,\nrespectively, while for the left ventricular epicardium (${LV}_{epi}$), values\nof 87.35 and 87.80, respectively, were achieved. This represents an improvement\nranging between 0.54 and 1.11 compared with the best-performing model.",
        "Limited English Proficiency (LEP) patients face higher risks of adverse\nhealth outcomes due to communication barriers, making timely medical\ninterpreting services essential for mitigating those risks. This paper\naddresses the scheduling of medical interpreting services under uncertainty.\nThe problem is formulated as a two-stage stochastic programming model that\naccounts for uncertainties in emergency patients' arrival and service time. The\nmodel handles the hiring decisions of part-time interpreters and the assignment\nof full-time and hired part-time interpreters. The objective is to minimize the\ntotal cost, which encompasses full-time interpreters' overtime cost, the fixed\nand variable costs of part-time interpreters, and the penalty cost for not\nserving LEP patients on time. The model is solved using the Sample Average\nApproximation (SAA) algorithm. To overcome the computational burden of the SAA\nalgorithm, a Tabu Search (TS) algorithm was used to solve the model. A\nreal-life case study is used to validate and evaluate the proposed solution\nalgorithms. The results demonstrate the effectiveness of the proposed\nstochastic programming-based solutions in concurrently reducing both the total\ncost and the waiting time. Further, sensitivity analysis reveals how the\nincrease in some key parameters, such as the arrival rate of emergency patients\nwith LEP, impacts scheduling outcomes.",
        "Here we present extinction, extirpation and coexistence conditions where \/\nwhen two communities combine. We consider one specific model where two\ncommunities coalesce, and another model where the communities coexist side by\nside, blending in a transitionary zone called the ecotone. Specifically, (1) we\nanalytically calculate the shifts in abundances as a function of mixing\nstrength. (2) Obtain a critical value for the mixing strength leading to\nextinction. (3) Derive an inequality condition for full coexistent mixing. (4)\nfind how the individual communities penetrate into one other as a function of\nmixing strength. (5) derive the conditions for one species to cross the ecotone\nand invade an neighboring community and (6) conditions for a native species to\nget extirpated. Lastly, (7) we spatially investigate the species richness\nwithin the ecotone and derive a condition that determines whether the ecotone\nwill have higher or lower richness compared to its surrounding habitats.",
        "We give a dynamical, relatively elementary proof of an \"absorption theorem\"\nwhich is closely related to a well-known result due to Matui. The construction\nis in the spirit of an earlier joint work of the author and S. Robert. In an\nappendix we explain how to use this result to correct the dynamical proof given\nby Melleray--Robert of a classification theorem for orbit equivalence of\nminimal ample groups due to Giordano, Putnam and Skau (the original argument\nhad a gap).",
        "This paper addresses the challenge of integrating semantic communication\nprinciples into operated networks, traditionally optimized based on\nnetwork-centric metrics rather than application-specific needs. Operated\nnetworks strongly adhere to the principle of ``separation of concerns\", which\nemphasizes a clear distinction between network operation and application.\nDespite the initial perceived incompatibility between semantic communication\nand the principles of operated networks, this paper provides solutions to\nreconcile them. The foundations of these solutions include the adoption of\nnon-arbitrary semantic representations as a standard encoding for\ncommunications, the establishment of a standard interface between the\napplication and network, and a dedicated network control plane. These enable\nthe application to describe the data typology and the nature of the task, and\nto agree upon a transmission scheme tailored to the supported task. Through\nthree scenarios involving an application transmitting text representations, we\nillustrate the implementation of the proposal and demonstrate the potential of\nthe approach.",
        "The proliferation of digital interactions across diverse domains, such as\nhealthcare, e-commerce, gaming, and finance, has resulted in the generation of\nvast volumes of event stream (ES) data. ES data comprises continuous sequences\nof timestamped events that encapsulate detailed contextual information relevant\nto each domain. While ES data holds significant potential for extracting\nactionable insights and enhancing decision-making, its effective utilization is\nhindered by challenges such as the scarcity of labeled data and the fragmented\nnature of existing research efforts. Self-Supervised Learning (SSL) has emerged\nas a promising paradigm to address these challenges by enabling the extraction\nof meaningful representations from unlabeled ES data. In this survey, we\nsystematically review and synthesize SSL methodologies tailored for ES modeling\nacross multiple domains, bridging the gaps between domain-specific approaches\nthat have traditionally operated in isolation. We present a comprehensive\ntaxonomy of SSL techniques, encompassing both predictive and contrastive\nparadigms, and analyze their applicability and effectiveness within different\napplication contexts. Furthermore, we identify critical gaps in current\nresearch and propose a future research agenda aimed at developing scalable,\ndomain-agnostic SSL frameworks for ES modeling. By unifying disparate research\nefforts and highlighting cross-domain synergies, this survey aims to accelerate\ninnovation, improve reproducibility, and expand the applicability of SSL to\ndiverse real-world ES challenges.",
        "We prove a criterion for the geometric and algebraic finiteness properties of\nvertex stabilisers of $G$-CW-complexes, given the finiteness properties of the\ngroup $G$ and of the stabilisers of positive dimensional cells. This\ngeneralises a result of Haglund--Wise for groups acting on trees to higher\ndimensions. As an application, for $n\\ge 2$, we deduce the existence of\nuncountably many quasi-isometry classes of one-ended groups that are of type\n$\\mathsf{FP}_n$ and not of type $\\mathsf{FP}_{n+1}$.",
        "Let $H$ be a quasiperiodic Schr\\\"{o}dinger operator generated by a monotone\npotential, as defined in [16]. Following [20], we study the connection between\nthe Lyapunov exponent $L\\left(E\\right)$, arithmetic properties of the frequency\n$\\alpha$, and certain fractal-dimensional properties of the spectral measures\nof $H$.",
        "Generative game engines have the potential to revolutionize game development\nby autonomously creating new content and reducing manual workload. However,\nexisting video-based game generation methods fail to address the critical\nchallenge of scene generalization, limiting their applicability to existing\ngames with fixed styles and scenes. In this paper, we present GameFactory, a\nframework focused on exploring scene generalization in game video generation.\nTo enable the creation of entirely new and diverse games, we leverage\npre-trained video diffusion models trained on open-domain video data. To bridge\nthe domain gap between open-domain priors and small-scale game dataset, we\npropose a multi-phase training strategy that decouples game style learning from\naction control, preserving open-domain generalization while achieving action\ncontrollability. Using Minecraft as our data source, we release GF-Minecraft, a\nhigh-quality and diversity action-annotated video dataset for research.\nFurthermore, we extend our framework to enable autoregressive\naction-controllable game video generation, allowing the production of\nunlimited-length interactive game videos. Experimental results demonstrate that\nGameFactory effectively generates open-domain, diverse, and action-controllable\ngame videos, representing a significant step forward in AI-driven game\ngeneration. Our dataset and project page are publicly available at\n\\url{https:\/\/vvictoryuki.github.io\/gamefactory\/}.",
        "Understanding and managing uncertainty is crucial in machine learning,\nespecially in high-stakes domains like healthcare, where class imbalance can\nimpact predictions. This paper introduces RIGA, a novel pipeline that mitigates\nclass imbalance using generative AI. By converting tabular healthcare data into\nimages, RIGA leverages models like cGAN, VQVAE, and VQGAN to generate balanced\nsamples, improving classification performance. These representations are\nprocessed by CNNs and later transformed back into tabular format for seamless\nintegration. This approach enhances traditional classifiers like XGBoost,\nimproves Bayesian structure learning, and strengthens ML model robustness by\ngenerating realistic synthetic data for underrepresented classes.",
        "This paper proposes a novel approach using Graph Neural Networks (GNNs) to\nsolve the AC Power Flow problem in power grids. AC OPF is essential for\nminimizing generation costs while meeting the operational constraints of the\ngrid. Traditional solvers struggle with scalability, especially in large\nsystems with renewable energy sources. Our approach models the power grid as a\ngraph, where buses are nodes and transmission lines are edges. We explore\ndifferent GNN architectures, including GCN, GAT, SAGEConv, and GraphConv to\npredict AC power flow solutions efficiently. Our experiments on IEEE test\nsystems show that GNNs can accurately predict power flow solutions and scale to\nlarger systems, outperforming traditional solvers in terms of computation time.\nThis work highlights the potential of GNNs for real-time power grid management,\nwith future plans to apply the model to even larger grid systems.",
        "Large Language Models (LLMs) have garnered significant attention for their\nimpressive general-purpose capabilities. For applications requiring intricate\ndomain knowledge, Retrieval-Augmented Generation (RAG) has shown a distinct\nadvantage in incorporating domain-specific information into LLMs. However,\nexisting RAG research has not fully addressed the challenges of Multiple Choice\nQuestion Answering (MCQA) in telecommunications, particularly in terms of\nretrieval quality and mitigating hallucinations. To tackle these challenges, we\npropose a novel first token probability guided RAG framework. This framework\nleverages confidence scores to optimize key hyperparameters, such as chunk\nnumber and chunk window size, while dynamically adjusting the context. Our\nmethod starts by retrieving the most relevant chunks and generates a single\ntoken as the potential answer. The probabilities of all options are then\nnormalized to serve as confidence scores, which guide the dynamic adjustment of\nthe context. By iteratively optimizing the hyperparameters based on these\nconfidence scores, we can continuously improve RAG performance. We conducted\nexperiments to validate the effectiveness of our framework, demonstrating its\npotential to enhance accuracy in domain-specific MCQA tasks.",
        "The majority of online marketplaces offer promotion programs to sellers to\nacquire additional customers for their products. These programs typically allow\nsellers to allocate advertising budgets to promote their products, with higher\nbudgets generally correlating to improve ad performance. Auction mechanisms\nwith budget pacing are commonly employed to implement such ad systems. While\nauctions deliver satisfactory average effectiveness, ad performance under\nallocated budgets can be unfair in practice.\n  To address this issue, we propose a novel ad allocation model that departs\nfrom traditional auction mechanics. Our approach focuses on solving a global\noptimization problem that balances traffic allocation while considering\nplatform efficiency and fairness constraints.\n  This study presents the following contributions. First, we introduce a\nfairness metric based on the Gini index. Second, we formulate the optimization\nproblem incorporating efficiency and fairness objectives. Third, we offer an\nonline algorithm to solve this optimization problem. Finally, we demonstrate\nthat our approach achieves superior fairness compared to baseline auction-based\nalgorithms without sacrificing efficiency. We contend that our proposed method\ncan be effectively applied in real-time ad allocation scenarios and as an\noffline benchmark for evaluating the fairness-efficiency trade-off of existing\nauction-based systems.",
        "In this paper, we address the full discretization of Friedrichs' systems with\na two-field structure, such as Maxwell's equations or the acoustic wave\nequation in div-grad form, cf. [14]. We focus on a discontinuous Galerkin space\ndiscretization applied to a locally refined mesh or a small region with high\nwave speed. This results in a stiff system of ordinary differential equations,\nwhere the stiffness is mainly caused by a small region of the spatial mesh.\nWhen using explicit time-integration schemes, the time step size is severely\nrestricted by a few spatial elements, leading to a loss of efficiency. As a\nremedy, we propose and analyze a general leapfrog-based scheme which is\nmotivated by [5]. The new, fully explicit, local time-integration method\nfilters the stiff part of the system in such a way that its CFL condition is\nsignificantly weaker than that of the leapfrog scheme while its computational\ncost is only slightly larger. For this scheme, the filter function is a\nsuitably scaled and shifted Chebyshev polynomial. While our main interest is in\nexplicit local-time stepping schemes, the filter functions can be much more\ngeneral, for instance, a certain rational function leads to the locally\nimplicit method, proposed and analyzed in [24]. Our analysis provides\nsufficient conditions on the filter function to ensure full order of\nconvergence in space and second order in time for the whole class of local\ntime-integration schemes.",
        "This paper explores the embedding of lattice structures $L \\subseteq\n\\mathbb{R}^n$ into smooth manifolds $M \\subseteq \\mathbb{R}^n$ through a\nrigorous mathematical framework. Building upon the foundational results\nestablished in \"Embedding of a Discrete Lattice Structure in a Smooth\nManifold,\" this work investigates the existence and solvability of partial\ndifferential equations (PDEs) governing the embedding process. The primary aim\nis to derive and analyze solutions to these PDEs while preserving the geometric\nand topological properties of $L$ and $M$.\n  The solutions are shown to exist under initial boundary conditions, with the\ngeometric structure of $M$ and the discrete topology of $L$ playing crucial\nroles in ensuring well-posedness and regularity.\n  This paper provides a detailed exposition of the mathematical interplay\nbetween discrete and continuous spaces, offering novel insights into embedding\ntheory and the geometry of manifolds interacting with discrete substructures.",
        "Let $\\mathbb{Z}_S$ be the ring generated by the inverses of all elements of a\nnon-empty proper subset $S$ of integer primes. We show that the ring generated\nby the values of unary definable sets in the model-theoretic Grothendieck ring\nof $(\\mathbb{Z}_S;+,<)$ is a quotient of $(\\mathbb{Z}\/q\\mathbb{Z})[T]\/(T+T^2)$,\nwhere $q$ is the largest odd integer that divides $p-1$ for all $p \\notin S$.\n  This implies that the Grothendieck ring of $(\\mathbb{Z}_S;+,<)$ is trivial in\nvarious salient cases, for example when $S$ is finite, or when $S$ does not\ncontain any prime of the form $2^n+1$, $n\\in \\mathbb{N}$.",
        "This paper proposes a novel framework for real-time adaptive-bitrate video\nstreaming by integrating latent diffusion models (LDMs) within the FFmpeg\ntechniques. This solution addresses the challenges of high bandwidth usage,\nstorage inefficiencies, and quality of experience (QoE) degradation associated\nwith traditional constant bitrate streaming (CBS) and adaptive bitrate\nstreaming (ABS). The proposed approach leverages LDMs to compress I-frames into\na latent space, offering significant storage and semantic transmission savings\nwithout sacrificing high visual quality. While it keeps B-frames and P-frames\nas adjustment metadata to ensure efficient video reconstruction at the user\nside, the proposed framework is complemented with the most state-of-the-art\ndenoising and video frame interpolation (VFI) techniques. These techniques\nmitigate semantic ambiguity and restore temporal coherence between frames, even\nin noisy wireless communication environments. Experimental results demonstrate\nthe proposed method achieves high-quality video streaming with optimized\nbandwidth usage, outperforming state-of-the-art solutions in terms of QoE and\nresource efficiency. This work opens new possibilities for scalable real-time\nvideo streaming in 5G and future post-5G networks.",
        "We present I2V3D, a novel framework for animating static images into dynamic\nvideos with precise 3D control, leveraging the strengths of both 3D geometry\nguidance and advanced generative models. Our approach combines the precision of\na computer graphics pipeline, enabling accurate control over elements such as\ncamera movement, object rotation, and character animation, with the visual\nfidelity of generative AI to produce high-quality videos from coarsely rendered\ninputs. To support animations with any initial start point and extended\nsequences, we adopt a two-stage generation process guided by 3D geometry: 1)\n3D-Guided Keyframe Generation, where a customized image diffusion model refines\nrendered keyframes to ensure consistency and quality, and 2) 3D-Guided Video\nInterpolation, a training-free approach that generates smooth, high-quality\nvideo frames between keyframes using bidirectional guidance. Experimental\nresults highlight the effectiveness of our framework in producing controllable,\nhigh-quality animations from single input images by harmonizing 3D geometry\nwith generative models. The code for our framework will be publicly released.",
        "In integrable spin chains, the spectral problem can be solved by the method\nof Bethe ansatz, which transforms the problem of diagonalization of the\nHamiltonian into the problem of solving a set of algebraic equations named\nBethe equations. In this work, we systematically investigate the spin-$s$ XXX\nchain with twisted and open boundary conditions using the rational $Q$-system,\nwhich is a powerful tool to solve Bethe equations. We establish basic\nframeworks of the rational $Q$-system and confirm its completeness numerically\nin both cases. For twisted boundaries, we investigate the polynomiality\nconditions of the rational $Q$-system and derive physical conditions for\nsingular solutions of Bethe equations. For open boundaries, we uncover novel\nphenomena such as hidden symmetries and magnetic strings under specific\nboundary parameters. Hidden symmetries lead to the appearance of extra\ndegeneracies in the Hilbert space, while the magnetic string is a novel type of\nexact string configuration, whose length depends on the boundary magnetic\nfields. These findings, supported by both analytical and numerical evidences,\noffer new insights into the interplay between symmetries and boundary\nconditions.",
        "For any odd prime $p$ and any integer $n>0$ with $p^2|n$, we show that the\nmod $p$ cohomology ring of the classifying space of the projective unitary\ngroup $PU(n)$ is not completely detected by elementary abelian $p$-subgroups,\nproviding counterexamples to a conjecture due to J. F. Adams. We also give an\napplication involving Milnor operations and Brown-Peterson cohomology.",
        "We propose Serenade, a novel framework for the singing style conversion (SSC)\ntask. Although singer identity conversion has made great strides in the\nprevious years, converting the singing style of a singer has been an unexplored\nresearch area. We find three main challenges in SSC: modeling the target style,\ndisentangling source style, and retaining the source melody. To model the\ntarget singing style, we use an audio infilling task by predicting a masked\nsegment of the target mel-spectrogram with a flow-matching model using the\ncomplement of the masked target mel-spectrogram along with disentangled\nacoustic features. On the other hand, to disentangle the source singing style,\nwe use a cyclic training approach, where we use synthetic converted samples as\nsource inputs and reconstruct the original source mel-spectrogram as a target.\nFinally, to retain the source melody better, we investigate a post-processing\nmodule using a source-filter-based vocoder and resynthesize the converted\nwaveforms using the original F0 patterns. Our results showed that the Serenade\nframework can handle generalized SSC tasks with the best overall similarity\nscore, especially in modeling breathy and mixed singing styles. Moreover,\nalthough resynthesizing with the original F0 patterns alleviated out-of-tune\nsinging and improved naturalness, we found a slight tradeoff in similarity due\nto not changing the F0 patterns into the target style.",
        "CFD acceleration for virtual nuclear reactors or digital twin technology is a\nprimary goal in the nuclear industry. This study compares advanced\nconvolutional neural network (CNN) architectures for accelerating unsteady\ncomputational fluid dynamics (CFD) simulations using small datasets based on a\nchallenging natural convection flow dataset. The advanced architectures such as\nautoencoders, UNet, and ConvLSTM-UNet, were evaluated under identical\nconditions to determine their predictive accuracy and robustness in\nautoregressive time-series predictions. ConvLSTM-UNet consistently outperformed\nother models, particularly in difference value calculation, achieving lower\nmaximum errors and stable residuals. However, error accumulation remains a\nchallenge, limiting reliable predictions to approximately 10 timesteps. This\nhighlights the need for enhanced strategies to improve long-term prediction\nstability. The novelty of this work lies in its fair comparison of\nstate-of-the-art CNN models within the RePIT framework, demonstrating their\npotential for accelerating CFD simulations while identifying limitations under\nsmall data conditions. Future research will focus on exploring alternative\nmodels, such as graph neural networks and implicit neural representations.\nThese efforts aim to develop a robust hybrid approach for long-term unsteady\nCFD acceleration, contributing to practical applications in virtual nuclear\nreactor.",
        "Object affordance reasoning, the ability to infer object functionalities\nbased on physical properties, is fundamental for task-oriented planning and\nactivities in both humans and Artificial Intelligence (AI). This capability,\nrequired for planning and executing daily activities in a task-oriented manner,\nrelies on commonsense knowledge of object physics and functionalities,\nextending beyond simple object recognition. Current computational models for\naffordance reasoning from perception lack generalizability, limiting their\napplicability in novel scenarios. Meanwhile, comprehensive Large Language\nModels (LLMs) with emerging reasoning capabilities are challenging to deploy on\nlocal devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a\nlarge-scale dataset comprising 1,496 tasks and 119k images, designed to enhance\nthe generalizability of affordance reasoning from perception. Utilizing this\ndataset, we develop Afford-X, an end-to-end trainable affordance reasoning\nmodel that incorporates Verb Attention and Bi-Fusion modules to improve\nmulti-modal understanding. This model achieves up to a 12.1% performance\nimprovement over the best-reported results from non-LLM methods, while also\ndemonstrating a 1.2% enhancement compared to our previous conference paper.\nAdditionally, it maintains a compact 187M parameter size and infers nearly 50\ntimes faster than the GPT-4V API. Our work demonstrates the potential for\nefficient, generalizable affordance reasoning models that can be deployed on\nlocal devices for task-oriented manipulations. We showcase Afford-X's\neffectiveness in enabling task-oriented manipulations for robots across various\ntasks and environments, underscoring its efficiency and broad implications for\nadvancing robotics and AI systems in real-world applications.",
        "In this paper, we mainly consider nonnegative weak solutions $u\\in\nD^{1,p}(\\R^{N})$ to the doubly $D^{1,p}(\\R^{N})$-critical nonlocal quasi-linear\nSchr\\\"{o}dinger-Hartree equation: \\begin{align*} -\\Delta_p u- \\mu\n\\frac{u^{p-1}}{|x|^p}=\\left(|x|^{-2p}\\ast |u|^{p}\\right)|u|^{p-2}u \\qquad\n&\\mbox{in} \\,\\, \\mathbb{R}^N, \\end{align*} where $N\\geq3$, $0\\leq\\mu<\n\\bar{\\mu}:=\\left( (N-p)\/p \\right)^p$ and $1<p<\\frac{N}{2}$. When $\\mu>0$, due\nto appearance of the Hardy potential, the equation has singularity at\n$0\\in\\mathbb{R}^{N}$ and hence is not translation invariant, so sharp\nasymptotic estimates near the origin must be involved. First, we establish\nregularity and the sharp estimates on asymptotic behaviors near the origin and\nthe infinity for any positive solution $u\\in D^{1,p}(\\R^{N})$ (and $|\\nabla\nu|$) to more general equation $-\\triangle_p u - \\mu\n\\frac{1}{|x|^p}u^{p-1}=V(x)\\frac{1}{|x|^s}u^{p-1}$ with $N\\geq2$, $0\\leq\\mu<\n\\bar{\\mu}$, $1<p<N$, $0\\leq s < p$ and $0\\leq V(x)\\in L^\\frac{N}{p-s}(\\R^N)$.\nThen, as a consequence, we can apply the method of moving planes to prove that\nall the nontrivial nonnegative solutions in $D^{1,p}(\\R^{N})$ are radially\nsymmetric and strictly radially decreasing about the origin\n$0\\in\\mathbb{R}^{N}$. The sharp asymptotic estimates and radial symmetry for\nmore general weighted doubly $D^{1,p}$-critical nonlocal quasi-linear equations\nwere also derived. Our results extend the results in \\cite{DLL} from the\nspecial case $\\mu=0$ to general cases $0\\leq\\mu<\\bar{\\mu}$."
      ]
    }
  },
  {
    "id":2411.1945,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Auto-Encoding Variational Bayes",
    "start_abstract":"How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
      ],
      "abstract":[
        "It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products."
      ],
      "categories":[
        "physics.gen-ph"
      ]
    },
    "list":{
      "title":[
        "Isometric Gelfand transforms of complete Nevanlinna-Pick spaces",
        "On extensivity of morphisms",
        "On Branch-and-Price for Project Scheduling",
        "Extreme Shape Coexistence Observed in $^{70}$Co",
        "Learning Memory and Material Dependent Constitutive Laws",
        "Exploring the Technology Landscape through Topic Modeling, Expert\n  Involvement, and Reinforcement Learning",
        "Models for the Eremenko-Lyubich class",
        "Subcode Ensemble Decoding of Linear Block Codes",
        "How does non-metricity affect particle creation and evaporation in\n  bumblebee gravity?",
        "A mesh-free hybrid Chebyshev-Tucker tensor format with applications to\n  multi-particle modelling",
        "Investigating Parameter-Efficiency of Hybrid QuGANs Based on Geometric\n  Properties of Generated Sea Route Graphs",
        "Complex potential and open system applications in heavy-ions and cold\n  atoms",
        "A Pristine-UNIONS view on the Galaxy: Kinematics of the distant spur\n  feature of the Sagittarius stream traced by Blue Horizontal Branch stars",
        "Implementation and verification of coherent error suppression using\n  randomized compiling for Grover's algorithm on a trapped-ion device",
        "Dynkin Systems and the One-Point Geometry",
        "Tri-layer SiN-on-Si 8x8 Optical Switches with Thermo-optic and\n  Electro-optic Actuators",
        "Sub-GeV dark matter and nano-Hertz gravitational waves from a\n  classically conformal dark sector",
        "A Framework for Stochastic Fairness in Dominant Resource Allocation with\n  Cloud Computing Applications",
        "Approximating non-Gaussian Bayesian partitions with normalising flows:\n  statistics, inference and application to cosmology",
        "Accurate myocardial T1 mapping at 5T using an improved MOLLI method: A\n  validation study",
        "On the Spectral Analysis of Power Graph of Dihedral Groups",
        "A Formalism for Calibrating the Instrumental Polarization of Radio\n  Interferometric Arrays at Meter Wavelengths using Unpolarized Sky: A\n  Demonstration using the MWA Observations",
        "Metalens array for complex-valued optical discrete Fourier transform",
        "The physics of oscillating surfaces and sounds",
        "Reconfigurable Intelligent Surfaces-Assisted Integrated Access and\n  Backhaul",
        "Classification of Homogeneous Local Representations of the Singular\n  Braid Monoid",
        "Approximation of Permutation Invariant Polynomials by Transformers:\n  Efficient Construction in Column-Size",
        "Which Features are Best for Successor Features?",
        "Observation of the $\\Omega$(2012) baryon at the LHC"
      ],
      "abstract":[
        "We show that any complete Nevanlinna-Pick space whose multiplier algebra has\nisometric Gelfand transform (or commutative C*-envelope) is essentially the\nHardy space on the open unit disk.",
        "Extensivity of a category may be described as a property of coproducts in the\ncategory, namely, that they are disjoint and universal. An alternative\nviewpoint is that it is a property of morphisms in a category. This paper\nexplores this point of view through a natural notion of extensive and\ncoextensive morphism. Through these notions, topics in universal algebra, such\nas the strict refinement and Fraser-Horn properties, take categorical form and\nthereby enjoy the benefits of categorical generalisation. On the other hand,\nthe universal algebraic theory surrounding these topics inspire categorical\nresults. One such result we establish in this paper is that a Barr-exact\ncategory is coextensive if and only if every split monomorphism in the category\nis coextensive.",
        "Integer programs for resource-constrained project scheduling problems are\nnotoriously hard to solve due to their weak linear relaxations. Several papers\nhave proposed reformulating project scheduling problems via Dantzig-Wolfe\ndecomposition to strengthen their linear relaxation and decompose large problem\ninstances. The reformulation gives rise to a master problem that has a large\nnumber of variables. Therefore, the master problem is solved by a column\ngeneration procedure embedded in a branching framework, also known as\nbranch-and-price. While branch-and-price has been successfully applied to many\nproblem classes, it turns out to be ineffective for most project scheduling\nproblems. This paper identifies drivers of the ineffectiveness by analyzing the\nstructure of the reformulated problem and the strength of different branching\nschemes. Our analysis shows that the reformulated problem has an unfavorable\nstructure for column generation: It is highly degenerate, slowing down the\nconvergence of column generation, and for many project scheduling problems, it\nyields the same or only slightly stronger linear relaxations as classical\nformulations at the expense of large increases in runtime. Our computational\nexperiments complement our theoretical findings.",
        "The shape of the atomic nucleus is a property which underpins our\nunderstanding of nuclear systems, impacts the limits of nuclear existence, and\nenables probes of physics beyond the Standard Model. Nuclei can adopt a variety\nof shapes, including spheres, axially deformed spheroids, and pear shapes. In\nsome regions of the nuclear chart where a spherical nucleus would naively be\nexpected, deformed nuclear states can result from collective action of\nconstituent protons and neutrons. In a small subset of nuclei both spherical\nand deformed nuclear states have been experimentally observed, a phenomenon\ntermed shape coexistence. We present spectroscopic evidence for the coexistence\nof $J^{\\pi}=1+$ spherical and deformed states in $^{70}$Co, separated by less\nthan 275~keV. This close degeneracy of levels with the same $J^{\\pi}$ and\ndifferent shapes demonstrates an extreme example of shape coexistence resulting\nfrom the interplay of independent particle motion and collective behavior in\nhighly unstable nuclear systems and identifies the Co isotopes as a transition\npoint between deformed ground states observed in the Cr isotopes and spherical\nconfigurations observed in the closed-shell Ni isotopes.",
        "The theory of homogenization provides a systematic approach to the derivation\nof macroscale constitutive laws, obviating the need to repeatedly resolve\ncomplex microstructure. However, the unit cell problem that defines the\nconstitutive model is typically not amenable to explicit evaluation. It is\ntherefore of interest to learn constitutive models from data generated by the\nunit cell problem. Many viscoelastic and elastoviscoplastic materials are\ncharacterized by memory-dependent constitutive laws. In order to amortize the\ncomputational investment in finding such memory-dependent constitutive laws, it\nis desirable to learn their dependence on the material microstructure. While\nprior work has addressed learning memory dependence and material dependence\nseparately, their joint learning has not been considered. This paper focuses on\nthe joint learning problem and proposes a novel neural operator framework to\naddress it.\n  In order to provide firm foundations, the homogenization problem for linear\nKelvin-Voigt viscoelastic materials is studied. The theoretical properties of\nthe cell problem in this Kelvin-Voigt setting are used to motivate the proposed\ngeneral neural operator framework; these theoretical properties are also used\nto prove a universal approximation theorem for the learned macroscale\nconstitutive model. This formulation of learnable constitutive models is then\ndeployed beyond the Kelvin-Voigt setting. Numerical experiments are presented\nshowing that the resulting data-driven methodology accurately learns history-\nand microstructure-dependent linear viscoelastic and nonlinear\nelastoviscoplastic constitutive models, and numerical results also demonstrate\nthat the resulting constitutive models can be deployed in macroscale simulation\nof material deformation.",
        "In today's rapidly evolving technological landscape, organizations face the\nchallenge of integrating external insights into their decision-making processes\nto stay competitive. To address this issue, this study proposes a method that\ncombines topic modeling, expert knowledge inputs, and reinforcement learning\n(RL) to enhance the detection of technological changes. The method has four\nmain steps: (1) Build a relevant topic model, starting with textual data like\ndocuments and reports to find key themes. (2) Create aspect-based topic models.\nExperts use curated keywords to build models that showcase key domain-specific\naspects. (3) Iterative analysis and RL driven refinement: We examine metrics\nsuch as topic magnitude, similarity, entropy shifts, and how models change over\ntime. We optimize topic selection with RL. Our reward function balances the\ndiversity and similarity of the topics. (4) Synthesis and operational\nintegration: Each iteration provides insights. In the final phase, the experts\ncheck these insights and reach new conclusions. These conclusions are designed\nfor use in the firm's operational processes. The application is tested by\nforecasting trends in quantum communication. Results demonstrate the method's\neffectiveness in identifying, ranking, and tracking trends that align with\nexpert input, providing a robust tool for exploring evolving technological\nlandscapes. This research offers a scalable and adaptive solution for\norganizations to make informed strategic decisions in dynamic environments.",
        "If $f$ is in the Eremenko-Lyubich class (transcendental entire functions with\nbounded singular set) then $\\Omega= \\{ z: |f(z)| > R\\}$ and $f|_\\Omega$ must\nsatisfy certain simple topological conditions when $R$ is sufficiently large. A\nmodel $(\\Omega, F)$ is an open set $\\Omega$ and a holomorphic function $F$ on\n$\\Omega$ that satisfy these same conditions. We show that any model can be\napproximated by an Eremenko-Lyubich function in a precise sense. In many cases,\nthis allows the construction of functions in the Eremenko-Lyubich with a\ndesired property to be reduced to the construction of a model with that\nproperty, and this is often much easier to do.",
        "Low-density parity-check (LDPC) codes together with belief propagation (BP)\ndecoding yield exceptional error correction capabilities in the large block\nlength regime. Yet, there remains a gap between BP decoding and maximum\nlikelihood decoding for short block length LDPC codes. In this context,\nensemble decoding schemes yield both reduced latency and good error rates. In\nthis paper, we propose subcode ensemble decoding (SCED), which employs an\nensemble of decodings on different subcodes of the code. To ensure that all\ncodewords are decodable, we use the concept of linear coverings and explore\napproaches for sampling suitable ensembles for short block length LDPC codes.\nMonte-Carlo simulations conducted for three LDPC codes demonstrate that SCED\nimproves decoding performance compared to stand-alone decoding and automorphism\nensemble decoding. In particular, in contrast to existing schemes, e.g.,\nmultiple bases belief propagation and automorphism ensemble decoding, SCED does\nnot require the NP-complete search for low-weight dual codewords or knowledge\nof the automorphism group of the code, which is often unknown.",
        "In this work, we analyze the impact of non-metricity on particle creation and\nthe evaporation process of black holes within the framework of bumblebee\ngravity. In general lines, we compare black holes in the metric formalism [1]\nand the metric-affine approach [2]. Initially, we focus on bosonic particle\nmodes to investigate Hawking radiation. Using the Klein-Gordon equation, we\ncompute the Bogoliubov coefficients and derive the Hawking temperature.\nSubsequently, we examine Hawking radiation as a tunneling process, resolving\ndivergent integrals through the residue method. The analysis is then extended\nto fermionic particle modes, also within the tunneling framework. Particle\ncreation densities are calculated for both bosonic and fermionic cases.\nAdditionally, greybody bounds are estimated for bosonic and fermionic\nparticles. Finally, we explore the evaporation process, considering the final\nstate of the black holes. In a general panorama, non-metricity in bumblebee\ngravity raises particle density for bosons while reducing it for fermions,\nincreases greybody factors (for both bosons and fermions), amplifies the\nemission rate, and accelerates the evaporation process.",
        "In this paper, we introduce a mesh-free two-level hybrid Tucker tensor format\nfor approximation of multivariate functions, which combines the product\nChebyshev interpolation with the ALS-based Tucker decomposition of the tensor\nof Chebyshev coefficients. It allows to avoid the expenses of the\nrank-structured approximation of function-related tensors defined on large\nspacial grids, while benefiting from the Tucker decomposition of the rather\nsmall core tensor of Chebyshev coefficients. This leads to nearly optimal\nTucker rank parameters which are close to the results for well established\nTucker-ALS algorithm applied to the large grid-based tensors. These rank\nparameters inherited from the Tucker-ALS decomposition of the coefficient\ntensor can be much less than the polynomial degrees of the initial Chebyshev\ninterpolant via function independent basis set. Furthermore, the tensor product\nChebyshev polynomials discretized on a tensor grid leads to a low-rank\ntwo-level orthogonal algebraic Tucker tensor that approximates the initial\nfunction with controllable accuracy. It is shown that our techniques could be\ngainfully applied to the long-range part of the electrostatic potential of\nmulti-particle systems approximated in the range-separated tensor format. Error\nand complexity estimates of the proposed methods are presented. We demonstrate\nthe efficiency of the suggested method numerically on examples of the\nlong-range components of multi-particle interaction potentials generated by 3D\nNewton kernel for large bio-molecule systems and lattice-type compounds.",
        "The demand for artificially generated data for the development, training and\ntesting of new algorithms is omnipresent. Quantum computing (QC), does offer\nthe hope that its inherent probabilistic functionality can be utilised in this\nfield of generative artificial intelligence. In this study, we use\nquantum-classical hybrid generative adversarial networks (QuGANs) to\nartificially generate graphs of shipping routes. We create a training dataset\nbased on real shipping data and investigate to what extent QuGANs are able to\nlearn and reproduce inherent distributions and geometric features of this data.\nWe compare hybrid QuGANs with classical Generative Adversarial Networks (GANs),\nwith a special focus on their parameter efficiency. Our results indicate that\nQuGANs are indeed able to quickly learn and represent underlying geometric\nproperties and distributions, although they seem to have difficulties in\nintroducing variance into the sampled data. Compared to classical GANs of\ngreater size, measured in the number of parameters used, some QuGANs show\nsimilar result quality. Our reference to concrete use cases, such as the\ngeneration of shipping data, provides an illustrative example and demonstrate\nthe potential and diversity in which QC can be used.",
        "Since the discovery of the complex potential of quarkonium at high\ntemperatures, quarkonium has been regarded as an open quantum system in the\nquark-gluon plasma. Recently, a similar issue regarding in-medium bound states\nof impurities has also emerged in particle physics and cold atomic physics. We\nwill provide an overview of recent advancements in understanding key quantities\nsuch as complex potential and transport coefficients for heavy impurities in\nfinite temperature QCD and cold atomic systems.",
        "Providing a detailed picture of the Sagittarius (Sgr) stream offers important\nconstraints on the build-up of the Galactic halo as well as its gravitational\npotential at large radii. While several attempts have been made to model the\nstructure of the Sgr stream, no model has yet been able to match all the\nfeatures observed for the stream. Moreover, for several of these features,\nobservational characterisation of their properties is rather limited,\nparticularly at large distances. The aim of this work is to investigate the\nkinematics of the Sgr stream outermost spur feature using blue horizontal\nbranch (BHB) stars. Candidate BHB stars were selected by combining two\napproaches; one capitalising on Pan-STARRS1 3$\\Pi$ griz and u photometry taken\nas part of UNIONS, the other using Pristine Survey CaHK and SDSS ugr\nphotometry. Follow-up optical spectra are obtained using ESO\/VLT\/FORS2 to\nconfirm their BHB nature and obtain line-of-sight (LOS) velocities. Of our 25\ncandidates, 20 stars can be confirmed as bona fide BHB stars. Their LOS\nvelocities, together with the 3D positions of these stars qualitatively match\nwell with Sgr model predictions and trace the outer apocentre of the trailing\narm and its spur feature very nicely. The quantitative offsets that are found\nbetween our data and the different models can be used to provide information\nabout the Galactic gravitational potential at large distances. We present a\nfirst, tentative, analysis in this direction, showing that the model of\nVasiliev et al. (2021) would provide better agreement with our observations if\nthe enclosed mass of the Milky Way within 100 kpc were lowered to\n$(5.3\\!\\pm\\!0.4)\\!\\times\\!10^{11}$ M$_\\odot$ (versus\n$(5.6\\!\\pm\\!0.4)\\!\\times\\!10^{11}$ M$_\\odot$). Our selection of BHB stars\nprovides a new view on the outermost structure in 3D positions and LOS\nvelocities of the Sgr debris.",
        "In near-term quantum computations that do not employ error correction, noise\ncan proliferate rapidly, corrupting the quantum state and making results\nunreliable. These errors originate from both decoherence and control\nimprecision. The latter can manifest as coherent noise that is especially\ndetrimental. Here, we study the impact of coherent errors and their mitigation\nunder standard error-reduction techniques, both theoretically and\nexperimentally on a trapped-ion quantum computer. As a representative case\nstudy, we implement a range of Grover's algorithm circuits containing up to 10\nqubits and 26 two-qubit gates. We demonstrate the effectiveness of randomized\ncompiling (RC) and algorithm error detection (ED), where the latter is realized\nvia post-selection on ancillary qubits that ideally return to the ground state\nat the end of each circuit. Our results highlight a synergetic effect:\ncombining RC and ED yields the largest reductions in errors, indicating that\nthese methods can work together to extend the capabilities of near-term quantum\ndevices for moderately deep circuits.",
        "In this note I demonstrate that the collection of Dynkin systems on finite\nsets assembles into a Connes-Consani $\\mathbb{F}_1$-module, with the collection\nof partitions of finite sets as a sub-module. The underlying simplicial set of\nthis $\\mathbb{F}_1$-module is shown to be isomorphic to the delooping of the\nKrasner hyperfield $\\mathbb{K}$, where $1+1=\\{0,1\\}$. The face and degeneracy\nmaps of the underlying simplicial set of the $\\mathbb{F}_1$-module of\npartitions correspond to merging partition blocks and introducing singleton\nblocks, respectively. I also show that the $\\mathbb{F}_1$-module of partitions\ncannot correspond to a set with a binary operation (even partially defined or\nmultivalued) under the ``Eilenberg-MacLane'' embedding. These results imply\nthat the $n$-fold sum of the Dynkin $\\mathbb{F}_1$-module with itself is\nisomorphic to the $\\mathbb{F}_1$-module of the discrete projective geometry on\n$n$ points.",
        "We present two spatial-multiplexed switch-and-select (S&S) 8x8 optical\nswitches incorporating a tri-layer SiN-on-Si platform, one equipped with\nthermo-optic (T-O) and the other electro-optic (E-O) switching elements. To the\nbest of our knowledge, the electro-optic switch fabric is the first-of-its-kind\ndevice assembled in such a multi-layer platform. The shuffle between the\nmultiplexer and demultiplexer array is established via a tri-layer Si-SiN-SiN\nstructure, creating a three-dimensional crossing-free photonic shuffle network.\nAt the same time, the implementation of the S&S topology can effectively\nsuppress the first-order crosstalk. The measured on-chip losses for the T-O\nswitch range from 2.1 to 11.5 dB, with a 5.2 dB average, while the E-O device\nexhibits losses between 8.7 to 19.6 dB, with a 15.1 dB average. Both switches\ndemonstrate ultra-low crosstalk, with measured ranges of 38.9 to 50.8 dB and\n42.8 to 51.9 dB, for the T-O and E-O devices respectively. The switching times\nare 17.6 us for the T-O switch and 5.9 ns with the E-O actuated one. These\nperformance metrics highlight the potential of these switches for\nnext-generation data center applications.",
        "Strong first-order phase transitions in a dark sector offer a compelling\nexplanation for the stochastic gravitational wave background in the nano-Hertz\nrange recently detected by pulsar timing arrays (PTAs). We explore the\npossibility that such a phase transition at the same time gives mass to a\nstable fermion that accounts for the observed dark matter abundance and leads\nto testable effects in laboratory experiments. Concretely, we consider a\nclassically conformal dark sector with a hidden $U(1)^\\prime$ gauge symmetry\nthat couples to the Standard Model via kinetic mixing. Since the PTA signal\nrequires a phase transition in the MeV temperature range, spontaneous symmetry\nbreaking gives rise to a sub-GeV dark matter candidate that couples to the\nStandard Model via a dark photon mediator and obtains its relic abundance via\nannihilations into electrons and dark Higgs bosons. Such a scenario is tightly\nconstrained by laboratory searches for dark photons and cosmological\nconstraints on the decays of dark Higgs bosons after the phase transition. We\nshow that viable parameter regions can be found both for the case that the dark\nHiggs bosons remain in equilibrium with the Standard Model and that they\ndecouple and only decay much later. In the latter case, the parameter regions\npreferred by the PTA signal and the dark matter relic abundance can be fully\nexplored by future beam-dump experiments searching for missing energy.",
        "Allocation of limited resources under uncertain requirements often\nnecessitates fairness considerations, with applications in computer systems,\nhealth systems, and humanitarian logistics. This paper introduces a\ndistributionally robust (DR) stochastic fairness framework for multi-resource\nallocation, leveraging rough estimates of the mean and variance of resource\nrequirement distributions. The framework employs a sampled approximation DR\n(SA-DR) model to develop the concept of stochastic fairness, satisfying key\nproperties such as stochastic Pareto efficiency, stochastic sharing incentive,\nand stochastic envy-freeness under suitable conditions. We show the convergence\nof the SA-DR model to the DR model and propose a finitely convergent algorithm\nto solve the SA-DR model. We empirically evaluate the performance of our\nmoment-based SA-DR model -- which uses only rough estimates of the mean and\nvariance of the resource requirement distribution -- against alternative\nresource allocation models under varying levels of information availability. We\ndemonstrate that our moment-based partial-information SA-DR model can achieve\nperformance closer to the full-information model than the worst-case\ninformation model. Convergence of the sampled approximation model and\ncomparisons across models are illustrated using data from cloud computing\napplications.",
        "Subject of this paper is the simplification of Markov chain Monte Carlo\nsampling as used in Bayesian statistical inference by means of normalising\nflows, a machine learning method which is able to construct an invertible and\ndifferentiable transformation between Gaussian and non-Gaussian random\ndistributions. We use normalising flows to compute Bayesian partition functions\nfor non-Gaussian distributions and show how normalising flows can be employed\nin finding analytical expressions for posterior distributions beyond the\nGaussian limit. Flows offer advantages for the numerical evaluation of the\npartition function itself, as well as for cumulants and for the information\nentropy. We demonstrate how normalising flows in conjunction with Bayes\npartitions can be used in inference problems in cosmology and apply them to the\nposterior distribution for the matter density $\\Omega_m$ and a dark energy\nequation of state parameter $w_0$ on the basis of supernova data.",
        "Purpose: To develop 5T-SRIS, an improved 5T myocardial T1 mapping method\nbased on MOLLI, which addresses limitations in inversion efficiency, readout\nperturbations, and imperfect magnetization recovery. Methods: The proposed\n5T-SRIS method is based on a modified 5-(3)-3 MOLLI sequence with ECG gating\nand gradient echo readout. To improve inversion efficiency at 5T, the inversion\npulse was redesigned using adiabatic hyperbolic secant (HSn) and\ntangent\/hyperbolic tangent (Tan\/Tanh) pulses. Signal evolution was modeled\nrecursively with inversion efficiency and a correction factor (C) to correct\ninversion imperfections, and T1 values were estimated via nonlinear\noptimization. The method was validated in phantom studies, as well as in 21\nhealthy volunteers and 9 patients at 5T. Results: The optimized IR pulse based\non the tangent\/hyperbolic tangent pulse was found to outperform the\nconventional hyperbolic secant IR pulse at the 5T scanner. This optimized IR\npulse achieves an average inversion factor of 0.9014within a B0 range of 250Hz\nand a B1 range of -50% to 20%. Phantom studies show that the 5T-SRIS achieved\nhigh accuracy with errors within 5%. In vivo studies with 21 healthy\nvolunteers, the native myocardial T1 values were 1468 ms (apex), 1514 ms\n(middle), and 1545 ms (base). In vivo studies with 9 heart patients, the native\nmyocardial T1 values were 1484 ms (apex), 1532 ms (middle), and 1581 ms (base).\nAnd the post myocardial T1 values were 669 ms (apex), 698 ms (middle), and 675\nms (base). Conclusion: The 5T-SRIS technique is robust and suitable for\nclinical cardiac imaging. This study demonstrates its feasibility for accurate\nmyocardial T1 mapping at 5T, despite challenges related to magnetic field\ninhomogeneity. Keywords: Myocardial T1 mapping, 5T, improved MOLLI, 5T-SRIS",
        "The power graph \\( \\mathcal{G}_G \\) of a group \\( G \\) is a graph whose\nvertex set is \\( G \\), and two elements \\( x, y \\in G \\) are adjacent if one is\nan integral power of the other. In this paper, we determine the adjacency,\nLaplacian, and signless Laplacian spectra of the power graph of the dihedral\ngroup \\( D_{2pq} \\), where \\( p \\) and \\( q \\) are distinct primes. Our\nfindings demonstrate that the results of Romdhini et al. [2024], published in\nthe \\textit{European Journal of Pure and Applied Mathematics}, do not hold\nuniversally for all \\( n \\geq 3 \\). Our analysis demonstrates that their\nresults hold true exclusively when \\( n = p^m \\) where \\( p \\) is a prime\nnumber and \\( m \\) is a positive integer. The research examines their\nmethodology via explicit counterexamples to expose its boundaries and establish\ncorrected results. This study improves past research by expanding the spectrum\nevaluation of power graphs linked to dihedral groups.",
        "Calibration of instrumental polarization is critical for measuring polarized\nradio emissions from astrophysical sources to extract the magnetic field\ninformation in astrophysical, heliospheric, and terrestrial plasmas. At meter\nwavelengths, calibration of radio polarimetric observations is particularly\nchallenging because of the scarcity of bright polarized sources due to\nsignificant Faraday depolarization. Here, we present a novel formalism for\npolarization calibration using an unpolarized sky model. The formalism is\nspecifically designed for wide-field, low-frequency instruments like the\nMurchison Widefield Array (MWA), the LOw Frequency ARray (LOFAR), New Extension\nin Nan\\c{c}ay Upgrading LoFAR (NenuFAR), Owens Valley Radio Observatory - Long\nWavelength Array (OVRO-LWA), low-frequency telescope of the Square Kilometre\nArray Observatory (SKAO-low), etc. By leveraging the apparent polarization of\nthe unpolarized sky induced by the polarized primary beam of the radio\ntelescope, this method avoids dependence on bright polarized calibrators. It is\nalso immune to ionospheric Faraday rotation. The validation of the approach via\nMWA observations confirms the accuracy of the method. This formalism provides a\nrobust framework for low-frequency polarization calibration. It addresses the\nlongstanding calibration challenges and advances the field of low-frequency\npolarimetry by enabling polarization studies of astrophysical radio sources.",
        "Photonic computing has emerged as a promising platform for accelerating\ncomputational tasks with high degrees of parallelism, such as image processing\nand neural network. We present meta-DFT (discrete Fourier transform), a single\nlayer metasurface device, designed to perform optical complex-to-complex DFT\nwith O(N) time complexity. One critical challenge in free-space analog optical\ncomputing is to control the measurement error. Our scheme addresses this issue\nby focusing light on spatially separated focal points and reconstructing the\ncomplex phase, which enable error correction. We systematically evaluate the\ndevice's performance using input vectors with random complex amplitudes and\nphases, to demonstrate its robust accuracy. Our findings pave the way towards\nadvancement of metasurface-based computation, offering a robust framework that\nis readily extensible to an arbitrary complex-valued matrix-vector\nmultiplication (MVM).",
        "The longitudinal oscillations of air columns composed of contractions and\nrarefaction make up sound. Sound amplification is widely used in medical,\nelectronic and communication fields. A simplistic technique for producing and\namplifying can be rewarding. In this study, we investigate a simplistic DIY\nspeaker configuration that can be utilized for sound creation and modulation by\nimplementing response of magnets and a solenoid to an oscillating input signal.\nWe use steady state solution of forced simple harmonic oscillator with damping\nparameters to analyze our design and show its characteristic frequencies. We\npresent an analytical way of obtaining optimal parameters of the setup to\ntheoretically obtain experimental characteristic frequencies and provide an\nin-depth investigation of the setup.",
        "In this paper, we study the impact of reconfigurable intelligent surfaces\n(RISs) on the coverage extension of integrated access and backhaul (IAB)\nnetworks. Particularly, using a finite stochastic geometry model, with random\ndistributions of user equipments (UEs) in a finite region, and planned\nhierachical architecture for IAB, we study the service coverage probability\ndefined as the probability of the event that the UEs' minimum rate requirements\nare satisfied. We present comparisons between different cases including\nIAB-only, IAB assisted with RIS for backhaul as well as IAB assisted by network\ncontrolled repeaters (NCRs). Our investigations focus on wide-area IAB assisted\nwith RIS through the lens of different design architectures and deployments,\nrevealing both conflicts and synergies for minimizing the effect of tree\nfoliage over seasonal changes. Our simulation results reveal both opportunities\nand challenges towards the implementation of RIS in IAB.",
        "For a natural number $n$, denote by $B_n$ the braid group on $n$ strings and\nby $SM_n$ the singular braid monoid on $n$ strings. $SM_n$ is one of the most\nimportant extensions of $B_n$. In [13], Y. Mikhalchishina classified all\nhomogeneous $2$-local representations of $B_n$ for all $n \\geq 3$. In this\narticle, we extend the result of Mikhalchishina in two ways. First, we classify\nall homogeneous $3$-local representations of $B_n$ for all $n \\geq 4$. Second,\nwe classify all homogeneous $2$-local representations of $SM_n$ for all $n\\geq\n2$ and all homogeneous $3$-local representations of $SM_n$ for all $n\\geq 4$.",
        "Transformers are a type of neural network that have demonstrated remarkable\nperformance across various domains, particularly in natural language processing\ntasks. Motivated by this success, research on the theoretical understanding of\ntransformers has garnered significant attention. A notable example is the\nmathematical analysis of their approximation power, which validates the\nempirical expressive capability of transformers. In this study, we investigate\nthe ability of transformers to approximate column-symmetric polynomials, an\nextension of symmetric polynomials that take matrices as input. Consequently,\nwe establish an explicit relationship between the size of the transformer\nnetwork and its approximation capability, leveraging the parameter efficiency\nof transformers and their compatibility with symmetry by focusing on the\nalgebraic properties of symmetric polynomials.",
        "In reinforcement learning, universal successor features (SFs) are a way to\nprovide zero-shot adaptation to new tasks at test time: they provide optimal\npolicies for all downstream reward functions lying in the linear span of a set\nof base features. But it is unclear what constitutes a good set of base\nfeatures, that could be useful for a wide set of downstream tasks beyond their\nlinear span. Laplacian eigenfunctions (the eigenfunctions of\n$\\Delta+\\Delta^\\ast$ with $\\Delta$ the Laplacian operator of some reference\npolicy and $\\Delta^\\ast$ that of the time-reversed dynamics) have been argued\nto play a role, and offer good empirical performance.\n  Here, for the first time, we identify the optimal base features based on an\nobjective criterion of downstream performance, in a non-tautological way\nwithout assuming the downstream tasks are linear in the features. We do this\nfor three generic classes of downstream tasks: reaching a random goal state,\ndense random Gaussian rewards, and random ``scattered'' sparse rewards. The\nfeatures yielding optimal expected downstream performance turn out to be the\n\\emph{same} for these three task families. They do not coincide with Laplacian\neigenfunctions in general, though they can be expressed from $\\Delta$: in the\nsimplest case (deterministic environment and decay factor $\\gamma$ close to\n$1$), they are the eigenfunctions of $\\Delta^{-1}+(\\Delta^{-1})^\\ast$.\n  We obtain these results under an assumption of large behavior cloning\nregularization with respect to a reference policy, a setting often used for\noffline RL. Along the way, we get new insights into\nKL-regularized\\option{natural} policy gradient, and into the lack of SF\ninformation in the norm of Bellman gaps.",
        "A signal consistent with the $\\Omega$(2012) baryon has been observed with a\nsignificance of $15\\sigma$ in pp collisions at $\\sqrt{s} = 13$ TeV at the LHC.\nIn this paper, the analysis technique is described and measurements of the mass\nand width of the $\\Omega$(2012) are reported, along with the first measurement\nof its transverse-momentum spectrum and yield. This paper corroborates the\nobservation by Belle of this excited $\\Omega$ state and the observation that\nthe $\\Omega$(2012) has a rather narrow width for a strongly decaying resonance.\nThe yield measurement is combined with a statistical thermal model calculation\nof strange baryon yield ratios to obtain estimates of the $\\Omega{\\rm\n(2012)}^{-} \\rightarrow \\Xi\\overline{\\rm K}$ branching ratios. These results\nwill improve our understanding of the internal structure and mass spectrum of\nexcited baryon states and serve as a baseline for searches regarding\nmodifications of these properties in high-temperature media."
      ]
    }
  },
  {
    "id":2411.05237,
    "research_type":"basic",
    "start_id":"b0",
    "start_title":"Factors Influencing Physicians' Clinical Decision-making at Upazila Health Complexes in Bangladesh",
    "start_abstract":"Selecting the most appropriate treatment for each patient is key activity in patient-physician encounters and providing healthcare services. Achieving desirable clinical goals mostly depends on making right decision at time any setting. But little known about physicians' decision-making primary care setting Bangladesh. Therefore, this study explored factors that influence decisions prescribing medications, ordering pathologic tests, counseling patients, average length of visits a consultation session, referral patients to other physicians or hospitals by Upazila Health Complexes (UHCs) country. It also structure social networks their association with process.This was cross-sectional descriptive used data collected from 85 physicians. The respondents, who work UHCs Rajshahi Division, were selected purposively. analyzed statistics including frequency, percentage, one-way analysis variance, linear regression understand relationships among variables.The results reveal multiple visits, referring UHCs. Most prescribe drugs keeping mind purchasing capacity. Risk violence patients' relatives better management are two decisions. professional personal play an influential role process. found dedicate 16.17 minutes session. influenced various distance between residence workplace, level education, number colleagues whom they have regular contact can seek help.The yielded some novel insights complexity everyday tasks would be interest public health researchers policy makers.",
    "start_categories":[
      "Healthcare"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Interactive Teaching Algorithms for Inverse Reinforcement Learning"
      ],
      "abstract":[
        "We study the problem of inverse reinforcement learning (IRL) with added twist that learner is assisted by a helpful teacher. More formally, we tackle following algorithmic question: How could teacher provide an informative sequence demonstrations to IRL speed up process? present interactive teaching framework where adaptively chooses next demonstration based on learner's current policy. In particular, design algorithms for two concrete settings: omniscient setting has full knowledge about dynamics and blackbox minimal knowledge. Then, sequential variant popular MCE-IRL prove convergence guarantees our algorithm in setting. Extensive experiments car driving simulator environment show progress can be speeded drastically as compared uninformative"
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Hamilton-Jacobi equations involving a Caputo time-fractional derivative",
        "Uniting Quantum Processing Nodes of Cavity-coupled Ions with Rare-earth\n  Quantum Repeaters Using Single-photon Pulse Shaping Based on Atomic Frequency\n  Comb",
        "Orderable Thompson-like groups arising from Ore categories",
        "Less Biased Noise Scale Estimation for Threshold-Robust RANSAC",
        "Dynami-CAL GraphNet: A Physics-Informed Graph Neural Network Conserving\n  Linear and Angular Momentum for Dynamical Systems",
        "Non-(strong) ergodicity criteria for discrete time Markov chains on\n  general state spaces",
        "Benchmark on Peer Review Toxic Detection: A Challenging Task with a New\n  Dataset",
        "Noise avalanche and its quantum quenching in bosonic chains with random\n  off-diagonal disorder",
        "MIRROR: Multi-Modal Pathological Self-Supervised Representation Learning\n  via Modality Alignment and Retention",
        "Towards Data-Efficient Pretraining for Atomic Property Prediction",
        "Oscillations Make Neural Networks Robust to Quantization",
        "Pulling Back Theorem for Generalizing the Diagonal Averaging Principle\n  in Symplectic Geometry Mode Decomposition and Singular Spectrum Analysis",
        "Self-Adaptive Ising Machines for Constrained Optimization",
        "A $p$-adic Gross-Zagier formula for twisted triple product $p$-adic\n  $L$-functions attached to finite slope families",
        "From High-Entropy Alloys to Alloys with High Entropy: A New Paradigm in\n  Materials Science and Engineering for Advancing Sustainable Metallurgy",
        "CVKAN: Complex-Valued Kolmogorov-Arnold Networks",
        "Fair densest subgraph across multiple graphs",
        "QBIOL: A quantum bioelectrochemical software based on point stochastic\n  processes",
        "Existence and Design of Target Output Controllers",
        "The Extraordinary Long-lasting Infrared Echo of PS16dtm Reveals an\n  Extremely Energetic Nuclear Outburst",
        "ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant\n  Tightness",
        "Evolution of X-ray Gas in SN 1987A from 2007 to 2021: Ring Fading and\n  Ejecta Brightening Unveiled through Differential Emission Measure Analysis",
        "Ab initio thermal conductivity of Ge$_x$Sn$_{1-x}$O$_2$ alloys",
        "New Stress-dependent Elastic Wave Velocity Models for Reservoir Rocks\n  with Applications",
        "Many-body effects of two-level systems in superconducting qubits",
        "GRIP: A General Robotic Incremental Potential Contact Simulation Dataset\n  for Unified Deformable-Rigid Coupled Grasping",
        "TimelineKGQA: A Comprehensive Question-Answer Pair Generator for\n  Temporal Knowledge Graphs",
        "InfoBridge: Mutual Information estimation via Bridge Matching",
        "Kise-Manitow's Hand in Space: Securing Communication and Connections in\n  Space"
      ],
      "abstract":[
        "We prove a representation formula of intrinsic Hopf-Lax type for subsolutions\nto Hamilton-Jacobi equations involving a Caputo time-fractional derivative.",
        "We present an architecture for remotely connecting cavity-coupled trapped\nions via a quantum repeater based on rare-earth-doped crystals. The main\nchallenge for its realization lies in interfacing these two physical platforms,\nwhich produce photons with a typical temporal mismatch of one or two orders of\nmagnitude. To address this, we propose an efficient protocol that enables\ncustom temporal reshaping of single-photon pulses whilst preserving purity. Our\napproach is to modify a commonly used memory protocol, called atomic frequency\ncomb, for systems exhibiting inhomogeneous broadening like rare-earth-doped\ncrystals. Our results offer a viable solution for uniting quantum processing\nnodes with a quantum repeater backbone.",
        "We give sufficient conditions for left- and bi-orderability of fundamental\ngroups of Ore categories in terms of indirect factors, including Thompson\ngroups and many of their generalizations. Besides recovering known results, we\nprove that braided groups of fractions of digit rewriting systems (which\ngeneralize braided Thompson groups to the wider setting of topological full\ngroups of edge shift) are left-orderable, and that their purely braided\ncounterparts are bi-orderable. In particular, the braided Houghton groups are\nleft-orderable.",
        "The gold-standard for robustly estimating relative pose through image\nmatching is RANSAC. While RANSAC is powerful, it requires setting the inlier\nthreshold that determines whether the error of a correspondence under an\nestimated model is sufficiently small to be included in its consensus set.\nSetting this threshold is typically done by hand, and is difficult to tune\nwithout a access to ground truth data. Thus, a method capable of automatically\ndetermining the optimal threshold would be desirable. In this paper we revisit\ninlier noise scale estimation, which is an attractive approach as the inlier\nnoise scale is linear to the optimal threshold. We revisit the noise scale\nestimation method SIMFIT and find bias in the estimate of the noise scale. In\nparticular, we fix underestimates from using the same data for fitting the\nmodel as estimating the inlier noise, and from not taking the threshold itself\ninto account. Secondly, since the optimal threshold within a scene is\napproximately constant we propose a multi-pair extension of SIMFIT++, by\nfiltering of estimates, which improves results. Our approach yields robust\nperformance across a range of thresholds, shown in Figure 1.",
        "Accurate, interpretable, and real-time modeling of multi-body dynamical\nsystems is essential for predicting behaviors and inferring physical properties\nin natural and engineered environments. Traditional physics-based models face\nscalability challenges and are computationally demanding, while data-driven\napproaches like Graph Neural Networks (GNNs) often lack physical consistency,\ninterpretability, and generalization. In this paper, we propose Dynami-CAL\nGraphNet, a Physics-Informed Graph Neural Network that integrates the learning\ncapabilities of GNNs with physics-based inductive biases to address these\nlimitations. Dynami-CAL GraphNet enforces pairwise conservation of linear and\nangular momentum for interacting nodes using edge-local reference frames that\nare equivariant to rotational symmetries, invariant to translations, and\nequivariant to node permutations. This design ensures physically consistent\npredictions of node dynamics while offering interpretable, edge-wise linear and\nangular impulses resulting from pairwise interactions. Evaluated on a 3D\ngranular system with inelastic collisions, Dynami-CAL GraphNet demonstrates\nstable error accumulation over extended rollouts, effective extrapolations to\nunseen configurations, and robust handling of heterogeneous interactions and\nexternal forces. Dynami-CAL GraphNet offers significant advantages in fields\nrequiring accurate, interpretable, and real-time modeling of complex multi-body\ndynamical systems, such as robotics, aerospace engineering, and materials\nscience. By providing physically consistent and scalable predictions that\nadhere to fundamental conservation laws, it enables the inference of forces and\nmoments while efficiently handling heterogeneous interactions and external\nforces.",
        "For discrete time Markov chains on general state spaces, we provide criteria\nfor non-ergodicity and non-strong ergodicity. By taking advantage of minimal\nnon-negative solution theory, our criteria are in terms of the existence of\nsolutions to inequalities involving the one step transition semigroup of the\nchain. Based on Dynkin's formula, Lyapunov-type conditions for non-strong\nergodicity are also obtained.",
        "Peer review is crucial for advancing and improving science through\nconstructive criticism. However, toxic feedback can discourage authors and\nhinder scientific progress. This work explores an important but underexplored\narea: detecting toxicity in peer reviews. We first define toxicity in peer\nreviews across four distinct categories and curate a dataset of peer reviews\nfrom the OpenReview platform, annotated by human experts according to these\ndefinitions. Leveraging this dataset, we benchmark a variety of models,\nincluding a dedicated toxicity detection model, a sentiment analysis model,\nseveral open-source large language models (LLMs), and two closed-source LLMs.\nOur experiments explore the impact of different prompt granularities, from\ncoarse to fine-grained instructions, on model performance. Notably,\nstate-of-the-art LLMs like GPT-4 exhibit low alignment with human judgments\nunder simple prompts but achieve improved alignment with detailed instructions.\nMoreover, the model's confidence score is a good indicator of better alignment\nwith human judgments. For example, GPT-4 achieves a Cohen's Kappa score of 0.56\nwith human judgments, which increases to 0.63 when using only predictions with\na confidence score higher than 95%. Overall, our dataset and benchmarks\nunderscore the need for continued research to enhance toxicity detection\ncapabilities of LLMs. By addressing this issue, our work aims to contribute to\na healthy and responsible environment for constructive academic discourse and\nscientific collaboration.",
        "Here we discuss a phenomenon of sharp increase in the photon number noise at\ninitial stages of propagation in tight-binding bosonic chains with off-diagonal\ndisorder. Such a \"noise avalanche\" occurs under classical coherent excitation\nof waveguides and leads to high super-thermal photon bunching. Additional\nclassical excitation slows but cannot quench this noise avalanche. However, an\nadditional single-photon excitation stops the avalanche.",
        "Histopathology and transcriptomics are fundamental modalities in oncology,\nencapsulating the morphological and molecular aspects of the disease.\nMulti-modal self-supervised learning has demonstrated remarkable potential in\nlearning pathological representations by integrating diverse data sources.\nConventional multi-modal integration methods primarily emphasize modality\nalignment, while paying insufficient attention to retaining the\nmodality-specific structures. However, unlike conventional scenarios where\nmulti-modal inputs share highly overlapping features, histopathology and\ntranscriptomics exhibit pronounced heterogeneity, offering orthogonal yet\ncomplementary insights. Histopathology provides morphological and spatial\ncontext, elucidating tissue architecture and cellular topology, whereas\ntranscriptomics delineates molecular signatures through gene expression\npatterns. This inherent disparity introduces a major challenge in aligning them\nwhile maintaining modality-specific fidelity. To address these challenges, we\npresent MIRROR, a novel multi-modal representation learning method designed to\nfoster both modality alignment and retention. MIRROR employs dedicated encoders\nto extract comprehensive features for each modality, which is further\ncomplemented by a modality alignment module to achieve seamless integration\nbetween phenotype patterns and molecular profiles. Furthermore, a modality\nretention module safeguards unique attributes from each modality, while a style\nclustering module mitigates redundancy and enhances disease-relevant\ninformation by modeling and aligning consistent pathological signatures within\na clustering space. Extensive evaluations on TCGA cohorts for cancer subtyping\nand survival analysis highlight MIRROR's superior performance, demonstrating\nits effectiveness in constructing comprehensive oncological feature\nrepresentations and benefiting the cancer diagnosis.",
        "This paper challenges the recent paradigm in atomic property prediction that\nlinks progress to growing dataset sizes and computational resources. We show\nthat pretraining on a carefully selected, task-relevant dataset can match or\neven surpass large-scale pretraining, while using as little as 1\/24th of the\ncomputational cost. We introduce the Chemical Similarity Index (CSI), a novel\nmetric inspired by computer vision's Fr\\'echet Inception Distance, for\nmolecular graphs which quantifies the alignment between upstream pretraining\ndatasets and downstream tasks. By selecting the most relevant dataset with\nminimal CSI distance, we show that models pretrained on a smaller, focused\ndataset consistently outperform those pretrained on massive, mixed datasets\nsuch as JMP, even when those larger datasets include the relevant dataset.\nCounterintuitively, we also find that indiscriminately adding more data can\ndegrade model performance when the additional data poorly aligns with the task\nat hand. Our findings highlight that quality often outperforms quantity in\npretraining for atomic property prediction.",
        "We challenge the prevailing view that oscillations in Quantization Aware\nTraining (QAT) are merely undesirable artifacts caused by the Straight-Through\nEstimator (STE). Through theoretical analysis of QAT in linear models, we\ndemonstrate that the gradient of the loss function can be decomposed into two\nterms: the original full-precision loss and a term that causes quantization\noscillations. Based on these insights, we propose a novel regularization method\nthat induces oscillations to improve quantization robustness. Contrary to\ntraditional methods that focuses on minimizing the effects of oscillations, our\napproach leverages the beneficial aspects of weight oscillations to preserve\nmodel performance under quantization. Our empirical results on ResNet-18 and\nTiny ViT demonstrate that this counter-intuitive strategy matches QAT accuracy\nat >= 3-bit weight quantization, while maintaining close to full precision\naccuracy at bits greater than the target bit. Our work therefore provides a new\nperspective on model preparation for quantization, particularly for finding\nweights that are robust to changes in the bit of the quantizer -- an area where\ncurrent methods struggle to match the accuracy of QAT at specific bits.",
        "The symplectic geometry mode decomposition (SGMD) is a powerful method for\nanalyzing time sequences. The SGMD is based on the upper conversion via\nembedding and down conversion via diagonal averaging principle (DAP) inherited\nfrom the singular spectrum analysis (SSA). However, there are two defects in\nthe DAP: it just hold for the time delay $\\tau=1$ in the trajectory matrix and\nit fails for the time sequence of type-1 with the form $X=\\{x[n]\\}^N_{n=1}$. In\norder to overcome these disadvantages, the inverse step for embedding is\nexplored with binary Diophantine equation in number theory. The contributions\nof this work lie in three aspects: firstly, the pulling back theorem is\nproposed and proved, which state the general formula for converting the\ncomponent of trajectory matrix to the component of time sequence for the\ngeneral representation of time sequence and for any time delay $\\tau\\ge 1$;\nsecondly a unified framework for decomposing both the deterministic and random\ntime sequences into multiple modes is presented and explained; finally, the\nguidance of configuring the time delay is suggested, namely the time delay\nshould be selected in a limited range via balancing the efficiency of matrix\ncomputation and accuracy of state estimation. It could be expected that the\npulling back theorem will help the researchers and engineers to deepen the\nunderstanding of the theory and extend the applications of the SGMD and SSA in\nanalyzing time sequences.",
        "Ising machines (IM) are physics-inspired alternatives to von Neumann\narchitectures for solving hard optimization tasks. By mapping binary variables\nto coupled Ising spins, IMs can naturally solve unconstrained combinatorial\noptimization problems such as finding maximum cuts in graphs. However, despite\ntheir importance in practical applications, constrained problems remain\nchallenging to solve for IMs that require large quadratic energy penalties to\nensure the correspondence between energy ground states and constrained optimal\nsolutions. To relax this requirement, we propose a self-adaptive IM that\niteratively shapes its energy landscape using a Lagrange relaxation of\nconstraints and avoids prior tuning of penalties. Using a probabilistic-bit\n(p-bit) IM emulated in software, we benchmark our algorithm with\nmultidimensional knapsack problems (MKP) and quadratic knapsack problems (QKP),\nthe latter being an Ising problem with linear constraints. For QKP with 300\nvariables, the proposed algorithm finds better solutions than state-of-the-art\nIMs such as Fujitsu's Digital Annealer and requires 7,500x fewer samples. Our\nresults show that adapting the energy landscape during the search can speed up\nIMs for constrained optimization.",
        "Our main objective in the present paper is to generalise the work of\nBlanco-Chac\\'{o}n and Fornea on the $p$-adic Gross-Zagier formula for twisted\ntriple product $p$-aidc $L$-function. We extend their main result to the case\nof finite slope families of Hilbert modular forms and also allow the prime $p$\nto be inert in the real quadratic field $L$.",
        "The development of high-entropy alloys (HEAs) has marked a paradigm shift in\nalloy design, moving away from traditional methods that prioritize a dominant\nbase metal enhanced by minor elements. HEAs instead incorporate multiple\nalloying elements with no single dominant component, broadening the scope of\nalloy design. This shift has led to the creation of diverse alloys with high\nentropy (AHEs) families, including high-entropy steels, superalloys, and\nintermetallics, each highlighting the need to consider additional factors such\nas stacking fault energy (SFE), lattice misfit, and anti-phase boundary energy\n(APBE) due to their significant influence on microstructure and performance.\nLeveraging multiple elements in alloying opens up promising possibilities for\ndeveloping new alloys from multi-component scrap and electronic waste, reducing\nreliance on critical metals and emphasizing the need for advanced data\ngeneration techniques. With the vast possibilities offered by these\nmulti-component feedstocks, modelling and Artificial Intelligence based tools\nare essential to efficiently explore and optimize new alloys, supporting\nsustainable progress in metallurgy. These advancements call for a reimagined\nalloy design framework, emphasizing robust data acquisition, alternative design\nparameters, and advanced computational tools over traditional\ncomposition-focused methodologies.",
        "In this work we propose CVKAN, a complex-valued KAN, to join the intrinsic\ninterpretability of KANs and the advantages of Complex-Valued Neural Networks\n(CVNNs). We show how to transfer a KAN and the necessary associated mechanisms\ninto the complex domain. To confirm that CVKAN meets expectations we conduct\nexperiments on symbolic complex-valued function fitting and physically\nmeaningful formulae as well as on a more realistic dataset from knot theory.\nOur proposed CVKAN is more stable and performs on par or better than\nreal-valued KANs while requiring less parameters and a shallower network\narchitecture, making it more explainable.",
        "Many real-world networks can be modeled as graphs. Finding dense subgraphs is\na key problem in graph mining with applications in diverse domains. In this\npaper, we consider two variants of the densest subgraph problem where multiple\ngraph snapshots are given and the goal is to find a fair densest subgraph\nwithout over-representing the density among the graph snapshots. More formally,\ngiven a set of graphs and input parameter $\\alpha$, we find a dense subgraph\nmaximizing the sum of densities across snapshots such that the difference\nbetween the maximum and minimum induced density is at most $\\alpha$. We prove\nthat this problem is NP-hard and present an integer programming based, exact\nalgorithm and a practical polynomial-time heuristic. We also consider a\nminimization variant where given an input parameter $\\sigma$, we find a dense\nsubgraph which minimizes the difference between the maximum and minimum density\nwhile inducing a total density of at least $\\sigma$ across the graph snapshots.\nWe prove the NP-hardness of the problem and propose two algorithms: an\nexponential time algorithm based on integer programming and a greedy algorithm.\nWe present an extensive experimental study that shows that our algorithms can\nfind the ground truth in synthetic dataset and produce good results in\nreal-world datasets. Finally, we present case studies that show the usefulness\nof our problem.",
        "Bioelectrochemistry is crucial for understanding biological functions and\ndriving applications in synthetic biology, healthcare, and catalysis. However,\ncurrent simulation methods fail to capture both the stochastic nature of\nmolecular motion and electron transfer across the relevant picosecond-to-minute\ntimescales. We present QBIOL, a web-accessible software that integrates\nmolecular dynamics, applied mathematics, GPU programming, and quantum charge\ntransport to address this challenge. QBIOL enables quantitative stochastic\nelectron transfer simulations and has the potential to reproduce numerically\nany (bio) electrochemical experiments. We illustrate this potential by\ncomparing our simulations with experimental data on the current generated by\nelectrode-attached redox-labeled DNA, or by nanoconfined redox species, in\nresponse to a variety of electrical excitation waveforms, configurations of\ninterest in biosensing and catalysis. The adaptable architecture of QBIOL\nextends to the development of devices for quantum and molecular technologies,\npositioning our software as a powerful tool for enabling new research in this\nrapidly evolving field.",
        "This paper introduces new conditions for target output controllability and\nprovides existence conditions for placing a specific number of poles with a\ntarget output controller. Additionally, an algorithm is presented for the\ndesign of a target output controller. Controllability of the system under\nconsideration is not required for designing target output controllers in this\ncontext. The findings in this paper extend the principles of full state\nfeedback control. Moreover, we present conditions for static output feedback\ncontrol under specific constraints. Several numerical examples are provided to\nillustrate the results.",
        "PS16dtm is one of the earliest reported candidate tidal disruption events\n(TDEs) in active galactic nuclei (AGNs) and displays a remarkably bright and\nlong-lived infrared (IR) echo revealed by multi-epoch photometry from the\nWide-field Infrared Survey Explorer (WISE). After a rapid rise in the first\nyear, the echo remains persistently at a high state from July 2017 to July\n2024, the latest epoch, and keeps an almost constant color. We have fitted the\nextraordinary IR emission with a refined dust echo model by taking into account\nthe dust sublimation process. The fitting suggests that an extremely giant dust\nstructure with a new inner radius of $\\sim1.6$ pc and an ultra-high peak\nbolometric luminosity, i.e., $\\sim6\\times10^{46} \\rm erg~s^{-1}$ for typical\n0.1$\\mu$m-sized silicate grain, is required to account for the IR echo. This\nwork highlights the distinctive value of IR echoes in measuring the accurate\nintrinsic bolometric luminosity, and thus the total radiated energy of TDEs,\nwhich could be severely underestimated by traditional methods, i.e. probably by\nmore than an order of magnitude in PS16dtm. Such large energetic output\ncompared to normal TDEs could be boosted by the pre-existing accretion disk and\ngas clouds around the black hole. Our model can be validated in the near future\nby IR time-domain surveys such as Near-Earth Object (NEO) Surveyor, given the\nrecent retirement of WISE. In addition, the potential for spatially resolving a\nreceding dusty torus after a TDE could also be an exciting subject in the era\nof advanced IR interferometry.",
        "Fitting a body to a 3D clothed human point cloud is a common yet challenging\ntask. Traditional optimization-based approaches use multi-stage pipelines that\nare sensitive to pose initialization, while recent learning-based methods often\nstruggle with generalization across diverse poses and garment types. We propose\nEquivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline\nthat estimates cloth-to-body surface mapping through locally approximate SE(3)\nequivariance, encoding tightness as displacement vectors from the cloth surface\nto the underlying body. Following this mapping, pose-invariant body features\nregress sparse body markers, simplifying clothed human fitting into an\ninner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show\nthat ETCH significantly outperforms state-of-the-art methods -- both\ntightness-agnostic and tightness-aware -- in body fitting accuracy on loose\nclothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant\ntightness design can even reduce directional errors by (67.2% ~ 89.8%) in\none-shot (or out-of-distribution) settings. Qualitative results demonstrate\nstrong generalization of ETCH, regardless of challenging poses, unseen shapes,\nloose clothing, and non-rigid dynamics. We will release the code and models\nsoon for research purposes at https:\/\/boqian-li.github.io\/ETCH\/.",
        "As the nearest supernova (SN) observed since Kepler's SN of 1604, SN 1987A\nprovides an unprecedented opportunity to study in detail the early evolution of\nsupernova remnants (SNRs). Despite extensive studies through both observations\nand simulations, there is still an urgent need for a more effective approach to\nintegrate the results from two sides. In this study, we conducted a detailed\ndifferential emission measure (DEM) analysis on the XMM-Newton observations\ntaken in 2007 to 2021 to characterize the continuous temperature structure of\nSN 1987A, which can be better compared with simulations. The X-ray plasma\nexhibit a temperature distribution with a major peak at $\\sim0.5$-$1$ keV and a\nhigh-temperature tail extending to $\\gtrsim5$ keV. The emission measure (EM) of\nthe major peak started to decline around 2014, while the EM of the tail\ncontinued increasing and appears to have formed a secondary peak at $\\sim3$-$5$\nkeV in recent years. Our DEM results consistent well with simulations, which\nhelp to further identify the major peak as originating from the equatorial ring\nand the secondary peak as arising from the newly shocked ejecta. Together with\nthe simulations, our DEM analysis reveals recent fading of the ring and\nbrightening of the ejecta in X-rays from SN 1987A. Additionally, we observed a\nrecent decrease in the centroid energy of Fe K line, providing further evidence\nof newly shocked ejecta.",
        "Rutile GeO2 is an emerging ultra-wide band gap semiconductor (UWBG) that has\ndemonstrated excellent potential for applications in power electronic devices.\nAlloys of rutile SnO2, a well-established UWBG semiconducting oxide, with GeO2\nare promising for tuning the material properties for applications. The thermal\nconductivity, in particular, is a key property which is significantly impacted\nby alloy disorder, but which is also essential in assessing the operation and\ndegradation of materials in high-power electronic applications. Here, we\npresent first-principles calculations of the thermal conductivity of rutile\nGeO2, SnO2, and their alloys, and quantify the effects of scattering by alloy\ndisorder, temperature, and isotope mass distribution. We show that the\nrelatively high thermal conductivity of the binary compounds is reduced by\nalloying, grain boundaries, and isotope disorder. However, we also find that\nthe room-temperature thermal conductivity of the alloys is still comparable to\nor surpasses the values for beta-Ga2O3, an established UWBG semiconducting\noxide. Our findings provide a roadmap for the codesign of the thermal\nproperties of rutile GexSn1-xO2 alloys for electronic device applications.",
        "This study presents new elastic velocity-effective stress laws for reservoir\nrocks. These models are grounded in previously established correlations between\nelastic modulus and porosity, which incorporate critical porosity. The accuracy\nof the models is validated against wave velocities from 38 core samples,\nyielding coefficients of determination ($\\mathrm{R}^2$) of 0.9994 for\ncompressional wave and 0.9985 for shear wave. A sensitivity analysis reveals\nthat the maximum uncertainties for compressional and shear waves are less than\n$\\pm$5.5% and $\\pm$7.5%, respectively. To demonstrate the applicability of the\nproposed models, a case study was conducted on three wells in the Northern\nCarnarvon Basin, where the new elastic wave velocity-effective stress laws\nproduced reliable predictions for velocity logs in the studied formations. The\nrelationships reported herein may prove beneficial for hydrocarbon exploration,\nproduction, and ensuring drilling safety in both unconventional and\nconventional fields.",
        "Superconducting qubits are often adversely affected by two-level systems\n(TLSs) within the Josephson junction, which contribute to decoherence and\nsubsequently limit the performance of the qubit. By treating the TLS as a soft\n(i.e., low-frequency) bosonic mode localized in real space, we find that a\nsingle TLS in either the amorphous oxide surface or the superconducting bulk\nmay result in a localized \"hot spot\" of amplified Josephson energy. Such\namplification is shown to have a non-negligible effect on the $T_1$ time of\ncertain superconducting qubits, regardless of whether or not the TLS is on\nresonance with the qubit frequency. With this study, we identify sources of\ndecoherence unique to the superconducting element of superconducting quantum\ndevices.",
        "Grasping is fundamental to robotic manipulation, and recent advances in\nlarge-scale grasping datasets have provided essential training data and\nevaluation benchmarks, accelerating the development of learning-based methods\nfor robust object grasping. However, most existing datasets exclude deformable\nbodies due to the lack of scalable, robust simulation pipelines, limiting the\ndevelopment of generalizable models for compliant grippers and soft\nmanipulands. To address these challenges, we present GRIP, a General Robotic\nIncremental Potential contact simulation dataset for universal grasping. GRIP\nleverages an optimized Incremental Potential Contact (IPC)-based simulator for\nmulti-environment data generation, achieving up to 48x speedup while ensuring\nefficient, intersection- and inversion-free simulations for compliant grippers\nand deformable objects. Our fully automated pipeline generates and evaluates\ndiverse grasp interactions across 1,200 objects and 100,000 grasp poses,\nincorporating both soft and rigid grippers. The GRIP dataset enables\napplications such as neural grasp generation and stress field prediction.",
        "Question answering over temporal knowledge graphs (TKGs) is crucial for\nunderstanding evolving facts and relationships, yet its development is hindered\nby limited datasets and difficulties in generating custom QA pairs. We propose\na novel categorization framework based on timeline-context relationships, along\nwith \\textbf{TimelineKGQA}, a universal temporal QA generator applicable to any\nTKGs. The code is available at: \\url{https:\/\/github.com\/PascalSun\/TimelineKGQA}\nas an open source Python package.",
        "Diffusion bridge models have recently become a powerful tool in the field of\ngenerative modeling. In this work, we leverage their power to address another\nimportant problem in machine learning and information theory - the estimation\nof the mutual information (MI) between two random variables. We show that by\nusing the theory of diffusion bridges, one can construct an unbiased estimator\nfor data posing difficulties for conventional MI estimators. We showcase the\nperformance of our estimator on a series of standard MI estimation benchmarks.",
        "The increasing complexity of space systems, coupled with their critical\noperational roles, demands a robust, scalable, and sustainable security\nframework. This paper presents a novel system-of-systems approach for the\nupcoming Lunar Gateway. We demonstrate the application of the\nsecure-by-component approach to the two earliest deployed systems in the\nGateway, emphasizing critical security controls both internally and for\nexternal communication and connections. Additionally, we present a phased\napproach for the integration of Canadarm3, addressing the unique security\nchallenges that arise from both inter-system interactions and the arm's\nautonomous capabilities."
      ]
    }
  },
  {
    "id":2411.05237,
    "research_type":"basic",
    "start_id":"b3",
    "start_title":"Interactive Teaching Algorithms for Inverse Reinforcement Learning",
    "start_abstract":"We study the problem of inverse reinforcement learning (IRL) with added twist that learner is assisted by a helpful teacher. More formally, we tackle following algorithmic question: How could teacher provide an informative sequence demonstrations to IRL speed up process? present interactive teaching framework where adaptively chooses next demonstration based on learner's current policy. In particular, design algorithms for two concrete settings: omniscient setting has full knowledge about dynamics and blackbox minimal knowledge. Then, sequential variant popular MCE-IRL prove convergence guarantees our algorithm in setting. Extensive experiments car driving simulator environment show progress can be speeded drastically as compared uninformative",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Factors Influencing Physicians' Clinical Decision-making at Upazila Health Complexes in Bangladesh"
      ],
      "abstract":[
        "Selecting the most appropriate treatment for each patient is key activity in patient-physician encounters and providing healthcare services. Achieving desirable clinical goals mostly depends on making right decision at time any setting. But little known about physicians' decision-making primary care setting Bangladesh. Therefore, this study explored factors that influence decisions prescribing medications, ordering pathologic tests, counseling patients, average length of visits a consultation session, referral patients to other physicians or hospitals by Upazila Health Complexes (UHCs) country. It also structure social networks their association with process.This was cross-sectional descriptive used data collected from 85 physicians. The respondents, who work UHCs Rajshahi Division, were selected purposively. analyzed statistics including frequency, percentage, one-way analysis variance, linear regression understand relationships among variables.The results reveal multiple visits, referring UHCs. Most prescribe drugs keeping mind purchasing capacity. Risk violence patients' relatives better management are two decisions. professional personal play an influential role process. found dedicate 16.17 minutes session. influenced various distance between residence workplace, level education, number colleagues whom they have regular contact can seek help.The yielded some novel insights complexity everyday tasks would be interest public health researchers policy makers."
      ],
      "categories":[
        "Healthcare"
      ]
    },
    "list":{
      "title":[
        "Cavity-enhanced solid-state nuclear spin gyroscope",
        "Strong-damping limit of quantum Brownian motion in a disordered\n  environment",
        "Non-Hermitian electron-positron annihilation under thermal effects",
        "PLMP -- Point-Line Minimal Problems for Projective SfM",
        "JWST ASPIRE: How Did Galaxies Complete Reionization? Evidence for Excess\n  IGM Transmission around ${\\rm [O\\,{\\scriptstyle III}]}$ Emitters during\n  Reionization",
        "The General Position Problem: A Survey",
        "Edgeworth Expansion for Semi-hard Triplet Loss",
        "Solution of Uncertain Multiobjective Optimization Problems by Using\n  Nonlinear Conjugate Gradient Method",
        "Relatively non-degenerate integrated decay estimates for massless Vlasov\n  fields on Schwarzschild spacetimes",
        "Mass Matrix Rules and the Flat Pattern of Quarks",
        "Imperfect detectors for adversarial tasks with applications to quantum\n  key distribution",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Enhancing Offline Model-Based RL via Active Model Selection: A Bayesian\n  Optimization Perspective",
        "Data-Driven Sequential Sampling for Tail Risk Mitigation",
        "Characterizing the Conformational States of G Protein Coupled Receptors\n  Generated with AlphaFold",
        "Counting Frobenius Pseudoprimes",
        "Discovery of a Highly Anisotropic Type-II Ferromagnetic Weyl State\n  Exhibiting a 3D Quantum Hall Effect",
        "On the Bogomolov-Positselski Conjecture",
        "On Robust Aggregation for Distributed Data",
        "Strong Long-Wave Infrared Optical Response in a Topological\n  Semiconductor with a Mexican Hat Band Structure",
        "Topologically protected edge states in time photonic crystals with\n  chiral symmetry",
        "Probing the ringdown perturbation in binary black hole coalescences with\n  an improved quasi-normal mode extraction algorithm",
        "The extended Dirichlet space and criticality theory for nonlinear\n  Dirichlet forms",
        "Accelerating Equity: Overcoming the Gender Gap in VC Funding",
        "Explicit Codes approaching Generalized Singleton Bound using Expanders",
        "Entropic bottlenecks to nematic ordering in an $RP^{2}$ apolar spin\n  model",
        "Random attraction in TASEP with time-varying hopping rates",
        "The $\\ell_{\\infty}$ Directed Spanning Forest",
        "Variational and nonvariational solutions for double phase variable\n  exponent problems"
      ],
      "abstract":[
        "Solid-state quantum sensors based on ensembles of nitrogen-vacancy (NV)\ncenters in diamond have emerged as powerful tools for precise sensing\napplications. Nuclear spin sensors are particularly well-suited for\napplications requiring long coherence times, such as inertial sensing, but\nremain underexplored due to control complexity and limited optical readout\nefficiency. In this work, we propose cooperative cavity quantum electrodynamic\n(cQED) coupling to achieve efficient nuclear spin readout. Unlike previous cQED\nmethods used to enhance electron spin readout, here we employ two-field\ninterference in the NV hyperfine subspace to directly probe the nuclear spin\ntransitions. We model the nuclear spin NV-cQED system (nNV-cQED) and observe\nseveral distinct regimes, including electromagnetically induced transparency,\nmasing without inversion, and oscillatory behavior. We then evaluate the\nnNV-cQED system as an inertial sensor, indicating a rotation sensitivity\nimproved by three orders of magnitude compared to previous solid-state spin\ndemonstrations. Furthermore, we show that the NV electron spin can be\nsimultaneously used as a comagnetometer, and the four crystallographic axes of\nNVs can be employed for vector resolution in a single nNV-cQED system. These\nresults showcase the applications of two-field interference using the nNV-cQED\nplatform, providing critical insights into the manipulation and control of\nquantum states in hybrid NV systems and unlocking new possibilities for\nhigh-performance quantum sensing.",
        "We consider a microscopic model of an inhomogeneous environment where an\narbitrary quantum system is locally coupled to a harmonic bath via a\nfinite-range interaction. We show that in the overdamped regime the position\ndistribution obeys a classical Kramers-Moyal equation that involves an infinite\nnumber of higher derivatives, implying that the finite bath correlation length\nleads to non-Gaussian Markovian noise. We analytically solve the equation for a\nharmonically bound particle and analyze its non-Gaussian diffusion as well as\nits steady-state properties.",
        "In this paper we examine the thermal effects into the $e^{+}e^{-}\\to\n\\ell^{+}\\ell^{-}$ scattering in a non-hermitian extension of QED. We compute\nthe thermal contributions to this scattering cross-section within the Thermo\nField Dynamics approach. In order to highlight the non-hermitian effects we\nhave considered some limits of interest: i) zero-temperature limit and\nhigh-energy limit and ii) high-temperature regime. Since this type of\nscattering possesses accurate experimental data for the cross-section (for muon\nand tau at the final state) it can be used to set stringent bounds upon the\nnon-hermitian parameters.",
        "We completely classify all minimal problems for Structure-from-Motion (SfM)\nwhere arrangements of points and lines are fully observed by multiple\nuncalibrated pinhole cameras. We find 291 minimal problems, 73 of which have\nunique solutions and can thus be solved linearly. Two of the linear problems\nallow an arbitrary number of views, while all other minimal problems have at\nmost 9 cameras. All minimal problems have at most 7 points and at most 12\nlines. We compute the number of solutions of each minimal problem, as this\ngives a measurement of the problem's intrinsic difficulty, and find that these\nnumber are relatively low (e.g., when comparing with minimal problems for\ncalibrated cameras). Finally, by exploring stabilizer subgroups of\nsubarrangements, we develop a geometric and systematic way to 1) factorize\nminimal problems into smaller problems, 2) identify minimal problems in\nunderconstrained problems, and 3) formally prove non-minimality.",
        "The spatial correlation between galaxies and the Ly$\\alpha$ forest provides\ninsights into how galaxies reionized the Universe. Here, we present initial\nresults on the spatial cross-correlation between [OIII] emitters and Ly$\\alpha$\nforest at 5.4<z<6.5 from the JWST ASPIRE NIRCam\/F356W Grism Spectroscopic\nSurvey in z>6.5 QSO fields. Using data from five QSO fields, we find $2\\sigma$\nevidence for excess Ly$\\alpha$ forest transmission at ~20-40 cMpc around [OIII]\nemitters at z=5.86, indicating that [OIII] emitters reside within a highly\nionized IGM. At smaller scales, the Ly$\\alpha$ forest is preferentially\nabsorbed, suggesting gas overdensities around [OIII] emitters. Comparing with\nmodels including THESAN simulations, we interpret the observed\ncross-correlation as evidence for significant large-scale fluctuations of the\nIGM and the late end of reionization at z<6, characterized by ionized bubbles\nover 50 cMpc around [OIII] emitters. The required UV background necessitates an\nunseen population of faint galaxies around the [OIII] emitters. Furthermore, we\nfind that the number of observed [OIII] emitters near individual transmission\nspikes is insufficient to sustain reionization in their surroundings, even\nassuming all [OIII] emitters harbour AGN with 100 % LyC escape fractions.\nDespite broad agreement, a careful analysis of ASPIRE and THESAN, using the\nobserved host halo mass from the clustering of [OIII] emitters, suggests that\nthe simulations underpredict the observed excess IGM transmission around [OIII]\nemitters, challenging our model of reionization. Potential solutions include\nlarger ionized bubbles at z<6, more enhanced large-scale UV background or\ntemperature fluctuations of the IGM, and possibly a patchy early onset of\nreionization at z>10. Current observational errors are dominated by cosmic\nvariance, meaning future analyses of more QSO fields from JWST will improve the\nresults.",
        "Inspired by a chessboard puzzle of Dudeney, the general position problem in\ngraph theory asks for the largest sets $S$ of vertices in a graph such that no\nthree elements of $S$ lie on a common shortest path. The number of vertices in\nsuch a largest set is the general position number of the graph. This paper\nprovides a survey of this rapidly growing problem, which now has an extensive\nliterature. We cover exact results for various graph classes and the behaviour\nof the general position number under various graph products and operations. We\nalso discuss interesting variations of the general position problem, for\nexample variants corresponding to different graph convexities, as well as\ndynamic, fractional, colouring and game versions of the problem.",
        "We develop a higher-order asymptotic analysis for the semi-hard triplet loss\nusing the Edgeworth expansion. It is known that this loss function enforces\nthat embeddings of similar samples are close while those of dissimilar samples\nare separated by a specified margin. By refining the classical central limit\ntheorem, our approach quantifies the impact of the margin parameter and the\nskewness of the underlying data distribution on the loss behavior. In\nparticular, we derive explicit Edgeworth expansions that reveal first-order\ncorrections in terms of the third cumulant, thereby characterizing non-Gaussian\neffects present in the distribution of distance differences between\nanchor-positive and anchor-negative pairs. Our findings provide detailed\ninsight into the sensitivity of the semi-hard triplet loss to its parameters\nand offer guidance for choosing the margin to ensure training stability.",
        "This paper introduces a nonlinear conjugate gradient method (NCGM) for\naddressing the robust counterpart of uncertain multiobjective optimization\nproblems (UMOPs). Here, the robust counterpart is defined as the minimum across\nobjective-wise worst-case scenarios. There are some drawbacks to using\nscalarization techniques to solve the robust counterparts of UMOPs, such as the\npre-specification and restrictions of weights, and function importance that is\nunknown beforehand. NCGM is free from any kind of priori chosen scalars or\nordering information of objective functions as accepted in scalarization\nmethods. With the help of NCGM, we determine the critical point for the robust\ncounterpart of UMOP, which is the robust critical point for UMOP. To tackle\nthis robust counterpart using the NCGM, the approach involves constructing and\nsolving a subproblem to determine a descent direction. Subsequently, a new\ndirection is derived based on parameter selection methods such as\nFletcher-Reeves, conjugate descent, Dai-Yuan, Polak-Ribi$\\grave{e}$re-Polyak,\nand Hestenes-Stiefel. An Armijo-type inexact line search is employed to\nidentify an appropriate step length. Utilizing descent direction and step\nlength, a sequence is generated, and convergence of the proposed method is\nestablished. The effectiveness of the proposed method is verified and compared\nagainst an existing method using a set of test problems.",
        "In this article, we make use of a weight function capturing the concentration\nphenomenon of unstable future-trapped causal geodesics. A projection $V_+$, on\nthe tangent space of the null-shell, of the associated symplectic gradient\nturns out to enjoy good commutation properties with the massless Vlasov\noperator. This reflects that $V_+f$ decays exponentially locally near the\nphoton sphere, for any smooth solution $f$ to the massless Vlasov equation.\n  By identifying a well-chosen modification of $V_+$, we are able to construct\na $W_{x,p}^{1,1}$ weighted norm for which any smooth solution to the massless\nVlasov equation verifies an integrated local energy decay estimate without\nrelative degeneration. Together with the $r^p$-weighted energy method of\nDafermos--Rodnianski, we establish time decay for the energy norm. This norm\nallows for the control of the energy-momentum tensor $\\mathrm{T}[f]$ as well as\nall its first order derivatives.\n  The method developed in this paper is in particular compatible with\napproaches recently developed for the study of quasi-linear wave equations on\nblack hole spacetimes.",
        "Seeking mass patterns is a key to decoding the unknown flavor puzzles in\nparticle physics. Inspired by quark hierarchical masses, the mass matrix can\nuniversally be factorized into a family-diagonal phase matrix $K_L^q$ and a\nreal symmetric matrix $M_N^q$ characterized by only two parameters. The\nfactorized structure provides model-independent rules to the mass matrix. We\ndemonstrate that the large $\\delta_{CP}$ naturally arises from the degeneracy\nof the first two quark families in the mass hierarchy limit. As an application,\nthe flat pattern with elements close to unity in the matrix is checked by\nfitting quark masses and the CKM mixing. It achieves a precise description of\nflavor structure with minimal parameters.",
        "Security analyses in quantum key distribution (QKD) and other adversarial\nquantum tasks often assume perfect device models. However, real-world\nimplementations often deviate from these models. Thus, it is important to\ndevelop security proofs that account for such deviations from ideality. In this\nwork, we develop a general framework for analysing imperfect threshold\ndetectors, treating uncharacterised device parameters such as dark counts and\ndetection efficiencies as adversarially controlled within some ranges. This\napproach enables a rigorous worst-case analysis, ensuring security proofs\nremain valid under realistic conditions. Our results strengthen the connection\nbetween theoretical security and practical implementations by introducing a\nflexible framework for integrating detector imperfections into adversarial\nquantum protocols.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "Offline model-based reinforcement learning (MBRL) serves as a competitive\nframework that can learn well-performing policies solely from pre-collected\ndata with the help of learned dynamics models. To fully unleash the power of\noffline MBRL, model selection plays a pivotal role in determining the dynamics\nmodel utilized for downstream policy learning. However, offline MBRL\nconventionally relies on validation or off-policy evaluation, which are rather\ninaccurate due to the inherent distribution shift in offline RL. To tackle\nthis, we propose BOMS, an active model selection framework that enhances model\nselection in offline MBRL with only a small online interaction budget, through\nthe lens of Bayesian optimization (BO). Specifically, we recast model selection\nas BO and enable probabilistic inference in BOMS by proposing a novel\nmodel-induced kernel, which is theoretically grounded and computationally\nefficient. Through extensive experiments, we show that BOMS improves over the\nbaseline methods with a small amount of online interaction comparable to only\n$1\\%$-$2.5\\%$ of offline training data on various RL tasks.",
        "Given a finite collection of stochastic alternatives, we study the problem of\nsequentially allocating a fixed sampling budget to identify the optimal\nalternative with a high probability, where the optimal alternative is defined\nas the one with the smallest value of extreme tail risk. We particularly\nconsider a situation where these alternatives generate heavy-tailed losses\nwhose probability distributions are unknown and may not admit any specific\nparametric representation. In this setup, we propose data-driven sequential\nsampling policies that maximize the rate at which the likelihood of falsely\nselecting suboptimal alternatives decays to zero. We rigorously demonstrate the\nsuperiority of the proposed methods over existing approaches, which is further\nvalidated via numerical studies.",
        "G-Protein Coupled Receptors (GPCRs) are integral to numerous physiological\nprocesses and are the target of approximately one-third of FDA-approved\ntherapeutics. Despite their significance, only a limited subset of GPCRs has\nbeen successfully targeted, primarily due to challenges in accurately modeling\ntheir structures. AlphaFold, a state-of-the-art deep learning model, has\ndemonstrated remarkable capability in predicting protein structures with high\naccuracy. This study conducts an evaluation of AlphaFold performance in\npredicting GPCR structures and their conformational states by comparing its\npredictions to experimentally determined structures using metrics such as\naverage deformation between alpha carbon atoms and the Helix 3 - Helix 6\n(H3-H6) distance. Our analysis reveals that both AlphaFold 2 (AF2) and\nAlphaFold 3 (AF3) produce more accurate predictions for GPCRs in inactive\nconformations, with lower activity levels correlating with smaller\ndeformations. Conversely, higher activity levels are associated with increased\nvariability in AlphaFold performance due to difficulties with accurately\npredicting conformational changes upon GPCR activation and ligand binding.\nAdditionally, AlphaFold performance varies across different GPCR classes,\ninfluenced by the availability and quality of training data as well as the\nstructural complexity and diversity of the receptors. These findings\ndemonstrate the potential of AlphaFold in advancing drug discovery efforts,\nwhile also highlighting the necessity for continued refinement to enhance\npredictive accuracy for active conformations.",
        "We generalize the work of Erdos-Pomerance and Fiori-Shallue on counting\nFrobenius pseudoprimes from the cases of degree one and two respectively to\narbitrary degree. More specifically we provide formulas for counting the number\nof false witnesses for a number $n$ with respect to Grantham's Frobenius\nprimality test. We also provide conditional assymptotic lower bounds on the\naverage number of Frobenius pseudoprimes and assymptotic upper bounds on the\nsame.",
        "Topological semimetals, particularly Weyl semimetals (WSMs), are crucial\nplatforms for exploring emergent quantum phenomena due to their unique\nelectronic structures and potential to transition into various topological\nphases. In this study, we report the discovery of a ferromagnetic (FM) type-II\nWSM in Mn(Bi1-xSbx)4Te7, which exhibits a remarkable three-dimensional (3D)\nquantum Hall effect (QHE). By precisely tuning the chemical potential through\nSb doping, we obtained samples with the Fermi level near the charge neutrality\npoint for x = ~ 0.27. This was confirmed by spectroscopy measurements (ARPES\nand STS), and these samples showed strong quantum oscillations along with a key\ntransport signature of a Weyl state - chiral anomaly, and Fermi surface\nreconstruction driven by FM ordering. Our theoretical analysis indicates that\nthis Weyl state evolves from a parent nodal ring state, where higher-order\nk-terms split the nodal line into type-II Weyl nodes. The Weyl state exhibits\nsignificant anisotropy, characterized by a pronounced reduction in Fermi\nvelocity along the kz-axis, likely accounting for the observed 3D QHE. These\nresults not only highlight the exceptional tunability of the Mn(Bi1-xSbx)4Te7\nsystem, where precise control of the chemical potential and magnetic properties\nopens access to novel quantum phases, but also advance the understanding of FM\nWSMs.",
        "Let $p$ be a prime, we say that a Kummerian oriented pro-$p$ group\n$(G,\\theta)$ has the Bogomolov-Positselski property if $I_\\theta(G)$ is a free\npro-$p$ group. We give a new criterion for an oriented pro-$p$ group to have\nthe Bogomolov-Positselski property based on previous work by Positselski\n(arXiv:1405.0965) and Quadrelli and Weigel (arXiv:2103.12438) linking their\nseemingly unrelated approaches and thereby answering a question posed by\nQuadrelli and Weigel.\n  Under further assumptions, we derive two additional criteria. The first of\nwhich strongly resembles an analogue of the Merkujev-Suslin theorem. The second\nallows to relax the conditions given by Positselski in Theorem 2 of\narXiv:1405.0965. In addition, we show how to make those weaker assumptions\ncomputationally effective in some special cases.",
        "When data are stored across multiple locations, directly pooling all the data\ntogether for statistical analysis may be impossible due to communication costs\nand privacy concerns. Distributed computing systems allow the analysis of such\ndata, by getting local servers to separately process their own statistical\nanalyses and using a central processor to aggregate the local statistical\nresults. Naive aggregation of local statistics using simple or weighted\naverages, is vulnerable to contamination within a distributed computing system.\nThis paper develops and investigates a Huber-type aggregation method for\nlocally computed M-estimators to handle contamination in the local estimates.\nOur implementation of this aggregation method requires estimating the\nasymptotic variance-covariance matrix of the M-estimator, which we accomplish\nusing a robust spatial median approach. Theoretically, the Huber-type\naggregation achieves the same convergence rate as if all the data were pooled.\nWe establish its asymptotic normality for making inferences, including\njustifying a two-step approach for detecting contamination in the distributed\ncomputing system. Extensive simulation studies are conducted to validate the\ntheoretical results and the usefulness of our proposed approach is demonstrated\non U.S. airline data.",
        "Light sources and photodetectors operating in the far- to mid-infrared\n(FIR\/MIR) band ($8$-$12~\\rm \\mu m$, $0.1$-$0.15~\\rm eV$) remain relatively\npoorly developed compared to their counterparts operating in the visible and\nnear-infrared ranges, despite extensive application potential for thermal\nimaging, standoff sensing, and other technologies. This is attributable in part\nto the lack of narrow-gap materials ($<0.1~\\rm eV$) with high optical gain and\nabsorption. In this work, a narrow-gap semiconductor, $\\rm Pb_{0.7}Sn_{0.3}Se$,\nis demonstrated to exhibit an optical response $>10\\times$ larger than that of\n$\\rm Hg_{x}Cd_{1-x}Te$ (MCT), the dominant material for FIR\/MIR photodetectors.\nA previous theoretical investigation indicated that chalcogen $p$ and metal $d$\nband inversion in this material creates a Mexican hat band structure (MHBS),\nwhich results in a dramatic increase in the joint density of states at the\noptical transition edge compared to typical semiconductors. This prediction is\nexperimentally validated here using single-crystal specimens of $\\rm\nPb_{0.7}Sn_{0.3}Se$ measured using temperature-dependent spectroscopic\nellipsometry over a wavelength range of $1.7$-$20~\\rm \\mu m$ ($0.73$-$0.062~\\rm\neV$). These measurements demonstrate a large enhancement in extinction\ncoefficient and refractive index characteristic of a MHBS in the vicinity of\nthe absorption edge, in agreement with theoretical predictions. The realization\nof topological semiconductors with a MHBS is expected to lead to\nhigh-efficiency detectors operating in the FIR\/MID range.",
        "Time photonic crystals are media in which their electromagnetic parameters\nare modulated periodically in time, showing promising applications in\nnon-resonant lasers and particle accelerators, among others. Traditionally\nutilized to study space photonic crystals, topological band theory has also\nbeen translated recently to analyze time photonic crystals with time inversion\nsymmetry, enabling the construction of the temporal version of topological edge\nstates. However, temporal disorder can readily break time inversion symmetry in\npractice, hence likely destroying the edge states associated with this type of\ntime photonic crystals. To overcome this limitation, here we propose a new\nclass of time photonic crystals presenting chiral symmetry instead, whose edge\nstates exhibit superior robustness over the time-reversal-symmetry-protected\ncounterparts. Our time photonic crystal is equivalent to a temporal version of\nthe Su-Schrieffer-Heeger model, and the chiral symmetry of this type of time\nphotonic crystals quantizes the winding number defined in the Bloch frequency\nband. Remarkably, random temporal disorders do not impact the eigenfrequencies\nof these chiral-symmetry-protected edge states, while instead enhancing their\ntemporal localizations. Our findings thus provide a promising paradigm to\ncontrol field amplification with exceptional robustness as well as being a\nfeasible platform to investigate various topological phases in time-varying\nmedia.",
        "Using gravitational waves to probe the geometry of the ringing remnant black\nhole formed in a binary black hole coalescence is a well-established way to\ntest Einstein's theory of general relativity. However, doing so requires\nknowledge of when the predictions of black hole perturbation theory, i.e.,\nquasi-normal modes (QNMs), are a valid description of the emitted gravitational\nwave as well as what the amplitudes of these excitations are. In this work, we\ndevelop an algorithm to systematically extract QNMs from the ringdown of black\nhole merger simulations. Our algorithm improves upon previous ones in three\nways: it fits over the two-sphere, enabling a complete model of the strain; it\nperforms a reverse-search in time for QNMs using a more robust nonlinear least\nsquares routine called \\texttt{VarPro}; and it checks the variance of QNM\namplitudes, which we refer to as ``stability'', over an interval matching the\nnatural time scale of each QNM. Using this algorithm, we not only demonstrate\nthe stability of a multitude of QNMs and their overtones across the parameter\nspace of quasi-circular, non-precessing binary black holes, but we also\nidentify new quadratic QNMs that may be detectable in the near future using\nground-based interferometers. Furthermore, we provide evidence which suggests\nthat the source of remnant black hole perturbations is roughly independent of\nthe overtone index in a given angular harmonic across binary parameter space,\nat least for overtones with $n\\leq2$. This finding may hint at the\nspatiotemporal structure of ringdown perturbations in black hole coalescences,\nas well as the regime of validity of perturbation theory in the ringdown of\nthese events. Our algorithm is made publicly available at the following GitHub\nrepository: https:\/\/github.com\/keefemitman\/qnmfinder.",
        "In this paper we establish the existence of the extended Dirichlet space for\nnonlinear Dirichlet forms under mild conditions. We employ it to introduce and\ncharacterize criticality (recurrence) and subcriticality (transience) and\nestablish basics of a potential theory.",
        "We examine the growing gender gap in venture capital funding, focusing on\naccelerator programs in the U.S. We collect a unique dataset with detailed\ninformation on accelerators and startups. Using a two-stage methodology, we\nfirst estimate a matching model between startups and accelerators, and then use\nits output to analyze the gender gap in post-graduation outcomes through a\ncontrol function approach. Our results show that female-founded startups face a\nsignificant funding disadvantage, primarily due to relocation challenges tied\nto family obligations. However, larger cohorts and higher-quality accelerators\nhelp reduce this gap by offering female founders better networking\nopportunities and mentorship.",
        "We construct a new family of explicit codes that are list decodable to\ncapacity and achieve an optimal list size of $O(\\frac{1}{\\epsilon})$. In\ncontrast to existing explicit constructions of codes achieving list decoding\ncapacity, our arguments do not rely on algebraic structure but utilize simple\ncombinatorial properties of expander graphs.\n  Our construction is based on a celebrated distance amplification procedure\ndue to Alon, Edmonds, and Luby [FOCS'95], which transforms any high-rate code\ninto one with near-optimal rate-distance tradeoff. We generalize it to show\nthat the same procedure can be used to transform any high-rate code into one\nthat achieves list decoding capacity. Our proof can be interpreted as a\n\"local-to-global\" phenomenon for (a slight strengthening of) the generalized\nSingleton bound. Using this construction, for every $R, \\epsilon \\in (0,1)$ and\n$k \\in \\mathbb{N}^+$, we obtain an \\emph{explicit} family of codes $\\mathcal{C}\n\\subseteq \\Sigma^n$, with rate $R$ such that,\n  - They achieve the $\\epsilon$-relaxed generalized Singleton bound: for any $g\n\\in \\Sigma^n$ and any list $\\mathcal{H}$ of at most $k$ codewords, we have, \\[\n\\underset{h \\in \\mathcal{H}}{\\mathbb{E}} [\\Delta(g,h)] ~\\geq~\n\\frac{|\\mathcal{H}|-1}{|\\mathcal{H}|} \\cdot (1 - R - \\epsilon). \\]\n  - The alphabet size is a constant depending only on $\\epsilon$ and $k$.\n  - They can be list decoded up to radius $\\frac{k-1}{k}(1-R-\\epsilon)$, in\ntime $n^{O_{k,\\epsilon}(1)}$.\n  As a corollary of our result, we also obtain the first explicit construction\nof LDPC codes achieving list decoding capacity, and in fact arbitrarily close\nto the generalized Singleton bound.",
        "The Lebwohl-Lasher model of liquid crystals with (d = 2, n = 3) describes\ninteracting apolar spins, with an $RP^{2}$ order-parameter topology.\nSimulations with a modified Wang-Landau Monte Carlo protocol, that includes a\ndensity of states (DoS) factor, had previously found a zero latent-heat\ntransition at $T=T_{n}$ to a novel nematic order, coexisting with unbound\ndefects whose binding is completed only on cooling. We find through this\nentropically augmented MC protocol, that there is a deep dip in the DoS at an\nenergy preceding global ordering, reflecting 'sparse' intermediate\nconfigurations, or entropy barriers. The narrow entropic bottleneck induces a\ncusp in the initially rising nematic correlation length, at a micro-canonical\n'precursor' temperature $T=T_{p}$. A finite-scale cooperativity of defects and\nnematic clusters penetrates the bottleneck at $T_{p}$ to enable a third-order\nphase transition at a lower $T_{n}$: a rare pathway, overlooked by energy-only\nacceptance protocols.",
        "The totally asymmetric simple exclusion principle (TASEP) is a fundamental\nmodel in nonequilibrium statistical mechanics. It describes the stochastic\nunidirectional movement of particles along a 1D chain of ordered sites. We\nconsider the continuous-time version of TASEP with a finite number of sites and\nwith time-varying hopping rates between the sites. We show how to formulate\nthis model as a nonautonomous random dynamical system (NRDS) with a finite\nstate-space. We provide conditions guaranteeing that random pullback and\nforward attractors of such an NRDS exist and consist of singletons. In the\ncontext of the nonautonomous TASEP these conditions imply almost sure\nsynchronization of the individual random paths. This implies in particular that\nperturbations that change the state of the particles along the chain are\n\"filtered out\" in the long run. We demonstrate that the required conditions are\ntight by providing examples where these conditions do not hold and consequently\nthe forward attractor does not exist or the pullback attractor is not a\nsingleton. The results in this paper generalize our earlier results for\nautonomous TASEP in https:\/\/doi.org\/10.1137\/20M131446X and contain these as a\nspecial case.",
        "We study the $\\ell_{\\infty}$\\textit{ directed spanning forest}(DSF), which is\na directed forest with vertex set given by a homogeneous Poisson point process\nsuch that each Poisson point connects to the nearest Poisson point (in\n$\\ell_{\\infty}$ distance) with a strictly larger $y$-coordinate. In this paper,\nwe prove that the $\\ell_{\\infty}$ DSF is connected and we find optimal\nestimates on the tail distribution of coalescing time of two $\\ell_{\\infty}$\nDSF paths. Similar estimates were earlier obtained in \\cite{coupier20212d} for\nthe $\\ell_2$ (Euclidean) DSF and showed that when properly scaled, it converges\nin distribution to the Brownian web. The geometry of $\\ell_\\infty$ balls compel\nus to develop new argument.",
        "In this article, we examine two double-phase variable exponent problems, each\nformulated within a distinct framework. The first problem is non-variational,\nas the nonlinear term may depend on the gradient of the solution. The first\nmain result establishes an existence property from the nonlinear monotone\noperator theory given by Browder and Minty. The second problem is set up within\na variational framework, where we employ a well-known critical point result by\nBonanno and Chinn\\`{\\i}. In both cases, we demonstrate the existence of at\nleast one nontrivial solution. To illustrate the practical application of the\nmain results, we provide examples for each problem."
      ]
    }
  },
  {
    "id":2412.09927,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Three-dimensional nanomagnetism",
    "start_abstract":"Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities.",
    "start_categories":[
      "cond-mat.mes-hall"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "MagNet: machine learning enhanced three-dimensional magnetic reconstruction"
      ],
      "abstract":[
        "Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "The Factorizable Feigin-Frenkel center",
        "A BERT Based Hybrid Recommendation System For Academic Collaboration",
        "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding\n  and Expert Reasoning Abilities",
        "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and\n  Attacks",
        "Quantum Feature-Empowered Deep Classification for Fast Mangrove Mapping",
        "Antenna Position and Beamforming Optimization for Movable Antenna\n  Enabled ISAC: Optimal Solutions and Efficient Algorithms",
        "Do Unit Proofs Work? An Empirical Study of Compositional Bounded Model\n  Checking for Memory Safety Verification",
        "A New Statistical Approach to the Performance Analysis of Vision-based\n  Localization",
        "PAID: A Framework of Product-Centric Advertising Image Design",
        "How much should we care about what others know? Jump signals in optimal\n  investment under relative performance concerns",
        "Optimal PMU Placement for Kalman Filtering of DAE Power System Models",
        "CubeDiff: Repurposing Diffusion-Based Image Models for Panorama\n  Generation",
        "Bias Analysis of Experiments for Multi-Item Multi-Period Inventory\n  Control Policies",
        "Robust Phantom-Assisted Framework for Multi-Person Localization and\n  Vital Signs Monitoring Using MIMO FMCW Radar",
        "On the existence of twisted Shalika periods: the Archimedean case",
        "On reflected isotropic stable processes",
        "A Comprehensive Survey on Long Context Language Modeling",
        "Design and Implementation of a Dual Uncrewed Surface Vessel Platform for\n  Bathymetry Research under High-flow Conditions",
        "AI Load Dynamics--A Power Electronics Perspective",
        "Looking into the Future of Health-Care Services: Can Life-Like Agents\n  Change the Future of Health-Care Services?",
        "Enhancing Vision-Language Compositional Understanding with Multimodal\n  Synthetic Data",
        "Bounded conciseness in the space of marked groups",
        "CRDT-Based Game State Synchronization in Peer-to-Peer VR",
        "Exercises on the Kepler ellipses through a fixed point in space, after\n  Otto Laporte",
        "Simplifying Formal Proof-Generating Models with ChatGPT and Basic\n  Searching Techniques",
        "FairDiverse: A Comprehensive Toolkit for Fair and Diverse Information\n  Retrieval Algorithms",
        "Minimum Time Strategies for a Differential Drive Robot Escaping from a\n  Circular Detection Region",
        "Intelligent Reflecting Surface-Aided Electromagnetic Stealth over\n  Extended Regions",
        "Continuity of Hausdorff Dimension at Hopf Bifurcation"
      ],
      "abstract":[
        "We prove a factorizable version of the Feigin-Frenkel theorem on the center\nof the completed enveloping algebra of the affine Kac-Moody algebra attached to\na simple Lie algebra at the critical level. On any smooth curve C we consider a\nsheaf of complete topological Lie algebras whose fiber at any point is the\nusual affine algebra at the critical level and consider its sheaf of completed\nenveloping algebras. We show that the center of this sheaf is a factorization\nalgebra and establish that it is canonically isomorphic, in a factorizable\nmanner, with the factorization algebra of functions on Opers on the pointed\ndisk.",
        "Universities serve as a hub for academic collaboration, promoting the\nexchange of diverse ideas and perspectives among students and faculty through\ninterdisciplinary dialogue. However, as universities expand in size,\nconventional networking approaches via student chapters, class groups, and\nfaculty committees become cumbersome. To address this challenge, an\nacademia-specific profile recommendation system is proposed to connect\nlike-minded stakeholders within any university community. This study evaluates\nthree techniques: Term Frequency-Inverse Document Frequency (TF-IDF),\nBidirectional Encoder Representations from Transformers (BERT), and a hybrid\napproach to generate effective recommendations. Due to the unlabelled nature of\nthe dataset, Affinity Propagation cluster-based relabelling is performed to\nunderstand the grouping of similar profiles. The hybrid model demonstrated\nsuperior performance, evidenced by its similarity score, Silhouette score,\nDavies-Bouldin index, and Normalized Discounted Cumulative Gain (NDCG),\nachieving an optimal balance between diversity and relevance in\nrecommendations. Furthermore, the optimal model has been implemented as a\nmobile application, which dynamically suggests relevant profiles based on\nusers' skills and collaboration interests, incorporating contextual\nunderstanding. The potential impact of this application is significant, as it\npromises to enhance networking opportunities within large academic institutions\nthrough the deployment of intelligent recommendation systems.",
        "Understanding and reasoning over non-speech sounds and music are crucial for\nboth humans and AI agents to interact effectively with their environments. In\nthis paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM)\nwith advanced audio understanding and reasoning capabilities. AF2 leverages (i)\na custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio\nreasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves\nstate-of-the-art performance with only a 3B parameter small language model,\nsurpassing large open-source and proprietary models across over 20 benchmarks.\nNext, for the first time, we extend audio understanding to long audio segments\n(30 secs to 5 mins) and propose LongAudio, a large and novel dataset for\ntraining ALMs on long audio captioning and question-answering tasks.\nFine-tuning AF2 on LongAudio leads to exceptional performance on our proposed\nLongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio\nunderstanding capabilities. We conduct extensive ablation studies to confirm\nthe efficacy of our approach. Project Website:\nhttps:\/\/research.nvidia.com\/labs\/adlr\/AF2\/.",
        "Embodied AI systems, including robots and autonomous vehicles, are\nincreasingly integrated into real-world applications, where they encounter a\nrange of vulnerabilities stemming from both environmental and system-level\nfactors. These vulnerabilities manifest through sensor spoofing, adversarial\nattacks, and failures in task and motion planning, posing significant\nchallenges to robustness and safety. Despite the growing body of research,\nexisting reviews rarely focus specifically on the unique safety and security\nchallenges of embodied AI systems. Most prior work either addresses general AI\nvulnerabilities or focuses on isolated aspects, lacking a dedicated and unified\nframework tailored to embodied AI. This survey fills this critical gap by: (1)\ncategorizing vulnerabilities specific to embodied AI into exogenous (e.g.,\nphysical attacks, cybersecurity threats) and endogenous (e.g., sensor failures,\nsoftware flaws) origins; (2) systematically analyzing adversarial attack\nparadigms unique to embodied AI, with a focus on their impact on perception,\ndecision-making, and embodied interaction; (3) investigating attack vectors\ntargeting large vision-language models (LVLMs) and large language models (LLMs)\nwithin embodied systems, such as jailbreak attacks and instruction\nmisinterpretation; (4) evaluating robustness challenges in algorithms for\nembodied perception, decision-making, and task planning; and (5) proposing\ntargeted strategies to enhance the safety and reliability of embodied AI\nsystems. By integrating these dimensions, we provide a comprehensive framework\nfor understanding the interplay between vulnerabilities and safety in embodied\nAI.",
        "A mangrove mapping (MM) algorithm is an essential classification tool for\nenvironmental monitoring. The recent literature shows that compared with other\nindex-based MM methods that treat pixels as spatially independent,\nconvolutional neural networks (CNNs) are crucial for leveraging spatial\ncontinuity information, leading to improved classification performance. In this\nwork, we go a step further to show that quantum features provide radically new\ninformation for CNN to further upgrade the classification results. Simply\nspeaking, CNN computes affine-mapping features, while quantum neural network\n(QNN) offers unitary-computing features, thereby offering a fresh perspective\nin the final decision-making (classification). To address the challenging MM\nproblem, we design an entangled spatial-spectral quantum feature extraction\nmodule. Notably, to ensure that the quantum features contribute genuinely novel\ninformation (unaffected by traditional CNN features), we design a separate\nnetwork track consisting solely of quantum neurons with built-in\ninterpretability. The extracted pure quantum information is then fused with\ntraditional feature information to jointly make the final decision. The\nproposed quantum-empowered deep network (QEDNet) is very lightweight, so the\nimprovement does come from the cooperation between CNN and QNN (rather than\nparameter augmentation). Extensive experiments will be conducted to demonstrate\nthe superiority of QEDNet.",
        "In this paper, we propose an integrated sensing and communication (ISAC)\nsystem enabled by movable antennas (MAs), which can dynamically adjust antenna\npositions to enhance both sensing and communication performance for future\nwireless networks. To characterize the benefits of MA-enabled ISAC systems, we\nfirst derive the Cram\\'er-Rao bound (CRB) for angle estimation error, which is\nthen minimized for optimizing the antenna position vector (APV) and beamforming\ndesign, subject to a pre-defined signal-to-noise ratio (SNR) constraint to\nensure the communication performance. In particular, for the case with receive\nMAs only, we provide a closed-form optimal antenna position solution, and show\nthat employing MAs over conventional fixed-position antennas (FPAs) can achieve\na sensing performance gain upper-bounded by 4.77 dB. On the other hand, for the\ncase with transmit MAs only, we develop a boundary traversal breadth-first\nsearch (BT-BFS) algorithm to obtain the global optimal solution in the\nline-of-sight (LoS) channel scenario, along with a lower-complexity boundary\ntraversal depth-first search (BT-DFS) algorithm to find a local optimal\nsolution efficiently. While in the scenario with non-LoS (NLoS) channels, a\nmajorization-minimization (MM) based Rosen's gradient projection (RGP)\nalgorithm with an efficient initialization method is proposed to obtain\nstationary solutions for the considered problem, which can be extended to the\ngeneral case with both transmit and receive MAs. Extensive numerical results\nare presented to verify the effectiveness of the proposed algorithms, and\ndemonstrate the superiority of the considered MA-enabled ISAC system over\nconventional ISAC systems with FPAs in terms of sensing and communication\nperformance trade-off.",
        "Memory safety defects pose a major threat to software reliability, enabling\ncyberattacks, outages, and crashes. To mitigate these risks, organizations\nadopt Compositional Bounded Model Checking (BMC), using unit proofs to formally\nverify memory safety. However, methods for creating unit proofs vary across\norganizations and are inconsistent within the same project, leading to errors\nand missed defects. In addition, unit proofing remains understudied, with no\nsystematic development methods or empirical evaluations.\n  This work presents the first empirical study on unit proofing for memory\nsafety verification. We introduce a systematic method for creating unit proofs\nthat leverages verification feedback and objective criteria. Using this\napproach, we develop 73 unit proofs for four embedded operating systems and\nevaluate their effectiveness, characteristics, cost, and generalizability. Our\nresults show unit proofs are cost-effective, detecting 74\\% of recreated\ndefects, with an additional 9\\% found with increased BMC bounds, and 19 new\ndefects exposed. We also found that embedded software requires small unit\nproofs, which can be developed in 87 minutes and executed in 61 minutes on\naverage. These findings provide practical guidance for engineers and empirical\ndata to inform tooling design.",
        "Many modern wireless devices with accurate positioning needs also have access\nto vision sensors, such as a camera, radar, and Light Detection and Ranging\n(LiDAR). In scenarios where wireless-based positioning is either inaccurate or\nunavailable, using information from vision sensors becomes highly desirable for\ndetermining the precise location of the wireless device. Specifically, vision\ndata can be used to estimate distances between the target (where the sensors\nare mounted) and nearby landmarks. However, a significant challenge in\npositioning using these measurements is the inability to uniquely identify\nwhich specific landmark is visible in the data. For instance, when the target\nis located close to a lamppost, it becomes challenging to precisely identify\nthe specific lamppost (among several in the region) that is near the target.\nThis work proposes a new framework for target localization using range\nmeasurements to multiple proximate landmarks. The geometric constraints\nintroduced by these measurements are utilized to narrow down candidate landmark\ncombinations corresponding to the range measurements and, consequently, the\ntarget's location on a map. By modeling landmarks as a marked Poisson point\nprocess (PPP), we show that three noise-free range measurements are sufficient\nto uniquely determine the correct combination of landmarks in a two-dimensional\nplane. For noisy measurements, we provide a mathematical characterization of\nthe probability of correctly identifying the observed landmark combination\nbased on a novel joint distribution of key random variables. Our results\ndemonstrate that the landmark combination can be identified using ranges, even\nwhen individual landmarks are visually indistinguishable.",
        "Creating visually appealing advertising images is often a labor-intensive and\ntime-consuming process. Is it possible to automatically generate such images\nusing only basic product information--specifically, a product foreground image,\ntaglines, and a target size? Existing methods mainly focus on parts of the\nproblem and fail to provide a comprehensive solution. To address this gap, we\npropose a novel multistage framework called Product-Centric Advertising Image\nDesign (PAID). It consists of four sequential stages to highlight product\nforegrounds and taglines while achieving overall image aesthetics: prompt\ngeneration, layout generation, background image generation, and graphics\nrendering. Different expert models are designed and trained for the first three\nstages: First, we use a visual language model (VLM) to generate background\nprompts that match the products. Next, a VLM-based layout generation model\narranges the placement of product foregrounds, graphic elements (taglines and\ndecorative underlays), and various nongraphic elements (objects from the\nbackground prompt). Following this, we train an SDXL-based image generation\nmodel that can simultaneously accept prompts, layouts, and foreground controls.\nTo support the PAID framework, we create corresponding datasets with over\n50,000 labeled images. Extensive experimental results and online A\/B tests\ndemonstrate that PAID can produce more visually appealing advertising images.",
        "We present a multi-agent and mean-field formulation of a game between\ninvestors who receive private signals informing their investment decisions and\nwho interact through relative performance concerns. A key tool in our model is\na Poisson random measure which drives jumps in both market prices and signal\nprocesses and thus captures common and idiosyncratic noise. Upon receiving a\njump signal, an investor evaluates not only the signal's implications for stock\nprice movements but also its implications for the signals received by her peers\nand for their subsequent investment decisions. A crucial aspect of this\nassessment is the distribution of investor types in the economy. These types\ndetermine their risk aversion, performance concerns, and the quality and\nquantity of their signals. We demonstrate how these factors are reflected in\nthe corresponding HJB equations, characterizing an agent's optimal response to\nher peers' signal-based strategies. The existence of equilibria in both the\nmulti-agent and mean-field game is established using Schauder's Fixed Point\nTheorem under suitable conditions on investor characteristics, particularly\ntheir signal processes. Finally, we present numerical case studies that\nillustrate these equilibria from a financial-economic perspective. This allows\nus to address questions such as how much investors should care about the\ninformation known by their peers.",
        "Optimal sensor placement is essential for minimizing costs and ensuring\naccurate state estimation in power systems. This paper introduces a novel\nmethod for optimal sensor placement for dynamic state estimation of power\nsystems modeled by differential-algebraic equations. The method identifies\noptimal sensor locations by minimizing the steady-state covariance matrix of\nthe Kalman filter, thus minimizing the error of joint differential and\nalgebraic state estimation. The problem is reformulated as a mixed-integer\nsemidefinite program and effectively solved using off-the-shelf numerical\nsolvers. Numerical results demonstrate the merits of the proposed approach by\nbenchmarking its performance in phasor measurement unit placement in comparison\nto greedy algorithms.",
        "We introduce a novel method for generating 360{\\deg} panoramas from text\nprompts or images. Our approach leverages recent advances in 3D generation by\nemploying multi-view diffusion models to jointly synthesize the six faces of a\ncubemap. Unlike previous methods that rely on processing equirectangular\nprojections or autoregressive generation, our method treats each face as a\nstandard perspective image, simplifying the generation process and enabling the\nuse of existing multi-view diffusion models. We demonstrate that these models\ncan be adapted to produce high-quality cubemaps without requiring\ncorrespondence-aware attention layers. Our model allows for fine-grained text\ncontrol, generates high resolution panorama images and generalizes well beyond\nits training set, whilst achieving state-of-the-art results, both qualitatively\nand quantitatively. Project page: https:\/\/cubediff.github.io\/",
        "Randomized experiments, or A\/B testing, are the gold standard for evaluating\ninterventions but are underutilized in the area of inventory management. This\nstudy addresses this gap by analyzing A\/B testing strategies in multi-item,\nmulti-period inventory systems with lost sales and capacity constraints. We\nexamine switchback experiments, item-level randomization, pairwise\nrandomization, and staggered rollouts, analyzing their biases theoretically and\ncomparing them through numerical experiments. Our findings provide actionable\nguidance for selecting experimental designs across various contexts in\ninventory management.",
        "With the rising prevalence of cardiovascular and respiratory disorders and an\naging global population, healthcare systems face increasing pressure to adopt\nefficient, non-contact vital sign monitoring (NCVSM) solutions. This study\nintroduces a robust framework for multi-person localization and vital signs\nmonitoring, using multiple-input-multiple-output frequency-modulated continuous\nwave radar, addressing challenges in real-world, cluttered environments. Two\nkey contributions are presented. First, a custom hardware phantom was developed\nto simulate multi-person NCVSM scenarios, utilizing recorded thoracic impedance\nsignals to replicate realistic cardiopulmonary dynamics. The phantom's design\nfacilitates repeatable and rapid validation of radar systems and algorithms\nunder diverse conditions to accelerate deployment in human monitoring. Second,\naided by the phantom, we designed a robust algorithm for multi-person\nlocalization utilizing joint sparsity and cardiopulmonary properties, alongside\nharmonics-resilient dictionary-based vital signs estimation, to mitigate\ninterfering respiration harmonics. Additionally, an adaptive signal refinement\nprocedure is introduced to enhance the accuracy of continuous NCVSM by\nleveraging the continuity of the estimates. Performance was validated and\ncompared to existing techniques through 12 phantom trials and 12 human trials,\nincluding both single- and multi-person scenarios, demonstrating superior\nlocalization and NCVSM performance. For example, in multi-person human trials,\nour method achieved average respiration rate estimation accuracies of 94.14%,\n98.12%, and 98.69% within error thresholds of 2, 3, and 4 breaths per minute,\nrespectively, and heart rate accuracies of 87.10%, 94.12%, and 95.54% within\nthe same thresholds. These results highlight the potential of this framework\nfor reliable multi-person NCVSM in healthcare and IoT applications.",
        "Let $\\K$ be an archimedean local field. We investigate the existence of the\ntwisted Shalika functionals on irreducible admissible smooth representations of\n$\\GL_{2n}(\\K)$ in terms of their L-parameters. As part of our proof, we\nestablish a Hochschild-Serre spectral sequence for nilpotent normal subgroups\nand a Kunneth formula in the framework of Schwartz homology. We also prove the\nanalogous result for twisted linear periods using theta correspondence. The\nexistence of twisted Shalika functionals on representations of\n$\\GL_{2n}^{+}(\\R)$ is also studied, which is of independent interest.",
        "We build two types of isotropic stable processes reflected in a strongly\nconvex bounded domain $\\mathcal{D}$. In both cases, when the process tries to\njump across the boundary, it is stopped at the unique point where\n$\\partial\\mathcal{D}$ intersects the line segment defined by the attempted\njump. It then leaves the boundary either continuously (for the first type) or\nby a power-law distributed jump (for the second type). The construction of\nthese processes is done via an It\\^o synthesis: we concatenate their excursions\nin the domain, which are obtained by translating, rotating and stopping the\nexcursions of some stable processes reflected in the half-space. The key\ningredient in this procedure is the construction of the boundary processes,\ni.e. the processes time-changed by their local time on the boundary, which\nsolve stochastic differential equations driven by some Poisson measures of\nexcursions. The well-posedness of these boundary processes relies on delicate\nestimates involving some geometric inequalities and the laws of the undershoot\nand overshoot of the excursion when it leaves the domain. After having\nconstructed the processes, we show that they are Markov and Feller, we study\ntheir infinitesimal generator and write down the reflected fractional heat\nequations satisfied by their time-marginals.",
        "Efficient processing of long contexts has been a persistent pursuit in\nNatural Language Processing. With the growing number of long documents,\ndialogues, and other textual data, it is important to develop Long Context\nLanguage Models (LCLMs) that can process and analyze extensive inputs in an\neffective and efficient way. In this paper, we present a comprehensive survey\non recent advances in long-context modeling for large language models. Our\nsurvey is structured around three key aspects: how to obtain effective and\nefficient LCLMs, how to train and deploy LCLMs efficiently, and how to evaluate\nand analyze LCLMs comprehensively. For the first aspect, we discuss data\nstrategies, architectural designs, and workflow approaches oriented with long\ncontext processing. For the second aspect, we provide a detailed examination of\nthe infrastructure required for LCLM training and inference. For the third\naspect, we present evaluation paradigms for long-context comprehension and\nlong-form generation, as well as behavioral analysis and mechanism\ninterpretability of LCLMs. Beyond these three key aspects, we thoroughly\nexplore the diverse application scenarios where existing LCLMs have been\ndeployed and outline promising future development directions. This survey\nprovides an up-to-date review of the literature on long-context LLMs, which we\nwish to serve as a valuable resource for both researchers and engineers. An\nassociated GitHub repository collecting the latest papers and repos is\navailable at:\n\\href{https:\/\/github.com\/LCLM-Horizon\/A-Comprehensive-Survey-For-Long-Context-Language-Modeling}{\\color[RGB]{175,36,67}{LCLM-Horizon}}.",
        "Bathymetry, the study of underwater topography, relies on sonar mapping of\nsubmerged structures. These measurements, critical for infrastructure health\nmonitoring, often require expensive instrumentation. The high financial risk\nassociated with sensor damage or vessel loss creates a reluctance to deploy\nuncrewed surface vessels (USVs) for bathymetry. However, the crewed-boat\nbathymetry operations, are costly, pose hazards to personnel, and frequently\nfail to achieve the stable conditions necessary for bathymetry data collection,\nespecially under high currents. Further research is essential to advance\nautonomous control, navigation, and data processing technologies, with a\nparticular focus on bathymetry. There is a notable lack of accessible hardware\nplatforms that allow for integrated research in both bathymetry-focused\nautonomous control and navigation, as well as data evaluation and processing.\nThis paper addresses this gap through the design and implementation of two\ncomplementary USV systems tailored for uncrewed bathymetry research. This\nincludes a low-cost USV for Navigation And Control research (NAC-USV) and a\nsecond, high-end USV equipped with a high-resolution multi-beam sonar and the\nassociated hardware for Bathymetry data quality Evaluation and Post-processing\nresearch (BEP-USV). The NAC-USV facilitates the investigation of autonomous,\nfail-safe navigation and control, emphasizing the stability requirements for\nhigh-quality bathymetry data collection while minimizing the risk to equipment.\nThe BEP-USV, which mirrors the NAC-USV hardware, is then used for additional\ncontrol validation and in-depth exploration of bathymetry data evaluation and\npost-processing methodologies. We detail the design and implementation of both\nsystems, and open source the design. Furthermore, we demonstrate the system's\neffectiveness in a range of operational scenarios.",
        "As AI-driven computing infrastructures rapidly scale, discussions around data\ncenter design often emphasize energy consumption, water and electricity usage,\nworkload scheduling, and thermal management. However, these perspectives often\noverlook the critical interplay between AI-specific load transients and power\nelectronics. This paper addresses that gap by examining how large-scale AI\nworkloads impose unique demands on power conversion chains and, in turn, how\nthe power electronics themselves shape the dynamic behavior of AI-based\ninfrastructure. We illustrate the fundamental constraints imposed by\nmulti-stage power conversion architectures and highlight the key role of\nfinal-stage modules in defining realistic power slew rates for GPU clusters.\nOur analysis shows that traditional designs, optimized for slower-varying or\nCPU-centric workloads, may not adequately accommodate the rapid load ramps and\ndrops characteristic of AI accelerators. To bridge this gap, we present\ninsights into advanced converter topologies, hierarchical control methods, and\nenergy buffering techniques that collectively enable robust and efficient power\ndelivery. By emphasizing the bidirectional influence between AI workloads and\npower electronics, we hope this work can set a good starting point and offer\npractical design considerations to ensure future exascale-capable data centers\ncan meet the stringent performance, reliability, and scalability requirements\nof next-generation AI deployments.",
        "Time constraints on doctor patient interaction and restricted access to\nspecialists under the managed care system led to increasingly referring to\ncomputers as a medical information source and a self-health-care management\ntool. However, research show that less than 40% of information seekers\nindicated that online information helped them to make a decision about their\nhealth. Searching multiple web sites that need basic computer skills, lack of\ninteraction and no face to face interaction in most search engines and some\nsocial issues, led us to develop a specialized life-like agent that would\novercome mentioned problems.",
        "Despite impressive advancements in various multimodal tasks, vision-language\nmodels (VLMs) still struggle with compositional understanding due to limited\nexposure to training samples that contain subtle variations within paired\nexamples. With advances in multimodal generative models, a natural solution is\nto generate synthetic samples with subtle variations for training VLMs.\nHowever, generating and training on synthetic samples with subtle variations\npresents two challenges: difficulty in accurately creating precise variations\nand inconsistency in cross-modal alignment quality. To address these\nchallenges, we propose SVD-GT (Subtle Variation Data Generation and Training),\nwhich integrates image feature injection into a text-to-image generative model\nto enhance the quality of synthetic variations and employs an adaptive margin\nloss to differentiate samples using adaptive margins, which help filter out\npotentially incorrect synthetic samples and focus the learning on informative\nhard samples. Evaluations on four compositional understanding benchmarks\ndemonstrate that SVD-GT significantly improves the compositionality of VLMs,\nboosting the average accuracy of CLIP by over 8% across all benchmarks and\noutperforming state-of-the-art methods by 2% on three benchmarks.",
        "We prove that bounded conciseness is a closed property in the space of marked\ngroups. As a consequence, we reformulate a conjecture of Fern\\'andez-Alcober\nand Shumyatsky [7] about conciseness in the class of residually finite groups.",
        "Virtual presence demands ultra-low latency, a factor that centralized\narchitectures, by their nature, cannot minimize. Local peer-to-peer\narchitectures offer a compelling alternative, but also pose unique challenges\nin terms of network infrastructure. This paper introduces a prototype\nleveraging Conflict-Free Replicated Data Types (CRDTs) to enable real-time\ncollaboration in a shared virtual environment. Using this prototype, we\ninvestigate latency, synchronization, and the challenges of decentralized\ncoordination in dynamic non-Byzantine contexts. We aim to question prevailing\nassumptions about decentralized architectures and explore the practical\npotential of P2P in advancing virtual presence. This work challenges the\nconstraints of mediated networks and highlights the potential of decentralized\narchitectures to redefine collaboration and interaction in digital spaces.",
        "This article has a twofold purpose. On the one hand I would like to draw\nattention to some nice exercises on the Kepler laws, due to Otto Laporte from\n1970. Our discussion here has a more geometric flavour than the original\nanalytic approach of Laporte. On the other hand it serves as an addendum to a\npaper of mine from 1998 on the quantum integrability of the Kovalevsky top.\nLater I learned that this integrability result had been obtained already long\nbefore by Laporte in 1933.",
        "The challenge of formal proof generation has a rich history, but with modern\ntechniques, we may finally be at the stage of making actual progress in\nreal-life mathematical problems. This paper explores the integration of ChatGPT\nand basic searching techniques to simplify generating formal proofs, with a\nparticular focus on the miniF2F dataset. We demonstrate how combining a large\nlanguage model like ChatGPT with a formal language such as Lean, which has the\nadded advantage of being verifiable, enhances the efficiency and accessibility\nof formal proof generation. Despite its simplicity, our best-performing\nLean-based model surpasses all known benchmarks with a 31.15% pass rate. We\nextend our experiments to include other datasets and employ alternative\nlanguage models, showcasing our models' comparable performance in diverse\nsettings and allowing for a more nuanced analysis of our results. Our findings\noffer insights into AI-assisted formal proof generation, suggesting a promising\ndirection for future research in formal mathematical proof.",
        "In modern information retrieval (IR). achieving more than just accuracy is\nessential to sustaining a healthy ecosystem, especially when addressing\nfairness and diversity considerations. To meet these needs, various datasets,\nalgorithms, and evaluation frameworks have been introduced. However, these\nalgorithms are often tested across diverse metrics, datasets, and experimental\nsetups, leading to inconsistencies and difficulties in direct comparisons. This\nhighlights the need for a comprehensive IR toolkit that enables standardized\nevaluation of fairness- and diversity-aware algorithms across different IR\ntasks. To address this challenge, we present FairDiverse, an open-source and\nstandardized toolkit. FairDiverse offers a framework for integrating fair and\ndiverse methods, including pre-processing, in-processing, and post-processing\ntechniques, at different stages of the IR pipeline. The toolkit supports the\nevaluation of 28 fairness and diversity algorithms across 16 base models,\ncovering two core IR tasks (search and recommendation) thereby establishing a\ncomprehensive benchmark. Moreover, FairDiverse is highly extensible, providing\nmultiple APIs that empower IR researchers to swiftly develop and evaluate their\nown fairness and diversity aware models, while ensuring fair comparisons with\nexisting baselines. The project is open-sourced and available on\nhttps:\/\/github.com\/XuChen0427\/FairDiverse.",
        "A Differential Drive Robot (DDR) located inside a circular detection region\nin the plane wants to escape from it in minimum time. Various robotics\napplications can be modeled like the previous problem, such as a DDR escaping\nas soon as possible from a forbidden\/dangerous region in the plane or running\nout from the sensor footprint of an unmanned vehicle flying at a constant\naltitude. In this paper, we find the motion strategies to accomplish its goal\nunder two scenarios. In one, the detection region moves slower than the DDR and\nseeks to prevent escape; in another, its position is fixed. We formulate the\nproblem as a zero-sum pursuit-evasion game, and using differential games\ntheory, we compute the players' time-optimal motion strategies. Given the DDR's\nspeed advantage, it can always escape by translating away from the center of\nthe detection region at maximum speed. In this work, we show that the previous\nstrategy could be optimal in some cases; however, other motion strategies\nemerge based on the player's speed ratio and the players' initial\nconfigurations.",
        "Compared to traditional electromagnetic stealth (ES) materials, which are\neffective only within specific frequencies and orientations, intelligent\nreflecting surface (IRS) technology introduces a novel paradigm for achieving\ndynamic and adaptive ES by adapting its reflection pattern in real time to\nneutralize radar probing signals echoed back from the target. In this letter,\nwe study an IRS-aided ES system mounted on an aerial target to evade radar\ndetection admist uncertain\/moving radar positions over an extended area.\nSpecifically, we aim to optimize the IRS's passive reflection to minimize the\nmaximum received signal-to-noise ratio (SNR) of the target echo signal in the\narea. A semi-closed-form solution is derived by first discretizing the\ncontinuous spatial frequency deviation to approximate the semi-infinite\nreflection gain constraint and then leveraging the Lagrange dual method.\nSimulation results are provided to validate that the proposed IRS-aided ES\nstrategy can consistently reduce the reflection gains for radars located across\na large region.",
        "We investigate the continuity of Hausdorff dimension and box dimension (limit\ncapacity) of non-hyperbolic repellers of diffeomorphisms derived from\ntransitive Anosov diffeomophisms through a Hopf bifurcation studied by Horita\nand Viana (see Discret. Contin. Dyn. Syst., 13 (2005), 1125-1137). Here, we\nextend their work showing that both dimensions are continuous at paremeter\nbifurcation. In the proof, we consider maps with holes introduced by Horita and\nViana in Journal of Statistical Physics 105(2001), 835-862 and further\ndeveloped by Dysman in Journal of Statistical Physics 120(2005),479-509,\nrelating the Hausdorff dimension with the volume of the hole."
      ]
    }
  },
  {
    "id":2412.09927,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"MagNet: machine learning enhanced three-dimensional magnetic reconstruction",
    "start_abstract":"Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Three-dimensional nanomagnetism"
      ],
      "abstract":[
        "Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities."
      ],
      "categories":[
        "cond-mat.mes-hall"
      ]
    },
    "list":{
      "title":[
        "A new framework for Ljusternik-Schnirelmann theory and its application\n  to planar Choquard equations",
        "Implicit Generative Modeling by Kernel Similarity Matching",
        "k-Sample inference via Multimarginal Optimal Transport",
        "A Drinfeld Presentation of the Queer Super-Yangian",
        "Implicit Bias in Matrix Factorization and its Explicit Realization in a\n  New Architecture",
        "An experimental technique for measuring radial coherence",
        "Probing prethermal nonergodicity through measurement outcomes of\n  monitored quantum dynamics",
        "Four Total Eclipsing Contact Binary Systems: The First Photometric Light\n  Curve Solutions Employing TESS and Gaia Surveys",
        "Time-Variant Vector Field Visualization for Magnetic Fields of Neutron\n  Star Simulations",
        "Quantum Birkhoff Normal Form in the $\\sigma$-Bruno-R\\\"{u}ssmann\n  non-resonant condition",
        "From de Bruijn graphs to variation graphs-relationships between\n  pangenome models",
        "High-accuracy multi-ion spectroscopy with mixed-species Coulomb crystals",
        "A Liouville-type theorem for the p-Laplacian on complete non-compact\n  Riemannian manifolds",
        "Gradient Flows and the Curvature of Theory Space",
        "Causal survival analysis, Estimation of the Average Treatment Effect\n  (ATE): Practical Recommendations",
        "Clustering Does Not Always Imply Latent Geometry",
        "Gravitomagnetic tidal response of relativistic stars in partially\n  screened scalar-tensor theories",
        "Stabilizing open photon condensates by ghost-attractor dynamics",
        "Scalar Field Fluctuations and the Production of Dark Matter",
        "The Interplay between Dust Dynamics and Turbulence Induced by the\n  Vertical Shear Instability",
        "Photogalvanic Shift Currents in BiFeO3 --LaFeO3 Superlattices",
        "Quasi-Large Hole Polarons in BiVO4-Implications for Photocatalysis and\n  Solar Energy Conversion",
        "Satire: Computing Rigorous Bounds for Floating-Point Rounding Error in\n  Mixed-Precision Loop-Free Programs",
        "Mapping the $\\Lambda_{\\rm s}$CDM scenario to $f(T)$ modified gravity:\n  Effects on structure growth rate",
        "Tensor network method for solving the Ising model with a magnetic field",
        "Study of hadron interactions and compositeness",
        "Quantum Reservoir Computing and Risk Bounds",
        "A novel metric for species vulnerability and coexistence in\n  spatially-extended ecosystems",
        "Microchip semiconductor membrane external-cavity surface-emitting laser"
      ],
      "abstract":[
        "We consider the planar logarithmic Choquard equation $$- \\Delta u + a(x)u +\n(\\log|\\cdot| \\ast u^2)u = 0,\\qquad \\text{in } \\mathbb{R}^2$$ in the strongly\nindefinite and possibly degenerate setting where no sign condition is imposed\non the linear potential $a \\in L^\\infty(\\mathbb{R}^2)$. In particular, we shall\nprove the existence of a sequence of high energy solutions to this problem in\nthe case where $a$ is invariant under $\\mathbb{Z}^2$-translations.\n  The result extends to a more general $G$-equivariant setting, for which we\ndevelop a new variational approach which allows us to find critical points of\nLjusternik-Schnirelmann type. In particular, our method resolves the problem\nthat the energy functional $\\Phi$ associated with the logarithmic Choquard\nequation is only defined on a subspace $X \\subset H^1(\\mathbb{R}^2)$ with the\nproperty that $\\|\\cdot\\|_X$ is not translation invariant. The new approach is\nbased on a new $G$-equivariant version of the Cerami condition and on\ndeformation arguments adapted to a family of suitably constructed scalar\nproducts $\\langle \\cdot, \\cdot \\rangle_u$, $u \\in X$ with the $G$-equivariance\nproperty $\\langle g \\ast v , g \\ast w \\rangle_{g \\ast u} = \\langle v , w\n\\rangle_u.$",
        "Understanding how the brain encodes stimuli has been a fundamental problem in\ncomputational neuroscience. Insights into this problem have led to the design\nand development of artificial neural networks that learn representations by\nincorporating brain-like learning abilities. Recently, learning representations\nby capturing similarity between input samples has been studied to tackle this\nproblem. This approach, however, has thus far been used to only learn\ndownstream features from an input and has not been studied in the context of a\ngenerative paradigm, where one can map the representations back to the input\nspace, incorporating not only bottom-up interactions (stimuli to latent) but\nalso learning features in a top-down manner (latent to stimuli). We investigate\na kernel similarity matching framework for generative modeling. Starting with a\nmodified sparse coding objective for learning representations proposed in prior\nwork, we demonstrate that representation learning in this context is equivalent\nto maximizing similarity between the input kernel and a latent kernel. We show\nthat an implicit generative model arises from learning the kernel structure in\nthe latent space and show how the framework can be adapted to learn manifold\nstructures, potentially providing insights as to how task representations can\nbe encoded in the brain. To solve the objective, we propose a novel Alternate\nDirection Method of Multipliers (ADMM) based algorithm and discuss the\ninterpretation of the optimization process. Finally, we discuss how this\nrepresentation learning problem can lead towards a biologically plausible\narchitecture to learn the model parameters that ties together representation\nlearning using similarity matching (a bottom-up approach) with predictive\ncoding (a top-down approach).",
        "This paper proposes a Multimarginal Optimal Transport ($MOT$) approach for\nsimultaneously comparing $k\\geq 2$ measures supported on finite subsets of\n$\\mathbb{R}^d$, $d \\geq 1$. We derive asymptotic distributions of the optimal\nvalue of the empirical $MOT$ program under the null hypothesis that all $k$\nmeasures are same, and the alternative hypothesis that at least two measures\nare different. We use these results to construct the test of the null\nhypothesis and provide consistency and power guarantees of this $k$-sample\ntest. We consistently estimate asymptotic distributions using bootstrap, and\npropose a low complexity linear program to approximate the test cut-off. We\ndemonstrate the advantages of our approach on synthetic and real datasets,\nincluding the real data on cancers in the United States in 2004 - 2020.",
        "We introduce a Drinfeld presentation for the super-Yangian\n$\\mathrm{Y}(\\mathfrak{q}_n)$ associated with the queer Lie superalgebra\n$\\mathfrak{q}_n$. The Drinfeld generators of $\\mathrm{Y}(\\mathfrak{q}_n)$ are\nobtained by a block version Gauss decomposition of the generator matrix in its\nRTT presentation, and the Drinfeld relations are explicitly computed by\nutilizing a block version of its RTT relations.",
        "Gradient descent for matrix factorization is known to exhibit an implicit\nbias toward approximately low-rank solutions. While existing theories often\nassume the boundedness of iterates, empirically the bias persists even with\nunbounded sequences. We thus hypothesize that implicit bias is driven by\ndivergent dynamics markedly different from the convergent dynamics for data\nfitting. Using this perspective, we introduce a new factorization model:\n$X\\approx UDV^\\top$, where $U$ and $V$ are constrained within norm balls, while\n$D$ is a diagonal factor allowing the model to span the entire search space.\nOur experiments reveal that this model exhibits a strong implicit bias\nregardless of initialization and step size, yielding truly (rather than\napproximately) low-rank solutions. Furthermore, drawing parallels between\nmatrix factorization and neural networks, we propose a novel neural network\nmodel featuring constrained layers and diagonal components. This model achieves\nstrong performance across various regression and classification tasks while\nfinding low-rank solutions, resulting in efficient and lightweight networks.",
        "Coherence refers to correlations between field vibrations at two separate\npoints in degrees of freedom such as space, time, and polarisation. In the\ncontext of space, coherence theory has been formulated between two transverse\npositions which can be described either in the cartesian coordinates or in the\ncylindrical coordinates. When expressed in cylindrical coordinates, spatial\ncoherence is described in terms of azimuthal and radial coordinates. The\ndescription of spatial coherence in radial degree of freedom has been\nformulated only recently in JOSA A 40, 411 (2023). In the present article, we\ndemonstrate an efficient experimental technique for measuring radial coherence,\nand we report measurement of radial coherence of two different types of\nradially partially coherent optical fields.",
        "Projective measurements are a key element in quantum physics and enable rich\nphenomena in monitored quantum dynamics. Here, we show that the measurement\noutcomes, recorded during monitored dynamics, can provide crucial information\nabout the properties of the monitored dynamical system itself. We demonstrate\nthis for a Floquet model of many-body localization, where we find that the\nprethermal many-body localized regime becomes unstable against rare\nmeasurements, yielding an unusual enhancement of quantum entanglement. Through\nan unsupervised learning and mutual information analysis on the classical\ndataset of measurement outcomes, we find that the information loss in the\nsystem, reflected by the increased entanglement, is compensated by an emergent\nstructure in this classical dataset. Our findings highlight the crucial role of\nmeasurements and corresponding classical outcomes in capturing prethermal\nnonergodicity, offering a promising perspective for applications to other\nmonitored quantum dynamics.",
        "We presented the first photometric light curve solutions of four W Ursae\nMajoris (W UMa)-type contact binary systems. This investigation utilized\nphotometric data from the Transiting Exoplanet Survey Satellite (TESS) and Gaia\nData Release 3 (DR3). We used the PHysics Of Eclipsing BinariEs (PHOEBE) Python\ncode and the Markov Chain Monte Carlo (MCMC) method for these light curve\nsolutions. Only TIC 249064185 among the target systems needed a cold starspot\nto be included in the analysis. Based on the estimated mass ratios for these\ntotal eclipse systems, three of them are categorized as low mass ratio contact\nbinary stars. The absolute parameters of the systems were estimated using the\nGaia DR3 parallax method and the orbital period and semi-major axis ($P-a$)\nempirical relationship. We defined that TIC 318015356 and TIC 55522736 systems\nare A-subtypes, while TIC 249064185 and TIC 397984843 are W-subtypes, depending\non each component's effective temperature and mass. We estimated the initial\nmasses of the stars, the mass lost by the binary system, and the systems' ages.\nWe displayed star positions in the mass-radius, mass-luminosity, and total\nmass-orbital angular momentum diagrams. In addition, our findings indicate a\ngood agreement with the mass-temperature empirical parameter relationship for\nthe primary stars.",
        "We present a novel visualization application designed to explore the\ntime-dependent development of magnetic fields of neutron stars. The strongest\nmagnetic fields in the universe can be found within neutron stars, potentially\nplaying a role in initiating astrophysical jets and facilitating the outflow of\nneutron-rich matter, ultimately resulting in the production of heavy elements\nduring binary neutron star mergers. Since such effects may be dependent on the\nstrength and configuration of the magnetic field, the formation and parameters\nof such fields are part of current research in astrophysics. Magnetic fields\nare investigated using simulations in which various initial configurations are\ntested. However, the long-term configuration is an open question, and current\nsimulations do not achieve a stable magnetic field. Neutron star simulations\nproduce data quantities in the range of several terabytes, which are both\nspatially in 3D and temporally resolved. Our tool enables physicists to\ninteractively explore the generated data. We first convert the data in a\npre-processing step and then we combine sparse vector field visualization using\nstreamlines with dense vector field visualization using line integral\nconvolution. We provide several methods to interact with the data responsively.\nThis allows the user to intuitively investigate data-specific issues.\nFurthermore, diverse visualization techniques facilitate individual exploration\nof the data and enable real-time processing of specific domain tasks, like the\ninvestigation of the time-dependent evolution of the magnetic field. In a\nqualitative study, domain experts tested the tool, and the usability was\nqueried. Experts rated the tool very positively and recommended it for their\ndaily work.",
        "The aim of this paper is to construct a Gevrey quantum Birkhoff normal form\nfor the $h$-differential operator $P_{h}(t),$ where $\nt\\in(-\\frac{1}{2},\\frac{1}{2})$, in the neighborhood of the union $\\Lambda$ of\nKAM tori. This construction commences from an appropriate Birkhoff normal form\nof $H$ around $\\Lambda$ and proceeds under the $\\sigma$-Bruno-R\\\"{u}ssmann\ncondition with $\\sigma>1$.",
        "Pangenomes serve as a framework for joint analysis of genomes of related\norganisms. Several pangenome models were proposed, offering different\nfunctionalities, applications provided by available tools, their efficiency\netc. Among them, two graph-based models are particularly widely used: variation\ngraphs and de Bruijn graphs. In the current paper we propose an axiomatization\nof the desirable properties of a graph representation of a collection of\nstrings. We show the relationship between variation graphs satisfying these\ncriteria and de Bruijn graphs. This relationship can be used to efficiently\nbuild a variation graph representing a given set of genomes, transfer\nannotations between both models, compare the results of analyzes based on each\nmodel etc.",
        "Multi-ion optical clocks offer the possibility of overcoming the low\nsignal-to-noise ratio of single-ion clocks, while still providing low\nsystematic uncertainties. We present simultaneous spectroscopy of up to four\n${}^{115}$In${}^+$ clock ions in a linear Coulomb crystal, sympathetically\ncooled with ${}^{172}$Yb${}^+$ ions. In first clock comparisons, we see\nagreement below $1\\times10^{-17}$ with results obtained using a single In${}^+$\nion, for which we have evaluated the systematic uncertainty to be\n$2.5\\times10^{-18}$. Operation with four clock ions reduces the instability\nfrom $1.6\\times10^{-15}\/\\sqrt{t\/(1\\;\\mathrm{s})}$ to\n$9.2\\times10^{-16}\/\\sqrt{t\/(1\\;\\mathrm{s})}$. We derive a model for\ndecay-related dead time during state preparation, which matches the observed\nscaling of instability with clock ion number $N$, and indicates that\n$1\/\\sqrt{N}$ scaling can be achieved with the addition of a repump laser.",
        "A Liouville-type result for the p-Laplacian on complete Riemannian manifolds\nis proved. As an application are present some results concerning complete\nnon-compact hypersurfaces immersed in a suitable warped product manifold.",
        "The metric and potential associated with the gradient property of\nrenormalisation group flow in multiscalar models in $d=4-\\varepsilon$\ndimensions are studied. The metric is identified with the Zamolodchikov metric\nof nearly marginal operators on the sphere. An explicit form for the associated\nRicci scalar in $d=4-\\varepsilon$ is derived, which shows that the space of\nmultiscalar field theories is curved. The potential is identified with a\nquantity $\\widetilde{F}$ that was previously proposed as a weakly monotonic\nfunction interpolating between the $a$-theorem in four dimensions and the\n$F$-theorem in three dimensions. This implies that the $\\widetilde{F}$-theorem\ncan be extended perturbatively to a theorem about gradient flow in\n$d=4-\\varepsilon$.",
        "Causal survival analysis combines survival analysis and causal inference to\nevaluate the effect of a treatment or intervention on a time-to-event outcome,\nsuch as survival time. It offers an alternative to relying solely on Cox models\nfor assessing these effects. In this paper, we present a comprehensive review\nof estimators for the average treatment effect measured with the restricted\nmean survival time, including regression-based methods, weighting approaches,\nand hybrid techniques. We investigate their theoretical properties and compare\ntheir performance through extensive numerical experiments. Our analysis focuses\non the finite-sample behavior of these estimators, the influence of nuisance\nparameter selection, and their robustness and stability under model\nmisspecification. By bridging theoretical insights with practical evaluation,\nwe aim to equip practitioners with both state-of-the-art implementations of\nthese methods and practical guidelines for selecting appropriate estimators for\ntreatment effect estimation. Among the approaches considered, G-formula\ntwo-learners, AIPCW-AIPTW, Buckley-James estimators, and causal survival\nforests emerge as particularly promising.",
        "The latent space approach to complex networks has revealed fundamental\nprinciples and symmetries, enabling geometric methods. However, the conditions\nunder which network topology implies geometricity remain unclear. We provide a\nmathematical proof and empirical evidence showing that the multiscale\nself-similarity of complex networks is a crucial factor in implying latent\ngeometry. Using degree-thresholding renormalization, we prove that any random\nscale-free graph in a $d$-dimensional homogeneous and isotropic manifold is\nself-similar when interactions are pairwise. Hence, both clustering and\nself-similarity are required to imply geometricity. Our findings highlight that\ncorrelated links can lead to finite clustering without self-similarity, and\ntherefore without inherent latent geometry. The implications are significant\nfor network mapping and ensemble equivalence between graphs and continuous\nspaces.",
        "In scalar-tensor theories beyond Horndeski, the Vainshtein screening\nmechanism is only partially effective inside astrophysical bodies. We\ninvestigate the potential to detect this partial breaking of Vainshtein\nscreening through the tidal response of fluid bodies. Specifically, we\ncalculate the gravitomagnetic tidal Love numbers and analyze how deviations\nfrom general relativity depend on parameters governing the breaking of\nVainshtein screening in the weak-gravity regime. For fixed parameter values,\nthe relative deviations increase with higher multipoles and larger compactness.\nHowever, we demonstrate that these parameters alone are insufficient to fully\ncharacterize the tidal response of relativistic bodies in scalar-tensor\ntheories beyond Horndeski.",
        "We study the temporal, driven-dissipative dynamics of open photon\nBose-Einstein condensates (BEC) in a dye-filled microcavity, taking the\ncondensate amplitude and the noncondensed fluctuations into account on the same\nfooting by means of a cumulant expansion within the Lindblad formalism. The\nfluctuations fundamentally alter the dynamics in that the BEC always dephases\nto zero for sufficiently long time. However, a ghost-attractor, although it is\noutside of the physically accessible configuration space, attracts the dynamics\nand leads to a plateau-like stabilization of the BEC for an exponentially long\ntime, consistent with experiments. We also show that the photon BEC and the\nlasing state are separated by a true phase transition, since they are\ncharacterized by different fixed points. The ghost-attractor nonequilibrium\nstabilization mechanism is alternative to prethermalization and may possibly be\nrealized on other dynamical platforms as well.",
        "One of the simplest possible candidates for dark matter is a stable scalar\nsinglet beyond the Standard Model. If its mass is below the Hubble scale during\ninflation, long-wavelength modes of this scalar will be excited during\ninflation, and their subsequent evolution may lead to the correct relic density\nof dark matter. In this work, we provide a comprehensive analysis of the\nevolution of a spectator scalar. We examine three cases: (1) a non-interacting\nmassive scalar, (2) a massive scalar with self-interactions of the form\n$\\lambda_\\chi \\chi^p$, and (3) a massive scalar coupled to the inflaton $\\phi$\nthrough an interaction term of the form $\\sigma_{n,m} \\phi^n \\chi^m$. In all\ncases, we assume minimal coupling to gravity and compare these results with the\nproduction of short-wavelength modes arising from single graviton exchange. The\nevolution is tracked during the reheating phase. Our findings are summarized\nusing $(m_\\chi, T_{\\rm RH})$ parameter planes, where $m_\\chi$ is the mass of\nthe scalar field and $T_{\\rm RH}$ is the reheating temperature after inflation.\nThe non-interacting scalar is highly constrained, requiring $m_\\chi > 3 \\times\n10^{12}~\\rm {GeV}$ and $ T_{\\rm RH} \\lesssim 7~\\text{TeV}$ for an inflationary\npotential with a quadratic minimum. However, when self-interactions or\ncouplings to the inflaton are included, the viable parameter space expands\nconsiderably. In these cases, sub-GeV and even sub-eV scalar masses can yield\nthe correct relic abundance, opening new possibilities for light dark matter\ncandidates. In all cases, we also impose additional constraints arising from\nthe production of isocurvature fluctuations, the prevention of a secondary\ninflationary phase triggered by the spectator field, and the fragmentation of\nscalar condensates.",
        "The interaction between gas and dust in protoplanetary disks (PPDs) plays a\ncrucial role in setting the stage of planet formation. In particular, the\nstreaming instability (SI) is well recognized as the mechanism for planetesimal\nformation out of this interaction. The outer region of PPDs is likely subject\nto the vertical shear instability (VSI), representing a major source of disk\nturbulence characterized by vertical corrugation that leads to strong dust\nstirring. In the meantime, the VSI turbulence in 3D generates vortices through\nthe Rossby wave instability (RWI), which can trap dust and thereby promote dust\nconcentration. In this study, we use the multifluid dust module in Athena++ to\nconduct 2D axisymmetric global simulations of PPDs with mesh refinement and 3D\nglobal simulations with modest resolution. In 2D, the VSI corrugation mode is\nweakened by dust back-reaction, while the SI can still survive regardless of\ninitial conditions. Dust clumping occurs and is seeded by VSI-induced zonal\nflows. In 3D, dust can settle even more with increased dusty buoyancy,\nsuppressing the VSI corrugation mode. Meanwhile, dust back-reaction enhances\ndust concentration in RWI vortices, though higher resolution is needed to\nassess dust clumping.",
        "Designing materials with controlled photovoltaic response may lead to\nimproved solar cells or photosensors. In this regard, ferroelectric\nsuperlattices have emerged as a rich platform to engineer functional\nproperties. In addition, ferroelectrics are naturally endowed with a bulk\nphotovoltaic response stemming from non-thermalized photoexcited carriers,\nwhich can overcome the fundamental limits of current solar cells. Yet, their\nphotovoltaic output has been limited by poor optical absorption and poor charge\ncollection or photo-excited carrier mean free path. We use Density Functional\nTheory and Wannierization to compute the so-called Bulk Photovoltaic shift\ncurrent and the optical properties of BiFeO3\/LaFeO3 superlattices. We show\nthat, by stacking these two materials, not only the optical absorption is\nimproved at larger wavelengths (due to LaFeO3 smaller bandgap), but the\nphotovolgavanic shift current is also enhanced compared to that of pure BiFeO3\n, by suppressing the destructive interferences occurring between different\nwavelengths.",
        "Bismuth vanadate (BiVO4 BVO) is a promising photocatalyst for solar energy\nconversion, but its efficiency is limited by small polaron formation. However,\nsome physical properties of BVO deviate from typical small polaron behavior.\nUsing the state-of-the-art first-principles calculations, we demonstrate that\nBVO forms a quasi-large hole polaron with a radius around 2 nm, resembling free\ncarriers with high mobility. This polaron is stabilized primarily by acoustic\nphonon modes, creating a shallow trap state near the valence band maximum,\nwhich prolongs its lifetime. Simultaneously, it retains a redox potential\ncomparable to that of free carriers. We propose that such large polarons\nexplain the superior properties of BVO and other transition metal oxide\nphotocatalysts. Tuning phonon modes to stabilize large polarons offers a\npromising strategy for designing materials with enhanced solar energy\nconversion efficiency.",
        "Techniques that rigorously bound the overall rounding error exhibited by a\nnumerical program are of significant interest for communities developing\nnumerical software. However, there are few available tools today that can be\nused to rigorously bound errors in programs that employ conditional statements\n(a basic need) as well as mixed-precision arithmetic (a direction of\nsignificant future interest) employing global optimization in error analysis.\nIn this paper, we present a new tool that fills this void while also employing\nan abstraction-guided optimization approach to allow designers to trade\nerror-bound tightness for gains in analysis time -- useful when searching for\ndesign alternatives. We first present the basic rigorous analysis framework of\nSatire and then show how to extend it to incorporate abstractions,\nconditionals, and mixed-precision arithmetic. We begin by describing Satire's\ndesign and its performance on a collection of benchmark examples. We then\ndescribe these aspects of Satire: (1) how the error-bound and tool execution\ntime vary with the abstraction level; (2) the additional machinery to handle\nconditional expression branches, including defining the concepts of instability\njumps and instability window widths and measuring these quantities; and (3) how\nthe error changes when a mix of precision values are used. To showcase how\n\\satire can add value during design, we start with a Conjugate Gradient solver\nand demonstrate how its step size and search direction are affected by\ndifferent precision settings. Satire is freely available for evaluation, and\ncan be used during the design of numerical routines to effect design tradeoffs\nguided by rigorous empirical error guarantees.",
        "The concept of a rapidly sign-switching cosmological constant, interpreted as\na mirror AdS-dS transition in the late universe and known as the $\\Lambda_{\\rm\ns}$CDM, has significantly improved the fit to observational data, offering a\npromising framework for alleviating major cosmological tensions such as the\n$H_0$ and $S_8$ tensions. However, when considered within general relativity,\nthis scenario does not predict any effects on the evolution of the matter\ndensity contrast beyond modifications to the background functions. In this\nwork, we propose a new gravitational model in which the background dynamics\npredicted by the $\\Lambda_{\\rm s}$CDM framework are mapped into $f(T)$ gravity,\ndubbed $f(T)-\\Lambda_{\\rm s}$CDM, rendering the models indistinguishable at the\nbackground level. However, in this new scenario, the sign-switching\ncosmological constant dynamics modify the evolution of linear matter\nperturbations through an effective gravitational constant, $G_{\\rm eff}$. We\ninvestigate the evolution of the growth rate and derive new observational\nconstraints for this scenario using RSD measurements. We also present new\nconstraints in the standard $\\Lambda_{\\rm s}$CDM case, incorporating the latest\nType Ia supernovae data samples available in the literature, along with BAO\ndata from DESI. Our findings indicate that the new corrections expected at the\nlinear perturbative level, as revealed through RSD samples, can provide\nsignificant evidence in favor of this new scenario. Additionally, this model\nmay be an excellent candidate for resolving the current $S_8$ tension.",
        "We study the two-dimensional square lattice Ising ferromagnet and\nantiferromagnet with a magnetic field by using tensor network method. Focusing\non the role of guage fixing, we present the partition function in terms of a\ntensor network. The tensor has a different symmetry property for ferromagnets\nand antiferromagnets. The tensor network of the partition function is\ninterpreted as a multiple product of the one-dimensional quantum Hamiltonian.\nWe perform infinite density matrix renormalization group to contract the\ntwo-dimensional tensor network. We present the numerical result of\nmagnetization and entanglement entropy for the Ising ferromagnet and\nantiferromagnet side by side. In order to determine the critical line in the\nparameter space of temperature and magnetic field, we use the half-chain\nentanglement entropy of the one-dimensional quantum state. The entanglement\nentropy precisely indicates the critical line forming the parabolic shape for\nthe antiferromagnetic case, but shows the critical point for the ferromagnetic\ncase.",
        "Plenty of hadrons have been established experimentally, yet the\nnonperturbative nature of the strong interaction complicates a comprehensive\nunderstanding of their internal structure, particularly for exotic hadrons that\nextend beyond conventional mesons and baryons. One prominent candidate for the\ninternal structure of exotic hadrons is the hadronic molecule, a loosely bound\nsystem of hadrons analogous to atomic nuclei. Understanding such systems\nrequires precise knowledge of hadron interactions, which traditional scattering\nexperiments struggle to provide, especially in the low-energy region. Recent\nadvances, including precise baryon-baryon interaction measurements, femtoscopy\ntechniques that probe momentum correlations between particles, and\nfirst-principles lattice QCD calculations, have significantly improved our\nunderstanding of hadron interactions. Here, we review these recent\ndevelopments, demonstrate the successful application of femtoscopy to\nantikaon-nucleon interactions, utilize the concept of compositeness to quantify\nthe hadronic molecular component of the Lambda(1405), and discuss both\nexperimental and theoretical prospects, including future studies at J-PARC.",
        "We propose a way to bound the generalisation errors of several classes of\nquantum reservoirs using the Rademacher complexity. We give specific,\nparameter-dependent bounds for two particular quantum reservoir classes. We\nanalyse how the generalisation bounds scale with growing numbers of qubits.\nApplying our results to classes with polynomial readout functions, we find that\nthe risk bounds converge in the number of training samples. The explicit\ndependence on the quantum reservoir and readout parameters in our bounds can be\nused to control the generalisation error to a certain extent. It should be\nnoted that the bounds scale exponentially with the number of qubits $n$. The\nupper bounds on the Rademacher complexity can be applied to other reservoir\nclasses that fulfill a few hypotheses on the quantum dynamics and the readout\nfunction.",
        "We develop a theoretical framework to understand the persistence and\ncoexistence of competitive species in a spatially explicit metacommunity model\nwith a heterogeneous dispersal kernel. Our analysis, based on methods from the\nphysics of disordered systems and non-Gaussian dynamical mean field theory,\nreveals that species coexistence is governed by a single key parameter, which\nwe term competitive balance. From competitive balance, we derive a novel metric\nto quantitatively assess the vulnerability of a species, showing that abundance\nalone is not sufficient to determine it. Rather, a species' vulnerability\ncrucially depends on the state of the metacommunity as a whole. We test our\ntheory by analyzing two distinct tropical forest datasets, finding excellent\nagreement with our theoretical predictions. A key step in our analysis is the\nintroduction of a new quantity - the competitive score - which disentangles the\nabundance distribution and enables us to circumvent the challenge of estimating\nboth the colonization kernel and the joint abundance distribution. Our findings\nprovide novel and fundamental insights into the ecosystem-level trade-offs\nunderlying macroecological patterns and introduce a robust approach for\nestimating extinction risks.",
        "We demonstrate the first microchip semiconductor membrane external-cavity\nsurface-emitting laser. This compact type of laser consists solely of a\nsemiconductor gain region present as a micron-thin membrane, sandwiched between\ntwo transparent heat spreaders. The heat spreaders have a highly reflective\ncoating on their outer facets, which assembles the laser's plane-parallel\nsolid-state cavity with a total length of just ~1 mm. One of the coatings with\nslightly reduced reflectivity acts as outcoupling mirror. The microchip\nmembrane external-cavity surface-emitting laser (microchip MECSEL) is optically\npumped with a standard fiber-coupled diode laser module emitting at 808 nm and\nstabilizes itself due to an occurring thermal lens. More than one watt of\ncontinuous wave output power around 1123 nm and a record value in fitted slope\nefficiency of ~51.4 % with MECSELs, while maintaining excellent beam quality\n(TEM_00, M^2 < 1.05), is demonstrated. Important properties of semiconductor\nlasers such as the efficiency, beam quality, and polarization were\ninvestigated. Further, this setup was used to characterize the thermal lens and\nit's dependence on the absorbed pump power in the microchip MECSEL. Such\nsystems represent an attractive solution, when high-power output at\ncustomizable emission wavelength with excellent beam quality is needed in\ncombination with very compact built size."
      ]
    }
  },
  {
    "id":2412.19927,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Model-free simulations of turbulent reactive flows",
    "start_abstract":"A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given.",
    "start_categories":[
      "physics.flu-dyn"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b40"
      ],
      "title":[
        "Physics guided neural networks for spatio-temporal superresolution of turbulent flows"
      ],
      "abstract":[
        "Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Continuous spectrum-shrinking maps and applications to preserver\n  problems",
        "Aligning LLMs with Domain Invariant Reward Models",
        "People Reduce Workers' Compensation for Using Artificial Intelligence\n  (AI)",
        "DnD Filter: Differentiable State Estimation for Dynamic Systems using\n  Diffusion Models",
        "The Commutators of $n$-dimensional Rough Fractional Hardy Operators on\n  Two Weighted Grand Herz-Morrey Spaces with Variable Exponents",
        "Hypernetwork-based approach for optimal composition design in partially\n  controlled multi-agent systems",
        "Science mapping of the Revista General de Informacion y Documentacion\n  (2005-2022)",
        "Error norm estimates for the block conjugate gradient algorithm",
        "TFBS-Finder: Deep Learning-based Model with DNABERT and Convolutional\n  Networks to Predict Transcription Factor Binding Sites",
        "Scalable Video Conferencing Using SDN Principles",
        "Mining Diamonds in labeled Transition Systems",
        "TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of\n  Tools",
        "Monte-Carlo based non-line-of-sight underwater wireless optical\n  communication channel modeling and system performance analysis under\n  turbulence",
        "AIoT-based smart traffic management system",
        "Phase space analysis of CCDM cosmologies",
        "Understanding Abandonment and Slowdown Dynamics in the Maven Ecosystem",
        "Lifts of Brauer characters in characteristic two, II",
        "From Retrieval to Generation: Comparing Different Approaches",
        "Lagrangian chaos and unique ergodicity for stochastic primitive\n  equations",
        "Making the Peers' Subjective Well-being Visible Impairs\n  Cooperator-centered Experimental Social Networks",
        "Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models",
        "Negativity in Self-Admitted Technical Debt: How Sentiment Influences\n  Prioritization",
        "$H^\\infty$-control for a class of boundary controlled hyperbolic PDEs",
        "Self-supervised Normality Learning and Divergence Vector-guided Model\n  Merging for Zero-shot Congenital Heart Disease Detection in Fetal Ultrasound\n  Videos",
        "Random Subwords and Billiard Walks in Affine Weyl Groups",
        "Kise-Manitow's Hand in Space: Securing Communication and Connections in\n  Space",
        "Higher-Order Belief in Incomplete Information MAIDs",
        "Forward-backward Contention Resolution Schemes for Fair Rationing",
        "A Bridge to Nowhere: A Healthcare Case Study for Non-Reformist Design"
      ],
      "abstract":[
        "For a positive integer $n$ let $\\mathcal{X}_n$ be either the algebra $M_n$ of\n$n \\times n$ complex matrices, the set $N_n$ of all $n \\times n$ normal\nmatrices, or any of the matrix Lie groups $\\mathrm{GL}(n)$, $\\mathrm{SL}(n)$\nand $\\mathrm{U}(n)$. We first give a short and elementary argument that for two\npositive integers $m$ and $n$ there exists a continuous spectrum-shrinking map\n$\\phi : \\mathcal{X}_n \\to M_m$ (i.e.\\ $\\mathrm{sp}(\\phi(X))\\subseteq\n\\mathrm{sp}(X)$ for all $X \\in \\mathcal{X}_n$) if and only if $n$ divides $m$.\nMoreover, in that case we have the equality of characteristic polynomials\n$k_{\\phi(X)}(\\cdot) = k_{X}(\\cdot)^\\frac{m}{n}$ for all $X \\in \\mathcal{X}_n$,\nwhich in particular shows that $\\phi$ preserves spectra. Using this we show\nthat whenever $n \\geq 3$, any continuous commutativity preserving and\nspectrum-shrinking map $\\phi : \\mathcal{X}_n \\to M_n$ is of the form\n$\\phi(\\cdot)=T(\\cdot)T^{-1}$ or $\\phi(\\cdot)=T(\\cdot)^tT^{-1}$, for some $T\\in\n\\mathrm{GL}(n)$. The analogous results fail for the special unitary group\n$\\mathrm{SU}(n)$, and slightly more elaborate versions hold for the spaces of\nsemisimple elements in either $\\mathrm{GL}(n)$ or $\\mathrm{SL}(n)$, where a\nqualitatively new (and surprising) phenomenon arises: the map sending\n$SNS^{-1}$ to $S^{-1}NS$ for positive invertible $S$ and normal $N$ is also an\nexample. As a consequence, we also recover (a strengthened version of)\n\\v{S}emrl's influential characterization of Jordan automorphisms of $M_n$ via\npreserving properties.",
        "Aligning large language models (LLMs) to human preferences is challenging in\ndomains where preference data is unavailable. We address the problem of\nlearning reward models for such target domains by leveraging feedback collected\nfrom simpler source domains, where human preferences are easier to obtain. Our\nkey insight is that, while domains may differ significantly, human preferences\nconvey \\emph{domain-agnostic} concepts that can be effectively captured by a\nreward model. We propose \\method, a framework that trains domain-invariant\nreward models by optimizing a dual loss: a domain loss that minimizes the\ndivergence between source and target distribution, and a source loss that\noptimizes preferences on the source domain. We show \\method is a general\napproach that we evaluate and analyze across 4 distinct settings: (1)\nCross-lingual transfer (accuracy: $0.621 \\rightarrow 0.661$), (2)\nClean-to-noisy (accuracy: $0.671 \\rightarrow 0.703$), (3) Few-shot-to-full\ntransfer (accuracy: $0.845 \\rightarrow 0.920$), and (4) Simple-to-complex tasks\ntransfer (correlation: $0.508 \\rightarrow 0.556$). Our code, models and data\nare available at \\url{https:\/\/github.com\/portal-cornell\/dial}.",
        "We investigate whether and why people might reduce compensation for workers\nwho use AI tools. Across 10 studies (N = 3,346), participants consistently\nlowered compensation for workers who used AI tools. This \"AI Penalization\"\neffect was robust across (1) different types of work and worker statuses and\nworker statuses (e.g., full-time, part-time, or freelance), (2) different forms\nof compensation (e.g., required payments or optional bonuses) and their timing,\n(3) various methods of eliciting compensation (e.g., slider scale, multiple\nchoice, and numeric entry), and (4) conditions where workers' output quality\nwas held constant, subject to varying inferences, or controlled for. Moreover,\nthe effect emerged not only in hypothetical compensation scenarios (Studies\n1-5) but also with real gig workers and real monetary compensation (Study 6).\nPeople reduced compensation for workers using AI tools because they believed\nthese workers deserved less credit than those who did not use AI (Studies 3 and\n4). This effect weakened when it is less permissible to reduce worker\ncompensation, such as when employment contracts provide stricter constraints\n(Study 4). Our findings suggest that adoption of AI tools in the workplace may\nexacerbate inequality among workers, as those protected by structured contracts\nface less vulnerability to compensation reductions, while those without such\nprotections risk greater financial penalties for using AI.",
        "This paper proposes the DnD Filter, a differentiable filter that utilizes\ndiffusion models for state estimation of dynamic systems. Unlike conventional\ndifferentiable filters, which often impose restrictive assumptions on process\nnoise (e.g., Gaussianity), DnD Filter enables a nonlinear state update without\nsuch constraints by conditioning a diffusion model on both the predicted state\nand observational data, capitalizing on its ability to approximate complex\ndistributions. We validate its effectiveness on both a simulated task and a\nreal-world visual odometry task, where DnD Filter consistently outperforms\nexisting baselines. Specifically, it achieves a 25\\% improvement in estimation\naccuracy on the visual odometry task compared to state-of-the-art\ndifferentiable filters, and even surpasses differentiable smoothers that\nutilize future measurements. To the best of our knowledge, DnD Filter\nrepresents the first successful attempt to leverage diffusion models for state\nestimation, offering a flexible and powerful framework for nonlinear estimation\nunder noisy measurements.",
        "In this paper, we obtain the boundedness of $m$th order commutators generated\nby the $n$-dimensional fractional Hardy operator with rough kernel and its\nadjoint operator with BMO functions on two weighted grand Herz-Morrey spaces\nwith variable exponents. Replacing Lipschitz functions with BMO functions the\ncorresponding result is also given.",
        "Partially Controlled Multi-Agent Systems (PCMAS) are comprised of\ncontrollable agents, managed by a system designer, and uncontrollable agents,\noperating autonomously. This study addresses an optimal composition design\nproblem in PCMAS, which involves the system designer's problem, determining the\noptimal number and policies of controllable agents, and the uncontrollable\nagents' problem, identifying their best-response policies. Solving this\nbi-level optimization problem is computationally intensive, as it requires\nrepeatedly solving multi-agent reinforcement learning problems under various\ncompositions for both types of agents. To address these challenges, we propose\na novel hypernetwork-based framework that jointly optimizes the system's\ncomposition and agent policies. Unlike traditional methods that train separate\npolicy networks for each composition, the proposed framework generates policies\nfor both controllable and uncontrollable agents through a unified hypernetwork.\nThis approach enables efficient information sharing across similar\nconfigurations, thereby reducing computational overhead. Additional\nimprovements are achieved by incorporating reward parameter optimization and\nmean action networks. Using real-world New York City taxi data, we demonstrate\nthat our framework outperforms existing methods in approximating equilibrium\npolicies. Our experimental results show significant improvements in key\nperformance metrics, such as order response rate and served demand,\nhighlighting the practical utility of controlling agents and their potential to\nenhance decision-making in PCMAS.",
        "A study of the Revista General de Informacion y Documentacion, from 2005 to\n2022. The objective is aimed at qualifying the structure of the research field\nand assessing the trajectory of the thematic areas covered. Applying as\nmethodology the analysis of co-words, the construction of bibliometric networks\nand the creation of scientific maps. 514 documents are extracted from the Web\nof Science (WoS) database. The keywords assigned by the authors of the\ndocuments are selected and divided into three subperiods: 2005-2010, 2011-2016\nand 2017-2022. In the results, 1701 author keywords and 37 bibliometric\nnetworks are obtained. In the period 2005-2010, the structure of the research\nfield is represented on the scientific map with very few central and\nspecialized topics, considering an initial and underdeveloped organization. In\nthe period 2011-2016, the structure of the research field is distributed on the\nscientific map with a more varied number of central and specialized topics, but\nstill insufficient, considering an organization in the process of development.\nIn the period 2017-2022, the structure of the research field is shown on the\nmap with all kinds of family of topics (central, specialized, transversal,\nemerging or disappearing), being valued as a dynamic, complex and heterogeneous\norganization. Regarding the evolution of the thematic areas, the map shows\nsolid progress between the last two periods. The morphology of the thematic\nfield treated in RGID is outlined in three phases: foundation, process of\ndevelopment and consolidation.",
        "In the book [Meurant and Tichy, SIAM, 2024] we discussed the estimation of\nerror norms in the conjugate gradient (CG) algorithm for solving linear systems\n$Ax=b$ with a symmetric positive definite matrix $A$, where $b$ and $x$ are\nvectors. In this paper, we generalize the most important formulas for\nestimating the $A$-norm of the error to the block case. First, we discuss in\ndetail the derivation of various variants of the block CG (BCG) algorithm from\nthe block Lanczos algorithm. We then consider BCG and derive the related block\nGauss and block Gauss-Radau quadrature rules. We show how to obtain lower and\nupper bounds on the $A$-norm of the error of each system, both in terms of the\nquantities computed in BCG and in terms of the underlying block Lanczos\nalgorithm. Numerical experiments demonstrate the behavior of the bounds in\npractical computations.",
        "Transcription factors are proteins that regulate the expression of genes by\nbinding to specific genomic regions known as Transcription Factor Binding Sites\n(TFBSs), typically located in the promoter regions of those genes. Accurate\nprediction of these binding sites is essential for understanding the complex\ngene regulatory networks underlying various cellular functions. In this regard,\nmany deep learning models have been developed for such prediction, but there is\nstill scope of improvement. In this work, we have developed a deep learning\nmodel which uses pre-trained DNABERT, a Convolutional Neural Network (CNN)\nmodule, a Modified Convolutional Block Attention Module (MCBAM), a Multi-Scale\nConvolutions with Attention (MSCA) module and an output module. The pre-trained\nDNABERT is used for sequence embedding, thereby capturing the long-term\ndependencies in the DNA sequences while the CNN, MCBAM and MSCA modules are\nuseful in extracting higher-order local features. TFBS-Finder is trained and\ntested on 165 ENCODE ChIP-seq datasets. We have also performed ablation studies\nas well as cross-cell line validations and comparisons with other models. The\nexperimental results show the superiority of the proposed method in predicting\nTFBSs compared to the existing methodologies. The codes and the relevant\ndatasets are publicly available at\nhttps:\/\/github.com\/NimishaGhosh\/TFBS-Finder\/.",
        "Video-conferencing applications face an unwavering surge in traffic,\nstressing their underlying infrastructure in unprecedented ways. This paper\nrethinks the key building block for conferencing infrastructures -- selective\nforwarding units (SFUs). SFUs relay and adapt media streams between\nparticipants and, today, run in software on general-purpose servers. Our main\ninsight, discerned from dissecting the operation of production SFU servers, is\nthat SFUs largely mimic traditional packet-processing operations such as\ndropping and forwarding. Guided by this, we present Scallop, an SDN-inspired\nSFU that decouples video-conferencing applications into a hardware-based data\nplane for latency-sensitive and frequent media operations, and a software\ncontrol plane for the (infrequent) remaining tasks, such as analyzing feedback\nsignals. Our Tofino-based implementation fully supports WebRTC and delivers\n7-210 times improved scaling over a 32-core commodity server, while reaping\nperformance improvements by cutting forwarding-induced latency by 26 times.",
        "Labeled transition systems can be a great way to visualize the complex\nbehavior of parallel and communicating systems. However, if, during a\nparticular timeframe, no synchronization or communication between processes\noccurs, then multiple parallel sequences of actions are able to interleave\narbitrarily, and the resulting graph quickly becomes too complex for the human\neye to understand easily. With that in mind, we propose an exact formalization\nof these arbitrary interleavings, and an algorithm to find all said\ninterleavings in deterministic LTSs, to reduce the visual complexity of labeled\ntransition systems.",
        "Precision therapeutics require multimodal adaptive models that generate\npersonalized treatment recommendations. We introduce TxAgent, an AI agent that\nleverages multi-step reasoning and real-time biomedical knowledge retrieval\nacross a toolbox of 211 tools to analyze drug interactions, contraindications,\nand patient-specific treatment strategies. TxAgent evaluates how drugs interact\nat molecular, pharmacokinetic, and clinical levels, identifies\ncontraindications based on patient comorbidities and concurrent medications,\nand tailors treatment strategies to individual patient characteristics. It\nretrieves and synthesizes evidence from multiple biomedical sources, assesses\ninteractions between drugs and patient conditions, and refines treatment\nrecommendations through iterative reasoning. It selects tools based on task\nobjectives and executes structured function calls to solve therapeutic tasks\nthat require clinical reasoning and cross-source validation. The ToolUniverse\nconsolidates 211 tools from trusted sources, including all US FDA-approved\ndrugs since 1939 and validated clinical insights from Open Targets. TxAgent\noutperforms leading LLMs, tool-use models, and reasoning agents across five new\nbenchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC,\ncovering 3,168 drug reasoning tasks and 456 personalized treatment scenarios.\nIt achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing\nGPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning.\nTxAgent generalizes across drug name variants and descriptions. By integrating\nmulti-step inference, real-time knowledge grounding, and tool-assisted\ndecision-making, TxAgent ensures that treatment recommendations align with\nestablished clinical guidelines and real-world evidence, reducing the risk of\nadverse events and improving therapeutic decision-making.",
        "Compared with line-of-sight (LOS) communication, nonline-of-sight (NLOS)\nunderwater wireless optical communication (UWOC) systems have garnered\nextensive attention because of their heightened suitability for the intricate\nand dynamic underwater environment. In the NLOS channel, photons can reach the\nreceiver by sea surface reflection or particle scattering. However, research\nlacks comprehensive channel models that incorporate sea surface reflection and\nparticle scattering. Moreover, the presence of ocean turbulence introduces\nrandom fluctuations in the received optical signal based on the average light\nintensity. Consequently, this paper adopts the Monte Carlo simulation method\n(MCS) to solve the fading-free impulse response of the joint\nreflection-scattering channel. Furthermore, a weighted double gamma function\n(WDGF) is proposed to characterize the channel impulse response (CIR). Based on\nthe closed CIR model, the average bit error rate and the performance of the\ninterruption probability of the UWOC system under turbulence are analyzed. The\nconclusions obtained are intended to assist in the design and performance\nevaluation of NLOS UWOC systems.",
        "This paper presents a novel AI-based smart traffic management system\nde-signed to optimize traffic flow and reduce congestion in urban environments.\nBy analysing live footage from existing CCTV cameras, this approach eliminates\nthe need for additional hardware, thereby minimizing both deployment costs and\nongoing maintenance expenses. The AI model processes live video feeds to\naccurately count vehicles and assess traffic density, allowing for adaptive\nsignal control that prioritizes directions with higher traffic volumes. This\nreal-time adaptability ensures smoother traffic flow, reduces congestion, and\nminimizes waiting times for drivers. Additionally, the proposed system is\nsimulated using PyGame to evaluate its performance under various traffic\nconditions. The simulation results demonstrate that the AI-based system\nout-performs traditional static traffic light systems by 34%, leading to\nsignificant improvements in traffic flow efficiency. The use of AI to optimize\ntraffic signals can play a crucial role in addressing urban traffic challenges,\noffering a cost-effective, scalable, and efficient solution for modern cities.\nThis innovative system represents a key advancement in the field of smart city\ninfra-structure and intelligent transportation systems.",
        "We perform a detailed investigation of the CCDM (creation of cold dark\nmatter) cosmologies using the powerful techniques of qualitative analysis of\ndynamical systems. Considering a wide variety of the creation rates ranging\nfrom constant to dynamical, we examine the nature of critical points and their\nstability obtained from the individual scenario consisting of only cold dark\nmatter, or cold dark matter plus a second fluid with constant equation of\nstate. According to our analyses, these scenarios predict unstable dark matter\ndominated critical points, stable accelerating attractors dominated either by\ndark matter or the second fluid, scaling attractors in which both dark matter\nand the second fluid co-exist. Along with these critical points, these\nscenarios also indicate the possibility of decelerating attractors or\ndecelerating scaling attractors in the future which are new results in this\ndirection. These altogether suggest that CCDM cosmologies are viable\nalternatives to the mainstream cosmological models.",
        "The sustainability of libraries is critical for modern software development,\nyet many libraries face abandonment, posing significant risks to dependent\nprojects. This study explores the prevalence and patterns of library\nabandonment in the Maven ecosystem. We investigate abandonment trends over the\npast decade, revealing that approximately one in four libraries fail to survive\nbeyond their creation year. We also analyze the release activities of\nlibraries, focusing on their lifespan and release speed, and analyze the\nevolution of these metrics within the lifespan of libraries. We find that while\nslow release speed and relatively long periods of inactivity are often\nprecursors to abandonment, some abandoned libraries exhibit bursts of high\nfrequent release activity late in their life cycle. Our findings contribute to\na new understanding of library abandonment dynamics and offer insights for\npractitioners to identify and mitigate risks in software ecosystems.",
        "In 2007, J. P. Cossey conjectured that if $G$ is a finite $p$-solvable group\nand $\\varphi$ is an irreducible Brauer character of $G$ with vertex $Q$, then\nthe number of lifts of $\\varphi$ is at most $|Q:Q'|$. In this paper we\nrevisited Cossey's conjecture for $p=2$ from the perspective of Navarro\nvertices and obtained a new way to count the number of lifts of $\\varphi$. Some\napplications were given.",
        "Knowledge-intensive tasks, particularly open-domain question answering\n(ODQA), document reranking, and retrieval-augmented language modeling, require\na balance between retrieval accuracy and generative flexibility. Traditional\nretrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently\nretrieve from large corpora but often lack semantic depth. Generative models\nlike GPT-4-o provide richer contextual understanding but face challenges in\nmaintaining factual consistency. In this work, we conduct a systematic\nevaluation of retrieval-based, generation-based, and hybrid models, with a\nprimary focus on their performance in ODQA and related retrieval-augmented\ntasks. Our results show that dense retrievers, particularly DPR, achieve strong\nperformance in ODQA with a top-1 accuracy of 50.17\\% on NQ, while hybrid models\nimprove nDCG@10 scores on BEIR from 43.42 (BM25) to 52.59, demonstrating their\nstrength in document reranking. Additionally, we analyze language modeling\ntasks using WikiText-103, showing that retrieval-based approaches like BM25\nachieve lower perplexity compared to generative and hybrid methods,\nhighlighting their utility in retrieval-augmented generation. By providing\ndetailed comparisons and practical insights into the conditions where each\napproach excels, we aim to facilitate future optimizations in retrieval,\nreranking, and generative models for ODQA and related knowledge-intensive\napplications.",
        "We show that the Lagrangian flow associated with the stochastic 3D primitive\nequations (PEs) with non-degenerate noise is chaotic, i.e., the corresponding\ntop Lyapunov exponent is strictly positive almost surely. This result builds on\nthe landmark work by Bedrossian, Blumenthal, and Punshon-Smith on Lagrangian\nchaos in stochastic fluid mechanics. Our primary contribution is establishing\nan instance where Lagrangian chaos can be proven for a fluid flow with\nsupercritical energy, a key characteristic of 3D fluid dynamics. For the 3D\nPEs, establishing the existence of the top Lyapunov exponent is already a\nchallenging task. We address this difficulty by deriving new estimates for the\ninvariant measures of the 3D PEs, which capture the anisotropic smoothing in\nthe dynamics of the PEs. As a by-product of our results, we also obtain the\nfirst uniqueness result for invariant measures of stochastic PEs.",
        "Past experiments show that reputation or the knowledge of peers' past\ncooperation can enhance cooperation in human social networks. On the other\nhand, the knowledge of peers' wealth undermines cooperativeness, and that of\npeers' interconnectedness and network structure does not affect it. However, it\nis unknown if making peers' subjective well-being (SWB) available or visible in\nsocial networks may enhance or undermine cooperation. Therefore, we implemented\nonline network experiments (N = 662 in 50 networked groups with 15 rounds of\ninteractions), in which study participants cooperated with or defected against\nconnected peers through Public Goods Game, made and cut social ties with\nothers, and rated their SWB. We manipulated the visibility of connected peers'\nSWB (25 visible vs. 25 invisible SWB networked groups) while keeping the\nconnected peers' reputation and in-game wealth visible. Results show that\nmaking the peers\/ SWB visible did not alter overall cooperativeness, wealth,\ninter-connectedness, or SWB. In contrast, the visible SWB networked groups\nexhibited a higher number of communities and lower transitivity (the proportion\nof the cases where a peer of a peer is also a peer) than the invisible SWB\nnetworked groups. These phenomena are explained by an altered decision-making\npattern in the visible SWB networks: cooperators were less likely to connect\nwith cooperators and more likely to connect with defectors, and consequently,\ncooperators could not maintain their popularity or stay in the center of the\nnetworks.",
        "Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.",
        "Self-Admitted Technical Debt, or SATD, is a self-admission of technical debt\npresent in a software system. To effectively manage SATD, developers need to\nestimate its priority and assess the effort required to fix the described\ntechnical debt. About a quarter of descriptions of SATD in software systems\nexpress some form of negativity or negative emotions when describing technical\ndebt. In this paper, we report on an experiment conducted with 59 respondents\nto study whether negativity expressed in the description of SATD\n\\textbf{actually} affects the prioritization of SATD. The respondents are a mix\nof professional developers and students, and in the experiment, we asked\nparticipants to prioritize four vignettes: two expressing negativity and two\nexpressing neutral sentiment. To ensure realism, vignettes were based on\nexisting SATD. We find that negativity causes between one-third and half of\ndevelopers to prioritize SATD, in which negativity is expressed as having more\npriority. Developers affected by negativity when prioritizing SATD are twice as\nlikely to increase their estimation of urgency and 1.5 times as likely to\nincrease their estimation of importance and effort for SATD compared to the\nlikelihood of decreasing these prioritization scores. Our findings show how\ndevelopers actively use negativity in SATD to determine how urgently a\nparticular instance of TD should be addressed. However, our study also\ndescribes a gap in the actions and belief of developers. Even if 33% to 50% use\nnegativity to prioritize SATD, 67% of developers believe that using negativity\nas a proxy for priority is unacceptable. Therefore, we would not recommend\nusing negativity as a proxy for priority. However, we also recognize that\ndevelopers might unavoidably express negativity when describing technical debt.",
        "A solution to the suboptimal $H^\\infty$-control problem is given for a class\nof hyperbolic partial differential equations (PDEs). The first result of this\nmanuscript shows that the considered class of PDEs admits an equivalent\nrepresentation as an infinite-dimensional discrete-time system. Taking\nadvantage of this, this manuscript shows that it is equivalent to solve the\nsuboptimal $H^\\infty$-control problem for a finite-dimensional discrete-time\nsystem whose matrices are derived from the PDEs. After computing the solution\nto this much simpler problem, the solution to the original problem can be\ndeduced easily. In particular, the optimal compensator solution to the\nsuboptimal $H^\\infty$-control problem is governed by a set of hyperbolic PDEs,\nactuated and observed at the boundary. We illustrate our results with a\nboundary controlled and boundary observed vibrating string.",
        "Congenital Heart Disease (CHD) is one of the leading causes of fetal\nmortality, yet the scarcity of labeled CHD data and strict privacy regulations\nsurrounding fetal ultrasound (US) imaging present significant challenges for\nthe development of deep learning-based models for CHD detection. Centralised\ncollection of large real-world datasets for rare conditions, such as CHD, from\nlarge populations requires significant co-ordination and resource. In addition,\ndata governance rules increasingly prevent data sharing between sites. To\naddress these challenges, we introduce, for the first time, a novel\nprivacy-preserving, zero-shot CHD detection framework that formulates CHD\ndetection as a normality modeling problem integrated with model merging. In our\nframework dubbed Sparse Tube Ultrasound Distillation (STUD), each hospital site\nfirst trains a sparse video tube-based self-supervised video anomaly detection\n(VAD) model on normal fetal heart US clips with self-distillation loss. This\nenables site-specific models to independently learn the distribution of healthy\ncases. To aggregate knowledge across the decentralized models while maintaining\nprivacy, we propose a Divergence Vector-Guided Model Merging approach,\nDivMerge, that combines site-specific models into a single VAD model without\ndata exchange. Our approach preserves domain-agnostic rich spatio-temporal\nrepresentations, ensuring generalization to unseen CHD cases. We evaluated our\napproach on real-world fetal US data collected from 5 hospital sites. Our\nmerged model outperformed site-specific models by 23.77% and 30.13% in accuracy\nand F1-score respectively on external test sets.",
        "Let $W$ be an irreducible affine Weyl group, and let $\\mathsf{b}$ be a finite\nword over the alphabet of simple reflections of $W$. Fix a probability\n$p\\in(0,1)$. For each integer $K\\geq 0$, let $\\mathsf{sub}_p(\\mathsf{b}^K)$ be\nthe random subword of $\\mathsf{b}^K$ obtained by deleting each letter\nindependently with probability $1-p$. Let $v_p(\\mathsf{b}^K)$ be the element of\n$W$ represented by $\\mathsf{sub}_p(\\mathsf{b}^K)$. One can view\n$v_p(\\mathsf{b}^K)$ geometrically as a random alcove; in many cases, this\nalcove can be seen as the location after a certain amount of time of a random\nbilliard trajectory that, upon hitting a hyperplane in the Coxeter arrangement\nof $W$, reflects off of the hyperplane with probability $1-p$. We show that the\nasymptotic distribution of $v_p(\\mathsf{b}^K)$ is a central spherical\nmultivariate normal distribution with some variance $\\sigma_{\\mathsf{b}}^2$\ndepending on $\\mathsf{b}$ and $p$. We provide a formula to compute\n$\\sigma_{\\mathsf{b}}^2$ that is remarkably simple when $\\mathsf{b}$ contains\nonly one occurrence of the simple reflection that is not in the associated\nfinite Weyl group. As a corollary, we provide an asymptotic formula for\n$\\mathbb{E}[\\ell(v_p(\\mathsf{b}^K))]$, the expected Coxeter length of\n$v_p(\\mathsf{b}^K)$. For example, when $W=\\widetilde A_{r}$ and $\\mathsf{b}$\ncontains each simple reflection exactly once, we find that\n\\[\\lim_{K\\to\\infty}\\frac{1}{\\sqrt{K}}\\mathbb{E}[\\ell(v_p(\\mathsf{b}^K))]=\\sqrt{\\frac{2}{\\pi}r(r+1)\\frac{p}{1-p}}.\\]",
        "The increasing complexity of space systems, coupled with their critical\noperational roles, demands a robust, scalable, and sustainable security\nframework. This paper presents a novel system-of-systems approach for the\nupcoming Lunar Gateway. We demonstrate the application of the\nsecure-by-component approach to the two earliest deployed systems in the\nGateway, emphasizing critical security controls both internally and for\nexternal communication and connections. Additionally, we present a phased\napproach for the integration of Canadarm3, addressing the unique security\nchallenges that arise from both inter-system interactions and the arm's\nautonomous capabilities.",
        "Multi-agent influence diagrams (MAIDs) are probabilistic graphical models\nwhich represent strategic interactions between agents. MAIDs are equivalent to\nextensive form games (EFGs) but have a more compact and informative structure.\nHowever, MAIDs cannot, in general, represent settings of incomplete information\n-- wherein agents have different beliefs about the game being played, and\ndifferent beliefs about each-other's beliefs. In this paper, we introduce\nincomplete information MAIDs (II-MAIDs). We define both infinite and\nfinite-depth II-MAIDs and prove an equivalence relation to EFGs with incomplete\ninformation and no common prior over types. We prove that II-MAIDs inherit\nclassical equilibria concepts via this equivalence, but note that these\nsolution concepts are often unrealistic in the setting with no common prior\nbecause they violate common knowledge of rationality. We define a more\nrealistic solution concept based on recursive best-response. Throughout, we\ndescribe an example with a hypothetical AI agent undergoing evaluation to\nillustrate the applicability of II-MAIDs.",
        "We use contention resolution schemes (CRS) to derive algorithms for the fair\nrationing of a single resource when agents have stochastic demands. We aim to\nprovide ex-ante guarantees on the level of service provided to each agent, who\nmay measure service in different ways (Type-I, II, or III), calling for CRS\nunder different feasibility constraints (rank-1 matroid or knapsack). We are\nparticularly interested in two-order CRS where the agents are equally likely to\narrive in a known forward order or its reverse, which is motivated by online\nrationing at food banks.\n  In particular, we derive a two-order CRS for rank-1 matroids with guarantee\n$1\/(1+e^{-1\/2})\\approx 0.622$, which we prove is tight. This improves upon the\n$1\/2$ guarantee that is best-possible under a single order (Alaei, SIAM J.\nComput. 2014), while achieving separation with the $1-1\/e\\approx 0.632$\nguarantee that is possible for random-order CRS (Lee and Singla, ESA 2018).\nBecause CRS guarantees imply prophet inequalities, this also beats the\ntwo-order prophet inequality with ratio $(\\sqrt{5}-1)\/2\\approx 0.618$ from\n(Arsenis, SODA 2021), which was tight for single-threshold policies. Rank-1\nmatroids suffice to provide guarantees under Type-II or III service, but Type-I\nservice requires knapsack. Accordingly, we derive a two-order CRS for knapsack\nwith guarantee $1\/3$, improving upon the $1\/(3+e^{-2})\\approx 0.319$ guarantee\nthat is best-possible under a single order (Jiang et al., SODA 2022). To our\nknowledge, $1\/3$ provides the best-known guarantee for knapsack CRS even in the\noffline setting. Finally, we provide an upper bound of $1\/(2+e^{-1})\\approx\n0.422$ for two-order knapsack CRS, strictly smaller than the upper bound of\n$(1-e^{-2})\/2\\approx0.432$ for random-order knapsack CRS.",
        "In the face of intensified datafication and automation in public sector\nindustries, frameworks like design justice and the feminist practice of refusal\nprovide help to identify and mitigate structural harm and challenge inequities\nreproduced in digitized infrastructures. This paper applies those frameworks to\nemerging efforts across the U.S. healthcare industry to automate prior\nauthorization -- a process whereby insurance companies determine whether a\ntreatment or service is 'medically necessary' before agreeing to cover it.\nFederal regulatory interventions turn to datafication and automation to reduce\nthe harms of this widely unpopular process shown to delay vital treatments and\ncreate immense administrative burden for healthcare providers and patients.\nThis paper explores emerging prior authorization reforms as a case study,\napplying the frameworks of design justice and refusal to highlight the inherent\nconservatism of interventions oriented towards improving the user experience of\nextractive systems. I further explore how the abolitionist framework of\nnon-reformist reform helps to clarify alternative interventions that would\nmitigate the harms of prior authorization in ways that do not reproduce or\nextend the power of insurance companies. I propose a set of four tenets for\nnonreformist design to mitigate structural harms and advance design justice in\na broad set of domains."
      ]
    }
  },
  {
    "id":2412.19927,
    "research_type":"applied",
    "start_id":"b40",
    "start_title":"Physics guided neural networks for spatio-temporal superresolution of turbulent flows",
    "start_abstract":"Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Model-free simulations of turbulent reactive flows"
      ],
      "abstract":[
        "A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given."
      ],
      "categories":[
        "physics.flu-dyn"
      ]
    },
    "list":{
      "title":[
        "To Hedge or Not to Hedge: Optimal Strategies for Stochastic Trade Flow\n  Management",
        "Freelance Holography, Part II: Moving Boundary in Gauge\/Gravity\n  Correspondence",
        "Dynamically Learning to Integrate in Recurrent Neural Networks",
        "Automated DC Voltage and DC Resistance Real-time Multiple Standard for\n  Artifact Calibration of Calibrators and multimeters",
        "Two almost planetary mass survivors of common envelope evolution",
        "Electrochemically induced hyperfluorescence based on the formation of\n  charge-transfer excimers",
        "Computation of generalised magnetic coordinates asymptotically close to\n  the separatrix",
        "The diffuse extragalactic gamma-ray background radiation: star-forming\n  galaxies are not the dominant component",
        "Tight Analysis of Difference-of-Convex Algorithm (DCA) Improves\n  Convergence Rates for Proximal Gradient Descent",
        "An Open Source Validation System for Continuous Arterial Blood Pressure\n  Measuring Sensors",
        "Updated analysis of neutron magnetic form factor and the nucleon\n  transverse densities",
        "Power residue symbols and the exponential local-global principle",
        "In-medium bottomonium properties from lattice NRQCD calculations with\n  extended meson operators",
        "MXMap: A Multivariate Cross Mapping Framework for Causal Discovery in\n  Dynamical Systems",
        "Adsorption Behavior of Greenhouse Gases on Carbon Nanobelts: A\n  Semi-Empirical Tight-Binding Approach for Environmental Application",
        "Atomic Altermagnetism",
        "Positive weighted partitions generated by double series",
        "Running vacuum and H^4-inflation",
        "Interplay between tidal flows and magnetic fields in nonlinear\n  simulations of stellar and planetary convective envelopes",
        "Complementary signatures of $\\alpha-$attractor inflation in CMB and\n  cosmic string Gravitational Waves",
        "Improved Two-source Extractors against Quantum Side Information",
        "Exceptional Topology of Non-Hermitian Brillouin Klein Bottles",
        "Blob velocities and sizes in the Alcator C-Mod scrape-off layer for\n  ohmic and high confinement mode plasmas",
        "Power-Efficient Over-the-Air Aggregation with Receive Beamforming for\n  Federated Learning",
        "Reed-Solomon Codes Against Insertions and Deletions: Full-Length and\n  Rate-$1\/2$ Codes",
        "Causality constraints on radiative transfer",
        "Analysis of Learning-based Offshore Wind Power Prediction Models with\n  Various Feature Combinations",
        "Local Quantum Mechanical Prediction of the Singlet State",
        "A Fast and Robust Reformulation of the UVN-Flash Problem via Direct\n  Entropy Maximization"
      ],
      "abstract":[
        "This paper addresses the trade-off between internalisation and\nexternalisation in the management of stochastic trade flows. We consider agents\nwho must absorb flows and manage risk by deciding whether to warehouse it or\nhedge in the market, thereby incurring transaction costs and market impact.\nUnlike market makers, these agents cannot skew their quotes to attract\noffsetting flows and deter risk-increasing ones, leading to a fundamentally\ndifferent problem. Within the Almgren-Chriss framework, we derive\nalmost-closed-form solutions in the case of quadratic execution costs, while\nmore general cases require numerical methods. In particular, we discuss the\nchallenges posed by artificial boundary conditions when using classical\ngrid-based numerical PDE techniques and propose reinforcement learning methods\nas an alternative.",
        "We continue developing the freelance holography program, formulating\ngauge\/gravity correspondence where the gravity side is formulated on a space\nbounded by a generic timelike codimension-one surface inside AdS and arbitrary\nboundary conditions are imposed on the gravity fields on the surface. Our\nanalysis is performed within the Covariant Phase Space Formalism (CPSF). We\ndiscuss how a given boundary condition on the bulk fields on a generic boundary\nevolves as we move the boundary to another boundary inside AdS and work out how\nthis evolution is encoded in deformations of the holographic boundary theory.\nOur analyses here extend the extensively studied T$\\bar{\\text{T}}$-deformation\nby relaxing the boundary conditions at asymptotic AdS or at the cutoff surface\nto be any arbitrary one (besides Dirichlet). We discuss some of the\nimplications of our general freelance holography setting.",
        "Learning to remember over long timescales is fundamentally challenging for\nrecurrent neural networks (RNNs). While much prior work has explored why RNNs\nstruggle to learn long timescales and how to mitigate this, we still lack a\nclear understanding of the dynamics involved when RNNs learn long timescales\nvia gradient descent. Here we build a mathematical theory of the learning\ndynamics of linear RNNs trained to integrate white noise. We show that when the\ninitial recurrent weights are small, the dynamics of learning are described by\na low-dimensional system that tracks a single outlier eigenvalue of the\nrecurrent weights. This reveals the precise manner in which the long timescale\nassociated with white noise integration is learned. We extend our analyses to\nRNNs learning a damped oscillatory filter, and find rich dynamical equations\nfor the evolution of a conjugate pair of outlier eigenvalues. Taken together,\nour analyses build a rich mathematical framework for studying dynamical\nlearning problems salient for both machine learning and neuroscience.",
        "An automated temperature-controlled electrical DC voltage and DC resistance\nmultiple reference standard (MRS) has been developed by Measurements\nInternational (MI) with the scientific support from the Istituto Nazionale di\nRicerca Metrologica (INRIM). The MRS includes a 10 V, a 1 {\\Omega}, and a 10\nk{\\Omega} standards selectable via a switch unit. This setup allows the\nartifact calibration of high-end calibrators and multimeters used in\nlow-frequency electrical measurements. The two resistors are high-stability\nstandards from MI, while the 10 V standard is based on a low-noise circuit\ndeveloped by INRIM in collaboration with MI. A key innovation is the internal\nreal-time clock calendar, which displays the calibration values of the MRS\nstandards and their updated values internally calculated. This ensures reliable\nuse of the MRS standards over extended periods between calibrations,\neffectively minimizing uncertainties due to their drift. The standards are\nhoused in a thermal box, minimizing temperature variations. The MRS standards\nmeet the uncertainty requirements defined by calibrators and multimeters\nmanufacturers for artifact calibration and can also serve as laboratory\nreferences or travelling standards for interlaboratory comparisons (ILCs). MI\nis currently commercializing the MRS.",
        "White dwarfs are often found in close binaries with stellar or even\nsubstellar companions. It is generally thought that these compact binaries form\nvia common envelope evolution, triggered by the progenitor of the white dwarf\nexpanding after it evolved off the main-sequence and engulfing its companion.\nTo date, a handful of white dwarfs in compact binaries with substellar\ncompanions have been found, typically with masses greater than around 50\nM$_\\mathrm{Jup}$. Here we report the discovery of two eclipsing white dwarf\nplus brown dwarf binaries containing very low mass brown dwarfs. ZTF J1828+2308\nconsists of a hot ($15900\\pm75$ K) $0.610\\pm0.004$ M$_{\\odot}$ white dwarf in a\n2.7 hour binary with a $0.0186\\pm0.0008$ M$_{\\odot}$ ($19.5\\pm0.8$\nM$_\\mathrm{Jup}$) brown dwarf. ZTF J1230$-$2655 contains a cool ($10000\\pm110$\nK) $0.65\\pm0.02$ M$_{\\odot}$ white dwarf in a 5.7 hour binary with a companion\nthat has a mass of less than 0.0211 M$_{\\odot}$ (22.1 M$_\\mathrm{Jup}$). While\nthe brown dwarf in ZTF J1828+2308 has a radius consistent with its mass and\nage, ZTF J1230$-$2655 contains a roughly 20 per cent overinflated brown dwarf\nfor its age. We are only able to reconstruct the common envelope phase for\neither system if it occurred after the first thermal pulse, when the white\ndwarf progenitor had already lost a significant fraction of its original mass.\nThis is true even for very high common envelope ejection efficiencies\n($\\alpha_\\mathrm{CE}\\sim 1$), unless both systems have extremely low\nmetallicities. It may be that the lowest mass companions can only survive a\ncommon envelope phase if it occurs at this very late stage.",
        "Despite the extensive use of electrochemiluminescence in sensing\napplications, its potential in lighting and display technology has been\nconstrained by the low luminance and short operational lifetime of\nelectrochemiluminescence devices (ECLDs). Here, we demonstrate a substantial\nenhancement in the luminance, efficiency, and operational longevity of ECLDs by\nintroducing electrochemically induced hyperfluorescence (ECiHF) via\nelectrogeneration of charge-transfer (CT) excimers and subsequent energy\ntransfer to fluorescent acceptors. By assuming a double-decker arrangement of\nthe electron donor and acceptor groups, the molecule TpAT-tFFO supports\nsolution-state thermally activated delayed fluorescence from a CT excimer state\nand efficient energy transfer to the rubrene dye TBRb. Optimized ECLDs based on\nthis material combination achieve an unprecedented luminance of 6,220 cd\/m2 and\ntheir operational lifetime (LT50) at an initial luminance of 100 cd\/m2 exceeds\n20 minutes, more than 10-fold longer than other ECLDs with meaningful\nefficiency or brightness. We identify energy level alignment between the\nexcimer and the emitter as a crucial factor for efficient ECiHF. In mixtures\nwith energy gaps > 0.5 eV, electron transfer results in reduced performance and\nrenders the operation strongly dependent on applied voltage and frequency. By\ncontrast, spectroelectrochemical analysis reveals that devices with favorable\nenergy level alignment operate on a pure excimer mechanism across a wide range\nof frequencies. These findings highlight the innovative potential of ECiHF in\nimproving the performance of ECLD, which can be widely applied in future\ncommercial lighting solutions.",
        "Integrals to calculate generalised magnetic coordinates from an input\nmagnetic flux function asymptotically close to the separatrix are presented,\nand implemented in the GPEC\/DCON code suite. These integrals allow\ncharacterisation of the magnetic equilibrium of a diverted tokamak, in magnetic\ncoordinates, arbitrarily close to the last closed flux surface, avoiding the\nnumerical issues associated with calculating diverging field line integrals\nnear a magnetic x-point. These methods provide an important first step in the\ndevelopment of robust asymptotic equilibrium behaviour for spectral 3D MHD\ncodes at the separatrix.",
        "Star-forming galaxies (SFGs) are considered to be an important component of\nthe diffuse extragalactic gamma-ray background (EGB) radiation observed in 0.1\n-- 820 GeV, but their quantitative contribution has not yet been precisely\ndetermined. In this study, we aim to provide the currently most reliable\nestimate of the contribution of SFGs based on careful calibration with\ngamma-ray luminosities of nearby galaxies and physical quantities (star\nformation rate, stellar mass, and size) of galaxies observed by high-redshift\ngalaxy surveys. Our calculations are based on the latest database of particle\ncollision cross-sections and energy spectra of secondary particles, and take\ninto account not only hadronic but also leptonic processes with various\nradiation fields in a galaxy. We find that SFGs are not the dominant component\nof the unresolved EGB measured by Fermi; the largest contribution is around 50%\n-- 60% in the 1 -- 10 GeV region, and the contribution falls rapidly in lower\nand higher energy ranges. This result appears to contradict a previous study,\nwhich claimed that SFGs are the dominant component of the unresolved EGB, and\nthe origin of the discrepancy is examined. In calculations of cosmic-ray\nproduction, propagation, and interaction in a galaxy, we try models developed\nby two independent groups and find that they have little impact on EGB.",
        "We investigate a difference-of-convex (DC) formulation where the second term\nis allowed to be weakly convex. We examine the precise behavior of a single\niteration of the difference-of-convex algorithm (DCA), providing a tight\ncharacterization of the objective function decrease, distinguishing between six\ndistinct parameter regimes.\n  Our proofs, inspired by the performance estimation framework, are notably\nsimplified compared to related prior research. We subsequently derive sublinear\nconvergence rates for the DCA towards critical points, assuming at least one of\nthe functions is smooth.\n  Additionally, we explore the underexamined equivalence between proximal\ngradient descent (PGD) and DCA iterations, demonstrating how DCA, a\nparameter-free algorithm, without the need for a stepsize, serves as a tool for\nstudying the exact convergence rates of PGD.",
        "Measuring the blood pressure waveform is becoming a more frequently studied\narea. The development of sensor technologies opens many new ways to be able to\nmeasure high-quality signals. The development of such an aim-specific sensor\ncan be time-consuming, expensive, and difficult to test or validate with known\nand consistent waveforms. In this paper, we present an open source blood\npressure waveform simulator with an open source Python validation package to\nreduce development costs for early-stage sensor development and research. The\nsimulator mainly consists of 3D printed parts which technology has become a\nwidely available and cheap solution. The core part of the simulator is a 3D\nprinted cam that can be generated based on real blood pressure waveforms. The\nvalidation framework can create a detailed comparison between the signal\nwaveform used to design the cam and the measured time series from the sensor\nbeing validated. The presented simulator proved to be robust and accurate in\nshort- and long-term use, as it produced the signal waveform consistently and\naccurately. To validate this solution, a 3D force sensor was used, which was\nproven earlier to be able to measure high-quality blood pressure waveforms on\nthe radial artery at the wrist. The results showed high similarity between the\nmeasured and the nominal waveforms, meaning that comparing the normalized\nsignals, the RMSE value ranged from $0.0276 \\pm 0.0047$ to $0.0212 \\pm 0.0023$,\nand the Pearson correlation ranged from $0.9933 \\pm 0.0027$ to $0.9978 \\pm\n0.0005$. Our validation framework is available at\nhttps:\/\/github.com\/repat8\/cam-bpw-sim. Our hardware framework, which allows\nreproduction of the presented solution, is available at\nhttps:\/\/github.com\/repat8\/cam-bpw-sim-hardware. The entire design is an open\nsource project and was developed using free software.",
        "We provide an updated global extraction of the neutron magnetic form factor,\nincluding new extractions from $^3$H-$^3$He comparisons at Jefferson Lab. Our\nnew global fit addresses discrepancies between previous data sets at modest\nmomentum transfer by separating the uncertainties from world data into\nnormalization and uncorrelated uncertainties. We use this updated global fit,\nalong with previous fits for the other form factors, to extract the neutron and\nproton transverse charge and magnetization densities and their uncertainties.",
        "The exponential local-global principle, or Skolem conjecture, says: Suppose\nthat \\(b\\) is a positive integer, and that the sequence \\((u_{n})_{n =\n-\\infty}^{\\infty}\\) is such that every term is in \\(\\mathbb{Z}[1\/b]\\), the\nlinear recurrence \\(u_{n + d} = a_{1}u_{n + d - 1} + \\cdots + a_{d}u_{n}\\)\nholds for all integers \\(n\\), and every root of \\(x^{d} - a_{1}x^{d - 1} -\na_{2}x^{d - 2} - \\cdots - a_{d}\\) is nonzero and simple; then there is no zero\nterm \\(u_{n}\\) if and only if, for some integer \\(m\\) that is larger than \\(1\\)\nand relatively prime to \\(b\\), every term \\(u_{n}\\) is not in\n\\(m\\mathbb{Z}[1\/b]\\).\n  Particular cases of the conjecture are known, but the general conjecture is\nopen. This paper proves some apparently new quadratic and degenerate cubic\ncases of the exponential local-global principle via power residue symbols.\n  This work was presented at the Stellenbosch Number Theory Conference 2025 in\nJanuary 2025 at Stellenbosch University; much of the work was also presented at\nthe 67th Annual Congress of the South African Mathematical Society in December\n2024 at the University of Pretoria.",
        "We calculate the temperature dependence of bottomonium correlators in\n(2+1)-flavor lattice QCD with the aim to constrain in-medium properties of\nbottomonia at high temperature. The lattice calculations are performed using\nHISQ action with physical strange quark mass and light quark masses twenty\ntimes smaller than the strange quark mass at two lattice spacings $a=0.0493$ fm\nand $0.0602$ fm, and temporal extents $N_{\\tau}=16-30$, corresponding to the\ntemperatures $T=133-250$ MeV. We use a tadpole-improved NRQCD action including\nspin-dependent $v^6$ corrections for the heavy quarks and extended meson\noperators in order to be sensitive to in-medium properties of the bottomonium\nstates of interest. We find that within estimated errors the bottomonium masses\ndo not change compared to their vacuum values for all temperatures under our\nconsideration; however, we find different nonzero widths for the various\nbottomonium states.",
        "Convergent Cross Mapping (CCM) is a powerful method for detecting causality\nin coupled nonlinear dynamical systems, providing a model-free approach to\ncapture dynamic causal interactions. Partial Cross Mapping (PCM) was introduced\nas an extension of CCM to address indirect causality in three-variable systems\nby comparing cross-mapping quality between direct cause-effect mapping and\nindirect mapping through an intermediate conditioning variable. However, PCM\nremains limited to univariate delay embeddings in its cross-mapping processes.\nIn this work, we extend PCM to the multivariate setting, introducing multiPCM,\nwhich leverages multivariate embeddings to more effectively distinguish\nindirect causal relationships. We further propose a multivariate cross-mapping\nframework (MXMap) for causal discovery in dynamical systems. This two-phase\nframework combines (1) pairwise CCM tests to establish an initial causal graph\nand (2) multiPCM to refine the graph by pruning indirect causal connections.\nThrough experiments on simulated data and the ERA5 Reanalysis weather dataset,\nwe demonstrate the effectiveness of MXMap. Additionally, MXMap is compared\nagainst several baseline methods, showing advantages in accuracy and causal\ngraph refinement.",
        "This research investigates the adsorption characteristics of carbon nanobelts\n(CNB) and Mobius carbon nanobelts (MCNB) interacting with various greenhouse\ngases, including NH3, CO2, CO, H2S, CH4, CH3OH, NO2, NO, and COCl2. The study\nemploys semi-empirical tight-binding calculations via xTB software,\ncomplemented by topological analysis using MULTIWFN software. Comparative\nanalysis reveals MCNB's superior adsorption properties, particularly for\nspecific gases. Notable adsorption energies for MCNB were measured at -1.595eV,\n-0.669eV, and -0.637eV for NO, COCl2, and NO2, respectively, significantly\nexceeding the corresponding CNB values of -0.636eV, -0.449eV, and -0.438eV. The\ninvestigation of desorption kinetics demonstrates rapid recovery times\n(sub-millisecond) for most gas-nanobelt interactions, with the notable\nexception of the MCNB+NO system, which exhibits persistent bonding. Topological\nanalysis confirms chemisorption mechanisms for NO, COCl2, and NO2 on both\nnanobelt variants, characterized by complex hybridizations of covalent and\nnon-covalent interactions. Molecular dynamics simulations conducted in both\npacked configurations and dry air mixtures demonstrate the nanobelts' effective\ngas-attracting properties, maintaining consistent capture performance across\ndifferent environmental conditions. These findings establish carbon nanobelts,\nparticularly the Mobius configuration, as promising candidates for greenhouse\ngas capture technologies, offering potential applications in environmental\nremediation and climate change mitigation strategies.",
        "Altermagnetism has been recently experimentally verified by photoemission\nmapping of the spin order in momentum space in MnTe and CrSb, which feature two\nanisotropic sublattices with antiparallel magnetic dipole moments. In this\nwork, we explicitly demonstrate the presence of an even-parity ferroically\nordered non-dipolar spin density on the atomic sites, i.e. atomic\naltermagnetism, in MnTe, $La_2O_3Mn_2Se_2$ and $Ba_2CaOsO_6$. We do so through\nspin-symmetry analysis and partial-wave decomposition of the spin density\nobtained by first-principles calculations. In MnTe we show a ferroically\nordered g-wave form factor in the spin density around the Mn site. In the\n$A_2O_3M_2Se_2$ family (A= La, Sr and M= Mn, Fe, Co), we show that there is a\nferroically ordered d-wave form factor coexisting with the antiferroic magnetic\ndipoles in the M site, while the O site shows no dipole but a pure d-wave\natomic spin density. In the Mott-insulating candidate $Ba_2CaOsO_6$, as a key\nresult, we reveal a pure form of atomic altermagnetism - absent of any dipolar\nsublattice order. This highlights that the altermagnetic order can exist\nwithout a N\\'eel vector formed by antiferroic dipole moments on an even number\nof crystal sublattices, underlining its distinction from collinear N\\'eel\nantiferromagnetic order. Our calculations predict that $La_2O_3Mn_2Se_2$ and\n$Ba_2CaOsO_6$ can exhibit giant spin-splitter angles of up to 42{\\deg} and\n26{\\deg} respectively, thus demonstrating the possibility of large\naltermagnetic responses without requiring the staggered N\\'eel order of local\ndipole moments.",
        "We investigate some weighted integer partitions whose generating functions\nare double-series. We will establish closed formulas for these $q$-double\nseries and deduce that their coefficients are non-negative. This leads to\ninequalities among integer partitions.",
        "Recent studies of QFT in cosmological spacetime indicate that the speeding up\nof the present universe may not just be associated with a rigid cosmological\nterm but with a running one that evolves with the expansion rate:\n$\\Lambda=\\Lambda(H)$. This running is inherited from the cosmic evolution of\nthe vacuum energy density (VED), $\\rho_{\\rm vac}$, which is sensitive to\nquantum effects in curved spacetime that ultimately trigger that running. The\nVED is a function of the Hubble rate and its time derivatives: $\\rho_{\\rm\nvac}=\\rho_{\\rm vac}(H, \\dot{H},\\ddot{H},...)$. Two nearby points of the cosmic\nevolution during the FLRW epoch are smoothly related as $\\delta\\rho_{\\rm\nvac}\\sim {\\cal O}(H^2)$. In the very early universe, in contrast, the higher\npowers of the Hubble rate take over and bring about a period of fast inflation.\nThey originate from quantum effects on the effective action of vacuum, which we\ncompute. Herein we focus on the lowest possible power for inflation to occur:\n$H^4$. During the inflationary phase, $H$ remains approximately constant and\nvery large. Subsequently, the universe enters the usual FLRW radiation epoch.\nThis new mechanism (`RVM-inflation') is not based on any supplementary\n`inflaton' field, it is fueled by pure QFT effects on the dynamical background\nand is different from Starobinsky's inflation, in which $H$ is never constant.",
        "Stars and planets in close systems are magnetised but the influence of\nmagnetic fields on their tidal responses (and vice versa) and dissipation rates\nhas not been well explored. We present exploratory nonlinear\nmagnetohydrodynamical (MHD) simulations of tidally-excited inertial waves in\nconvective envelopes. These waves probably provide the dominant contribution to\ntidal dissipation in several astrophysical settings, including tidal\ncircularisation of solar-type binary stars and hot Jupiters, and orbital\nmigration of the moons of Jupiter and Saturn. We model convective envelopes as\nincompressible magnetised fluids in spherical shells harbouring an initially\n(rotationally-aligned) dipolar magnetic field. We find that depending on its\nstrength (quantified by its Lehnert number Le) and the magnetic Prandtl number\nPm, the magnetic field can either deeply modify the tidal response or be\nsubstantially altered by tidal flows. Simulations with small Le exhibit strong\ntidally-generated differential rotation (zonal flows) for sufficiently large\ntidal amplitudes, such that both the amplitude and topology of the initial\nmagnetic field are tidally impacted. In contrast, strong magnetic fields can\ninhibit these zonal flows through large-scale magnetic torques, and by Maxwell\nstresses arising from magneto-rotational instability, which we identify and\ncharacterise in our simulations, along with the role of torsional Alfv\\'en\nwaves. Without tidally-driven zonal flows, the resulting tidal dissipation is\nclose to the linear predictions. We quantify the transition Le as a function of\nPm, finding it to be comparable to realistic values found in solar-like stars,\nsuch that we predict complex interactions between tidal flows and magnetic\nfields.",
        "When cosmic strings are formed during inflation, they regrow to reach a\nscaling regime, leaving distinct imprints on the stochastic gravitational wave\nbackground (SGWB). Such signatures, associated with specific primordial\nfeatures, can be detected by upcoming gravitational wave observatories, such as\nthe LISA and Einstein Telescope (ET). Our analysis explores scenarios in which\ncosmic strings form either before or during inflation. We examine how the\nnumber of e-folds experienced by cosmic strings during inflation correlates\nwith the predictions of inflationary models observable in cosmic microwave\nbackground (CMB) measurements. This correlation provides a testable link\nbetween inflationary physics and the associated gravitational wave signals in a\ncomplementary manner. Focusing on $\\alpha$-attractor models of inflation, with\nthe Polynomial $\\alpha$-attractor serving as an illustrative example, we find\nconstraints, for instance, on the spectral index $n_s$ to $0.962 \\lesssim n_s\n\\lesssim 0.972$ for polynomial exponent $n=1$, $0.956 \\lesssim n_s \\lesssim\n0.968$ for $n=2$, $0.954 \\lesssim n_s \\lesssim 0.965$ for $n=3$, and $0.963\n\\lesssim n_s \\lesssim 0.964$ for $n=4$, which along with the GW signals from\nLISA, are capable of detecting local cosmic strings that have experienced $\\sim\n34 - 47$ e-folds of inflation consistent with current Planck data and are also\ntestable in upcoming CMB experiments such as LiteBIRD and CMB-S4.",
        "Two-source extractors aim to extract randomness from two independent sources\nof weak randomness. It has been shown that any two-source extractor which is\nsecure against classical side information remains secure against quantum side\ninformation. Unfortunately, this generic reduction comes with a significant\npenalty to the performance of the extractor. In this paper, we show that the\ntwo-source extractor from Dodis et al. performs equally well against quantum\nside information as in the classical realm, surpassing previously known results\nabout this extractor. Additionally, we derive a new quantum XOR-Lemma which\nallows us to re-derive the generic reduction but also allows for improvements\nfor a large class of extractors.",
        "Exceptional points (EPs) are prominent non-Hermitian band degeneracies that\ngive rise to a variety of intriguing and unconventional phenomena. Similar to\nWeyl and Dirac points, EPs carry topological charges and comply with the\ncelebrated fermion doubling theorems in lattices. Beyond these characteristics,\nEPs exhibit more exotic topological properties, particularly non-Abelian\nbraiding topologies not seen in conventional degeneracies. Here, we investigate\nthese foundational concepts of EPs in two-dimensional non-Hermitian lattices\nwhere the fundamental domain of the Brillouin zone is a Klein bottle, rather\nthan a torus assumed in previous studies. We find that EPs do not necessarily\nappear in pairs with opposite topological charges in the Brillouin Klein\nbottle, thus violating the fermion doubling theorem. The violation occurs\nbecause, without crossing the boundary, the sum of the topological charges of\nEPs is in fact an even number rather than zero. Moreover, we uncover unique\nbraiding topologies of EPs that cannot be captured by existing theories.\nSpecifically, the composite braidings around all EPs equals the braiding along\nthe boundary of the Brillouin Klein bottle. This novel braiding topology\nfurther confirms the failure of the fermion doubling theorem, and allows us to\nexplore the non-Abelian braidings of EPs beyond the scope of topological\ncharges. Our work highlights the fundamental role of Brillouin-zone topology in\nnon-Hermitian systems.",
        "An improved time delay estimation method is used to calculate the velocity of\ncross-field blob motion in the scrape-off layer of Alcator C-Mod for an ohmic\nand two high confinement (H-mode) plasmas; an edge localized mode free and an\nenhanced D-alpha H-mode. The gas puff imaging data analysis results are\ninterpreted in the framework of a stochastic model that describes the\nfluctuations as a super-position of uncorrelated blob-like structures. In all\nconfinement modes investigated, the scrape-off layer is dominated by large\namplitude, blob-like filaments moving radially outwards with velocities in the\nrange from 400 to 1000 m\/s. Blobs in high confinement mode plasmas have similar\nvelocities and sizes as in ohmic plasma, which is consistent with the close\nsimilarity of conditionally averaged burst shapes and frequency spectra for the\nconfinement modes investigated.",
        "This paper studies power-efficient uplink transmission design for federated\nlearning (FL) that employs over-the-air analog aggregation and multi-antenna\nbeamforming at the server. We jointly optimize device transmit weights and\nreceive beamforming at each FL communication round to minimize the total device\ntransmit power while ensuring convergence in FL training. Through our\nconvergence analysis, we establish sufficient conditions on the aggregation\nerror to guarantee FL training convergence. Utilizing these conditions, we\nreformulate the power minimization problem into a unique bi-convex structure\nthat contains a transmit beamforming optimization subproblem and a receive\nbeamforming feasibility subproblem. Despite this unconventional structure, we\npropose a novel alternating optimization approach that guarantees monotonic\ndecrease of the objective value, to allow convergence to a partial optimum. We\nfurther consider imperfect channel state information (CSI), which requires\naccounting for the channel estimation errors in the power minimization problem\nand FL convergence analysis. We propose a CSI-error-aware joint beamforming\nalgorithm, which can substantially outperform one that does not account for\nchannel estimation errors. Simulation with canonical classification datasets\ndemonstrates that our proposed methods achieve significant power reduction\ncompared to existing benchmarks across a wide range of parameter settings,\nwhile attaining the same target accuracy under the same convergence rate.",
        "The performance of Reed-Solomon codes (RS codes, for short) in the presence\nof insertion and deletion errors has been studied recently in several papers.\nIn this work, we further study this intriguing mathematical problem, focusing\non two regimes. First, we study the question of how well full-length RS codes\nperform against insertions and deletions. For 2-dimensional RS codes, we fully\ncharacterize which codes cannot correct even a single insertion or deletion and\nshow that (almost) all 2-dimensional RS codes correct at least $1$ insertion or\ndeletion error. Moreover, for large enough field size $q$, and for any $k \\ge\n2$, we show that there exists a full-length $k$-dimensional RS code that\ncorrects $q\/10k$ insertions and deletions. Second, we focus on rate $1\/2$ RS\ncodes that can correct a single insertion or deletion error. We present a\npolynomial time algorithm that constructs such codes for $q = O(k^4)$. This\nresult matches the existential bound given in \\cite{con2023reed}.",
        "The standard formula, due to Spiegel, for the smoothing of temperature\nfluctuations by radiative transfer is unstable in relativity. This is due to\nthe fact that Spiegel neglected the transit time of light, thereby allowing the\ntransport coefficients to move outside the convex geometry compatible with\ncausality (the \"hydrohedron\"). Here, we fix this pathology. First, we prove\nthat the linearized radiative transfer equations are causal and covariantly\nstable by construction. Then, we repeat Spiegel's calculation accounting for\nthe finite speed of photons. We find that the full transfer problem can be\nsolved analytically. All the infinite (exact) transport coefficients arising\nfrom it fall inside the hydrohedron. Our analysis also accounts for isotropic\nscattering.",
        "Accurate wind speed prediction is crucial for designing and selecting sites\nfor offshore wind farms. This paper investigates the effectiveness of various\nmachine learning models in predicting offshore wind power for a site near the\nGulf of Mexico by analyzing meteorological data. After collecting and\npreprocessing meteorological data, nine different input feature combinations\nwere designed to assess their impact on wind power predictions at multiple\nheights. The results show that using wind speed as the output feature improves\nprediction accuracy by approximately 10% compared to using wind power as the\noutput. In addition, the improvement of multi-feature input compared with\nsingle-feature input is not obvious mainly due to the poor correlation among\nkey features and limited generalization ability of models. These findings\nunderscore the importance of selecting appropriate output features and\nhighlight considerations for using machine learning in wind power forecasting,\noffering insights that could guide future wind power prediction models and\nconversion techniques.",
        "We deduce the quantum mechanical prediction of $-{\\bf a}\\cdot{\\bf b}$ for the\nsinglet spin state employing local measurement functions following Bell's\napproach. Our derivation is corroborated through a computational simulation\nconducted via the Mathematica programming environment.",
        "We investigate the phase equilibrium problem for multicomponent mixtures\nunder specified internal energy (U), volume (V), and mole numbers (N1,N2, . . .\n,Nn), commonly known as the UVN-flash problem. While conventional phase\nequilibrium calculations typically use pressure-temperature-mole number (PTN)\nspecifications, the UVN formulation is essential for dynamic simulations of\nclosed systems and energy balance computations. Existing approaches, including\nthose based on iterative pressure-temperature updates and direct entropy\nmaximization, suffer from computational inefficiencies due to nested iterations\nand reliance on inner Newton solvers. In this work, we present a novel\nreformulation of the UVN-flash problem as a direct entropy maximization problem\nthat eliminates the need for inner Newton iterations, addressing key\ncomputational bottlenecks. We derive two new novel formulations: 1) a\nformulation based on entropy and internal energy and (2) an alternative\nformulation based on Helmholtz free energy. We begin with a stability analysis\nframework, followed by a reformulation of the UVN flash problem in natural\nvariables. We then introduce our novel approach and discuss the numerical\nmethods used, including gradient and Hessian computations. The proposed method\nis validated against benchmark cases, demonstrating improved efficiency and\nrobustness."
      ]
    }
  },
  {
    "id":2412.10196,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Quality assurance procedures for mass spectrometry untargeted metabolomics. a review",
    "start_abstract":"Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "XGBoost: A Scalable Tree Boosting System"
      ],
      "abstract":[
        "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Target Selection for the Redshift-Limited WAVES-Wide with Machine\n  Learning",
        "Auxiliary Discrminator Sequence Generative Adversarial Networks\n  (ADSeqGAN) for Few Sample Molecule Generation",
        "Competing Effects of Local Solvation Structures on Chemical Shift\n  Changes of Liquid Electrolyte",
        "Diffusion Models for Cayley Graphs",
        "Funzac at CoMeDi Shared Task: Modeling Annotator Disagreement from\n  Word-In-Context Perspectives",
        "A Comprehensive Reanalysis of K2-18 b's JWST NIRISS+NIRSpec Transmission\n  Spectrum",
        "Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles",
        "Equilibrium Moment Analysis of It\\^o SDEs",
        "NaFM: Pre-training a Foundation Model for Small-Molecule Natural\n  Products",
        "DAST: Difficulty-Adaptive Slow-Thinking for Large Reasoning Models",
        "Hypersurfaces passing through the Galois orbit of a point",
        "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs",
        "Optimal joint reconstruction from CMB observations: application to\n  cosmic birefringence, patchy reionization and CMB lensing",
        "Wireless Network Topology Inference: A Markov Chains Approach",
        "Learning the RoPEs: Better 2D and 3D Position Encodings with STRING",
        "Specific Aspects of Intellectual Property Management in the\n  Knowledge-Based Economy",
        "Challenges and Innovations in LLM-Powered Fake News Detection: A\n  Synthesis of Approaches and Future Directions",
        "GSBA$^K$: $top$-$K$ Geometric Score-based Black-box Attack",
        "Reducing Hallucinations in Language Model-based SPARQL Query Generation\n  Using Post-Generation Memory Retrieval",
        "Monolayer transition metal dichalcogenides under finite-pulse polarized\n  radiation",
        "A Fair Federated Learning Framework for Collaborative Network Traffic\n  Prediction and Resource Allocation",
        "Highly correlated electronic state in a ferrimagnetic quadruple\n  perovskite CuCu$_3$Fe$_2$Re$_2$O$_{12}$",
        "Large Model Empowered Metaverse: State-of-the-Art, Challenges and\n  Opportunities",
        "Probing muonic force with periastron advance in binary pulsar systems",
        "$\\Lambda$(1405) in the flavor SU(3) limit using a separable potential in\n  the HAL QCD method",
        "Wettability and sp2\/sp3 ratio effects on supercapacitor performance of\n  N-doped hydrogenated amorphous Carbon Nanofoam",
        "Kinks of fractional $\\phi^4$ models: existence, uniqueness,\n  monotonicity, stability, and sharp asymptotics",
        "Inter3D: A Benchmark and Strong Baseline for Human-Interactive 3D Object\n  Reconstruction",
        "Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented\n  Conversational AI Through Accountability Modeling"
      ],
      "abstract":[
        "The forthcoming Wide Area Vista Extragalactic Survey (WAVES) on the 4-metre\nMulti-Object Spectroscopic Telescope (4MOST) has a key science goal of probing\nthe halo mass function to lower limits than possible with previous surveys. For\nthat purpose, in its Wide component, galaxies targetted by WAVES will be\nflux-limited to $Z<21.1$ mag and will cover the redshift range of $z<0.2$, at a\nspectroscopic success rate of $\\sim95\\%$. Meeting this completeness\nrequirement, when the redshift is unknown a priori, is a challenge. We solve\nthis problem with supervised machine learning to predict the probability of a\ngalaxy falling within the WAVES-Wide redshift limit, rather than estimate each\nobject's redshift. This is done by training an XGBoost tree-based classifier to\ndecide if a galaxy should be a target or not. Our photometric data come from\n9-band VST+VISTA observations, including KiDS+VIKING surveys. The redshift\nlabels for calibration are derived from an extensive spectroscopic sample\noverlapping with KiDS and ancillary fields. Our current results indicate that\nwith our approach, we should be able to achieve the completeness of $\\sim95\\%$,\nwhich is the WAVES success criterion.",
        "In this work, we introduce Auxiliary Discriminator Sequence Generative\nAdversarial Networks (ADSeqGAN), a novel approach for molecular generation in\nsmall-sample datasets. Traditional generative models often struggle with\nlimited training data, particularly in drug discovery, where molecular datasets\nfor specific therapeutic targets, such as nucleic acids binders and central\nnervous system (CNS) drugs, are scarce. ADSeqGAN addresses this challenge by\nintegrating an auxiliary random forest classifier as an additional\ndiscriminator into the GAN framework, significantly improves molecular\ngeneration quality and class specificity.\n  Our method incorporates pretrained generator and Wasserstein distance to\nenhance training stability and diversity. We evaluate ADSeqGAN on a dataset\ncomprising nucleic acid-targeting and protein-targeting small molecules,\ndemonstrating its superior ability to generate nucleic acid binders compared to\nbaseline models such as SeqGAN, ORGAN, and MolGPT. Through an oversampling\nstrategy, ADSeqGAN also significantly improves CNS drug generation, achieving a\nhigher yield than traditional de novo models. Critical assessments, including\ndocking simulations and molecular property analysis, confirm that\nADSeqGAN-generated molecules exhibit strong binding affinities, enhanced\nchemical diversity, and improved synthetic feasibility.\n  Overall, ADSeqGAN presents a novel framework for generative molecular design\nin data-scarce scenarios, offering potential applications in computational drug\ndiscovery. We have demonstrated the successful applications of ADSeqGAN in\ngenerating synthetic nucleic acid-targeting and CNS drugs in this work.",
        "Understanding the solvation structure of electrolytes is critical for\noptimizing the electrochemical performance of rechargeable batteries, as it\ndirectly influences properties such as ionic conductivity, viscosity, and\nelectrochemical stability. The highly complex structures and strong\ninteractions in high-concentration electrolytes make accurate modeling and\ninterpretation of their ``structure-property\" relationships even more\nchallenging with spectroscopic methods. In this study, we present a machine\nlearning-based approach to predict dynamic $^7$Li NMR chemical shifts in\nLiFSI\/DME electrolyte solutions. Additionally, we provide a comprehensive\nstructural analysis to interpret the observed chemical shift behavior in our\nexperiments, particularly the abrupt changes in $^7$Li chemical shifts at high\nconcentrations. Using advanced modeling techniques, we quantitatively establish\nthe relationship between molecular structure and NMR spectra, offering critical\ninsights into solvation structure assignments. Our findings reveal the\ncoexistence of two competing local solvation structures that shift in dominance\nas electrolyte concentration approaches the concentrated limit, leading to\nanomalous reverse of $^7$Li NMR chemical shift in our experiment. This work\nprovides a detailed molecular-level understanding of the intricate solvation\nstructures probed by NMR spectroscopy, leading the way for enhanced electrolyte\ndesign.",
        "We review the problem of finding paths in Cayley graphs of groups and group\nactions, using the Rubik's cube as an example, and we list several more\nexamples of significant mathematical interest. We then show how to formulate\nthese problems in the framework of diffusion models. The exploration of the\ngraph is carried out by the forward process, while finding the target nodes is\ndone by the inverse backward process. This systematizes the discussion and\nsuggests many generalizations. To improve exploration, we propose a ``reversed\nscore'' ansatz which substantially improves over previous comparable\nalgorithms.",
        "In this work, we evaluate annotator disagreement in Word-in-Context (WiC)\ntasks exploring the relationship between contextual meaning and disagreement as\npart of the CoMeDi shared task competition. While prior studies have modeled\ndisagreement by analyzing annotator attributes with single-sentence inputs,\nthis shared task incorporates WiC to bridge the gap between sentence-level\nsemantic representation and annotator judgment variability. We describe three\ndifferent methods that we developed for the shared task, including a feature\nenrichment approach that combines concatenation, element-wise differences,\nproducts, and cosine similarity, Euclidean and Manhattan distances to extend\ncontextual embedding representations, a transformation by Adapter blocks to\nobtain task-specific representations of contextual embeddings, and classifiers\nof varying complexities, including ensembles. The comparison of our methods\ndemonstrates improved performance for methods that include enriched and\ntask-specfic features. While the performance of our method falls short in\ncomparison to the best system in subtask 1 (OGWiC), it is competitive to the\nofficial evaluation results in subtask 2 (DisWiC).",
        "Sub-Neptunes are the most common type of planet in our galaxy. Interior\nstructure models suggest that the coldest sub-Neptunes could host liquid water\noceans underneath their hydrogen envelopes - sometimes called 'hycean' planets.\nJWST transmission spectra of the $\\sim$ 250 K sub-Neptune K2-18 b were recently\nused to report detections of CH$_4$ and CO$_2$, alongside weaker evidence of\n(CH$_3$)$_2$S (dimethyl sulfide, or DMS). Atmospheric CO$_2$ was interpreted as\nevidence for a liquid water ocean, while DMS was highlighted as a potential\nbiomarker. However, these notable claims were derived using a single data\nreduction and retrieval modeling framework, which did not allow for standard\nrobustness tests. Here we present a comprehensive reanalysis of K2-18 b's JWST\nNIRISS SOSS and NIRSpec G395H transmission spectra, including the first\nanalysis of the second-order NIRISS SOSS data. We incorporate multiple\nwell-tested data reduction pipelines and retrieval codes, spanning 60 different\ndata treatments and over 250 atmospheric retrievals. We confirm the detection\nof CH$_4$ ($\\approx$ 4$\\sigma$), with a volume mixing ratio of log CH$_4$ =\n$-1.15^{+0.40}_{-0.52}$, but we find no statistically significant or reliable\nevidence for CO$_2$ or DMS. Finally, we quantify the observed atmospheric\ncomposition using photochemical-climate and interior models, demonstrating that\nour revised composition of K2-18 b can be explained by an oxygen-poor\nmini-Neptune without requiring a liquid water surface or life.",
        "User simulators are crucial for replicating human interactions with dialogue\nsystems, supporting both collaborative training and automatic evaluation,\nespecially for large language models (LLMs). However, existing simulators often\nrely solely on text utterances, missing implicit user traits such as\npersonality, speaking style, and goals. In contrast, persona-based methods lack\ngeneralizability, as they depend on predefined profiles of famous individuals\nor archetypes. To address these challenges, we propose User Simulator with\nimplicit Profiles (USP), a framework that infers implicit user profiles from\nhuman-machine conversations and uses them to generate more personalized and\nrealistic dialogues. We first develop an LLM-driven extractor with a\ncomprehensive profile schema. Then, we refine the simulation through\nconditional supervised fine-tuning and reinforcement learning with cycle\nconsistency, optimizing it at both the utterance and conversation levels.\nFinally, we adopt a diverse profile sampler to capture the distribution of\nreal-world user profiles. Experimental results demonstrate that USP outperforms\nstrong baselines in terms of authenticity and diversity while achieving\ncomparable performance in consistency. Furthermore, dynamic multi-turn\nevaluations based on USP strongly align with mainstream benchmarks,\ndemonstrating its effectiveness in real-world applications.",
        "Stochastic differential equations have proved to be a valuable governing\nframework for many real-world systems which exhibit ``noise'' or randomness in\ntheir evolution. One quality of interest in such systems is the shape of their\nequilibrium probability distribution, if such a thing exists. In some cases a\nstraightforward integral equation may yield this steady-state distribution, but\nin other cases the equilibrium distribution exists and yet that integral\nequation diverges. Here we establish a new equilibrium-analysis technique based\non the logic of finite-timestep simulation which allows us to glean information\nabout the equilibrium regardless -- in particular, a relationship between the\nraw moments of the equilibrium distribution. We utilize this technique to\nextract information about one such equilibrium resistant to direct definition.",
        "Natural products, as metabolites from microorganisms, animals, or plants,\nexhibit diverse biological activities, making them crucial for drug discovery.\nNowadays, existing deep learning methods for natural products research\nprimarily rely on supervised learning approaches designed for specific\ndownstream tasks. However, such one-model-for-a-task paradigm often lacks\ngeneralizability and leaves significant room for performance improvement.\nAdditionally, existing molecular characterization methods are not well-suited\nfor the unique tasks associated with natural products. To address these\nlimitations, we have pre-trained a foundation model for natural products based\non their unique properties. Our approach employs a novel pretraining strategy\nthat is especially tailored to natural products. By incorporating contrastive\nlearning and masked graph learning objectives, we emphasize evolutional\ninformation from molecular scaffolds while capturing side-chain information.\nOur framework achieves state-of-the-art (SOTA) results in various downstream\ntasks related to natural product mining and drug discovery. We first compare\ntaxonomy classification with synthesized molecule-focused baselines to\ndemonstrate that current models are inadequate for understanding natural\nsynthesis. Furthermore, by diving into a fine-grained analysis at both the gene\nand microbial levels, NaFM demonstrates the ability to capture evolutionary\ninformation. Eventually, our method is experimented with virtual screening,\nillustrating informative natural product representations that can lead to more\neffective identification of potential drug candidates.",
        "Recent advancements in slow-thinking reasoning models have shown exceptional\nperformance in complex reasoning tasks. However, these models often exhibit\noverthinking-generating redundant reasoning steps for simple problems, leading\nto excessive computational resource usage. While current mitigation strategies\nuniformly reduce reasoning tokens, they risk degrading performance on\nchallenging tasks that require extended reasoning. This paper introduces\nDifficulty-Adaptive Slow-Thinking (DAST), a novel framework that enables models\nto autonomously adjust the length of Chain-of-Thought(CoT) based on problem\ndifficulty. We first propose a Token Length Budget (TLB) metric to quantify\ndifficulty, then leveraging length-aware reward shaping and length preference\noptimization to implement DAST. DAST penalizes overlong responses for simple\ntasks while incentivizing sufficient reasoning for complex problems.\nExperiments on diverse datasets and model scales demonstrate that DAST\neffectively mitigates overthinking (reducing token usage by over 30\\% on\naverage) while preserving reasoning accuracy on complex problems.",
        "Asgarli, Ghioca, and Reichstein recently proved that if $K$ is a field with\n$|K|>2$, then for any positive integers $d$ and $n$, and separable field\nextension $L\/K$ with degree $m=\\binom{n+d}{d}$, there exists a point $P\\in\n\\mathbb{P}^n(L)$ which does not lie on any degree $d$ hypersurface defined over\n$K$. They asked whether the result holds when $|K| = 2$. We answer their\nquestion in the affirmative by combining various ideas from arithmetic\ngeometry. More generally, we show that for each positive integer $r$ and\nseparable field extension $L\/K$ with degree $r$, there exists a point $P \\in\n\\mathbb{P}^n(L)$ such that the vector space of degree $d$ forms over $K$ that\nvanish at $P$ has the expected dimension. We also discuss applications to\nlinear systems of hypersurfaces with special properties.",
        "Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or\nnonsensical information) when serving as AI assistants in various domains.\nSince hallucinations always come with truthful content in the LLM responses,\nprevious factuality alignment methods that conduct response-level preference\nlearning inevitably introduced noises during training. Therefore, this paper\nproposes a fine-grained factuality alignment method based on Direct Preference\nOptimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as\nmask signals, Mask-DPO only learns from factually correct sentences in the\npreferred samples and prevents the penalty on factual contents in the not\npreferred samples, which resolves the ambiguity in the preference learning.\nExtensive experimental results demonstrate that Mask-DPO can significantly\nimprove the factuality of LLMs responses to questions from both in-domain and\nout-of-domain datasets, although these questions and their corresponding topics\nare unseen during training. Only trained on the ANAH train set, the score of\nLlama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%,\neven surpassing the score of Llama3.1-70B-Instruct (53.44%), while its\nFactScore on the out-of-domain Biography dataset is also improved from 30.29%\nto 39.39%. We further study the generalization property of Mask-DPO using\ndifferent training sample scaling strategies and find that scaling the number\nof topics in the dataset is more effective than the number of questions. We\nprovide a hypothesis of what factual alignment is doing with LLMs, on the\nimplication of this phenomenon, and conduct proof-of-concept experiments to\nverify it. We hope the method and the findings pave the way for future research\non scaling factuality alignment.",
        "Line-of-sight distortions of the cosmic microwave background (CMB), including\ngravitational lensing, cosmic birefringence, and patchy screening, encode\ncrucial cosmological information. While quadratic estimators (QE) have been\nexcellent tools for extracting these signals, they become suboptimal for\ncurrent- and next-generation CMB surveys, failing to maximize signal-to-noise\nand suffering from bias contamination that standard bias-hardening techniques\ncannot mitigate. We present a joint maximum a posteriori framework that\nsimultaneously reconstructs multiple distortion fields while explicitly\naccounting for their mutual contamination. For cosmic birefringence searches,\nour method achieves up to 2.5 improvement in reconstruction noise compared to\nthe QE, while significantly reducing CMB lensing-induced biases up to a factor\nof 5. These gains in lensing biases manifest not only in deep polarization\nsurveys like CMB-S4 and SPT-3G, but also in higher-noise experiments like\nSimons Observatory, where our method reduces lensing-induced biases by a factor\nof two thanks to the power of delensing. Our code provides a first step to\nrobust analyses of CMB secondary anisotropies, their cross-correlations with\nlarge-scale structure, and ultimately enabling more sensitive searches for\nprimordial $B$-modes.",
        "In this work, we address the problem of inferring the topology of a wireless\nnetwork using limited observational data. Specifically, we assume that we can\ndetect when a node is transmitting, but no further information regarding the\ntransmission is available. We propose a novel network estimation procedure\ngrounded in the following abstract problem: estimating the parameters of a\nfinite discrete-time Markov chain by observing, at each time step, which states\nare visited by multiple ``anonymous'' copies of the chain. We develop a\nconsistent estimator that approximates the transition matrix of the chain in\nthe operator norm, with the number of required samples scaling roughly linearly\nwith the size of the state space. Applying this estimation procedure to\nwireless networks, our numerical experiments demonstrate that the proposed\nmethod accurately infers network topology across a wide range of parameters,\nconsistently outperforming transfer entropy, particularly under conditions of\nhigh network congestion.",
        "We introduce STRING: Separable Translationally Invariant Position Encodings.\nSTRING extends Rotary Position Encodings, a recently proposed and widely used\nalgorithm in large language models, via a unifying theoretical framework.\nImportantly, STRING still provides exact translation invariance, including\ntoken coordinates of arbitrary dimensionality, whilst maintaining a low\ncomputational footprint. These properties are especially important in robotics,\nwhere efficient 3D token representation is key. We integrate STRING into Vision\nTransformers with RGB(-D) inputs (color plus optional depth), showing\nsubstantial gains, e.g. in open-vocabulary object detection and for robotics\ncontrollers. We complement our experiments with a rigorous mathematical\nanalysis, proving the universality of our methods.",
        "This paper addresses the issue of intellectual property management in the\nknowledge-based economy. The starting point in carrying out the study is the\npresentation of some concepts regarding in the first phase, the intellectual\ncapital. Arguments are made that the knowledge-based economy is a challenge for\nthe current century. The subject of intellectual property is approached through\nthe prism of a topical concept operationalized in the current global economic\ncontext. The main institutions that are directly related to this concept are\nmentioned. The topic of patents related to WOS indexed scientific papers is\nalso debated, along with a series of statistics and studies on the state of\npatent protection worldwide in the top fields. The last part of the paper\ncontains the conclusions and own points of view on the debated topic.",
        "The pervasiveness of the dissemination of fake news through social media\nplatforms poses critical risks to the trust of the general public, societal\nstability, and democratic institutions. This challenge calls for novel\nmethodologies in detection, which can keep pace with the dynamic and\nmulti-modal nature of misinformation. Recent works include powering the\ndetection using large language model advances in multimodal frameworks,\nmethodologies using graphs, and adversarial training in the literature of fake\nnews. Based on the different approaches which can bring success, some key\nhighlights will be underlined: enhanced LLM-improves accuracy through more\nadvanced semantics and cross-modality fusion for robust detections. The review\nfurther identifies critical gaps in adaptability to dynamic social media\ntrends, real-time, and cross-platform detection capabilities, as well as the\nethical challenges thrown up by the misuse of LLMs. Future directions underline\nthe development of style-agnostic models, cross-lingual detection frameworks,\nand robust policies with a view to mitigating LLM-driven misinformation. This\nsynthesis thus lays a concrete foundation for those researchers and\npractitioners committed to reinforcing fake news detection systems with\ncomplications that keep on growing in the digital landscape.",
        "Existing score-based adversarial attacks mainly focus on crafting $top$-1\nadversarial examples against classifiers with single-label classification.\nTheir attack success rate and query efficiency are often less than\nsatisfactory, particularly under small perturbation requirements; moreover, the\nvulnerability of classifiers with multi-label learning is yet to be studied. In\nthis paper, we propose a comprehensive surrogate free score-based attack, named\n\\b geometric \\b score-based \\b black-box \\b attack (GSBA$^K$), to craft\nadversarial examples in an aggressive $top$-$K$ setting for both untargeted and\ntargeted attacks, where the goal is to change the $top$-$K$ predictions of the\ntarget classifier. We introduce novel gradient-based methods to find a good\ninitial boundary point to attack. Our iterative method employs novel gradient\nestimation techniques, particularly effective in $top$-$K$ setting, on the\ndecision boundary to effectively exploit the geometry of the decision boundary.\nAdditionally, GSBA$^K$ can be used to attack against classifiers with $top$-$K$\nmulti-label learning. Extensive experimental results on ImageNet and PASCAL VOC\ndatasets validate the effectiveness of GSBA$^K$ in crafting $top$-$K$\nadversarial examples.",
        "The ability to generate SPARQL queries from natural language questions is\ncrucial for ensuring efficient and accurate retrieval of structured data from\nknowledge graphs (KG). While large language models (LLMs) have been widely\nadopted for SPARQL query generation, they are often susceptible to\nhallucinations and out-of-distribution errors when producing KG elements like\nUniform Resource Identifiers (URIs) based on internal parametric knowledge.\nThis often results in content that appears plausible but is factually\nincorrect, posing significant challenges for their use in real-world\ninformation retrieval (IR) applications. This has led to increased research\naimed at detecting and mitigating such errors. In this paper, we introduce PGMR\n(Post-Generation Memory Retrieval), a modular framework that incorporates a\nnon-parametric memory module to retrieve KG elements and enhance LLM-based\nSPARQL query generation. Our experimental results indicate that PGMR\nconsistently delivers strong performance across diverse datasets, data\ndistributions, and LLMs. Notably, PGMR significantly mitigates URI\nhallucinations, nearly eliminating the problem in several scenarios.",
        "Recent advances in time-resolved angle-resolved photoemission spectroscopy\nhave enabled access to ultrafast electron states and their spin dynamics in\nsolids. Atomically thin transition metal dichalcogenides are paradigmatic\ntwo-dimensional materials where electron momentum and spin degrees of freedom\nare coupled, being suitable candidates for time-resolved spectroscopy studies.\nIn this work, we present a thorough study of the electron dynamics when these\nmaterials are subject to an intense finite-pulse driving radiation. We extend\nthe scope of the conventional Floquet engineering and rely of the so-called\n$t-t^{\\prime}$ formalism to deal with driving fields described with two\ndistinct time scales, namely the envelope amplitude timescale and the time\nperiod of the external field. The interplay between the finite-pulse timescales\nand the intrinsic properties of the electrons gives rise to transient valley\npolarization and dynamical modifications of band structures, revealed by the\ntime-dependent circular dichroism of the sample.",
        "In the beyond 5G era, AI\/ML empowered realworld digital twins (DTs) will\nenable diverse network operators to collaboratively optimize their networks,\nultimately improving end-user experience. Although centralized AI-based\nlearning techniques have been shown to achieve significant network traffic\naccuracy, resulting in efficient network operations, they require sharing of\nsensitive data among operators, leading to privacy and security concerns.\nDistributed learning, and specifically federated learning (FL), that keeps data\nisolated at local clients, has emerged as an effective and promising solution\nfor mitigating such concerns. Federated learning poses, however, new challenges\nin ensuring fairness both in terms of collaborative training contributions from\nheterogeneous data and in mitigating bias in model predictions with respect to\nsensitive attributes. To address these challenges, a fair FL framework is\nproposed for collaborative network traffic prediction and resource allocation.\nTo demonstrate the effectiveness of the proposed approach, noniid and\nimbalanced federated datasets based on real-word traffic traces are utilized\nfor an elastic optical network. The assumption is that different optical nodes\nmay be managed by different operators. Fairness is evaluated according to the\ncoefficient of variations measure in terms of accuracy across the operators and\nin terms of quality-of-service across the connections (i.e., reflecting\nend-user experience). It is shown that fair traffic prediction across the\noperators result in fairer resource allocations across the connections.",
        "Recently synthesized quadruple perovskite CuCu$_3$Fe$_2$Re$_2$O$_{12}$\npossesses strong ferromagnetism and unusual electron properties, including\nenhanced electronic specific heat. Application of the first principles\nelectronic structure approaches unambiguously shows importance of the many-body\neffects in this compound. While CuCu$_3$Fe$_2$Re$_2$O$_{12}$ is half-metallic\nferrimagnet in the DFT+U method, in the density functional theory (DFT)\ncombined with the dynamical mean-field theory (DMFT) it appears to be a metal.\nStrong correlations lead to a renormalization of electronic spectrum and\nformation of incoherent states close to the Fermi level. Electronic specific\nheat and magnetic properties obtained in the DFT+DMFT approach are in better\nagreement with available experimental data than derived by other band structure\ntechniques.",
        "The Metaverse represents a transformative shift beyond traditional mobile\nInternet, creating an immersive, persistent digital ecosystem where users can\ninteract, socialize, and work within 3D virtual environments. Powered by large\nmodels such as ChatGPT and Sora, the Metaverse benefits from precise\nlarge-scale real-world modeling, automated multimodal content generation,\nrealistic avatars, and seamless natural language understanding, which enhance\nuser engagement and enable more personalized, intuitive interactions. However,\nchallenges remain, including limited scalability, constrained responsiveness,\nand low adaptability in dynamic environments. This paper investigates the\nintegration of large models within the Metaverse, examining their roles in\nenhancing user interaction, perception, content creation, and service quality.\nTo address existing challenges, we propose a generative AI-based framework for\noptimizing Metaverse rendering. This framework includes a cloud-edge-end\ncollaborative model to allocate rendering tasks with minimal latency, a\nmobility-aware pre-rendering mechanism that dynamically adjusts to user\nmovement, and a diffusion model-based adaptive rendering strategy to fine-tune\nvisual details. Experimental results demonstrate the effectiveness of our\napproach in enhancing rendering efficiency and reducing rendering overheads,\nadvancing large model deployment for a more responsive and immersive Metaverse.",
        "Pulsars, highly magnetized, rotating neutron stars, can have significant muon\nabundances in their dense cores, making them promising environments to probe\nultralight mediators coupled to muons. The precise measurement of periastron\nadvance in binary pulsar systems provides a sensitive probe of such long-range\nforces. In this work, we study the periastron advance constraints from binary\npulsar systems on the ultralight muonic mediators. We compute the muon number\nfraction in neutron stars, by properly taking into account the suppression\neffect of the long-range muonic force. We find that the periastron advance\nconstraints impose the most stringent constraints on ultralight muonic\nmediators in the mass range of $\\simeq(10^{-17},\\,2\\times10^{-15})$ eV, probing\nmuonic couplings as small as $\\mathcal{O}(10^{-21})$, which surpass the limits\nfrom LIGO\/Virgo gravitational wave measurements, by about an order of\nmagnitude.",
        "Using the HAL QCD method, we investigate S-wave meson-baryon interactions in\nsinglet and two octet channels in the flavor SU(3) limit, where the chiral\nunitary model predicts that a combination of bound-state poles in these\nchannels corresponds to $\\Lambda(1405)$. To avoid the singular behavior of the\nleading-order potentials of these channels in the derivative expansion, we\ninstead employ a separable potential in the time-dependent HAL QCD method. To\ncalculate all-to-all propagators in the three-point correlation functions\nincluding $\\Lambda$-baryon source operators with zero momentum, we employ the\nconventional stochastic estimation combined with the covariant approximation\naveraging. Separable potentials both in the singlet and octet channels show\nattraction without singular behavior. Our results of the corresponding phase\nshifts indicate that the attractive interaction in the singlet channel is\nstronger than that in the octet. Binding energies are consistent with the\nestimates from the two-point correlation function within one (two) sigma for\nthe singlet (octet) channel, and this ordering of binding energies is\nconsistent with the mass hierarchy suggested by the chiral unitary model.",
        "Pulsed laser-deposited amorphous carbon nanofoams could be a potential\ncandidate for electrochemical energy storage applications due to their\nproperties such as ultralightweight, huge volumetric void fractions, and\nco-existence of sp, sp2 and sp3 carbon hybridization. It is known that the\ncharge-storage of carbon nanostructures containing disordered sp2-domains is\ndetermined by the wettability, surface area, and porosity of carbon\nnanostructures. However, their charge-storage performance is limited to the\nareal capacitance of the order of a few mF\/cm2. We enhanced the supercapacitor\nperformance of nitrogen-doped amorphous carbon nanofoam by engineering its\nwettability and sp2-C\/sp3-C ratio by vacuum annealing. The specific capacitance\nwas enhanced around fifty times and the widened voltage of the device increased\nfrom 0.8 to 1.1 V compared to as-grown nanofoam. In addition, we examined for\nthe first time the initial increase in specific capacitance of the aqueous\nsymmetric supercapacitor with respect to the scan rate, employing in-situ\nmeasurements coupling Raman spectroscopy and electrochemistry. We attribute\nthis effect, although observed in previous literatures but unexplained, to the\nelectrochemical activation induced by structural changes during the charge\nstorage performance. This optimization of pulsed laser-deposited carbon\nnanofoam may open an avenue for fabricating lightweight and porous\nnanostructures for advanced macro-to-micro-supercapacitor devices.",
        "In the present work we construct kink solutions for different (parabolic and\nwave) variants of the fractional $\\phi^4$ model, in both the sub-Laplacian and\nsuper-Laplacian setting. We establish existence and monotonicity results (for\nthe sub - Laplacian case), along with sharp asymptotics which are corroborated\nthrough numerical computations. Importantly, in the sub-Laplacian regime, we\nprovide the explicit and numerically verifiable spectral condition, which\nguarantees uniqueness for odd kinks. We check numerically the relevant\ncondition to confirm the uniqueness of such solutions. In addition, we show\nasymptotic stability for the stationary kinks in the parabolic setting and\nalso, the spectral stability for the traveling kinks in the corresponding wave\nequation.",
        "Recent advancements in implicit 3D reconstruction methods, e.g., neural\nrendering fields and Gaussian splatting, have primarily focused on novel view\nsynthesis of static or dynamic objects with continuous motion states. However,\nthese approaches struggle to efficiently model a human-interactive object with\nn movable parts, requiring 2^n separate models to represent all discrete\nstates. To overcome this limitation, we propose Inter3D, a new benchmark and\napproach for novel state synthesis of human-interactive objects. We introduce a\nself-collected dataset featuring commonly encountered interactive objects and a\nnew evaluation pipeline, where only individual part states are observed during\ntraining, while part combination states remain unseen. We also propose a strong\nbaseline approach that leverages Space Discrepancy Tensors to efficiently\nmodelling all states of an object. To alleviate the impractical constraints on\ncamera trajectories across training states, we propose a Mutual State\nRegularization mechanism to enhance the spatial density consistency of movable\nparts. In addition, we explore two occupancy grid sampling strategies to\nfacilitate training efficiency. We conduct extensive experiments on the\nproposed benchmark, showcasing the challenges of the task and the superiority\nof our approach.",
        "Recent LLMs have enabled significant advancements for conversational agents.\nHowever, they are also well known to hallucinate, producing responses that seem\nplausible but are factually incorrect. On the other hand, users tend to\nover-rely on LLM-based AI agents, accepting AI's suggestion even when it is\nwrong. Adding positive friction, such as explanations or getting user\nconfirmations, has been proposed as a mitigation in AI-supported\ndecision-making systems. In this paper, we propose an accountability model for\nLLM-based task-oriented dialogue agents to address user overreliance via\nfriction turns in cases of model uncertainty and errors associated with\ndialogue state tracking (DST). The accountability model is an augmented LLM\nwith an additional accountability head that functions as a binary classifier to\npredict the relevant slots of the dialogue state mentioned in the conversation.\nWe perform our experiments with multiple backbone LLMs on two established\nbenchmarks (MultiWOZ and Snips). Our empirical findings demonstrate that the\nproposed approach not only enables reliable estimation of AI agent errors but\nalso guides the decoder in generating more accurate actions. We observe around\n3% absolute improvement in joint goal accuracy (JGA) of DST output by\nincorporating accountability heads into modern LLMs. Self-correcting the\ndetected errors further increases the JGA from 67.13 to 70.51, achieving\nstate-of-the-art DST performance. Finally, we show that error correction\nthrough user confirmations (friction turn) achieves a similar performance gain,\nhighlighting its potential to reduce user overreliance."
      ]
    }
  },
  {
    "id":2412.10196,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"XGBoost: A Scalable Tree Boosting System",
    "start_abstract":"Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Quality assurance procedures for mass spectrometry untargeted metabolomics. a review"
      ],
      "abstract":[
        "Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Stability of 2-class groups in the $\\mathbb{Z}_2$-extension of certain\n  real biquadratic fields",
        "Obstructions for Morin and fold maps: Stiefel-Whitney classes and Euler\n  characteristics of singularity loci",
        "Chiral broadband High Harmonic Generation Source by Vectorial\n  Time-Polarization-Gating",
        "Crystal skeletons: Combinatorics and axioms",
        "Quantum crystal spin Hall effect in two-dimensional altermagnetic\n  systems",
        "Random Variables, Conditional Independence and Categories of Abstract\n  Sample Spaces",
        "First-principles Hubbard parameters with automated and reproducible\n  workflows",
        "Constant-Overhead Fault-Tolerant Bell-Pair Distillation using High-Rate\n  Codes",
        "Bounded Dark Energy",
        "Probing Type II Seesaw Leptogenesis Through Lepton Flavor Violation",
        "Strategic Queues with Priority Classes",
        "Predicting Liquidity-Aware Bond Yields using Causal GANs and Deep\n  Reinforcement Learning with LLM Evaluation",
        "$\\mathrm{G}_2$-structures with torsion and the deformed Shatashvili-Vafa\n  vertex algebra",
        "A new class of non-stationary Gaussian fields with general smoothness on\n  metric graphs",
        "Isolating the hard core of phaseless inference: the Phase selection\n  formulation",
        "Automated DC Voltage and DC Resistance Real-time Multiple Standard for\n  Artifact Calibration of Calibrators and multimeters",
        "Probing Magnetism in Self-Assembled Organometallic Complexes using Kondo\n  Spectroscopy",
        "Bluetooth sensors in phyphox with Arduino and MicroPython -- Paving the\n  way from an idea to an experiment for teachers and learners",
        "Impact of radiative accelerations on the stellar characterization of\n  FGK-type stars using spectroscopic and seismic constraints",
        "Functional equation arising in behavioral sciences: solvability and\n  collocation scheme in H\\\"older spaces",
        "Rough Stochastic Pontryagin Maximum Principle and an Indirect Shooting\n  Method",
        "Universal Anyon Tunneling in a Chiral Luttinger Liquid",
        "Invariants recovering the reduction type of a hyperelliptic curve",
        "The Engel--Minkowski question-mark function",
        "A Full AGN Feedback Prescription for Numerical Models: Negative,\n  Positive and Hot Gas-Ejection Mode",
        "Critical Equations Involving Nonlocal Subelliptic Operators on\n  Stratified Lie Groups: Spectrum, Bifurcation and Multiplicity",
        "Partial-dual genus polynomial of graphs",
        "Well-posedness for the dNLS hierarchy",
        "Spectral distribution of the free Jacobi process with equal rank\n  projections"
      ],
      "abstract":[
        "Greenberg's conjecture on the stability of $\\ell$-class groups in the\ncyclotomic $\\mathbb{Z}_{\\ell}$-extension of a real field has been proven for\nvarious infinite families of real quadratic fields for the prime $\\ell=2$. In\nthis work, we consider an infinite family of real biquadratic fields $K$. With\nsome extensive use of elementary group theoretic and class field theoretic\narguments, we investigate the $2$-class groups of the $n$-th layers $K_n$ of\nthe cyclotomic $\\mathbb{Z}_2$-extension of $K$ and verify Greenberg's\nconjecture. We also relate capitulation of ideal classes of certain\nsub-extensions of $K_n$ to the relative sizes of the $2$-class groups.",
        "For a singularity type $\\eta$, let the $\\eta$-avoiding number of an\n$n$-dimensional manifold $M$ be the lowest $k$ for which there is a map\n$M\\to\\mathbb{R}^{n+k}$ without $\\eta$ type singular points. For instance, the\ncase of $\\eta=\\Sigma^1$ is the case of immersions, which has been extensively\nstudied in the case of real projective spaces. In this paper we study the\n$\\eta$-avoiding number for other singularity types. Our results come in two\nlevels: first we give an abstract reasoning that a non-zero cohomology class is\nsupported on the singularity locus $\\eta(f)$, proving that $\\eta(f)$ cannot be\nempty. Second, we interpret this obstruction as a non-zero invariant of the\nsingularity locus $\\eta(f)$ for generic $f$. The main technique that we employ\nis Sullivan's Stiefel-Whitney classes, which are mod 2, real analogues of the\nChern-Schwartz-MacPherson (CSM) classes. We introduce the Segre-Stiefel-Whitney\nclasses of a singularity ${\\rm s}^{\\rm sw}_\\eta$ whose lowest degree term is\nthe mod 2 Thom polynomial of $\\eta$. Using these techniques we compute some\nuniversal formulas for the Euler characteristic of a singularity locus.",
        "Chiral (highly helical) extreme ultraviolet (XUV) sources are pivotal for\ninvestigating chiroptical phenomena on the ultrafast electronic timescale.\nTable-top, coherent High Harmonic Generation (HHG)-based sources are\nparticularly well-suited for these studies. However, chiral materials, such as\norganic chiral molecules and solid-state magnetic materials, exhibit fine\nspectral features which necessitate broadband radiation for their complete\ninterrogation. The generation of radiation that is both broadband and helical\nthrough HHG presents a seemingly paradoxical challenge: while chiral HHG\nemission requires at least two recollisions occurring along different\ndirections in the polarization plane, the Floquet limit might already be\nreached with as few as three recollisions, resulting in a sparse spectrum\ncharacterized by pronounced discrete harmonic peaks. Here we propose a\nstraightforward scheme that enables the interrogation of fine spectral\nfeatures, in principle restricted only by the resolution of the XUV\nspectrometer, with chiral XUV light. Our method is based on using a vectorial\ntwo-color driver with close central-frequencies with slight symmetry breaking.\nIt integrates the time-gating and polarization-gating techniques to generate a\nvectorial driver which induces well-controlled bursts of recollisions,\noccurring along different directions in the polarization plane. The method\nsatisfies the dual requirements of an XUV source which is both broadband and\nhelical. We perform polarization scan and demonstrate that the broadband XUV\nradiation exhibits rapid modulations in its spectral ellipticity, and fast\nalternation in its spectral helicities. The phase of modulations could be\ncontrolled by introducing a slight symmetry breaking. This allows us to control\nand modulate the XUV polarization state, which should enable the detection of\nchiroptical signals with enhanced sensitivity.",
        "Crystal skeletons were introduced by Maas-Gari\\'epy in 2023 by contracting\nquasi-crystal components in a crystal graph. On the representation theoretic\nlevel, crystal skeletons model the expansion of Schur functions into Gessel's\nquasisymmetric functions. Motivated by questions of Schur positivity, we\nprovide a combinatorial description of crystal skeletons, and prove many new\nproperties, including a conjecture by Maas-Gari\\'epy that crystal skeletons\ngeneralize dual equivalence graphs. We then present a new axiomatic approach to\ncrystal skeletons. We give three versions of the axioms based on\n$GL_n$-branching, $S_n$-branching, and local axioms in analogy to the local\nStembridge axioms for crystals based on novel commutation relations.",
        "In the field of condensed matter physics, time-reversal symmetry provides the\nfoundation for a number of interesting quantum phenomena, in particular the\ntopological materials and the quantum spin Hall physics that have been\nextensively studied in recent years. Here, based on the first-principles\nelectronic-structure calculations, symmetry analysis, and model simulations, we\ndemonstrate that time-reversal symmetry is not fundamentally necessary for the\nquantum spin Hall effect. In altermagnetic materials, as an alternative, it can\nalso be protected by crystal symmetry, which can be referred to as the quantum\ncrystal spin Hall effect.",
        "Two high-level \"pictures\" of probability theory have emerged: one that takes\nas central the notion of random variable, and one that focuses on distributions\nand probability channels (Markov kernels). While the channel-based picture has\nbeen successfully axiomatized, and widely generalized, using the notion of\nMarkov category, the categorical semantics of the random variable picture\nremain less clear. Simpson's probability sheaves are a recent approach, in\nwhich probabilistic concepts like random variables are allowed vary over a site\nof sample spaces. Simpson has identified rich structure on these sites, most\nnotably an abstract notion of conditional independence, and given examples\nranging from probability over databases to nominal sets. We aim bring this\ndevelopment together with the generality and abstraction of Markov categories:\nWe show that for any suitable Markov category, a category of sample spaces can\nbe defined which satisfies Simpson's axioms, and that a theory of probability\nsheaves can be developed purely synthetically in this setting. We recover\nSimpson's examples in a uniform fashion from well-known Markov categories, and\nconsider further generalizations.",
        "We introduce an automated, flexible framework (aiida-hubbard) to\nself-consistently calculate Hubbard $U$ and $V$ parameters from\nfirst-principles. By leveraging density-functional perturbation theory, the\ncomputation of the Hubbard parameters is efficiently parallelized using\nmultiple concurrent and inexpensive primitive cell calculations. Furthermore,\nthe intersite $V$ parameters are defined on-the-fly during the iterative\nprocedure to account for atomic relaxations and diverse coordination\nenvironments. We demonstrate the scalability and reliability of the framework\nby computing in high-throughput fashion the self-consistent onsite $U$ and\nintersite $V$ parameters for 115 Li-containing bulk solids. Our analysis of the\nHubbard parameters calculated reveals a significant correlation of the onsite\n$U$ values on the oxidation state and coordination environment of the atom on\nwhich the Hubbard manifold is centered, while intersite $V$ values exhibit a\ngeneral decay with increasing interatomic distance. We find, e.g., that the\nnumerical values of $U$ for Fe and Mn 3d orbitals can vary up to 3 eV and 6 eV,\nrespectively; their distribution is characterized by typical shifts of about\n0.5 eV and 1.0 eV upon change in oxidation state, or local coordination\nenvironment. For the intersite $V$ a narrower spread is found, with values\nranging between 0.2 eV and 1.6 eV when considering transition metal and oxygen\ninteractions. This framework paves the way for the exploration of redox\nmaterials chemistry and high-throughput screening of $d$ and $f$ compounds\nacross diverse research areas, including the discovery and design of novel\nenergy storage materials, as well as other technologically-relevant\napplications.",
        "We present a fault-tolerant Bell-pair distillation scheme achieving constant\noverhead through high-rate quantum low-density parity-check (qLDPC) codes. Our\napproach maintains a constant distillation rate equal to the code rate - as\nhigh as $1\/3$ in our implementations - while requiring no additional overhead\nbeyond the physical qubits of the code. Full circuit-level analysis\ndemonstrates fault-tolerance for input Bell pair infidelities below a threshold\n$\\sim 5\\%$, readily achievable with near-term capabilities. Unlike previous\nproposals, our scheme keeps the output Bell pairs encoded in qLDPC codes at\neach node, eliminating decoding overhead and enabling direct use in distributed\nquantum applications through recent advances in qLDPC computation. These\nresults establish qLDPC-based distillation as a practical route toward\nresource-efficient quantum networks and distributed quantum computing.",
        "Recent cosmological observations suggest that the dark energy equation of\nstate may have changed in the latter stages of cosmic history. We introduce a\nquintessence scenario, termed bounded dark energy, capable of explaining this\nfeature in a technically natural way. Our approach is motivated from a\nbottom-up perspective, based on the concept of mirage cut-off, where we\ndemonstrate the stability of the quintessence potential against large quantum\ncorrections. At the same time, the bounded dark energy framework aligns well\nwith top-down considerations motivated from quantum gravity arguments. We\nemploy both human-driven insights and machine learning techniques to identify\nexplicit realizations of bounded dark energy models. We then perform an\nanalysis based on Markov Chain Monte-Carlo to assess their predictions against\nCMB, galaxy surveys, and supernova data, showing that bounded dark energy\nprovides a good fit to current observations. We also discuss how upcoming\nmeasurements can further test and refine our proposal.",
        "Lepton flavor violation (LFV) offers a powerful probe of physics beyond the\nStandard Model, particularly in models addressing neutrino masses and the\nbaryon asymmetry of the universe. In this study, we investigate LFV processes\nwithin the framework of type II seesaw leptogenesis, where the Standard Model\nis extended by an $SU(2)_L$ triplet Higgs field. We focus on key LFV processes\nincluding $\\mu^+\\to e^+\\gamma$, $\\mu^+ \\to e^+e^-e^+$, and $\\mu \\rightarrow e$\nconversion in nuclei, deriving stringent constraints on the parameter space\nfrom current experimental data. We scan the 3$\\sigma$ range of neutrino\noscillation parameters and identify the most conservative bounds consistent\nwith existing measurements. Our results reveal that the MEG experiment\ncurrently provides the strongest constraints in the normal ordering (NO)\nscenario, while the SINDRUM experiment offers comparable sensitivity in the\ninverted ordering (IO) case. Future experiments, such as MEG II, Mu3e, Mu2e,\nand COMET, are predicted to significantly improve the sensitivity, testing\nlarger regions of the parameter space. This work underscores the crucial role\nof LFV experiments in probing type II seesaw leptogenesis, providing an avenue\nto explore the connections between neutrino mass generation, baryogenesis, and\ninflation at experimentally accessible energy scales.",
        "We consider a strategic M\/M\/1 queueing model under a first-come-first-served\nregime, where customers are split into two classes and class $A$ has priority\nover class $B$. Customers can decide whether to join the queue or balk, and, in\ncase they have joined the queue, whether and when to renege. We study the\nequilibrium strategies and compare the equilibrium outcome and the social\noptimum in the two cases where the social optimum is or is not constrained by\npriority.",
        "Financial bond yield forecasting is challenging due to data scarcity,\nnonlinear macroeconomic dependencies, and evolving market conditions. In this\npaper, we propose a novel framework that leverages Causal Generative\nAdversarial Networks (CausalGANs) and Soft Actor-Critic (SAC) reinforcement\nlearning (RL) to generate high-fidelity synthetic bond yield data for four\nmajor bond categories (AAA, BAA, US10Y, Junk). By incorporating 12 key\nmacroeconomic variables, we ensure statistical fidelity by preserving essential\nmarket properties. To transform this market dependent synthetic data into\nactionable insights, we employ a finetuned Large Language Model (LLM)\nQwen2.5-7B that generates trading signals (BUY\/HOLD\/SELL), risk assessments,\nand volatility projections. We use automated, human and LLM evaluations, all of\nwhich demonstrate that our framework improves forecasting performance over\nexisting methods, with statistical validation via predictive accuracy, MAE\nevaluation(0.103%), profit\/loss evaluation (60% profit rate), LLM evaluation\n(3.37\/5) and expert assessments scoring 4.67 out of 5. The reinforcement\nlearning-enhanced synthetic data generation achieves the least Mean Absolute\nError of 0.103, demonstrating its effectiveness in replicating real-world bond\nmarket dynamics. We not only enhance data-driven trading strategies but also\nprovides a scalable, high-fidelity synthetic financial data pipeline for risk &\nvolatility management and investment decision-making. This work establishes a\nbridge between synthetic data generation, LLM driven financial forecasting, and\nlanguage model evaluation, contributing to AI-driven financial decision-making.",
        "We construct representations of the deformed Shatashvili-Vafa vertex algebra\n$\\mathrm{SV}_a$, with parameter $a \\in \\mathbb{C}$, as recently proposed in the\nphysics literature by Fiset and Gaberdiel. The geometric input for our\nconstruction are integrable $\\mathrm{G}_2$-structures with closed torsion,\nsolving the heterotic $\\mathrm{G}_2$ system with $\\alpha'=0$ on the group\nmanifolds $S^3\\times T^4$ and $S^3\\times S^3\\times S^1$. From considerations in\nstring theory, one expects the chiral algebra of these backgrounds to include\n$\\mathrm{SV}_a$, and we provide a mathematical realization of this expectation\nby obtaining embeddings of $\\mathrm{SV}_a$ in the corresponding superaffine\nvertex algebra and the chiral de Rham complex. In our examples, the parameter\n$a$ is proportional to the scalar torsion class of the $\\mathrm{G}_2$\nstructure, $a \\sim \\tau_0$, as expected from previous work in the\nsemi-classical limit by the second author, jointly with De la Ossa and\nMarchetto.",
        "The increasing availability of network data has driven the development of\nadvanced statistical models specifically designed for metric graphs, where\nGaussian processes play a pivotal role. While models such as Whittle-Mat\\'ern\nfields have been introduced, there remains a lack of practically applicable\noptions that accommodate flexible non-stationary covariance structures or\ngeneral smoothness. To address this gap, we propose a novel class of\ngeneralized Whittle-Mat\\'ern fields, which are rigorously defined on general\ncompact metric graphs and permit both non-stationarity and arbitrary\nsmoothness. We establish new regularity results for these fields, which extend\neven to the standard Whittle-Mat\\'ern case. Furthermore, we introduce a method\nto approximate the covariance operator of these processes by combining the\nfinite element method with a rational approximation of the operator's\nfractional power, enabling computationally efficient Bayesian inference for\nlarge datasets. Theoretical guarantees are provided by deriving explicit\nconvergence rates for the covariance approximation error, and the practical\nutility of our approach is demonstrated through simulation studies and an\napplication to traffic speed data, highlighting the flexibility and\neffectiveness of the proposed model class.",
        "Real-valued Phase retrieval is a non-convex continuous inference problem,\nwhere a high-dimensional signal is to be reconstructed from a dataset of\nsignless linear measurements. Focusing on the noiseless case, we aim to\ndisentangle the two distinct sub-tasks entailed in the Phase retrieval problem:\nthe hard combinatorial problem of retrieving the missing signs of the\nmeasurements, and the nested convex problem of regressing the input-output\nobservations to recover the hidden signal. To this end, we introduce and\nanalytically characterize a two-level formulation of the problem, called\n``Phase selection''. Within the Replica Theory framework, we perform a large\ndeviation analysis to characterize the minimum mean squared error achievable\nwith different guesses for the hidden signs. Moreover, we study the free-energy\nlandscape of the problem when both levels are optimized simultaneously, as a\nfunction of the dataset size. At low temperatures, in proximity to the\nBayes-optimal threshold -- previously derived in the context of Phase retrieval\n-- we detect the coexistence of two free-energy branches, one connected to the\nrandom initialization condition and a second to the signal. We derive the phase\ndiagram for a first-order transition after which the two branches merge.\nInterestingly, introducing an $L_2$ regularization in the regression sub-task\ncan anticipate the transition to lower dataset sizes, at the cost of a bias in\nthe signal reconstructions which can be removed by annealing the regularization\nintensity. Finally, we study the inference performance of three meta-heuristics\nin the context of Phase selection: Simulated Annealing, Approximate Message\nPassing, and Langevin Dynamics on the continuous relaxation of the sign\nvariables. With simultaneous annealing of the temperature and the $L_2$\nregularization, they are shown to approach the Bayes-optimal sample efficiency.",
        "An automated temperature-controlled electrical DC voltage and DC resistance\nmultiple reference standard (MRS) has been developed by Measurements\nInternational (MI) with the scientific support from the Istituto Nazionale di\nRicerca Metrologica (INRIM). The MRS includes a 10 V, a 1 {\\Omega}, and a 10\nk{\\Omega} standards selectable via a switch unit. This setup allows the\nartifact calibration of high-end calibrators and multimeters used in\nlow-frequency electrical measurements. The two resistors are high-stability\nstandards from MI, while the 10 V standard is based on a low-noise circuit\ndeveloped by INRIM in collaboration with MI. A key innovation is the internal\nreal-time clock calendar, which displays the calibration values of the MRS\nstandards and their updated values internally calculated. This ensures reliable\nuse of the MRS standards over extended periods between calibrations,\neffectively minimizing uncertainties due to their drift. The standards are\nhoused in a thermal box, minimizing temperature variations. The MRS standards\nmeet the uncertainty requirements defined by calibrators and multimeters\nmanufacturers for artifact calibration and can also serve as laboratory\nreferences or travelling standards for interlaboratory comparisons (ILCs). MI\nis currently commercializing the MRS.",
        "Control of individual spins at the atomic level holds great promise for\nminiaturized spintronics, quantum sensing, and quantum information processing.\nBoth single atomic and molecular spin centers are prime candidates for these\napplications and are often individually addressed and manipulated using\nscanning tunneling microscopy (STM). In this work, we present a hybrid approach\nand demonstrate a robust method for self-assembly of magnetic organometallic\ncomplexes consisting of individual iron (Fe) atoms and molecules on a silver\nsubstrate using STM. We employ two types of molecules, bis(dibenzoylmethane)\ncopper(II) [Cu(dbm)2] and iron phthalocyanine (FePc). We show that in both\ncases the Fe atoms preferentially attach underneath the benzene ring ligand of\nthe molecules, effectively forming an organometallic half-sandwich arene\ncomplex, Fe(C6H6), that is akin to the properties of metallocenes. In both\nsituations, a molecule can be combined with up to two Fe atoms. In addition, we\nobserve a change in the magnetic properties of the attached Fe atoms in\nscanning tunneling spectroscopy, revealing a distinct Kondo signature at the Fe\nsites. We explain the latter using density functional theory calculations, and\nfind that the bond formation between the Fe 3d-orbitals and the benzene\n{\\pi}-molecular orbitals creates a favorable situation for Kondo screening of\nthe d_xz- and d_yz-like orbitals. Thus, this work establishes a reliable design\nprinciple for forming hybrid organometallic complexes and simultaneous tuning\nof their atomic spin states.",
        "In order to extend the available sensors of smartphone experiments with cheap\nmicrocontroller-based external sensors, the smartphone experimentation app\n\"phyphox\" has been extended with a generic Bluetooth Low Energy interface.\nSince its application requires an in-depth understanding of the underlying\ntechnologies, the direct use of that interface for educational purposes is\nlimited. To avoid this difficulty, the functionality was encapsulated into an\nArduino and MicroPython library. With these, also educators and learners with\nonly rudimentary programming knowledge can integrate an app-based interface\ninto microcontroller projects with only few lines of code. This opens a wide\nrange of new learning opportunities, which are described exemplarily.",
        "Chemical transport mechanisms are fundamental processes in stellar evolution\nmodels. They are responsible for the chemical distribution, and their impact\ndetermines how accurately we can characterize stars. Radiative accelerations\nare one of these processes. They allow the accumulation of elements at\ndifferent depths in the star. We aim to assess the impact of radiative\naccelerations on the modeling of FGK-type stars and their impact on the\nprediction of surface abundances. To reduce the cost of the computation of\nradiative accelerations, we implemented the single-valued parameters (SVP)\nmethod in the stellar evolution code MESA. The SVP method is more efficient in\ncalculating radiative accelerations, which enables computations of large enough\ngrids of models for stellar characterization. Compared to models that include\natomic diffusion (with only gravitational settling), the inclusion of radiative\naccelerations has a small effect on the inference of fundamental properties,\nwith an impact of 2\\%, 0.7\\%, and 5\\% for mass, radius, and age. However, the\ntreatment of radiative accelerations is necessary to predict the chemical\ncomposition of and accurately characterize stars.",
        "We consider a generalization of a functional equation that models the\nlearning process in various animal species. The equation can be considered\nnonlocal, as it is built with a convex combination of the unknown function\nevaluated at mixed arguments. This makes the equation contain two terms with\nvanishing delays. We prove the existence and uniqueness of the solution in the\nH\\\"older space which is a natural function space to consider. In the second\npart of the paper, we devise an efficient numerical collocation method used to\nfind an approximation to the main problem. We prove the convergence of the\nscheme and, in passing, several properties of the linear interpolation operator\nacting on the H\\\"older space. Numerical simulations verify that the order of\nconvergence of the method (measured in the supremum norm) is equal to the order\nof H\\\"older continuity.",
        "We derive first-order Pontryagin optimality conditions for stochastic optimal\ncontrol with deterministic controls for systems modeled by rough differential\nequations (RDE) driven by Gaussian rough paths. This Pontryagin Maximum\nPrinciple (PMP) applies to systems following stochastic differential equations\n(SDE) driven by Brownian motion, yet it does not rely on forward-backward SDEs\nand involves the same Hamiltonian as the deterministic PMP. The proof consists\nof first deriving various integrable error bounds for solutions to nonlinear\nand linear RDEs by leveraging recent results on Gaussian rough paths. The PMP\nthen follows using standard techniques based on needle-like variations. As an\napplication, we propose the first indirect shooting method for nonlinear\nstochastic optimal control and show that it converges 10x faster than a direct\nmethod on a stabilization task.",
        "The edge modes of fractional quantum Hall liquids are described by chiral\nLuttinger liquid theory. Despite many years of experimental investigation\nfractional quantum Hall edge modes remain enigmatic with significant\ndiscrepancies between experimental observations and detailed predictions of\nchiral Luttinger liquid theory. Here we report measurements of tunneling\nconductance between counterpropagating edge modes at $\\nu=1\/3$ across a quantum\npoint contact fabricated on an AlGaAs\/GaAs heterostructure designed to promote\na sharp confinement potential. We present evidence for tunneling of anyons\nthrough an $\\nu=1\/3$ incompressible liquid that exhibits universal scaling\nbehavior with respect to temperature, source-drain bias, and barrier\ntransmission, as originally proposed by Wen[1,2]. We measure the tunneling\nexponent $g = 0.334 \\pm 0.001$, consistent with the scaling dimension $\\Delta =\ng\/2 = 1\/6$ for a Laughlin quasiparticle at the edge. When combined with\nmeasurements of the fractional charge $e^*=e\/3$ and the recently observed\nanyonic statistical angle $\\theta_a=\\frac{2\\pi}{3}$, the measured tunneling\nexponent fully characterizes the topological order of the primary Laughlin\nstate at $\\nu=1\/3$.",
        "Tate's algorithm tells us that for an elliptic curve $E$ over a local field\n$K$ of residue characteristic $\\geq 5$, $E\/K$ has potentially good reduction if\nand only if $\\text{ord}(j_E)\\geq 0$. It also tells us that when $E\/K$ is\nsemistable the dual graph of the special fibre of the minimal regular model of\n$E\/K^{\\text{unr}}$ can be recovered from $\\text{ord}(j_E)$. We generalise these\nresults to hyperelliptic curves of genus $g\\geq 2$ over local fields of odd\nresidue characteristic $K$ by defining a list of absolute invariants that\ndetermine the potential stable model of a genus $g$ hyperelliptic curve $C$.\nThey also determine the dual graph of the special fibre of the minimal regular\nmodel of $C\/K^{\\text{unr}}$ if $C\/K$ is semistable. This list depends only on\nthe genus of $C$, and the absolute invariants can be written in terms of the\ncoefficients of a Weierstrass equation for $C$. We explicitly describe the\nmethod by which the valuations of the invariants recover the dual graphs.\nAdditionally, we show by way of a counterexample that if $g \\geq 2$, there is\nno list of invariants whose valuations determine the dual graph of the special\nfibre of the minimal regular model of a genus $g$ hyperelliptic curve $C$ over\na local field $K$ of odd residue characteristic when $C$ is not assumed to be\nsemistable.",
        "The present article deals with properties of a certain function of the\nMinkowski type with arguments defined by Engel series. Differential, integral,\nand other properties of the function were considered.",
        "We build upon the state-of-the-art semi-analytic model \\texttt{FEGA24}\n(Formation and Evolution of GAlaxies, \\citealt{contini2024d}), which integrates\nthe latest prescriptions relevant to galaxy formation and evolution, alongside\na comprehensive AGN feedback model. This model incorporates three modes of\nfeedback: negative (preventing excessive cooling), positive (enhancing star\nformation), and hot gas ejection (expelling gas beyond the virial radius of\nhalos). These modes operate in a coordinated manner: the negative mode\nregulates the cooling process, the positive mode promotes bursts of star\nformation, and the hot gas ejection mode expels gas beyond the virial radius\nwhen the AGN is sufficiently powerful. Our updated semi-analytic model,\n\\texttt{FEGA25}, retains the qualitative and quantitative consistency of the\nanalyses presented in \\cite{contini2024d}, while delivering more robust\nresults. Notably, \\texttt{FEGA25} provides a more detailed characterization of\nthe fraction of red galaxies as a function of stellar mass, predicts a main\nsequence of star-forming galaxies more consistent with observations, and\nestimates the fraction of hot gas in halos closer to observed values. These\nfindings underscore the importance of a physical mechanism capable of ejecting\nhot gas beyond the virialized region of dark matter halos without significantly\naltering the stellar and cold gas components. Such a mechanism is crucial to\nensure the proper functioning of other processes, such as cooling and star\nformation. Since supernova feedback is already modeled at its maximum\nefficiency, AGN feedback emerges as the natural candidate for this role.",
        "In this paper, we explore the bifurcation phenomena and establish the\nexistence of multiple solutions for the nonlocal subelliptic Brezis-Nirenberg\nproblem:\n  \\begin{equation*}\n  \\begin{cases} (-\\Delta_{\\mathbb{G}})^s u= |u|^{2_s^*-2}u+\\lambda u \\quad\n&\\text{in}\\quad \\Omega, \\\\ u=0\\quad & \\text{in}\\quad \\mathbb{G}\\backslash\n\\Omega, \\end{cases}\n  \\end{equation*} where $(-\\Delta_{\\mathbb{G}})^s$ is the fractional\nsub-Laplacian on the stratified Lie group $\\mathbb{G}$ with homogeneous\ndimension $Q,$ $\\Omega$ is a open bounded subset of $\\mathbb{G},$ $s \\in\n(0,1)$, $Q> 2s,$ $2_s^*:=\\frac{2Q}{Q-2s}$ is subelliptic fractional Sobolev\ncritical exponent, $\\lambda>0$ is a real parameter. This work extends the\nseminal contributions of Cerami, Fortunato, and Struwe to nonlocal subelliptic\noperators on stratified Lie groups. A key component of our study involves\nanalyzing the subelliptic $(s, p)$-eigenvalue problem for the (nonlinear)\nfractional $p$-sub-Laplacian $(-\\Delta_{p,{\\mathbb{G}}})^s$ \\begin{align*}\n  (-\\Delta_{p,{\\mathbb{G}}})^s u&=\\lambda\n|u|^{p-2}u,~\\text{in}~\\Omega,\\nonumber\n  u&=0~\\text{ in }~{\\mathbb{G}}\\setminus\\Omega, \\end{align*} with\n$0<s<1<p<\\infty$ and $Q>ps$, over the fractional Folland-Stein-Sobolev spaces\non stratified Lie groups applying variational methods. Particularly, we prove\nthat the $(s, p)$-spectrum of $(-\\Delta_{p,{\\mathbb{G}}})^s$ is closed and the\nsecond eigenvalue $\\lambda_2(\\Omega)$ with\n$\\lambda_2(\\Omega)>\\lambda_1(\\Omega)$ is well-defined and provides a\nvariational characterization of $\\lambda_2(\\Omega)$. We emphasize that the\nresults obtained here are also novel for $\\mathbb{G}$ being the Heisenberg\ngroup.",
        "Recently, Chmutov introduced the partial duality of ribbon graphs, which can\nbe regarded as a generalization of the classical Euler-Poincar\\'e duality. The\npartial-dual genus polynomial $^\\partial\\varepsilon_G(z)$ is an enumeration of\nthe partial duals of $G$ by Euler genus. For an intersection graph derived from\na given chord diagram, the partial-dual genus polynomial can be defined by\nconsidering the ribbon graph associated to the chord diagram. In this paper, we\nprovide a combinatorial approach to the partial-dual genus polynomial in terms\nof intersection graphs without referring to chord diagrams. After extending the\ndefinition of the partial-dual genus polynomial from intersection graphs to all\ngraphs, we prove that it satisfies the four-term relation of graphs. This\nprovides an answer to a problem proposed by Chmutov.",
        "We prove well-posedness for higher-order equations in the so-called dNLS\nhierarchy (also known as part of the Kaup-Newell hierarchy) in almost critical\nFourier-Lebesgue and in modulation spaces. Leaning in on estimates proven by\nthe author in a previous instalment Adams (2024), where a similar\nwell-posedness theory was developed for the equations of the NLS hierarchy, we\nshow the $j$th equation in the dNLS hierarchy is locally well-posed for initial\ndata in $\\hat H^s_r(\\mathbb{R})$ for $s \\ge \\frac{1}{2} + \\frac{j-1}{r'}$ and\n$1 < r \\le 2$ and also in $M^s_{2, p}(\\mathbb{R})$ for $s \\ge \\frac{j}{2}$ and\n$2 \\le p < \\infty$. Supplementing our results with corresponding ill-posedness\nresults in Fourier-Lebesgue and modulation spaces shows optimality.\n  Our arguments are based on the Fourier restriction norm method in Bourgain\nspaces adapted to our data spaces and the gauge-transformation commonly\nassociated with the dNLS equation. For the latter we establish bi-Lipschitz\ncontinuity between appropriate modulation spaces and that even for higher-order\nequations `bad' cubic nonlinear terms are lifted from the equation.",
        "The free Jacobi process is the radial part of the compression of the free\nunitary Brownian motion by two free orthogonal projections. In this paper, we\ndetermine the characteristic curves of the partial differential equation\nsatisfied by its spectral distribution when both projections have the same rank\n$\\alpha \\in (0,1)$. Doing so leads for any fixed time $t >0$ to an expression\nof the moment generating function in a neighborhood of the origin, extending\nour previous results valid for $\\alpha = 1\/2$. Moreover, the obtained\ncharacteristic curves are encoded by an $\\alpha$-deformation of the\n$\\xi$-transform of the spectral distribution of the free unitary Brownian\nmotion, of which we study mapping properties. We also prove a dynamical version\nof a recent identity pointed out by T. Kunisky and relating the stationary\ndistributions of the free Jacobi processes corresponding to the set of\nparameters $(\\alpha, \\alpha)$ and $(1\/2,\\alpha)$. Actually, our dynamical\nversion relates the Cauchy-Stieltjes transforms of the densities of the finite\ntime spectral distributions."
      ]
    }
  }
]