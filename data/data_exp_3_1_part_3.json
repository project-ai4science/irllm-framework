[
  {
    "id":2411.18784,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"nnu-net: a self-configuring method for deep learning-based biomedical image segmentation",
    "start_abstract":"Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Multimodal Breast Parenchymal Patterns Correlation Using a Patient-Specific Biomechanical Model"
      ],
      "abstract":[
        "In this paper, we aim to produce a realistic 2-D projection of the breast parenchymal distribution from a 3-D breast magnetic resonance image (MRI). To evaluate the accuracy of our simulation, we compare our results with the local breast density (i.e., density map) obtained from the complementary full-field digital mammogram. To achieve this goal, we have developed a fully automatic framework, which registers MRI volumes to X-ray mammograms using a subject-specific biomechanical model of the breast. The optimization step modifies the position, orientation, and elastic parameters of the breast model to perform the alignment between the images. When the model reaches an optimal solution, the MRI glandular tissue is projected and compared with the one obtained from the corresponding mammograms. To reduce the loss of information during the ray-casting, we introduce a new approach that avoids resampling the MRI volume. In the results, we focus our efforts on evaluating the agreement of the distributions of glandular tissue, the degree of structural similarity, and the correlation between the real and synthetic density maps. Our approach obtained a high-structural agreement regardless the glandularity of the breast, whilst the similarity of the glandular tissue distributions and correlation between both images increase in denser breasts. Furthermore, the synthetic images show continuity with respect to large structures in the density maps."
      ],
      "categories":[
        "Imaging"
      ]
    },
    "list":{
      "title":[
        "Mantra: Rewriting Quantum Programs to Minimize Trap-Movements for Zoned\n  Rydberg Atom Arrays",
        "Investigating quantum criticality through charged scalar fields near the\n  BTZ black hole horizon",
        "Discovery of High-Temperature Superconducting Ternary Hydrides via Deep\n  Learning",
        "Cove-edged Chiral Graphene Nanoribbons with Chirality-Dependent Bandgap\n  and Carrier Mobility",
        "Nonlinear Einstein-Power-Yang-Mills AdS Black Holes: From Quantum\n  Tunneling to Aschenbach Effect",
        "Multi-set variational quantum dynamics algorithm for simulating\n  nonadiabatic dynamics on quantum computers",
        "Cooperative Decay of N Atoms in a Ring Configuration",
        "NeuralFoil: An Airfoil Aerodynamics Analysis Tool Using Physics-Informed\n  Machine Learning",
        "On a geometric extremum problem for convex cones",
        "Streamlining Compliance And Risk Management with Regtech Solutions",
        "Rationalizability and Monotonocity in Games with Incomplete Information",
        "On representations of the crystallization of the quantized function\n  algebra C(SUq(n + 1))",
        "Strong Long-Wave Infrared Optical Response in a Topological\n  Semiconductor with a Mexican Hat Band Structure",
        "Remote State Estimation over a Wearing Channel: Information Freshness\n  vs. Channel Aging",
        "Polynomial maps with constants on split octonion algebras",
        "Exponentially Tilted Thermodynamic Maps (expTM): Predicting Phase\n  Transitions Across Temperature, Pressure, and Chemical Potential",
        "Photon-by-photon quantum light state engineering",
        "Storing quantum coherence in a quantum dot nuclear spin ensemble for\n  over 100 milliseconds",
        "Uhlmann's theorem for relative entropies",
        "Arithmetic properties for Overpartitions where nonoverlined parts are\n  $\\ell$-regular",
        "On the possibility of chiral symmetry breaking in liquid hydrogen\n  peroxide",
        "Multi-channel pattern reconstruction through $L$-directional associative\n  memories",
        "New Bounds for Sparse Variational Gaussian Processes",
        "Automated Microsolvation for Minimum Energy Path Construction in\n  Solution",
        "Quantitative observability for the Schr\\\"{o}dinger equation with an\n  anharmonic oscillator",
        "Higher Eckmann-Hilton Arguments in Type Theory",
        "Leveraging Pre-Trained Visual Transformers for Multi-Band Photometric\n  Light Curve Classification",
        "Probabilistic adaptation of language comprehension for individual\n  speakers: Evidence from neural oscillations",
        "An Air-Gap Element for the Isogeometric Space-Time-Simulation of\n  Electric Machines"
      ],
      "abstract":[
        "A zoned neutral atom architecture achieves exceptional fidelity by\nsegregating the execution spaces of 1- and 2-qubit gates, being a promising\ncandidate for high-accuracy quantum systems. Unfortunately, naively applying\nprograms designed for static qubit topologies to zoned architectures may result\nin most execution time being consumed by inter-zone travels of atoms. To\naddress this, we introduce Mantra (Minimizing trAp movemeNts for aTom aRray\nArchitectures), which rewrites quantum programs to reduce the interleaving of\nsingle- and two-qubit gates. Mantra incorporates three strategies: (i) a\nfountain-shaped controlled-Z (CZ) chain, (ii) ZZ-interaction protocol without a\n1-qubit gate, and (iii) preemptive gate scheduling. Mantra reduces inter-zone\nmovements by 68%, physical gate counts by 35%, and improves circuit fidelities\nby 17% compared to the standard executions.",
        "We examine a charged scalar field with a position-dependent mass \\( m(\\rho) =\nm_0 + \\mathcal{S}(\\rho) \\), where \\(\\mathcal{S}(\\rho)\\) represents a Lorentz\nscalar potential, near a BTZ black hole in the presence of an external magnetic\nfield. By deriving the Klein-Gordon equation for this setup, we explore two\nscenarios: (i) a mass-modified scalar field with \\(m(\\rho) = m_0 - a\/\\rho\\) (an\nexactly solvable case), and (ii) a scenario involving both mass modification\nand an external magnetic field (conditionally exactly solvable). We identify\nquantum critical points (QCPs) associated with the coupling constant \\(a\\). In\nthe first scenario, for massless charged scalar fields, critical points occur\nat \\(a = n + 1\/2\\) for all radial quantum numbers \\(n \\geq 0\\) and magnetic\nquantum numbers \\(|m| \\geq 0\\). In the second scenario, these critical points\nshift to \\(a = n + 3\/2\\) for \\(n \\geq 0\\) and \\(|m| > 0\\), with the case \\(m =\n0\\) excluded. For massive scalar fields, QCPs emerge at \\(a = (n + 1\/2)\/2\\),\nleading to non-propagating fields at zero frequency. At these QCPs, the field\nfrequencies drop to zero, marking transitions from stable oscillatory modes to\nnon-propagating states. Below the critical points, the system exhibits\ninstability, characterized by negative imaginary frequencies that suggest rapid\ndecay and high dissipation. Above the critical points, the modes stabilize and\npropagate, indicating a transition to a superconducting-like phase, where\ndissipation vanishes and stable excitations dominate.",
        "The discovery of novel high-temperature superconductor materials holds\ntransformative potential for a wide array of technological applications.\nHowever, the combinatorially vast chemical and configurational search space\nposes a significant bottleneck for both experimental and theoretical\ninvestigations. In this study, we employ the design of high-temperature ternary\nsuperhydride superconductors as a representative case to demonstrate how this\nchallenge can be well addressed through a deep-learning-driven theoretical\nframework. This framework integrates high-throughput crystal structure\nexploration, physics-informed screening, and accurate prediction of\nsuperconducting critical temperatures. Our approach enabled the exploration of\napproximately 36 million ternary hydride structures across a chemical space of\n29 elements, leading to the identification of 144 potential high-Tc\nsuperconductors with predicted Tc > 200 K and superior thermodynamic stability\nat 200 GPa. Among these, 129 compounds spanning 27 novel structural prototypes\nare reported for the first time, representing a significant expansion of the\nknown structural landscape for hydride superconductors. This work not only\ngreatly expands the known repertoire of high-Tc hydride superconductors but\nalso establishes a scalable and efficient methodology for navigating the\ncomplex landscape of multinary hydrides.",
        "Graphene nanoribbons (GNRs) have garnered significant interest due to their\nhighly customizable physicochemical properties and potential utility in\nnanoelectronics. Besides controlling widths and edge structures, the inclusion\nof chirality in GNRs brings another dimension for fine-tuning their\noptoelectronic properties, but related studies remain elusive owing to the\nabsence of feasible synthetic strategies. Here, we demonstrate a novel class of\ncove-edged chiral GNRs (CcGNRs) with a tunable chiral vector (n,m). Notably,\nthe bandgap and effective mass of (n,2)- CcGNR show a distinct positive\ncorrelation with the increasing value of n, as indicated by theory. Within this\nGNR family, two representative members, namely, (4,2)- CcGNR and (6,2)-CcGNR,\nare successfully synthesized. Both CcGNRs exhibit prominently curved geometries\narising from the incorporated [4]helicene motifs along their peripheries, as\nalso evidenced by the single-crystal structures of the two respective model\ncompounds (1 and 2). The chemical identities and optoelectronic properties of\n(4,2)- and (6,2)-CcGNRs are comprehensively investigated via a combination of\nIR, Raman, solid-state NMR, UV-vis, and THz spectroscopies as well as\ntheoretical calculations. In line with theoretical expectation, the obtained\n(6,2)-CcGNR possesses a low optical bandgap of 1.37 eV along with charge\ncarrier mobility of 8 cm2\/Vs, whereas (4,2)-CcGNR exhibits a narrower bandgap\nof 1.26 eV with increased mobility of 14 cm2\/Vs. This work opens up a new\navenue to precisely engineer the bandgap and carrier mobility of GNRs by\nmanipulating their chiral vector.",
        "This study investigates the thermodynamic and quantum properties of\nEinstein-Power-Yang-Mills (EPYM) black holes in an Anti-de Sitter background,\nfocusing on the effects of the nonlinear Yang-Mills charge parameter $\\gamma$.\nWe derive the metric function, analyze Hawking radiation through boson\ntunneling, and calculate thermodynamic properties including temperature and\nphase transitions. The quantum tunneling of $W^+$ bosons is examined using the\nWKB approximation and Hamilton-Jacobi formalism, revealing how nonlinearity\nmodifies the radiation spectrum. We compute the effective potential governing\nphoton orbits and null geodesics, demonstrating significant alterations in\nlight behavior in strong gravitational fields. Additionally, we explore the\nAschenbach effect, showing that this phenomenon, which is typically associated\nwith rotating black holes, can emerge in spherically symmetric EPYM spacetimes\nbecause of non-linear field interactions. Our results may yield observational\nmarkers that can be identified with instruments such as the Event Horizon\nTelescope and upcoming gravitational wave detectors.",
        "Accelerating quantum dynamical simulations with quantum computing has\nreceived considerable attention but remains a significant challenge. In\nvariational quantum algorithms for quantum dynamics, designing an expressive\nand shallow-depth parameterized quantum circuit (PQC) is a key difficulty.\nHere, we proposed a multi-set variational quantum dynamics algorithm (MS-VQD)\ntailored for nonadiabatic dynamics involving multiple electronic states. MS-VQD\nemploys multiple PQCs to represent the electronic-nuclear coupled wavefunction,\nwith each circuit adapting to the motion of nuclear wavepacket on a specific\npotential energy surface. By simulating excitation energy transfer dynamics in\nmolecular aggregates described by the Frenkel-Holstein model, we demonstrated\nthat MS-VQD achieves the same accuracy as traditional VQD while requiring\nsignificantly shallower PQCs. Notably, its advantage increases with the number\nof electronic states, making it suitable for simulating nonadiabatic quantum\ndynamics in complex molecular systems.",
        "We provide an analytic expression of the spectrum of the cooperative decay\nrate of N two-level atoms regularly distributed on a ring in the\nsingle-excitation configuration. The results are obtained first for the scalar\nmodel and then extended to the vectorial light model, assuming all the dipoles\nare aligned.",
        "NeuralFoil is an open-source Python-based tool for rapid aerodynamics\nanalysis of airfoils, similar in purpose to XFoil. Speedups ranging from 8x to\n1,000x over XFoil are demonstrated, after controlling for equivalent accuracy.\nNeuralFoil computes both global and local quantities (lift, drag, velocity\ndistribution, etc.) over a broad input space, including: an 18-dimensional\nspace of airfoil shapes, possibly including control deflections; a 360 degree\nrange of angles of attack; Reynolds numbers from $10^2$ to $10^{10}$; subsonic\nflows up to the transonic drag rise; and with varying turbulence parameters.\nResults match those of XFoil closely: the mean relative error of drag is 0.37%\non simple cases, and remains as low as 2.0% on a test dataset with numerous\npost-stall and transitional cases. NeuralFoil facilitates gradient-based design\noptimization, due to its $C^\\infty$-continuous solutions,\nautomatic-differentiation-compatibility, and bounded computational cost without\nnon-convergence issues.\n  NeuralFoil is a hybrid of physics-informed machine learning techniques and\nanalytical models. Here, physics information includes symmetries that are\nstructurally embedded into the model architecture, feature engineering using\ndomain knowledge, and guaranteed extrapolation to known limit cases. This work\nalso introduces a new approach for surrogate model uncertainty quantification\nthat enables robust design optimization.\n  This work discusses the methodology and performance of NeuralFoil with\nseveral case studies, including a practical airfoil design optimization study\nincluding both aerodynamic and non-aerodynamic constraints. Here, NeuralFoil\noptimization is able to produce airfoils nearly identical in performance and\nshape to expert-designed airfoils within seconds; these\ncomputationally-optimized airfoils provide a useful starting point for further\nexpert refinement.",
        "We discuss the optimization problem for minimizing the $(n-1)$-volume of the\nintersection of a convex cone in $\\Bbb R^n$ with a hyperplane through a given\npoint.",
        "RegTech is a rapidly rising financial services sector focused on using\ncutting-edge technology to improve the process of regulatory compliance.\nRegTech solutions are characterized by numerous features and benefits that can\nconsiderably contribute to helping organizations operate effectively in the\nincreasingly regulated environment, when it comes to compliance and risk\nmanagement. This paper sheds light on why RegTech will be one of the most\npromising markets, driven by the rising cost of compliance and the growing\nreliance on technology in crisis management. Moreover, this paper will examine\nthe advantages of using such solutions to strike a balance between compliance\nand operational efficiencies. This paper will deepen the understanding of\nregulatory compliance, introduce RegTech, and examine the benefits of using\nthese solutions to achieve compliance.",
        "This paper examines games with strategic complements or substitutes and\nincomplete information, where players are uncertain about the opponents'\nparameters. We assume that the players' beliefs about the opponent's parameters\nare selected from some given set of beliefs. One extreme is the case where\nthese sets only contain a single belief, representing a scenario where the\nplayers' actual beliefs about the parameters are commonly known among the\nplayers. Another extreme is the situation where these sets contain all possible\nbeliefs, representing a scenario where the players have no information about\nthe opponents' beliefs about parameters. But we also allow for intermediate\ncases, where these sets contain some, but not all, possible beliefs about the\nparameters. We introduce an assumption of weakly increasing differences that\ntakes both the choice belief and parameter belief of a player into account.\nUnder this assumption, we demonstrate that greater choice-parameter beliefs\nleads to greater optimal choices. Moreover, we show that the greatest and least\npoint rationalizable choice of a player is increasing in their parameter, and\nthese can be determined through an iterative procedure. In each round of the\niterative procedure, the lowest surviving choice is optimal for the lowest\nchoice-parameter belief, while the greatest surviving choice is optimal for the\nhighest choice-parameter belief.",
        "The crystallization C(SU0(n + 1)) of the q-family of C*-algebras C(SUq(n +\n1)) was defined by Giri & Pal for all n \\geq 2 and they observed that any\nirreducible representation at the q = 0 level is the norm limit of irreducible\nrepresentations of C(SUq(n + 1)) as q\\to 0+. In this article, we will\ngeneralize this to any non-degenerate representation (respectively faithful) of\nC(SU0(n+1)). As a consequence, one can alternatively realize C(SU0(n+1)) as the\nC*-algebra generated by the limit operators of faithful representations of\nC(SUq(n+1)).",
        "Light sources and photodetectors operating in the far- to mid-infrared\n(FIR\/MIR) band ($8$-$12~\\rm \\mu m$, $0.1$-$0.15~\\rm eV$) remain relatively\npoorly developed compared to their counterparts operating in the visible and\nnear-infrared ranges, despite extensive application potential for thermal\nimaging, standoff sensing, and other technologies. This is attributable in part\nto the lack of narrow-gap materials ($<0.1~\\rm eV$) with high optical gain and\nabsorption. In this work, a narrow-gap semiconductor, $\\rm Pb_{0.7}Sn_{0.3}Se$,\nis demonstrated to exhibit an optical response $>10\\times$ larger than that of\n$\\rm Hg_{x}Cd_{1-x}Te$ (MCT), the dominant material for FIR\/MIR photodetectors.\nA previous theoretical investigation indicated that chalcogen $p$ and metal $d$\nband inversion in this material creates a Mexican hat band structure (MHBS),\nwhich results in a dramatic increase in the joint density of states at the\noptical transition edge compared to typical semiconductors. This prediction is\nexperimentally validated here using single-crystal specimens of $\\rm\nPb_{0.7}Sn_{0.3}Se$ measured using temperature-dependent spectroscopic\nellipsometry over a wavelength range of $1.7$-$20~\\rm \\mu m$ ($0.73$-$0.062~\\rm\neV$). These measurements demonstrate a large enhancement in extinction\ncoefficient and refractive index characteristic of a MHBS in the vicinity of\nthe absorption edge, in agreement with theoretical predictions. The realization\nof topological semiconductors with a MHBS is expected to lead to\nhigh-efficiency detectors operating in the FIR\/MID range.",
        "We study the remote estimation of a linear Gaussian system over a\nnonstationary channel that wears out over time and with every use. The sensor\ncan either transmit a fresh measurement in the current time slot, restore the\nchannel quality at the cost of downtime, or remain silent. More frequent\ntransmissions yield accurate estimates but incur significant wear on the\nchannel. Renewing the channel too often improves channel conditions but results\nin poor estimation quality. What is the optimal timing to transmit measurements\nand restore the channel? We formulate the problem as a Markov decision process\n(MDP) and show the monotonicity properties of an optimal policy. A structured\npolicy iteration algorithm is proposed to find the optimal policy.",
        "Let $\\mathbf{O}(\\mathbb{F})$ be the split octonion algebra over an\nalgebraically closed field $\\mathbb{F}$. For positive integers $k_1, k_2\\geq\n2$, we study surjectivity of the map $A_1(x^{k_1}) + A_2(y^{k_2}) \\in\n\\mathbf{O}(\\mathbb{F})\\langle x, y\\rangle$ on $\\mathbf{O}(\\mathbb{F})$. For\nthis, we use the orbit representatives of the ${G}_2(\\mathbb{F})$-action on\n$\\mathbf{O}(\\mathbb{F}) \\times \\mathbf{O}(\\mathbb{F}) $ for the tuple $(A_1,\nA_2)$, and characterize the ones which give a surjective map.",
        "Predicting and characterizing phase transitions is crucial for understanding\ngeneric physical phenomena such as crystallization, protein folding and others.\nHowever, directly observing phase transitions is not always easy, and often one\nhas limited observations far from the phase boundary and measured under some\nspecific thermodynamic conditions. In this study, we propose a statistical\nphysics and Generative AI driven framework that can take such limited\ninformation to generate samples of different phases under arbitrary\nthermodynamic conditions, which we name Exponentially Tilted Thermodynamic Maps\n(expTM). The central idea is to map collected data into a tractable simple\nprior expressed as an exponentially tilted Gaussian. We demonstrate how the\nvariance and mean of the prior can be correlated with pairs of thermodynamic\ncontrol variables, including temperature, pressure, and chemical potential.\nThis gives us the ability to generate thermodynamically correct samples under\nany values of the control variables. To demonstrate the practical applicability\nof this approach, we use expTM to sample the lattice gas models with the Grand\nCanonical ensemble, capturing phase transitions under varying chemical\npotentials and temperatures. We further demonstrate how expTM can model the\nisothermal-isobaric ensemble, with which we predict different phases of CO2\nunder varying pressure conditions. Both examples are trained on very limited\ndata far from the phase boundary. These results establish expTM as a robust\ntool for understanding phase transitions across diverse thermodynamic\nconditions requiring only a small number of observations.",
        "The ability to manipulate light at the level of single photons, its\nelementary excitation quanta, has recently made it possible to produce a rich\nvariety of tailor-made quantum states and arbitrary quantum operations, of high\ninterest for fundamental science and applications. Here we present a concise\nreview of the progress made over the last few decades in the engineering of\nquantum light states. Although far from exhaustive, this review aims at\nproviding a sufficiently wide and updated introduction that may serve as the\nentry point to such a fascinating and rapidly evolving field.",
        "States with long coherence are a crucial requirement for qubits and quantum\nmemories. Nuclear spins in epitaxial quantum dots are a great candidate,\noffering excellent isolation from external environments and on-demand coupling\nto optical flying qubits. However, coherence times are limited to $\\lesssim1$\nms by the dipole-dipole interactions between the nuclei and their quadrupolar\ncoupling to inhomogeneous crystal strain. Here, we combine strain engineering\nof the nuclear spin ensemble and tailored dynamical decoupling sequences to\nachieve nuclear spin coherence times exceeding 100 ms. Recently, a reversible\ntransfer of quantum information into nuclear spin ensembles has been\ndemonstrated in quantum dots. Our results provide a path to develop this\nconcept into a functioning solid-state quantum memory suitable for quantum\nrepeaters in optical quantum communication networks.",
        "Uhlmann's theorem states that, for any two quantum states $\\rho_{AB}$ and\n$\\sigma_A$, there exists an extension $\\sigma_{AB}$ of $\\sigma_A$ such that the\nfidelity between $\\rho_{AB}$ and $\\sigma_{AB}$ equals the fidelity between\ntheir reduced states $\\rho_A$ and $\\sigma_A$. In this work, we generalize\nUhlmann's theorem to $\\alpha$-R\\'enyi relative entropies for $\\alpha \\in\n[\\frac{1}{2},\\infty]$, a family of divergences that encompasses fidelity,\nrelative entropy, and max-relative entropy corresponding to\n$\\alpha=\\frac{1}{2}$, $\\alpha=1$, and $\\alpha=\\infty$, respectively.",
        "In this paper, we study the partition function $\\overline{R_\\ell^\\ast}(n)$,\nwhich counts the number of overpartitions of $n$ where the non-overlined parts\nare $\\ell$-regular. Using the theory of modular forms, we establish various\ncongruences and infinite families for this function when $\\ell = 6,8$.\nAdditionally, we present some intriguing conjectures.",
        "Molecular chirality is a key concept in chemistry with implications for the\norigin of life and the manufacturing of pharmaceuticals. Previous simulations\nof a chiral molecular model with an energetic bias towards homochiral\ninteractions show a spontaneous symmetry-breaking transition from a\nsupercritical racemic liquid into a subcritical liquid enriched in one of the\ntwo enantiomers. Here, we employ molecular dynamics simulations in order to\ntest the possible existence of this phenomenon in hydrogen peroxide, the\nsmallest chiral molecule. For this purpose, we study the fluid phase of this\nsubstance between 100 K and 1500 K, and from $10^{-4}$ GPa to 1 GPa. We find a\nglass transition and we suggest that hydrogen bonds play a central role in such\nbehavior. We also test the possibility of observing chiral symmetry breaking by\nperforming both constant temperature and cooling simulations at multiple\npressures, and we do not observe the phenomenon. An analysis of the structure\nof the liquid shows negligible differences between homochiral and heterochiral\ninteractions, supporting the difficulty in observing chiral symmetry breaking.\nIf hydrogen peroxide manifests spontaneous chiral symmetry breaking, it likely\ntakes place significantly below room temperature and is hidden by other\nphenomena, such as the glass transition or crystallization. More broadly, our\nresults, and recent experimental observations, suggest that greater molecular\ncomplexity is needed for spontaneous chiral symmetry breaking in the liquid\nphase to occur.",
        "We consider $L$-directional associative memories, composed of $L$ Hopfield\nnetworks, displaying imitative Hebbian intra-network interactions and\nanti-imitative Hebbian inter-network interactions, where couplings are built\nover a set of hidden binary patterns. We evaluate the model's performance in\nreconstructing the whole set of hidden binary patterns when provided with\nmixtures of noisy versions of these patterns. Our numerical results demonstrate\nthe model's high effectiveness in the reconstruction task for structureless and\nstructured datasets.",
        "Sparse variational Gaussian processes (GPs) construct tractable posterior\napproximations to GP models. At the core of these methods is the assumption\nthat the true posterior distribution over training function values ${\\bf f}$\nand inducing variables ${\\bf u}$ is approximated by a variational distribution\nthat incorporates the conditional GP prior $p({\\bf f} | {\\bf u})$ in its\nfactorization. While this assumption is considered as fundamental, we show that\nfor model training we can relax it through the use of a more general\nvariational distribution $q({\\bf f} | {\\bf u})$ that depends on $N$ extra\nparameters, where $N$ is the number of training examples. In GP regression, we\ncan analytically optimize the evidence lower bound over the extra parameters\nand express a tractable collapsed bound that is tighter than the previous\nbound. The new bound is also amenable to stochastic optimization and its\nimplementation requires minor modifications to existing sparse GP code.\nFurther, we also describe extensions to non-Gaussian likelihoods. On several\ndatasets we demonstrate that our method can reduce bias when learning the\nhyperpaparameters and can lead to better predictive performance.",
        "Describing chemical reactions in solution on a molecular level is a\nchallenging task due to the high mobility of weakly interacting solvent\nmolecules which requires configurational sampling. For instance, polar and\nprotic solvents can interact strongly with solutes and may interfere in\nreactions. However, to define and identify representative arrangements of\nsolvent molecules modulating a transition state is a non-trivial task. Here, we\npropose to monitor their active participation in the decaying normal mode at a\ntransition state, which defines active solvent molecules. Moreover, it is\ndesirable to prepare a low-dimensional microsolvation model in a well-defined,\nfully automated, high-throughput, and easy-to-deploy fashion, which we propose\nto derive in a stepwise protocol. First, transition state structures are\noptimized in a sufficiently solvated quantum-classical hybrid model, which are\nthen subjected to a re-definition of a then reduced quantum region. From the\nreduced model, minimally microsolvated structures are extracted that contain\nonly active solvent molecules. Modeling the remaining solvation effects is\ndeferred to a continuum model. To establish an easy-to-use free-energy model,\nwe combine the standard thermochemical gas-phase model with a correction for\nthe cavity entropy in solution. We assess our microsolvation and free-energy\nmodels for methanediol formation from formaldehyde, for the hydration of carbon\ndioxide (which we consider in a solvent mixture to demonstrate the versatility\nof our approach), and, finally, for the chlorination of phenol with\nhypochlorous acid.",
        "This paper studies the observability inequalities for the Schr\\\"{o}dinger\nequation associated with an anharmonic oscillator $H=-\\frac{\\d^2}{\\d x^2}+|x|$.\nWe build up the observability inequality over an arbitrarily short time\ninterval $(0,T)$, with an explicit expression for the observation constant\n$C_{obs}$ in terms of $T$, for some observable set that has a different\ngeometric structure compared to those discussed in \\cite{HWW}. We obtain the\nsufficient conditions and the necessary conditions for observable sets,\nrespectively. We also present counterexamples to demonstrate that half-lines\nare not observable sets, highlighting a major difference in the geometric\nproperties of observable sets compared to those of Schr\\\"{o}dinger operators\n$H=-\\frac{\\d^2}{\\d x^2}+|x|^{2m}$ with $m\\ge 1$.\n  Our approach is based on the following ingredients: First, the use of an\nIngham-type spectral inequality constructed in this paper; second, the\nadaptation of a quantitative unique compactness argument, inspired by the work\nof Bourgain-Burq-Zworski \\cite{Bour13}; third, the application of the\nSzeg\\\"{o}'s limit theorem from the theory of Toeplitz matrices, which provides\na new mathematical tool for proving counterexamples of observability\ninequalities.",
        "We use a type theory for omega-categories to produce higher-dimensional\ngeneralisations of the Eckmann-Hilton argument. The heart of our construction\nis a family of padding and repadding techniques, which give a notion of\ncongruence between cells of different types. This gives explicit witnesses in\nall dimensions that, for cells with degenerate boundary, all composition\noperations are congruent and commutative. Our work has been implemented,\nallowing us to explicitly compute these witnesses, and we show these grow\nrapidly in complexity as the parameters are varied. Our results can also be\nexported as elements of identity types in Martin-Lof type theory, and hence are\nof relevance in homotopy type theory.",
        "This study investigates the potential of a pre-trained visual transformer\n(VT) model, specifically the Swin Transformer V2 (SwinV2), to classify\nphotometric light curves without the need for feature extraction or multi-band\npreprocessing. The goal is to assess whether this image-based approach can\naccurately differentiate astronomical phenomena and serve as a viable option\nfor working with multi-band photometric light curves. We transformed each\nmulti-band light curve into an image. These images serve as input to the SwinV2\nmodel, which is pre-trained on ImageNet-21K. The datasets employed include the\npublic Catalog of Variable Stars from the Massive Compact Halo Object (MACHO)\nsurvey, using both one and two bands, and the first round of the recent\nExtended LSST Astronomical Time-Series Classification Challenge (ELAsTiCC),\nwhich includes six bands. The performance of the model was evaluated on six\nclasses for the MACHO dataset and 20 distinct classes of variable stars and\ntransient events for the ELAsTiCC dataset. The fine-tuned SwinV2 achieved\nbetter performance than models specifically designed for light curves, such as\nAstromer and the Astronomical Transformer for Time Series and Tabular Data\n(ATAT). When trained on the full MACHO dataset, it attained a macro F1-score of\n80.2 and outperformed Astromer in single-band experiments. Incorporating a\nsecond band further improved performance, increasing the F1-score to 84.1. In\nthe ELAsTiCC dataset, SwinV2 achieved a macro F1-score of 65.5, slightly\nsurpassing ATAT by 1.3.",
        "Listeners adapt language comprehension based on their mental representations\nof speakers, but how these representations are dynamically updated remains\nunclear. We investigated whether listeners probabilistically adapt their\ncomprehension based on the likelihood of speakers producing\nstereotype-incongruent utterances. Our findings reveal two potential\nmechanisms: a speaker-general mechanism that adjusts overall expectations about\nspeaker-content relationships, and a speaker-specific mechanism that updates\nindividual speaker models. In two EEG experiments, participants heard speakers\nmake stereotype-congruent or incongruent utterances, with incongruency base\nrate manipulated between blocks. In Experiment 1, speaker incongruency\nmodulated both high-beta (21-30 Hz) and theta (4-6 Hz) oscillations:\nincongruent utterances decreased oscillatory power in low base rate condition\nbut increased it in high base rate condition. The theta effect varied with\nlisteners' openness trait: less open participants showed theta increases to\nspeaker-incongruencies, suggesting maintenance of speaker-specific information,\nwhile more open participants showed theta decreases, indicating flexible model\nupdating. In Experiment 2, we dissociated base rate from the target speaker by\nmanipulating the overall base rate using an alternative non-target speaker.\nOnly the high-beta effect persisted, showing power decrease for\nspeaker-incongruencies in low base rate condition but no effect in high base\nrate condition. The high-beta oscillations might reflect the speaker-general\nadjustment, while theta oscillations may index the speaker-specific model\nupdating. These findings provide evidence for how language processing is shaped\nby social cognition in real time.",
        "Space-time methods promise more efficient time-domain simulations, in\nparticular of electrical machines. However, most approaches require the motion\nto be known in advance so that it can be included in the space-time mesh. To\novercome this problem, this paper proposes to use the well-known air-gap\nelement for the rotor-stator coupling of an isogeometric machine model. First,\nwe derive the solution in the air-gap region and then employ it to couple the\nrotor and stator. This coupling is angle dependent and we show how to\nefficiently update the coupling matrices to a different angle, avoiding\nexpensive quadrature. Finally, the resulting time-dependent problem is solved\nin a space-time setting. The spatial discretization using isogeometric analysis\nis particularly suitable for coupling via the air-gap element, as NURBS can\nexactly represent the geometry of the air-gap. Furthermore, the model including\nthe air-gap element can be seamlessly transferred to the space-time setting.\nHowever, the air-gap element is well known in the literature. The originality\nof this work is the application to isogeometric analysis and space-time."
      ]
    }
  },
  {
    "id":2411.18784,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Multimodal Breast Parenchymal Patterns Correlation Using a Patient-Specific Biomechanical Model",
    "start_abstract":"In this paper, we aim to produce a realistic 2-D projection of the breast parenchymal distribution from a 3-D breast magnetic resonance image (MRI). To evaluate the accuracy of our simulation, we compare our results with the local breast density (i.e., density map) obtained from the complementary full-field digital mammogram. To achieve this goal, we have developed a fully automatic framework, which registers MRI volumes to X-ray mammograms using a subject-specific biomechanical model of the breast. The optimization step modifies the position, orientation, and elastic parameters of the breast model to perform the alignment between the images. When the model reaches an optimal solution, the MRI glandular tissue is projected and compared with the one obtained from the corresponding mammograms. To reduce the loss of information during the ray-casting, we introduce a new approach that avoids resampling the MRI volume. In the results, we focus our efforts on evaluating the agreement of the distributions of glandular tissue, the degree of structural similarity, and the correlation between the real and synthetic density maps. Our approach obtained a high-structural agreement regardless the glandularity of the breast, whilst the similarity of the glandular tissue distributions and correlation between both images increase in denser breasts. Furthermore, the synthetic images show continuity with respect to large structures in the density maps.",
    "start_categories":[
      "Imaging"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "nnu-net: a self-configuring method for deep learning-based biomedical image segmentation"
      ],
      "abstract":[
        "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Signs of Non-Monotonic Finite-Volume Corrections to $g_A$",
        "Taming Knowledge Conflicts in Language Models",
        "Dual-Polarized Intelligent Omni-Surfaces for Independent\n  Reflective-Refractive Transmission",
        "Turbulent fragmentation as the primary driver of core formation in\n  Polaris Flare and Lupus I",
        "An Unconventional Ultra-Sub-Wavelength Receiving Nano-Antenna Activated\n  by ac Spin Pumping and the ac Inverse Spin Hall Effect",
        "When GNNs meet symmetry in ILPs: an orbit-based feature augmentation\n  approach",
        "CoCoI: Distributed Coded Inference System for Straggler Mitigation",
        "When is Cat(Q) cartesian closed?",
        "Text Data Analysis of Maternal Narratives: Albanian Women in Italy",
        "Going deeper into the dark with COSMOS-Web: JWST unveils the total\n  contribution of Radio-Selected NIRfaint galaxies to the cosmic Star Formation\n  Rate Density",
        "Fusion systems related to polynomial representations of\n  $\\mathrm{SL}_2(q)$",
        "Glacier data assimilation on an Arctic glacier: Learning from large\n  ensemble twin experiments",
        "NL2SQL-BUGs: A Benchmark for Detecting Semantic Errors in NL2SQL\n  Translation",
        "Asymmetric results about graph homomorphisms",
        "Bridging Information Gaps with Comprehensive Answers: Improving the\n  Diversity and Informativeness of Follow-Up Questions",
        "Iterative Feature Space Optimization through Incremental Adaptive\n  Evaluation",
        "Large Language Models for Healthcare Text Classification: A Systematic\n  Review",
        "On Qualitative Preference in Alternating-time Temporal Logic with\n  Strategy Contexts",
        "On the local analyticity for the Euler equations",
        "BYOS: Knowledge-driven Large Language Models Bring Your Own Operating\n  System More Excellent",
        "Complexity and Algorithm for the Matching vertex-cutset Problem",
        "Bigger Isn't Always Better: Towards a General Prior for Medical Image\n  Reconstruction",
        "Higher Weil-Petersson volumes of the moduli space of super Riemann\n  surfaces",
        "Gravitational Waves from Resonant Transitions of Tidally Perturbed\n  Gravitational Atoms",
        "Semi-classical limit of the massive Klein-Gordon-Maxwell system toward\n  the relativistic Euler-Maxwell system via an adapted modulated energy method",
        "Distilling heterogeneous treatment effects: Stable subgroup estimation\n  in causal inference",
        "Phase Alignment Enhances Oscillatory Power in Neural Mass Models\n  Optimized for Class Encoding",
        "Back to business: SLX 1746--331 after 13 years of silence",
        "MGSR: 2D\/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface\n  Reconstruction under Various Light Conditions"
      ],
      "abstract":[
        "We study finite-volume (FV) corrections to determinations of $g_A$ via\nlattice quantum chromodynamics (QCD) using analytic results and numerical\nanalysis. We observe that $SU(2)$ Heavy Baryon Chiral Perturbation Theory does\nnot provide an unambiguous prediction for the sign of the FV correction, which\nis not surprising when one also considers large-$N_c$ constraints on the axial\ncouplings. We further show that non-monotonic FV corrections are naturally\nallowed when one considers either including explicit $\\Delta$-resonance degrees\nof freedom or one works to higher orders in the chiral expansion. We\ninvestigate the potential impact of these FV corrections with a precision study\nof $g_A$ using models of FV corrections that are monotonic and non-monotonic.\nUsing lattice QCD data that is approximately at the 1% level of precision, we\ndo not see significant evidence of non-monotonic corrections. Looking forward\nto the next phase of lattice QCD calculations, we estimate that calculations\nthat are between the 0.1%-1%-level of precision may be sensitive to these FV\nartifacts. Finally, we present an update of the CalLat prediction of $g_A$ in\nthe isospin limit with sub-percent precision, $g_A^{\\rm QCD} = 1.2674(96)$.",
        "Language Models (LMs) often encounter knowledge conflicts when parametric\nmemory contradicts contextual knowledge. Previous works attribute this conflict\nto the interplay between \"memory heads\" and \"context heads\", attention heads\nassumed to promote either memory or context exclusively. In this study, we go\nbeyond this fundamental assumption by uncovering a critical phenomenon we term\nthe \"superposition of contextual information and parametric memory\", where\nhighly influential attention heads could simultaneously contribute to both\nmemory and context. Building upon this insight, we propose Just Run Twice\n(JUICE), a test-time attention intervention method that steers LMs toward\neither parametric beliefs or contextual knowledge without requiring\nfine-tuning. JUICE identifies a set of reliable attention heads and leverages a\ndual-run approach to mitigate the superposition effects. Extensive experiments\nacross 11 datasets and 6 model architectures demonstrate that JUICE sets the\nnew state-of-the-art performance and robust generalization, achieving\nsignificant and consistent improvement across different domains under various\nconflict types. Finally, we theoretically analyze knowledge conflict and the\nsuperposition of contextual information and parametric memory in attention\nheads, which further elucidates the effectiveness of JUICE in these settings.",
        "Intelligent omni-surface (IOS), which are capable of providing service\ncoverage to mobile users (MUs) in a reflective and a refractive manner, has\nrecently attracted widespread attention. However, the performance of\ntraditionally IOS-aid systems is limited by the intimate coupling between the\nrefraction and reflection behavior of IOS elements. In this letter, to overcome\nthis challenge, we introduce the concept of dual-polarized IOS-assisted\ncommunication. More precisely, by employing the polarization domain in the\ndesign of IOS, full independent refraction and reflection modes can be\ndelivered. We consider a downlink dual-polarized IOS-aided system, while also\naccounting for the leakage between different polarizations. To maximize the sum\nrate, we formulate a joint IOS phase shift and BS beamforming problem and\nproposed an iterative algorithm to solve the non-convex program. Simulation\nresults validate that dual-polarized IOS significantly enhances the performance\nthan the traditional one.",
        "Stars form from dense cores in turbulent molecular clouds. According to the\nstandard scenario of star formation, dense cores are created by cloud\nfragmentation. However, the physical mechanisms driving this process are still\nnot fully understood from an observational standpoint. However, the physical\nmechanisms driving this process are still not fully understood from an\nobservational standpoint. Our goal is to investigate the process of cloud\nfragmentation using observational data from nearby clouds. Specifically, we aim\nto examine the role of self-gravity and turbulence, both of which are key to\nthe dynamical evolution of clouds. We applied astrodendro to the Herschel H2\ncolumn density maps to identify dense cores and determine their mass and\nseparation in two nearby low-mass clouds: the Polaris Flare and Lupus I clouds.\nWe then compared the observed core masses and separations with predictions from\nmodels of gravitational and turbulent fragmentation. For turbulent\nfragmentation, the key scales are the cloud sonic scale and its corresponding\nmass. The average core masses are estimated to be 0.242 Msun for Lupus I and\n0.276 Msun for the Polaris Flare. The core separations peak at 0.1 - 0.2 pc in\nboth clouds. These separations are significantly smaller than the Jeans length\nbut agree well with the cloud sonic scale. Additionally, the density\nprobability distribution functions of the dense cores follow log-normal\ndistributions, which is consistent with the predictions of turbulent\nfragmentation. These findings suggest that the primary process driving core\nformation in the observed low-mass star-forming regions is not gravitational\nfragmentation but rather turbulent fragmentation. We found no evidence that\nfilament fragmentation plays a significant role in the formation of dense\ncores.",
        "We report an extreme sub-wavelength unconventional receiving antenna. It\nconsists of an array of nanomagnets connected to heavy metal nanostrips.\nIncident electromagnetic (EM) radiation generates intrinsic and extrinsic spin\nwaves in the nanomagnets, which pump spin into the heavy metal nanostrips at\ntheir own frequencies giving rise to a polychromatic alternating voltage across\nthe latter owing to the ac inverse spin Hall effect. This implements a\nreceiving nano-antenna. We demonstrate its operation at two different EM wave\nfrequencies of 1.5 GHz and 2.4 GHz - the latter being the Bluetooth and Wi-Fi\nfrequency. We measure the receiving gain at 2.4 GHz to be approximately -9 db.\nThe free space radiated wavelength \"lambda\" at 2.4 GHz is 12.5 cm while the\nantenna area A is merely 160 micron^2, making the ratio A\/lambda^2 =\n0.97x10^-8. This antenna's receiving gain should be very poor because of the\ntiny size. Yet the measured gain is more than 4000 times larger than the\ntheoretical limit for a conventional antenna of this size at this wavelength\nbecause of the unconventional operating principle.",
        "A common characteristic in integer linear programs (ILPs) is symmetry,\nallowing variables to be permuted without altering the underlying problem\nstructure. Recently, GNNs have emerged as a promising approach for solving\nILPs. However, a significant challenge arises when applying GNNs to ILPs with\nsymmetry: classic GNN architectures struggle to differentiate between symmetric\nvariables, which limits their predictive accuracy. In this work, we investigate\nthe properties of permutation equivariance and invariance in GNNs, particularly\nin relation to the inherent symmetry of ILP formulations. We reveal that the\ninteraction between these two factors contributes to the difficulty of\ndistinguishing between symmetric variables. To address this challenge, we\nexplore the potential of feature augmentation and propose several guiding\nprinciples for constructing augmented features. Building on these principles,\nwe develop an orbit-based augmentation scheme that first groups symmetric\nvariables and then samples augmented features for each group from a discrete\nuniform distribution. Empirical results demonstrate that our proposed approach\nsignificantly enhances both training efficiency and predictive performance.",
        "Convolutional neural networks (CNNs) are widely applied in real-time\napplications on resource-constrained devices. To accelerate CNN inference,\nprior works proposed to distribute the inference workload across multiple\ndevices. However, they did not address stragglers and device failures in\ndistributed inference, which is challenging due to the devices' time-varying\nand possibly unknown computation\/communication capacities. To address this, we\npropose a distributed coded inference system, called CoCoI. It splits the\nconvolutional layers of CNN, considering the data dependency of\nhigh-dimensional inputs and outputs, and then adapts coding schemes to generate\ntask redundancy. With CoCoI, the inference results can be determined once a\nsubset of devices complete their subtasks, improving robustness against\nstragglers and failures. To theoretically analyze the tradeoff between\nredundancy and subtask workload, we formulate an optimal splitting problem to\nminimize the expected inference latency. Despite its non-convexity, we\ndetermine an approximate strategy with minor errors, and prove that CoCoI\noutperforms uncoded benchmarks. For performance evaluation, we build a testbed\nwith Raspberry Pi 4Bs. The experimental results show that the approximate\nstrategy closely matches the optimal solution. When compared with uncoded\nbenchmarks, CoCoI reduces inference latency by up to 34.2% in the presence of\nstragglers and device failures.",
        "An example in the paper \"Exponentiable functors between quantaloid-enriched\ncategories\" is mistaken: it is not true that the category of categories and\nfunctors enriched in any free quantaloid is cartesian closed. To correct this,\nwe give here an elementary characterization of those quantaloids Q for which\nCat(Q) is cartesian closed. With this characterization, we unify several known\ncases (previously proven using ad hoc methods) and we give some new examples.",
        "Despite growing interest in migration studies, research on motherhood among\nmigrant women in Italy remains limited. This study contributes to the\nliterature by examining the family trajectories of Albanian women in Italy,\nexploring how their migration patterns and experiences have shaped these life\naspects. We conducted a comprehensive textual analysis to find the main topics\nof 30 semi-structured interviews with Albanian mothers living in Milan, Rome,\nand Bari. After pre-processing the text, we performed an exploratory analysis\nto identify key features and explore word relationships. The predominant\ndimensions that emerged relate to family management, work paths and schedules,\nand strategies and concerns arising from the trade-off between work and\nchildcare. Subsequently, we stratified the sample by entry channel into Italy\n(study and work, reunification, and irregular channel) and applied Latent\nDirichlet Allocation to model each sub-corpus as a mixture of topics. Our\nresults resonate with existing literature [1] on the key role of female\nmigratory patterns in shaping post-migration fertility. Interviewees who\nentered Italy through various migratory channels not only differ in their\ncharacteristics and migration experiences but also exhibit dissimilar fertility\ndesires and behaviors, motherhood trajectories, and conceptions of their role\nas mothers and family ideals. These differences influence their priorities and\nlevel of commitment to family and work obligations.",
        "We present the first follow-up with JWST of radio-selected NIRfaint galaxies\nas part of the COSMOS-Web survey. By selecting galaxies detected at radio\nfrequencies ($S_{\\rm 3 GHz}>11.5$ $\\mu$Jy; i.e. S\/N$>5$) and with faint\ncounterparts at NIR wavelengths (F150W$>26.1$ mag), we collect a sample of 127\nlikely dusty star-forming galaxies (DSFGs). We estimate their physical\nproperties through SED fitting, compute the first radio luminosity function for\nthese types of sources, and their contribution to the total cosmic star\nformation rate density. Our analysis confirms that these sources represent a\npopulation of highly dust-obscured ($\\langle A_{\\rm v} \\rangle \\sim3.5$ mag),\nmassive ($\\langle M_\\star \\rangle \\sim10^{10.8}$ M$_\\odot$) and star-forming\ngalaxies ($\\langle {\\rm SFR} \\rangle\\sim300$ M$_\\odot$ yr$^{-1}$) located at\n$\\langle z \\rangle\\sim3.6$, representing the high-redshift tail of the full\ndistribution of radio sources. Our results also indicate that these galaxies\ncould dominate the bright end of the radio luminosity function and reach a\ntotal contribution to the cosmic star formation rate density equal to that\nestimated only considering NIR-bright sources at $z\\sim4.5$. Finally, our\nanalysis further confirms that the radio selection can be employed to collect\nstatistically significant samples of DSFGs, representing a complementary\nalternative to the other selections based on JWST colors or detection at\nFIR\/(sub)mm wavelengths.",
        "Let $q$ be a power of a fixed prime $p$. We classify up to isomorphism all\nsimple saturated fusion systems on a certain class of $p$-groups constructed\nfrom the polynomial representations of $\\mathrm{SL}_2(q)$, which includes the\nSylow $p$-subgroups of $\\mathrm{GL}_3(q)$ and $\\mathrm{Sp}_4(q)$ as special\ncases. The resulting list includes all Clelland--Parker fusion systems, a\nsimple exotic fusion system discovered by Henke--Shpectorov, and a new infinite\nfamily of exotic examples.",
        "Glacier modeling is crucial for quantifying the evolution of cryospheric\nprocesses. At the same time, uncertainties hamper process understanding and\npredictive accuracy. Here, we suggest improving glacier mass balance\nsimulations for the Kongsvegen glacier in Svalbard through the application of\nBayesian data assimilation techniques in a set of large ensemble twin\nexperiments. Noisy synthetic observations of albedo and snow depth, generated\nusing the multilayer CryoGrid community model with a full energy balance, are\nassimilated using two ensemble-based data assimilation schemes: the particle\nbatch smoother and the ensemble smoother. A comprehensive evaluation exercise\ndemonstrates that the joint assimilation of albedo and snow depth improves the\nsimulation skill by up to 86% relative to the prior in specific glacier\nregions. The particle batch smoother excels in representing albedo dynamics,\nwhile the ensemble smoother is particularly effective for snow depth under low\nsnowfall conditions. By combining the strengths of both observations, the joint\nassimilation achieves improved mass balance simulations across different\nglacier zones using either assimilation scheme. This work underscores the\npotential of ensemble-based data assimilation methods for refining glacier\nmodels by offering a robust framework to enhance predictive accuracy and reduce\nuncertainties in cryospheric simulations. Further advances in glacier data\nassimilation will be critical to better understanding the fate and role of\nArctic glaciers in a changing climate.",
        "Natural Language to SQL (i.e., NL2SQL) translation is crucial for\ndemocratizing database access, but even state-of-the-art models frequently\ngenerate semantically incorrect SQL queries, hindering the widespread adoption\nof these techniques by database vendors. While existing NL2SQL benchmarks\nprimarily focus on correct query translation, we argue that a benchmark\ndedicated to identifying common errors in NL2SQL translations is equally\nimportant, as accurately detecting these errors is a prerequisite for any\nsubsequent correction-whether performed by humans or models. To address this\ngap, we propose NL2SQL-BUGs, the first benchmark dedicated to detecting and\ncategorizing semantic errors in NL2SQL translation. NL2SQL-BUGs adopts a\ntwo-level taxonomy to systematically classify semantic errors, covering 9 main\ncategories and 31 subcategories. The benchmark consists of 2018\nexpert-annotated instances, each containing a natural language query, database\nschema, and SQL query, with detailed error annotations for semantically\nincorrect queries. Through comprehensive experiments, we demonstrate that\ncurrent large language models exhibit significant limitations in semantic error\ndetection, achieving an average detection accuracy of only 75.16%. Despite\nthis, the models were able to successfully detect 106 errors (accounting for\n6.91%) in the widely-used NL2SQL dataset, BIRD, which were previously\nannotation errors in the benchmark. This highlights the importance of semantic\nerror detection in NL2SQL systems.",
        "Many important results in extremal graph theory can be roughly summarised as\n\"if a triangle-free graph $G$ has certain properties, then it has a\nhomomorphism to a triangle-free graph $\\Gamma$ of bounded size\". For example,\nbounds on homomorphism thresholds give such a statement if $G$ has sufficiently\nhigh minimum degree, and the approximate homomorphism theorem gives such a\nstatement for all $G$, if one weakens the notion of homomorphism appropriately.\n  In this paper, we study asymmetric versions of these results, where the\nassumptions on $G$ and $\\Gamma$ need not match. For example, we prove that if\n$G$ is a graph with odd girth at least $9$ and minimum degree at least $\\delta\n|G|$, then $G$ is homomorphic to a triangle-free graph whose size depends only\non $\\delta$. Moreover, the odd girth assumption can be weakened to odd girth at\nleast $7$ if $G$ has bounded VC dimension or bounded domination number. This\ngives a new and improved proof of a result of Huang et al.\n  We also prove that in the asymmetric approximate homomorphism theorem, the\nbounds exhibit a rather surprising ``double phase transition'': the bounds are\nsuper-exponential if $G$ is only assumed to be triangle-free, they become\nexponential if $G$ is assumed to have odd girth $7$ or $9$, and become linear\nif $G$ has odd girth at least $11$.\n  Our proofs use a wide variety of techniques, including entropy arguments, the\nFrieze--Kannan weak regularity lemma, properties of the generalised Mycielskian\nconstruction, and recent work on abundance and the asymmetric removal lemma.",
        "Effective conversational systems are expected to dynamically generate\ncontextual follow-up questions to elicit new information while maintaining the\nconversation flow. While humans excel at asking diverse and informative\nquestions by intuitively assessing both obtained and missing information,\nexisting models often fall short of human performance on this task. To mitigate\nthis, we propose a method that generates diverse and informative questions\nbased on targeting unanswered information using a hypothetical LLM-generated\n\"comprehensive answer\". Our method is applied to augment an existing follow-up\nquestions dataset. The experimental results demonstrate that language models\nfine-tuned on the augmented datasets produce follow-up questions of\nsignificantly higher quality and diversity. This promising approach could be\neffectively adopted to future work to augment information-seeking dialogues for\nreducing ambiguities and improving the accuracy of LLM answers.",
        "Iterative feature space optimization involves systematically evaluating and\nadjusting the feature space to improve downstream task performance. However,\nexisting works suffer from three key limitations:1) overlooking differences\namong data samples leads to evaluation bias; 2) tailoring feature spaces to\nspecific machine learning models results in overfitting and poor\ngeneralization; 3) requiring the evaluator to be retrained from scratch during\neach optimization iteration significantly reduces the overall efficiency of the\noptimization process. To bridge these gaps, we propose a gEneralized Adaptive\nfeature Space Evaluator (EASE) to efficiently produce optimal and generalized\nfeature spaces. This framework consists of two key components: Feature-Sample\nSubspace Generator and Contextual Attention Evaluator. The first component aims\nto decouple the information distribution within the feature space to mitigate\nevaluation bias. To achieve this, we first identify features most relevant to\nprediction tasks and samples most challenging for evaluation based on feedback\nfrom the subsequent evaluator. This decoupling strategy makes the evaluator\nconsistently target the most challenging aspects of the feature space. The\nsecond component intends to incrementally capture evolving patterns of the\nfeature space for efficient evaluation. We propose a weighted-sharing\nmulti-head attention mechanism to encode key characteristics of the feature\nspace into an embedding vector for evaluation. Moreover, the evaluator is\nupdated incrementally, retaining prior evaluation knowledge while incorporating\nnew insights, as consecutive feature spaces during the optimization process\nshare partial information. Extensive experiments on fourteen real-world\ndatasets demonstrate the effectiveness of the proposed framework. Our code and\ndata are publicly available.",
        "Large Language Models (LLMs) have fundamentally transformed approaches to\nNatural Language Processing (NLP) tasks across diverse domains. In healthcare,\naccurate and cost-efficient text classification is crucial, whether for\nclinical notes analysis, diagnosis coding, or any other task, and LLMs present\npromising potential. Text classification has always faced multiple challenges,\nincluding manual annotation for training, handling imbalanced data, and\ndeveloping scalable approaches. With healthcare, additional challenges are\nadded, particularly the critical need to preserve patients' data privacy and\nthe complexity of the medical terminology. Numerous studies have been conducted\nto leverage LLMs for automated healthcare text classification and contrast the\nresults with existing machine learning-based methods where embedding,\nannotation, and training are traditionally required. Existing systematic\nreviews about LLMs either do not specialize in text classification or do not\nfocus on the healthcare domain. This research synthesizes and critically\nevaluates the current evidence found in the literature regarding the use of\nLLMs for text classification in a healthcare setting. Major databases (e.g.,\nGoogle Scholar, Scopus, PubMed, Science Direct) and other resources were\nqueried, which focused on the papers published between 2018 and 2024 within the\nframework of PRISMA guidelines, which resulted in 65 eligible research\narticles. These were categorized by text classification type (e.g., binary\nclassification, multi-label classification), application (e.g., clinical\ndecision support, public health and opinion analysis), methodology, type of\nhealthcare text, and metrics used for evaluation and validation. This review\nreveals the existing gaps in the literature and suggests future research lines\nthat can be investigated and explored.",
        "We show how to add and eliminate binary preference on plays in\nAlternating-time Temporal Logic (ATL) with strategy contexts on Concurrent Game\nModels (CGMs) by means of a translation which preserves satisfaction in models\nwhere preference-indiscernibility between plays is an equivalence relation of\nfinite index. The elimination technique also works for a companion second-order\npath quantifier, which makes quantified path variables range over sets of plays\nthat are closed under preference-indiscernibility. We argue that the preference\noperator and the specialized quantifier facilitate formulating interesting\nsolution concepts such as Nash equilibrium and secure equilibrium in a\nstraightforward way. We also present a novel translation from ATL with strategy\ncontexts to Quantified Computation Tree Logic (QCTL). Together with the\ntranslation which eliminates preference and the specialized form of\nquantification, this translation allows reasoning about infinite multiplayer\nsynchronous games on CGMs to be translated from the proposed extension of ATL\nwith strategy contexts into QCTL. The setting is related to that of ordered\nobjectives in the works of Bouyer, Brenguier, Markey and Ummels, except that\nour focus is on the use of the temporal logic languages mentioned above, and we\nrely on translations into QCTL for the algorithmic solutions.",
        "In this paper, we study the existence and uniqueness of solutions to the\nEuler equations with initial conditions that exhibit analytic regularity near\nthe boundary and Sobolev regularity away from it. A key contribution of this\nwork is the introduction of the diamond-analyticity framework, which captures\nthe spatial decay of the analyticity radius in a structured manner, improving\nupon uniform analyticity approaches. We employ the Leray projection and a\nnonstandard mollification technique to demonstrate that the quotient between\nthe imaginary and real parts of the analyticity radius remains unrestricted,\nthus extending the analyticity persistence results beyond traditional\nconstraints. Our methodology combines analytic-Sobolev estimates with an\niterative scheme which is nonstandard in the Cauchy-Kowalevskaya framework,\nensuring rigorous control over the evolution of the solution. These results\ncontribute to a deeper understanding of the interplay between analyticity and\nboundary effects in fluid equations. They might have implications for the study\nof the inviscid limit of the Navier-Stokes equations and the role of complex\nsingularities in fluid dynamics.",
        "Kernel configurations play an important role in the performance of Operating\nSystem (OS). However, with the rapid iteration of OS, finding the proper\nconfigurations that meet specific requirements can be challenging, which can be\nprimarily attributed to the default kernel provided by vendors does not take\nthe requirements of specific workloads into account, and the heavyweight tuning\nprocess cannot catch up with the rapid evolving pace of the kernel. To address\nthese challenges, we propose BYOS, a novel framework powered by Large Language\nModels (LLMs) to customize kernel configurations for diverse user requirements.\nBy integrating OS-oriented Dual-layer Knowledge Graph (OD-KG) and corresponding\nreasoning strategy, BYOS enhanced the LLM's understanding of the\ncharacteristics and capabilities of OS, thus enabling customized,\ncost-effective, and convenient generation of kernel configurations. Experiments\nshow that the kernels configured by BYOS outperform the default\nvendor-configured kernels by 7.1% to 155.4%, demonstrating the effectiveness\nand efficiency of BYOS in customizing kernel configurations. Our code is\navailable at https:\/\/github.com\/LHY-24\/BYOS.",
        "In 1985, Chv\\'{a}tal introduced the concept of star cutsets as a means to\ninvestigate the properties of perfect graphs, which inspired many researchers\nto study cutsets with some specific structures, for example, star cutsets,\nclique cutsets, stable cutsets. In recent years, approximation algorithms have\ndeveloped rapidly, the computational complexity associated with determining the\nminimum vertex cut possessing a particular structural property have attracted\nconsiderable academic attention.\n  In this paper, we demonstrate that determining whether there is a matching\nvertex-cutset in $H$ with size at most $k$, is $\\mathbf{NP}$-complete, where\n$k$ is a given positive integer and $H$ is a connected graph. Furthermore, we\ndemonstrate that for a connected graph $H$, there exists a $2$-approximation\nalgorithm in $O(nm^2)$ for us to find a minimum matching vertex-cutset.\nFinally, we show that every plane graph $H$ satisfying $H\\not\\in\\{K_2, K_4\\}$\ncontains a matching vertex-cutset with size at most three, and this bound is\ntight.",
        "Diffusion model have been successfully applied to many inverse problems,\nincluding MRI and CT reconstruction. Researchers typically re-purpose models\noriginally designed for unconditional sampling without modifications. Using two\ndifferent posterior sampling algorithms, we show empirically that such large\nnetworks are not necessary. Our smallest model, effectively a ResNet, performs\nalmost as good as an attention U-Net on in-distribution reconstruction, while\nbeing significantly more robust towards distribution shifts. Furthermore, we\nintroduce models trained on natural images and demonstrate that they can be\nused in both MRI and CT reconstruction, out-performing model trained on medical\nimages in out-of-distribution cases. As a result of our findings, we strongly\ncaution against simply re-using very large networks and encourage researchers\nto adapt the model complexity to the respective task. Moreover, we argue that a\nkey step towards a general diffusion-based prior is training on natural images.",
        "Inspired by the theory of JT supergravity, Stanford-Witten derived a\nremarkable recursion formula of Weil-Petersson volumes of moduli space of super\nRiemann surfaces. It is the super version of the celebrated Mirzakhani's\nrecursion formula. In this paper, we generalize Stanford-Witten's formula to\ninclude high degree kappa classes.",
        "Light bosons can form gravitational atoms (GA) around spinning black holes\nthrough the superradiance process. Considering the black hole to be part of a\nbinary system,the tidal potential of the companion periodically perturbs the GA\nsuch that an atomic transition occurs between two of its energy eigenstates.\nThe resonant transition is modeled by the Landau-Zener system,where the orbital\nfrequency of the companion determines the relevant transition. In this work, we\nstudy a novel gravitational wave signal originating directly from the atomic\ntransition of the GA in a binary system. We derive the analytical formulae of\nboth the strain waveform and frequency spectrum of the signal. We further\npresent the GA-binary systems that can have a large signal-to-noise ratio in\nLISA's frequency band. Finally, we discuss the implications of detection of the\nsignal:inferring model parameters,including the boson mass and black hole\nspin,and computing the phase shift and Doppler shift of the gravitational wave\nsignal for equal mass binaries.",
        "We show that the momentum, the density, and the electromagnetic field\nassociated with the massive KleinGordon-Maxwell equations converge in the\nsemi-classical limit towards their respective equivalents associated with the\nrelativistic Euler-Maxwell equations. The proof relies on a modulated\nstress-energy method and a compactness argument. We also give a proof of the\nwell-posedness of the relativistic Euler-Maxwell equations and show how this\nsystem, and so the semi-classical limit of Klein-Gordon-Maxwell, is related to\nthe relativistic massive Vlasov-Maxwell equations.",
        "Recent methodological developments have introduced new black-box approaches\nto better estimate heterogeneous treatment effects; however, these methods fall\nshort of providing interpretable characterizations of the underlying\nindividuals who may be most at risk or benefit most from receiving the\ntreatment, thereby limiting their practical utility. In this work, we introduce\ncausal distillation trees (CDT) to estimate interpretable subgroups. CDT allows\nresearchers to fit any machine learning model to estimate the individual-level\ntreatment effect, and then leverages a simple, second-stage tree-based model to\n\"distill\" the estimated treatment effect into meaningful subgroups. As a\nresult, CDT inherits the improvements in predictive performance from black-box\nmachine learning models while preserving the interpretability of a simple\ndecision tree. We derive theoretical guarantees for the consistency of the\nestimated subgroups using CDT, and introduce stability-driven diagnostics for\nresearchers to evaluate the quality of the estimated subgroups. We illustrate\nour proposed method on a randomized controlled trial of antiretroviral\ntreatment for HIV from the AIDS Clinical Trials Group Study 175 and show that\nCDT out-performs state-of-the-art approaches in constructing stable, clinically\nrelevant subgroups.",
        "Neural encoding of objects and cognitive states remains an elusive yet\ncrucial aspect of brain function. While traditional feed-forward machine\nlearning neural networks have enormous potential to encode information, modern\narchitectures provide little insight into the brain's mechanisms. In this work,\na Jansen and Rit neural mass model was constructed to encode different sets of\ninputs, aiming to understand how simple neural circuits can represent\ninformation. A genetic algorithm was used to optimize parameters that maximized\nthe differences in responses to particular inputs. These differences in\nresponses manifested as phase-shifted oscillations across the set of inputs. By\ndelivering impulses of excitation synchronized with a particular phase-shifted\noscillation, we demonstrated that the encoded phase could be decoded by\nmeasuring oscillatory power. These findings demonstrate the capability of\nneural dynamical circuits to encode and decode information through phase.",
        "The black hole candidate system SLX 1746--331 was back to business in 2023,\nafter a long silence of roughly 13 years. An outburst was observed thoroughly\nby \\textit{Insight}-HXMT and \\textit{NICER}. The outburst is characterized by\nspectral dominance of the soft state, where the joint \\textit{Insight}-HXMT and\n\\textit{NICER} spectral analysis shows the temperature dependence of the disk\nflux follows $T_{\\rm in}^{3.98}$, and thus suggests that the inner disk reaches\nto ISCO during almost the entire outburst. By assuming 0.3 $L_{\\rm Edd}$ for\nthe peak flux and an inclination angle of zero degrees, the lower limit of the\ncompact object hosted in this system is estimated as 3.28$\\pm 2.14 M_\\odot$. We\nalso look into the relation of the disk temperature and disk flux for a sample\nof black hole systems, and by taking the disk temperature derived in the\noutburst of SLX 1746--331, such a relation results in a mass estimation of $5.2\n\\pm 4.5M_\\odot$. Finally, the spin of the compact object is constrained to\nlarger than 0.8 with a spectral model of kerrbb.",
        "Novel view synthesis (NVS) and surface reconstruction (SR) are essential\ntasks in 3D Gaussian Splatting (3D-GS). Despite recent progress, these tasks\nare often addressed independently, with GS-based rendering methods struggling\nunder diverse light conditions and failing to produce accurate surfaces, while\nGS-based reconstruction methods frequently compromise rendering quality. This\nraises a central question: must rendering and reconstruction always involve a\ntrade-off? To address this, we propose MGSR, a 2D\/3D Mutual-boosted Gaussian\nsplatting for Surface Reconstruction that enhances both rendering quality and\n3D reconstruction accuracy. MGSR introduces two branches--one based on 2D-GS\nand the other on 3D-GS. The 2D-GS branch excels in surface reconstruction,\nproviding precise geometry information to the 3D-GS branch. Leveraging this\ngeometry, the 3D-GS branch employs a geometry-guided illumination decomposition\nmodule that captures reflected and transmitted components, enabling realistic\nrendering under varied light conditions. Using the transmitted component as\nsupervision, the 2D-GS branch also achieves high-fidelity surface\nreconstruction. Throughout the optimization process, the 2D-GS and 3D-GS\nbranches undergo alternating optimization, providing mutual supervision. Prior\nto this, each branch completes an independent warm-up phase, with an early\nstopping strategy implemented to reduce computational costs. We evaluate MGSR\non a diverse set of synthetic and real-world datasets, at both object and scene\nlevels, demonstrating strong performance in rendering and surface\nreconstruction."
      ]
    }
  },
  {
    "id":2412.00663,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Deep Learning for Automatic Gross Tumor Volumes Contouring in Esophageal Cancer Based on Contrast-Enhanced Computed Tomography Images: A Multi-Institutional Study",
    "start_abstract":"Purpose To develop and externally validate an automatic artificial intelligence (AI) tool for delineating gross tumor volume (GTV) in patients with esophageal squamous cell carcinoma (ESCC), which can assist in neo-adjuvant or radical radiation therapy treatment planning. Methods and Materials In this multi-institutional study, contrast-enhanced CT images from 580 eligible ESCC patients were retrospectively collected. The GTV contours delineated by 2 experts via consensus were used as ground truth. A 3-dimensional deep learning model was developed for GTV contouring in the training cohort and internally and externally validated in 3 validation cohorts. The AI tool was compared against 12 board-certified experts in 25 patients randomly selected from the external validation cohort to evaluate its assistance in improving contouring performance and reducing variation. Contouring performance was measured using dice similarity coefficient (DSC) and average surface distance. Additionally, our previously established radiomics model for predicting pathologic complete response was used to compare AI-generated and ground truth contours, to assess the potential of the AI contouring tool in radiomics analysis. Results The AI tool demonstrated good GTV contouring performance in multicenter validation cohorts, with median DSC values of 0.865, 0.876, and 0.866 and median average surface distance values of 0.939, 0.789, and 0.875 mm, respectively. Furthermore, the AI tool significantly improved contouring performance for half of 12 board-certified experts (DSC values, 0.794-0.835 vs 0.856-0.881, P = .003-0.048), reduced the intra- and interobserver variations by 37.4% and 55.2%, respectively, and saved contouring time by 77.6%. In the radiomics analysis, 88.7% of radiomic features from ground truth and AI-generated contours demonstrated stable reproducibility, and similar pathologic complete response prediction performance for these contours (P = .430) was observed. Conclusions Our AI contouring tool can improve GTV contouring performance and facilitate radiomics analysis in ESCC patients, which indicates its potential for GTV contouring during radiation therapy treatment planning and radiomics studies.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "MRI-Guided Adaptive Radiation Therapy"
      ],
      "abstract":[
        "Magnetic resonance imaging-guided radiation therapy (MRIgRT) has improved soft tissue contrast over computed tomography (CT) based image-guided RT. Superior visualization of the target and surrounding radiosensitive structures has the potential to improve oncological outcomes partly due to safer dose-escalation and adaptive planning. In this review, we highlight the workflow of adaptive MRIgRT planning, which includes simulation imaging, daily MRI, identifying isocenter shifts, contouring, plan optimization, quality control, and delivery. Increased utilization of MRIgRT will depend on addressing technical limitations of this technology, while addressing treatment efficacy, cost-effectiveness, and workflow training."
      ],
      "categories":[
        "Oncology"
      ]
    },
    "list":{
      "title":[
        "Optically Detected Magnetic Resonance Imaging and Sensing Within\n  Functionalized Additively Manufactured Microporous Structures",
        "A type-theoretic approach to semistrict higher categories",
        "Theory of spin magnetization driven by chiral phonons",
        "Well-posedness of McKean-Vlasov equations under the weak H\\\"ormander\n  condition",
        "A filtered Hochschild-Kostant-Rosenberg theorem for real Hochschild\n  homology",
        "Spectral Analysis and Stability of Wave Equations with Dispersive\n  Nonlinearity",
        "Dynamic Programming in Ordered Vector Space",
        "Characterization of Markarian 421 during its most violent year:\n  Multiwavelength variability and correlations",
        "Hyperbolic Monopoles, (Semi-)Holomorphic Chern-Simons Theories, and\n  Generalized Chiral Potts Models",
        "FEDS: Feature and Entropy-Based Distillation Strategy for Efficient\n  Learned Image Compression",
        "Selection from Hierarchical Data with Conformal e-values",
        "Generating logical magic states with the aid of non-Abelian topological\n  order",
        "Minimizing Human-Induced Variability in Quantitative Angiography for\n  Robust and Explainable AI-Based Occlusion Prediction",
        "Investigating the shadows of new regular black holes with a Minkowski\n  core: Effects of spherical accretion and core type differences",
        "Sparks of cognitive flexibility: self-guided context inference for\n  flexible stimulus-response mapping by attentional routing",
        "A quantitative sampling method for elastic and electromagnetic sources",
        "Turbulence-Induced Fluctuating Interfaces in Heterogeneously-Active\n  Suspensions",
        "On the learning power of Friedman-Stanley jumps",
        "Magnetic Field Structures In and Around Seyfert Galaxy Outflows",
        "A new cutoff criterion for non-negatively curved chains",
        "SynLlama: Generating Synthesizable Molecules and Their Analogs with\n  Large Language Models",
        "Observation of the dimer-singlet phase in the one-dimensional S = 1\/2\n  Heisenberg antiferromagnet Cu(Ampy)ClBr (Ampy= C6H8N2 =\n  2-(Aminomethyl)pyridine)",
        "Interpretable and Equation-Free Response Theory for Complex Systems",
        "A Web-Based Application Leveraging Geospatial Information to Automate\n  On-Farm Trial Design",
        "Genesis of the James Webb Space Telescope Architecture: The Designers'\n  Story",
        "Hypoelliptic Regularization in the Obstacle Problem for the Kolmogorov\n  Operator",
        "Exploring the Reform and Development Pathways of AIIB's Climate\n  Accountability Mechanism in the Context of Global Climate Governance",
        "An integral transformation approach to differential games: a climate\n  model application",
        "UNGT: Ultrasound Nasogastric Tube Dataset for Medical Image Analysis"
      ],
      "abstract":[
        "Quantum sensing with nitrogen-vacancy centers in diamond has emerged as a\npowerful tool for measuring diverse physical parameters, yet the versatility of\nthese measurement approaches is often limited by the achievable layout and\ndimensionality of bulk-crystal platforms. Here, we demonstrate a versatile\napproach to creating designer quantum sensors by surface-functionalizing\nmultiphoton lithography microstructures with NV-containing nanodiamonds. We\nshowcase this capability by fabricating a 150 $\\mu$m x 150 $\\mu$m x 150 $\\mu$m\ntriply periodic minimal surface gyroid structure with millions of attached\nnanodiamonds. We demonstrate a means to volumetrically image these structures\nusing a refractive index matching confocal imaging technique, and extract ODMR\nspectra from 1.86 $\\mu$m x 1.86 $\\mu$m areas of highly concentrated\nnanodiamonds across a cross section of the gyroid. Furthermore, the high\ndensity of sensing elements enables ensemble temperature measurements with\nsensitivity of 0.548 {\\deg}K\/$\\sqrt{Hz}$ at 5 mW excitation power. This\napproach to creating quantum-enabled microarchitectures opens new possibilities\nfor multimodal sensing in complex three-dimensional environments.",
        "Weak $\\infty$-categories are known to be more expressive than their strict\ncounterparts, but are more difficult to work with, as constructions in such a\ncategory involve the manipulation of explicit coherence data. This motivates\nthe search for definitions of semistrict $\\infty$-categories, where some, but\nnot all, of the operations have been strictified.\n  We introduce a general framework for adding definitional equality to the type\ntheory $\\mathsf{Catt}$, a type theory whose models correspond to globular weak\n$\\infty$-categories, which was introduced by Finster and Mimram. Adding\nequality to this theory causes the models to exhibit semistrict behaviour,\ntrivialising some operations while leaving others weak. The framework consists\nof a generalisation of $\\mathsf{Catt}$ extended with an equality relation\ngenerated by an arbitrary set of equality rules $\\mathcal{R}$, which we name\n$\\mathsf{Catt}_{\\mathcal{R}}$. We study this framework in detail, formalising\nmuch of its metatheory in the proof assistant Agda, and studying how certain\noperations of $\\mathsf{Catt}$ behave in the presence of definitional equality.\n  We use this framework to introduce two type theories,\n$\\mathsf{Catt}_{\\mathsf{su}}$ and $\\mathsf{Catt}_{\\mathsf{sua}}$, which are\ninstances of this general framework. Further, we provide terminating and\nconfluent reduction systems that generate the equality of both systems. We\ntherefore prove that the equality, and hence typechecking, of both theories is\ndecidable. This is used to give an implementation of these type theories, which\nuses an approach inspired by normalisation by evaluation to efficiently find\nnormal forms for terms. We further introduce a bidirectional typechecking\nalgorithm used by the implementation which allows for terms to be defined in a\nconvenient syntax where many arguments can be left implicit.",
        "We construct a general theory of spin magnetization driven by chiral phonons\nunder an adiabatic process, in which atoms rotate around their equilibrium\npositions with a low phonon frequency. Here the spin magnetization originates\nfrom the modulated electronic states with spin-orbital coupling by atomic\nrotations. Under the adiabatic approximation, the time-dependent spin\nmagnetization can be calculated by a Berry-phase method. In this paper, we\nfocus on its time average, which is evaluated by assuming that the phonon\ndisplacement is small. As a result, the time average of the spin magnetization\nis concisely formulated in the form of the Berry curvature defined in the\nphonon-displacement space as an intrinsic property of atomic rotations. Our\nformula for spin magnetization reflects the chiral nature of phonons, and is\nconvenient for $ab$ $initio$ calculations.",
        "We consider the McKean-Vlasov equation $dX_t = b(t, X_t, [X_t])dt + \\sigma(t,\nX_t, [X_t])dW_t$ where $[X_t]$ is the law of $X_t$. We focus specifically on\nthe \"kinetic\" case, where the equation is degenerate in the sense that the\ndimension of the Brownian motion $W$ can be strictly less than the dimension of\nthe solution $X$. Assuming H\\\"older continuous coefficients and a\nhypoellipticity condition, we prove the well-posedness of the equation. This\nresult complements the existing literature by addressing the case where the\ndiffusion coefficient $\\sigma$ depends on the law $[X_t]$, which have been\npreviously unexplored. Our proof employs a novel technique that offers a\nsimplified and direct argument, eliminating the need for PDEs involving\nderivatives with respect to the measure argument and exploiting the\nsub-Riemannian metric structure induced by the corresponding Fokker-Planck\noperator.",
        "In this paper, we introduce a notion of derived involutive algebras in $ C_2\n$-Mackey functors which simultaneously generalize commutative rings with\ninvolution and the (non-equivariant) derived algebras of Bhatt--Mathew and\nRaksit. We show that the $ \\infty $-category of derived involutive algebras\nadmits involutive enhancements of the cotangent complexes, de Rham complex, and\nde Rham cohomology functors; furthermore, their real Hochschild homology is\ndefined. We identify a filtration on the real Hochschild homology of these\nderived involutive algebras via a universal property and show that its\nassociated graded may be identified with the involutive de Rham complex. Using\n$ C_2 $-$ \\infty $-categories of Barwick--Dotto--Glasman--Nardin--Shah, we show\nthat our filtered real Hochschild homology specializes to the HKR-filtered\nHochschild homology considered by Raksit.",
        "This study employs spectral methods to capture the behaviour of wave equation\nwith dispersive-nonlinearity. We describe the evolution of hump initial data\nand track the conservation of the mass and energy functionals. The\ndispersive-nonlinearity results to solution in an extended Schwartz space via\nanalytic approach. We construct numerical schemes based on spectral methods to\nsimulate soliton interactions under Schwartzian initial data. The computational\nanalysis includes validation of energy and mass conservation to ensure\nnumerical accuracy. Results show that initial data from the Schwartz space\ndecompose into smaller wave-packets due to the weaker dispersive-nonlinearity\nbut leads to wave collapse as a result of stronger dispersive-nonlinearity. We\nconjecture that the hyperbolic equation with a positive nonlinearity and\nexponent greater or equal 2 admits global solutions, while lower exponents lead\nto localized solutions. A stability analysis of solitonic solutions of the\nequation is provided via the perturbation approach.",
        "Recent approaches to the theory of dynamic programming view dynamic programs\nas families of policy operators acting on partially ordered sets. In this\npaper, we extend these ideas by shifting from arbitrary partially ordered sets\nto ordered vector space. The advantage of working in this setting is that\nordered vector spaces have well integrated algebric and order structure, which\nleads to sharper fixed point results. These fixed point results can then be\nexploited to obtain strong optimality properties. We illustrate our results\nthrough a range of applications, including new findings for several useful\nmodels.",
        "Mrk 421 was in its most active state around early 2010, which led to the\nhighest TeV gamma-ray flux ever recorded from any active galactic nuclei. We\naim to characterize the multiwavelength behavior during this exceptional year\nfor Mrk 421, and evaluate whether it is consistent with the picture derived\nwith data from other less exceptional years. We investigated the period from\nNovember 5, 2009, (MJD 55140) until July 3, 2010, (MJD 55380) with extensive\ncoverage from very-high-energy (VHE; E$\\,>\\,$100$\\,$GeV) gamma rays to radio\nwith MAGIC, VERITAS, Fermi-LAT, RXTE, Swift, GASP-WEBT, VLBA, and a variety of\nadditional optical and radio telescopes. We investigated the variability and\ncorrelation behavior among different energy bands in great detail. We find the\nstrongest variability in X-rays and VHE gamma rays, and PSDs compatible with\npower-law functions. We observe strong correlations between X-rays and VHE\ngamma rays. We also report a marginally significant positive correlation\nbetween high-energy (HE; E$\\,>\\,$100$\\,$MeV) gamma rays and the ultraviolet\nband. We detected marginally significant correlations between the HE and VHE\ngamma rays, and between HE gamma rays and the X-ray, that disappear when the\nlarge flare in February 2010 is excluded from the correlation study. The\nactivity of Mrk 421 also yielded the first ejection of features in the VLBA\nimages of the jet of Mrk 421. Yet the large uncertainties in the ejection times\nof these radio features prevent us from firmly associating them to the specific\nflares recorded during the campaign. We also show that the collected\nmulti-instrument data are consistent with a scenario where the emission is\ndominated by two regions, a compact and extended zone, which could be\nconsidered as a simplified implementation of an energy-stratified jet as\nsuggested by recent IXPE observations.",
        "We study the relation between spectral data of magnetic monopoles in\nhyperbolic space and the curve of the spectral parameter of generalized chiral\nPotts models (gCPM) through the lens of (semi-)holomorphic field theories. We\nrealize the identification of the data on the two sides, which we call the\nhyperbolic monopole\/gCPM correspondence. For the group $\\text{SU}(2)$, this\ncorrespondence had been observed by Atiyah and Murray in the 80s. Here, we\nrevisit and generalize this correspondence and establish its origin. By\ninvoking the work of Murray and Singer on hyperbolic monopoles, we first\ngeneralize the observation of Atiyah and Murray to the group $\\text{SU}(n)$. We\nthen propose a technology to engineer gCPM within the 4d Chern-Simons (CS)\ntheory, which explains various features of the model, including the lack of\nrapidity-difference property of its R-matrix and its peculiarity of having a\ngenus$\\,\\ge 2$ curve of the spectral parameter. Finally, we investigate the\norigin of the correspondence. We first clarify how the two sides of the\ncorrespondence can be realized from the 6d holomorphic CS theory on\n$\\mathbb{P}S(M)$, the projective spinor bundle of the Minkowski space\n$M=\\mathbb{R}^{1,3}$, for hyperbolic $\\text{SU}(n)$-monopoles, and the\nEuclidean space $M=\\mathbb{R}^4$, for the gCPM. We then establish that\n$\\mathbb{P}S(M)$ can be holomorphically embedded into\n$\\mathbb{P}S(\\mathbb{C}^{1,3})$, the projective spinor bundle of\n$\\mathbb{C}^{1,3}$, of complex dimension five with a fixed complex structure.\nWe finally explain how the 6d CS theory on $\\mathbb{P}S(M)$ can be realized as\nthe dimensional reduction of the 10d holomorphic CS theory on\n$\\mathbb{P}S(\\mathbb{C}^{1,3})$. As the latter theory is only sensitive to the\ncomplex structure of $\\mathbb{P}S(\\mathbb{C}^{1,3})$, which has been fixed, we\nrealize the correspondence as two incarnations of the same physics in ten\ndimensions.",
        "Learned image compression (LIC) methods have recently outperformed\ntraditional codecs such as VVC in rate-distortion performance. However, their\nlarge models and high computational costs have limited their practical\nadoption. In this paper, we first construct a high-capacity teacher model by\nintegrating Swin-Transformer V2-based attention modules, additional residual\nblocks, and expanded latent channels, thus achieving enhanced compression\nperformance. Building on this foundation, we propose a \\underline{F}eature and\n\\underline{E}ntropy-based \\underline{D}istillation \\underline{S}trategy\n(\\textbf{FEDS}) that transfers key knowledge from the teacher to a lightweight\nstudent model. Specifically, we align intermediate feature representations and\nemphasize the most informative latent channels through an entropy-based loss. A\nstaged training scheme refines this transfer in three phases: feature\nalignment, channel-level distillation, and final fine-tuning. Our student model\nnearly matches the teacher across Kodak (1.24\\% BD-Rate increase), Tecnick\n(1.17\\%), and CLIC (0.55\\%) while cutting parameters by about 63\\% and\naccelerating encoding\/decoding by around 73\\%. Moreover, ablation studies\nindicate that FEDS generalizes effectively to transformer-based networks. The\nexperimental results demonstrate our approach strikes a compelling balance\namong compression performance, speed, and model parameters, making it\nwell-suited for real-time or resource-limited scenarios.",
        "Distribution-free predictive inference beyond the construction of prediction\nsets has gained a lot of interest in recent applications. One such application\nis the selection task, where the objective is to design a reliable selection\nrule to pick out individuals with desired unobserved outcomes while controlling\nthe error rate. In this work, we address the selection problem in the context\nof hierarchical data, where groups of observations may exhibit distinct\nwithin-group distributions. This generalizes existing techniques beyond the\nstandard i.i.d.\/exchangeable data settings. As a correction, For hierarchical\ndata, we introduce methods to construct valid conformal e-values, enabling\ncontrol of the false discovery rate (FDR) through the e-BH procedure. In\nparticular, we introduce and compare two approaches -- subsampling conformal\ne-values and hierarchical conformal e-values. Empirical results demonstrate\nthat both approaches achieve valid FDR control while highlighting a tradeoff\nbetween stability and power. The subsampling-based method, though random,\ntypically offers higher power, whereas the hierarchical approach, being\ndeterministic, tends to be slightly less powerful. The effectiveness of the\nproposed methods is illustrated in two real-world applications.",
        "In fault-tolerant quantum computing with the surface code, non-Clifford gates\nare crucial for universal computation. However, implementing these gates using\nmethods like magic state distillation and code switching requires significant\nresources. In this work, we propose a new protocol that combines magic state\npreparation and code switching to realize logical non-Clifford operations with\nthe potential for fault tolerance. Our approach begins with a special logical\nstate in the $\\mathbb{Z}_4$ surface code. By applying a sequence of\ntransformations, the system goes through different topological codes, including\nthe non-Abelian $D_4$ quantum double model. This process ultimately produces a\nmagic state in a condensed $\\mathbb{Z}_2$ surface code, which enables the\nimplementation of a logical $T$ gate in the standard $\\mathbb{Z}_2$ surface\ncode. In our analysis, we employ a framework where the topological codes are\nrepresented by their topological orders and all the transformations are\nconsidered as topological manipulations such as gauging symmetries and\ncondensing anyons. This perspective is particularly useful for understanding\ncode switching between topological codes.",
        "Bias from contrast injection variability is a significant obstacle to\naccurate intracranial aneurysm occlusion prediction using quantitative\nangiography and deep neural networks . This study explores bias removal and\nexplainable AI for outcome prediction. This study used angiograms from 458\npatients with flow diverters treated IAs with six month follow up defining\nocclusion status. We minimized injection variability by deconvolving the parent\nartery input to isolate the impulse response of aneurysms, then reconvolving it\nwith a standardized injection curve. A deep neural network trained on these QA\nderived biomarkers predicted six month occlusion. Local Interpretable Model\nAgnostic Explanations identified the key imaging features influencing the\nmodel, ensuring transparency and clinical relevance.",
        "We investigate the shadows and optical appearances of a new type of regular\nblack holes (BHs) with a Minkowski core under different spherical accretion.\nThese BHs are constructed by modifying the Newtonian potential based on the\nminimum observable length in the Generalized Uncertainty Principle (GUP). They\ncorrespond one-to-one with the traditional regular BHs with a de-Sitter (dS)\ncore (such as Bardeen\/Hayward BHs), characterized by quantum gravity effect\nparameter ($\\alpha_0$) and spacetime deformation factor ($n$). We find that the\ncharacteristic parameters give rise to some novel observable features. For\nthese new BHs, the shadow radius, photon sphere radius, and total observed\nintensity increase with the increase of $\\alpha_0$ but decrease with the\nincrease of $n$. Under different spherical accretion, the shadow and photon\nsphere radius are identical, but the total observed intensity under the static\nspherical accretion is greater than that under the infalling spherical\naccretion. In addition, we find that these regular BHs with different cores\nshow differences in shadows and optical appearances, especially under the\nstatic spherical accretion. Compared with Bardeen BH, the new BH has a smaller\ntotal observed intensity, dimmer maximum luminosity, and smaller shadow and\nphoton sphere radius. The larger $\\alpha_0$ leads to more significant\ndifferences, and a similar trend is also seen in the comparison with Hayward\nBH. Under the infalling spherical accretion, these regular BHs with different\ncores only have slight differences in total observed intensity, shadow and\nphoton sphere radius, which become more obvious when $\\alpha_0$ is relatively\nlarge. It suggests that the unique spacetime features of these regular BHs with\ndifferent cores can be distinguished through astronomical observations.",
        "Flexible cognition demands discovering hidden rules to quickly adapt\nstimulus-response mappings. Standard neural networks struggle in such tasks\nrequiring rapid, context-driven remapping. Recently, Hummos (2023) introduced a\nfast-and-slow learning algorithm to mitigate this shortcoming, but its\nscalability to complex, image-computable tasks was unclear. Here, we propose\nthe Wisconsin Neural Network (WiNN), which extends Hummos' fast-and-slow\nlearning to image-computable tasks demanding flexible rule-based behavior. WiNN\nemploys a pretrained convolutional neural network for vision, coupled with an\nadjustable \"context state\" that guides attention to relevant features. If WiNN\nproduces an incorrect response, it first iteratively updates its context state\nto refocus attention on task-relevant cues, then performs minimal parameter\nupdates to attention and readout layers. This strategy preserves generalizable\nrepresentations in the sensory and attention networks, reducing catastrophic\nforgetting. We evaluate WiNN on an image-based extension of the Wisconsin Card\nSorting Task, revealing several markers of cognitive flexibility: (i) WiNN\nautonomously infers underlying rules, (ii) requires fewer examples to do so\nthan control models reliant on large-scale parameter updates, (iii) can perform\ncontext-based rule inference solely via context-state adjustments-further\nenhanced by slow updates of attention and readout parameters, and (iv)\ngeneralizes to unseen compositional rules through context-state updates alone.\nBy blending fast context inference with targeted attentional guidance, WiNN\nachieves \"sparks\" of flexibility. This approach offers a path toward\ncontext-sensitive models that retain knowledge while rapidly adapting to\ncomplex, rule-based tasks.",
        "This work is dedicated to a novel sampling method for accurately\nreconstructing elastic and electromagnetic sources from the far field patterns.\nWe show that the proposed indicators in the form of integrals with full far\nfield patterns are exactly the source functions. These facts not only give\nconstructive uniqueness proofs of the inverse source problems, but also\nestablish the theoretical basis of the proposed sampling methods. Furthermore,\nwe derive the stability estimates for the corresponding discrete indicators\nusing the far field patterns with finitely many observations and frequencies.\nWe have also proposed the indicators with partial far field patterns and proved\ntheir validity for providing the derivative information of the unknown sources.\nNumerical examples are presented to verify the accuracy and stability of the\nproposed quantitative sampling method.",
        "We investigate the effects of heterogeneous (spatially varying) activity in a\nhydrodynamical model for dense bacterial suspensions, confining ourselves to\nexperimentally realizable, simple, quenched, activity patterns. We show that\nthe evolution of the bacterial velocity field under such activity patterning\nleads to the emergence of hydrodynamic interfaces separating spatially\nlocalized turbulence from jammed frictional surroundings. We characterise the\nintermittent and multiscale fluctuations of this interface and also investigate\nhow heterogeneity influences mixing via the residence times of Lagrangian\ntracers. This work reveals how naturally occurring heterogeneities could\ndecisively steer active flows into more complex configurations than those\ntypically studied, opening up parallels to droplet dynamics, front propagation\nand turbulent mixing layers.",
        "Recently, a surprising connection between algorithmic learning of algebraic\nstructures and descriptive set theory has emerged. Following this line of\nresearch, we define the learning power of an equivalence relation $E$ on a\ntopological space as the class of isomorphism relations with countably many\nequivalence classes that are continuously reducible to $E$. In this paper, we\ndescribe the learning power of the finite Friedman-Stanley jumps of\n$=_{\\mathbb{N}}$ and $=_{\\mathbb{N}^\\mathbb{N}}$, proving that these\nequivalence relations learn the families of countable structures that are\npairwise distinguished by suitable infinitary sentences. Our proof techniques\nintroduce new ideas for assessing the continuous complexity of Borel\nequivalence relations.",
        "We present radio polarimetric images of 12 Seyfert and Low-Ionization Nuclear\nEmission-line Region (LINER) galaxies belonging to the Centre for Astrophysics\n(CfA)+12 micron sample exhibiting kiloparsec-scale radio outflows (KSRs). These\nobservations have been carried out at 10 GHz with Karl G. Jansky Very Large\nArray (VLA) in D-array and at 1.4 GHz with the BnA$\\rightarrow$A array\nconfigurations. We find signatures of organized magnetic (B-) field structures\nin the cores, jets and lobes of these galaxies. The linear polarization\nfraction varies from a few per cent in the cores to $47\\pm18$ per cent in the\nlobes. The inferred B-fields are toroidal in the cores of several sources\nmaking them consistent with the presence of either a sheath-like or a wind-like\ncomponent surrounding the jet. The in-band spectral index images typically show\nthe presence of flat\/inverted spectrum cores and steep spectrum lobes. Radio\ncores with flatter spectra are found to have lower Eddington ratios while the\nsteeper ones have higher. A strong correlation is observed between the\nSeyfert\/LINER radio outflow properties and the mass of the supermassive black\nholes (SMBHs); correlations with Eddington ratios are weaker. We find\nsignatures of jet-medium interaction and both positive and negative AGN\nfeedback in these sources. Overall, our study indicates that radio-quiet (RQ)\nAGN with KSRs possess radio outflows driven by magnetic fields anchored to\ntheir black holes - accretion disks, which significantly impact their\nenvironments.",
        "The cutoff phenomenon was recently shown to systematically follow from\nnon-negative curvature and the product condition, for all Markov diffusions.\nThe proof crucially relied on a classical \\emph{chain rule} satisfied by the\ncarr\\'e du champ operator, which is specific to differential generators and\nhence fails on discrete spaces. In the present paper, we show that an\napproximate version of this chain rule in fact always holds, with an extra cost\nthat depends on the log-Lipschitz regularity of the considered observable. As a\nconsequence, we derive a new cutoff criterion for non-negatively curved chains\non finite spaces. The latter allows us to recover, in a simple and unified way,\na number of historical instances of cutoff that had been established through\nmodel-specific arguments. Emblematic examples include random walk on the\nhypercube, random transpositions, random walk on the multislice, or MCMC\nsamplers for popular spin systems such as the Ising and Hard-core models on\nbounded-degree graphs.",
        "Generative machine learning models for small molecule drug discovery have\nshown immense promise, but many molecules generated by this approach are too\ndifficult to synthesize to be worth further investigation or further\ndevelopment. We present a novel approach by fine-tuning Meta's Llama3 large\nlanguage models (LLMs) to create SynLlama, which generates full synthetic\npathways made of commonly accessible Enamine building blocks and robust organic\nreaction templates. SynLlama explores a large synthesizable space using\nsignificantly less data compared to other state-of-the-art methods, and offers\nstrong performance in bottom-up synthesis, synthesizable analog generation, and\nhit expansion, offering medicinal chemists a valuable tool for drug discovery\ndevelopments. We find that SynLlama can effectively generalize to unseen yet\npurchasable building blocks, meaning that its reconstruction capabilities\nextend to a broader synthesizable chemical space than the training data.",
        "Spin-1\/2 Heisenberg antiferromagnetic frustrated spin chain systems display\nexotic ground states with unconventional excitations and distinct quantum phase\ntransitions as the ratio of next-nearest-neighbor to nearest-neighbor coupling\nis tuned. We present a comprehensive investigation of the structural, magnetic,\nand thermodynamics properties of the spin-1\/2 compound, Cu(Ampy)ClBr (Ampy=\nC6H8N2 = 2-(Aminomethyl)pyridine) via x-ray diffraction, magnetization,\nspecific heat, 1H nuclear magnetic resonance (NMR), electron spin resonance\n(ESR), and muon spin relaxation (muSR) techniques. The crystal structure\nfeatures an anisotropic triangular chain lattice of magnetic Cu2+ ions. Our\nbulk and local probe experiments detect neither long-range magnetic ordering\nnor spin freezing down to 0.06 K despite the presence of moderate\nantiferromagnetic interaction between Cu2+ spins as reflected by a Curie-Weiss\ntemperature of about -9 K from the bulk susceptibility data. A broad maximum is\nobserved at about 9 K in magnetic susceptibility and specific heat data,\nindicating the onset of short-range spin correlations. At low temperatures, the\nzero-field magnetic specific heat and the 1H NMR spin-lattice relaxation rate\nfollow an exponential temperature dependence, indicating the presence of gapped\nmagnetic excitations. Furthermore, persistent spin dynamics down to 0.088 K\nobserved by zero-field muSR evidences lack of any static magnetism. We\nattribute these experimental results to the stabilization of a dimer-singlet\nphase in the presence of a next-near neighbor interaction and of a randomness\nin the exchange coupling driven by Cl\/Br mixing.",
        "Response theory provides a pathway for understanding the sensitivity of a\nsystem and, more in general, to predict how its statistical properties change\nas a possibly time-dependent perturbation is applied. Recently discovered\ngeneral forms of the celebrated Fluctuation-Dissipation Theorem allow for\nexpressing response operators as correlation functions of suitably defined\nobservables in the unperturbed state, also when such a state is far from\nequilibrium. In the case of complex and multiscale systems, to achieved\nenhanced practical applicability, response theory must be interpretable,\ncapable of focusing of relevant timescales, and amenable to implemented by\ndata-driven approaches that are potentially equation-agnostic. Complex systems\ntypically exhibit a hierarchy of temporal behaviors, and unresolved or\nundesired timescales can obscure the dominant mechanisms driving macroscopic\nresponses. As an element of this desired framework, in the spirit of Markov\nstate modelling, we propose here a comprehensive analysis of the linear and\nnonlinear response of Markov chains to general time-dependent perturbations. We\nobtain simple and easily implementable formulas that can be used to predict the\nresponse of observables as well as higher-order correlations of the system. The\nmethodology proposed here can be implemented in a purely data-driven setting\nand even if we do not know the underlying evolution equations. The use of\nalgebraic expansions inspired by Koopmanism allow to elucidate the role of\ndifferent time scales and find explicit and interpretable expressions for the\nGreen's functions at all orders. This is a major advantage of the framework\nproposed here. We illustrate our methodology in a very simple yet instructive\nmetastable system. Finally, our results provide a dynamical foundation for the\nProny method, which is commonly used for the statistical analysis of discrete\ntime signals.",
        "On-farm sensor data have allowed farmers to implement field management\ntechniques and intensively track the corresponding responses. These data\ncombined with historical records open the door for real-time field management\nimprovements with the help of current advancements in computing power. However,\ndespite these advances, the statistical design of experiments is rarely used to\nevaluate the performance of field management techniques accurately.\nTraditionally, randomized block design is prevalent in statistical designs of\nfield trials, but in practice it is limited in dealing with large variations in\nsoil classes, management practices, and crop varieties. More specifically,\nalthough this experimental design is suited for most trial types, it is not the\noptimal choice when multiple factors are tested over multifarious natural\nvariations in farms, due to the economic constraints caused by the sheer number\nof variables involved. Experimental refinement is required to better estimate\nthe effects of the primary factor in the presence of auxiliary factors. In this\nway, farmers can better understand the characteristics and limitations of the\nprimary factor. This work presents a framework for automating the analysis of\nlocal field variations by fusing soil classification data and lidar topography\ndata with historical yield. This framework will be leveraged to automate the\ndesigning of field experiments based on multiple topographic features",
        "The James Webb Space Telescope, launched in 2021, is an infrared observatory\nof novel design: deployable, with active optics, fully open to space for\nradiative cooling and orbiting the Lagrange point no. 2. This article explains\nthe rationale leading to this specific design and describes the various other\narchitectures that were considered along the way: from a monolithic 10-meter\ntelescope in geosynchronous orbit to a 6-meter one in High Earth Orbit, then a\n16-meter observatory on the Moon, a 4- or 6-meter one in an elliptical\nheliocentric orbit, and a segmented 8-meter one passively cooled to 50 K at L2,\nwhich was finally descoped to 6.6 meters. It also addresses the optimization\nfor scientific performance, the challenge of dealing with such an ultra-low\noperating temperature, cost issues, supporting technology, modifications made\nduring final design and, finally, how the architecture performs on orbit.",
        "We study the obstacle problem associated with the Kolmogorov operator\n$\\Delta_v - \\partial_t - v\\cdot\\nabla_x$, which arises from the theory of\noptimal control in Asian-American options pricing models.\n  Our first main contribution is to improve the known regularity of solutions,\nfrom $C^{0,1}_t \\cap C^{0,2\/3}_x \\cap C^{1,1}_v$ to $C^{0,1}_{t,x} \\cap\nC^{1,1}_v$. The previous result in the literature, which has been called\noptimal, corresponds to $C^{1,1}$ regularity with respect to the Kolmogorov\ndistance. This is the expected regularity for solutions to obstacle problems.\nOur unexpected improvement of regularity in the $x$ variable is obtained using\nBernstein's technique and an approach drawing on ideas from Evans-Krylov\ntheory.\n  We then use this improvement in regularity of the solution to prove the first\nknown free boundary regularity result. We show that under a standard thickness\ncondition, the free boundary is a $C^{0,1\/2}_{t,x} \\cap C^{0,1}_v$ regular\nsurface. This result constitutes the first step in the program of free boundary\nregularity. Critically, our arguments rely on a new monotonicity formula and a\ncommutator estimate that are only made possible by the solution's enhanced\nregularity in $x$.",
        "After the Asian Infrastructure Investment Bank (AIIB) revised its\nEnvironmental and Social Framework, it has committed to certain climate-related\nobjectives, yet an independent climate accountability mechanism has not been\nestablished. The absence of clear evaluation principles and procedural rules\npresents challenges in effectively addressing environmental investment\ndisputes. This article reviews both domestic and international literature\nrelated to AIIB's climate accountability mechanism, identifying that the\ncurrent Environmental and Social Framework's principled content and reliance on\ntraditional approaches have resulted in issues of enforceability and the\nabsence of an independent accountability institution. To enhance the\neffectiveness of AIIB's climate accountability mechanism, a reassessment of its\ndevelopment path is necessary. Future developments should transition from an\nadditive path to a substitutive path, focusing on the application of\ninternational environmental and social standards, increasing stakeholder\nrecognition and participation, promoting comprehensive reforms within AIIB, and\nestablishing a coordinated independent accountability system. These measures\naim to support the robust development of AIIB's climate accountability\nmechanism.",
        "We develop an Integral Transformation Method (ITM) for the study of suitable\noptimal control and differential game models. This allows for a solution to\nsuch dynamic problems to be found through solving a family of optimization\nproblems parametrized by time. The method is quite flexible, and it can be used\nin several economic applications where the state equation and the objective\nfunctional are linear in a state variable. We illustrate the ITM in the context\nof a two-country integrated assessment climate model. We characterize\nemissions, consumption, transfers, and welfare by computing the Nash equilibria\nof the associated dynamic game. We then compare them to efficiency benchmarks.\nFurther, we apply the ITM in a robust control setup, where we investigate how\n(deep) uncertainty affects climate outcomes.",
        "We develop a novel ultrasound nasogastric tube (UNGT) dataset to address the\nlack of public nasogastric tube datasets. The UNGT dataset includes 493 images\ngathered from 110 patients with an average image resolution of approximately\n879 $\\times$ 583. Four structures, encompassing the liver, stomach, tube, and\npancreas are precisely annotated. Besides, we propose a semi-supervised\nadaptive-weighting aggregation medical segmenter to address data limitation and\nimbalance concurrently. The introduced adaptive weighting approach tackles the\nsevere unbalanced challenge by regulating the loss across varying categories as\ntraining proceeds. The presented multiscale attention aggregation block\nbolsters the feature representation by integrating local and global contextual\ninformation. With these, the proposed AAMS can emphasize sparse or small\nstructures and feature enhanced representation ability. We perform extensive\nsegmentation experiments on our UNGT dataset, and the results show that AAMS\noutperforms existing state-of-the-art approaches to varying extents. In\naddition, we conduct comprehensive classification experiments across varying\nstate-of-the-art methods and compare their performance. The dataset and code\nwill be available upon publication at https:\/\/github.com\/NUS-Tim\/UNGT."
      ]
    }
  },
  {
    "id":2412.00663,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"ConTEXTual Net: A Multimodal Vision-Language Model for Segmentation of Pneumothorax",
    "start_abstract":"Radiology narrative reports often describe characteristics of a patient's disease, including its location, size, and shape. Motivated by the recent success multimodal learning, we hypothesized that this descriptive text could guide medical image analysis algorithms. We proposed novel vision-language model, ConTEXTual Net, for task pneumothorax segmentation on chest radiographs. Net extracts language features from physician-generated free-form radiology using pre-trained model. then introduced cross-attention between intermediate embeddings an encoder-decoder convolutional neural network to enable guidance analysis. was trained CANDID-PTX dataset consisting 3196 positive cases with annotations 6 different physicians as well clinical reports. Using cross-validation, achieved Dice score 0.716\u00b10.016, which similar degree inter-reader variability (0.712\u00b10.044) computed subset data. It outperformed vision-only models (Swin UNETR: 0.670\u00b10.015, ResNet50 U-Net: 0.677\u00b10.015, GLoRIA: 0.686\u00b10.014, nnUNet 0.694\u00b10.016) competing model (LAVT: 0.706\u00b10.009). Ablation studies confirmed it information led performance gains. Additionally, show certain augmentation methods degraded Net's breaking image-text concordance. also evaluated effects activation functions in module, highlighting efficacy our chosen architectural design.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "MRI-Guided Adaptive Radiation Therapy"
      ],
      "abstract":[
        "Magnetic resonance imaging-guided radiation therapy (MRIgRT) has improved soft tissue contrast over computed tomography (CT) based image-guided RT. Superior visualization of the target and surrounding radiosensitive structures has the potential to improve oncological outcomes partly due to safer dose-escalation and adaptive planning. In this review, we highlight the workflow of adaptive MRIgRT planning, which includes simulation imaging, daily MRI, identifying isocenter shifts, contouring, plan optimization, quality control, and delivery. Increased utilization of MRIgRT will depend on addressing technical limitations of this technology, while addressing treatment efficacy, cost-effectiveness, and workflow training."
      ],
      "categories":[
        "Oncology"
      ]
    },
    "list":{
      "title":[
        "Optically Detected Magnetic Resonance Imaging and Sensing Within\n  Functionalized Additively Manufactured Microporous Structures",
        "A type-theoretic approach to semistrict higher categories",
        "Theory of spin magnetization driven by chiral phonons",
        "Well-posedness of McKean-Vlasov equations under the weak H\\\"ormander\n  condition",
        "A filtered Hochschild-Kostant-Rosenberg theorem for real Hochschild\n  homology",
        "Spectral Analysis and Stability of Wave Equations with Dispersive\n  Nonlinearity",
        "Dynamic Programming in Ordered Vector Space",
        "Characterization of Markarian 421 during its most violent year:\n  Multiwavelength variability and correlations",
        "Hyperbolic Monopoles, (Semi-)Holomorphic Chern-Simons Theories, and\n  Generalized Chiral Potts Models",
        "FEDS: Feature and Entropy-Based Distillation Strategy for Efficient\n  Learned Image Compression",
        "Selection from Hierarchical Data with Conformal e-values",
        "Generating logical magic states with the aid of non-Abelian topological\n  order",
        "Minimizing Human-Induced Variability in Quantitative Angiography for\n  Robust and Explainable AI-Based Occlusion Prediction",
        "Investigating the shadows of new regular black holes with a Minkowski\n  core: Effects of spherical accretion and core type differences",
        "Sparks of cognitive flexibility: self-guided context inference for\n  flexible stimulus-response mapping by attentional routing",
        "A quantitative sampling method for elastic and electromagnetic sources",
        "Turbulence-Induced Fluctuating Interfaces in Heterogeneously-Active\n  Suspensions",
        "On the learning power of Friedman-Stanley jumps",
        "Magnetic Field Structures In and Around Seyfert Galaxy Outflows",
        "A new cutoff criterion for non-negatively curved chains",
        "SynLlama: Generating Synthesizable Molecules and Their Analogs with\n  Large Language Models",
        "Observation of the dimer-singlet phase in the one-dimensional S = 1\/2\n  Heisenberg antiferromagnet Cu(Ampy)ClBr (Ampy= C6H8N2 =\n  2-(Aminomethyl)pyridine)",
        "Interpretable and Equation-Free Response Theory for Complex Systems",
        "A Web-Based Application Leveraging Geospatial Information to Automate\n  On-Farm Trial Design",
        "Genesis of the James Webb Space Telescope Architecture: The Designers'\n  Story",
        "Hypoelliptic Regularization in the Obstacle Problem for the Kolmogorov\n  Operator",
        "Exploring the Reform and Development Pathways of AIIB's Climate\n  Accountability Mechanism in the Context of Global Climate Governance",
        "An integral transformation approach to differential games: a climate\n  model application",
        "UNGT: Ultrasound Nasogastric Tube Dataset for Medical Image Analysis"
      ],
      "abstract":[
        "Quantum sensing with nitrogen-vacancy centers in diamond has emerged as a\npowerful tool for measuring diverse physical parameters, yet the versatility of\nthese measurement approaches is often limited by the achievable layout and\ndimensionality of bulk-crystal platforms. Here, we demonstrate a versatile\napproach to creating designer quantum sensors by surface-functionalizing\nmultiphoton lithography microstructures with NV-containing nanodiamonds. We\nshowcase this capability by fabricating a 150 $\\mu$m x 150 $\\mu$m x 150 $\\mu$m\ntriply periodic minimal surface gyroid structure with millions of attached\nnanodiamonds. We demonstrate a means to volumetrically image these structures\nusing a refractive index matching confocal imaging technique, and extract ODMR\nspectra from 1.86 $\\mu$m x 1.86 $\\mu$m areas of highly concentrated\nnanodiamonds across a cross section of the gyroid. Furthermore, the high\ndensity of sensing elements enables ensemble temperature measurements with\nsensitivity of 0.548 {\\deg}K\/$\\sqrt{Hz}$ at 5 mW excitation power. This\napproach to creating quantum-enabled microarchitectures opens new possibilities\nfor multimodal sensing in complex three-dimensional environments.",
        "Weak $\\infty$-categories are known to be more expressive than their strict\ncounterparts, but are more difficult to work with, as constructions in such a\ncategory involve the manipulation of explicit coherence data. This motivates\nthe search for definitions of semistrict $\\infty$-categories, where some, but\nnot all, of the operations have been strictified.\n  We introduce a general framework for adding definitional equality to the type\ntheory $\\mathsf{Catt}$, a type theory whose models correspond to globular weak\n$\\infty$-categories, which was introduced by Finster and Mimram. Adding\nequality to this theory causes the models to exhibit semistrict behaviour,\ntrivialising some operations while leaving others weak. The framework consists\nof a generalisation of $\\mathsf{Catt}$ extended with an equality relation\ngenerated by an arbitrary set of equality rules $\\mathcal{R}$, which we name\n$\\mathsf{Catt}_{\\mathcal{R}}$. We study this framework in detail, formalising\nmuch of its metatheory in the proof assistant Agda, and studying how certain\noperations of $\\mathsf{Catt}$ behave in the presence of definitional equality.\n  We use this framework to introduce two type theories,\n$\\mathsf{Catt}_{\\mathsf{su}}$ and $\\mathsf{Catt}_{\\mathsf{sua}}$, which are\ninstances of this general framework. Further, we provide terminating and\nconfluent reduction systems that generate the equality of both systems. We\ntherefore prove that the equality, and hence typechecking, of both theories is\ndecidable. This is used to give an implementation of these type theories, which\nuses an approach inspired by normalisation by evaluation to efficiently find\nnormal forms for terms. We further introduce a bidirectional typechecking\nalgorithm used by the implementation which allows for terms to be defined in a\nconvenient syntax where many arguments can be left implicit.",
        "We construct a general theory of spin magnetization driven by chiral phonons\nunder an adiabatic process, in which atoms rotate around their equilibrium\npositions with a low phonon frequency. Here the spin magnetization originates\nfrom the modulated electronic states with spin-orbital coupling by atomic\nrotations. Under the adiabatic approximation, the time-dependent spin\nmagnetization can be calculated by a Berry-phase method. In this paper, we\nfocus on its time average, which is evaluated by assuming that the phonon\ndisplacement is small. As a result, the time average of the spin magnetization\nis concisely formulated in the form of the Berry curvature defined in the\nphonon-displacement space as an intrinsic property of atomic rotations. Our\nformula for spin magnetization reflects the chiral nature of phonons, and is\nconvenient for $ab$ $initio$ calculations.",
        "We consider the McKean-Vlasov equation $dX_t = b(t, X_t, [X_t])dt + \\sigma(t,\nX_t, [X_t])dW_t$ where $[X_t]$ is the law of $X_t$. We focus specifically on\nthe \"kinetic\" case, where the equation is degenerate in the sense that the\ndimension of the Brownian motion $W$ can be strictly less than the dimension of\nthe solution $X$. Assuming H\\\"older continuous coefficients and a\nhypoellipticity condition, we prove the well-posedness of the equation. This\nresult complements the existing literature by addressing the case where the\ndiffusion coefficient $\\sigma$ depends on the law $[X_t]$, which have been\npreviously unexplored. Our proof employs a novel technique that offers a\nsimplified and direct argument, eliminating the need for PDEs involving\nderivatives with respect to the measure argument and exploiting the\nsub-Riemannian metric structure induced by the corresponding Fokker-Planck\noperator.",
        "In this paper, we introduce a notion of derived involutive algebras in $ C_2\n$-Mackey functors which simultaneously generalize commutative rings with\ninvolution and the (non-equivariant) derived algebras of Bhatt--Mathew and\nRaksit. We show that the $ \\infty $-category of derived involutive algebras\nadmits involutive enhancements of the cotangent complexes, de Rham complex, and\nde Rham cohomology functors; furthermore, their real Hochschild homology is\ndefined. We identify a filtration on the real Hochschild homology of these\nderived involutive algebras via a universal property and show that its\nassociated graded may be identified with the involutive de Rham complex. Using\n$ C_2 $-$ \\infty $-categories of Barwick--Dotto--Glasman--Nardin--Shah, we show\nthat our filtered real Hochschild homology specializes to the HKR-filtered\nHochschild homology considered by Raksit.",
        "This study employs spectral methods to capture the behaviour of wave equation\nwith dispersive-nonlinearity. We describe the evolution of hump initial data\nand track the conservation of the mass and energy functionals. The\ndispersive-nonlinearity results to solution in an extended Schwartz space via\nanalytic approach. We construct numerical schemes based on spectral methods to\nsimulate soliton interactions under Schwartzian initial data. The computational\nanalysis includes validation of energy and mass conservation to ensure\nnumerical accuracy. Results show that initial data from the Schwartz space\ndecompose into smaller wave-packets due to the weaker dispersive-nonlinearity\nbut leads to wave collapse as a result of stronger dispersive-nonlinearity. We\nconjecture that the hyperbolic equation with a positive nonlinearity and\nexponent greater or equal 2 admits global solutions, while lower exponents lead\nto localized solutions. A stability analysis of solitonic solutions of the\nequation is provided via the perturbation approach.",
        "Recent approaches to the theory of dynamic programming view dynamic programs\nas families of policy operators acting on partially ordered sets. In this\npaper, we extend these ideas by shifting from arbitrary partially ordered sets\nto ordered vector space. The advantage of working in this setting is that\nordered vector spaces have well integrated algebric and order structure, which\nleads to sharper fixed point results. These fixed point results can then be\nexploited to obtain strong optimality properties. We illustrate our results\nthrough a range of applications, including new findings for several useful\nmodels.",
        "Mrk 421 was in its most active state around early 2010, which led to the\nhighest TeV gamma-ray flux ever recorded from any active galactic nuclei. We\naim to characterize the multiwavelength behavior during this exceptional year\nfor Mrk 421, and evaluate whether it is consistent with the picture derived\nwith data from other less exceptional years. We investigated the period from\nNovember 5, 2009, (MJD 55140) until July 3, 2010, (MJD 55380) with extensive\ncoverage from very-high-energy (VHE; E$\\,>\\,$100$\\,$GeV) gamma rays to radio\nwith MAGIC, VERITAS, Fermi-LAT, RXTE, Swift, GASP-WEBT, VLBA, and a variety of\nadditional optical and radio telescopes. We investigated the variability and\ncorrelation behavior among different energy bands in great detail. We find the\nstrongest variability in X-rays and VHE gamma rays, and PSDs compatible with\npower-law functions. We observe strong correlations between X-rays and VHE\ngamma rays. We also report a marginally significant positive correlation\nbetween high-energy (HE; E$\\,>\\,$100$\\,$MeV) gamma rays and the ultraviolet\nband. We detected marginally significant correlations between the HE and VHE\ngamma rays, and between HE gamma rays and the X-ray, that disappear when the\nlarge flare in February 2010 is excluded from the correlation study. The\nactivity of Mrk 421 also yielded the first ejection of features in the VLBA\nimages of the jet of Mrk 421. Yet the large uncertainties in the ejection times\nof these radio features prevent us from firmly associating them to the specific\nflares recorded during the campaign. We also show that the collected\nmulti-instrument data are consistent with a scenario where the emission is\ndominated by two regions, a compact and extended zone, which could be\nconsidered as a simplified implementation of an energy-stratified jet as\nsuggested by recent IXPE observations.",
        "We study the relation between spectral data of magnetic monopoles in\nhyperbolic space and the curve of the spectral parameter of generalized chiral\nPotts models (gCPM) through the lens of (semi-)holomorphic field theories. We\nrealize the identification of the data on the two sides, which we call the\nhyperbolic monopole\/gCPM correspondence. For the group $\\text{SU}(2)$, this\ncorrespondence had been observed by Atiyah and Murray in the 80s. Here, we\nrevisit and generalize this correspondence and establish its origin. By\ninvoking the work of Murray and Singer on hyperbolic monopoles, we first\ngeneralize the observation of Atiyah and Murray to the group $\\text{SU}(n)$. We\nthen propose a technology to engineer gCPM within the 4d Chern-Simons (CS)\ntheory, which explains various features of the model, including the lack of\nrapidity-difference property of its R-matrix and its peculiarity of having a\ngenus$\\,\\ge 2$ curve of the spectral parameter. Finally, we investigate the\norigin of the correspondence. We first clarify how the two sides of the\ncorrespondence can be realized from the 6d holomorphic CS theory on\n$\\mathbb{P}S(M)$, the projective spinor bundle of the Minkowski space\n$M=\\mathbb{R}^{1,3}$, for hyperbolic $\\text{SU}(n)$-monopoles, and the\nEuclidean space $M=\\mathbb{R}^4$, for the gCPM. We then establish that\n$\\mathbb{P}S(M)$ can be holomorphically embedded into\n$\\mathbb{P}S(\\mathbb{C}^{1,3})$, the projective spinor bundle of\n$\\mathbb{C}^{1,3}$, of complex dimension five with a fixed complex structure.\nWe finally explain how the 6d CS theory on $\\mathbb{P}S(M)$ can be realized as\nthe dimensional reduction of the 10d holomorphic CS theory on\n$\\mathbb{P}S(\\mathbb{C}^{1,3})$. As the latter theory is only sensitive to the\ncomplex structure of $\\mathbb{P}S(\\mathbb{C}^{1,3})$, which has been fixed, we\nrealize the correspondence as two incarnations of the same physics in ten\ndimensions.",
        "Learned image compression (LIC) methods have recently outperformed\ntraditional codecs such as VVC in rate-distortion performance. However, their\nlarge models and high computational costs have limited their practical\nadoption. In this paper, we first construct a high-capacity teacher model by\nintegrating Swin-Transformer V2-based attention modules, additional residual\nblocks, and expanded latent channels, thus achieving enhanced compression\nperformance. Building on this foundation, we propose a \\underline{F}eature and\n\\underline{E}ntropy-based \\underline{D}istillation \\underline{S}trategy\n(\\textbf{FEDS}) that transfers key knowledge from the teacher to a lightweight\nstudent model. Specifically, we align intermediate feature representations and\nemphasize the most informative latent channels through an entropy-based loss. A\nstaged training scheme refines this transfer in three phases: feature\nalignment, channel-level distillation, and final fine-tuning. Our student model\nnearly matches the teacher across Kodak (1.24\\% BD-Rate increase), Tecnick\n(1.17\\%), and CLIC (0.55\\%) while cutting parameters by about 63\\% and\naccelerating encoding\/decoding by around 73\\%. Moreover, ablation studies\nindicate that FEDS generalizes effectively to transformer-based networks. The\nexperimental results demonstrate our approach strikes a compelling balance\namong compression performance, speed, and model parameters, making it\nwell-suited for real-time or resource-limited scenarios.",
        "Distribution-free predictive inference beyond the construction of prediction\nsets has gained a lot of interest in recent applications. One such application\nis the selection task, where the objective is to design a reliable selection\nrule to pick out individuals with desired unobserved outcomes while controlling\nthe error rate. In this work, we address the selection problem in the context\nof hierarchical data, where groups of observations may exhibit distinct\nwithin-group distributions. This generalizes existing techniques beyond the\nstandard i.i.d.\/exchangeable data settings. As a correction, For hierarchical\ndata, we introduce methods to construct valid conformal e-values, enabling\ncontrol of the false discovery rate (FDR) through the e-BH procedure. In\nparticular, we introduce and compare two approaches -- subsampling conformal\ne-values and hierarchical conformal e-values. Empirical results demonstrate\nthat both approaches achieve valid FDR control while highlighting a tradeoff\nbetween stability and power. The subsampling-based method, though random,\ntypically offers higher power, whereas the hierarchical approach, being\ndeterministic, tends to be slightly less powerful. The effectiveness of the\nproposed methods is illustrated in two real-world applications.",
        "In fault-tolerant quantum computing with the surface code, non-Clifford gates\nare crucial for universal computation. However, implementing these gates using\nmethods like magic state distillation and code switching requires significant\nresources. In this work, we propose a new protocol that combines magic state\npreparation and code switching to realize logical non-Clifford operations with\nthe potential for fault tolerance. Our approach begins with a special logical\nstate in the $\\mathbb{Z}_4$ surface code. By applying a sequence of\ntransformations, the system goes through different topological codes, including\nthe non-Abelian $D_4$ quantum double model. This process ultimately produces a\nmagic state in a condensed $\\mathbb{Z}_2$ surface code, which enables the\nimplementation of a logical $T$ gate in the standard $\\mathbb{Z}_2$ surface\ncode. In our analysis, we employ a framework where the topological codes are\nrepresented by their topological orders and all the transformations are\nconsidered as topological manipulations such as gauging symmetries and\ncondensing anyons. This perspective is particularly useful for understanding\ncode switching between topological codes.",
        "Bias from contrast injection variability is a significant obstacle to\naccurate intracranial aneurysm occlusion prediction using quantitative\nangiography and deep neural networks . This study explores bias removal and\nexplainable AI for outcome prediction. This study used angiograms from 458\npatients with flow diverters treated IAs with six month follow up defining\nocclusion status. We minimized injection variability by deconvolving the parent\nartery input to isolate the impulse response of aneurysms, then reconvolving it\nwith a standardized injection curve. A deep neural network trained on these QA\nderived biomarkers predicted six month occlusion. Local Interpretable Model\nAgnostic Explanations identified the key imaging features influencing the\nmodel, ensuring transparency and clinical relevance.",
        "We investigate the shadows and optical appearances of a new type of regular\nblack holes (BHs) with a Minkowski core under different spherical accretion.\nThese BHs are constructed by modifying the Newtonian potential based on the\nminimum observable length in the Generalized Uncertainty Principle (GUP). They\ncorrespond one-to-one with the traditional regular BHs with a de-Sitter (dS)\ncore (such as Bardeen\/Hayward BHs), characterized by quantum gravity effect\nparameter ($\\alpha_0$) and spacetime deformation factor ($n$). We find that the\ncharacteristic parameters give rise to some novel observable features. For\nthese new BHs, the shadow radius, photon sphere radius, and total observed\nintensity increase with the increase of $\\alpha_0$ but decrease with the\nincrease of $n$. Under different spherical accretion, the shadow and photon\nsphere radius are identical, but the total observed intensity under the static\nspherical accretion is greater than that under the infalling spherical\naccretion. In addition, we find that these regular BHs with different cores\nshow differences in shadows and optical appearances, especially under the\nstatic spherical accretion. Compared with Bardeen BH, the new BH has a smaller\ntotal observed intensity, dimmer maximum luminosity, and smaller shadow and\nphoton sphere radius. The larger $\\alpha_0$ leads to more significant\ndifferences, and a similar trend is also seen in the comparison with Hayward\nBH. Under the infalling spherical accretion, these regular BHs with different\ncores only have slight differences in total observed intensity, shadow and\nphoton sphere radius, which become more obvious when $\\alpha_0$ is relatively\nlarge. It suggests that the unique spacetime features of these regular BHs with\ndifferent cores can be distinguished through astronomical observations.",
        "Flexible cognition demands discovering hidden rules to quickly adapt\nstimulus-response mappings. Standard neural networks struggle in such tasks\nrequiring rapid, context-driven remapping. Recently, Hummos (2023) introduced a\nfast-and-slow learning algorithm to mitigate this shortcoming, but its\nscalability to complex, image-computable tasks was unclear. Here, we propose\nthe Wisconsin Neural Network (WiNN), which extends Hummos' fast-and-slow\nlearning to image-computable tasks demanding flexible rule-based behavior. WiNN\nemploys a pretrained convolutional neural network for vision, coupled with an\nadjustable \"context state\" that guides attention to relevant features. If WiNN\nproduces an incorrect response, it first iteratively updates its context state\nto refocus attention on task-relevant cues, then performs minimal parameter\nupdates to attention and readout layers. This strategy preserves generalizable\nrepresentations in the sensory and attention networks, reducing catastrophic\nforgetting. We evaluate WiNN on an image-based extension of the Wisconsin Card\nSorting Task, revealing several markers of cognitive flexibility: (i) WiNN\nautonomously infers underlying rules, (ii) requires fewer examples to do so\nthan control models reliant on large-scale parameter updates, (iii) can perform\ncontext-based rule inference solely via context-state adjustments-further\nenhanced by slow updates of attention and readout parameters, and (iv)\ngeneralizes to unseen compositional rules through context-state updates alone.\nBy blending fast context inference with targeted attentional guidance, WiNN\nachieves \"sparks\" of flexibility. This approach offers a path toward\ncontext-sensitive models that retain knowledge while rapidly adapting to\ncomplex, rule-based tasks.",
        "This work is dedicated to a novel sampling method for accurately\nreconstructing elastic and electromagnetic sources from the far field patterns.\nWe show that the proposed indicators in the form of integrals with full far\nfield patterns are exactly the source functions. These facts not only give\nconstructive uniqueness proofs of the inverse source problems, but also\nestablish the theoretical basis of the proposed sampling methods. Furthermore,\nwe derive the stability estimates for the corresponding discrete indicators\nusing the far field patterns with finitely many observations and frequencies.\nWe have also proposed the indicators with partial far field patterns and proved\ntheir validity for providing the derivative information of the unknown sources.\nNumerical examples are presented to verify the accuracy and stability of the\nproposed quantitative sampling method.",
        "We investigate the effects of heterogeneous (spatially varying) activity in a\nhydrodynamical model for dense bacterial suspensions, confining ourselves to\nexperimentally realizable, simple, quenched, activity patterns. We show that\nthe evolution of the bacterial velocity field under such activity patterning\nleads to the emergence of hydrodynamic interfaces separating spatially\nlocalized turbulence from jammed frictional surroundings. We characterise the\nintermittent and multiscale fluctuations of this interface and also investigate\nhow heterogeneity influences mixing via the residence times of Lagrangian\ntracers. This work reveals how naturally occurring heterogeneities could\ndecisively steer active flows into more complex configurations than those\ntypically studied, opening up parallels to droplet dynamics, front propagation\nand turbulent mixing layers.",
        "Recently, a surprising connection between algorithmic learning of algebraic\nstructures and descriptive set theory has emerged. Following this line of\nresearch, we define the learning power of an equivalence relation $E$ on a\ntopological space as the class of isomorphism relations with countably many\nequivalence classes that are continuously reducible to $E$. In this paper, we\ndescribe the learning power of the finite Friedman-Stanley jumps of\n$=_{\\mathbb{N}}$ and $=_{\\mathbb{N}^\\mathbb{N}}$, proving that these\nequivalence relations learn the families of countable structures that are\npairwise distinguished by suitable infinitary sentences. Our proof techniques\nintroduce new ideas for assessing the continuous complexity of Borel\nequivalence relations.",
        "We present radio polarimetric images of 12 Seyfert and Low-Ionization Nuclear\nEmission-line Region (LINER) galaxies belonging to the Centre for Astrophysics\n(CfA)+12 micron sample exhibiting kiloparsec-scale radio outflows (KSRs). These\nobservations have been carried out at 10 GHz with Karl G. Jansky Very Large\nArray (VLA) in D-array and at 1.4 GHz with the BnA$\\rightarrow$A array\nconfigurations. We find signatures of organized magnetic (B-) field structures\nin the cores, jets and lobes of these galaxies. The linear polarization\nfraction varies from a few per cent in the cores to $47\\pm18$ per cent in the\nlobes. The inferred B-fields are toroidal in the cores of several sources\nmaking them consistent with the presence of either a sheath-like or a wind-like\ncomponent surrounding the jet. The in-band spectral index images typically show\nthe presence of flat\/inverted spectrum cores and steep spectrum lobes. Radio\ncores with flatter spectra are found to have lower Eddington ratios while the\nsteeper ones have higher. A strong correlation is observed between the\nSeyfert\/LINER radio outflow properties and the mass of the supermassive black\nholes (SMBHs); correlations with Eddington ratios are weaker. We find\nsignatures of jet-medium interaction and both positive and negative AGN\nfeedback in these sources. Overall, our study indicates that radio-quiet (RQ)\nAGN with KSRs possess radio outflows driven by magnetic fields anchored to\ntheir black holes - accretion disks, which significantly impact their\nenvironments.",
        "The cutoff phenomenon was recently shown to systematically follow from\nnon-negative curvature and the product condition, for all Markov diffusions.\nThe proof crucially relied on a classical \\emph{chain rule} satisfied by the\ncarr\\'e du champ operator, which is specific to differential generators and\nhence fails on discrete spaces. In the present paper, we show that an\napproximate version of this chain rule in fact always holds, with an extra cost\nthat depends on the log-Lipschitz regularity of the considered observable. As a\nconsequence, we derive a new cutoff criterion for non-negatively curved chains\non finite spaces. The latter allows us to recover, in a simple and unified way,\na number of historical instances of cutoff that had been established through\nmodel-specific arguments. Emblematic examples include random walk on the\nhypercube, random transpositions, random walk on the multislice, or MCMC\nsamplers for popular spin systems such as the Ising and Hard-core models on\nbounded-degree graphs.",
        "Generative machine learning models for small molecule drug discovery have\nshown immense promise, but many molecules generated by this approach are too\ndifficult to synthesize to be worth further investigation or further\ndevelopment. We present a novel approach by fine-tuning Meta's Llama3 large\nlanguage models (LLMs) to create SynLlama, which generates full synthetic\npathways made of commonly accessible Enamine building blocks and robust organic\nreaction templates. SynLlama explores a large synthesizable space using\nsignificantly less data compared to other state-of-the-art methods, and offers\nstrong performance in bottom-up synthesis, synthesizable analog generation, and\nhit expansion, offering medicinal chemists a valuable tool for drug discovery\ndevelopments. We find that SynLlama can effectively generalize to unseen yet\npurchasable building blocks, meaning that its reconstruction capabilities\nextend to a broader synthesizable chemical space than the training data.",
        "Spin-1\/2 Heisenberg antiferromagnetic frustrated spin chain systems display\nexotic ground states with unconventional excitations and distinct quantum phase\ntransitions as the ratio of next-nearest-neighbor to nearest-neighbor coupling\nis tuned. We present a comprehensive investigation of the structural, magnetic,\nand thermodynamics properties of the spin-1\/2 compound, Cu(Ampy)ClBr (Ampy=\nC6H8N2 = 2-(Aminomethyl)pyridine) via x-ray diffraction, magnetization,\nspecific heat, 1H nuclear magnetic resonance (NMR), electron spin resonance\n(ESR), and muon spin relaxation (muSR) techniques. The crystal structure\nfeatures an anisotropic triangular chain lattice of magnetic Cu2+ ions. Our\nbulk and local probe experiments detect neither long-range magnetic ordering\nnor spin freezing down to 0.06 K despite the presence of moderate\nantiferromagnetic interaction between Cu2+ spins as reflected by a Curie-Weiss\ntemperature of about -9 K from the bulk susceptibility data. A broad maximum is\nobserved at about 9 K in magnetic susceptibility and specific heat data,\nindicating the onset of short-range spin correlations. At low temperatures, the\nzero-field magnetic specific heat and the 1H NMR spin-lattice relaxation rate\nfollow an exponential temperature dependence, indicating the presence of gapped\nmagnetic excitations. Furthermore, persistent spin dynamics down to 0.088 K\nobserved by zero-field muSR evidences lack of any static magnetism. We\nattribute these experimental results to the stabilization of a dimer-singlet\nphase in the presence of a next-near neighbor interaction and of a randomness\nin the exchange coupling driven by Cl\/Br mixing.",
        "Response theory provides a pathway for understanding the sensitivity of a\nsystem and, more in general, to predict how its statistical properties change\nas a possibly time-dependent perturbation is applied. Recently discovered\ngeneral forms of the celebrated Fluctuation-Dissipation Theorem allow for\nexpressing response operators as correlation functions of suitably defined\nobservables in the unperturbed state, also when such a state is far from\nequilibrium. In the case of complex and multiscale systems, to achieved\nenhanced practical applicability, response theory must be interpretable,\ncapable of focusing of relevant timescales, and amenable to implemented by\ndata-driven approaches that are potentially equation-agnostic. Complex systems\ntypically exhibit a hierarchy of temporal behaviors, and unresolved or\nundesired timescales can obscure the dominant mechanisms driving macroscopic\nresponses. As an element of this desired framework, in the spirit of Markov\nstate modelling, we propose here a comprehensive analysis of the linear and\nnonlinear response of Markov chains to general time-dependent perturbations. We\nobtain simple and easily implementable formulas that can be used to predict the\nresponse of observables as well as higher-order correlations of the system. The\nmethodology proposed here can be implemented in a purely data-driven setting\nand even if we do not know the underlying evolution equations. The use of\nalgebraic expansions inspired by Koopmanism allow to elucidate the role of\ndifferent time scales and find explicit and interpretable expressions for the\nGreen's functions at all orders. This is a major advantage of the framework\nproposed here. We illustrate our methodology in a very simple yet instructive\nmetastable system. Finally, our results provide a dynamical foundation for the\nProny method, which is commonly used for the statistical analysis of discrete\ntime signals.",
        "On-farm sensor data have allowed farmers to implement field management\ntechniques and intensively track the corresponding responses. These data\ncombined with historical records open the door for real-time field management\nimprovements with the help of current advancements in computing power. However,\ndespite these advances, the statistical design of experiments is rarely used to\nevaluate the performance of field management techniques accurately.\nTraditionally, randomized block design is prevalent in statistical designs of\nfield trials, but in practice it is limited in dealing with large variations in\nsoil classes, management practices, and crop varieties. More specifically,\nalthough this experimental design is suited for most trial types, it is not the\noptimal choice when multiple factors are tested over multifarious natural\nvariations in farms, due to the economic constraints caused by the sheer number\nof variables involved. Experimental refinement is required to better estimate\nthe effects of the primary factor in the presence of auxiliary factors. In this\nway, farmers can better understand the characteristics and limitations of the\nprimary factor. This work presents a framework for automating the analysis of\nlocal field variations by fusing soil classification data and lidar topography\ndata with historical yield. This framework will be leveraged to automate the\ndesigning of field experiments based on multiple topographic features",
        "The James Webb Space Telescope, launched in 2021, is an infrared observatory\nof novel design: deployable, with active optics, fully open to space for\nradiative cooling and orbiting the Lagrange point no. 2. This article explains\nthe rationale leading to this specific design and describes the various other\narchitectures that were considered along the way: from a monolithic 10-meter\ntelescope in geosynchronous orbit to a 6-meter one in High Earth Orbit, then a\n16-meter observatory on the Moon, a 4- or 6-meter one in an elliptical\nheliocentric orbit, and a segmented 8-meter one passively cooled to 50 K at L2,\nwhich was finally descoped to 6.6 meters. It also addresses the optimization\nfor scientific performance, the challenge of dealing with such an ultra-low\noperating temperature, cost issues, supporting technology, modifications made\nduring final design and, finally, how the architecture performs on orbit.",
        "We study the obstacle problem associated with the Kolmogorov operator\n$\\Delta_v - \\partial_t - v\\cdot\\nabla_x$, which arises from the theory of\noptimal control in Asian-American options pricing models.\n  Our first main contribution is to improve the known regularity of solutions,\nfrom $C^{0,1}_t \\cap C^{0,2\/3}_x \\cap C^{1,1}_v$ to $C^{0,1}_{t,x} \\cap\nC^{1,1}_v$. The previous result in the literature, which has been called\noptimal, corresponds to $C^{1,1}$ regularity with respect to the Kolmogorov\ndistance. This is the expected regularity for solutions to obstacle problems.\nOur unexpected improvement of regularity in the $x$ variable is obtained using\nBernstein's technique and an approach drawing on ideas from Evans-Krylov\ntheory.\n  We then use this improvement in regularity of the solution to prove the first\nknown free boundary regularity result. We show that under a standard thickness\ncondition, the free boundary is a $C^{0,1\/2}_{t,x} \\cap C^{0,1}_v$ regular\nsurface. This result constitutes the first step in the program of free boundary\nregularity. Critically, our arguments rely on a new monotonicity formula and a\ncommutator estimate that are only made possible by the solution's enhanced\nregularity in $x$.",
        "After the Asian Infrastructure Investment Bank (AIIB) revised its\nEnvironmental and Social Framework, it has committed to certain climate-related\nobjectives, yet an independent climate accountability mechanism has not been\nestablished. The absence of clear evaluation principles and procedural rules\npresents challenges in effectively addressing environmental investment\ndisputes. This article reviews both domestic and international literature\nrelated to AIIB's climate accountability mechanism, identifying that the\ncurrent Environmental and Social Framework's principled content and reliance on\ntraditional approaches have resulted in issues of enforceability and the\nabsence of an independent accountability institution. To enhance the\neffectiveness of AIIB's climate accountability mechanism, a reassessment of its\ndevelopment path is necessary. Future developments should transition from an\nadditive path to a substitutive path, focusing on the application of\ninternational environmental and social standards, increasing stakeholder\nrecognition and participation, promoting comprehensive reforms within AIIB, and\nestablishing a coordinated independent accountability system. These measures\naim to support the robust development of AIIB's climate accountability\nmechanism.",
        "We develop an Integral Transformation Method (ITM) for the study of suitable\noptimal control and differential game models. This allows for a solution to\nsuch dynamic problems to be found through solving a family of optimization\nproblems parametrized by time. The method is quite flexible, and it can be used\nin several economic applications where the state equation and the objective\nfunctional are linear in a state variable. We illustrate the ITM in the context\nof a two-country integrated assessment climate model. We characterize\nemissions, consumption, transfers, and welfare by computing the Nash equilibria\nof the associated dynamic game. We then compare them to efficiency benchmarks.\nFurther, we apply the ITM in a robust control setup, where we investigate how\n(deep) uncertainty affects climate outcomes.",
        "We develop a novel ultrasound nasogastric tube (UNGT) dataset to address the\nlack of public nasogastric tube datasets. The UNGT dataset includes 493 images\ngathered from 110 patients with an average image resolution of approximately\n879 $\\times$ 583. Four structures, encompassing the liver, stomach, tube, and\npancreas are precisely annotated. Besides, we propose a semi-supervised\nadaptive-weighting aggregation medical segmenter to address data limitation and\nimbalance concurrently. The introduced adaptive weighting approach tackles the\nsevere unbalanced challenge by regulating the loss across varying categories as\ntraining proceeds. The presented multiscale attention aggregation block\nbolsters the feature representation by integrating local and global contextual\ninformation. With these, the proposed AAMS can emphasize sparse or small\nstructures and feature enhanced representation ability. We perform extensive\nsegmentation experiments on our UNGT dataset, and the results show that AAMS\noutperforms existing state-of-the-art approaches to varying extents. In\naddition, we conduct comprehensive classification experiments across varying\nstate-of-the-art methods and compare their performance. The dataset and code\nwill be available upon publication at https:\/\/github.com\/NUS-Tim\/UNGT."
      ]
    }
  },
  {
    "id":2412.00663,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"MRI-Guided Adaptive Radiation Therapy",
    "start_abstract":"Magnetic resonance imaging-guided radiation therapy (MRIgRT) has improved soft tissue contrast over computed tomography (CT) based image-guided RT. Superior visualization of the target and surrounding radiosensitive structures has the potential to improve oncological outcomes partly due to safer dose-escalation and adaptive planning. In this review, we highlight the workflow of adaptive MRIgRT planning, which includes simulation imaging, daily MRI, identifying isocenter shifts, contouring, plan optimization, quality control, and delivery. Increased utilization of MRIgRT will depend on addressing technical limitations of this technology, while addressing treatment efficacy, cost-effectiveness, and workflow training.",
    "start_categories":[
      "Oncology"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b4",
        "b2"
      ],
      "title":[
        "Deep Learning for Automatic Gross Tumor Volumes Contouring in Esophageal Cancer Based on Contrast-Enhanced Computed Tomography Images: A Multi-Institutional Study",
        "ConTEXTual Net: A Multimodal Vision-Language Model for Segmentation of Pneumothorax"
      ],
      "abstract":[
        "Purpose To develop and externally validate an automatic artificial intelligence (AI) tool for delineating gross tumor volume (GTV) in patients with esophageal squamous cell carcinoma (ESCC), which can assist in neo-adjuvant or radical radiation therapy treatment planning. Methods and Materials In this multi-institutional study, contrast-enhanced CT images from 580 eligible ESCC patients were retrospectively collected. The GTV contours delineated by 2 experts via consensus were used as ground truth. A 3-dimensional deep learning model was developed for GTV contouring in the training cohort and internally and externally validated in 3 validation cohorts. The AI tool was compared against 12 board-certified experts in 25 patients randomly selected from the external validation cohort to evaluate its assistance in improving contouring performance and reducing variation. Contouring performance was measured using dice similarity coefficient (DSC) and average surface distance. Additionally, our previously established radiomics model for predicting pathologic complete response was used to compare AI-generated and ground truth contours, to assess the potential of the AI contouring tool in radiomics analysis. Results The AI tool demonstrated good GTV contouring performance in multicenter validation cohorts, with median DSC values of 0.865, 0.876, and 0.866 and median average surface distance values of 0.939, 0.789, and 0.875 mm, respectively. Furthermore, the AI tool significantly improved contouring performance for half of 12 board-certified experts (DSC values, 0.794-0.835 vs 0.856-0.881, P = .003-0.048), reduced the intra- and interobserver variations by 37.4% and 55.2%, respectively, and saved contouring time by 77.6%. In the radiomics analysis, 88.7% of radiomic features from ground truth and AI-generated contours demonstrated stable reproducibility, and similar pathologic complete response prediction performance for these contours (P = .430) was observed. Conclusions Our AI contouring tool can improve GTV contouring performance and facilitate radiomics analysis in ESCC patients, which indicates its potential for GTV contouring during radiation therapy treatment planning and radiomics studies.",
        "Radiology narrative reports often describe characteristics of a patient's disease, including its location, size, and shape. Motivated by the recent success multimodal learning, we hypothesized that this descriptive text could guide medical image analysis algorithms. We proposed novel vision-language model, ConTEXTual Net, for task pneumothorax segmentation on chest radiographs. Net extracts language features from physician-generated free-form radiology using pre-trained model. then introduced cross-attention between intermediate embeddings an encoder-decoder convolutional neural network to enable guidance analysis. was trained CANDID-PTX dataset consisting 3196 positive cases with annotations 6 different physicians as well clinical reports. Using cross-validation, achieved Dice score 0.716\u00b10.016, which similar degree inter-reader variability (0.712\u00b10.044) computed subset data. It outperformed vision-only models (Swin UNETR: 0.670\u00b10.015, ResNet50 U-Net: 0.677\u00b10.015, GLoRIA: 0.686\u00b10.014, nnUNet 0.694\u00b10.016) competing model (LAVT: 0.706\u00b10.009). Ablation studies confirmed it information led performance gains. Additionally, show certain augmentation methods degraded Net's breaking image-text concordance. also evaluated effects activation functions in module, highlighting efficacy our chosen architectural design."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Reducing T Gates with Unitary Synthesis",
        "Thermodynamics of strongly magnetized dense quark matter from hard dense\n  loop perturbation theory",
        "ChatIoT: Large Language Model-based Security Assistant for Internet of\n  Things with Retrieval-Augmented Generation",
        "Generalized symmetries and the dimensional reduction of 6d so SCFTs",
        "Searches for light Dark Matter with Spherical Proportional Counters",
        "PDE-DKL: PDE-constrained deep kernel learning in high dimensionality",
        "Sken and cluster algebras of punctured surfaces",
        "Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More",
        "UAV Swarm-enabled Collaborative Post-disaster Communications in Low\n  Altitude Economy via a Two-stage Optimization Approach",
        "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model\n  Pretraining",
        "Impact of trace amounts of water on the stability of Micro-Pattern\n  Gaseous Detectors",
        "Universal law for the dispersal of motile microorganisms in porous media",
        "Unveiling the Oxidation Mechanisms of Octa-Penta Graphene: A\n  Multidimensional Exploration from First-Principles to Machine Learning",
        "Strong conciseness and equationally Noetherian groups",
        "Evaluating Prediction-based Interventions with Human Decision Makers In\n  Mind",
        "Derived derivations govern contraderived deformations of dg algebras\n  over dg (pr)operads",
        "Large Language Models as Common-Sense Heuristics",
        "On the kernel learning problem",
        "Impilict Runge-Kutta based sparse identification of governing equations\n  in biologically motivated systems",
        "The quantum enigma of teleportation near black holes",
        "Assessing workflow impact and clinical utility of AI-assisted brain\n  aneurysm detection: a multi-reader study",
        "Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment",
        "Evaluating Hallucination in Large Vision-Language Models based on\n  Context-Aware Object Similarities",
        "DVFS-Aware DNN Inference on GPUs: Latency Modeling and Performance\n  Analysis",
        "Improving the stellar age determination through joint modeling of\n  binarity and asteroseismology -- Grid modeling of the seismic red-giant\n  binary KIC 9163796",
        "Protein Structure Tokenization: Benchmarking and New Recipe",
        "Accurate, transferable, and verifiable machine-learned interatomic\n  potentials for layered materials",
        "Observational evidence for a correlation between the magnetic field of\n  jets and star formation rate in host galaxies"
      ],
      "abstract":[
        "Quantum error correction is essential for achieving practical quantum\ncomputing but has a significant computational overhead. Among fault-tolerant\n(FT) gate operations, non-Clifford gates, such as $T$, are particularly\nexpensive due to their reliance on magic state distillation. These costly $T$\ngates appear frequently in FT circuits as many quantum algorithms require\narbitrary single-qubit rotations, such as $R_x$ and $R_z$ gates, which must be\ndecomposed into a sequence of $T$ and Clifford gates. In many quantum circuits,\n$R_x$ and $R_z$ gates can be fused to form a single $U3$ unitary. However,\nexisting synthesis methods, such as gridsynth, rely on indirect decompositions,\nrequiring separate $R_z$ decompositions that result in a threefold increase in\n$T$ count.\n  This work presents a novel FT synthesis algorithm that directly synthesizes\narbitrary single-qubit unitaries, avoiding the overhead of separate $R_z$\ndecompositions. By leveraging tensor network-based search, our approach enables\nnative $U3$ synthesis, reducing the $T$ count, Clifford gate count, and\napproximation error. Compared to gridsynth-based circuit synthesis, for 187\nrepresentative benchmarks, our design reduces the $T$ count by up to\n$3.5\\times$, and Clifford gates by $7\\times$, resulting in up to $4\\times$\nimprovement in overall circuit infidelity.",
        "We discuss the hard dense loop perturbation theory approach for studying the\nthermodynamics of strongly magnetized dense quark matter. The free energy of\nquarks and gluons have been calculated for one-loop quark and gluon\nself-energies, respectively. The longitudinal and transverse components of\npressure, magnetization, second-order quark number susceptibility, and speed of\nsound have been computed, and their behavior with chemical potential and\nmagnetic field has been analyzed. Our numerical results show that the\nlongitudinal pressure increases with chemical potential and magnetic field,\nwhile for the transverse component, it is diminished. We also analyze the\nlongitudinal component of the speed of sound at high chemical potentials, which\napproaches the speed of light in the asymptotic limit. The obtained results may\nbe helpful in studying magnetized quark matter in the core of neutron stars and\nmagnetars.",
        "Internet of Things (IoT) has gained widespread popularity, revolutionizing\nindustries and daily life. However, it has also emerged as a prime target for\nattacks. Numerous efforts have been made to improve IoT security, and\nsubstantial IoT security and threat information, such as datasets and reports,\nhave been developed. However, existing research often falls short in leveraging\nthese insights to assist or guide users in harnessing IoT security practices in\na clear and actionable way. In this paper, we propose ChatIoT, a large language\nmodel (LLM)-based IoT security assistant designed to disseminate IoT security\nand threat intelligence. By leveraging the versatile property of\nretrieval-augmented generation (RAG), ChatIoT successfully integrates the\nadvanced language understanding and reasoning capabilities of LLM with\nfast-evolving IoT security information. Moreover, we develop an end-to-end data\nprocessing toolkit to handle heterogeneous datasets. This toolkit converts\ndatasets of various formats into retrievable documents and optimizes chunking\nstrategies for efficient retrieval. Additionally, we define a set of common use\ncase specifications to guide the LLM in generating answers aligned with users'\nspecific needs and expertise levels. Finally, we implement a prototype of\nChatIoT and conduct extensive experiments with different LLMs, such as LLaMA3,\nLLaMA3.1, and GPT-4o. Experimental evaluations demonstrate that ChatIoT can\ngenerate more reliable, relevant, and technical in-depth answers for most use\ncases. When evaluating the answers with LLaMA3:70B, ChatIoT improves the above\nmetrics by over 10% on average, particularly in relevance and technicality,\ncompared to using LLMs alone.",
        "We consider the dimensional reduction on a torus of the family of 6d $(1,0)$\nSCFTs UV completing an $so(N)$ gauge theory with $N-8$ vector hypermultiplets.\nThese SCFTs are known to possess a rich structure of discrete symmetries,\nnotably 0-form and 1-form symmetries, which often merge to form a higher group\nstructure, both split and non-split. We investigate what happens to this\nsymmetry structure once the theory is reduced on a circle to 5d and on a torus\nto 4d, especially when a non-trivial Stiefel-Whitney class for the flavor\nsymmetry is turned on. Unlike in Lagrangian theories, here the 1-form\nsymmetries of the 6d theory reduce to non-trivially acting 1-form and 0-form\nsymmetries, and the original higher group structure leads to an extension of\nthe 0-form symmetries.",
        "Elucidating the nature of dark matter is a key priority that would involve\ndiscovering new fundamental physics and is essential for understanding the\nstructure and evolution of the universe. Despite the decades-long\never-more-sensitive searches, the particle content of dark matter remains\nelusive. Direct searches for dark matter candidates, to-date, focused mainly on\ncandidates in the 10 GeV to 1 TeV, however, more recently lighter candidates\nwith sub-GeV mass have been brought to the spotlight. This is an experimentally\nchallenging mass region, which remains largely uncharted. The spherical\nproportional counter is a new type of gaseous detector which exhibits several\nfeatures that make it ideally suited for the exploration of this mass range. In\nthis article the invention and development of the spherical proportional\ncounter are presented, its applications in the search for particle dark matter\nand beyond are reviewed, and possible future directions are discussed.",
        "Many physics-informed machine learning methods for PDE-based problems rely on\nGaussian processes (GPs) or neural networks (NNs). However, both face\nlimitations when data are scarce and the dimensionality is high. Although GPs\nare known for their robust uncertainty quantification in low-dimensional\nsettings, their computational complexity becomes prohibitive as the\ndimensionality increases. In contrast, while conventional NNs can accommodate\nhigh-dimensional input, they often require extensive training data and do not\noffer uncertainty quantification. To address these challenges, we propose a\nPDE-constrained Deep Kernel Learning (PDE-DKL) framework that combines DL and\nGPs under explicit PDE constraints. Specifically, NNs learn a low-dimensional\nlatent representation of the high-dimensional PDE problem, reducing the\ncomplexity of the problem. GPs then perform kernel regression subject to the\ngoverning PDEs, ensuring accurate solutions and principled uncertainty\nquantification, even when available data are limited. This synergy unifies the\nstrengths of both NNs and GPs, yielding high accuracy, robust uncertainty\nestimates, and computational efficiency for high-dimensional PDEs. Numerical\nexperiments demonstrate that PDE-DKL achieves high accuracy with reduced data\nrequirements. They highlight its potential as a practical, reliable, and\nscalable solver for complex PDE-based applications in science and engineering.",
        "We prove the full Fock-Goncharov conjecture for\n$\\mathcal{A}_{SL_2,\\Sigma_{g,p}}$--the $\\mathcal{A}$-cluster variety associated\nto representation of $SL_2$ local systems on most punctured surfaces with at\nleast 2 punctures in the classical $q\\to 1$ setting, that is, the tagged skein\nalgebra coincides with the upper cluster algebra (namely $Sk^{ta}=U(\\Sigma)$ or\n$mid(\\mathcal{A})=up(\\mathcal{A})$), with methods being potentially useful to\ntackle the quantum case. We deduce similar results for the Roger Yang skein\nalgebra via a birational geometric description, obtaining\n$Sk^{RY}=U(\\Sigma)[v_i^{\\pm1}]$ as conjectured by Shen, Sun and Weng, proving\nimportant algebraic properties of $Sk^{RY}$ including normality and\nCohen-Macaulayness. Our result is complementary to what and Mandel and Qin have\nshown in arXiv:2301.11101 for surface with marked points, based on\narXiv:1411.1394 . The once-punctured case where cluster structures are\nsignificantly different is also discussed in the paper, and relevant\nconjectures are proposed (and proved in the once-punctured torus case).\n  By contrast, we define the ordinary cluster algebra with potentials added\n$A(\\Sigma)[v_i^{\\pm1}]$, introduced by Shen, Sun and Weng, which is shown to be\nusually smaller than $Sk^{RY}$ and $U(\\Sigma)[v_i^{\\pm1}]$. This strengthen the\nresult of arXiv:2201.08833 that the classical $A=U$ fails for $\\Sigma_{g,p}$\nwith $g\\geq 1, p\\geq 1$.",
        "Since the introduction of Vision Transformer (ViT), patchification has long\nbeen regarded as a de facto image tokenization approach for plain visual\narchitectures. By compressing the spatial size of images, this approach can\neffectively shorten the token sequence and reduce the computational cost of\nViT-like plain architectures. In this work, we aim to thoroughly examine the\ninformation loss caused by this patchification-based compressive encoding\nparadigm and how it affects visual understanding. We conduct extensive patch\nsize scaling experiments and excitedly observe an intriguing scaling law in\npatchification: the models can consistently benefit from decreased patch sizes\nand attain improved predictive performance, until it reaches the minimum patch\nsize of 1x1, i.e., pixel tokenization. This conclusion is broadly applicable\nacross different vision tasks, various input scales, and diverse architectures\nsuch as ViT and the recent Mamba models. Moreover, as a by-product, we discover\nthat with smaller patches, task-specific decoder heads become less critical for\ndense prediction. In the experiments, we successfully scale up the visual\nsequence to an exceptional length of 50,176 tokens, achieving a competitive\ntest accuracy of 84.6% with a base-sized model on the ImageNet-1k benchmark. We\nhope this study can provide insights and theoretical foundations for future\nworks of building non-compressive vision models. Code is available at\nhttps:\/\/github.com\/wangf3014\/Patch_Scaling.",
        "The low-altitude economy (LAE) plays an indispensable role in cargo\ntransportation, healthcare, infrastructure inspection, and especially\npost-disaster communication. Specifically, unmanned aerial vehicles (UAVs), as\none of the core technologies of the LAE, can be deployed to provide\ncommunication coverage, facilitate data collection, and relay data for trapped\nusers, thereby significantly enhancing the efficiency of post-disaster response\nefforts. In this paper, we design an efficient and robust UAV-swarm enabled\ncollaborative self-organizing network to facilitate post-disaster\ncommunications. Specifically, a ground device transmits data to UAV swarms,\nwhich then use collaborative beamforming (CB) technique to form virtual antenna\narrays and relay the data to a remote access point (AP) efficiently. Then, we\nformulate a rescue-oriented post-disaster transmission rate maximization\noptimization problem (RPTRMOP). Then, we propose a two-stage optimization\napproach to address it. In the first stage, the optimal traffic routing and the\ntheoretical upper bound on the transmission rate of the network are derived. In\nthe second stage, we transform the formulated RPTRMOP into a variant named\nV-RPTRMOP, and a diffusion model-enabled particle swarm optimization (DM-PSO)\nalgorithm is proposed to deal with the V-RPTRMOP. Simulation results show the\neffectiveness of the proposed two-stage optimization approach in improving the\ntransmission rate of the constructed network, which demonstrates the great\npotential for post-disaster communications. Moreover, the robustness of the\nconstructed network is also validated via evaluating the impact of two\nunexpected situations on the system transmission rate.",
        "Pretraining large language models (LLMs) on vast and heterogeneous datasets\nis crucial for achieving state-of-the-art performance across diverse downstream\ntasks. However, current training paradigms treat all samples equally,\noverlooking the importance or relevance of individual samples throughout the\ntraining process. Existing reweighting strategies, which primarily focus on\ngroup-level data importance, fail to leverage fine-grained instance-level\ninformation and do not adapt dynamically to individual sample importance as\ntraining progresses. In this paper, we introduce novel algorithms for dynamic,\ninstance-level data reweighting aimed at improving both the efficiency and\neffectiveness of LLM pretraining. Our methods adjust the weight of each\ntraining sample based on its loss value in an online fashion, allowing the\nmodel to dynamically focus on more informative or important samples at the\ncurrent training stage. In particular, our framework allows us to\nsystematically devise reweighting strategies deprioritizing redundant or\nuninformative data, which we find tend to work best. Furthermore, we develop a\nnew theoretical framework for analyzing the impact of loss-based reweighting on\nthe convergence of gradient-based optimization, providing the first formal\ncharacterization of how these strategies affect convergence bounds. We\nempirically validate our approach across a spectrum of tasks, from pretraining\n7B and 1.4B parameter LLMs to smaller-scale language models and linear\nregression problems, demonstrating that our loss-based reweighting approach can\nlead to faster convergence and significantly improved performance.",
        "In this study, we investigate the influence of humidity on the performance of\nvarious non-resistive Micro Pattern Gaseous Detectors, such as GEM, Thick-GEM,\nand Micromegas, operated with Ar-CO$_2$ (90-10) gas mixture. The water content\nis introduced in a range of $0-5000~ppm_{\\mathrm{V}}$. It is observed that the\npresence of increased humidity does not significantly degrade any of the\nstudied performance criteria. On the contrary, our measurements suggest an\nimprovement in discharge stability with increasing humidity levels at the\nhighest gains and fields. No significant difference is observed at the lower\ngains, indicating that humidity helps to reduce the rate of spurious discharges\nrelated to electrode defects or charging-up of the insulating layers. We\nconclude that adding a small amount of water to the gas mixture may be\nbeneficial for the stable operation of an MPGD.",
        "Dispersal is essential to the plethora of motile microorganisms living in\nporous environments, yet how it relates to movement patterns and pore space\nstructure remains largely unknown. Here we investigate numerically the\nlong-time dispersal of a run-and-tumble microorganism that remains trapped at\nsolid surfaces and escapes from them by tumbling. We find that dispersal and\nmean run time are connected by a universal relation, that applies for a variety\nof porous microstructures and swimming strategies. We explain how this generic\ndependence originates in the invariance of the mean free path with respect to\nthe movement pattern, and we discuss the optimal strategy that maximizes\ndispersal. Finally, we extend our approach to microorganisms moving along the\nsurface. Our results provide a general framework to quantify dispersal that\nworks across the vast diversity of movement patterns and porous media.",
        "Octa-penta graphene (OPG), a novel carbon allotrope characterized by its\ndistinctive arrangement of pentagonal and octagonal rings, has garnered\nconsiderable attention due to its exceptional structure and functional\nproperties. This study systematically investigates the oxidation mechanisms of\nOPG and elucidates the oxygen migration patterns on the OPG monolayer through\nfirst-principles calculations and machine-learning-based molecular dynamics\n(MLMD) simulations. Specifically, the oxidation processes on OPG-L and OPG-Z\ninvolve exothermic chemisorption, where oxygen molecules dissociate at the\nsurfaces, forming stable epoxy groups. Furthermore, the integrated-crystal\norbital Hamilton population (ICOHP) and Bader charge analyses provide insights\ninto the physical mechanisms of oxygen atom adsorption. Importantly, we found\nthat oxidation also impact the electronic properties of OPG, with OPG-L\nretaining its metallic characteristics post-oxygen adsorption, whereas OPG-Z\nundergoes a transformation from a metallic to a semiconducting state due to the\nintroduction of oxygen. Oxygen migration on OPG monolayer involves breaking and\nreforming of C-O bonds, with varying stability across adsorption sites and\nlimited migration along the basal plane. MLMD simulations corroborate these\nmigration patterns, offering detailed migration trajectories consistent with\ntheoretical predictions. These findings enhance the understanding of oxygen\nmigration dynamics on OPG, facilitate its experimental validations, and\nhighlight its potential as a novel 2D material for applications in batteries,\nheat-resistant materials, and oxidation-resistant coatings.",
        "A word $w$ is said to be concise in a class of groups if, for every $G$ in\nthat class such that the set of $w$-values $w\\{G\\}$ is finite, the verbal\nsubgroup $w(G)$ is also finite. In the context of profinite groups, the notion\nof strong conciseness imposes a more demanding condition on $w$, requiring that\n$w(G)$ is finite whenever $|w\\{G\\}|< 2^{\\aleph_0}$. We investigate the relation\nbetween these two properties and the notion of equationally Noetherian groups,\nby proving that in a profinite group $G$ with a dense equationally Noetherian\nsubgroup, $w\\{G\\}$ is finite whenever $|w\\{G\\}|< 2^{\\aleph_0}$. Consequently,\nwe conclude that every word is strongly concise in the classes of profinite\nlinear groups, pro-$\\mathcal{C}$ completions of residually $\\mathcal{C}$ linear\ngroups and pro-$\\mathcal{C}$ completions of virtually abelian-by-polycyclic\ngroups, thereby extending well-known conciseness properties of these classes of\ngroups.",
        "Automated decision systems (ADS) are broadly deployed to inform and support\nhuman decision-making across a wide range of consequential settings. However,\nvarious context-specific details complicate the goal of establishing meaningful\nexperimental evaluations for prediction-based interventions. Notably, current\nexperiment designs rely on simplifying assumptions about human decision making\nin order to derive causal estimates. In reality, specific experimental design\ndecisions may induce cognitive biases in human decision makers, which could\nthen significantly alter the observed effect sizes of the prediction\nintervention. In this paper, we formalize and investigate various models of\nhuman decision-making in the presence of a predictive model aid. We show that\neach of these behavioural models produces dependencies across decision subjects\nand results in the violation of existing assumptions, with consequences for\ntreatment effect estimation. This work aims to further advance the scientific\nvalidity of intervention-based evaluation schemes for the assessment of ADS\ndeployments.",
        "We show that Hinich's simplicial nerve of the differential graded Lie algebra\n(DGLA) of derived derivations of a dg algebra $A$ over a dg properad\n$\\mathcal{P}$ is equivalent to the space of deformations of $A$ as a\n$\\mathcal{P}_{\\infty}$-algebra in Positselski's contraderived dg category. This\nresolves Hinich's counterexamples to the general existence of derived\ndeformations. It also generalises his results when $A$ is homologically bounded\nbelow, since contraderived deformations are then precisely derived\ndeformations.",
        "While systems designed for solving planning tasks vastly outperform Large\nLanguage Models (LLMs) in this domain, they usually discard the rich semantic\ninformation embedded within task descriptions. In contrast, LLMs possess\nparametrised knowledge across a wide range of topics, enabling them to leverage\nthe natural language descriptions of planning tasks in their solutions.\nHowever, current research in this direction faces challenges in generating\ncorrect and executable plans. Furthermore, these approaches depend on the LLM\nto output solutions in an intermediate language, which must be translated into\nthe representation language of the planning task. We introduce a novel planning\nmethod, which leverages the parametrised knowledge of LLMs by using their\noutput as a heuristic for Hill-Climbing Search. This approach is further\nenhanced by prompting the LLM to generate a solution estimate to guide the\nsearch. Our method outperforms the task success rate of similar systems within\na common household environment by 22 percentage points, with consistently\nexecutable plans. All actions are encoded in their original representation,\ndemonstrating that strong results can be achieved without an intermediate\nlanguage, thus eliminating the need for a translation step.",
        "The classical kernel ridge regression problem aims to find the best fit for\nthe output $Y$ as a function of the input data $X\\in \\mathbb{R}^d$, with a\nfixed choice of regularization term imposed by a given choice of a reproducing\nkernel Hilbert space, such as a Sobolev space. Here we consider a\ngeneralization of the kernel ridge regression problem, by introducing an extra\nmatrix parameter $U$, which aims to detect the scale parameters and the feature\nvariables in the data, and thereby improve the efficiency of kernel ridge\nregression. This naturally leads to a nonlinear variational problem to optimize\nthe choice of $U$. We study various foundational mathematical aspects of this\nvariational problem, and in particular how this behaves in the presence of\nmultiscale structures in the data.",
        "Identifying governing equations in physical and biological systems from\ndatasets remains a long-standing challenge across various scientific\ndisciplines, providing mechanistic insights into complex system evolution.\nCommon methods like sparse identification of nonlinear dynamics (SINDy) often\nrely on precise derivative estimations, making them vulnerable to data scarcity\nand noise. This study presents a novel data-driven framework by integrating\nhigh order implicit Runge-Kutta methods (IRKs) with the sparse identification,\ntermed IRK-SINDy. The framework exhibits remarkable robustness to data scarcity\nand noise by leveraging the lower stepsize constraint of IRKs. Two methods for\nincorporating IRKs into sparse regression are introduced: one employs iterative\nschemes for numerically solving nonlinear algebraic system of equations, while\nthe other utilizes deep neural networks to predict stage values of IRKs. The\nperformance of IRK-SINDy is demonstrated through numerical experiments on\nbenchmark problems with varied dynamical behaviors, including linear and\nnonlinear oscillators, the Lorenz system, and biologically relevant models like\npredator-prey dynamics, logistic growth, and the FitzHugh-Nagumo model. Results\nindicate that IRK-SINDy outperforms conventional SINDy and the RK4-SINDy\nframework, particularly under conditions of extreme data scarcity and noise,\nyielding interpretable and generalizable models.",
        "The enigma surrounding the existence of black holes has recently been\nsubstantiated through the groundbreaking work of experimental physicists\n\\cite{genzel2024}. Exploring quantum systems under the gravitational influence\nof black holes has emerged as a pivotal area of research. Among the frontier\nworks in quantum information processing is the utilization of quantum states as\nquantum channels. A fundamental quantum information protocol is teleportation,\nin which two parties, Alice and Bob, share entangled states. In this protocol,\nthe sender, Alice, who holds an unknown qubit, utilizes local operations and\nclassical communication (LOCC) to recreate the qubit at the recipient's (Bob's)\nend. Notably, during the execution of this protocol, Alice loses the unknown\nqubit on her side. The teleportation protocol, originally proposed by Bennett\net al. \\cite{bennett1993}, has been extensively studied with various states and\nunder different physical setups. Researchers have explored both modifications\nto the protocol itself and the viability of various quantum states as\nteleportation channels. In this paper, we investigate whether bipartite mixed\nstates derived from two inequivalent classes of tripartite pure states,\nsubjected to the gravitational influence of two different black hole models,\ncan still serve as efficient quantum channels for teleportation. We emphasize\nthe teleportation fidelity of these states, a critical factor for determining\ntheir efficacy as quantum channels. Specifically, the fidelity must exceed the\nclassical limit of $\\frac{2}{3}$ to be considered effective \\cite{pop1994}. We\nconjecture that, even under the gravitational influence of black holes, the\nquantum characteristics of the given states are preserved, enabling them to\nfunction effectively as quantum channels for teleportation.",
        "Despite the plethora of AI-based algorithms developed for anomaly detection\nin radiology, subsequent integration into clinical setting is rarely evaluated.\nIn this work, we assess the applicability and utility of an AI-based model for\nbrain aneurysm detection comparing the performance of two readers with\ndifferent levels of experience (2 and 13 years). We aim to answer the following\nquestions: 1) Do the readers improve their performance when assisted by the AI\nalgorithm? 2) How much does the AI algorithm impact routine clinical workflow?\nWe reuse and enlarge our open-access, Time-Of-Flight Magnetic Resonance\nAngiography dataset (N=460). We use 360 subjects for training\/validating our\nalgorithm and 100 as unseen test set for the reading session. Even though our\nmodel reaches state-of-the-art results on the test set (sensitivity=74%, false\npositive rate=1.6), we show that neither the junior nor the senior reader\nsignificantly increase their sensitivity (p=0.59, p=1, respectively). In\naddition, we find that reading time for both readers is significantly higher in\nthe \"AI-assisted\" setting than in the \"Unassisted\" (+15 seconds, on average;\np=3x10^(-4) junior, p=3x10^(-5) senior). The confidence reported by the readers\nis unchanged across the two settings, indicating that the AI assistance does\nnot influence the certainty of the diagnosis. Our findings highlight the\nimportance of clinical validation of AI algorithms in a clinical setting\ninvolving radiologists. This study should serve as a reminder to the community\nto always examine the real-word effectiveness and workflow impact of proposed\nalgorithms.",
        "Recent research has shown that carefully crafted jailbreak inputs can induce\nlarge language models to produce harmful outputs, despite safety measures such\nas alignment. It is important to anticipate the range of potential Jailbreak\nattacks to guide effective defenses and accurate assessment of model safety. In\nthis paper, we present a new approach for generating highly effective Jailbreak\nattacks that manipulate the attention of the model to selectively strengthen or\nweaken attention among different parts of the prompt. By harnessing attention\nloss, we develop more effective jailbreak attacks, that are also transferrable.\nThe attacks amplify the success rate of existing Jailbreak algorithms including\nGCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example,\nthe amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack\non Llama2-7B\/AdvBench, using less than a third of the generation time).",
        "Despite their impressive performance on multi-modal tasks, large\nvision-language models (LVLMs) tend to suffer from hallucinations. An important\ntype is object hallucination, where LVLMs generate objects that are\ninconsistent with the images shown to the model. Existing works typically\nattempt to quantify object hallucinations by detecting and measuring the\nfraction of hallucinated objects in generated captions. Additionally, more\nrecent work also measures object hallucinations by directly querying the LVLM\nwith binary questions about the presence of likely hallucinated objects based\non object statistics like top-k frequent objects and top-k co-occurring\nobjects. In this paper, we present Context-Aware Object Similarities (CAOS), a\nnovel approach for evaluating object hallucination in LVLMs using object\nstatistics as well as the generated captions. CAOS uniquely integrates object\nstatistics with semantic relationships between objects in captions and\nground-truth data. Moreover, existing approaches usually only detect and\nmeasure hallucinations belonging to a predetermined set of in-domain objects\n(typically the set of all ground-truth objects for the training dataset) and\nignore generated objects that are not part of this set, leading to\nunder-evaluation. To address this, we further employ language model--based\nobject recognition to detect potentially out-of-domain hallucinated objects and\nuse an ensemble of LVLMs for verifying the presence of such objects in the\nquery image. CAOS also examines the sequential dynamics of object generation,\nshedding light on how the order of object appearance influences hallucinations,\nand employs word embedding models to analyze the semantic reasons behind\nhallucinations. CAOS aims to offer a nuanced understanding of the hallucination\ntendencies of LVLMs by providing a systematic framework to identify and\ninterpret object hallucinations.",
        "The rapid development of deep neural networks (DNNs) is inherently\naccompanied by the problem of high computational costs. To tackle this\nchallenge, dynamic voltage frequency scaling (DVFS) is emerging as a promising\ntechnology for balancing the latency and energy consumption of DNN inference by\nadjusting the computing frequency of processors. However, most existing models\nof DNN inference time are based on the CPU-DVFS technique, and directly\napplying the CPU-DVFS model to DNN inference on GPUs will lead to significant\nerrors in optimizing latency and energy consumption. In this paper, we propose\na DVFS-aware latency model to precisely characterize DNN inference time on\nGPUs. We first formulate the DNN inference time based on extensive experiment\nresults for different devices and analyze the impact of fitting parameters.\nThen by dividing DNNs into multiple blocks and obtaining the actual inference\ntime, the proposed model is further verified. Finally, we compare our proposed\nmodel with the CPU-DVFS model in two specific cases. Evaluation results\ndemonstrate that local inference optimization with our proposed model achieves\na reduction of no less than 66% and 69% in inference time and energy\nconsumption respectively. In addition, cooperative inference with our proposed\nmodel can improve the partition policy and reduce the energy consumption\ncompared to the CPU-DVFS model.",
        "Context. Typical uncertainties of ages determined for single star giants from\nisochrone fitting using single-epoch spectroscopy and photometry without any\nadditional constraints are 30-50 %. Binary systems, particularly double-lined\nspectroscopic (SB2) binaries, provide an opportunity to study the intricacies\nof internal stellar physics and better determine stellar parameters,\nparticularly the stellar age. Aims. By using the constraints from binarity and\nasteroseismology, we aim to obtain precise age and stellar parameters for the\nred giant-subgiant binary system KIC 9163796, a system with a mass ratio of\n1.015 but distinctly different positions in the Hertzsprung-Russell diagram\n(HRD). Methods. We compute a multidimensional model grid of individual stellar\nmodels. From different combinations of figures of merit, we use the constraints\ndrawn from binarity, spectroscopy, and asteroseismology to determine the\nstellar mass, chemical composition, and age of KIC 9163796. Results. Our\ncombined-modeling approach leads to an age estimation of the binary system KIC\n9163796 of 2.44$^{+0.25}_{-0.20}$ Gyr, which corresponds to a relative error in\nthe age of 9 %. Furthermore, we found both components exhibiting equal initial\nhelium abundance of 0.27 to 0.30, significantly higher than the primordial\nhelium abundance, and an initial heavy metal abundance below the spectroscopic\nvalue. The masses of our models are in agreement with masses derived from the\nasteroseismic scaling relations. Conclusions. By exploiting the unique,\ndistinct positions of KIC 9163796, we successfully demonstrated that combining\nasteroseismic and binary constraints leads to a significant improvement of\nprecision in age estimation, that have a relative error below 10% for a giant\nstar.",
        "Recent years have witnessed a surge in the development of protein structural\ntokenization methods, which chunk protein 3D structures into discrete or\ncontinuous representations. Structure tokenization enables the direct\napplication of powerful techniques like language modeling for protein\nstructures, and large multimodal models to integrate structures with protein\nsequences and functional texts. Despite the progress, the capabilities and\nlimitations of these methods remain poorly understood due to the lack of a\nunified evaluation framework. We first introduce StructTokenBench, a framework\nthat comprehensively evaluates the quality and efficiency of structure\ntokenizers, focusing on fine-grained local substructures rather than global\nstructures, as typical in existing benchmarks. Our evaluations reveal that no\nsingle model dominates all benchmarking perspectives. Observations of codebook\nunder-utilization led us to develop AminoAseed, a simple yet effective strategy\nthat enhances codebook gradient updates and optimally balances codebook size\nand dimension for improved tokenizer utilization and quality. Compared to the\nleading model ESM3, our method achieves an average of 6.31% performance\nimprovement across 24 supervised tasks, with sensitivity and utilization rates\nincreased by 12.83% and 124.03%, respectively.",
        "Twisted layered van-der-Waals materials often exhibit unique electronic and\noptical properties absent in their non-twisted counterparts. Unfortunately,\npredicting such properties is hindered by the difficulty in determining the\natomic structure in materials displaying large moir\\'e domains. Here, we\nintroduce a split machine-learned interatomic potential and dataset curation\napproach that separates intralayer and interlayer interactions and\nsignificantly improves model accuracy -- with a tenfold increase in energy and\nforce prediction accuracy relative to conventional models. We further\ndemonstrate that traditional MLIP validation metrics -- force and energy errors\n-- are inadequate for moir\\'e structures and develop a more holistic,\nphysically-motivated metric based on the distribution of stacking\nconfigurations. This metric effectively compares the entirety of large-scale\nmoir\\'e domains between two structures instead of relying on conventional\nmeasures evaluated on smaller commensurate cells. Finally, we establish that\none-dimensional instead of two-dimensional moir\\'e structures can serve as\nefficient surrogate systems for validating MLIPs, allowing for a practical\nmodel validation protocol against explicit DFT calculations. Applying our\nframework to HfS2\/GaS bilayers reveals that accurate structural predictions\ndirectly translate into reliable electronic properties. Our model-agnostic\napproach integrates seamlessly with various intralayer and interlayer\ninteraction models, enabling computationally tractable relaxation of moir\\'e\nmaterials, from bilayer to complex multilayers, with rigorously validated\naccuracy.",
        "Accretion supermassive black holes in the center of active galaxies usually\nproduce ``jets''-collimated bipolar outflows of relativistic particles.\nMagnetic fields near the black hole event horizon may play a crucial role in\nthe formation of jets\/outflows. Both theory and observation indicate that\njets\/outflows driven by centrally active supermassive black holes (SMBHs) have\na feedback effect on the overall properties of the host galaxies. Therefore,\nthe magnetic field is a key ingredient for the formation and evolution of\ngalaxies. Here we report a clear correlation between the magnetic field of jets\nand star formation rate (SFR) for a large sample of 96 galaxies hosting\nsupermassive black holes, which suggests that the star formation of active\ngalactic nuclei (AGN) host galaxies may be powered by the jets."
      ]
    }
  },
  {
    "id":2411.18602,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"Rsna pneumonia detection challenge",
    "start_abstract":"In this competition, you\u2019re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs. Here\u2019s the backstory and why solving this problem matters. Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country. While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis. CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift. To improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA\u00ae) has reached out to Kaggle\u2019s machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge. The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018.",
    "start_categories":[
      "Imaging"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b14",
        "b9"
      ],
      "title":[
        "Adding Conditional Control to Text-to-Image Diffusion Models",
        "Highresolution image synthesis with latent diffusion models"
      ],
      "abstract":[
        "We present ControlNet, a neural network architecture to add spatial conditioning controls large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large models, and reuses their deep robust encoding layers with billions of images as strong backbone learn diverse set conditional controls. The is connected \"zero convolutions\" (zero-initialized convolution layers) that progressively grow parameters from zero ensure no harmful noise could affect finetuning. test various controls, e.g., edges, depth, segmentation, human pose, etc., Stable Diffusion, using single or multiple conditions, without prompts. show training ControlNets small (<50k) (>1m) datasets. Extensive results may facilitate wider applications control image",
        "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Certified algebraic curve projections by path tracking",
        "Galactic-scale emission-line outflow from the radio-loud quasar 3C 191",
        "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
        "Representation Engineering for Large-Language Models: Survey and\n  Research Challenges",
        "Strong Lensing analysis of SPT-CLJ2325$-$4111 and SPT-CLJ0049$-$2440,\n  two Powerful Cosmic Telescopes ($R_E > 40''$) from the SPT Clusters Sample",
        "Turing-Completeness and Undecidability in Coupled Nonlinear Optical\n  Resonators",
        "Cyclicity of Cowen-Douglas tuples",
        "Riemannian Proximal Sampler for High-accuracy Sampling on Manifolds",
        "An end to end gluing construction for metrics of constant Q-curvature",
        "Percolation in a three-dimensional non-symmetric multi-color loop model",
        "Projected Spread Models",
        "The long-term solar variability, as reconstructed from historical\n  sources: Several case studies in the 17th -- 18th centuries",
        "IDEA: Image Description Enhanced CLIP-Adapter",
        "Least-Squares Problem Over Probability Measure Space",
        "Fair Vertex Problems Parameterized by Cluster Vertex Deletion",
        "An Improved Deep Learning Model for Word Embeddings Based Clustering for\n  Large Text Datasets",
        "Attacker Control and Bug Prioritization",
        "One-Class Domain Adaptation via Meta-Learning",
        "Time-Varying Coronary Artery Deformation: A Dynamic Skinning Framework\n  for Surgical Training",
        "CAMEL: Continuous Action Masking Enabled by Large Language Models for\n  Reinforcement Learning",
        "ArcPro: Architectural Programs for Structured 3D Abstraction of Sparse\n  Points",
        "Advances in Set Function Learning: A Survey of Techniques and\n  Applications",
        "Constructing Merger Trees of Density Peaks Using Phase-Space Watershed\n  Segmentation Algorithm",
        "Two-Dimensional Deep ReLU CNN Approximation for Korobov Functions: A\n  Constructive Approach",
        "Magnetoelasticity in MnPt $L1_{0}$ system",
        "Recoil nuclear size corrections in hydrogenic systems",
        "Learning Humanoid Standing-up Control across Diverse Postures",
        "Anomalous Chern-Simons orbital magnetoelectric coupling of\n  three-dimensional Chern insulators: gauge-discontinuity formalism and\n  adiabatic pumping"
      ],
      "abstract":[
        "We present a certified algorithm that takes a smooth algebraic curve in\n$\\mathbb{R}^n$ and computes an isotopic approximation for a generic projection\nof the curve into $\\mathbb{R}^2$. Our algorithm is designed for curves given\nimplicitly by the zeros of $n-1$ polynomials, but it can be partially extended\nto parametrically defined curves. The main challenge in correctly computing the\nprojection is to guarantee the topological correctness of crossings in the\nprojection. Our approach combines certified path tracking and interval\narithmetic in a two-step procedure: first, we construct an approximation to the\ncurve in $\\mathbb{R}^n$, and, second, we refine the approximation until the\ntopological correctness of the projection can be guaranteed. We provide a\nproof-of-concept implementation illustrating the algorithm.",
        "Quasar feedback is routinely invoked as an indispensable ingredient in galaxy\nformation models. Galactic outflows are a crucial agent of quasar feedback that\nfrequently manifest themselves in absorption and emission lines. Measuring the\nsize and energetics of outflows based on absorption lines remains a challenge,\nand integral-field spectroscopy (IFS) mapping in emission lines is\ncomplementary. We present a VLT\/SINFONI IFS mapping of quasar 3C 191 at $z \\sim\n2$, in which the outflow has been analyzed in absorption line spectroscopy.\nThree components are found based on the morphology and kinetics of\n[OIII]-emitting gas: a unshifted component which consistent with the systemic\nredshift and the location of the nucleus, a blueshifted in the north, and a\nredshifted in the south. The latter two components have velocities $\\sim$ 600\nkm s$^{-1}$ and projected extents of 5 and 11 kpc, respectively, suggesting a\nbiconical outflow structure. The blueshifted component's velocity is consistent\nwith that derived from absorption lines. Using the electron density measured by\nthe absorption lines and the luminosity and velocity of [OIII] outflow, we\nderive the mass outflow rate to be $\\dot{M} \\sim $ 9.5-13.4 M$_\\odot$ yr$^{-1}$\nand kinetic luminosity $\\dot{E}_{\\rm kin}$ ~ 2.5-3.7 $\\times 10^{42}$ erg\ns$^{-1}$, consistent with absorption line analyses with VLT\/Xshooter spectrum.\nThe kinetic luminosity is only 0.01% of the bolometric luminosity, rendering a\nrelatively weak outflow compared to typical expectation for effective feedback.",
        "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
        "Large-language models are capable of completing a variety of tasks, but\nremain unpredictable and intractable. Representation engineering seeks to\nresolve this problem through a new approach utilizing samples of contrasting\ninputs to detect and edit high-level representations of concepts such as\nhonesty, harmfulness or power-seeking. We formalize the goals and methods of\nrepresentation engineering to present a cohesive picture of work in this\nemerging discipline. We compare it with alternative approaches, such as\nmechanistic interpretability, prompt-engineering and fine-tuning. We outline\nrisks such as performance decrease, compute time increases and steerability\nissues. We present a clear agenda for future research to build predictable,\ndynamic, safe and personalizable LLMs.",
        "We report the results from a study of two massive ($M_{500c} > 6.0 \\times\n10^{14} M_{\\odot}$) strong lensing clusters selected from the South Pole\nTelescope cluster survey for their high Einstein radius ($R_E > 40''$),\nSPT-CLJ2325$-$4111 and SPT-CLJ0049$-$2440. Ground-based and shallow HST imaging\nindicated extensive strong lensing evidence in these fields, with giant arcs\nspanning 18\\arcsec\\ and 31\\arcsec, respectively, motivating further space-based\nimaging followup. Here, we present multiband HST imaging and ground-based\nMagellan spectroscopy of the fields, from which we compile detailed strong\nlensing models. The lens models of SPT-CL\\,J2325$-$4111 and\nSPT-CL\\,J0049$-$2440 were optimized using 9, and 8 secure multiple-imaged\nsystems with a final image-plane rms of 0\\farcs63 and 0\\farcs73, respectively.\nFrom the lensing analysis, we measure the projected mass density within 500~kpc\nof $M(<500 ~{\\rm kpc}) = 7.30\\pm0.07 \\times 10^{14}$$M_{\\odot}$, and $M(<500\n~{\\rm kpc})=7.12^{+0.16}_{-0.19}\\times 10^{14}$ $M_{\\odot}$ for these two\nclusters, and a sub-halos mass ratio of $0.12\\pm{0.01}$ and\n$0.21^{+0.07}_{-0.05}$, respectively. Both clusters produce a large area with\nhigh magnification ($\\mu\\geq 3$) for a source at $z=9$, $A^{lens}_{| \\mu | \\geq\n3 }=4.93^{+0.03}_{-0.04} arcmin^2$, and $A^{lens}_{| \\mu | \\geq 3\n}=3.64^{+0.14}_{-0.10} arcmin^2$ respectively, placing them in the top tier of\nstrong lensing clusters. We conclude that these clusters are spectacular\nsightlines for further observations that will reduce the systematic\nuncertainties due to cosmic variance. This paper provides the community with\ntwo additional well-calibrated cosmic telescopes, as strong as the Frontier\nFields, suitable for studies of the highly magnified background Universe.",
        "Networks of coupled nonlinear optical resonators have emerged as an important\nclass of systems in ultrafast optical science, enabling richer and more complex\nnonlinear dynamics compared to their single-resonator or travelling-wave\ncounterparts. In recent years, these coupled nonlinear optical resonators have\nbeen applied as application-specific hardware accelerators for computing\napplications including combinatorial optimization and artificial intelligence.\nIn this work, we rigorously prove a fundamental result showing that coupled\nnonlinear optical resonators are Turing-complete computers, which endows them\nwith much greater computational power than previously thought. Furthermore, we\nshow that the minimum threshold of hardware complexity needed for\nTuring-completeness is surprisingly low, which has profound physical\nconsequences. In particular, we show that several problems of interest in the\nstudy of coupled nonlinear optical resonators are formally undecidable. These\ntheoretical findings can serve as the foundation for better understanding the\npromise of next-generation, ultrafast all-optical computers.",
        "The study of Cowen-Douglas operators involves not only operator-theoretic\ntools but also complex geometry on holomorphic vector bundles. By leveraging\nthe properties of holomorphic vector bundles, this paper investigates the\ncyclicity of Cowen-Douglas tuples and demonstrates conclusively that every such\ntuple is cyclic.",
        "We introduce the Riemannian Proximal Sampler, a method for sampling from\ndensities defined on Riemannian manifolds. The performance of this sampler\ncritically depends on two key oracles: the Manifold Brownian Increments (MBI)\noracle and the Riemannian Heat-kernel (RHK) oracle. We establish high-accuracy\nsampling guarantees for the Riemannian Proximal Sampler, showing that\ngenerating samples with $\\varepsilon$-accuracy requires\n$O(\\log(1\/\\varepsilon))$ iterations in Kullback-Leibler divergence assuming\naccess to exact oracles and $O(\\log^2(1\/\\varepsilon))$ iterations in the total\nvariation metric assuming access to sufficiently accurate inexact oracles.\nFurthermore, we present practical implementations of these oracles by\nleveraging heat-kernel truncation and Varadhan's asymptotics. In the latter\ncase, we interpret the Riemannian Proximal Sampler as a discretization of the\nentropy-regularized Riemannian Proximal Point Method on the associated\nWasserstein space. We provide preliminary numerical results that illustrate the\neffectiveness of the proposed methodology.",
        "We produce many new complete, constant Q-curvature metrics on finitely\npunctured spheres by gluing together known examples. In our construction we\ntruncate one end of each summand and glue the two summands together\n\"end-to-end,\" where we've truncated them. We use this construction to show that\nthe unmarked moduli space of solutions with a fixed number of punctures is\ntopologically nontrivial provided the number of punctures is at least four.",
        "We conducted Monte Carlo simulations to analyze the percolation transition of\na non-symmetric loop model on a regular three-dimensional lattice. We\ncalculated the critical exponents for the percolation transition of this model.\nThe percolation transition occurs at a temperature that is close to, but not\nexactly the thermal critical temperature. Our finite-size study on this model\nyielded a correlation length exponent that agrees with that of the\nthree-dimensional XY model with an error margin of six per cent.",
        "We present a disease transmission model that considers both explicit and\nnon-explicit factors. This approach is crucial for accurate prediction and\ncontrol of infectious disease spread. In this paper, we extend the spread model\nfrom our previous works \\cite{ban2021mathematical,ban2023randomspread,\nban2023mathematical, ban2023spread} to a projected spread model that considers\nboth hidden and explicit types. Additionally, we provide the spread rate for\nthe projected spread model corresponding to the topological and random models.\nFurthermore, examples and numerical results are provided to illustrate the\ntheory.",
        "On a centennial timescale, solar activity was quantified based on records of\ninstrumental sunspot observations. This article briefly discusses several\naspects of the recent archival investigations of historical sunspot records in\nthe 17th to 18th centuries. This article also reviews the recent updates for\nthe active day fraction and positions of the reported sunspot groups of the\nMaunder Minimum to show their significance within the observational history.\nThese archival investigations serve as base datasets for reconstructing solar\nactivity.",
        "CLIP (Contrastive Language-Image Pre-training) has attained great success in\npattern recognition and computer vision. Transferring CLIP to downstream tasks\n(e.g. zero- or few-shot classification) is a hot topic in multimodal learning.\nHowever, current studies primarily focus on either prompt learning for text or\nadapter tuning for vision, without fully exploiting the complementary\ninformation and correlations among image-text pairs. In this paper, we propose\nan Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to\nfew-shot image classification tasks. This method captures fine-grained features\nby leveraging both visual features and textual descriptions of images. IDEA is\na training-free method for CLIP, and it can be comparable to or even exceeds\nstate-of-the-art models on multiple tasks. Furthermore, we introduce\nTrainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable\ncomponents (i.e., a projector and a learnable latent space), further enhancing\nthe model's performance and achieving SOTA results on 11 datasets. As one\nimportant contribution, we employ the Llama model and design a comprehensive\npipeline to generate textual descriptions for images of 11 datasets, resulting\nin a total of 1,637,795 image-text pairs, named \"IMD-11\". Our code and data are\nreleased at https:\/\/github.com\/FourierAI\/IDEA.",
        "In this work, we investigate the variational problem $$\\rho_x^\\ast =\n\\text{argmin}_{\\rho_x} D(G\\#\\rho_x, \\rho_y)\\,, $$ where $D$ quantifies the\ndifference between two probability measures, and ${G}$ is a forward operator\nthat maps a variable $x$ to $y=G(x)$. This problem can be regarded as an\nanalogue of its counterpart in linear spaces (e.g., Euclidean spaces),\n$\\text{argmin}_x \\|G(x) - y\\|^2$. Similar to how the choice of norm $\\|\\cdot\\|$\ninfluences the optimizer in $\\mathbb R^d$ or other linear spaces, the minimizer\nin the probabilistic variational problem also depends on the choice of $D$. Our\nfindings reveal that using a $\\phi$-divergence for $D$ leads to the recovery of\na conditional distribution of $\\rho_y$, while employing the Wasserstein\ndistance results in the recovery of a marginal distribution.",
        "We study fair vertex problem metatheorems on graphs within the scope of\nstructural parameterization in parameterized complexity. Unlike typical graph\nproblems that seek the smallest set of vertices satisfying certain properties,\nthe goal here is to find such a set that does not contain too many vertices in\nany neighborhood of any vertex. Formally, the task is to find a set $X\\subseteq\nV(G)$ of fair cost $k$, i.e., such that for all $v\\in V(G)$ $|X\\cap N(v)|\\le\nk$. Recently, Knop, Masa\\v{r}\\'ik, and Toufar [MFCS 2019] showed that all fair\nMSO$_1$ definable problems can be solved in FPT time parameterized by the twin\ncover of a graph. They asked whether such a statement would be achievable for a\nmore general parameterization of cluster vertex deletion, i.e., the smallest\nnumber of vertices required to be removed from the graph such that what remains\nis a collection of cliques.\n  In this paper, we prove that in full generality this is not possible by\ndemonstrating a W[1]-hardness. On the other hand, we give a sufficient property\nunder which a fair MSO$_1$ definable problem admits an FPT algorithm\nparameterized by the cluster vertex deletion number. Our algorithmic\nformulation is very general as it captures the fair variant of many natural\nvertex problems such as the Fair Feedback Vertex Set, the Fair Vertex Cover,\nthe Fair Dominating Set, the Fair Odd Cycle Transversal, as well as a connected\nvariant of thereof. Moreover, we solve the Fair $[\\sigma,\\rho]$-Domination\nproblem for $\\sigma$ finite, or $\\sigma=\\mathbb{N}$ and $\\rho$ cofinite.\nSpecifically, given finite or cofinite $\\rho\\subseteq \\mathbb{N}$ and finite\n$\\sigma$, or $\\rho\\subseteq \\mathbb{N}$ cofinite and $\\sigma=\\mathbb{N}$, the\ntask is to find set of vertices $X\\subseteq V(G)$ of fair cost at most $k$ such\nthat for all $v\\in X$, $|N(v)\\cap X|\\in\\sigma$ and for all $v\\in V(G)\\setminus\nX$, $|N(v)\\cap X|\\in\\rho$.",
        "In this paper, an improved clustering technique for large textual datasets by\nleveraging fine-tuned word embeddings is presented. WEClustering technique is\nused as the base model. WEClustering model is fur-ther improvements\nincorporating fine-tuning contextual embeddings, advanced dimensionality\nreduction methods, and optimization of clustering algorithms. Experimental\nresults on benchmark datasets demon-strate significant improvements in\nclustering metrics such as silhouette score, purity, and adjusted rand index\n(ARI). An increase of 45% and 67% of median silhouette score is reported for\nthe proposed WE-Clustering_K++ (based on K-means) and WEClustering_A++ (based\non Agglomerative models), respec-tively. The proposed technique will help to\nbridge the gap between semantic understanding and statistical robustness for\nlarge-scale text-mining tasks.",
        "As bug-finding methods improve, bug-fixing capabilities are exceeded,\nresulting in an accumulation of potential vulnerabilities. There is thus a need\nfor efficient and precise bug prioritization based on exploitability. In this\nwork, we explore the notion of control of an attacker over a vulnerability's\nparameters, which is an often overlooked factor of exploitability. We show that\ntaint as well as straightforward qualitative and quantitative notions of\ncontrol are not enough to effectively differentiate vulnerabilities. Instead,\nwe propose to focus analysis on feasible value sets, which we call domains of\ncontrol, in order to better take into account threat models and expert insight.\nOur new Shrink and Split algorithm efficiently extracts domains of control from\npath constraints obtained with symbolic execution and renders them in an easily\nprocessed, human-readable form. This in turn allows to automatically compute\nmore complex control metrics, such as weighted Quantitative Control, which\nfactors in the varying threat levels of different values. Experiments show that\nour method is both efficient and precise. In particular, it is the only one\nable to distinguish between vulnerabilities such as cve-2019-14192 and\ncve-2022-30552, while revealing a mistake in the human evaluation of\ncve-2022-30790. The high degree of automation of our tool also brings us closer\nto a fully-automated evaluation pipeline.",
        "The deployment of IoT (Internet of Things) sensor-based machine learning\nmodels in industrial systems for anomaly classification tasks poses significant\nchallenges due to distribution shifts, as the training data acquired in\ncontrolled laboratory settings may significantly differ from real-time data in\nproduction environments. Furthermore, many real-world applications cannot\nprovide a substantial number of labeled examples for each anomalous class in\nevery new environment. It is therefore crucial to develop adaptable machine\nlearning models that can be effectively transferred from one environment to\nanother, enabling rapid adaptation using normal operational data. We extended\nthis problem setting to an arbitrary classification task and formulated the\none-class domain adaptation (OC-DA) problem setting. We took a meta-learning\napproach to tackle the challenge of OC-DA, and proposed a task sampling\nstrategy to adapt any bi-level meta-learning algorithm to OC-DA. We modified\nthe well-established model-agnostic meta-learning (MAML) algorithm and\nintroduced the OC-DA MAML algorithm. We provided a theoretical analysis showing\nthat OC-DA MAML optimizes for meta-parameters that enable rapid one-class\nadaptation across domains. The OC-DA MAML algorithm is evaluated on the\nRainbow-MNIST meta-learning benchmark and on a real-world dataset of\nvibration-based sensor readings. The results show that OC-DA MAML significantly\nimproves the performance on the target domains and outperforms MAML using the\nstandard task sampling strategy.",
        "Purpose: This study proposes a novel anatomically-driven dynamic modeling\nframework for coronary arteries using skeletal skinning weights computation,\naiming to achieve precise control over vessel deformation while maintaining\nreal-time performance for surgical simulation applications. Methods: We\ndeveloped a computational framework based on biharmonic energy minimization for\nskinning weight calculation, incorporating volumetric discretization through\ntetrahedral mesh generation. The method implements temporal sampling and\ninterpolation for continuous vessel deformation throughout the cardiac cycle,\nwith mechanical constraints and volume conservation enforcement. The framework\nwas validated using clinical datasets from 5 patients, comparing interpolated\ndeformation results against ground truth data obtained from frame-by-frame\nsegmentation across cardiac phases. Results: The proposed framework effectively\nhandled interactive vessel manipulation. Geometric accuracy evaluation showed\nmean Hausdorff distance of 4.96 +- 1.78 mm and mean surface distance of 1.78 +-\n0.75 mm between interpolated meshes and ground truth models. The Branch\nCompleteness Ratio achieved 1.82 +- 0.46, while Branch Continuity Score\nmaintained 0.84 +- 0.06 (scale 0-1) across all datasets. The system\ndemonstrated capability in supporting real-time guidewire-vessel collision\ndetection and contrast medium flow simulation throughout the complete coronary\ntree structure. Conclusion: Our skinning weight-based methodology enhances\nmodel interactivity and applicability while maintaining geometric accuracy. The\nframework provides a more flexible technical foundation for virtual surgical\ntraining systems, demonstrating promising potential for both clinical practice\nand medical education applications. The code is available at\nhttps:\/\/github.com\/ipoirot\/DynamicArtery.",
        "Reinforcement learning (RL) in continuous action spaces encounters persistent\nchallenges, such as inefficient exploration and convergence to suboptimal\nsolutions. To address these limitations, we propose CAMEL, a novel framework\nintegrating LLM-generated suboptimal policies into the RL training pipeline.\nCAMEL leverages dynamic action masking and an adaptive epsilon-masking\nmechanism to guide exploration during early training stages while gradually\nenabling agents to optimize policies independently. At the core of CAMEL lies\nthe integration of Python-executable suboptimal policies generated by LLMs\nbased on environment descriptions and task objectives. Although simplistic and\nhard-coded, these policies offer valuable initial guidance for RL agents. To\neffectively utilize these priors, CAMEL employs masking-aware optimization to\ndynamically constrain the action space based on LLM outputs. Additionally,\nepsilon-masking gradually reduces reliance on LLM-generated guidance, enabling\nagents to transition from constrained exploration to autonomous policy\nrefinement. Experimental validation on Gymnasium MuJoCo environments\ndemonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated\npolicies significantly improve sample efficiency, achieving performance\ncomparable to or surpassing expert masking baselines. For Walker2d-v4, where\nLLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust\nRL performance without notable degradation, highlighting the framework's\nadaptability across diverse tasks. While CAMEL shows promise in enhancing\nsample efficiency and mitigating convergence challenges, these issues remain\nopen for further research. Future work aims to generalize CAMEL to multimodal\nLLMs for broader observation-action spaces and automate policy evaluation,\nreducing human intervention and enhancing scalability in RL training pipelines.",
        "We introduce ArcPro, a novel learning framework built on architectural\nprograms to recover structured 3D abstractions from highly sparse and\nlow-quality point clouds. Specifically, we design a domain-specific language\n(DSL) to hierarchically represent building structures as a program, which can\nbe efficiently converted into a mesh. We bridge feedforward and inverse\nprocedural modeling by using a feedforward process for training data synthesis,\nallowing the network to make reverse predictions. We train an encoder-decoder\non the points-program pairs to establish a mapping from unstructured point\nclouds to architectural programs, where a 3D convolutional encoder extracts\npoint cloud features and a transformer decoder autoregressively predicts the\nprograms in a tokenized form. Inference by our method is highly efficient and\nproduces plausible and faithful 3D abstractions. Comprehensive experiments\ndemonstrate that ArcPro outperforms both traditional architectural proxy\nreconstruction and learning-based abstraction methods. We further explore its\npotential to work with multi-view image and natural language inputs.",
        "Set function learning has emerged as a crucial area in machine learning,\naddressing the challenge of modeling functions that take sets as inputs. Unlike\ntraditional machine learning that involves fixed-size input vectors where the\norder of features matters, set function learning demands methods that are\ninvariant to permutations of the input set, presenting a unique and complex\nproblem. This survey provides a comprehensive overview of the current\ndevelopment in set function learning, covering foundational theories, key\nmethodologies, and diverse applications. We categorize and discuss existing\napproaches, focusing on deep learning approaches, such as DeepSets and Set\nTransformer based methods, as well as other notable alternative methods beyond\ndeep learning, offering a complete view of current models. We also introduce\nvarious applications and relevant datasets, such as point cloud processing and\nmulti-label classification, highlighting the significant progress achieved by\nset function learning methods in these domains. Finally, we conclude by\nsummarizing the current state of set function learning approaches and\nidentifying promising future research directions, aiming to guide and inspire\nfurther advancements in this promising field.",
        "Structure identification in cosmological simulations plays an important role\nin analysing simulation outputs. The definition of these structures directly\nimpacts the inferred properties derived from these simulations. This paper\nproposes a more straightforward definition and model of structure by focusing\non density peaks rather than halos and clumps. It introduces a new watershed\nalgorithm that uses phase-space analysis to identify structures, especially in\ncomplex environments where traditional methods may struggle due to spatially\noverlapping structures. Additionally, a merger tree code is introduced to track\ndensity peaks across timesteps, making use of the boosted potential for\nidentifying the most bound particles for each peak.",
        "This paper investigates approximation capabilities of two-dimensional (2D)\ndeep convolutional neural networks (CNNs), with Korobov functions serving as a\nbenchmark. We focus on 2D CNNs, comprising multi-channel convolutional layers\nwith zero-padding and ReLU activations, followed by a fully connected layer. We\npropose a fully constructive approach for building 2D CNNs to approximate\nKorobov functions and provide rigorous analysis of the complexity of the\nconstructed networks. Our results demonstrate that 2D CNNs achieve near-optimal\napproximation rates under the continuous weight selection model, significantly\nalleviating the curse of dimensionality. This work provides a solid theoretical\nfoundation for 2D CNNs and illustrates their potential for broader applications\nin function approximation.",
        "Magnetic materials play an important role in the industry. Except for common\nmaterial parameters such as magnetocrystalline anisotropy, magnetoelastic\nbehavior can be significant for application serving in various devices, e.g. in\nacoustic actuators, transducers, or sensors providing desirable fast response\nand high efficiency. Magnetoelasticit properties have been studied for the\ncubic system. However, their magnetoelastic behavior is weak. Therefore, we\nfocus on Pt-based tetragonal binary compounds bearing engaging magnetic\nbehavior. We evaluate MnPt magnetoelastic behavior using the finite\ndisplacement method, discussing the influence of the magnetic structure, and\nfocusing on the origin. While, the theoretically obtained results are compared\nto our performed experiments.",
        "Formulas for the combined nuclear-recoil and finite-nuclear-size effects of\norder $(Z\\,\\alpha)^5$ and $(Z\\,\\alpha)^6$ are derived without any expansion in\nthe nuclear charge radius $r_C$, making them applicable to both electronic and\nmuonic atoms. The obtained results are particularly relevant for high-precision\ndeterminations of root-mean-square charge radii from muonic atom spectroscopy.\nWe demonstrate that calculations of the atomic isotope shift based on the\nwidely used Breit approximation give rise to an unphysical nuclear-size\ncontribution that is linear in the nuclear charge radius $r_C$ at order\n$(Z\\,\\alpha)^5$. This spurious term vanishes in a full QED treatment, leaving\nthe correct contribution quadratic in $r_C$. For electronic atoms, this\nquadratic term is significantly smaller than the spurious linear contribution.",
        "Standing-up control is crucial for humanoid robots, with the potential for\nintegration into current locomotion and loco-manipulation systems, such as fall\nrecovery. Existing approaches are either limited to simulations that overlook\nhardware constraints or rely on predefined ground-specific motion trajectories,\nfailing to enable standing up across postures in real-world scenes. To bridge\nthis gap, we present HoST (Humanoid Standing-up Control), a reinforcement\nlearning framework that learns standing-up control from scratch, enabling\nrobust sim-to-real transfer across diverse postures. HoST effectively learns\nposture-adaptive motions by leveraging a multi-critic architecture and\ncurriculum-based training on diverse simulated terrains. To ensure successful\nreal-world deployment, we constrain the motion with smoothness regularization\nand implicit motion speed bound to alleviate oscillatory and violent motions on\nphysical hardware, respectively. After simulation-based training, the learned\ncontrol policies are directly deployed on the Unitree G1 humanoid robot. Our\nexperimental results demonstrate that the controllers achieve smooth, stable,\nand robust standing-up motions across a wide range of laboratory and outdoor\nenvironments. Videos are available at\nhttps:\/\/taohuang13.github.io\/humanoid-standingup.github.io\/.",
        "Chern-Simons orbital magnetoelectric (OME) coupling is usually the hallmark\nof nontrivial band topology in three-dimensional (3D) crystalline insulators.\nHowever, if a 3D insulator exhibits nonzero Chern number within any\ntwo-dimensional plane of the Brillouin zone, then traditionally the\nChern-Simons coupling becomes ill defined for such 3D Chern insulators due to\ntopological obstructions. In this work, by employing a ``gauge-discontinuity\"\nformalism, we resolve this long-standing issue and rigorously derive a\nquantized layer-resolved OME response in 3D Chern insulators. We demonstrate\nthat the difference of the layer-resolved OME coupling between adjacent layers\nis universally quantized in unit of $-C e^2\/h$, where $C$ is the Chern number.\nThis quantization arises from an anomalous contribution to the Chern-Simons OME\ncoupling, which is closely associated with the Berry curvature of the occupied\nbands and the hybrid Wannier centers along the direction of the Chern vector\n$(0,0, C)$. Furthermore, we demonstrate that the anomalous Chern-Simons\ncoupling can be transported by an exact integer quantum from one unit cell to\nits neighboring cell through an adiabatic cyclic pumping process, accompanied\nby a quantized displacement of Wannier center along the direction of the Chern\nvector. Our work provides a rigorous theoretical framework for understanding\nmagnetoelectric response in 3D Chern insulators and opens avenues for designing\ntopological quantum phenomena in layered systems."
      ]
    }
  },
  {
    "id":2411.18602,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Adding Conditional Control to Text-to-Image Diffusion Models",
    "start_abstract":"We present ControlNet, a neural network architecture to add spatial conditioning controls large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large models, and reuses their deep robust encoding layers with billions of images as strong backbone learn diverse set conditional controls. The is connected \"zero convolutions\" (zero-initialized convolution layers) that progressively grow parameters from zero ensure no harmful noise could affect finetuning. test various controls, e.g., edges, depth, segmentation, human pose, etc., Stable Diffusion, using single or multiple conditions, without prompts. show training ControlNets small (<50k) (>1m) datasets. Extensive results may facilitate wider applications control image",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Rsna pneumonia detection challenge"
      ],
      "abstract":[
        "In this competition, you\u2019re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs. Here\u2019s the backstory and why solving this problem matters. Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country. While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis. CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift. To improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA\u00ae) has reached out to Kaggle\u2019s machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge. The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018."
      ],
      "categories":[
        "Imaging"
      ]
    },
    "list":{
      "title":[
        "The relationship between galaxy size and halo properties: Insights from\n  the IllustrisTNG simulations and differential clustering",
        "A Geometric Interpretation of Virtual Knotoids",
        "A simple model for entangled photon generation in resonant structures",
        "Validating uncertainty propagation approaches for two-stage Bayesian\n  spatial models using simulation-based calibration",
        "A versatile experimental method to measure the traction forces at\n  interfaces",
        "Spin-dependent dark matter scattering in quasi-two-dimensional magnets",
        "New Construction of Locally q-ary Sequential Recoverable Codes:\n  Parity-check Matrix Approach",
        "A higher algebraic approach to liftings of modules over derived\n  quotients",
        "Second-Order $\\Gamma$-Limit for the Cahn-Hilliard Functional with\n  Dirichlet Boundary Conditions, I",
        "Transmission Probability in Double Quantum Well with Triple Barrier",
        "Dynamic Circuits for the Quantum Lattice-Boltzmann Method",
        "Unveiling individual and collective temporal patterns in the tanker\n  shipping network",
        "Tracking time-varying signals with quantum-enhanced atomic magnetometers",
        "Duality viewpoint of noninvertible symmetry protected topological phases",
        "Actions Speak Louder Than Words: Rate-Reward Trade-off in Markov\n  Decision Processes",
        "Evolution of the pseudogap band structure in a system of\n  electron-correlated lattice polarons",
        "Semiclassical trace formula for the Bochner-Schr\\\"odinger operator",
        "Geometry-Driven Moir\\'e Engineering in Twisted Bilayers of\n  High-Pseudospin Fermions",
        "Detecting LHC Neutrinos at Surface Level",
        "Critical quasilinear equations on Riemannian manifolds",
        "Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning\n  Solution for Protocols Compliance",
        "5G Channel Models for Railway Use Cases at mmWave Band and the Path\n  Towards Terahertz",
        "Soliquidy: a descriptor for atomic geometrical confusion",
        "The Venusian Chronicles",
        "Improved amplitude amplification strategies for the quantum simulation\n  of classical transport problems",
        "Optimal regulation in a periodic environment: insights from a simple\n  model",
        "Quantum state exclusion for group-generated ensembles of pure states",
        "Parabolic dielectric reflector for extreme on-chip spot-size conversion\n  with broad bandwidth",
        "Search for the Cabibbo-suppressed decays\n  $\\Lambda_c^{+}\\to\\Sigma^0K^{+}\\pi^{0}$ and\n  $\\Lambda_c^{+}\\to\\Sigma^0K^{+}\\pi^{+}\\pi^{-}$"
      ],
      "abstract":[
        "The physical origin of the radial sizes of galaxies and how galaxy sizes are\ncorrelated with the properties of their host dark matter halos is an open\nquestion in galaxy formation. In observations, the large-scale clustering of\ngalaxies selected by stellar mass is significantly different for large and\nsmall galaxies, and Behroozi et al. (2022) showed that these results are in\ntension with some of the correlations between galaxy size and halo properties\nin the literature. We analyze the IllustrisTNG suite of large volume\ncosmological hydrodynamic simulations along with dark matter only simulations\nwith matched initial conditions. We investigate correlations between the ratio\nof galaxy size to halo virial radius ($r_{\\rm gal}\/R_{\\rm vir}$) and halo spin,\nconcentration, and formation time at redshift 0-3. We find a significant\ncorrelation between $r_{\\rm gal}\/R_{\\rm vir}$ and concentration, but only above\na critical value $c \\simeq 16$, and we also find a correlation between $r_{\\rm\ngal}\/R_{\\rm vir}$ and halo formation time. We suggest that galaxy formation\nhistory and environment, in addition to halo properties at a given output time,\nplay an important role in shaping galaxy size. In addition, we directly measure\nsize-based differential clustering in the TNG300 simulation and compare\ndirectly with the observational results. We find significant scale-dependent\nsize-based differential clustering in TNG, in qualitative agreement with\nobservations. However, correlations between $r_{\\rm gal}\/R_{\\rm vir}$ and\nsecondary halo properties are not the drivers of the differential clustering in\nthe simulations; instead, we find that most of this signal in TNG arises from\nsatellite galaxies.",
        "In this paper, we give a three-dimensional geometric interpretation of\nvirtual knotoids. Then we show that virtual knotoid theory is a generalization\nof classical knotoid theory by proving a conjecture of Kauffman and the first\nauthor given in arXiv:1602.03579.",
        "The ability to engineer pairs of entangled photons is essential to quantum\ninformation science, and generating these states using spontaneous parametric\ndown-conversion (SPDC) in nano- and micrometer-scale materials offers numerous\nadvantages. To properly engineer such sources, a reliable model describing\nnano- and micrometer-scale SPDC is necessary; however, such a theoretical\ndescription remains a challenge. Here, we propose and derive a simplified model\nto describe SPDC in resonant structures, which considers the generation of\nphoton pairs and the resonant enhancement of spectral bands to be separate\nprocesses, even though they actually occur simultaneously. We compare our\nsimplified model to both the rigorous theory of SPDC in an etalon - a simple\nexample of a resonant structure - and our experiments on SPDC in etalons and\nfind agreement for low-gain SPDC. By simplifying the calculations required to\ngenerate photon pairs, our model promises to make designing complex resonant\nstructures easier, and it promises to hasten the iteration of designs across\nthe field of quantum state engineering.",
        "This work tackles the problem of uncertainty propagation in two-stage\nBayesian models, with a focus on spatial applications. A two-stage modeling\nframework has the advantage of being more computationally efficient than a\nfully Bayesian approach when the first-stage model is already complex in\nitself, and avoids the potential problem of unwanted feedback effects. Two ways\nof doing two-stage modeling are the crude plug-in method and the posterior\nsampling method. The former ignores the uncertainty in the first-stage model,\nwhile the latter can be computationally expensive. This paper validates the two\naforementioned approaches and proposes a new approach to do uncertainty\npropagation, which we call the $\\mathbf{Q}$ uncertainty method, implemented\nusing the Integrated Nested Laplace Approximation (INLA). We validate the\ndifferent approaches using the simulation-based calibration method, which tests\nthe self-consistency property of Bayesian models. Results show that the crude\nplug-in method underestimates the true posterior uncertainty in the\nsecond-stage model parameters, while the resampling approach and the proposed\nmethod are correct. We illustrate the approaches in a real life data\napplication which aims to link relative humidity and Dengue cases in the\nPhilippines for August 2018.",
        "Measurement of surface forces, including cohesive forces and contact forces,\nis critical for understanding and controlling interactions at interfaces to\noptimize the interfacial performance of applications. The objective of this\npaper is to introduce a general in-situ method that enables the measurement of\n3D micron-scale displacements and corresponding force distribution at\ninterfaces in dry or wet environment. Stereo digital image correlation was used\nto measure the 3D-displacement of a soft and deformable substrate. The\nefficiency and accuracy of the technique were evaluated by applying compression\nto the substrate using a steel ball, with the measured 3D displacements\naligning closely with finite element analysis simulations. To further assess\nthe method's applicability, the wet adhesion between mussel plaques and\nsubstrate was tested under aqueous conditions. The interfacial displacements\nand forces at different stages during the test were measured. The application\nof the technique can be extended for varied circumstances regarding force range\nand substrate materials based on Winkler Spring model.",
        "We study the prospects of detecting dark matter coupled to the spin of the\nelectron, such that it may scatter and excite magnons - collective excitations\nof electronic spins. We show that materials exhibiting long-range magnetic\norder where the spins are coupled only along a plane may act as directional\ndark matter detectors. These quasi-2D materials possess anisotropic dispersion\nrelations and structure functions which induce a sidereal modulation in the\nexcitation rate. We calculate the expected signal rate for some candidate\n(anti)ferromagnets, demonstrating a possible route to the direct detection of\nspin-dependent dark matter in the keV to MeV mass range.",
        "This paper develops a new family of locally recoverable codes for distributed\nstorage systems, Sequential Locally Recoverable Codes (SLRCs) constructed to\nhandle multiple erasures in a sequential recovery approach. We propose a new\nconnection between parallel and sequential recovery, which leads to a general\nconstruction of q-ary linear codes with information $(r, t_i,\n\\delta)$-sequential-locality where each of the $i$-th information symbols is\ncontained in $t_i$ punctured subcodes with length $(r+\\delta-1)$ and minimum\ndistance $\\delta$. We prove that such codes are $(r, t)_q$-SLRC ($t \\geq \\delta\nt_i+1$), which implies that they permit sequential recovery for up to $t$\nerasures each one by $r$ other code symbols.",
        "We show a certain existence of a lifting of modules under the\nself-$\\mathrm{Ext}^2$-vanishing condition over the \"derived quotient\" by using\nthe notion of higher algebra. This refines a work of Auslander--Ding--Solberg's\nsolution of the Auslander--Reiten conjecture for complete interesctions.\nTogether with Auslander's zero-divisor theorem, we show that the existence of\nsuch $\\mathrm{Ext}$-vanishing module over derived quotients is equivalent to\nbeing local complete intersections.",
        "This paper addresses the asymptotic development of order 2 by the $\\Gamma$\n-convergence of the Cahn-Hilliard functional with Dirichlet boundary\nconditions. The Dirichlet data are assumed to be well separated from one of the\ntwo wells. In the case where there are no interfaces, it is shown that there is\na transition layer near the boundary of the domain.",
        "Quantum well of AlGaAs\/GaAs is very important to study transport properties\nof electrons due to its wider application in electronic devices. Hence, the\ndouble well of AlGaAs\/GaAs with triple barrier is taken to study transmission\nprobability. Transmission probability is found to decrease with the increase in\nthe height and width of the barrier. Transmission probability with energy of\nelectron shows two peaks while taking all three barrier of the same height.\nWhereas a single and higher value of peak is found when the height of the\ncentral barrier is slightly reduced.",
        "We propose a quantum algorithm for the linear advection-diffusion equation\n(ADE) Lattice-Boltzmann method (LBM) that leverages dynamic circuits. Dynamic\nquantum circuits allow for an optimized collision-operator quantum algorithm,\nintroducing partial measurements as an integral step. Efficient adaptation of\nthe quantum circuit during execution based on digital information obtained\nthrough mid-circuit measurements is achieved. The proposed new collision\nalgorithm is implemented as a fully unitary operator, which facilitates the\ncomputation of multiple time steps without state reinitialization. Unlike\nprevious quantum collision operators that rely on linear combinations of\nunitaries, the proposed algorithm does not exhibit a probabilistic failure\nrate. Moreover, additional qubits no longer depend on the chosen velocity set,\nwhich reduces both qubit overhead and circuit complexity. Validation of the\nquantum collision algorithm is performed by comparing results with digital LBM\nin one and two dimensions, demonstrating excellent agreement. Performance\nanalysis for multiple time steps highlights advantages compared to previous\nmethods. As an additional variant, a hybrid quantum-digital approach is\nproposed, which reduces the number of mid-circuit measurements, therefore\nimproving the efficiency of the quantum collision algorithm.",
        "The global shipping network, which moves over 80% of the world's goods, is\nnot only a vital backbone of the global economy but also one of the most\npolluting industries. Studying how this network operates is crucial for\nimproving its efficiency and sustainability. While the transport of solid goods\nlike packaged products and raw materials has been extensively researched, far\nless is known about the competitive trade of crude oil and petroleum, despite\nthese commodities accounting for nearly 30% of the market. Using 4 years of\nhigh-resolution data on oil tanker movements, we employ sequential motif mining\nand dynamic mode decomposition to uncover global spatio-temporal patterns in\nthe movement of individual ships. Across all ship classes, we demonstrate that\nmaximizing the proportion of time ships spend carrying cargo -- a metric of\nefficiency -- is achieved through strategic diversification of routes and the\neffective use of intra-regional ports for trips without cargo. Moreover, we\nuncover a globally stable travel structure in the fleet, with pronounced\nseasonal variations linked to annual and semi-annual regional climate patterns\nand economic cycles. Our findings highlight the importance of integrating\nhigh-resolution data with innovative analysis methods not only to improve our\nunderstanding of the underlying dynamics of shipping patterns, but to design\nand evaluate strategies aimed at reducing their environmental impact.",
        "Quantum entanglement, in the form of spin squeezing, is known to improve the\nsensitivity of atomic instruments to static or slowly-varying quantities.\nSensing transient events presents a distinct challenge, requires different\nanalysis methods, and has not been shown to benefit from entanglement in\npractically-important scenarios such as spin-precession magnetometry (SPM).\nHere we adapt estimation control techniques introduced in\narXiv:2403.14764(2024) to the experimental setting of SPM and analogous\ntechniques. We demonstrate that real-time tracking of fluctuating fields\nbenefits from measurement-induced spin-squeezing and that quantum limits\ndictated by decoherence are within reach of today's experiments. We illustrate\nthis quantum advantage by single-shot tracking, within the coherence time of a\nspin-precession magnetometer, of a magnetocardiography signal overlain with\nbroadband noise.",
        "Recent advancements in generalized symmetries have drawn significant\nattention to gapped phases of matter exhibiting novel symmetries, such as\nnoninvertible symmetries. By leveraging the duality transformations, the\nclassification and construction of gapped phases with noninvertible symmetry\ncan be mapped to those involving conventional group symmetries. We demonstrate\nthis approach by classifying symmetry-protected topological phases with a broad\nclass of noninvertible symmetries in arbitrary spacetime dimensions. Our\nresults reveal new classifications that extend beyond those based on group\nsymmetries. Additionally, we construct lattice models in $(1+1)d$ and $(2+1)d$\nthat realize these new phases and explore their anomalous interfaces.",
        "The impact of communication on decision-making systems has been extensively\nstudied under the assumption of dedicated communication channels. We instead\nconsider communicating through actions, where the message is embedded into the\nactions of an agent which interacts with the environment in a Markov decision\nprocess (MDP) framework. We conceptualize the MDP environment as a finite-state\nchannel (FSC), where the actions of the agent serve as the channel input, while\nthe states of the MDP observed by another agent (i.e., receiver) serve as the\nchannel output. Here, we treat the environment as a communication channel over\nwhich the agent communicates through its actions, while at the same time,\ntrying to maximize its reward. We first characterize the optimal information\ntheoretic trade-off between the average reward and the rate of reliable\ncommunication in the infinite-horizon regime. Then, we propose a novel\nframework to design a joint control\/coding policy, termed \\textit{Act2Comm},\nwhich seamlessly embeds messages into actions. From a communication\nperspective, \\textit{Act2Comm} functions as a learning-based channel coding\nscheme for non-differentiable FSCs under input-output constraints. From a\ncontrol standpoint, \\textit{Act2Comm} learns an MDP policy that incorporates\ncommunication capabilities, though at the cost of some control performance.\nOverall, \\textit{Act2Comm} effectively balances the dual objectives of control\nand communication in this environment. Experimental results validate\n\\textit{Act2Comm}'s capability to enable reliable communication while\nmaintaining a certain level of control performance.",
        "The evolution of the role of lattice vibrations in the formation of the\npseudogap state in strongly correlated electron systems has been investigated\nconcerning changes in the electron-phonon coupling parameters and the\nconcentration of doped charge carriers. We apply the polaronic version of the\ngeneralized tight-binding method to analyze the band structure of a realistic\nmultiband two-dimensional model that incorporates the electron-lattice\ncontributions of both Holstein and Peierls types. It has been demonstrated that\nthe emergence of polaronic effects begins with the modulation of spectral\nfunction intensity. However, within a specific region of the phase diagram, a\nsignificant transformation of the electron band structure and pseudogap state\noccurs. It results from coherent polaron excitations that create a partially\nflat band near the Fermi level. This process leads to a change in the topology\nof the Fermi surface and the emergence of corresponding features in the density\nof states.",
        "We study the semiclassical Bochner-Schr\\\"odinger operator\n$H_{p}=\\frac{1}{p^2}\\Delta^{L^p\\otimes E}+V$ on tensor powers $L^p$ of a\nHermitian line bundle $L$ twisted by a Hermitian vector bundle $E$ on a\nRiemannian manifold of bounded geometry. For any function $\\varphi\\in\nC^\\infty_c(\\mathbb R)$, we consider the bounded linear operator $\\varphi(H_p)$\nin $L^2(X,L^p\\otimes E)$ defined by the spectral theorem. We prove that its\nsmooth Schwartz kernel on the diagonal admits a complete asymptotic expansion\nin powers of $p^{-1}$ in the semiclassical limit $p\\to \\infty$. In particular,\nwhen the manifold is compact, we get a complete asymptotic expansion for the\ntrace of $\\varphi(H_p)$.",
        "Moir\\'e engineering offers new pathways for manipulating emergent states in\ntwisted layered materials and lattice-mismatched heterostructures. With the key\nrole of the geometry of the underlying lattice in mind, here we introduce the\nwatermill lattice, a two-dimensional structure with low-energy states\ncharacterized by massless pseudospin-3\/2 fermions with high winding numbers.\nIts twisted bilayer is shown to exhibit magic angles, where four isolated flat\nbands emerge around the Fermi level, featuring elevated Wilson-loop windings\nand enhanced quantum geometric effects, such as an increase in the ratio of the\nBerezinskii-Kosterlitz-Thouless (BKT) transition temperature to the mean-field\ncritical temperature under a weak Bardeen-Cooper-Schrieffer (BCS) pairing. We\ndiscuss how the watermill lattice could be realized in the MXene and group-IV\nmaterials. Our study highlights the potential of exploiting lattice geometry in\nmoir\\'e engineering to uncover novel quantum phenomena and tailor emergent\nelectronic properties in materials.",
        "The first direct detection of neutrinos at the LHC not only marks the\nbeginning of a novel collider neutrino program at CERN but also motivates\nconsidering additional neutrino detectors to fully exploit the associated\nphysics potential. We investigate the feasibility and physics potential of\nneutrino experiments located at the surface-level. A topographic desk study was\nperformed to identify all points at which the LHC's neutrino beams exit the\nearth. The closest location lies about 9 km east of the CMS interaction point,\nat the bottom of Lake Geneva. Several detectors to be placed at this location\nare considered, including a water Cherenkov detector and an emulsion detector.\nThe detector concepts are introduced, and projections for their contribution to\nthe LHC forward neutrino program and searches for dark sector particles are\npresented. However, the dilution of the neutrino flux over distance reduces the\nneutrino yield significantly, limiting the physics potential of surface-level\ndetectors compared to ones closer to the interaction point, including the\nproposed FPF.",
        "In this paper, we investigate critical quasilinear elliptic partial\ndifferential equations on a complete Riemannian manifold with nonnegative Ricci\ncurvature. By exploiting a new and sharp nonlinear Kato inequality and\nestablishing some Cheng-Yau type gradient estimates for positive solutions, we\nclassify positive solutions to the critical $p$-Laplace equation and show\nrigidity concerning the ambient manifold. Our results extend and improve some\nprevious conclusions in the literature. Similar results are obtained for\nsolutions to the quasilinear Liouville equation involving the $n$-Laplace\noperator, where $n$ corresponds to the dimension of the ambient manifold.",
        "Neural network-based decoding methods have shown promise in enhancing error\ncorrection performance, but traditional approaches struggle with the challenges\nposed by punctured codes. In particular, these methods fail to address the\ncomplexities of variable code rates and the need for protocol compatibility.\nThis paper presents a unified Long Short-Term Memory (LSTM)-based decoding\narchitecture specifically designed to overcome these challenges. The proposed\nmethod unifies punctured convolutional and Turbo codes. A puncture embedding\nmechanism integrates puncturing patterns directly into the network, enabling\nseamless adaptation to varying code rates, while balanced bit error rate\ntraining ensures robustness across different code lengths, rates, and channels,\nmaintaining protocol flexibility. Extensive simulations in Additive White\nGaussian Noise and Rayleigh fading channels demonstrate that the proposed\napproach outperforms conventional decoding techniques, providing significant\nimprovements in decoding accuracy and robustness. These results underscore the\npotential of LSTM-based decoding as a promising solution for next-generation\nartificial intelligence powered communication systems.",
        "High-speed trains are one of the most relevant scenarios for the\nfifth-generation (5G) mobile communications and the \"smart rail mobility\"\nvision, where a high-data-rate wireless connectivity with up to several GHz\nbandwidths will be required. This is a strong motivation for the exploration of\nmillimeter wave (mmWave) band. In this article, we identify the main challenges\nand make progress towards realistic 5G mmWave channel models for railway use\ncases. In order to cope with the challenge of including the railway features in\nthe channel models, we define reference scenarios to help the parameterization\nof channel models for railway use at mmWave band. Simulations and the\nsubsequent measurements used to validate the model reflect the detailed\ninfluence of railway objects and the accuracy of the simulations. Finally, we\npoint out the future directions towards the full version of the smart rail\nmobility which will be powered by terahertz (THz) communications.",
        "Tailoring material properties often requires understanding the solidification\nprocess. Herein, we introduce the geometric descriptor Soliquidy, which\nnumerically captures the Euclidean transport cost between the translationally\ndisordered versus ordered states of a materials. As a testbed, we apply\nSoliquidy to the classification of glass-forming metal alloys. By extending and\ncombining an experimental library of metallic thin-films (glass\/no-glass) with\nthe aflow.org computational database (geometrical and energetic information of\nmixtures) we found that the combination of Soliquity and formation enthalpies\ngenerates an effective classifier for glass formation. Such classifier is then\nused to tackle a public dataset of metallic glasses showing that the\nglass-agnostic assumptions of Soliquity can be useful for understanding\nkinetically-controlled phase transitions.",
        "Venus' atmosphere -- specifically its clouds buoyed up 40 to 60 km above the\nsurface -- has long been suspected to encompass a biosphere where Earth-like\nliving organisms could grow and flourish. This idea has been recently rekindled\nby the observation (signal-to-noise ratio of about 15$\\sigma$) of a phosphine\n(PH$_3$) absorption-line profile against the thermal background from deeper,\nhotter layers of the atmosphere. There is a chance that this observation could\nbe a sign of life, because the PH$_3$ gas observed on Earth originates mostly\nin decaying organic material. Furthermore, it has been shown that there is no\nother natural process on Venus that could otherwise produce the observed PH$_3$\nabsorption line. On the other hand, cosmic rays and the particle cascades they\nproduce in the Earth's atmosphere are hazardous to living organisms, because\nthe ionizing radiation produced in air showers can blow apart chemical\nmolecules and disrupt biochemical processes, causing cells to die or undergo\ndangerous mutations. Compared to Earth, the hypothesized biosphere of Venus\ncould be exposed to substantially more ionizing radiation. This is because\nVenus has no protective intrinsic magnetic field, orbits closer to the Sun, and\nthe entire eventual habitable region lies in the clouds high in the atmosphere.\nThereby, if the clouds were sterilized there would be no reservoir of deeper\nlife that can recolonize afterwards. In this communication we study the effects\nof particle cascades in the venusian atmosphere using the AIRES simulation\npackage properly configured. We show that the effects of cosmic radiation in\nthe habitable zone would be comparable to those on the Earth's surface and so\nwould not have any hazardous effect on possible venusian microorganisms.",
        "The quantum simulation of classical fluids often involves the use of\nprobabilistic algorithms that encode the result of the dynamics in the form of\nthe amplitude of the selected quantum state. In most cases, however, the\namplitude probability is too low to allow an efficient use of these algorithms,\nthereby hindering the practical viability of the quantum simulation. The\noblivious amplitude amplification algorithm is often presented as a solution to\nthis problem, but to no avail for most classical problems, since its\napplicability is limited to unitary dynamics. In this paper, we show\nanalytically that oblivious amplitude amplification when applied to non-unitary\ndynamics leads to a distortion of the quantum state and to an accompanying\nerror in the quantum update. We provide an analytical upper bound of such error\nas a function of the degree of non-unitarity of the dynamics and we test it\nagainst a quantum simulation of an advection-diffusion-reaction equation, a\ntransport problem of major relevance in science and engineering. Finally, we\nalso propose an amplification strategy that helps mitigate the distortion\nerror, while still securing an enhanced success probability.",
        "We perform a detailed study of a simple mathematical model addressing the\nproblem of optimally regulating a process subject to periodic external forcing,\nwhich is interesting both in view of its direct applications and as a prototype\nfor more general problems. In this model one must determine an optimal\ntime-periodic `effort' profile, and the natural setting for the problem is in a\nspace of periodic non-negative measures. We prove that there exists a unique\nsolution for the problem in the space of measures, and then turn to\ncharacterizing this solution. Under some regularity conditions on the problem's\ndata, we prove that its solution is an absolutely continuous measure, and\nprovide an explicit formula for the measure's density. On the other hand, when\nthe problem's data is discontinuous, the solution measure can also include\natomic components. Complementing our analytical results, we carry out numerical\ncomputations to obtain solutions of the problem in various instances, which\nenable us to examine the interesting ways in which the solution's structure\nvaries as the problem's data is varied.",
        "Quantum state exclusion is the task of determining which states from a given\nset a system was not prepared in. We provide a complete solution to optimal\nquantum state exclusion for arbitrary sets of pure states generated by finite\ngroups, establishing necessary and sufficient conditions for perfect\n(zero-error conclusive) exclusion. When perfect exclusion is impossible, we\nintroduce two natural extensions: minimum-error and unambiguous exclusion. For\nboth, we derive the optimal protocols and present analytical expressions for\nthe corresponding failure probabilities and measurements, offering a new\nperspective on how quantum states encode information.",
        "Spot-size converters are key for efficient coupling of light between\nwaveguides of different sizes. While adiabatic tapers are well suited for small\nsize differences, they become impractically long for expansion factors around\nx100 which are often required when coupling integrated waveguides and\nfree-space beams. Evanescent couplers and bragg deflectors can be used in this\nscenario, but their operation is inherently limited in bandwidth. Here we\npropose a solution based on a parabolic dielectric interface that couples light\nfrom a 0.5 um-wide waveguide to a 285 um-wide waveguide, i.e. an expansion\nfactor of x570. We experimentally demonstrate an unprecedented bandwidth of\nmore than 380 nm with insertion losses below 0.35 dB. We furthermore provide\nanalytical expressions for the design of such parabolic spot-size-converters\nfor arbitrary expansion factors.",
        "Utilizing 4.5 $fb^-$ of $e^+e^-$ annihilation data collected at\ncenter-of-mass energies ranging from 4599.53 MeV to 4698.82 MeV by the BESIII\ndetector at the BEPCII collider, we search for the singly Cabibbo-suppressed\nhadronic decays $\\Lambda_{c}^{+}\\to\\Sigma^{0} K^{+}\\pi^{0}$ and\n$\\Lambda_{c}^{+}\\to\\Sigma^{0}K^{+}\\pi^+\\pi^-$ with a single-tag method. No\nsignificant signals are observed for both decays. The upper limits on the\nbranching fractions at the $90\\%$ confidence level are determined to be\n$5.0\\times 10^{-4}$ for $\\Lambda_{c}^{+}\\to\\Sigma^{0} K^{+}\\pi^{0}$ and\n$6.5\\times 10^{-4}$ for $\\Lambda_c^{+}\\to\\Sigma^0K^{+}\\pi^{+}\\pi^{-}$."
      ]
    }
  },
  {
    "id":2411.18602,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Highresolution image synthesis with latent diffusion models",
    "start_abstract":"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Rsna pneumonia detection challenge"
      ],
      "abstract":[
        "In this competition, you\u2019re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs. Here\u2019s the backstory and why solving this problem matters. Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country. While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis. CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift. To improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA\u00ae) has reached out to Kaggle\u2019s machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge. The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review. Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018."
      ],
      "categories":[
        "Imaging"
      ]
    },
    "list":{
      "title":[
        "The relationship between galaxy size and halo properties: Insights from\n  the IllustrisTNG simulations and differential clustering",
        "A Geometric Interpretation of Virtual Knotoids",
        "A simple model for entangled photon generation in resonant structures",
        "Validating uncertainty propagation approaches for two-stage Bayesian\n  spatial models using simulation-based calibration",
        "A versatile experimental method to measure the traction forces at\n  interfaces",
        "Spin-dependent dark matter scattering in quasi-two-dimensional magnets",
        "New Construction of Locally q-ary Sequential Recoverable Codes:\n  Parity-check Matrix Approach",
        "A higher algebraic approach to liftings of modules over derived\n  quotients",
        "Second-Order $\\Gamma$-Limit for the Cahn-Hilliard Functional with\n  Dirichlet Boundary Conditions, I",
        "Transmission Probability in Double Quantum Well with Triple Barrier",
        "Dynamic Circuits for the Quantum Lattice-Boltzmann Method",
        "Unveiling individual and collective temporal patterns in the tanker\n  shipping network",
        "Tracking time-varying signals with quantum-enhanced atomic magnetometers",
        "Duality viewpoint of noninvertible symmetry protected topological phases",
        "Actions Speak Louder Than Words: Rate-Reward Trade-off in Markov\n  Decision Processes",
        "Evolution of the pseudogap band structure in a system of\n  electron-correlated lattice polarons",
        "Semiclassical trace formula for the Bochner-Schr\\\"odinger operator",
        "Geometry-Driven Moir\\'e Engineering in Twisted Bilayers of\n  High-Pseudospin Fermions",
        "Detecting LHC Neutrinos at Surface Level",
        "Critical quasilinear equations on Riemannian manifolds",
        "Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning\n  Solution for Protocols Compliance",
        "5G Channel Models for Railway Use Cases at mmWave Band and the Path\n  Towards Terahertz",
        "Soliquidy: a descriptor for atomic geometrical confusion",
        "The Venusian Chronicles",
        "Improved amplitude amplification strategies for the quantum simulation\n  of classical transport problems",
        "Optimal regulation in a periodic environment: insights from a simple\n  model",
        "Quantum state exclusion for group-generated ensembles of pure states",
        "Parabolic dielectric reflector for extreme on-chip spot-size conversion\n  with broad bandwidth",
        "Search for the Cabibbo-suppressed decays\n  $\\Lambda_c^{+}\\to\\Sigma^0K^{+}\\pi^{0}$ and\n  $\\Lambda_c^{+}\\to\\Sigma^0K^{+}\\pi^{+}\\pi^{-}$"
      ],
      "abstract":[
        "The physical origin of the radial sizes of galaxies and how galaxy sizes are\ncorrelated with the properties of their host dark matter halos is an open\nquestion in galaxy formation. In observations, the large-scale clustering of\ngalaxies selected by stellar mass is significantly different for large and\nsmall galaxies, and Behroozi et al. (2022) showed that these results are in\ntension with some of the correlations between galaxy size and halo properties\nin the literature. We analyze the IllustrisTNG suite of large volume\ncosmological hydrodynamic simulations along with dark matter only simulations\nwith matched initial conditions. We investigate correlations between the ratio\nof galaxy size to halo virial radius ($r_{\\rm gal}\/R_{\\rm vir}$) and halo spin,\nconcentration, and formation time at redshift 0-3. We find a significant\ncorrelation between $r_{\\rm gal}\/R_{\\rm vir}$ and concentration, but only above\na critical value $c \\simeq 16$, and we also find a correlation between $r_{\\rm\ngal}\/R_{\\rm vir}$ and halo formation time. We suggest that galaxy formation\nhistory and environment, in addition to halo properties at a given output time,\nplay an important role in shaping galaxy size. In addition, we directly measure\nsize-based differential clustering in the TNG300 simulation and compare\ndirectly with the observational results. We find significant scale-dependent\nsize-based differential clustering in TNG, in qualitative agreement with\nobservations. However, correlations between $r_{\\rm gal}\/R_{\\rm vir}$ and\nsecondary halo properties are not the drivers of the differential clustering in\nthe simulations; instead, we find that most of this signal in TNG arises from\nsatellite galaxies.",
        "In this paper, we give a three-dimensional geometric interpretation of\nvirtual knotoids. Then we show that virtual knotoid theory is a generalization\nof classical knotoid theory by proving a conjecture of Kauffman and the first\nauthor given in arXiv:1602.03579.",
        "The ability to engineer pairs of entangled photons is essential to quantum\ninformation science, and generating these states using spontaneous parametric\ndown-conversion (SPDC) in nano- and micrometer-scale materials offers numerous\nadvantages. To properly engineer such sources, a reliable model describing\nnano- and micrometer-scale SPDC is necessary; however, such a theoretical\ndescription remains a challenge. Here, we propose and derive a simplified model\nto describe SPDC in resonant structures, which considers the generation of\nphoton pairs and the resonant enhancement of spectral bands to be separate\nprocesses, even though they actually occur simultaneously. We compare our\nsimplified model to both the rigorous theory of SPDC in an etalon - a simple\nexample of a resonant structure - and our experiments on SPDC in etalons and\nfind agreement for low-gain SPDC. By simplifying the calculations required to\ngenerate photon pairs, our model promises to make designing complex resonant\nstructures easier, and it promises to hasten the iteration of designs across\nthe field of quantum state engineering.",
        "This work tackles the problem of uncertainty propagation in two-stage\nBayesian models, with a focus on spatial applications. A two-stage modeling\nframework has the advantage of being more computationally efficient than a\nfully Bayesian approach when the first-stage model is already complex in\nitself, and avoids the potential problem of unwanted feedback effects. Two ways\nof doing two-stage modeling are the crude plug-in method and the posterior\nsampling method. The former ignores the uncertainty in the first-stage model,\nwhile the latter can be computationally expensive. This paper validates the two\naforementioned approaches and proposes a new approach to do uncertainty\npropagation, which we call the $\\mathbf{Q}$ uncertainty method, implemented\nusing the Integrated Nested Laplace Approximation (INLA). We validate the\ndifferent approaches using the simulation-based calibration method, which tests\nthe self-consistency property of Bayesian models. Results show that the crude\nplug-in method underestimates the true posterior uncertainty in the\nsecond-stage model parameters, while the resampling approach and the proposed\nmethod are correct. We illustrate the approaches in a real life data\napplication which aims to link relative humidity and Dengue cases in the\nPhilippines for August 2018.",
        "Measurement of surface forces, including cohesive forces and contact forces,\nis critical for understanding and controlling interactions at interfaces to\noptimize the interfacial performance of applications. The objective of this\npaper is to introduce a general in-situ method that enables the measurement of\n3D micron-scale displacements and corresponding force distribution at\ninterfaces in dry or wet environment. Stereo digital image correlation was used\nto measure the 3D-displacement of a soft and deformable substrate. The\nefficiency and accuracy of the technique were evaluated by applying compression\nto the substrate using a steel ball, with the measured 3D displacements\naligning closely with finite element analysis simulations. To further assess\nthe method's applicability, the wet adhesion between mussel plaques and\nsubstrate was tested under aqueous conditions. The interfacial displacements\nand forces at different stages during the test were measured. The application\nof the technique can be extended for varied circumstances regarding force range\nand substrate materials based on Winkler Spring model.",
        "We study the prospects of detecting dark matter coupled to the spin of the\nelectron, such that it may scatter and excite magnons - collective excitations\nof electronic spins. We show that materials exhibiting long-range magnetic\norder where the spins are coupled only along a plane may act as directional\ndark matter detectors. These quasi-2D materials possess anisotropic dispersion\nrelations and structure functions which induce a sidereal modulation in the\nexcitation rate. We calculate the expected signal rate for some candidate\n(anti)ferromagnets, demonstrating a possible route to the direct detection of\nspin-dependent dark matter in the keV to MeV mass range.",
        "This paper develops a new family of locally recoverable codes for distributed\nstorage systems, Sequential Locally Recoverable Codes (SLRCs) constructed to\nhandle multiple erasures in a sequential recovery approach. We propose a new\nconnection between parallel and sequential recovery, which leads to a general\nconstruction of q-ary linear codes with information $(r, t_i,\n\\delta)$-sequential-locality where each of the $i$-th information symbols is\ncontained in $t_i$ punctured subcodes with length $(r+\\delta-1)$ and minimum\ndistance $\\delta$. We prove that such codes are $(r, t)_q$-SLRC ($t \\geq \\delta\nt_i+1$), which implies that they permit sequential recovery for up to $t$\nerasures each one by $r$ other code symbols.",
        "We show a certain existence of a lifting of modules under the\nself-$\\mathrm{Ext}^2$-vanishing condition over the \"derived quotient\" by using\nthe notion of higher algebra. This refines a work of Auslander--Ding--Solberg's\nsolution of the Auslander--Reiten conjecture for complete interesctions.\nTogether with Auslander's zero-divisor theorem, we show that the existence of\nsuch $\\mathrm{Ext}$-vanishing module over derived quotients is equivalent to\nbeing local complete intersections.",
        "This paper addresses the asymptotic development of order 2 by the $\\Gamma$\n-convergence of the Cahn-Hilliard functional with Dirichlet boundary\nconditions. The Dirichlet data are assumed to be well separated from one of the\ntwo wells. In the case where there are no interfaces, it is shown that there is\na transition layer near the boundary of the domain.",
        "Quantum well of AlGaAs\/GaAs is very important to study transport properties\nof electrons due to its wider application in electronic devices. Hence, the\ndouble well of AlGaAs\/GaAs with triple barrier is taken to study transmission\nprobability. Transmission probability is found to decrease with the increase in\nthe height and width of the barrier. Transmission probability with energy of\nelectron shows two peaks while taking all three barrier of the same height.\nWhereas a single and higher value of peak is found when the height of the\ncentral barrier is slightly reduced.",
        "We propose a quantum algorithm for the linear advection-diffusion equation\n(ADE) Lattice-Boltzmann method (LBM) that leverages dynamic circuits. Dynamic\nquantum circuits allow for an optimized collision-operator quantum algorithm,\nintroducing partial measurements as an integral step. Efficient adaptation of\nthe quantum circuit during execution based on digital information obtained\nthrough mid-circuit measurements is achieved. The proposed new collision\nalgorithm is implemented as a fully unitary operator, which facilitates the\ncomputation of multiple time steps without state reinitialization. Unlike\nprevious quantum collision operators that rely on linear combinations of\nunitaries, the proposed algorithm does not exhibit a probabilistic failure\nrate. Moreover, additional qubits no longer depend on the chosen velocity set,\nwhich reduces both qubit overhead and circuit complexity. Validation of the\nquantum collision algorithm is performed by comparing results with digital LBM\nin one and two dimensions, demonstrating excellent agreement. Performance\nanalysis for multiple time steps highlights advantages compared to previous\nmethods. As an additional variant, a hybrid quantum-digital approach is\nproposed, which reduces the number of mid-circuit measurements, therefore\nimproving the efficiency of the quantum collision algorithm.",
        "The global shipping network, which moves over 80% of the world's goods, is\nnot only a vital backbone of the global economy but also one of the most\npolluting industries. Studying how this network operates is crucial for\nimproving its efficiency and sustainability. While the transport of solid goods\nlike packaged products and raw materials has been extensively researched, far\nless is known about the competitive trade of crude oil and petroleum, despite\nthese commodities accounting for nearly 30% of the market. Using 4 years of\nhigh-resolution data on oil tanker movements, we employ sequential motif mining\nand dynamic mode decomposition to uncover global spatio-temporal patterns in\nthe movement of individual ships. Across all ship classes, we demonstrate that\nmaximizing the proportion of time ships spend carrying cargo -- a metric of\nefficiency -- is achieved through strategic diversification of routes and the\neffective use of intra-regional ports for trips without cargo. Moreover, we\nuncover a globally stable travel structure in the fleet, with pronounced\nseasonal variations linked to annual and semi-annual regional climate patterns\nand economic cycles. Our findings highlight the importance of integrating\nhigh-resolution data with innovative analysis methods not only to improve our\nunderstanding of the underlying dynamics of shipping patterns, but to design\nand evaluate strategies aimed at reducing their environmental impact.",
        "Quantum entanglement, in the form of spin squeezing, is known to improve the\nsensitivity of atomic instruments to static or slowly-varying quantities.\nSensing transient events presents a distinct challenge, requires different\nanalysis methods, and has not been shown to benefit from entanglement in\npractically-important scenarios such as spin-precession magnetometry (SPM).\nHere we adapt estimation control techniques introduced in\narXiv:2403.14764(2024) to the experimental setting of SPM and analogous\ntechniques. We demonstrate that real-time tracking of fluctuating fields\nbenefits from measurement-induced spin-squeezing and that quantum limits\ndictated by decoherence are within reach of today's experiments. We illustrate\nthis quantum advantage by single-shot tracking, within the coherence time of a\nspin-precession magnetometer, of a magnetocardiography signal overlain with\nbroadband noise.",
        "Recent advancements in generalized symmetries have drawn significant\nattention to gapped phases of matter exhibiting novel symmetries, such as\nnoninvertible symmetries. By leveraging the duality transformations, the\nclassification and construction of gapped phases with noninvertible symmetry\ncan be mapped to those involving conventional group symmetries. We demonstrate\nthis approach by classifying symmetry-protected topological phases with a broad\nclass of noninvertible symmetries in arbitrary spacetime dimensions. Our\nresults reveal new classifications that extend beyond those based on group\nsymmetries. Additionally, we construct lattice models in $(1+1)d$ and $(2+1)d$\nthat realize these new phases and explore their anomalous interfaces.",
        "The impact of communication on decision-making systems has been extensively\nstudied under the assumption of dedicated communication channels. We instead\nconsider communicating through actions, where the message is embedded into the\nactions of an agent which interacts with the environment in a Markov decision\nprocess (MDP) framework. We conceptualize the MDP environment as a finite-state\nchannel (FSC), where the actions of the agent serve as the channel input, while\nthe states of the MDP observed by another agent (i.e., receiver) serve as the\nchannel output. Here, we treat the environment as a communication channel over\nwhich the agent communicates through its actions, while at the same time,\ntrying to maximize its reward. We first characterize the optimal information\ntheoretic trade-off between the average reward and the rate of reliable\ncommunication in the infinite-horizon regime. Then, we propose a novel\nframework to design a joint control\/coding policy, termed \\textit{Act2Comm},\nwhich seamlessly embeds messages into actions. From a communication\nperspective, \\textit{Act2Comm} functions as a learning-based channel coding\nscheme for non-differentiable FSCs under input-output constraints. From a\ncontrol standpoint, \\textit{Act2Comm} learns an MDP policy that incorporates\ncommunication capabilities, though at the cost of some control performance.\nOverall, \\textit{Act2Comm} effectively balances the dual objectives of control\nand communication in this environment. Experimental results validate\n\\textit{Act2Comm}'s capability to enable reliable communication while\nmaintaining a certain level of control performance.",
        "The evolution of the role of lattice vibrations in the formation of the\npseudogap state in strongly correlated electron systems has been investigated\nconcerning changes in the electron-phonon coupling parameters and the\nconcentration of doped charge carriers. We apply the polaronic version of the\ngeneralized tight-binding method to analyze the band structure of a realistic\nmultiband two-dimensional model that incorporates the electron-lattice\ncontributions of both Holstein and Peierls types. It has been demonstrated that\nthe emergence of polaronic effects begins with the modulation of spectral\nfunction intensity. However, within a specific region of the phase diagram, a\nsignificant transformation of the electron band structure and pseudogap state\noccurs. It results from coherent polaron excitations that create a partially\nflat band near the Fermi level. This process leads to a change in the topology\nof the Fermi surface and the emergence of corresponding features in the density\nof states.",
        "We study the semiclassical Bochner-Schr\\\"odinger operator\n$H_{p}=\\frac{1}{p^2}\\Delta^{L^p\\otimes E}+V$ on tensor powers $L^p$ of a\nHermitian line bundle $L$ twisted by a Hermitian vector bundle $E$ on a\nRiemannian manifold of bounded geometry. For any function $\\varphi\\in\nC^\\infty_c(\\mathbb R)$, we consider the bounded linear operator $\\varphi(H_p)$\nin $L^2(X,L^p\\otimes E)$ defined by the spectral theorem. We prove that its\nsmooth Schwartz kernel on the diagonal admits a complete asymptotic expansion\nin powers of $p^{-1}$ in the semiclassical limit $p\\to \\infty$. In particular,\nwhen the manifold is compact, we get a complete asymptotic expansion for the\ntrace of $\\varphi(H_p)$.",
        "Moir\\'e engineering offers new pathways for manipulating emergent states in\ntwisted layered materials and lattice-mismatched heterostructures. With the key\nrole of the geometry of the underlying lattice in mind, here we introduce the\nwatermill lattice, a two-dimensional structure with low-energy states\ncharacterized by massless pseudospin-3\/2 fermions with high winding numbers.\nIts twisted bilayer is shown to exhibit magic angles, where four isolated flat\nbands emerge around the Fermi level, featuring elevated Wilson-loop windings\nand enhanced quantum geometric effects, such as an increase in the ratio of the\nBerezinskii-Kosterlitz-Thouless (BKT) transition temperature to the mean-field\ncritical temperature under a weak Bardeen-Cooper-Schrieffer (BCS) pairing. We\ndiscuss how the watermill lattice could be realized in the MXene and group-IV\nmaterials. Our study highlights the potential of exploiting lattice geometry in\nmoir\\'e engineering to uncover novel quantum phenomena and tailor emergent\nelectronic properties in materials.",
        "The first direct detection of neutrinos at the LHC not only marks the\nbeginning of a novel collider neutrino program at CERN but also motivates\nconsidering additional neutrino detectors to fully exploit the associated\nphysics potential. We investigate the feasibility and physics potential of\nneutrino experiments located at the surface-level. A topographic desk study was\nperformed to identify all points at which the LHC's neutrino beams exit the\nearth. The closest location lies about 9 km east of the CMS interaction point,\nat the bottom of Lake Geneva. Several detectors to be placed at this location\nare considered, including a water Cherenkov detector and an emulsion detector.\nThe detector concepts are introduced, and projections for their contribution to\nthe LHC forward neutrino program and searches for dark sector particles are\npresented. However, the dilution of the neutrino flux over distance reduces the\nneutrino yield significantly, limiting the physics potential of surface-level\ndetectors compared to ones closer to the interaction point, including the\nproposed FPF.",
        "In this paper, we investigate critical quasilinear elliptic partial\ndifferential equations on a complete Riemannian manifold with nonnegative Ricci\ncurvature. By exploiting a new and sharp nonlinear Kato inequality and\nestablishing some Cheng-Yau type gradient estimates for positive solutions, we\nclassify positive solutions to the critical $p$-Laplace equation and show\nrigidity concerning the ambient manifold. Our results extend and improve some\nprevious conclusions in the literature. Similar results are obtained for\nsolutions to the quasilinear Liouville equation involving the $n$-Laplace\noperator, where $n$ corresponds to the dimension of the ambient manifold.",
        "Neural network-based decoding methods have shown promise in enhancing error\ncorrection performance, but traditional approaches struggle with the challenges\nposed by punctured codes. In particular, these methods fail to address the\ncomplexities of variable code rates and the need for protocol compatibility.\nThis paper presents a unified Long Short-Term Memory (LSTM)-based decoding\narchitecture specifically designed to overcome these challenges. The proposed\nmethod unifies punctured convolutional and Turbo codes. A puncture embedding\nmechanism integrates puncturing patterns directly into the network, enabling\nseamless adaptation to varying code rates, while balanced bit error rate\ntraining ensures robustness across different code lengths, rates, and channels,\nmaintaining protocol flexibility. Extensive simulations in Additive White\nGaussian Noise and Rayleigh fading channels demonstrate that the proposed\napproach outperforms conventional decoding techniques, providing significant\nimprovements in decoding accuracy and robustness. These results underscore the\npotential of LSTM-based decoding as a promising solution for next-generation\nartificial intelligence powered communication systems.",
        "High-speed trains are one of the most relevant scenarios for the\nfifth-generation (5G) mobile communications and the \"smart rail mobility\"\nvision, where a high-data-rate wireless connectivity with up to several GHz\nbandwidths will be required. This is a strong motivation for the exploration of\nmillimeter wave (mmWave) band. In this article, we identify the main challenges\nand make progress towards realistic 5G mmWave channel models for railway use\ncases. In order to cope with the challenge of including the railway features in\nthe channel models, we define reference scenarios to help the parameterization\nof channel models for railway use at mmWave band. Simulations and the\nsubsequent measurements used to validate the model reflect the detailed\ninfluence of railway objects and the accuracy of the simulations. Finally, we\npoint out the future directions towards the full version of the smart rail\nmobility which will be powered by terahertz (THz) communications.",
        "Tailoring material properties often requires understanding the solidification\nprocess. Herein, we introduce the geometric descriptor Soliquidy, which\nnumerically captures the Euclidean transport cost between the translationally\ndisordered versus ordered states of a materials. As a testbed, we apply\nSoliquidy to the classification of glass-forming metal alloys. By extending and\ncombining an experimental library of metallic thin-films (glass\/no-glass) with\nthe aflow.org computational database (geometrical and energetic information of\nmixtures) we found that the combination of Soliquity and formation enthalpies\ngenerates an effective classifier for glass formation. Such classifier is then\nused to tackle a public dataset of metallic glasses showing that the\nglass-agnostic assumptions of Soliquity can be useful for understanding\nkinetically-controlled phase transitions.",
        "Venus' atmosphere -- specifically its clouds buoyed up 40 to 60 km above the\nsurface -- has long been suspected to encompass a biosphere where Earth-like\nliving organisms could grow and flourish. This idea has been recently rekindled\nby the observation (signal-to-noise ratio of about 15$\\sigma$) of a phosphine\n(PH$_3$) absorption-line profile against the thermal background from deeper,\nhotter layers of the atmosphere. There is a chance that this observation could\nbe a sign of life, because the PH$_3$ gas observed on Earth originates mostly\nin decaying organic material. Furthermore, it has been shown that there is no\nother natural process on Venus that could otherwise produce the observed PH$_3$\nabsorption line. On the other hand, cosmic rays and the particle cascades they\nproduce in the Earth's atmosphere are hazardous to living organisms, because\nthe ionizing radiation produced in air showers can blow apart chemical\nmolecules and disrupt biochemical processes, causing cells to die or undergo\ndangerous mutations. Compared to Earth, the hypothesized biosphere of Venus\ncould be exposed to substantially more ionizing radiation. This is because\nVenus has no protective intrinsic magnetic field, orbits closer to the Sun, and\nthe entire eventual habitable region lies in the clouds high in the atmosphere.\nThereby, if the clouds were sterilized there would be no reservoir of deeper\nlife that can recolonize afterwards. In this communication we study the effects\nof particle cascades in the venusian atmosphere using the AIRES simulation\npackage properly configured. We show that the effects of cosmic radiation in\nthe habitable zone would be comparable to those on the Earth's surface and so\nwould not have any hazardous effect on possible venusian microorganisms.",
        "The quantum simulation of classical fluids often involves the use of\nprobabilistic algorithms that encode the result of the dynamics in the form of\nthe amplitude of the selected quantum state. In most cases, however, the\namplitude probability is too low to allow an efficient use of these algorithms,\nthereby hindering the practical viability of the quantum simulation. The\noblivious amplitude amplification algorithm is often presented as a solution to\nthis problem, but to no avail for most classical problems, since its\napplicability is limited to unitary dynamics. In this paper, we show\nanalytically that oblivious amplitude amplification when applied to non-unitary\ndynamics leads to a distortion of the quantum state and to an accompanying\nerror in the quantum update. We provide an analytical upper bound of such error\nas a function of the degree of non-unitarity of the dynamics and we test it\nagainst a quantum simulation of an advection-diffusion-reaction equation, a\ntransport problem of major relevance in science and engineering. Finally, we\nalso propose an amplification strategy that helps mitigate the distortion\nerror, while still securing an enhanced success probability.",
        "We perform a detailed study of a simple mathematical model addressing the\nproblem of optimally regulating a process subject to periodic external forcing,\nwhich is interesting both in view of its direct applications and as a prototype\nfor more general problems. In this model one must determine an optimal\ntime-periodic `effort' profile, and the natural setting for the problem is in a\nspace of periodic non-negative measures. We prove that there exists a unique\nsolution for the problem in the space of measures, and then turn to\ncharacterizing this solution. Under some regularity conditions on the problem's\ndata, we prove that its solution is an absolutely continuous measure, and\nprovide an explicit formula for the measure's density. On the other hand, when\nthe problem's data is discontinuous, the solution measure can also include\natomic components. Complementing our analytical results, we carry out numerical\ncomputations to obtain solutions of the problem in various instances, which\nenable us to examine the interesting ways in which the solution's structure\nvaries as the problem's data is varied.",
        "Quantum state exclusion is the task of determining which states from a given\nset a system was not prepared in. We provide a complete solution to optimal\nquantum state exclusion for arbitrary sets of pure states generated by finite\ngroups, establishing necessary and sufficient conditions for perfect\n(zero-error conclusive) exclusion. When perfect exclusion is impossible, we\nintroduce two natural extensions: minimum-error and unambiguous exclusion. For\nboth, we derive the optimal protocols and present analytical expressions for\nthe corresponding failure probabilities and measurements, offering a new\nperspective on how quantum states encode information.",
        "Spot-size converters are key for efficient coupling of light between\nwaveguides of different sizes. While adiabatic tapers are well suited for small\nsize differences, they become impractically long for expansion factors around\nx100 which are often required when coupling integrated waveguides and\nfree-space beams. Evanescent couplers and bragg deflectors can be used in this\nscenario, but their operation is inherently limited in bandwidth. Here we\npropose a solution based on a parabolic dielectric interface that couples light\nfrom a 0.5 um-wide waveguide to a 285 um-wide waveguide, i.e. an expansion\nfactor of x570. We experimentally demonstrate an unprecedented bandwidth of\nmore than 380 nm with insertion losses below 0.35 dB. We furthermore provide\nanalytical expressions for the design of such parabolic spot-size-converters\nfor arbitrary expansion factors.",
        "Utilizing 4.5 $fb^-$ of $e^+e^-$ annihilation data collected at\ncenter-of-mass energies ranging from 4599.53 MeV to 4698.82 MeV by the BESIII\ndetector at the BEPCII collider, we search for the singly Cabibbo-suppressed\nhadronic decays $\\Lambda_{c}^{+}\\to\\Sigma^{0} K^{+}\\pi^{0}$ and\n$\\Lambda_{c}^{+}\\to\\Sigma^{0}K^{+}\\pi^+\\pi^-$ with a single-tag method. No\nsignificant signals are observed for both decays. The upper limits on the\nbranching fractions at the $90\\%$ confidence level are determined to be\n$5.0\\times 10^{-4}$ for $\\Lambda_{c}^{+}\\to\\Sigma^{0} K^{+}\\pi^{0}$ and\n$6.5\\times 10^{-4}$ for $\\Lambda_c^{+}\\to\\Sigma^0K^{+}\\pi^{+}\\pi^{-}$."
      ]
    }
  },
  {
    "id":2411.18902,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"Biomechanics and motor control of human movement",
    "start_abstract":"Preface to the Fourth Edition. 1 Biomechanics as an Interdiscipline. 1.0 Introduction. 1.1 Measurement, Description, Analysis, and Assessment. 1.2 its Relationship with Physiology Anatomy. 1.3 Scope of Textbook. 1.4 References. 2 Signal Processing. 2.0 2.1 Auto- Cross-Correlation Analyses. 2.2 Frequency Analysis. 2.3 Ensemble Averaging Repetitive Waveforms. 2.4 3 Kinematics. 3.0 Historical Development Complexity Problem. 3.1 Kinematic Conventions. 3.2 Direct Measurement Techniques. 3.3 Imaging 3.4 Processing Raw Data. 3.5 Calculation Other Variables. 3.6 Problems Based on 3.7 4 Anthropometry. 4.0 Anthropometry in Movement Biomechanics. 4.1 Density, Mass, Inertial Properties. 4.2 Experimental Measures. 4.3 Muscle 4.4 Anthropometric 4.5 5 Kinetics: Forces Moments Force. 5.0 Biomechanical Models. 5.1 Basic Link-Segment Equations-the Free-Body Diagram. 5.2 Force Transducers Plates. 5.3 Bone-on-Bone During Dynamic Conditions. 5.4 Kinetic 5.5 6 Mechanical Work, Energy, Power. 6.0 6.1 Efficiency. 6.2 Forms Energy Storage. 6.3 Internal External Work. 6.4 Power Balances at Joints Within Segments. 6.5 6.6 7 Three-Dimensional Kinematics Kinetics. 7.0 7.1 Axes Systems. 7.2 Marker Anatomical 7.3 Determination Segment Angular Velocities Accelerations. 7.4 Analysis Reaction Moments. 7.5 Suggested Further Reading. 7.6 8 Synthesis Human Movement-Forward Solutions. 8.0 8.1 Review Forward Solution 8.2 Mathematical Formulation. 8.3 System Energy. 8.4 Torques. 8.5 Designation Joints. 8.6 Illustrative Example. 8.7 Conclusions. 8.8 9 Mechanics. 9.0 9.1 Force-Length Characteristics Muscles. 9.2 Force-Velocity Characteristics. 9.3 Modeling. 9.4 10 Kinesiological Electromyography. 10.0 10.1 Electrophysiology Contraction. 10.2 Recording Electromyogram. 10.3 Electromyogram,. 10.4 between Electromyogram 10.5 11 Synergies. 11.0 11.1 The Support Moment Synergy. 11.2 Medial\/Lateral Anterior\/Posterior Balance Standing. 11.3 during Walking. 11.4 APPENDICES. A. Kinematic, Kinetic, Figure A.1 Walking Trial-Marker Locations Mass Frame Rate Information. Table Coordinate Data (cm). A.2( a ) Filtered Kinematics-Rib Cage Greater Trochanter (Hip). b Kinematics-Femoral Lateral Epicondyle (Knee) Head Fibula. c Kinematics-Lateral Malleolus (Ankle) Heel. d Kinematics-Fifth Metatarsal Toe. A.3( Linear Kinematics-Foot. Kinematics-Leg. Kinematics-Thigh. Kinematics-1\/2 HAT. A.4 Relative Joint Kinematics-Ankle, Knee, Hip. A.5( Force-Ankle Knee. Force-Hip. A.6 Potential, Total Energies-Foot, Leg, Thigh, and1\/2 A.7 Generation\/Absorption Transfer-Ankle, B. Units Definitions Related Electromyographical Measurements. B.1 Base SI Units. B.2 Derived Index.",
    "start_categories":[
      "Biomechanics"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
      ],
      "abstract":[
        "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution recurrent structured state space models (SSMs) have been developed to address Transformers' computational inefficiency long sequences, but they not performed well important modalities language. We identify that a key weakness is their inability perform content-based reasoning, make several improvements. First, simply letting SSM parameters be functions input addresses with discrete modalities, allowing model selectively propagate or forget information along sequence length dimension depending current token. Second, even though this change prevents use efficient convolutions, we design hardware-aware parallel algorithm mode. integrate these selective SSMs into simplified end-to-end neural network without MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) scaling length, performance improves real data up million-length sequences. As general backbone, achieves state-of-the-art across language, audio, genomics. On language modeling, our Mamba-3B outperforms Transformers same size matches twice size, both pretraining downstream evaluation."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "An Unsupervised C-Uniform Trajectory Sampler with Applications to Model\n  Predictive Path Integral Control",
        "Integrated Multiphysics Modeling of a Piezoelectric Micropump",
        "The Cosmic Microwave Background -- Secondary Anisotropies",
        "A neural network approach for line detection in complex atomic emission\n  spectra measured by high-resolution Fourier transform spectroscopy",
        "CLoCKDistill: Consistent Location-and-Context-aware Knowledge\n  Distillation for DETRs",
        "INDIGO+: A Unified INN-Guided Probabilistic Diffusion Algorithm for\n  Blind and Non-Blind Image Restoration",
        "SeqSee: A schema-based approach to spectral sequence visualization",
        "The Halo Occupation Distribution Modeling of the X-ray-selected AGNs at\n  0.6 < z < 2.6 in the COSMOS field",
        "SeWA: Selective Weight Average via Probabilistic Masking",
        "On the extremal eigenvalues of Jacobi ensembles at zero temperature",
        "Adaptive Negative Damping Control for User-Dependent Multi-Terrain\n  Walking Assistance with a Hip Exoskeleton",
        "AI Drawing Partner: Co-Creative Drawing Agent and Research Platform to\n  Model Co-Creation",
        "Web Execution Bundles: Reproducible, Accurate, and Archivable Web\n  Measurements",
        "4bit-Quantization in Vector-Embedding for RAG",
        "Frame-dependent Random Utility",
        "Complexity of Jelly-No and Hanano games with various constraints",
        "StarCast: A Secure and Spectrum-Efficient Group Communication Scheme for\n  LEO Satellite Networks",
        "Bifurcation of global energy minimizers for a diffusion-aggregation\n  model on sphere",
        "Isolating Unisolated Upsilons with Anomaly Detection in CMS Open Data",
        "Learning the Integral Quadratic Constraints on Plant-Model Mismatch",
        "A Laplace duality for integration",
        "Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient\n  Pruning",
        "QUOKKA-based understanding of outflows (QED) -- III. Outflow loading and\n  phase structure as a function of galactic environment",
        "Blockchain-Enabled Management Framework for Federated Coalition Networks",
        "TacticExpert: Spatial-Temporal Graph Language Model for Basketball\n  Tactics",
        "Automatic Linear Resource Bound Analysis for Rust via Prophecy\n  Potentials",
        "PythonPal: Enhancing Online Programming Education through Chatbot-Driven\n  Personalized Feedback",
        "Power-Efficient Deceptive Wireless Beamforming Against Eavesdroppers",
        "Compression in 3D Gaussian Splatting: A Survey of Methods, Trends, and\n  Future Directions"
      ],
      "abstract":[
        "Sampling-based model predictive controllers generate trajectories by sampling\ncontrol inputs from a fixed, simple distribution such as the normal or uniform\ndistributions. This sampling method yields trajectory samples that are tightly\nclustered around a mean trajectory. This clustering behavior in turn, limits\nthe exploration capability of the controller and reduces the likelihood of\nfinding feasible solutions in complex environments. Recent work has attempted\nto address this problem by either reshaping the resulting trajectory\ndistribution or increasing the sample entropy to enhance diversity and promote\nexploration. In our recent work, we introduced the concept of C-Uniform\ntrajectory generation [1] which allows the computation of control input\nprobabilities to generate trajectories that sample the configuration space\nuniformly. In this work, we first address the main limitation of this method:\nlack of scalability due to computational complexity. We introduce Neural\nC-Uniform, an unsupervised C-Uniform trajectory sampler that mitigates\nscalability issues by computing control input probabilities without relying on\na discretized configuration space. Experiments show that Neural C-Uniform\nachieves a similar uniformity ratio to the original C-Uniform approach and\ngenerates trajectories over a longer time horizon while preserving uniformity.\nNext, we present CU-MPPI, which integrates Neural C-Uniform sampling into\nexisting MPPI variants. We analyze the performance of CU-MPPI in simulation and\nreal-world experiments. Our results indicate that in settings where the optimal\nsolution has high curvature, CU-MPPI leads to drastic improvements in\nperformance.",
        "This paper presents an integrated multiphysics simulation approach of\npiezoelectric micropumps. Micropumps and micro blowers are essential devices in\nvarious cutting-edge industries like laboratory equipment, medical devices, and\nfuel cells. A piezoelectric micropump involves complex physics including\nmicrofluidics, flow-structure interaction, electricity, and piezoelectric\nmaterial. Hence, a comprehensive analysis of the interactions between different\nphysical phenomena, would be essential for the effective design and\noptimization of these micropumps. Prior studies on piezoelectric micropump were\nmainly focused on isolated physical aspects of these pumps, such as\npiezoelectric mechanics, fluid dynamics, electrical properties, and also\nfluid-structure Interactions. The present paper fills this gap by integrating\nthese aspects into a holistic simulation and design approach, introducing a new\nmethodology for micropump analysis. Advanced simulation and design tools like\nCOMSOL and SolidWorks were employed in accordance. A brief review of\npiezoelectric materials, and an exploration of different types of micropumps\nand their operating principles is discussed. Also, a comparison of various\npiezoelectric materials, including their properties and applications is\ninvestigated. Further, the paper discusses the simulation process of the\nmicropumps, using COMSOL software, and presents an in-depth analysis of the\nsimulation results. This structured approach provides a comprehensive\nunderstanding of piezoelectric micropumps, from theoretical underpinnings to\npractical design considerations. ..",
        "The cosmic microwave background (CMB), the relic radiation from the early\nUniverse, offers a unique window into both primordial conditions and the\nintervening large-scale structure (LSS) it traverses. Interactions between CMB\nphotons and the evolving Universe imprint secondary anisotropies --\nmodifications to the CMB's intensity and polarization caused by gravitational\neffects and scattering processes. These anisotropies serve as a powerful probe\nof fundamental physics while also revealing astrophysical processes governing\nthe thermodynamics and distribution of baryonic matter. In this chapter, we\nprovide a comprehensive review of the physical mechanisms underlying secondary\nanisotropies, their observational status, and their potential to advance\nprecision cosmology. With the increasing sensitivity of CMB experiments and\nsynergy with LSS surveys, secondary anisotropies are poised to unveil\nunprecedented insights into the Universe's composition, evolution, and\ndynamics.",
        "The atomic spectra and structure of the open d- and f-shell elements are\nextremely complex, where tens of thousands of transitions between fine\nstructure energy levels can be observed as spectral lines across the infrared\nand UV per species. Energy level quantum properties and transition wavenumbers\nof these elements underpins almost all spectroscopic plasma diagnostic\ninvestigations, with prominent demands from astronomy and fusion research.\nDespite their importance, these fundamental data are incomplete for many\nspecies. A major limitation for the analyses of emission spectra of the open d-\nand f-shell elements is the amount of time and human resource required to\nextract transition wavenumbers and intensities from the spectra. Here, the\nspectral line detection problem is approached by encoding the spectrum\npoint-wise using bidirectional Long Short-Term Memory networks, where\ntransition wavenumber positions are decoded by a fully connected neural\nnetwork. The model was trained using simulated atomic spectra and evaluated\nagainst experimental Fourier transform spectra of Ni ($Z=28$) covering\n1800-70,000 cm$^{-1}$ (5555-143 nm) and Nd ($Z=60$) covering 25,369-32,485\ncm$^{-1}$ (394-308 nm), measured under a variety of experimental set-ups.\nImprovements over conventional methods in line detection were evident,\nparticularly for spectral lines that are noisy, blended, and\/or distorted by\ninstrumental spectral resolution-limited ringing. In evaluating model\nperformance, a brief energy level analysis of Ni II using lines newly detected\nby the neural networks has led to the confident identification of two Ni II\nlevels, $3\\text{d}^8$$(^3\\text{F}_4)6\\text{f} [2]_{3\/2}$ at 134,261.8946 $\\pm$\n0.0081 cm$^{-1}$ and $3\\text{d}^8$$(^3\\text{F}_4)6\\text{f} [1]_{3\/2}$ at\n134,249.5264 $\\pm$ 0.0054 cm$^{-1}$, previously concluded to be unidentifiable\nusing previously analysed Ni spectra.",
        "Object detection has advanced significantly with Detection Transformers\n(DETRs). However, these models are computationally demanding, posing challenges\nfor deployment in resource-constrained environments (e.g., self-driving cars).\nKnowledge distillation (KD) is an effective compression method widely applied\nto CNN detectors, but its application to DETR models has been limited. Most KD\nmethods for DETRs fail to distill transformer-specific global context. Also,\nthey blindly believe in the teacher model, which can sometimes be misleading.\nTo bridge the gaps, this paper proposes Consistent Location-and-Context-aware\nKnowledge Distillation (CLoCKDistill) for DETR detectors, which includes both\nfeature distillation and logit distillation components. For feature\ndistillation, instead of distilling backbone features like existing KD methods,\nwe distill the transformer encoder output (i.e., memory) that contains valuable\nglobal context and long-range dependencies. Also, we enrich this memory with\nobject location details during feature distillation so that the student model\ncan prioritize relevant regions while effectively capturing the global context.\nTo facilitate logit distillation, we create target-aware queries based on the\nground truth, allowing both the student and teacher decoders to attend to\nconsistent and accurate parts of encoder memory. Experiments on the KITTI and\nCOCO datasets show our CLoCKDistill method's efficacy across various DETRs,\ne.g., single-scale DAB-DETR, multi-scale deformable DETR, and denoising-based\nDINO. Our method boosts student detector performance by 2.2% to 6.4%.",
        "Generative diffusion models are becoming one of the most popular prior in\nimage restoration (IR) tasks due to their remarkable ability to generate\nrealistic natural images. Despite achieving satisfactory results, IR methods\nbased on diffusion models present several limitations. First of all, most\nnon-blind approaches require an analytical expression of the degradation model\nto guide the sampling process. Secondly, most existing blind approaches rely on\nfamilies of pre-defined degradation models for training their deep networks.\nThe above issues limit the flexibility of these approaches and so their ability\nto handle real-world degradation tasks. In this paper, we propose a novel\nINN-guided probabilistic diffusion algorithm for non-blind and blind image\nrestoration, namely INDIGO and BlindINDIGO, which combines the merits of the\nperfect reconstruction property of invertible neural networks (INN) with the\nstrong generative capabilities of pre-trained diffusion models. Specifically,\nwe train the forward process of the INN to simulate an arbitrary degradation\nprocess and use the inverse to obtain an intermediate image that we use to\nguide the reverse diffusion sampling process through a gradient step. We also\nintroduce an initialization strategy, to further improve the performance and\ninference speed of our algorithm. Experiments demonstrate that our algorithm\nobtains competitive results compared with recently leading methods both\nquantitatively and visually on synthetic and real-world low-quality images.",
        "We present SeqSee, a software system that addresses spectral sequence\nvisualization through a schema-based approach. By introducing a standardized\nJSON schema as an intermediate representation, SeqSee decouples the\nmathematical computations of spectral sequences from their visualizations. We\ndemonstrate the system through a case study of the classical and C-motivic\nAdams spectral sequences.",
        "We conducted precise measurements of Active Galactic Nuclei (AGNs) clustering\nat $z\\sim1$ and $z\\sim2$ by measuring the two-point cross-correlation function\n(CCF) between galaxies and X-ray-selected AGNs, and the two-point\nauto-correlation function (ACF) of galaxies in the COSMOS field to interpret\nthe CCF results. The galaxy sample was selected from the COSMOS2015 catalog,\nwhile the AGN sample was chosen from the {\\sl Chandra} COSMOS-Legacy survey\ncatalog. For the AGN samples at $z\\sim1$ and $z\\sim2$, we calculated AGN bias\nvalues of $b=1.16\\ (1.16;1.31)$ and $b=2.95\\ (2.30;3.55)$, respectively. These\nvalues correspond to typical host dark matter halo (DMH) masses of log$(M_{\\rm\ntyp}\/M_{\\odot})=11.82\\ (11.82;12.12)$ and log$(M_{\\rm typ}\/M_{\\odot})=12.80\\\n(12.38;13.06)$, respectively. Subsequently, we performed Halo Occupation\nDistribution (HOD) modeling of X-ray-selected AGNs using the CCF and ACF of\ngalaxies. We have found a significant satellite AGN population at $z\\sim 1$ all\nover the DMH mass ($M_{\\rm DMH}$) range occupied by AGNs. While $z\\sim 2$ AGNs\nin our sample are associated with higher mass DMHs and smaller satellite\nfractions. The HOD analysis suggests a marginal tendency of increasing\nsatellite slope with redshift, but larger samples are needed to confirm this\nwith sufficient statistical significance. We find that the best-fit values of\nsatellite slope in both redshift bins are greater than 0, suggesting tendencies\nof increasing satellite AGN number with $M_{\\rm DMH}$.",
        "Weight averaging has become a standard technique for enhancing model\nperformance. However, methods such as Stochastic Weight Averaging (SWA) and\nLatest Weight Averaging (LAWA) often require manually designed procedures to\nsample from the training trajectory, and the results depend heavily on\nhyperparameter tuning. To minimize human effort, this paper proposes a simple\nyet efficient algorithm called Selective Weight Averaging (SeWA), which\nadaptively selects checkpoints during the final stages of training for\naveraging. Based on SeWA, we show that only a few points are needed to achieve\nbetter generalization and faster convergence. Theoretically, solving the\ndiscrete subset selection problem is inherently challenging. To address this,\nwe transform it into a continuous probabilistic optimization framework and\nemploy the Gumbel-Softmax estimator to learn the non-differentiable mask for\neach checkpoint. Further, we theoretically derive the SeWA's stability-based\ngeneralization bounds, which are sharper than that of SGD under both convex and\nnon-convex assumptions. Finally, solid extended experiments in various domains,\nincluding behavior cloning, image classification, and text classification,\nfurther validate the effectiveness of our approach.",
        "For the $\\beta$-Hermite, Laguerre, and Jacobi ensembles of dimension $N$\nthere exist central limit theorems for the freezing case $\\beta\\to\\infty$ such\nthat the associated means and covariances can be expressed in terms of the\nassociated Hermite, Laguerre, and Jacobi polynomials of order $N$ respectively\nas well as via the associated dual polynomials in the sense of de Boor and\nSaff. In this paper we derive limits for $N\\to\\infty$ for the covariances of\nthe $r\\in\\mathbb N$ largest (and smallest) eigenvalues for these frozen Jacobi\nensembles in terms of Bessel functions. These results correspond to the hard\nedge analysis in the frozen Laguerre cases by Andraus and Lerner-Brecher and to\nknown results for finite $\\beta$.",
        "Hip exoskeletons are known for their versatility in assisting users across\nvaried scenarios. However, current assistive strategies often lack the\nflexibility to accommodate for individual walking patterns and adapt to diverse\nlocomotion environments. In this work, we present a novel control strategy that\nadapts the mechanical impedance of the human-exoskeleton system. We design the\nhip assistive torques as an adaptive virtual negative damping, which is able to\ninject energy into the system while allowing the users to remain in control and\ncontribute voluntarily to the movements. Experiments with five healthy subjects\ndemonstrate that our controller reduces the metabolic cost of walking compared\nto free walking (average reduction of 7.2%), and it preserves the lower-limbs\nkinematics. Additionally, our method achieves minimal power losses from the\nexoskeleton across the entire gait cycle (less than 2% negative mechanical\npower out of the total power), ensuring synchronized action with the users'\nmovements. Moreover, we use Bayesian Optimization to adapt the assistance\nstrength and allow for seamless adaptation and transitions across multi-terrain\nenvironments. Our strategy achieves efficient power transmission under all\nconditions. Our approach demonstrates an individualized, adaptable, and\nstraightforward controller for hip exoskeletons, advancing the development of\nviable, adaptive, and user-dependent control laws.",
        "This paper describes the AI Drawing Partner, which is a co-creative drawing\nagent that also serves as a research platform to model co-creation. The AI\nDrawing Partner is an early example of a quantified co-creative AI system that\nautomatically models the co-creation that happens on the system. The method the\nsystem uses to capture this data is based on a new cognitive science framework\ncalled co-creative sense-making (CCSM). The CCSM is based on the cognitive\ntheory of enaction, which describes how meaning emerges through interaction\nwith the environment and other people in that environment in a process of\nsense-making. The CCSM quantifies elements of interaction dynamics to identify\nsense-making patterns and interaction trends. This paper describes a new\ntechnique for modeling the interaction and collaboration dynamics of\nco-creative AI systems with the co-creative sense-making (CCSM) framework. A\ncase study is conducted of ten co-creative drawing sessions between a human\nuser and the co-creative agent. The analysis includes showing the artworks\nproduced, the quantified data from the AI Drawing Partner, the curves\ndescribing interaction dynamics, and a visualization of interaction trend\nsequences. The primary contribution of this paper is presenting the AI Drawing\nPartner, which is a unique co-creative AI system and research platform that\ncollaborates with the user in addition to quantifying, modeling, and\nvisualizing the co-creative process using the CCSM framework.",
        "Recently, reproducibility has become a cornerstone in the security and\nprivacy research community, including artifact evaluations and even a new\nsymposium topic. However, Web measurements lack tools that can be reused across\nmany measurement tasks without modification, while being robust to\ncircumvention, and accurate across the wide range of behaviors in the Web. As a\nresult, most measurement studies use custom tools and varied archival formats,\neach of unknown correctness and significant limitations, systematically\naffecting the research's accuracy and reproducibility.\n  To address these limitations, we present WebREC, a Web measurement tool that\nis, compared against the current state-of-the-art, accurate (i.e., correctly\nmeasures and attributes events not possible with existing tools), general\n(i.e., reusable without modification for a broad range of measurement tasks),\nand comprehensive (i.e., handling events from all relevant browser behaviors).\nWe also present .web, an archival format for the accurate and reproducible\nmeasurement of a wide range of website behaviors. We empirically evaluate\nWebREC's accuracy by replicating well-known Web measurement studies and showing\nthat WebREC's results more accurately match our baseline. We then assess if\nWebREC and .web succeed as general-purpose tools, which could be used to\naccomplish many Web measurement tasks without modification. We find that this\nis so: 70% of papers discussed in a 2024 web crawling SoK paper could be\nconducted using WebREC as is, and a larger number (48%) could be leveraged\nagainst .web archives without requiring any new crawling.",
        "Retrieval-augmented generation (RAG) is a promising technique that has shown\ngreat potential in addressing some of the limitations of large language models\n(LLMs). LLMs have two major limitations: they can contain outdated information\ndue to their training data, and they can generate factually inaccurate\nresponses, a phenomenon known as hallucinations. RAG aims to mitigate these\nissues by leveraging a database of relevant documents, which are stored as\nembedding vectors in a high-dimensional space. However, one of the challenges\nof using high-dimensional embeddings is that they require a significant amount\nof memory to store. This can be a major issue, especially when dealing with\nlarge databases of documents. To alleviate this problem, we propose the use of\n4-bit quantization to store the embedding vectors. This involves reducing the\nprecision of the vectors from 32-bit floating-point numbers to 4-bit integers,\nwhich can significantly reduce the memory requirements. Our approach has\nseveral benefits. Firstly, it significantly reduces the memory storage\nrequirements of the high-dimensional vector database, making it more feasible\nto deploy RAG systems in resource-constrained environments. Secondly, it speeds\nup the searching process, as the reduced precision of the vectors allows for\nfaster computation. Our code is available at\nhttps:\/\/github.com\/taeheej\/4bit-Quantization-in-Vector-Embedding-for-RAG",
        "We explore the influence of framing on decision-making, where some products\nare framed (e.g., displayed, recommended, endorsed, or labeled). We introduce a\nnovel choice function that captures observed variations in framed alternatives.\nBuilding on this, we conduct a comprehensive revealed preference analysis,\nemploying the concept of frame-dependent utility using both deterministic and\nprobabilistic data. We demonstrate that simple and intuitive behavioral\nprinciples characterize our frame-dependent random utility model (FRUM), which\noffers testable conditions even with limited data. Finally, we introduce a\nparametric model to increase the tractability of FRUM. We also discuss how to\nrecover the choice types in our framework.",
        "This work shows new results on the complexity of games Jelly-No and Hanano\nwith various constraints on the size of the board and number of colours. Hanano\nand Jelly-No are one-player, 2D side-view puzzle games with a dynamic board\nconsisting of coloured, movable blocks disposed on platforms. These blocks can\nbe moved by the player and are subject to gravity. Both games somehow vary in\ntheir gameplay, but the goal is always to move the coloured blocks in order to\nreach a specific configuration and make them interact with each other or with\nother elements of the game. In Jelly-No the goal is to merge all coloured\nblocks of a same colour, which also happens when they make contact. In Hanano\nthe goal is to make all the coloured blocks bloom by making contact with\nflowers of the same colour. Jelly-No was proven by Chao Yang to be NP-Complete\nunder the restriction that all movable blocks are the same colour and NP-Hard\nfor more colours. Hanano was proven by Michael C. Chavrimootoo to be\nPSPACE-Complete under the restriction that all movable blocks are the same\ncolour. However, the question whether Jelly-No for more than one colours is\nalso PSPACE-complete or if it too stays in NP was left open. In this paper, we\nsettle this question, proving that Jelly-No is PSPACE-Complete with an\nunbounded number of colours. We further show that, if we allow black jellies\n(that is, jellies that do not need to be merged), the game is PSPACE-complete\neven for one colour. We further show that one-colour Jelly-No and Hanano remain\nNP-Hard even if the width or the height of the board are small constants.",
        "Low Earth Orbit (LEO) satellite networks serve as a cornerstone\ninfrastructure for providing ubiquitous connectivity in areas where terrestrial\ninfrastructure is unavailable. With the emergence of Direct-to-Cell (DTC)\nsatellites, these networks can provide direct access to mobile phones and IoT\ndevices without relying on terrestrial base stations, leading to a surge in\nmassive connectivity demands for the serving satellite. To address this issue,\ngroup communication is an effective paradigm that enables simultaneous content\ndelivery to multiple users and thus optimizes bandwidth reuse. Although\nextensive research has been conducted to improve group communication\nperformance, securing this communication without compromising its inherent\nspectrum efficiency remains a critical challenge. To address this, we introduce\nStarCast, a secure group encryption scheme for LEO satellite networks. Our\nsolution leverages ciphertext-policy attribute-based encryption (CP-ABE) to\nimplement fine-grained access control by embedding access policies directly\nwithin the ciphertext. Unlike standard secure communication approaches that\nrequire dedicated per-user channels and significantly deplete limited satellite\nspectrum resources, StarCast maintains efficient spectrum reuse within user\ngroups while ensuring that only authorized users can access transmitted data.\nAdditionally, it significantly reduces the costly key management overhead\nassociated with conventional encryption schemes.",
        "We consider a free energy functional defined on probability densities on the\nunit sphere $\\mathbb{S}^d$, and investigate its global minimizers. The energy\nconsists of two components: an entropy and a nonlocal interaction energy, which\nfavour spreading and aggregation behaviour, respectively. We find a threshold\nvalue for the size of the attractive interactions, and establish the global\nenergy minimizers in each case. The bifurcation at this threshold value is\ninvestigated. We also generalize the results to spaces consisting of an\narbitrary number of spheres (e.g., the flat torus $\\mathbb{S}^1 \\times\n\\mathbb{S}^1$).",
        "We present the first study of anti-isolated Upsilon decays to two muons\n($\\Upsilon \\to \\mu^+ \\mu^-$) in proton-proton collisions at the Large Hadron\nCollider. Using a machine learning (ML)-based anomaly detection strategy, we\n\"rediscover\" the $\\Upsilon$ in 13 TeV CMS Open Data from 2016, despite\noverwhelming anti-isolated backgrounds. We elevate the signal significance to\n$6.4 \\sigma$ using these methods, starting from $1.6 \\sigma$ using the dimuon\nmass spectrum alone. Moreover, we demonstrate improved sensitivity from using\nan ML-based estimate of the multi-feature likelihood compared to traditional\n\"cut-and-count\" methods. Our work demonstrates that it is possible and\npractical to find real signals in experimental collider data using ML-based\nanomaly detection, and we distill a readily-accessible benchmark dataset from\nthe CMS Open Data to facilitate future anomaly detection developments.",
        "While a characterization of plant-model mismatch is necessary for robust\ncontrol, the mismatch usually can not be described accurately due to the lack\nof knowledge about the plant model or the complexity of nonlinear plants.\nHence, this paper considers this problem in a data-driven way, where the\nmismatch is captured by parametric forms of integral quadratic constraints\n(IQCs) and the parameters contained in the IQC equalities are learned from\nsampled trajectories from the plant. To this end, a one-class support vector\nmachine (OC-SVM) formulation is proposed, and its generalization performance is\nanalyzed based on the statistical learning theory. The proposed approach is\ndemonstrated by a single-input-single-output time delay mismatch and a\nnonlinear two-phase reactor with a linear nominal model, showing accurate\nrecovery of frequency-domain uncertainties.",
        "We consider the integral v(y) = Ky f (x)dx on a domain Ky = {x $\\in$ R d :\ng(x) $\\le$ y}, where g is nonnegative and Ky is compact for all y $\\in$ [0,\n+$\\infty$). Under some assumptions, we show that for every y $\\in$ (0,\n$\\infty$) there exists a distinguished scalar $\\lambda$y $\\in$ (0, +$\\infty$)\nsuch that which is the counterpart analogue for integration of Lagrangian\nduality for optimization. A crucial ingredient is the Laplace transform, the\nanalogue for integration of Legendre-Fenchel transform in optimization. In\nparticular, if both f and g are positively homogeneous then $\\lambda$y is a\nsimple explicitly rational function of y. In addition if g is quadratic form\nthen computing v(y) reduces to computing the integral of f with respect to a\nspecific Gaussian measure for which exact and approximate numerical methods\n(e.g. cubatures) are available.",
        "Large Language Models (LLMs) have demonstrated their exceptional performance\nin various complex code generation tasks. However, their broader adoption is\nlimited by significant computational demands and high resource requirements,\nparticularly memory and processing power. To mitigate such requirements, model\npruning techniques are used to create more compact models with significantly\nfewer parameters. However, current approaches do not focus on the efficient\nextraction of programming-language-specific sub-models. In this work, we\nexplore the idea of efficiently deriving coding-specific sub-models through\nunstructured pruning (i.e., Wanda). We investigate the impact of different\ndomain-specific calibration datasets on pruning outcomes across three distinct\ndomains and extend our analysis to extracting four language-specific\nsub-models: Python, Java, C++, and JavaScript. We are the first to efficiently\nextract programming-language-specific sub-models using appropriate calibration\ndatasets while maintaining acceptable accuracy w.r.t. full models. We are also\nthe first to provide analytical evidence that domain-specific tasks activate\ndistinct regions within LLMs, supporting the creation of specialized sub-models\nthrough unstructured pruning. We believe that this work has significant\npotential to enhance LLM accessibility for coding by reducing computational\nrequirements to enable local execution on consumer-grade hardware, and\nsupporting faster inference times critical for real-time development feedback.",
        "We present results from a suite of 3D high-resolution hydrodynamic\nsimulations of supernova-driven outflows from galactic disc regions with a\nrange of gas surface density, metallicity, and supernova scale height. We use\nthis suite to quantify how outflow properties -- particularly the loading\nfactors for mass, metallicity, and energy -- vary with these parameters. We\nfind that the winds fall into three broad categories: steady and hot,\nmultiphase and moderately bursty, and cool and highly bursty. The first of\nthese is characterised by efficient metal and energy loading but weak mass\nloading, the second by moderate loading of mass, metals, and energy, and the\nthird by negligible metal and energy loading but substantial mass loading. The\nmost important factor in determining the kind of wind a galaxy will produce is\nthe ratio of supernova to gas gas scale heights, with the latter set by a\ncombination of supernova rate, metallicity-dependent cooling rate, and the\ngravitational potential. These often combine in counterintuitive ways -- for\nexample increased cooling causes cold clouds to sink into the galactic midplane\nmore rapidly, lowering the volume-filling factor of dense gas and making the\nenvironment more favourable for strong winds. Our findings suggest that the\nnature of galactic winds is likely highly sensitive to phenomena such as\nrunaway stars occuring at a large height and dense gas and are poorly captured\nin most simulations, and that metal loading factors for type Ia supernovae may\nbe substantially larger than those for type II, with important implications for\ngalactic chemical evolution.",
        "In a globalized and interconnected world, interoperability has become a key\nconcept for advancing tactical scenarios. Federated Coalition Networks (FCN)\nenable cooperation between entities from multiple nations while allowing each\nto maintain control over their systems. However, this interoperability\nnecessitates the sharing of increasing amounts of information between different\ntactical assets, raising the need for higher security measures. Emerging\ntechnologies like blockchain drive a revolution in secure communications,\npaving the way for new tactical scenarios. In this work, we propose a\nblockchain-based framework to enhance the resilience and security of the\nmanagement of these networks. We offer a guide to FCN design to help a broad\naudience understand the military networks in international missions by a use\ncase and key functions applied to a proposed architecture. We evaluate its\neffectiveness and performance in information encryption to validate this\nframework.",
        "The core challenge in basketball tactic modeling lies in efficiently\nextracting complex spatial-temporal dependencies from historical data and\naccurately predicting various in-game events. Existing state-of-the-art (SOTA)\nmodels, primarily based on graph neural networks (GNNs), encounter difficulties\nin capturing long-term, long-distance, and fine-grained interactions among\nheterogeneous player nodes, as well as in recognizing interaction patterns.\nAdditionally, they exhibit limited generalization to untrained downstream tasks\nand zero-shot scenarios. In this work, we propose a Spatial-Temporal\nPropagation Symmetry-Aware Graph Transformer for fine-grained game modeling.\nThis architecture explicitly captures delay effects in the spatial space to\nenhance player node representations across discrete-time slices, employing\nsymmetry-invariant priors to guide the attention mechanism. We also introduce\nan efficient contrastive learning strategy to train a Mixture of Tactics\nExperts module, facilitating differentiated modeling of offensive tactics. By\nintegrating dense training with sparse inference, we achieve a 2.4x improvement\nin model efficiency. Moreover, the incorporation of Lightweight Graph Grounding\nfor Large Language Models enables robust performance in open-ended downstream\ntasks and zero-shot scenarios, including novel teams or players. The proposed\nmodel, TacticExpert, delineates a vertically integrated large model framework\nfor basketball, unifying pretraining across multiple datasets and downstream\nprediction tasks. Fine-grained modeling modules significantly enhance\nspatial-temporal representations, and visualization analyzes confirm the strong\ninterpretability of the model.",
        "Rust has become a popular system programming language that strikes a balance\nbetween memory safety and performance. Rust's type system ensures the safety of\nlow-level memory controls; however, a well-typed Rust program is not guaranteed\nto enjoy high performance. This article studies static analysis for resource\nconsumption of Rust programs, aiming at understanding the performance of Rust\nprograms. Although there have been tons of studies on static resource analysis,\nexploiting Rust's memory safety -- especially the borrow mechanisms and their\nproperties -- to aid resource-bound analysis, remains unexplored. This article\npresents RaRust, a type-based linear resource-bound analysis for well-typed\nRust programs. RaRust follows the methodology of automatic amortized resource\nanalysis (AARA) to build a resource-aware type system. To support Rust's borrow\nmechanisms, including shared and mutable borrows, RaRust introduces shared and\nnovel prophecy potentials to reason about borrows compositionally. To prove the\nsoundness of RaRust, this article proposes Resource-Aware Borrow Calculus\n(RABC) as a variant of recently proposed Low-Level Borrow Calculus (LLBC). The\nexperimental evaluation of a prototype implementation of RaRust demonstrates\nthat RaRust is capable of inferring symbolic linear resource bounds for Rust\nprograms featuring shared and mutable borrows, reborrows, heap-allocated data\nstructures, loops, and recursion.",
        "The rise of online programming education has necessitated more effective,\npersonalized interactions, a gap that PythonPal aims to fill through its\ninnovative learning system integrated with a chatbot. This research delves into\nPythonPal's potential to enhance the online learning experience, especially in\ncontexts with high student-to-teacher ratios where there is a need for\npersonalized feedback. PythonPal's design, featuring modules for conversation,\ntutorials, and exercises, was evaluated through student interactions and\nfeedback. Key findings reveal PythonPal's proficiency in syntax error\nrecognition and user query comprehension, with its intent classification model\nshowing high accuracy. The system's performance in error feedback, though\nvaried, demonstrates both strengths and areas for enhancement. Student feedback\nindicated satisfactory query understanding and feedback accuracy but also\npointed out the need for faster responses and improved interaction quality.\nPythonPal's deployment promises to significantly enhance online programming\neducation by providing immediate, personalized feedback and interactive\nlearning experiences, fostering a deeper understanding of programming concepts\namong students. These benefits mark a step forward in addressing the challenges\nof distance learning, making programming education more accessible and\neffective.",
        "Eavesdroppers of wireless signals want to infer as much as possible regarding\nthe transmitter (Tx). Popular methods to minimize information leakage to the\neavesdropper include covert communication, directional modulation, and\nbeamforming with nulling. In this paper we do not attempt to prevent\ninformation leakage to the eavesdropper like the previous methods. Instead we\npropose to beamform the wireless signal at the Tx in such a way that it\nincorporates deceptive information. The beamformed orthogonal frequency\ndivision multiplexing (OFDM) signal includes a deceptive value for the Doppler\n(velocity) and range of the Tx. To design the optimal baseband waveform with\nthese characteristics, we define and solve an optimization problem for\npower-efficient deceptive wireless beamforming (DWB). The relaxed convex\nQuadratic Program (QP) is solved using a heuristic algorithm. Our simulation\nresults indicate that our DWB scheme can successfully inject deceptive\ninformation with low power consumption, while preserving the shape of the\ncreated beam.",
        "3D Gaussian Splatting (3DGS) has recently emerged as a pioneering approach in\nexplicit scene rendering and computer graphics. Unlike traditional neural\nradiance field (NeRF) methods, which typically rely on implicit,\ncoordinate-based models to map spatial coordinates to pixel values, 3DGS\nutilizes millions of learnable 3D Gaussians. Its differentiable rendering\ntechnique and inherent capability for explicit scene representation and\nmanipulation positions 3DGS as a potential game-changer for the next generation\nof 3D reconstruction and representation technologies. This enables 3DGS to\ndeliver real-time rendering speeds while offering unparalleled editability\nlevels. However, despite its advantages, 3DGS suffers from substantial memory\nand storage requirements, posing challenges for deployment on\nresource-constrained devices. In this survey, we provide a comprehensive\noverview focusing on the scalability and compression of 3DGS. We begin with a\ndetailed background overview of 3DGS, followed by a structured taxonomy of\nexisting compression methods. Additionally, we analyze and compare current\nmethods from the topological perspective, evaluating their strengths and\nlimitations in terms of fidelity, compression ratios, and computational\nefficiency. Furthermore, we explore how advancements in efficient NeRF\nrepresentations can inspire future developments in 3DGS optimization. Finally,\nwe conclude with current research challenges and highlight key directions for\nfuture exploration."
      ]
    }
  },
  {
    "id":2411.18902,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
    "start_abstract":"Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution recurrent structured state space models (SSMs) have been developed to address Transformers' computational inefficiency long sequences, but they not performed well important modalities language. We identify that a key weakness is their inability perform content-based reasoning, make several improvements. First, simply letting SSM parameters be functions input addresses with discrete modalities, allowing model selectively propagate or forget information along sequence length dimension depending current token. Second, even though this change prevents use efficient convolutions, we design hardware-aware parallel algorithm mode. integrate these selective SSMs into simplified end-to-end neural network without MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) scaling length, performance improves real data up million-length sequences. As general backbone, achieves state-of-the-art across language, audio, genomics. On language modeling, our Mamba-3B outperforms Transformers same size matches twice size, both pretraining downstream evaluation.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "Biomechanics and motor control of human movement"
      ],
      "abstract":[
        "Preface to the Fourth Edition. 1 Biomechanics as an Interdiscipline. 1.0 Introduction. 1.1 Measurement, Description, Analysis, and Assessment. 1.2 its Relationship with Physiology Anatomy. 1.3 Scope of Textbook. 1.4 References. 2 Signal Processing. 2.0 2.1 Auto- Cross-Correlation Analyses. 2.2 Frequency Analysis. 2.3 Ensemble Averaging Repetitive Waveforms. 2.4 3 Kinematics. 3.0 Historical Development Complexity Problem. 3.1 Kinematic Conventions. 3.2 Direct Measurement Techniques. 3.3 Imaging 3.4 Processing Raw Data. 3.5 Calculation Other Variables. 3.6 Problems Based on 3.7 4 Anthropometry. 4.0 Anthropometry in Movement Biomechanics. 4.1 Density, Mass, Inertial Properties. 4.2 Experimental Measures. 4.3 Muscle 4.4 Anthropometric 4.5 5 Kinetics: Forces Moments Force. 5.0 Biomechanical Models. 5.1 Basic Link-Segment Equations-the Free-Body Diagram. 5.2 Force Transducers Plates. 5.3 Bone-on-Bone During Dynamic Conditions. 5.4 Kinetic 5.5 6 Mechanical Work, Energy, Power. 6.0 6.1 Efficiency. 6.2 Forms Energy Storage. 6.3 Internal External Work. 6.4 Power Balances at Joints Within Segments. 6.5 6.6 7 Three-Dimensional Kinematics Kinetics. 7.0 7.1 Axes Systems. 7.2 Marker Anatomical 7.3 Determination Segment Angular Velocities Accelerations. 7.4 Analysis Reaction Moments. 7.5 Suggested Further Reading. 7.6 8 Synthesis Human Movement-Forward Solutions. 8.0 8.1 Review Forward Solution 8.2 Mathematical Formulation. 8.3 System Energy. 8.4 Torques. 8.5 Designation Joints. 8.6 Illustrative Example. 8.7 Conclusions. 8.8 9 Mechanics. 9.0 9.1 Force-Length Characteristics Muscles. 9.2 Force-Velocity Characteristics. 9.3 Modeling. 9.4 10 Kinesiological Electromyography. 10.0 10.1 Electrophysiology Contraction. 10.2 Recording Electromyogram. 10.3 Electromyogram,. 10.4 between Electromyogram 10.5 11 Synergies. 11.0 11.1 The Support Moment Synergy. 11.2 Medial\/Lateral Anterior\/Posterior Balance Standing. 11.3 during Walking. 11.4 APPENDICES. A. Kinematic, Kinetic, Figure A.1 Walking Trial-Marker Locations Mass Frame Rate Information. Table Coordinate Data (cm). A.2( a ) Filtered Kinematics-Rib Cage Greater Trochanter (Hip). b Kinematics-Femoral Lateral Epicondyle (Knee) Head Fibula. c Kinematics-Lateral Malleolus (Ankle) Heel. d Kinematics-Fifth Metatarsal Toe. A.3( Linear Kinematics-Foot. Kinematics-Leg. Kinematics-Thigh. Kinematics-1\/2 HAT. A.4 Relative Joint Kinematics-Ankle, Knee, Hip. A.5( Force-Ankle Knee. Force-Hip. A.6 Potential, Total Energies-Foot, Leg, Thigh, and1\/2 A.7 Generation\/Absorption Transfer-Ankle, B. Units Definitions Related Electromyographical Measurements. B.1 Base SI Units. B.2 Derived Index."
      ],
      "categories":[
        "Biomechanics"
      ]
    },
    "list":{
      "title":[
        "Counterexample to Winkler's conjecture on Venn diagrams",
        "On the sampling entropy of permutons",
        "A classical proof of quantum knowledge for multi-prover interactive\n  proof systems",
        "Impact of UC2, UC, UBC and UB2 target compositions on the release of\n  fission products",
        "Dual-Lagrange Encoding for Storage and Download in Elastic Computing for\n  Resilience",
        "MEXA-CTP: Mode Experts Cross-Attention for Clinical Trial Outcome\n  Prediction",
        "Outer space and finiteness properties for symmetric automorphisms of\n  RAAGs, and generalisations",
        "Non-Stationary Gradient Descent for Optimal Auto-Scaling in Serverless\n  Platforms",
        "A single-loop SPIDER-type stochastic subgradient method for\n  expectation-constrained nonconvex nonsmooth optimization",
        "Finite groups admitting a regular tournament $m$-semiregular\n  representation",
        "Elastic displacements and viscous hydrodynamic flows in wedge-shaped\n  geometries featuring a straight edge: Green's functions for forces oriented\n  parallel to the edge",
        "An Extremely Hot Pulsating Pre-White Dwarf from OGLE",
        "A Max-Min problem on spectral radius and connectedness of graphs",
        "New constraints on the galactic ionizing efficiency and escape fraction\n  at 2.5 < z < 6 based on quasar absorption spectra",
        "An Efficient Dual ADMM for Huber Regression with Fused Lasso Penalty",
        "A Bayesian Proportional Mean Model Using Panel Binary Data-An\n  Application to Health and Retirement Study",
        "MoSi$_2$N$_4$-like crystals -- the new family of two-dimensional\n  materials",
        "Simulation of Random LR Fuzzy Intervals",
        "Optimal control in phase space applied to minimal-time transfer of\n  thermal atoms in optical traps",
        "Biasing with an independent increment: Gaussian approximations and\n  proximity of Poisson mixtures",
        "Mid-infrared dual comb spectroscopy via continuous-wave optical\n  parametric oscillation",
        "Analysis and Mitigation of Cascading Failures Using a Stochastic\n  Interaction Graph with Eigen-analysis",
        "A spectroscopic map of the Galactic centre -- Observations and resolved\n  stars",
        "Duality breaking, mobility edges, and the connection between topological\n  Aubry-Andr\\'e and quantum Hall insulators in atomic wires with fermions",
        "Thermodynamics-Like Formalism for Immiscible and Incompressible\n  Two-Phase Flow in Porous Media",
        "Borel fractional perfect matchings in quasi-transitive amenable graphs",
        "Decentralized Federated Dataset Dictionary Learning for Multi-Source\n  Domain Adaptation",
        "Extensions of a theorem of P. Hall on indexes of maximal subgroups",
        "Scalar-Tensor Gravity and DESI 2024 BAO data"
      ],
      "abstract":[
        "In 1984, Peter Winkler conjectured that every simple Venn diagram with $n$\ncurves can be extended to a simple Venn diagram with $n+1$ curves. We present a\ncounterexample to his conjecture for $n=7$, which is obtained by combining\ntheoretical ideas with computer assistance from state-of-the-art SAT solvers.",
        "For a permuton $\\mu$ let $H_n(\\mu)$ denote the Shannon entropy of the\nsampling distribution of $\\mu$ on $n$ points. We investigate the asymptotic\ngrowth of $H_n(\\mu)$ for a wide class of permutons.\n  We prove that if $\\mu$ has a non-vanishing absolutely continuous part, then\n$H_n(\\mu)$ has a growth rate $\\Theta(n \\log n)$. We show that if $\\mu$ is the\ngraph of a piecewise continuously differentiable, measure-preserving function\n$f$, then $H_n(\\mu)\/n$ tends to the Kolmogorov--Sinai entropy of $f$. Using\ngenericity arguments, we also prove the existence of function permutons for\nwhich $H_n(\\mu)$ does not converge either after normalizing by $n$ or by $n\\log\nn$.\n  We study the sampling entropy of a natural family of random fractal-like\npermutons determined by a sequence of i.i.d. choices. It turns out that for\nevery $n$, $H_n(\\mu)\/n$ is heavily concentrated. We prove that the sequence\n$H_n(\\mu)\/n$ either converges or has deterministic log-periodic oscillations\nalmost surely, and argue towards the conjecture that in nondegenerate case,\noscillation holds. On the other hand, for a straightforward random perturbation\nof the model $\\tilde{\\mu}$ of $\\mu$, we prove the almost sure convergence of\n$H_n(\\tilde{\\mu})\/n$.",
        "In a proof of knowledge (PoK), a verifier becomes convinced that a prover\npossesses privileged information. In combination with zero-knowledge proof\nsystems, PoKs are an important part of secure protocols such as digital\nsignature schemes and authentication schemes as they enable a prover to\ndemonstrate possession of a certain piece of information (such as a private key\nor a credential), without revealing it. Formally, A PoK is defined via the\nexistence of an extractor, which is capable of reconstructing the key\ninformation that makes a verifier accept, given oracle access to the prover. We\nextend the concept of a PoK in the setting of a single classical verifier and\ntwo quantum provers, and exhibit the PoK property for a non-local game for the\nlocal Hamiltonian problem. More specifically, we construct an extractor which,\ngiven oracle access to a provers' strategy that leads to high acceptance\nprobability, is able to reconstruct the ground state of a local Hamiltonian.\nOur result can be seen as a new form of self-testing, where, in addition to\ncertifying a pre-shared entangled state and the prover's strategy, the verifier\nalso certifies a local quantum state. This technique thus provides a method to\nascertain that a prover has access to a quantum system, in particular, a ground\nstate, thus indicating a new level of verification for a proof of quantumness.",
        "The release properties of 4 targets (UC2, UC, UBC, UB2) were measured for 11\nelements (Kr, Sr, Ru, Sn, Sb, Te, I, Cs, Ba, La, and Ce) using an off-line\ntechnique. The crystal packing fraction and the size of the studied element\nplay a key role in the release process. However, physicochemical properties are\nalso involved, notably melting and boiling points in vacuum and the minimal\noxidation state. Principal component analysis was used to investigate the\ninterrelationships between the physicochemical properties of fission products\n(from Fe to Dy) and the observed releases, thereby enabling predictions to be\nmade about the release properties of the four crystallic configurations for\nelements that are inaccessible in off-line experiments.",
        "Coded elastic computing enables virtual machines to be preempted for\nhigh-priority tasks while allowing new virtual machines to join ongoing\ncomputation seamlessly. This paper addresses coded elastic computing for\nmatrix-matrix multiplications with straggler tolerance by encoding both storage\nand download using Lagrange codes. In 2018, Yang et al. introduced the first\ncoded elastic computing scheme for matrix-matrix multiplications, achieving a\nlower computational load requirement. However, this scheme lacks straggler\ntolerance and suffers from high upload cost. Zhong et al. (2023) later tackled\nthese shortcomings by employing uncoded storage and Lagrange-coded download.\nHowever, their approach requires each machine to store the entire dataset. This\npaper introduces a new class of elastic computing schemes that utilize Lagrange\ncodes to encode both storage and download, achieving a reduced storage size.\nThe proposed schemes efficiently mitigate both elasticity and straggler\neffects, with a storage size reduced to a fraction $\\frac{1}{L}$ of Zhong et\nal.'s approach, at the expense of doubling the download cost. Moreover, we\nevaluate the proposed schemes on AWS EC2 by measuring computation time under\ntwo different tasks allocations: heterogeneous and cyclic assignments. Both\nassignments minimize computation redundancy of the system while distributing\nvarying computation loads across machines.",
        "Clinical trials are the gold standard for assessing the effectiveness and\nsafety of drugs for treating diseases. Given the vast design space of drug\nmolecules, elevated financial cost, and multi-year timeline of these trials,\nresearch on clinical trial outcome prediction has gained immense traction.\nAccurate predictions must leverage data of diverse modes such as drug\nmolecules, target diseases, and eligibility criteria to infer successes and\nfailures. Previous Deep Learning approaches for this task, such as HINT, often\nrequire wet lab data from synthesized molecules and\/or rely on prior knowledge\nto encode interactions as part of the model architecture. To address these\nlimitations, we propose a light-weight attention-based model, MEXA-CTP, to\nintegrate readily-available multi-modal data and generate effective\nrepresentations via specialized modules dubbed \"mode experts\", while avoiding\nhuman biases in model design. We optimize MEXA-CTP with the Cauchy loss to\ncapture relevant interactions across modes. Our experiments on the Trial\nOutcome Prediction (TOP) benchmark demonstrate that MEXA-CTP improves upon\nexisting approaches by, respectively, up to 11.3% in F1 score, 12.2% in PR-AUC,\nand 2.5% in ROC-AUC, compared to HINT. Ablation studies are provided to\nquantify the effectiveness of each component in our proposed method.",
        "We define the symmetric (outer) automorphism group of a right-angled Artin\ngroup and construct for it a (spine of) Outer space. This `symmetric spine' is\na contractible cube complex upon which the symmetric outer automorphism group\nacts properly and cocompactly. One artefact of our technique is a strengthening\nof the proof of contractibility of the untwisted spine, mimicking the original\nproof that Culler--Vogtmann Outer space is contractible, which may be of\nindependent interest. We apply our results to derive finiteness properties for\ncertain subgroups of outer automorphisms. In particular, we prove that the\nsubgroup consisting of those outer automorphisms which permute any given finite\nset of conjugacy classes of a right-angled Artin group is of type \\emph{VF},\nand we show that the virtual cohomological dimension of the symmetric outer\nautomorphism group is equal to both the dimension of the symmetric spine and\nthe rank of a free abelian subgroup.",
        "To efficiently manage serverless computing platforms, a key aspect is the\nauto-scaling of services, i.e., the set of computational resources allocated to\na service adapts over time as a function of the traffic demand. The objective\nis to find a compromise between user-perceived performance and energy\nconsumption. In this paper, we consider the \\emph{scale-per-request}\nauto-scaling pattern and investigate how many function instances (or servers)\nshould be spawned each time an \\emph{unfortunate} job arrives, i.e., a job that\nfinds all servers busy upon its arrival. We address this problem by following a\nstochastic optimization approach: we develop a stochastic gradient descent\nscheme of the Kiefer--Wolfowitz type that applies \\emph{over a single run of\nthe state evolution}. At each iteration, the proposed scheme computes an\nestimate of the number of servers to spawn each time an unfortunate job arrives\nto minimize some cost function. Under natural assumptions, we show that the\nsequence of estimates produced by our scheme is asymptotically optimal almost\nsurely. In addition, we prove that its convergence rate is $O(n^{-2\/3})$ where\n$n$ is the number of iterations.\n  From a mathematical point of view, the stochastic optimization framework\ninduced by auto-scaling exhibits non-standard aspects that we approach from a\ngeneral point of view. We consider the setting where a controller can only get\nsamples of the \\emph{transient} -- rather than stationary -- behavior of the\nunderlying stochastic system. To handle this difficulty, we develop arguments\nthat exploit properties of the mixing time of the underlying Markov chain. By\nmeans of numerical simulations, we validate the proposed approach and quantify\nits gain with respect to common existing scale-up rules.",
        "Many real-world problems, such as those with fairness constraints, involve\ncomplex expectation constraints and large datasets, necessitating the design of\nefficient stochastic methods to solve them. Most existing research focuses on\ncases with no {constraint} or easy-to-project constraints or deterministic\nconstraints. In this paper, we consider nonconvex nonsmooth stochastic\noptimization problems with expectation constraints, for which we build a novel\nexact penalty model. We first show the relationship between the penalty model\nand the original problem. Then on solving the penalty problem, we present a\nsingle-loop SPIDER-type stochastic subgradient method, which utilizes the\nsubgradients of both the objective and constraint functions, as well as the\nconstraint function value at each iteration. Under certain regularity\nconditions (weaker than Slater-type constraint qualification or strong\nfeasibility assumed in existing works), we establish an iteration complexity\nresult of $O(\\epsilon^{-4})$ to reach a near-$\\epsilon$ stationary point of the\npenalized problem in expectation, matching the lower bound for such tasks.\nBuilding on the exact penalization, an $(\\epsilon,\\epsilon)$-KKT point of the\noriginal problem is obtained. For a few scenarios, our complexity of either the\n{objective} sample subgradient or the constraint sample function values can be\nlower than the state-of-the-art results by a factor of $\\epsilon^{-2}$.\nMoreover, on solving two fairness-constrained problems, our method is\nsignificantly (up to 466 times) faster than the state-of-the-art algorithms,\nincluding switching subgradient method and inexact proximal point methods.",
        "For a positive integer $m$, a finite group $G$ is said to admit a tournament\n$m$-semiregular representation (TmSR for short) if there exists a tournament\n$\\Gamma$ such that the automorphism group of $\\Gamma$ is isomorphic to $G$ and\nacts semiregularly on the vertex set of $\\Gamma$ with $m$ orbits. Clearly,\nevery finite group of even order does not admit a TmSR for any positive integer\n$m$, and T1SR is the well-known tournament regular representation (TRR for\nshort). In 1986, Godsil \\cite{god} proved, by a probabilistic approach, that\nthe only finite groups of odd order without a TRR are $\\mathbb{Z}_3^2$ and\n$\\mathbb{Z}_3^3$ .\n  More recently, Du \\cite{du} proved that every finite group of odd order has a\nTmSR for every $m \\geq 2$. The author of \\cite{du} observed that a finite group\nof odd order has no regular TmSR when $m$ is an even integer, a group of order\n$1$ has no regular T3SR, and $\\mathbb{Z}_3^2$ admits a regular T3SR. At the end\nof \\cite{du}, Du proposed the following problem.\n  \\noindent{\\sf\\it Problem.} \\ \\ {\\it For every odd integer $m\\geq 3$, classify\nfinite groups of odd order which have a regular TmSR.}\n  The motivation of this paper is to give an answer for the above problem. We\nproved that if $G$ is a finite group with odd order $n>1$, then $G$ admits a\nregular TmSR for any odd integer $m\\geq 3$.",
        "For homogeneous and isotropic linearly elastic solids and for incompressible\nfluids under low-Reynolds-number conditions the fundamental solutions of the\nassociated continuum equations were derived a long time ago for bulk systems.\nThat is, the corresponding Green's functions are available in infinitely\nextended systems, where boundaries do not play any role. However, introducing\nboundaries renders the situation significantly more complex. Here, we derive\nthe corresponding Green's functions for a linearly elastic homogeneous and\nisotropic material in a wedge-shaped geometry. Two flat boundaries confine the\nmaterial and meet at a straight edge. No-slip and free-slip conditions are\nconsidered. The force is oriented in a direction parallel to the straight edge\nof the wedge. Assuming incompressibility, our expressions also apply to the\nsituation of low-Reynolds-number hydrodynamic viscous fluid flows. Thus, they\nmay be used, for instance, to describe the motion of self-propelled objects\nguided by an edge or the distortion of soft elastic actuators in wedge-shaped\nenvironments of operation.",
        "We show that the blue 18.3-minute variable object discovered in the Galactic\ndisk by the OGLE-III survey and named OGLE-GD-WD-0001 is a pulsating pre-white\ndwarf of PG 1159 spectral type. With an effective temperature of about 160,000\nK it is among the hottest known pulsators being located close to the blue edge\nof the GW Virginis instability strip. The long-term OGLE observations indicate\nthat the object has a positive period change rate of about $5 \\times 10^{-10}$\n$s~s^{-1}$ and thus already contracts. There are no traces of a planetary\nnebula around this star.",
        "In the past decades, many scholars concerned which edge-extremal problems\nhave spectral analogues? Recently, Wang, Kang and Xue showed an interesting\nresult on $F$-free graphs [J. Combin. Theory Ser. B 159 (2023) 20--41]. In this\npaper, we study the above problem on critical graphs.Let $P$ be a property\ndefined on a family $\\mathbb{G}$ of graphs. A graph $G$ in $\\mathbb{G}$ is said\nto be $P$-critical,if it has the property $P$ but $G-e$ no longer has for any\nedge $e\\in E(G)$. Especially, a graph is minimally $k$-(edge)-connected,if it\nis $k$-connected (respectively, $k$-edge connected) and deleting an arbitrary\nedge always leaves a graph which is not $k$-connected (respectively,\n$k$-edge-connected). An interesting Max-Min problem asks what is the maximal\nspectral radius of an $n$-vertex minimally $k$-(edge)-connected graphs? In\n2019, Chen and Guo [Discrete Math. 342 (2019) 2092--2099] gave the answer for\n$k=2$. In 2021, Fan, Goryainov and Lin [Discrete Appl. Math. 305 (2021)\n154--163] determined the extremal spectral radius for minimally $3$-connected\ngraphs. We obtain some structural properties of minimally $k$-(edge)-connected\ngraphs. Furthermore, we solve the above Max-Min problem for $k\\geq3$, which\nimplies that every minimally $k$-(edge)-connected graph with maximal spectral\nradius also has maximal number of edges. Finally, a general problem is posed\nfor further research.",
        "Measurements of the ionization state of the intergalactic medium (IGM) can\nprobe the sources of the extragalactic ionizing background. We provide new\nmeasurements of the ionizing emissivity of galaxies using measurements of the\nionizing background and ionizing photon mean free path from high-redshift\nquasar spectra at $2.5 < z < 6$. Unlike most prior works, we account for\nradiative-transfer effects and possible neutral islands from the tail of\nreionization at $z > 5$. We combine our results with measurements of the UV\nluminosity function to constrain the average escaping ionizing efficiency of\ngalaxies, $\\langle f_{\\rm esc} \\xi_{\\rm ion}\\rangle_{L_{\\rm UV}}$. Assuming\ngalaxies with $M_{\\rm UV} < -11$ emit ionizing photons, we find $\\log (\\langle\nf_{\\rm esc} \\xi_{\\rm ion}\\rangle_{L_{\\rm UV}}\/{\\rm erg^{-1}Hz}) =\n24.47_{-0.17}^{+0.09}$ and $24.75_{-0.28}^{+0.15}$ at $z=5$ and $6$, and\n$1\\sigma$ upper limits of $24.48$ and $24.31$ at $z = 2.5$ and $4$,\nrespectively. We also estimate the population-averaged $f_{\\rm esc}$ using\nmeasurements of intrinsic ionizing efficiency from JWST. We find $\\langle\nf_{\\rm esc} \\rangle = 0.126_{-0.041}^{+0.034}$ and $0.224_{-0.108}^{+0.098}$ at\n$z=5$ and $6$, and $1\\sigma$ upper limits of $f_{\\rm esc}< 0.138$ and $0.096$\nat $z=2.5$ and $4$, respectively, for $M_{\\rm UV} < -11$. Our findings are\nconsistent with prior measurements of $f_{\\rm esc} \\lesssim 10\\%$ at $z \\leq\n4$, but indicate a factor of several increase between $z = 4$ and $6$. The\nsteepness of this evolution is sensitive to the highly uncertain mean free path\nand ionizing background intensity at $z>5$. Lastly, we find\n$1.10^{+0.21}_{-0.39}$ photons per H atom are emitted into the IGM between\n$z=6$ and $=5.3$. This is $\\approx 4\\times$ more than needed to complete the\nlast $20\\%$ of reionization absent recombinations, suggesting that\nreionization's end was likely absorption-dominated.",
        "The ordinary least squares estimate in linear regression is sensitive to the\ninfluence of errors with large variance, which reduces its robustness,\nespecially when dealing with heavy-tailed errors or outliers frequently\nencountered in real-world scenarios. To address this issue and accommodate the\nsparsity of coefficients along with their sequential disparities, we combine\nthe adaptive robust Huber loss function with a fused lasso penalty. This\ncombination yields a robust estimator capable of simultaneously achieving\nestimation and variable selection. Furthermore, we utilize an efficient\nalternating direction method of multipliers to solve this regression model from\na dual perspective. The effectiveness and efficiency of our proposed approach\nis demonstrated through numerical experiments carried out on both simulated and\nreal datasets.",
        "In recurrent event studies, panel binary data arise when subjects are\nobserved at discrete time points and only the recurrent event status within\neach observation window is recorded. Such data frequently occur in longitudinal\nstudies due to recall difficulties or participants' privacy concerns during\nfollow-ups, necessitating rigorous statistical analysis. While frequentist\nmethods exist for handling such data, Bayesian approaches remain largely\nunexplored. This article proposes an efficient Bayesian proportional mean model\nfor analysing recurrent events using panel binary data. In addition to the\nestimation procedure, the article introduces techniques for model validation,\nselection, and Bayesian influence diagnostics. Simulation studies demonstrate\nthe method's effectiveness and robustness in different practical scenarios. The\nproposed approach is then applied to analyse the latest version of the Health\nand Retirement Study dataset, identifying key risk factors influencing doctor\nvisits among the elderly. The analysis is therefore capable of providing\nvaluable insights into healthcare utilisation patterns in ageing populations.",
        "Recently-synthesised MoSi$_2$N$_4$ is the first septuple layer\ntwo-dimensional material, which doesn't naturally occurs as a layered crystal,\nand has been obtained with CVD growth. It can be considered as MoN$_2$ crystal\n(with a crystal structure of MoS2) intercalating Si2N2 two-dimensional layer\n(with the structure similar to InSe). Such classification gave rise to the\nunderstanding of the electronic properties of the material, but also to the\nprediction of other members of the family (many dozens of them) as well as to\nthe way to classify those. Whereas the originally-synthesised MoSi$_2$N$_4$ is\na semiconductor, some of the members of the family are also metallic and some\neven demonstrate magnetic properties. Interestingly, the room-temperature\nmobility predicted for such crystals can be as high few thousands cm$^2$\/V*s\n(hole mobility typically higher than electron) with some record cases as high\nas $5\\times 10^4$ cm$^2$\/V*s, making these materials strong contenders for\nfuture electronic applications. The major interest towards these materials is\ncoming from the septuple layer structure, which allows multiple crystal phases,\nbut also complex compositions, in particular those with broken\nmirror-reflection symmetry against the layer of metal atoms.",
        "Random fuzzy variables join the modeling of the impreciseness (due to their\n``fuzzy part'') and randomness. Statistical samples of such objects are widely\nused, and their direct, numerically effective generation is therefore\nnecessary. Usually, these samples consist of triangular or trapezoidal fuzzy\nnumbers. In this paper, we describe theoretical results and simulation\nalgorithms for another family of fuzzy numbers -- LR fuzzy numbers with\ninterval-valued cores. Starting from a simulation perspective on the piecewise\nlinear LR fuzzy numbers with the interval-valued cores, their limiting behavior\nis then considered. This leads us to the numerically efficient algorithm for\nsimulating a sample consisting of such fuzzy values.",
        "We present an optimal control procedure for the non-adiabatic transport of\nultracold neutral thermal atoms in optical tweezers arranged in a\none-dimensional array, with focus on reaching minimal transfer time. The\nparticle dynamics are modeled first using a classical approach through the\nLiouville equation and second through the quantum Wigner equation to include\nquantum effects. Both methods account for typical experimental noise described\nas stochastic effects through Fokker-Planck terms. The optimal control process\nis initialized with a trajectory computed for a single classical particle and\ndetermines the phase-space path that minimizes transport time and ensures high\ntransport fidelity to the target trap. This approach provides the fastest and\nmost efficient method for relocating atoms from an initial configuration to a\ndesired target arrangement, minimizing time and energy costs while ensuring\nhigh fidelity. Such an approach may be highly valuable to initialize large atom\narrays for quantum simulation or computation experiments.",
        "We establish three sets of approximation results: (a) bounds on the proximity\nof Poisson mixtures with infinitely divisible mixing distributions, (b) central\nlimit theorems with explicit error bounds for sums of associated or negatively\nassociated random variables which do not require boundedness of the underlying\ndistributions, and (c) a Gaussian approximation theorem under a vanishing third\nmoment condition. Each of these make use of the observation that size-biasing\nor zero-biasing an infinitely divisible random variable may be achieved by\nadding an independent increment, combined with tools from Stein's method for\ncompound Poisson and Gaussian approximations. Applications include a\nDickman-type limit theorem and simple random sampling.",
        "Dual-comb spectroscopy has demonstrated remarkable capabilities for rapid and\nsensitive measurements; however, significant challenges still exist in\ngenerating high-power, mutually coherent mid-infrared combs. Here we\ndemonstrate that a pair of near-infrared femtosecond frequency combs can be\nspectrally translated via a continuous-wave optical parametric oscillator. The\npair of spectrally translated combs demonstrated high mutual coherence, power\nper comb tooth in excess of hundreds of microwatts, and were tunable between 4\num and 5 um. Unlike previous approaches which relied upon synchronous optical\nparametric oscillation, the present approach avoids challenges associated with\ncomb stabilization, low power per comb tooth, and complex cavity designs.\nFurther it is readily amenable to high repetition rates (gigahertz-level and\nbeyond). The flexible and facile nature of this approach provides a robust path\nfor the spectral translation of mode-locked combs, achieving spectral\nbandwidths limited only by the phase matching bandwidth of the optical\nparametric oscillator. This approach holds significant promise for applications\nin chemical kinetics, remote sensing, combustion science, and precision\nspectroscopy, where the combination of high powers, broad bandwidths, and high\nmeasurement rates are transformative.",
        "In studies on complex network systems using graph theory, eigen-analysis is\ntypically performed on an undirected graph model of the network. However, when\nanalyzing cascading failures in a power system, the interactions among failures\nsuggest the need for a directed graph beyond the topology of the power system\nto model directions of failure propagation. To accurately quantify failure\ninteractions for effective mitigation strategies, this paper proposes a\nstochastic interaction graph model and associated eigen-analysis. Different\ntypes of modes on failure propagations are defined and characterized by the\neigenvalues of a stochastic interaction matrix, whose absolute values are\nunity, zero, or in between. Finding and interpreting these modes helps identify\nthe probable patterns of failure propagation, either local or widespread, and\nthe participating components based on eigenvectors. Then, by lowering the\nfailure probabilities of critical components highly participating in a mode of\nwidespread failures, cascading can be mitigated. The validity of the proposed\nstochastic interaction graph model, eigen-analysis and the resulting mitigation\nstrategies is demonstrated using simulated cascading failure data on an NPCC\n140-bus system.",
        "The Galactic Centre region contains a dense accumulation of stars, which can\nbe separated into two components: A flattened and dense nuclear star cluster\n(NSC), and a surrounding, more extended and more flattened, nuclear stellar\ndisc (NSD). Previous studies have collected a few thousand spectra of the inner\nNSC, and also the outer NSD, and measured line-of-sight velocities and\nmetallicities. Until now, such measurements exist only for a few 100 stars in\nthe region where the stellar surface density transitions from being dominated\nby the NSC into being dominated by the NSD. We want to study the stellar\npopulation from the centre of the NSC out to well beyond its effective radius,\nwhere the NSD dominates. We investigate whether and how the mean properties and\nkinematics of the stars change systematically. We conducted spectroscopic\nobservations with Flamingos-2 in the K-band via a continuous slit-scan. The\ndata extend from the central NSC into the inner NSD, out to 32 pc from Sgr A*\nalong Galactic longitude l. Based on their CO equivalent width, we classify the\nstars as hot or cool stars. The former are massive, young stars, while almost\nall of the latter are older than one to a few Gyr. We measure the overall\nmetallicity [M\/H] and line-of-sight velocity for >2,500 cool stars, and present\nthe first continuous spatial maps and profiles of the mean value of various\nstellar and kinematic parameters. We identify hot, young stars across the field\nof view. Some stars appear to be isolated, while others accumulate near the\nQuintuplet cluster or the central parsec cluster. The position-velocity curve\nof the cool stars shows no dependence on [M\/H], but it depends on the colour of\nthe stars. The colour may be a tracer of the line-of-sight distance and thus\ndistinguish stars located in the NSC from those in the NSD. [abridged]",
        "It is well known that the Aubry-Andr{\\'e} model lacks mobility edges due to\nits energy-independent self-duality but may exhibit edge states. When duality\nis broken, we show that mobility regions arise and non-trivial topological\nphases emerge. By varying the degree of duality breaking, we identify mobility\nregions and establish a connection between Aubry-Andr{\\'e} atomic wires with\nfermions and quantum Hall systems for a family of Hamiltonians that depends on\nthe relative phase of laser fields, viewed as a synthetic dimension. Depending\non the filling factor and the degree of duality breaking, we find three classes\nof non-trivial phases: conventional topological insulator, conventional\ntopological Aubry-Andr{\\'e} insulator, and unconventional (hybrid) topological\nAubry-Andr{\\'e} insulator. Finally, we discuss appropriate Chern numbers that\nillustrate the classification of topological phases of localized fermions in\natomic wires.",
        "It is possible to formulate immiscible and incompressible two-phase flow in\nporous media in a mathematical framework resembling thermodynamics based on the\nJaynes generalization of statistical mechanics. We review this approach and\ndiscuss the meaning of the emergent variables that appear, agiture, flow\nderivative and flow pressure, which are conjugate to the configurational\nentropy, the saturation and the porosity respectively. We conjecture that the\nagiture, the temperature-like variable, is directly related to the pressure\ngradient. This has as a consequence that the configurational entropy, a measure\nof how the fluids are distributed within the porous media and the accompanying\nvelocity field, and the differential mobility of the fluids are related. We\nalso develop elements of another version of the thermodynamics-like formalism\nwhere fractional flow rather than saturation is the control variable, since\nthis is typically the natural control variable in experiments.",
        "We show that if a locally finite Borel graph with quasitransitive amenable\ncomponents admits a fractional perfect matching, it will admit a Borel\nfractional perfect matching. In particular, if a countable amenable\nquasitransitive graph admits a fractional perfect matching then its Bernoulli\ngraph admits a Borel fractional perfect matching.",
        "Decentralized Multi-Source Domain Adaptation (DMSDA) is a challenging task\nthat aims to transfer knowledge from multiple related and heterogeneous source\ndomains to an unlabeled target domain within a decentralized framework. Our\nwork tackles DMSDA through a fully decentralized federated approach. In\nparticular, we extend the Federated Dataset Dictionary Learning (FedDaDiL)\nframework by eliminating the necessity for a central server. FedDaDiL leverages\nWasserstein barycenters to model the distributional shift across multiple\nclients, enabling effective adaptation while preserving data privacy. By\ndecentralizing this framework, we enhance its robustness, scalability, and\nprivacy, removing the risk of a single point of failure. We compare our method\nto its federated counterpart and other benchmark algorithms, showing that our\napproach effectively adapts source domains to an unlabeled target domain in a\nfully decentralized manner.",
        "We extend a classical theorem of P. Hall that claims that if the index of\nevery maximal subgroup of a finite group $G$ is a prime or the square of a\nprime, then $G$ is solvable. Precisely, we prove that if one allows, in\naddition, the possibility that every maximal subgroup of $G$ is nilpotent\ninstead of having prime or squared-prime index, then $G$ continues to be\nsolvable. Likewise, we obtain the solvability of $G$ when we assume that every\nproper non-maximal subgroup of $G$ lies in some subgroup of index prime or\nsquared prime.",
        "We discuss the implications of the DESI 2024 BAO data on scalar-tensor models\nof gravity. We consider four representative models: induced gravity (IG,\nequivalent to Jordan-Brans-Dicke), where we either fix today's value of the\neffective gravitational constant on cosmological scales to the Newton's\nconstant or allow them to differ, Jordan-Brans-Dicke supplemented with a\nGalileon term (BDG), and early modified gravity (EMG) with a conformal\ncoupling. In this way it is possible to investigate how different modified\ngravity models compare with each other when confronted with DESI 2024 BAO data.\nCompared to previous analyses, for all of these models, the combination of\nPlanck and DESI data favors a larger value of the key parameter of the theory,\nsuch as the nonminimal coupling to gravity or the Galileon term, leading also\nto a larger value of $H_0$, due to the known degeneracy between these\nparameters. These new results are mainly driven by the first two redshift bins\nof DESI. In BDG, in which we find the largest value for $H_0$ among the models\nconsidered, the combination of Planck and DESI is consistent with CCHP results\nand reduces the $H_0$ tension with the SH0ES measurement to $1.2\\sigma$\n(compared to $4.5\\sigma$ of $\\Lambda$CDM in our Planck + DESI analysis)."
      ]
    }
  },
  {
    "id":2411.03551,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Interobserver Variability in the CT Assessment of Honeycombing in the Lungs",
    "start_abstract":"To quantify observer agreement and analyze causes of disagreement in identifying honeycombing at chest computed tomography (CT).The institutional review board approved this multiinstitutional HIPAA-compliant retrospective study, informed patient consent was not required. Five core study members scored 80 CT images with a five-point scale (5 = definitely yes to 1 no) establish reference standard for the identification honeycombing. Forty-three observers from various subspecialties geographic regions by using same scoring system. Weighted \u03ba values scores compared were analyzed investigate intergroup differences. Images divided into four groups allow analysis imaging features cases which there disagreement: on presence honeycombing, absence other (none preceding three applied).Agreement 43 moderate (Cohen weighted values: 0.40-0.58). There no significant differences among defined either subspecialty or region (Tukey-Kramer test, P .38 >.99). In 29% cases, These included mixed traction bronchiectasis, large cysts, superimposed pulmonary emphysema.Identification is subjective, largely caused conditions that mimic",
    "start_categories":[
      "Radiology"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation"
      ],
      "abstract":[
        "Recently, One-stage Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained increasing interest due to simplification over its cumbersome multi-stage counterpart. Limited by the inherent ambiguity of Class Activation Map (CAM), we observe that one-stage pipelines often encounter confirmation bias caused by incorrect CAM pseudo-labels, impairing their final segmentation performance. Although recent works discard many unreliable pseudo-labels to implicitly alleviate this issue, they fail to exploit sufficient supervision for their models. To this end, we propose a dual student framework with trustworthy progressive learning (DuPL). Specifically, we propose a dual student network with a discrepancy loss to yield diverse CAMs for each sub-net. The two sub-nets generate supervision for each other, mitigating the confirmation bias caused by learning their own incorrect pseudo-labels. In this process, we progressively introduce more trustworthy pseudo-labels to be involved in the supervision through dynamic threshold adjustment with an adaptive noise filtering strategy. Moreover, we believe that every pixel, even discarded from supervision due to its unreliability, is important for WSSS. Thus, we develop consistency regularization on these discarded regions, providing supervision of every pixel. Experiment results demonstrate the superiority of the proposed DuPL over the recent state-of-the-art alternatives on PASCAL VOC 2012 and MS COCO datasets."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Discrete curve theory in space forms: planar elastic and\n  area-constrained elastic curves",
        "ScaleMAI: Accelerating the Development of Trusted Datasets and AI Models",
        "Ilargi: a GPU Compatible Factorized ML Model Training Framework",
        "Binary $k$-Center with Missing Entries: Structure Leads to Tractability",
        "3-symmetric spaces, Ricci solitons, and homogeneous structures",
        "Mixed Reality Outperforms Virtual Reality for Remote Error Resolution in\n  Pick-and-Place Tasks",
        "A numerical toy model of Langevin dynamics provides real-time\n  visualization of colloidal microdroplet evaporation",
        "Space-Time-Coupled Qubits for Enhanced Superconducting Quantum Computing",
        "A simple and flexible algorithm to generate real-world networks",
        "Development of a Test System for Data Links of the ATLAS Inner Tracker\n  (ITk) Upgrade Silicon Pixel Detector",
        "SFC-GAN: A Generative Adversarial Network for Brain Functional and\n  Structural Connectome Translation",
        "MoBA: Mixture of Block Attention for Long-Context LLMs",
        "CIBPU: A Conflict-Invisible Secure Branch Prediction Unit",
        "Cayley trees and increasing 1,2-trees: let's twist!",
        "Formation of Magnonic Waveguides via Surface Anisotropy-Induced Bragg\n  Mirrors",
        "Triple Difference Designs with Heterogeneous Treatment Effects",
        "On a new proof of the key step in the proof of Brouwer's fixed point\n  theorem",
        "From time crystals to time quasicrystals: Exploring novel phases in\n  transverse field Ising chains",
        "Efficient measure of information backflow with quasi-stochastic process",
        "The Paradox of Success in Evolutionary and Bioinspired Optimization:\n  Revisiting Critical Issues, Key Studies, and Methodological Pathways",
        "On a Problem of Kac concerning Anisotropic Lacunary Sums",
        "On Computational Complexity of 3D Ising Spin Glass: Lessons from D-Wave\n  Annealer",
        "Human Genome Book: Words, Sentences and Paragraphs",
        "Optimizing Return Distributions with Distributional Dynamic Programming",
        "Demystifying the Accuracy-Interpretability Trade-Off: A Case Study of\n  Inferring Ratings from Reviews",
        "Fast Variational Boosting for Latent Variable Models",
        "Matroid intersection and packing\/covering conjectures are true in the\n  class of finitary matroids",
        "Rigorous analytic solution to the gravitational-wave overlapping event\n  rates",
        "Orientation tracking method for anisotropic particles"
      ],
      "abstract":[
        "We propose a notion of discrete elastic and area-constrained elastic curves\nin 2-dimensional space forms. Our definition extends the well-known discrete\nEuclidean curvature equation to space forms and reflects various geometric\nproperties known from their smooth counterparts. Special emphasis is paid to\ndiscrete flows built from B\\\"acklund transformations in the respective space\nforms. The invariants of the flows form a hierarchy of curves and we show that\ndiscrete elastic and constrained elastic curves can be characterized as\nelements of this hierarchy. This work also includes an introductory chapter on\ndiscrete curve theory in space forms, where we find discrete Frenet-type\nformulas and describe an associated family related to a fundamental theorem.",
        "Building trusted datasets is critical for transparent and responsible Medical\nAI (MAI) research, but creating even small, high-quality datasets can take\nyears of effort from multidisciplinary teams. This process often delays AI\nbenefits, as human-centric data creation and AI-centric model development are\ntreated as separate, sequential steps. To overcome this, we propose ScaleMAI,\nan agent of AI-integrated data curation and annotation, allowing data quality\nand AI performance to improve in a self-reinforcing cycle and reducing\ndevelopment time from years to months. We adopt pancreatic tumor detection as\nan example. First, ScaleMAI progressively creates a dataset of 25,362 CT scans,\nincluding per-voxel annotations for benign\/malignant tumors and 24 anatomical\nstructures. Second, through progressive human-in-the-loop iterations, ScaleMAI\nprovides Flagship AI Model that can approach the proficiency of expert\nannotators (30-year experience) in detecting pancreatic tumors. Flagship Model\nsignificantly outperforms models developed from smaller, fixed-quality\ndatasets, with substantial gains in tumor detection (+14%), segmentation (+5%),\nand classification (72%) on three prestigious benchmarks. In summary, ScaleMAI\ntransforms the speed, scale, and reliability of medical dataset creation,\npaving the way for a variety of impactful, data-driven applications.",
        "The machine learning (ML) training over disparate data sources traditionally\ninvolves materialization, which can impose substantial time and space overhead\ndue to data movement and replication. Factorized learning, which leverages\ndirect computation on disparate sources through linear algebra (LA) rewriting,\nhas emerged as a viable alternative to improve computational efficiency.\nHowever, the adaptation of factorized learning to leverage the full\ncapabilities of modern LA-friendly hardware like GPUs has been limited, often\nrequiring manual intervention for algorithm compatibility. This paper\nintroduces Ilargi, a novel factorized learning framework that utilizes\nmatrix-represented data integration (DI) metadata to facilitate automatic\nfactorization across CPU and GPU environments without the need for costly\nrelational joins. Ilargi incorporates an ML-based cost estimator to\nintelligently selects between factorization and materialization based on data\nproperties, algorithm complexity, hardware environments, and their\ninteractions. This strategy ensures up to 8.9x speedups on GPUs and achieves\nover 20% acceleration in batch ML training workloads, thereby enhancing the\npracticability of ML training across diverse data integration scenarios and\nhardware platforms. To our knowledge, this work is the very first effort in\nGPU-compatible factorized learning.",
        "$\\kC$ clustering is a fundamental classification problem, where the task is\nto categorize the given collection of entities into $k$ clusters and come up\nwith a representative for each cluster, so that the maximum distance between an\nentity and its representative is minimized. In this work, we focus on the\nsetting where the entities are represented by binary vectors with missing\nentries, which model incomplete categorical data. This version of the problem\nhas wide applications, from predictive analytics to bioinformatics.\n  Our main finding is that the problem, which is notoriously hard from the\nclassical complexity viewpoint, becomes tractable as soon as the known entries\nare sparse and exhibit a certain structure. Formally, we show fixed-parameter\ntractable algorithms for the parameters vertex cover, fracture number, and\ntreewidth of the row-column graph, which encodes the positions of the known\nentries of the matrix. Additionally, we tie the complexity of the 1-cluster\nvariant of the problem, which is famous under the name Closest String, to the\ncomplexity of solving integer linear programs with few constraints. This\nimplies, in particular, that improving upon the running times of our algorithms\nwould lead to more efficient algorithms for integer linear programming in\ngeneral.",
        "The full classification of Riemannian $3$-symmetric spaces is presented. Up\nto Riemannian products the main building blocks consist in (possibly symmetric)\nspaces with semisimple isometry group, nilpotent Lie groups of step at most $2$\nand spaces of type III and IV.\n  For the most interesting family of examples, the Type III spaces, we produce\nan explicit description including results concerning the moduli space of all\n$3$-symmetric metrics living on a given Type III space. Each moduli space\ncontains a unique distinguished point corresponding to an (almost-K\\\"ahler)\nexpanding Ricci soliton metric. For certain classes of 3-symmetric metrics\nthere are many different groups acting transitively and isometrically on a\nfixed Riemannian 3-symmetric space. The construction of expanding Ricci\nsolitons on spaces of Type III is also shown to generalize to \\emph{any}\neffective representation of a simple Lie group of non-compact type, yielding a\nvery general construction of homogeneous Ricci solitons. We also give a\nprocedure to compute the isometry group of any Ambrose--Singer space.",
        "This study evaluates the performance and usability of Mixed Reality (MR),\nVirtual Reality (VR), and camera stream interfaces for remote error resolution\ntasks, such as correcting warehouse packaging errors. Specifically, we consider\na scenario where a robotic arm halts after detecting an error, requiring a\nremote operator to intervene and resolve it via pick-and-place actions.\nTwenty-one participants performed simulated pick-and-place tasks using each\ninterface. A linear mixed model (LMM) analysis of task resolution time,\nusability scores (SUS), and mental workload scores (NASA-TLX) showed that the\nMR interface outperformed both VR and camera interfaces. MR enabled\nsignificantly faster task completion, was rated higher in usability, and was\nperceived to be less cognitively demanding. Notably, the MR interface, which\nprojected a virtual robot onto a physical table, provided superior spatial\nunderstanding and physical reference cues. Post-study surveys further confirmed\nparticipants' preference for MR over other interfaces.",
        "We have developed and tested a simplified but versatile numerical model of\nnanoparticles' aggregation using Langevin dynamics. The model is particularly\ncapable of simulating aggregation in an evaporating (or condensing)\nmicrodroplet. It runs on a graphics processing unit (GPU), which makes it\nsufficiently fast for real-time conceptualization tasks. We have verified the\nresults of modeling against the findings from two types of experiments we\nconducted in electrodynamic traps. Firstly, our model helped us to elucidate\nthe phenomenon of scattering `revival', often observed during the evaporation\nof composite microdroplets. Further on, we were able to mimic our experiments,\nin which the microdroplets were dried up to form nanoparticle (NP) aggregates,\nand then soft-landed. Thus we could compare model predictions with SEM imaging.\nThe model was tested for up to $2.5\\times 10^5$ nanoparticles of several\ncoexisting types. Several types of interactions can be accounted for:\ninter-particle: Lennard-Jones and Coulomb; external: dispersion medium\nviscosity, centrifugal force, gravity, surface tension, and interface movement.\nBrownian motion of nanoparticles can be freely controlled. The core program is\naccompanied by scripts extracting statistical NP aggregates properties in\npost-processing -- fractal dimension and radial distribution functions. The\ncodes are made available in public repositories. Several diverse evolution\nscenarios are presented.",
        "The pursuit of scalable and robust quantum computing necessitates innovative\napproaches to overcome the inherent challenges of qubit connectivity,\ndecoherence, and susceptibility to noise and crosstalk. Conventional\nmonochromatic qubit coupling architectures, constrained by nearest-neighbor\ninteractions and limited algorithmic flexibility, exacerbate these issues,\nhindering the realization of practical large-scale quantum processors. In this\nwork, we introduce a paradigm leveraging a space-time-modulated\ncryogenic-compatible Josephson metasurface to enable polychromatic qubit\ncoupling. This metasurface facilitates frequency-selective interactions,\ntransforming nearest-neighbor connectivity into all-to-all qubit interactions,\nwhile significantly enhancing coherence, noise robustness, and entanglement\nfidelity. Our proposed approach capitalizes on the unique capabilities of\nspace-time-modulated Josephson metasurfaces, including dynamic four-dimensional\nwave manipulation, nonreciprocal state transmission, and state-frequency\nconversion, to mediate multi-frequency qubit interactions. By isolating qubit\ncouplings into distinct spectral channels, the cryogenic-compatible metasurface\nmitigates crosstalk and environmental decoherence, extending coherence times\nand preserving quantum state fidelity. Full-wave simulations and quantum\nperformance analyses demonstrate a significant enhancement in the operational\nefficiency of a superconducting qubit array, showcasing improved connectivity,\nrobustness, and entanglement stability. This study establishes the potential of\nspace-time-modulated cryogenic-compatible Josephson metasurfaces as a\ntransformative platform for next-generation quantum computing, addressing\ncritical bottlenecks and paving the way for scalable, high-performance quantum\nprocessors.",
        "This study introduces an algorithm that generates undirected graphs with\nthree main characteristics of real-world networks: scale-freeness, short\ndistances between nodes (small-world phenomenon), and large clustering\ncoefficients. The main idea is to perform random walks across the network and,\nat each iteration, add special edges with a decreasing probability to link more\ndistant nodes, following a specific probability distribution. A key advantage\nof our algorithm is its simplicity and flexibility in creating networks with\ndifferent characteristics without using global information about network\ntopology. We show how the parameters can be adjusted to generate networks with\nspecific average distances and clustering coefficients, maintaining a\nlong-tailed degree distribution. The implementation of our algorithm is\npublicly available on a GitHub repository.",
        "This contribution introduces a novel test system developed to evaluate the\nsignal transmission quality in high-speed data links for the 2026 Inner Tracker\n(ITk) upgrade of the ATLAS experiment. Using an FPGA-based data acquisition\n(DAQ) framework, the setup can run simultaneous Bit Error Rate (BER) tests for\nup to 64 channels and generate virtual eye diagrams, for qualifying the\n$\\sim$26k electrical links at the ATLAS ITk data rate of 1.28Gb\/s. The paper\nincludes results from system calibration, yielding its contribution to the\nmeasured losses, and preliminary results from tests of prototype and\npre-production assemblies of on-detector links of the three ATLAS ITk Pixel\nsubsystems.",
        "Modern brain imaging technologies have enabled the detailed reconstruction of\nhuman brain connectomes, capturing structural connectivity (SC) from diffusion\nMRI and functional connectivity (FC) from functional MRI. Understanding the\nintricate relationships between SC and FC is vital for gaining deeper insights\ninto the brain's functional and organizational mechanisms. However, obtaining\nboth SC and FC modalities simultaneously remains challenging, hindering\ncomprehensive analyses. Existing deep generative models typically focus on\nsynthesizing a single modality or unidirectional translation between FC and SC,\nthereby missing the potential benefits of bi-directional translation,\nespecially in scenarios where only one connectome is available. Therefore, we\npropose Structural-Functional Connectivity GAN (SFC-GAN), a novel framework for\nbidirectional translation between SC and FC. This approach leverages the\nCycleGAN architecture, incorporating convolutional layers to effectively\ncapture the spatial structures of brain connectomes. To preserve the\ntopological integrity of these connectomes, we employ a structure-preserving\nloss that guides the model in capturing both global and local connectome\npatterns while maintaining symmetry. Our framework demonstrates superior\nperformance in translating between SC and FC, outperforming baseline models in\nsimilarity and graph property evaluations compared to ground truth data, each\ntranslated modality can be effectively utilized for downstream classification.",
        "Scaling the effective context length is essential for advancing large\nlanguage models (LLMs) toward artificial general intelligence (AGI). However,\nthe quadratic increase in computational complexity inherent in traditional\nattention mechanisms presents a prohibitive overhead. Existing approaches\neither impose strongly biased structures, such as sink or window attention\nwhich are task-specific, or radically modify the attention mechanism into\nlinear approximations, whose performance in complex reasoning tasks remains\ninadequately explored.\n  In this work, we propose a solution that adheres to the ``less structure''\nprinciple, allowing the model to determine where to attend autonomously, rather\nthan introducing predefined biases. We introduce Mixture of Block Attention\n(MoBA), an innovative approach that applies the principles of Mixture of\nExperts (MoE) to the attention mechanism. This novel architecture demonstrates\nsuperior performance on long-context tasks while offering a key advantage: the\nability to seamlessly transition between full and sparse attention, enhancing\nefficiency without the risk of compromising performance. MoBA has already been\ndeployed to support Kimi's long-context requests and demonstrates significant\nadvancements in efficient attention computation for LLMs. Our code is available\nat https:\/\/github.com\/MoonshotAI\/MoBA.",
        "Previous schemes for designing secure branch prediction unit (SBPU) based on\nphysical isolation can only offer limited security and significantly affect\nBPU's prediction capability, leading to prominent performance degradation.\nMoreover, encryption-based SBPU schemes based on periodic key re-randomization\nhave the risk of being compromised by advanced attack algorithms, and the\nperformance overhead is also considerable. To this end, this paper proposes a\nconflict-invisible SBPU (CIBPU). CIBPU employs redundant storage design,\nload-aware indexing, and replacement design, as well as an encryption mechanism\nwithout requiring periodic key updates, to prevent attackers' perception of\nbranch conflicts. We provide a thorough security analysis, which shows that\nCIBPU achieves strong security throughout the BPU's lifecycle. We implement\nCIBPU in a RISC-V core model in gem5. The experimental results show that CIBPU\ncauses an average performance overhead of only 1.12%-2.20% with acceptable\nhardware storage overhead, which is the lowest among the state-of-the-art SBPU\nschemes. CIBPU has also been implemented in the open-source RISC-V core,\nSonicBOOM, which is then burned onto an FPGA board. The evaluation based on the\nboard shows an average performance degradation of 2.01%, which is approximately\nconsistent with the result obtained in gem5.",
        "An increasing 1,2-tree is a labeled graph formed by starting with a vertex\nand then repeatedly attaching a leaf to a vertex or a triangle to an edge, the\nlabeling of the vertices corresponding to the order in which the vertices are\nadded. Equivalently, increasing 1,2-trees are connected chordal graphs of\ntreewidth at most 2 labeled with a reversed perfect elimination ordering. We\nprove that this family is equinumerous with Cayley trees, which are\nunconstrained labeled trees. In particular, the number of triangles in an\nincreasing 1,2-tree corresponds to the number of twists. A twist (also called\nimproper edge) is an edge whose endpoint closer to vertex 1 has a greater label\nthan some vertex in the subtree rooted at the other endpoint of the edge. We\nprovide three proofs of this result, the rst being based on similar recursive\ndecompositions, the second on the resolution of generating functions, and the\nthird describing a bijection. Finally, we propose ecient random generators for\nthese two combinatorial families.",
        "Waveguides are fundamental components for signal transmission in integrated\nwave-based processing systems. In this paper, we address the challenges\nassociated with designing magnonic waveguides and propose a novel type with\npromising properties. Specifically, we study a magnonic waveguide formed within\na uniform ferromagnetic layer (Co$_{20}$Fe$_{60}$B$_{20}$) through surface\nanisotropy applied in stripe regions, creating Bragg mirror structures. The\nproposed waveguide enables the propagation of high-frequency spin waves with\nhigh velocities in the ferromagnetic layer while avoiding static demagnetizing\neffects. Using finite element simulations, we calculate the dispersion relation\nof the waveguide modes and analyze their spatial profiles. Additionally, we\nevaluate the group velocity and localization characteristics, providing a\ncomprehensive understanding of the waveguide's performance.",
        "Triple difference designs have become increasingly popular in empirical\neconomics. The advantage of a triple difference design is that, within\ntreatment group, it allows for another subgroup of the population --\npotentially less impacted by the treatment -- to serve as a control for the\nsubgroup of interest. While literature on difference-in-differences has\ndiscussed heterogeneity in treatment effects between treated and control groups\nor over time, little attention has been given to the implications of\nheterogeneity in treatment effects between subgroups. In this paper, I show\nthat interpretation of the usual triple difference parameter of interest, the\ndifference in average treatment effects on the treated between subgroups, may\nbe affected by this kind of heterogeneity. I propose a new parameter of\ninterest, the causal difference in average treatment effects on the treated,\nwhich makes causal comparisons between subgroups. I discuss assumptions for\nidentification and derive the semiparametric efficiency bounds for this\nparameter. I then propose doubly-robust, efficient estimators for this\nparameter. I use a simulation study to highlight the desirable finite-sample\nproperties of these estimators, as well as to show the difference between this\nparameter and the usual triple difference parameter of interest. An empirical\napplication shows the importance of considering treatment effect heterogeneity\nin practical applications.",
        "We present a solution of Exercise 1.2.1 of [2] which yields a short new proof\nof a key step in one of proofs of Brouwer's fixed point theorem, 1910. A few\npeople asked the author about the details of the solution and they might be\ninteresting to a broader audience. Our approach is absolutely different from\nthe ones using algebraic or differential topology or differential calculus and\nis based on a simple observation which somehow escaped many authors treating\nthis theorem in the past.",
        "Time crystals (TCs) and time quasicrystals (TQCs) represent novel phases of\nmatter that arise from the breaking of time translational symmetry in\nperiodically and quasiperiodically driven quantum systems. In this study, we\nexplore the formation of a TQC phase within the disordered quantum Ising chain\nmodel under a transverse field (ITF). Notably, TQCs demonstrate robust\nsubharmonic responses at multiple incommensurate frequencies, unlike\ntraditional TCs which respond at a single frequency. Our analysis reveals that\nthe TQC phase exhibits stable magnetization responses even in the presence of\ninteraction perturbations and imperfections in the quasiperiodic driving\nfields. Employing exact diagonalization techniques, we find that increasing the\nchain length further stabilizes both TC and TQC phases. These results suggest\npromising pathways for experimental realization of TQCs in cold atomic systems\nand quantum simulators, opening avenues for deeper investigation into these\nintriguing dynamical phenomena.",
        "Characterization and quantification of non-Markovian dynamics in open quantum\nsystems is a topical issue in the rapidly developing field of quantum\ncomputation and quantum communication. A standard approach based on the notion\nof information backflow detects the flow of information from the environment\nback to the system. Numerous measures of information backflow have been\nproposed using different definitions of distinguishability between pairs of\nquantum states. These measures, however, necessitate optimization over the\nstate space which can be analytically challenging or numerically demanding.\nHere, we propose an alternative witness and measure of information backflow\nthat is explicitly state-independent by utilizing the concept of\nquasiprobability representation and recent advances in the theory of\nmajorization for quasiprobabilities. We illustrate its use over several\nparadigmatic examples, demonstrating consistent Markovian conditions with known\nresults and also reported necessary and sufficient condition for qutrit system\nin random unitary channel. The paper concludes with discussions on the\nfoundational implications of quantum dynamical evolution.",
        "Evolutionary and bioinspired computation are crucial for efficiently\naddressing complex optimization problems across diverse application domains. By\nmimicking processes observed in nature, like evolution itself, these algorithms\noffer innovative solutions beyond the reach of traditional optimization\nmethods. They excel at finding near-optimal solutions in large, complex search\nspaces, making them invaluable in numerous fields. However, both areas are\nplagued by challenges at their core, including inadequate benchmarking,\nproblem-specific overfitting, insufficient theoretical grounding, and\nsuperfluous proposals justified only by their biological metaphor. This\noverview recapitulates and analyzes in depth the criticisms concerning the lack\nof innovation and rigor in experimental studies within the field. To this end,\nwe examine the judgmental positions of the existing literature in an informed\nattempt to guide the research community toward directions of solid contribution\nand advancement in these areas. We summarize guidelines for the design of\nevolutionary and bioinspired optimizers, the development of experimental\ncomparisons, and the derivation of novel proposals that take a step further in\nthe field. We provide a brief note on automating the process of creating these\nalgorithms, which may help align metaheuristic optimization research with its\nprimary objective (solving real-world problems), provided that our identified\npathways are followed. Our conclusions underscore the need for a sustained push\ntowards innovation and the enforcement of methodological rigor in prospective\nstudies to fully realize the potential of these advanced computational\ntechniques.",
        "Given a lacunary sequence $(n_k)_{k \\in \\mathbb{N}}$, arbitrary positive\nweights $(c_k)_{k \\in \\mathbb{N}}$ that satisfy a Lindeberg-Feller condition,\nand a function $f: \\mathbb{T} \\to \\mathbb{R}$ whose Fourier coefficients\n$\\hat{f_k}$ decay at rate $\\frac{1}{k^{1\/2 + \\varepsilon}}$, we prove central\nlimit theorems for $\\sum_{k \\leq N}c_kf(n_kx)$, provided $(n_k)_{k \\in\n\\mathbb{N}}$ satisfies a Diophantine condition that is necessary in general.\nThis addresses a question raised by M. Kac [Ann. of Math., 1946].",
        "Finding an exact ground state of a 3D Ising spin glass is proven to be an\nNP-hard problem. Given validity of the exponential time hypothesis, its\ncomputational complexity was proven to be no less than $2^{N^{2\/3}}$, where $N$\nis the total number of spins. Here we report results of extensive\nexperimentation with D-Wave 3D annealer with $N\\leq 5627$. We found exact\nground states (in a probabilistic sense) for typical realizations of 3D spin\nglasses with the efficiency, which scales as $2^{N\/\\beta}$ with $\\beta\\approx\n10^{3}$. Based on statistical analysis of low energy states, we argue that with\nan improvement of annealing protocols and device noise reduction, $\\beta$ can\nbe increased even further. This suggests that, for $N<\\beta^3$, annealing\ndevices provide a most efficient way to find the ground state.",
        "Since the completion of the human genome sequencing project in 2001,\nsignificant progress has been made in areas such as gene regulation editing and\nprotein structure prediction. However, given the vast amount of genomic data,\nthe segments that can be fully annotated and understood remain relatively\nlimited. If we consider the genome as a book, constructing its equivalents of\nwords, sentences, and paragraphs has been a long-standing and popular research\ndirection. Recently, studies on transfer learning in large language models have\nprovided a novel approach to this challenge.Multilingual transfer ability,\nwhich assesses how well models fine-tuned on a source language can be applied\nto other languages, has been extensively studied in multilingual pre-trained\nmodels. Similarly, the transfer of natural language capabilities to \"DNA\nlanguage\" has also been validated. Building upon these findings, we first\ntrained a foundational model capable of transferring linguistic capabilities\nfrom English to DNA sequences. Using this model, we constructed a vocabulary of\nDNA words and mapped DNA words to their English equivalents.Subsequently, we\nfine-tuned this model using English datasets for paragraphing and sentence\nsegmentation to develop models capable of segmenting DNA sequences into\nsentences and paragraphs. Leveraging these models, we processed the GRCh38.p14\nhuman genome by segmenting, tokenizing, and organizing it into a \"book\"\ncomprised of genomic \"words,\" \"sentences,\" and \"paragraphs.\" Additionally,\nbased on the DNA-to-English vocabulary mapping, we created an \"English version\"\nof the genomic book. This study offers a novel perspective for understanding\nthe genome and provides exciting possibilities for developing innovative tools\nfor DNA search, generation, and analysis.",
        "We introduce distributional dynamic programming (DP) methods for optimizing\nstatistical functionals of the return distribution, with standard reinforcement\nlearning as a special case. Previous distributional DP methods could optimize\nthe same class of expected utilities as classic DP. To go beyond expected\nutilities, we combine distributional DP with stock augmentation, a technique\npreviously introduced for classic DP in the context of risk-sensitive RL, where\nthe MDP state is augmented with a statistic of the rewards obtained so far\n(since the first time step). We find that a number of recently studied problems\ncan be formulated as stock-augmented return distribution optimization, and we\nshow that we can use distributional DP to solve them. We analyze distributional\nvalue and policy iteration, with bounds and a study of what objectives these\ndistributional DP methods can or cannot optimize. We describe a number of\napplications outlining how to use distributional DP to solve different\nstock-augmented return distribution optimization problems, for example\nmaximizing conditional value-at-risk, and homeostatic regulation. To highlight\nthe practical potential of stock-augmented return distribution optimization and\ndistributional DP, we combine the core ideas of distributional value iteration\nwith the deep RL agent DQN, and empirically evaluate it for solving instances\nof the applications discussed.",
        "Interpretable machine learning models offer understandable reasoning behind\ntheir decision-making process, though they may not always match the performance\nof their black-box counterparts. This trade-off between interpretability and\nmodel performance has sparked discussions around the deployment of AI,\nparticularly in critical applications where knowing the rationale of\ndecision-making is essential for trust and accountability. In this study, we\nconduct a comparative analysis of several black-box and interpretable models,\nfocusing on a specific NLP use case that has received limited attention:\ninferring ratings from reviews. Through this use case, we explore the intricate\nrelationship between the performance and interpretability of different models.\nWe introduce a quantitative score called Composite Interpretability (CI) to\nhelp visualize the trade-off between interpretability and performance,\nparticularly in the case of composite models. Our results indicate that, in\ngeneral, the learning performance improves as interpretability decreases, but\nthis relationship is not strictly monotonic, and there are instances where\ninterpretable models are more advantageous.",
        "We consider the problem of estimating complex statistical latent variable\nmodels using variational Bayes methods. These methods are used when exact\nposterior inference is either infeasible or computationally expensive, and they\napproximate the posterior density with a family of tractable distributions. The\nparameters of the approximating distribution are estimated using optimisation\nmethods. This article develops a flexible Gaussian mixture variational\napproximation, where we impose sparsity in the precision matrix of each\nGaussian component to reflect the appropriate conditional independence\nstructure in the model. By introducing sparsity in the precision matrix and\nparameterising it using the Cholesky factor, each Gaussian mixture component\nbecomes parsimonious (with a reduced number of non-zero parameters), while\nstill capturing the dependence in the posterior distribution. Fast estimation\nmethods based on global and local variational boosting moves combined with\nnatural gradients and variance reduction methods are developed. The local\nboosting moves adjust an existing mixture component, and optimisation is only\ncarried out on a subset of the variational parameters of a new component. The\nsubset is chosen to target improvement of the current approximation in aspects\nwhere it is poor. The local boosting moves are fast because only a small number\nof variational parameters need to be optimised. The efficacy of the approach is\nillustrated by using simulated and real datasets to estimate generalised linear\nmixed models and state space models.",
        "Given two finite matroids on the same ground set, a celebrated result of\nEdmonds says that the ground set can be partitioned into two disjoint subsets\nin a manner that there is a common independent set in both matroids whose\nintersection with the first subset spans that subset in the first matroid, and\nwhose intersection with the second subset spans that subset in the second\nmatroid.\n  There is a longstanding conjecture regarding the situation of two matroids\ndefined on the same infinite ground set. Infinite matroids were only recently\naxiomatized in the early 2010s in the work of Bruhn et al., while the\nconjecture had been proposed during the 1990s for the class of structures that\nare now called finitary matroids, which are matroids all of whose circuits are\nfinite sets.\n  The packing\/covering conjecture, due to Bowler and Carmesin, is a related\nconjecture in the sense that it is true in the class of all matroids if and\nonly if the matroid intersection conjecture is true in the class of all\nmatroids. Given any family of matroids on the same ground set, the conjecture\nasks if it is possible to partition the ground set into two disjoint subsets in\na way such that the corresponding family of matroids restricted to the first\nsubset admits a packing while the family of matroids contracted to the second\nsubset admits a covering.\n  We prove both of these conjectures in the class of finitary matroids. Our\nmain tool is nonstandard analysis, specifically the technique of iterated\nnonstandard extensions. Roughly, we first embed any infinite matroid inside a\nhyperfinite matroid defined on a subset of the nonstandard extension of the\noriginal ground set, and we iteratively nonstandardly extend the hyperfinite\nstructure again in order to prove results in the internal universe that can be\ndirectly transferred to obtain results about the matroid(s) we started with.",
        "In the era of the next-generation gravitational-wave detectors, signal\noverlaps will become prevalent due to high detection rate and long signal\nduration, posing significant challenges to data analysis. While effective\nalgorithms are being developed, there still lacks an integrated understanding\non the statistical properties for the population of overlapping events. For the\nfirst time we rigorously derive and establish analytical expressions for the\nexpectation and variance for the number of overlapping events to aid rapid and\nrobust estimation. We also mathematically prove that the time difference\nbetween events in a single observation run is described by the beta\ndistribution, offering an analytical prior reference for Bayesian analysis.",
        "A method for particle orientation tracking is developed and demonstrated\nspecifically for anisotropic particles. Using (high-speed) multi-camera\nrecordings of anisotropic particles from different viewpoints, we reconstruct\nthe 3D location and orientation of these particles using their known shape.\nThis paper describes an algorithm which tracks the location and orientation of\nmultiple anisotropic particles over time, enabling detailed investigations of\nlocation, orientation, and rotation statistics. The robustness and error of\nthis method is quantified, and we explore the effects of noise, image size, the\nnumber of used cameras, and the camera arrangement by applying the algorithm to\nsynthetic images. We showcase several use-cases of this method in several\nexperiments (in both quiescent and turbulent fluids), demonstrating the\neffectiveness and broad applicability of the described tracking method. The\nproposed method is shown to work for widely different particle shapes,\nsuccessfully tracks multiple particles simultaneously, and the method can\ndistinguish between different types of particles."
      ]
    }
  },
  {
    "id":2411.03551,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation",
    "start_abstract":"Recently, One-stage Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained increasing interest due to simplification over its cumbersome multi-stage counterpart. Limited by the inherent ambiguity of Class Activation Map (CAM), we observe that one-stage pipelines often encounter confirmation bias caused by incorrect CAM pseudo-labels, impairing their final segmentation performance. Although recent works discard many unreliable pseudo-labels to implicitly alleviate this issue, they fail to exploit sufficient supervision for their models. To this end, we propose a dual student framework with trustworthy progressive learning (DuPL). Specifically, we propose a dual student network with a discrepancy loss to yield diverse CAMs for each sub-net. The two sub-nets generate supervision for each other, mitigating the confirmation bias caused by learning their own incorrect pseudo-labels. In this process, we progressively introduce more trustworthy pseudo-labels to be involved in the supervision through dynamic threshold adjustment with an adaptive noise filtering strategy. Moreover, we believe that every pixel, even discarded from supervision due to its unreliability, is important for WSSS. Thus, we develop consistency regularization on these discarded regions, providing supervision of every pixel. Experiment results demonstrate the superiority of the proposed DuPL over the recent state-of-the-art alternatives on PASCAL VOC 2012 and MS COCO datasets.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Interobserver Variability in the CT Assessment of Honeycombing in the Lungs"
      ],
      "abstract":[
        "To quantify observer agreement and analyze causes of disagreement in identifying honeycombing at chest computed tomography (CT).The institutional review board approved this multiinstitutional HIPAA-compliant retrospective study, informed patient consent was not required. Five core study members scored 80 CT images with a five-point scale (5 = definitely yes to 1 no) establish reference standard for the identification honeycombing. Forty-three observers from various subspecialties geographic regions by using same scoring system. Weighted \u03ba values scores compared were analyzed investigate intergroup differences. Images divided into four groups allow analysis imaging features cases which there disagreement: on presence honeycombing, absence other (none preceding three applied).Agreement 43 moderate (Cohen weighted values: 0.40-0.58). There no significant differences among defined either subspecialty or region (Tukey-Kramer test, P .38 >.99). In 29% cases, These included mixed traction bronchiectasis, large cysts, superimposed pulmonary emphysema.Identification is subjective, largely caused conditions that mimic"
      ],
      "categories":[
        "Radiology"
      ]
    },
    "list":{
      "title":[
        "Quantum Spin Correlation Amplification Enables Macroscopic Detection of\n  Atomic-Level Fatigue in Ferromagnetic Metals",
        "Electron-phonon coupling and phonon dynamics in single-layer NbSe$_2$ on\n  graphene: the role of moir\\'e phonons",
        "Stability of N-front and N-back solutions in the Barkley model",
        "Optimizing the Critical Temperature and Superfluid Density of a\n  Metal-Superconductor Bilayer",
        "Medium-band Astrophysics with the Grism of NIRCam In Frontier fields\n  (MAGNIF): Spectroscopic Census of H$\\alpha$ Luminosity Functions and Cosmic\n  Star Formation at $z\\sim 4.5$ and 6.3",
        "Three-dimensional chiral active Ornstein-Uhlenbeck model for helical\n  motion of microorganisms",
        "Self-consistent solution to the semiclassical Einstein equations of a\n  star",
        "Star-crossed Clusters: Asteroseismic Ages for Individual Stars are in\n  Tension with the Ages of their Host Clusters",
        "Constraints on 1-0 texture through neutrino phenomenology and dark\n  matter in minimal inverse seesaw",
        "HyperNOs: Automated and Parallel Library for Neural Operators Research",
        "The MACIV multiscale seismic experiments in the French Massif Central\n  (2023-2027): deployment, data quality and availability",
        "On the convergence rate of noisy Bayesian Optimization with Expected\n  Improvement",
        "Microscopic description of quadrupole-hexadecapole coupling in radium,\n  thorium, uranium and plutonium isotopes with the Gogny energy density\n  functional",
        "Simulating Hawking radiation in quantum many-body systems: deviations\n  from the thermal spectrum",
        "Dynamics of DNA-Templated Ultrafine Silver Nanowires Formation",
        "Proceedings of the 14th International Computational Accelerator Physics\n  Conference (ICAP24)",
        "Quantization of the Momentum Map via $\\frak{g}$-adapted Formalities",
        "Annealed mean-field epidemiological model on scale-free networks with a\n  mitigating factor",
        "Anisotropic flows of identified hadrons in the equal-velocity quark\n  combination model at RHIC energy",
        "On refined enumerations of plane partitions of a given shape with\n  bounded entries",
        "Stable antiferroelectric phase in calcium-doped lead scandium tantalate",
        "Symmetric channel verification for purifying noisy quantum channels",
        "Measuring the degree of clustering and diffusion of trans-Neptunian\n  objects",
        "Do Short GRBs Exhibit an Anticorrelation between Their Intrinsic\n  Duration and Redshift?",
        "Asymmetry analysis of Autler-Townes doublet in the trap-loss\n  fluorescence spectroscopy of cesium MOT with single step Rydberg excitation",
        "Cylindrically confined $H$ atom in magnetic field: variational cut-off\n  factor",
        "The Normal Play of the Domination Game",
        "Perturbing finite temperature multicomponent DFT 1D Kohn-Sham systems:\n  Peierls Gap & Kohn Anomaly",
        "Populations of Neutron Star Ultraluminous X-ray Sources: Mind your b's\n  and B's"
      ],
      "abstract":[
        "Structural fatigue failures account for most of catastrophic metal component\nfailures, annually causing thousands of accidents, tens of thousands of\ncasualties, and $100 billion in global economic losses. Current detection\nmethods struggle to identify early-stage fatigue damage characterized by\nsub-nanometer atomic displacements and localized bond rupture. Here we present\na quantum-enhanced monitoring framework leveraging the fundamental symbiosis\nbetween metallic bonding forces and magnetic interactions. Through magnetic\nexcitation of quantum spin correlation in metallic structures, we establish a\nmacroscopic quantum spin correlation amplification technology that visualizes\nfatigue-induced magnetic flux variations corresponding to bond strength\ndegradation. Our multi-scale analysis integrates fatigue life prediction with\nquantum mechanical parameters (bonding force constants, crystal orbital overlap\npopulation) and ferromagnetic element dynamics, achieving unprecedented\nprediction accuracy (R^2>0.9, p<0.0001). In comprehensive fatigue trials\nencompassing 193 ferromagnetic metal specimens across 3,700 testing hours, this\nquantum magnetic signature consistently provided macroscopic fracture warnings\nprior to failure - a critical advance enabling 100% early detection success.\nThis transformative framework establishes the first operational platform for\npreemptive fatigue mitigation in critical infrastructure, offering a paradigm\nshift from post-failure analysis to quantum-enabled predictive maintenance.",
        "The interplay between substrate interactions and electron-phonon coupling in\ntwo-dimensional (2D) materials presents a significant challenge in\nunderstanding and controlling their electronic properties. Here, we present a\ncomparative study of the structural characteristics, phonon dynamics, and\nelectron-phonon interactions in bulk and monolayer NbSe$_2$ on epitaxial\nbilayer graphene (BLG) using helium atom scattering (HAS). High-resolution\nhelium diffraction reveals a (9x9)0$^{\\circ}$ superstructure within the\nNbSe$_2$ monolayer, commensurate with the BLG lattice, while out-of-plane HAS\ndiffraction spectra indicate a low-corrugated\n(3$\\sqrt{3}$x3$\\sqrt{3}$)30$^{\\circ}$ substructure. By monitoring the thermal\nattenuation of the specular peak across a temperature range of 100 K to 300 K,\nwe determined the electron-phonon coupling constant $\\lambda_{HAS}$ as 0.76 for\nbulk 2H-NbSe$_2$. In contrast, the NbSe$_2$ monolayer on graphene exhibits a\nreduced $\\lambda_{HAS}$ of 0.55, corresponding to a superconducting critical\ntemperature (T$_C$) of 1.56 K according to the MacMillan formula, consistent\nwith transport measurement findings. Inelastic HAS data provide, besides a set\nof dispersion curves of acoustic and lower optical phonons, a soft,\ndispersionless branch of phonons at 1.7 meV, attributed to the interface\nlocalized defects distributed with the superstructure period, and thus termed\nmoir\\'e phonons. Our data show that moir\\'e phonons contribute significantly to\nthe electron-phonon coupling in monolayer NbSe$_2$. These results highlight the\ncrucial role of the BLG on the electron-phonon coupling in monolayer NbSe$_2$,\nattributed to enhanced charge transfer effects, providing valuable insights\ninto substrate-dependent electronic interactions in 2D superconductors.",
        "In this paper we establish for an intermediate Reynolds number domain the\nstability of N-front and N-back solutions for each N > 1 corresponding to\ntraveling waves, in an experimentally validated model for the transition to\nturbulence in pipe flow proposed in [Barkley et al., Nature 526(7574):550-553,\n2015]. We base our work on the existence analysis of a heteroclinic loop\nbetween a turbulent and a laminar equilibrium proved by Engel, Kuehn and de\nRijk in [Engel, Kuehn, de Rijk, Nonlinearity 35:5903, 2022], as well as some\nresults from this work. The stability proof follows the verification of a set\nof abstract stability hypotheses stated by Sandstede in [SIAM Journal on\nMathematical Analysis 29.1 (1998), pp. 183-207] for traveling waves motivated\nby the FitzHugh-Nagumo equations. In particular, this completes the first\ndetailed analysis of Engel, Kuehn and de Rijk in [Engel, Kuehn, de Rijk,\nNonlinearity 35:5903, 2022] leading to a complete existence and stability\nstatement that nicely fits within the abstract framework of waves generated by\ntwisted heteroclinic loops.",
        "A promising path to realizing higher superconducting transition temperatures\n$T_c$ is the strategic engineering of artificial heterostructures. For example,\nquantum materials exhibiting some but not all of the characteristics necessary\nfor a robust superconducting state could, in principle, be coupled with other\nmaterials in a way that alleviates their intrinsic shortcomings. In this work,\nwe add numerical support to the hypothesis that a strongly interacting\nsuperconductor weakened by phase fluctuations can boost its $T_c$ by\nhybridizing the system with a metal. Using determinant quantum Monte Carlo\n(DQMC), we simulate a two-dimensional bilayer composed of an attractive Hubbard\nmodel and a metallic layer in two regimes of the interaction strength $-|U|$.\nIn the strongly interacting regime, we find that increasing the interlayer\nhybridization $t_\\perp$ results in a nonmonotonic enhancement of $T_c$, with an\noptimal value comparable to the maximum $T_c$ observed in the single-layer\nattractive Hubbard model, confirming trends inferred from other approaches. In\nthe intermediate coupling regime, when $-|U|$ is close to the value associated\nwith the maximum $T_c$ of the single-layer model, increasing $t_\\perp$ tends to\ndecrease $T_c$, implying that the correlated layer was already optimally tuned.\nImportantly, we demonstrate that the mechanism behind these trends is related\nto enhancement in the superfluid stiffness, as was initially proposed by\nKivelson [Physica B: Condensed Matter 318, 61 (2002)].",
        "We measure H$\\alpha$ luminosity functions (LFs) at redshifts $z \\sim 4.5$ and\n6.3 using the JWST MAGNIF (Medium-band Astrophysics with the Grism of NIRCam In\nFrontier fields) survey. MAGNIF obtained NIRCam grism spectra with the F360M\nand F480M filters in four Frontier Fields. We identify 248 H$\\alpha$ emitters\nbased on the grism spectra and photometric redshifts from combined HST and JWST\nimaging data. The numbers of the H$\\alpha$ emitters show a large field-to-field\nvariation, highlighting the necessity of multiple fields to mitigate cosmic\nvariance. We calculate both observed and dust-corrected H$\\alpha$ LFs in the\ntwo redshift bins. Thanks to the gravitational lensing, the measured H$\\alpha$\nLFs span three orders of magnitude in luminosity, and the faint-end luminosity\nreaches $L_{\\mathrm{H}\\alpha} \\sim 10^{40.3} \\mathrm{erg} \\mathrm{s}^{-1}$ at\n$z \\sim 4.5$ and $10^{41.5} \\mathrm{erg} \\mathrm{s}^{-1}$ at $z \\sim 6.3$,\ncorresponding to star-formation rates (SFRs) of $\\sim$ 0.1 and 1.7\n$\\mathrm{M}_\\odot \\mathrm{yr}^{-1}$. We conclude no or weak redshift evolution\nof the faint-end slope of H$\\alpha$ LF across $z\\simeq0.4-6.3$, and the\ncomparison with the faint-end slopes of UV LF indicates stochastic star\nformation history among low-mass H$\\alpha$ emitters. The derived cosmic SFR\ndensities are $0.058^{+0.008}_{-0.006}\\ \\ M_\\odot\\ \\mathrm{yr}^{-1}\\\n\\mathrm{Mpc}^{-3}$ at $z \\sim 4.5$ and $0.025^{+0.009}_{-0.007}\\ \\ M_\\odot\\\n\\mathrm{yr}^{-1}\\ \\mathrm{Mpc}^{-3}$ at $z \\sim 6.3$. These are approximately\n2.2 times higher than previous estimates based on dust-corrected UV LFs, but\nconsistent with recent measurements from infrared surveys. We discuss\nuncertainties in the H$\\alpha$ LF measurements, including those propagate from\nthe lens models, cosmic variance, and AGN contribution.",
        "Active movement is essential for the survival of microorganisms like\nbacteria, algae and unicellular parasites, for example the ones causing the\ndisease malaria. In three dimensions, both swimming and gliding microorganisms\noften exhibit helical trajectories. Here we introduce a stochastic dynamics\nmodel for chiral self-propelled particles, for which both propulsion force and\ntorque are internally generated, the latter stochastically by an Ornstein\nUhlenbeck process. We demonstrate that a truncated version of the full model\ncan be solved analytically, in very good agreement with computer simulations\nfor the full model. Our main result is that this model allows for larger\nlong-time mean squared displacements for helical compared to straight 3D\nmovement at the same speed, suggesting an evolutionary benefit of the often\nobserved helical movements of microorganisms, and opposite to the reduction of\ndiffusion caused by chirality in 2D. We then provide an experimental example by\nanalyzing imaging data for malaria parasites that glide through hydrogels on\nhelical trajectories.",
        "We present the interior solution for a static, spherically symmetric perfect\nfluid star backreacted by QFT in four dimensions invoking no arbitrary\nparameters. It corresponds to a constant energy density star and is fully\nnon-perturbative. The space of solutions includes ultra-compact configurations\nthat have neither singularities nor light rings inside the star and can exist\narbitrarily close to the Schwarzschild limit, showing that the classical\nparadigm of astrophysics does not hold once QFT in curved space is taken into\naccount.",
        "A meta-analysis of seismic ages determined for individual stars in the\nwell-studied open and globular clusters NGC 6819, NGC 6791, M67, M4, M19, M80,\nand M9 reveals both high variance across measurements and significant\ndiscrepancy with independent, isochrone-based age determinations for the\nclusters in which these stars reside. The scatter among asteroseismic ages for\nindividual stars in any one of these clusters far surpasses both the absolute\nage uncertainty computed for reference cluster M92 (5.4\\%) and the\nmodel-to-model systematic uncertainties in isochrones (roughly 10\\%). This\nsuggests that either binary processes are significantly altering the masses of\nstars in these clusters, or some additional corrections, perhaps as a function\nof mass, metallicity, or surface gravity, are required to bring the\nasteroseismic age scale into concordance with ages inferred from isochrone or\nsimilar model fitting.",
        "In this work we have realized texture zero structures of neutrino mass matrix\nthrough our study of neutrino phenomenology and dark matter. For analysing\nthese processes, we have constructed a model in minimal inverse seesaw,\nISS(2,3) by using $A_4$ discrete symmetry. The particle content of ISS(2.3) has\nbeen augmented by a scalar triplet $\\eta=(\\eta_1,\\eta_2,\\eta_3)$. The probable\ndark matter candidates for this model are the neutral components of $\\eta$. The\nthree mass matices of ISS(2,3), $M_D$, $M_{NS}$ and $M_S$ contribute to the\nstructure of light neutrino mass matrix $m_\\nu$. Here we try to examine the\nimpact on texture structures of $m_\\nu$ due to different possible 2-0\nstructures of $M_D$. To examine further possible contraints, we have evaluated\nthe neutrino parameters and calculated relic density of dark matter for the\nfavourable cases. From our analysis we find that out of the fifteen possible\n2-0 structures, only two of them ($M_{D3}$ and $M_{D6}$) successfully generates\nall the mixing angles in the allowed ranges.",
        "This paper introduces HyperNOs, a PyTorch library designed to streamline and\nautomate the process of exploring neural operators, with a special focus on\nhyperparameter optimization for comprehensive and exhaustive exploration.\nIndeed, HyperNOs takes advantage of state-of-the-art optimization algorithms\nand parallel computing implemented in the Ray-tune library to efficiently\nexplore the hyperparameter space of neural operators. We also implement many\nuseful functionalities for studying neural operators with a user-friendly\ninterface, such as the possibility to train the model with a fixed number of\nparameters or to train the model with multiple datasets and different\nresolutions. We integrate Fourier neural operators and convolutional neural\noperators in our library, achieving state of the art results on many\nrepresentative benchmarks, demonstrating the capabilities of HyperNOs to handle\nreal datasets and modern architectures. The library is designed to be easy to\nuse with the provided model and datasets, but also to be easily extended to use\nnew datasets and custom neural operator architectures.",
        "In the framework of the MACIV project, a consortium of French laboratories\nhas deployed a temporary seismic network of 100 broadband stations in the\nFrench Massif Central (FMC) for 3-4 years (2023-2027). The project aims at\nimaging the crust and upper mantle of the FMC to better assess the sources of\nvolcanism, and the impacts of the Variscan inheritance or the Cenozoic rift\nsystem on volcanic systems. A large-scale array of 35 broadband stations covers\nthe entire FMC and complements the permanent networks to reach a homogeneous\ncoverage with ~35 km spacing. This network, with XP code, is the French\ncontribution to AdriaArray. The XP array is complemented with 3 quasi-linear\nnorth-south, east-west and northwest-southeast profiles with inter-station\nspacing of 5-20 km, making up the XF network of 65 stations. The profiles cross\nvolcanic areas and the main Variscan structures. We describe the experimental\nsetup designed to optimize the performance\/cost ratio and minimize the number\nof field visits, the deployment, the state-of-health monitoring, the data\nmanagement and the data quality control strategies, outcomes of our 15-years'\nexperience with major temporary seismic experiments in France and neighboring\ncountries, including AlpArray. We also show some preliminary results including\nhypocenter locations and receiver function analysis. The 2 broadband arrays\nwill be supplemented in 2025 by a month-long deployment of 3 large-N dense\narrays of 625 3-C short-period nodes. These dense arrays will complete our\nmulti-scale seismic experiment and illuminate active faults and possible\nplumbing systems of the youngest volcanoes.",
        "Expected improvement (EI) is one of the most widely used acquisition\nfunctions in Bayesian optimization (BO). Despite its proven success in\napplications for decades, important open questions remain on the theoretical\nconvergence behaviors and rates for EI. In this paper, we contribute to the\nconvergence theory of EI in three novel and critical areas. First, we consider\nobjective functions that fit under the Gaussian process (GP) prior assumption,\nwhereas existing works mostly focus on functions in the reproducing kernel\nHilbert space (RKHS). Second, we establish for the first time the asymptotic\nerror bound and its corresponding rate for GP-EI with noisy observations under\nthe GP prior assumption. Third, by investigating the exploration and\nexploitation properties of the non-convex EI function, we establish improved\nerror bounds of GP-EI for both the noise-free and noisy cases.",
        "The emergence and stability of static hexadecapole deformations as well as\nthe impact in the development of dynamic deformation due to collective motion\nconsidering quadrupole-hexadecapole coupling are studied for a selected set of\nradium, thorium, uranium and plutonium isotopes, using the Gogny\nHartree-Fock-Bogoliubov and Generator Coordinate Method frameworks. Sizable\nhexadecapole deformations are found to play a significant role in the ground\nand excited states of nuclei in the neighborhood of $^{238}$U. For each of the\nstudied isotopic chains, it is shown that a region with small negative\nhexadecapole deformation, just below the neutron magic number $N =184$, remains\nstable once zero-point quadrupole-hexadecapole fluctuations are taken into\naccount. A transition is predicted, with increasing mass number, from a regime\nin which the quadrupole and hexadecapole degrees of freedom are interwoven to a\nregime in which they are decoupled, accompanied by an enhanced shape\ncoexistence in the more neutron-rich sectors of the isotopic chains. It is also\nshown, that quadrupole-hexadecapole configuration mixing brings a nontrivial\nadditional correlation energy gain comparable to the quadrupole correlation\nenergy itself.",
        "We investigate a recently proposed one-to-one correspondence between quantum\nfield theories in two-dimensional curved spacetime and quantum many-body\nsystems, which enables the simulation of Hawking radiation in static background\nspacetimes. In particular, we demonstrate that deviations from the thermal\nspectrum, as predicted by the well-known tunneling method, can be observed in\nmany-body simulations.",
        "Recent research on silver nanowires prepared on DNA templates has focused on\ntwo fundamental applications: nano-scale circuits and sensors. Despite its\nbroad potential, the formation kinetics of DNA-templated silver nanowires\nremains unclear. Here, we present an experimental demonstration of the\nformation of silver nanowires with a diameter of 2.2+0.4 nm at the\nsingle-molecule level through chemical reduction. We conducted equilibrium and\nperturbation kinetic experiments to measure force spectroscopy during the\nformation of Ag+ -DNA complexes and Ag-DNA complexes, using optical tweezers\ncombined with microfluidics. The addition of AgNO3 resulted in an increase in\nforce of 5.5-7.5 pN within 2 minutes, indicating that Ag+ compacts the DNA\nstructure. In contrast, the addition of hydroquinone caused the force to\ndecrease by 4-5 pN. Morphological characterization confirmed the presence of a\ndense structure formed by silver atoms bridging the DNA strands, and revealed\nconformational differences before and after metallization. We compare our\nexperimental data with Brownian dynamics simulations using a coarse-grained\ndouble-stranded DNA (dsDNA) model that provides insights on the dependency of\nthe force on the persistence length.",
        "This is the proceedings of the 14th International Computational Accelerator\nPhysics Conference, ICAP'24, which was held at the Lufthansa Seeheim Conference\nHotel in Germany from October 2-5, 2024, hosted by TU Darmstadt and GSI\nHelmholtzzentrum f\\\"ur Schwerionenforschung. ICAP'24 has focused on advances in\nComputational Accelerator Physics and their application to existing machines\nand future facilities. It has provided a forum for researchers in modeling and\nsimulation to exchange information and discuss new ideas that benefit a wide\narea of accelerator science and technology. Topics of the conference have\nincluded computational needs and challenges, beam dynamics and electromagnetic\nfield calculations, code development and validation, data processing and\nvisualization, high performance computing, machine learning and advanced\noptimization as well as emerging technologies that will impact computing for\naccelerator design.",
        "In this note, we provide a proof of the existence and complete classification\nof $G$-invariant star products with quantum momentum maps on Poisson manifolds\nby means of an equivariant version of the formality theorem.",
        "An annealed version of the quenched mean-field model for epidemic spread is\nintroduced and investigated analytically and assisted by numerical\ncalculations. The interaction between individuals follows a prescription that\nis used to generate a scale-free network, and we have adjusted the number of\nconnections to produce a sparse network. Specifically, the model's behavior\nnear the infection threshold is examined, as well as the behavior of the\nstationary prevalence and the probability that a connection between individuals\nencounters an infected one. We found that these functions display a\nmonotonically increasing dependence on the infection rate. Subsequently, a\nmodification that mimics the mitigation in the probability of encountering an\ninfected individual is introduced, following an old idea rooted in the\nMalthus-Verhulst model. We found that this modification drastically changes the\nprobability that a connection meets an infected individual. However, despite\nthis change, it does not alter the monotonically increasing behavior of the\nstationary prevalence.",
        "We employ an equal-velocity quark combination model to study anisotropic\nflows $v_{2}$, $v_{3}$ and $v_{4}$ of identified hadrons at mid-rapidity in\nheavy-ion collisions at RHIC energies. Under the equal-velocity combination\nmechanism of constituent quarks at hadronization, we build analytical formulas\nof anisotropic flows of hadrons in terms of those of quarks just before\nhadronization. We systematically analyze the contribution of higher order flows\nof quarks, and show how simple formulas of $v_{2}$, $v_{3}$ and $v_{4}$ of\nidentified hadrons with the desired precision can be obtained by neglecting the\nsmall contribution of higher order flows of quarks. We systematically test\nthese simple formulas of hadronic flows by the experimental data of $v_{2}$,\n$v_{3}$ and $v_{4}$ of identified hadrons $\\phi$, $\\Lambda$, $\\Xi^{-}$,\n$\\Omega^{-}$, $\\bar{\\Lambda}$, $\\bar{\\Xi}^{+}$, $\\bar{\\Omega}^{+}$, $p$ and\n$\\bar{p}$ in Au+Au collisions at $\\sqrt{s_{NN}}=$ 19.6, 54.4 and 200 GeV, and\nwe find that the equal-velocity quark combination model can well describe the\nmeasured $v_{2}$, $v_{3}$ and $v_{4}$ of identified hadrons in Au+Au collisions\nat those collision energies. We further study the obtained anisotropic flows of\nquarks and find two scaling properties\\textcolor{red}{{} }which can be\nqualitatively understood by the hydrodynamic evolution of thermal quark medium\nproduced in relativistic heavy-ion collisions.",
        "In this paper, we consider plane partitions $\\text{PP}(\\lambda; m)$ of a\ngiven shape $\\lambda$, with entries at most $m$. We prove that the\ndistributions of two statistics on $\\text{PP}(\\lambda; m)$ coincide: one is the\nnumber of rows containing $0$ and the other is the number of rows containing\n$m$. We also provide a bijective proof.",
        "Antiferroelectrics are valuable dielectric materials, offering promise for\nboth high energy storage and solid-state caloric cooling applications. However,\nfew antiferroelectrics are known or available. Therefore, it is crucial to\ndiscover or design materials showing antiferroelectric behaviour. In this\nstudy, we fabricated highly ordered lead scandium tantalate ceramics doped with\ncalcium. From calorimetry and polarization-electric field loops, we demonstrate\nthe effect of calcium on the thermal properties and phase transition sequences\nof lead scandium tantalate. We identify an antiferroelectric phase appearing at\ntemperatures intermediate between the ferroelectric and paraelectric phases,\nwhich becomes increasingly stable as the calcium concentration increases. These\nfindings are supported by density functional theory calculations and Raman\nspectroscopy. Finally, we propose a phase diagram for calcium-doped lead\nscandium tantalate. Our results highlight the potential of stabilizing\nantiferroelectricity in ferroelectric perovskite materials through A-site\ndoping.",
        "Symmetry inherent in quantum states has been widely used to reduce the effect\nof noise in quantum error correction and a quantum error mitigation technique\nknown as symmetry verification. However, these symmetry-based techniques\nexploit symmetry in quantum states rather than quantum channels, limiting their\napplication to cases where the entire circuit shares the same symmetry. In this\nwork, we propose symmetric channel verification (SCV), a channel purification\nprotocol that leverages the symmetry inherent in quantum channels. By\nintroducing different phases to each symmetric subspace and employing a quantum\nphase estimation-like circuit, SCV can detect and correct symmetry-breaking\nnoise in quantum channels. We further propose a hardware-efficient\nimplementation of SCV at the virtual level, which requires only a single-qubit\nancilla and is robust against the noise in the ancilla qubit. Our protocol is\napplied to various Hamiltonian simulation circuits and phase estimation\ncircuits, resulting in a significant reduction of errors. Furthermore, in\nsetups where only Clifford unitaries can be used for noise purification, which\nis relevant in the early fault-tolerant regime, we show that SCV under Pauli\nsymmetry represents the optimal purification method.",
        "The outer solar system is populated by a broad aggregate of minor bodies,\nwhich occupy orbits whose dynamical character ranges from long-term stable to\nrapidly diffusive. We investigate the chaotic properties of known distant\ntrans-Neptunian objects (TNOs) by numerically integrating TNO clones and\nstatistically analyzing their orbital diffusion. Comparing the measured\ndiffusion with an analytical criterion yields a dynamically motivated\nseparation into classes of stable, metastable and unstable objects. We then\nmeasure the level of clustering of the longitudes of perihelia and of the\norbital poles, as functions of orbital distance and of their stability\nproperties. Distant (meta)stable objects appear increasingly clustered in\nperihelion around $\\varpi \\sim 50^\\circ$ for increasing semi-major axis, while\nthe orbits of unstable objects are well described by two, roughly\nequally-populated groups of \"clustered\" and \"anti-clustered\" objects, with\nmeans around $\\sim 25^\\circ$ and $\\sim 205^\\circ$ respectively. We further find\nthat, compared to the solar system's total angular momentum vector, the mean\norbital poles of distant TNOs are significantly more misaligned for\n(meta)stable objects, while they remain roughly aligned for unstable objects.\nTNOs with intermediate orbital periods also appear to be misaligned with\nrespect to the forced plane predicted by secular theory with the known planets.\nThis gradation based on stability, if validated further by the upcoming VRO\nsurvey, necessitates a dynamical explanation.",
        "Gamma-ray bursts (GRBs) are violent stellar explosions that are traditionally\ndivided into two groups: short bursts (SGRBs) with an observed duration T90 < 2\ns, and long bursts (LGRBs) with an observed duration T90 > 2 s, where T90\nrefers to the time needed for 90% of the fluence to be detected. Studies of\nprogenitor models suggest that LGRBs emanate from the core collapse of massive\nstars, while SGRBs result from the merging of two compact objects, like two\nneutron stars or a neutron star and a black hole. Recent studies have found\nevidence that there is an anticorrelation between the intrinsic duration and\nthe redshift of long GRBs. In this study, we first check whether LGRBs exhibit\nan anticorrelation between their intrinsic duration and redshift using an\nexpanded dataset of long bursts that we have compiled. Next, we investigate\nwhether this anticorrelation applies to SGRBs as well using a sample of short\nGRBs that we have compiled. Our analysis confirms the results obtained by\nprevious studies regarding the anticorrelation for LGRBs. On the other hand,\nour results indicate that short GRBs do not exhibit such an anticorrelation. We\ndiscuss the implications of our results in the context of how metallicity\nevolves with redshift and the role that it might play in the aforementioned\nanticorrelation.",
        "Autler-Townes (AT) doublet, a fundamental manifestation of quantum\ninterference effects, serves as a critical tool for studying the dynamic\nbehavior of Rydberg atoms. Here, we investigate the asymmetry of the\nAutler-Townes (AT) doublet in trap-loss fluorescence spectroscopy (TLFS) of\ncesium (Cs) atoms confined in a magneto-optical trap (MOT) with single-step\nRydberg excitation using a 319-nm ultraviolet (UV) laser. A V-type three-level\nsystem involving the ground state $6\\text{S}_{1\/2}$ ($\\text{F}$=4), excited\nstate $6\\text{P}_{3\/2}$ ($\\text{F}^{'}$=5) , and Rydberg state\n($n\\text{P}_{3\/2}$ ($\\text{m}_\\text{J}$=+3\/2)) is theoretically modeled to\nanalyze the nonlinear dependence of the AT doublet's asymmetry and interval on\nthe cooling laser detuning. Experiments reveal that as the cooling laser\ndetuning $\\Delta_1$ decreases from $-$15 MHz to $-$10 MHz, the AT doublet\nexhibits increasing symmetry, while its interval shows a nonlinear decrease.\nTheoretical simulations based on the density matrix equation and Lindblad\nmaster equation align closely with experimental data, confirming the model's\nvalidity. This study provides insights into quantum interference dynamics in\nmulti-level systems and offers a systematic approach for optimizing precision\nmeasurements in cold atom spectroscopy.",
        "In the present study, we consider the hydrogen atom confined within an\nimpenetrable infinite cylindrical cavity of radius $\\rho_{0}$ in the presence\nof a constant magnetic field ${\\bf B} = B\\,\\hat{\\bf z}$ oriented along the main\ncylinder's axis. In the Born-Oppenheimer approximation, anchoring the nucleus\nto the geometric center of the cylinder, a physically meaningful 3-parametric\ntrial function is used to determine the ground state energy $E$ of the system.\nThis trial function incorporates the exact symmetries and key limiting\nbehaviors of the problem explicitly. In particular, it does not treat the\nCoulomb potential nor the magnetic interaction as a \\textit{perturbation}. The\nnovel inclusion of a variational cut-off factor $\\big(1 -\n\\big(\\frac{\\rho}{\\rho_0}\\big)^\\nu\\big)$, $\\nu \\geq 1$, appears to represent a\nsignificant improvement compared to the non-variational cut off factors\ncommonly employed in the literature. The dependence of the total energy\n$E=E(\\rho_0,\\,B)$ and the binding energy $E_b=E_b(\\rho_0,\\,B)$ on the cavity\nradius $\\rho_0 \\in [0.8,\\,5] \\,$a.u. and the magnetic field strength $B\\in\n[0.0,\\,1.0]\\,$a.u. is presented in detail. The expectation values $\\langle \\rho\n\\rangle$ and $\\langle|z| \\rangle$, and the Shannon entropy in position space\nare computed to provide additional insights into the system's localization. A\nbrief discussion is provided comparing the 2D and 3D cases as well.",
        "In 2010, Bre\\v{s}ar, Klav\\v{z}ar and Rall introduced the optimization variant\nof the graph domination game and the game domination number. In 2024, Leo\nVersteegen obtained the celebrated proof of the Conjecture $\\frac{3}{5}$ on\nthis variant of the domination game, proposed by Kinnersley, West and Zamani in\n2013. In this paper, we investigate for the first time the normal play of the\ndomination game, which we call \\textsc{Normal Domination Game}, that is an\nimpartial game where the last to play wins. We use the Sprague-Grundy theory to\nprove that Alice (the first player) wins in the path $P_n$ if and only if $n$\nis not a multiple of $4$, and wins in the cycle $C_n$ if and only if $n=4k+3$\nfor some integer $k$. Finally, we obtain a polynomial time algorithm to decide\nthe winner for any disjoint union of paths and cycles in the \\textsc{Normal\nDomination Game} and its natural partizan variant.",
        "One of the greatest challenges when designing new technologies that make use\nof non-trivial quantum materials is the difficulty associated with predicting\nmaterial-specific properties, such as critical temperature, gap parameter, etc.\nThere is naturally a great amount of interest in these types of condensed\nmatter systems because of their application to quantum sensing, quantum\nelectronics, and quantum computation; however, they are exceedingly difficult\nto address from first principles because of the famous many-body problem. For\nthis reason, a full electron-nuclear quantum calculation will likely remain\ncompletely out of reach for the foreseeable future. A practical alternative is\nprovided by finite temperature, multi component density functional theory\n(MCDFT), which is a formally exact method of computing the equilibrium state\nenergy of a many-body quantum system. In this work, we use this construction\nalongside a perturbative scheme to demonstrate that the phenomena Peierls\neffect and Kohn Anomaly are both natural features of the KS equations without\nadditional structure needed. We find the temperature dependent ionic density\nfor a simple 1D lattice which is then used to derive the ionic densities\ntemperature dependent affect on the electronic band structure. This is\naccomplished by Fourier transforming the ionic density term found within this\nKS electronic equation. Using the Peierls effect phonon distortion gap openings\nin relation to the Fermi level, we then perturb the KS ionic equation with a\nconduction electron density, deriving the Kohn Anomaly. This provides a\nworkable predictive strategy for interesting electro-phonon related material\nproperties which could be extended to 2D and 3D real materials while retaining\nthe otherwise complicated temperature dependence.",
        "Ultraluminous X-ray sources (ULXs) with neutron star (NS) accretors challenge\ntraditional accretion models, and have sparked a debate regarding the role of\ngeometrical beaming and strong magnetic fields (B). The reduction of the\nThomson cross-section in the presence of strong B, leads to a modification of\nthe Eddington limit, and therefore is expected to affect significantly the\nobservational appearance of NS-ULXs. We investigate the role of this\nmodification using population synthesis models, and explore its effects on the\nX-ray luminosity functions, spin-up rates, and outflow energetics of the\nobserved NS-ULXs. Our results show that the new prescription allows NS-ULXs to\nachieve super-Eddington luminosities with milder beaming compared to before,\nimproving the agreement with observations. In addition, it broadens the range\nof spin-up rates allowing for more diverse conditions in NS-ULXs in terms of\naccretion rates and magnetic fields. More importantly, the reduced beaming\nincreases the likelihood of observing the NS-ULXs within wind-powered nebulae\nsuch as NGC 5907 ULX-1. Our findings highlight the necessity of taking into\naccount B effects independently of the approach: geometrical beaming or strong\nB, and call for magnetospheric accretion prescriptions that can be integrated\nin population synthesis codes."
      ]
    }
  },
  {
    "id":2411.09469,
    "research_type":"applied",
    "start_id":"b36",
    "start_title":"CBAM: Convolutional block attention module",
    "start_abstract":"We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "The accuracy of colposcopic biopsy: Analyses from the placebo arm of the Gardasil clinical trials"
      ],
      "abstract":[
        "We evaluated the overall agreement between colposcopically directed biopsies and definitive excisional specimens within context of three clinical trials. A total 737 women aged 16-45 who had a cervical biopsy taken 6 months before their therapy were included. Per-protocol, colposcopists to also obtain representative immediately therapy. Using adjudicated histological diagnoses, initial same day correlated with surgically excised specimens. The therapy, diagnoses was 42% (weighted kappa = 0.34) (95% CI: 0.29-0.39). underestimation intraepithelial neoplasia grade 2\/3 or adenocarcinoma in situ (CIN2-3\/AIS) CIN3\/AIS 26 42%, respectively. When allowing for one degree variance correlation, 92% CIN2-3\/AIS. specimen 56% 0.41) 0.36-0.47), CIN2-3\/AIS 57%. There significant associations when patients stratified by age, number biopsies, lesion size, presence human papillomavirus (HPV)16\/18 region. Of 178 diagnostic endocervical curettages performed, 14 (7.9%) found any HPV disease. Colposcopic accuracy improved CIN2 grouped as single predictive measure high-grade Colposcopy functioned well allowed one-degree difference surgical histologic interpretations, done practice. Taking more than colposcopic could improve patient management."
      ],
      "categories":[
        "Clinical Trial"
      ]
    },
    "list":{
      "title":[
        "Beamfocusing and Power Allocation for AN-Based PLS in Multiuser XL-MIMO\n  with Multiple Eavesdroppers",
        "Understanding the core limitations of second-order correlation-based\n  functionals through: functional, orbital, and eigenvalue-driven analysis",
        "Two-quanta processes in coupled double-quantum-dot cavity systems",
        "Efficient cavity-mediated energy transfer between photosynthetic light\n  harvesting complexes from strong to weak coupling regime",
        "More on unconstrained descriptions of Higher Spin Massless Particles",
        "Freeze-and-release direct optimization method for variational\n  calculations of excited electronic states",
        "Unique existence of solution and Hyers-Ulam stability for a new\n  fractional differential quasi-variational inequality with Mittag-Leffler\n  kernel and its applications",
        "Compare Similarities Between DNA Sequences Using Permutation-Invariant\n  Quantum Kernel",
        "Advanced muon-spin spectroscopy with high lateral resolution using\n  Si-pixel detectors",
        "Cosmic Rays Masquerading as Hot CGM Gas: An Inverse-Compton Origin for\n  Diffuse X-ray Emission in the Circumgalactic Medium",
        "String islands, discrete theta angles and the 6D $\\mathcal{N} = (1,1)$\n  string landscape",
        "Highly reflective white clouds on the western dayside of an exo-Neptune",
        "Instability of Baryonic Black Branes",
        "Directional polynomial wavelets on spheres",
        "Halbach 2.0 -- Creating homogeneous fields with finite size magnets",
        "Preserving Simultaneity and Chronology for Sensing in Wireless\n  Perceptive Networks",
        "Large Anomalous Hall Effect in a Noncoplanar Magnetic Heterostructure",
        "Suppression of coherent errors during entangling operations in NV\n  centers in diamond",
        "Cosmological constraints from the Minkowski functionals of the BOSS\n  CMASS galaxy sample",
        "Finite symmetric algebras in tensor categories and Verlinde categories\n  of algebraic groups",
        "High-frequency readout free from transmon multi-excitation resonances",
        "A Bivariate Poisson-Gamma Distribution: Statistical Properties and\n  Practical Applications",
        "Mechanical resonant sensing of spin texture dynamics in a\n  two-dimensional antiferromagnet",
        "Kinematic power corrections to DVCS to twist-six accuracy",
        "The two extremal rays of some Hyper-K\\\"ahler fourfolds",
        "Construction of exact refinements for the two-dimensional HB\/THB-spline\n  de Rham complex",
        "Radiation of a particle performing helical motion in a multilayer\n  cylindrical waveguide",
        "Verification of the PICLS electromagnetic upgrade in mixed variables",
        "Si-compatible topological and infrared materials: the promise of Low-Sn\n  GeSn digital alloys"
      ],
      "abstract":[
        "This paper investigates the downlink (DL) physical layer security (PLS) in a\nnear-field (NF) extra-large multiple-input multiple-output MIMO (XL-MIMO)\nsystem. To enhance the secrecy rate (SR), null-space artificial noise (AN) is\ntransmitted alongside the confidential message, ensuring orthogonality with\nlegitimate user equipment (LUE) channels. The objective is to maximize the\nminimum SR by optimizing the NF beamfocusing matrix and power allocation\nbetween the signal and AN, considering various channel state information (CSI)\nconditions and transmit power constraints. The proposed approach uses\nsuccessive convex approximation (SCA) for beamfocusing optimization and golden\nsection search (GSS) for power allocation. The following open questions are\naddressed: (i) Can AN transmission further enhance SR for multiple LUEs in the\npresence of multiple eavesdropping user equipment (EUEs)? (ii) Can null-space\nAN transmission achieve attractive SR performance even without CSI availability\nfor EUEs? Both questions are affirmatively answered and explored in detail,\nwith an algorithm presented for joint beamfocusing design and AN-aided power\nallocation. The proposed method outperforms state-of-the-art approaches that\neither omit AN transmission or rely on maximal-ratio transmission (MRT) for\nbeamfocusing.",
        "Density Functional Theory has long struggled to obtain the exact\nexchange-correlational (XC) functional. Numerous approximations have been\ndesigned with the hope of achieving chemical accuracy. However, designing a\nfunctional involves numerous methodologies, which has a greater possibility for\nerror accumulation if the functionals are poorly formulated. This study aims to\ninvestigate the performance and limitations of second-order correlation\nfunctionals within the framework of density functional theory. Specifically, we\nfocus on three major classes of density functional approximations that\nincorporate second-order energy expressions: \\textit{ab initio} (primarily\nG\\\"{o}rling-Levy) functionals, adiabatic connection models, and double-hybrid\nfunctionals. The principal objectives of this research are to evaluate the\naccuracy of second-order correlation functionals, to understand how the choice\nof reference orbitals and eigenvalues affects the performance of these\nfunctionals, to identify the intrinsic limitations of second-order energy\nexpressions, especially when using arbitrary orbitals or non-canonical\nconfigurations, and propose strategies for improving their accuracy. By\naddressing these questions, we aim to provide deeper insights into the factors\ngoverning the accuracy of second-order correlation functionals, thereby guiding\nfuture functional development.",
        "The quantum dynamics of a compound sample consisting from a semiconductor\ndouble quantum dot (DQD) system non-linearly coupled with a leaking single-mode\nmicro-resonator is theoretically investigated. The focus is on the resonance\ncondition when the transition frequency of the double quantum dot equals to the\ndoubled resonator frequency, respectively, and the resulting interplay among\nthe involved phonon or photon channels. As a result, the steady-state quantum\ndynamics of this complex non-linear system exhibits a variety of possible\neffects that have been demonstrated here. Particularly, we have found the\nrelationship among the electrical current through the double quantum dot and\nthe microwave field inside the resonator that is nonlinearly coupled to it,\nwith a corresponding emphasizing on their critical behaviors. Additionally, the\nquantum correlations of the photon flux generated into the resonator mode vary\nfrom super-Poissonian to Poissonian photon statistics, leading to single-qubit\nlasing phenomena at microwave frequencies.",
        "Excitation energy transfer between photosynthetic light-harvesting complexes\nis vital for highly efficient primary photosynthesis. Controlling this process\nis the key for advancing the emerging artificial photosynthetic systems. Here,\nwe experimentally demonstrate the enhanced excitation energy transfer between\nphotosynthetic light-harvesting 2 complexes (LH2) mediated through the\nFabry-Perot optical microcavity. Using intensity-dependent pump-probe\nspectroscopy, we analyse the exciton-exciton annihilation (EEA) due to\ninter-LH2 energy transfer. Comparing EEA in LH2 within cavity samples and the\nbare LH2 films, we observe enhanced EEA in cavities indicating improved\nexcitation energy transfer via coupling to a common cavity mode. Surprisingly,\nthe effect remains even in the weak coupling regime. The enhancement is\nattributed to the additional connectivity between LH2s introduced by the\nresonant optical microcavity. Our results suggest that optical microcavities\ncan be a strategic tool for modifying excitation energy transfer between\nmolecular complexes, offering a promising approach towards efficient artificial\nlight harvesting.",
        "Here we suggest a new local action describing arbitrary integer spin-$s$\nmassless particles in terms of only two symmetric fields $\\varphi $ and\n$\\alpha$ of rank-$s$ and $(s-3)$ respectively. It is an unconstrained version\nof the Fronsdal theory where the double traceless constraint on the physical\nfield is evaded via a rank-$(s-4)$ Weyl like symmetry. The constrained higher\nspin diffeomorphism is enlarged to full diffeomorphism via the Stueckelberg\nfield $\\alpha$ through an appropriate field redefinition. After a partial gauge\nfixing where the Weyl symmetry is broken while preserving diffeomorphisms, the\nfield equations reproduce, for arbitrary integer spin-$s$, diffeomorphism\ninvariant equations of motion previously obtained via a truncation of the\nspectrum of the open bosonic string field theory in the tensionless limit. In\nthe $s=4$ case we show that the functional integration over $\\alpha$ leads to a\nunique non local Weyl and diffeomorphism invariant action given only in terms\nof the physical field $\\varphi$ whose spectrum is confirmed via an analysis of\nthe analytic structure of the spin-4 propagator for which we introduce a\ncomplete basis of projection and transition non local differential operators.\nWe also show that the elimination of $\\alpha$ after the Weyl gauge fixing leads\nto a non local diffeomorphism invariant action previously obtained in the\nliterature.",
        "Time-independent, orbital-optimized density functional approaches outperform\ntime-dependent density functional theory (TDDFT) in calculations of excited\nelectronic states involving a large rearrangement of the electron density, such\nas charge transfer excitations. However, optimizing orbitals for excited states\nremains challenging, as the latter typically correspond to saddle points on the\nelectronic energy surface. A simple and robust strategy for variational orbital\noptimization of excited states is presented. The approach involves two steps:\n(1) a constrained energy minimization, where a subset of orbitals changed by\nthe excitation are frozen, followed by (2) a fully unconstrained saddle point\noptimization. The constrained minimization step makes it possible to identify\nthe electronic degrees of freedom along which the energy needs to be maximized,\npreventing variational collapse. Both steps of this freeze-and-release strategy\nare carried out using direct optimization algorithms with a computational\nscaling comparable to ground state calculations. Numerical tests using a\nsemilocal functional are performed on intramolecular charge transfer states of\norganic molecules and intermolecular charge transfer states of molecular\ndimers. It is shown that the freeze-and-release direct optimization (FR-DO)\napproach can successfully converge challenging charge transfer states,\novercoming limitations of conventional algorithms based on the maximum overlap\nmethod, which either collapse to lower energy, charge-delocalized solutions or\nfail to converge. While FR-DO requires more iterations on average, the overall\nincrease in computational cost is small. For the NH3-F2 dimer, it is found that\nunlike TDDFT, orbital-optimized calculations reproduce the correct long-range\ndependency of the energy with respect to the donor-acceptor separation without\nthe need to include exact exchange in the long range.",
        "This paper considers a new fractional differential quasi-variational\ninequality with Mittag-Leffler kernel comprising a fractional differential\nequation with Mittag-Leffler kernel and a quasi-variational inequality in\nHilbert spaces. Some properties of the solution for the parameterized\nquasi-variational inequality are investigated, which improve the known results.\nMoreover, the unique existence of the solution and Hyers-Ulam stability are\nobtained for such a novel system under mild conditions. Finally, the obtained\nabstract results are applied to analyze the unique solvability and stability\nfor a multi-agent optimization problem and a price control problem.",
        "Computing the similarity between two DNA sequences is of vital importance in\nbioscience. However, traditional computational methods can be\nresource-intensive due to the enormous sequence length encountered in practice.\nRecently, applied quantum algorithms have been anticipated to provide potential\nadvantages over classical approaches. In this paper, we propose a\npermutation-invariant variational quantum kernel method specifically designed\nfor DNA comparison. To represent the four nucleotide bases in DNA sequences\nwith quantum states, we introduce a novel, theoretically motivated encoding\nscheme: the four distinct bases are encoded using the states of symmetric,\ninformationally complete, positive operator-valued measures (SIC-POVMs). This\nencoding ensures mutual equality: each pair of symbols is equidistant on the\nBloch sphere. Also, since permutation invariance is inherent to common DNA\nsimilarity measures such as Levenshtein distance, we realize it by using a\nspecially designed parameterized quantum layer. We show that our novel encoding\nmethod and parameterized layers used in the quantum kernel model can\neffectively capture the symmetric characteristics of the pairwise DNA sequence\ncomparison task. We validate our model through numerical experiments, which\nyield promising results on length-$8$ DNA sequences.",
        "Muon-spin spectroscopy at continuous sources has stagnated at a stopped muons\nrate of ~40 kHz for the last few decades. The major limiting factor is the\nrequirement of a single muon in the sample during the typical 10 {\\mu}s data\ngate window. To overcome this limit and to be able to perform muon-spin\nrelaxation ({\\mu}SR) measurements on millimeter-sized samples, one can use\nvertex reconstruction methods to construct {\\mu}SR spectra. This is now\npossible thanks to the availability of very thin monolithic Si-pixel chips,\nwhich offer minimal particle scattering and high count rate. Here we present\nresults from a Si-pixel based spectrometer that utilizes vertex reconstruction\nschemes for the incoming muons and emitted positrons. With this spectrometer we\nwere able to obtain a first vertex reconstructed {\\mu}SR (VR-{\\mu}SR) spectrum.\nThe unique capabilities and benefits of such a spectrometer are discussed.",
        "Observations have argued that Milky Way (MW), Andromeda, and lower-mass\ngalaxies exhibit extended soft X-ray diffuse halos to radii $R\\gtrsim100\\,$kpc\nin the circumgalactic medium (CGM). If interpreted as thermal emission, the\nshallow surface brightness profiles $S_{X}\\propto R^{-1}$ are difficult to\nexplain and contradict other observations. We show that such halos instead\narise from inverse Compton (IC) scattering of CMB photons with GeV cosmic ray\n(CR) electrons. GeV electrons have ~Gyr lifetimes and escape the galaxy,\nforming a shallow extended profile out to $\\gtrsim100\\,$kpc, where IC off the\nCMB should produce soft, thermal-like X-ray spectra peaked at ~keV. The\nobserved keV halo luminosities and brightness profiles agree well with those\nexpected for CRs observed in the local interstellar medium (LISM) escaping the\ngalaxy, with energetics consistent with known CRs from SNe and\/or AGN, around\ngalaxies with stellar masses $M_{\\ast}\\lesssim2\\times 10^{11}\\,M_{\\odot}$. At\nhigher masses observed X-ray luminosities are larger than predicted from IC and\nshould be dominated by hot gas. In the MW+M31, the same models of escaping CRs\nreproduce gamma-ray observations if we assume an LISM-like proton-to-electron\nratio and CR-pressure-dominated halo. In all other halos, the radio and\n$\\gamma$-ray brightness is below detectable limits. If true, the observations\nprovide qualitatively new constraints on CGM and CR physics: X-ray brightness\ndirectly traces the CR lepton energy density in the CGM. This agrees with LISM\nvalues within 10 kpc, which following the profile expected for escaping CRs in\nthe CGM. The inferred CR pressure is a major part of the MW CGM pressure\nbudget. X-ray surface brightness and luminosity allows one to further determine\nthe CGM diffusivity at radii $\\sim10-1000\\,$kpc. These also agree with LISM\nvalues at small radii but increase in the CGM.",
        "The complete classification of the landscape of 6D $\\mathcal{N} = (1,1)$\nstring vacua remains an open problem. In this work we prove a classification\ntheorem for 6D $\\mathcal{N} = (1,1)$ asymmetric orbifolds utilizing a\ncorrespondence with orbifolds of chiral 2D SCFTs with $c= 24$ (or $c= 12$).\nInterestingly, this class of theories can give rise to 6D vacua in which the\nonly massless degrees of freedom reside in the gravity multiplet, with no\nmoduli other than the dilaton, thus corresponding to truly isolated vacua,\ncalled string islands. It is expected that there exist five new type II islands\nwith as-yet-unknown constructions. In this work we construct them all using\nasymmetric $\\mathbb{Z}_n$-orbifolds of Type II on $T^4$ with $n = 5,8,10,12$.\nWe show that the cases $n = 5,8$ admit non-trivial discrete theta angles which\nhave important consequences for both the string and particle charge lattices.\nIn fact they provide examples of BPS-incompleteness and the strongest failure\nof the lattice weak gravity conjecture. Our work is expected to finalize our\nunderstanding of all perturbative 6D $\\mathcal{N} = (1,1)$ theories.",
        "Highly-irradiated gas giant exoplanets are predicted to show circulation\npatterns dominated by day-to-night heat transport and a spatial distribution of\nclouds that is driven by advection and local heating. Hot-Jupiters have been\nextensively studied from broadband phase-curve observations at infrared and\noptical wavelengths, but spectroscopic observations in the reflected light are\nrare and the regime of smaller and higher-metallicity ultra-hot planets, such\nas hot-Neptunes, remains largely unexplored to date. Here we present the\nphase-resolved reflected-light and thermal-emission spectroscopy of the\nultra-hot Neptune LTT 9779b, obtained through observing its full phase-curve\nfrom 0.6 to 2.8 $\\mu$m with JWST NIRISS\/SOSS. We detect an asymmetric dayside\nin reflected light (3.1$\\sigma$ significance) with highly-reflective white\nclouds on the western dayside (A = 0.79$\\pm$0.15) and a much lower-albedo\neastern dayside (A = 0.41$\\pm$0.10), resulting in an overall dayside albedo of\nA = 0.50$\\pm$0.07. The thermal phase curve is symmetric about the substellar\npoint, with a dayside effective temperature of T$_\\mathrm{eff,day}$ =\n2,260$^{+40}_{-50}$ K and a cold nightside (T$_\\mathrm{eff,night}$ <1,330 K at\n3-$\\sigma$ confidence), indicative of short radiative timescales. We propose an\natmospheric circulation and cloud distribution regime in which heat is\ntransported eastward from the dayside towards the cold nightside by an\nequatorial jet, leading to a colder western dayside where temperatures are\nsufficiently low for the condensation of silicate clouds.",
        "Baryonic black branes describe the quantum critical phase of the conformal\nconifold gauge theory at strong coupling. This phase extends to zero\ntemperature at a finite baryonic chemical potential, represented by extremal\nblack branes with $AdS_2\\times R^3\\times T^{1,1}$ throat in asymptotic\n$AdS_5\\times T^{1,1}$ geometry. We demonstrate here that this phase is\ndynamically unstable below some critical value of $T_c\/\\mu$: the instability is\nrepresented by a diffusive mode in the hydrodynamic sound channel with a\nnegative diffusion coefficient. We also identify a new (exotic) ordered phase\nof the conifold gauge theory: this phase originates at the same critical value\nof $T_c\/\\mu$, but extends to arbitrary high temperatures, and is characterized\nby an expectation value of a dimension-2 operator, ${\\cal O}_2\\propto T^2$, in\nthe limit $\\frac \\mu T\\to 0$.",
        "In this article, we construct discrete tight frames for\n$L^2(\\mathbb{S}^{d-1})$, $d\\geq3$, which consist of localized polynomial\nwavelets with adjustable degrees of directionality. In contrast to the well\nstudied isotropic case, these systems are well suited for the direction\nsensitive analysis of anisotropic features such as edges. The price paid for\nthis is the fact that at each scale the wavelet transform lives on the rotation\ngroup $SO(d)$, and not on $\\mathbb{S}^{d-1}$ as in the zonal setting. Thus, the\nstandard approach of building discrete frames by sampling the continuous\nwavelet transform requires a significantly larger amount of sample points.\nHowever, by keeping the directionality limited, this number can be greatly\nreduced to the point where it is comparable to the number of samples needed in\nthe isotropic case. Moreover, the limited directionality is reflected in the\nwavelets being steerable and their great localization in space leads to a fast\nconvergence of the wavelet expansion in the spaces $L^p(\\mathbb{S}^{d-1})$,\n$1\\leq p \\leq \\infty$.",
        "Homogeneous magnetic fields can be generated through the strategic\narrangement of permanent magnets. The Halbach array serves as a prominent\nexample of an effective design following this principle. However, it is a\ntwo-dimensional approach because it is optimal when placing infinitely long\nmagnets -- line dipoles -- on a circle. If shorter, more realistic magnets are\nto be used, the optimal arrangement of magnetic moments diverges from the\nclassical Halbach geometry. This paper presents optimal solutions for\nthree-dimensional arrangements calculated for point dipoles, including\noptimized orientations for single rings and stacks of two rings. They are\nsuperior to the original Halbach arrangement and a modification described in\nthe literature, both in terms of the strength and the homogeneity of the\nmagnetic field. Analytic formulae are provided for both cases and tested by\nexperimental realizations.",
        "We address the challenge of preserving the simultaneity and chronology of\nsensing events in multi-sensory systems with wireless links. The network uses\ntemporal windows of integration (TWIs), borrowed multi-sensory perception, to\npreserve the temporal structure of the sensing data at the application side. We\nintroduce a composite latency model for propagation, sensing, and communication\nthat leads to the derivation of the probability of simultaneity violation. This\nis used to select the TWI duration aiming to achieve the desired degrees of\nchronological preservation, while maintaining the throughput of events. The\nletter provides important insights and analytical tools about the TWI impact on\nthe event registration.",
        "The anomalous Hall effect (AHE) occurs in magnetic systems and also\nunexpectedly in non-magnetic materials adjacent to magnetic insulators via the\nheterointerface interactions. However, the AHE in heterostructures induced by\nmagnetic proximity effect remains quite weak, restricting their practical\ndevice applications. Here, we report a large intrinsic AHE with a resistivity\nof 114 n{\\Omega} cm at 5 K in noncoplanar magnetic heterostructures of\nCr5Te6\/Pt. This is the record-high AHE value among all the magnetic\ninsulators\/heavy metal heterostructures. A reversal of the AHE signal occurs\ndue to the reconstruction of Berry curvature at the Fermi level, which is\nverified by the first-principles calculations. Topological spin textures at the\ninterface are directly visualized via high-magnetic-field magnetic force\nmicroscopy, which accounts for the large AHE, as confirmed by the atomic\nsimulations. These findings open a new avenue for exploring the large AHE in\nheterointerfaces and facilitate the potential applications in topological\nspintronic devices.",
        "We consider entangling operations in a single nitrogen-vacancy (NV) center in\ndiamond where the hyperfine-coupled nuclear spin qubits are addressed with\nradio-frequency (rf) pulses conditioned on the state of the central electron\nspin. Limiting factors for the gate fidelity are coherent errors due to\noff-resonant driving of neighboring transitions in the dense, hyperfine-split\nenergy spectrum of the defect and non-negligible perpendicular hyperfine tensor\ncomponents that narrow the choice of $^{13}\\rm C$ nuclear spin qubits. We\naddress these issues by presenting protocols based on synchronization effects\nthat allow for a complete suppression of both error sources in state-of-the-art\nCNOT gate schemes. This is possible by a suitable choice of parameter sets that\nincorporate the error into the scheme instead of avoiding it. These results\ncontribute to the recent progress toward scalable quantum computation with\ndefects in solids.",
        "For the first time, we develop a simulation-based model for the Minkowski\nfunctionals (MFs) of large-scale structure, which allows us to extract the full\ninformation available from the MFs (including both the Gaussian and\nnon-Gaussian part), and apply it to the BOSS DR12 CMASS galaxy sample. Our\nmodel is based on high-fidelity mock galaxy catalogs constructed from the\n\\textsc{Abacus}\\textsc{Summit} simulations using the halo occupation\ndistribution (HOD) framework, which include the redshift-space distortions and\nAlcock-Paczynski distortions, incorporate survey realism, including survey\ngeometry and veto masks, and account for angular plus radial selection effects.\nThe cosmological and HOD parameter dependence of the MFs is captured with a\nneural network emulator trained from the galaxy mocks with various cosmological\nand HOD parameters. To benchmark the constraining power of the MFs, we also\ntrain an emulator for the galaxy 2-point correlation function (2PCF) using the\nsame pipeline. Having validated our approach through successful parameter\nrecovery tests on both internal and external mocks, including non-HOD forward\nmodels of the halo-galaxy connection, we apply our forward model to analyze the\nCMASS data in the redshift range $0.45<z<0.58$. We find the MFs provide\nstronger constraints on the cosmological parameters than the 2PCF. The\ncombination of the two gives $\\omega_{\\rm cdm}=0.1172^{+0.0020}_{-0.0023}$,\n$\\sigma_8=0.783\\pm 0.026$, and $n_s=0.966^{+0.019}_{-0.015}$, which are tighter\nby a factor of 2.0, 1.9, and 1.6 than the 2PCF alone. The derived constraint\n$f\\sigma_8=0.453 \\pm 0.016$ is also improved by a factor of 1.9, compared to\nthe 2PCF, and agrees well with Planck 2018 predictions and other results from a\nseries of studies in the literature.",
        "We investigate objects in symmetric tensor categories that have\nsimultaneously finite symmetric and finite exterior algebra. This forces the\ncharacteristic of the base field to be $p>0$, and the maximal degree of\nnon-vanishing symmetric and exterior powers to add up to a multiple of $p$. We\ngive a complete classification of objects in tensor categories for which this\nsum equals $p$. All resulting tensor categories are Verlinde categories of\nreductive groups and we fill in some gaps in the literature on these\ncategories.",
        "Quantum computation will rely on quantum error correction to counteract\ndecoherence. Successfully implementing an error correction protocol requires\nthe fidelity of qubit operations to be well-above error correction thresholds.\nIn superconducting quantum computers, measurement of the qubit state remains\nthe lowest-fidelity operation. For the transmon, a prototypical superconducting\nqubit, measurement is carried out by scattering a microwave tone off the qubit.\nConventionally, the frequency of this tone is of the same order as the transmon\nfrequency. The measurement fidelity in this approach is limited by\nmulti-excitation resonances in the transmon spectrum which are activated at\nhigh readout power. These resonances excite the qubit outside of the\ncomputational basis, violating the desired quantum non-demolition character of\nthe measurement. Here, we find that strongly detuning the readout frequency\nfrom that of the transmon exponentially suppresses the strength of spurious\nmulti-excitation resonances. By increasing the readout frequency up to twelve\ntimes the transmon frequency, we achieve a quantum non-demolition measurement\nfidelity of 99.93% with a residual probability of leakage to non-computational\nstates of only 0.02%.",
        "Although the specification of bivariate probability models using a collection\nof assumed conditional distributions is not a novel concept, it has received\nconsiderable attention in the last decade. In this study, a bivariate\ndistribution-the bivariate Poisson-Gamma conditional distribution-is\nintroduced, combining both univariate continuous and discrete distributions.\nThis work explores aspects of this model's structure and statistical inference\nthat have not been studied before. This paper contributes to the field of\nstatistical modeling and distribution theory through the use of maximum\nlikelihood estimation, along with simulations and analyses of real data.",
        "The coupling between the spin degrees of freedom and macroscopic mechanical\nmotions, including striction, shearing, and rotation, has attracted wide\ninterest with applications in actuation, transduction, and information\nprocessing. Experiments so far have established the mechanical responses to the\nlong-range ordered or isolated single spin states. However, it remains elusive\nwhether mechanical motions can couple to a different type of magnetic\nstructure, the non-collinear spin textures, which exhibit nanoscale spatial\nvariations of spin (domain walls, skyrmions, etc.) and are promising candidates\nto realize high-speed computing devices. Here, we report the detection of\ncollective spin texture dynamics with nanoelectromechanical resonators made of\ntwo-dimensional antiferromagnetic (AFM) MnPS3 with $10^{-9}$ strain\nsensitivity. By examining radio frequency mechanical oscillations under\nmagnetic fields, new magnetic transitions were identified with sharp dips in\nresonant frequency. They are attributed to the collective AFM domain wall\nmotions as supported by the analytical modeling of magnetostriction and\nlarge-scale spin-dynamics simulations. Additionally, an abnormally large\nmodulation in the mechanical nonlinearity at the transition field infers a\nfluid-like response due to the ultrafast domain motion. Our work establishes a\nstrong coupling between spin texture and mechanical dynamics, laying the\nfoundation for electromechanical manipulation of spin texture and developing\nquantum hybrid devices.",
        "We calculate $(\\sqrt{-t}\/Q)^k $ and $(m\/Q)^k$ power corrections with $k\\le\n4$, where $m$ is the target mass and $t$ is the momentum transfer, to several\nkey observables in Deeply Virtual Compton Scattering (DVCS). We find that the\npower expansion is well convergent up to $|t|\/Q^2\\lesssim 1\/4$ for most of the\nobservables, but is naturally organized in terms of $1\/(Q^2+t)$ rather than the\nnominal hard scale $1\/Q^2$. We also argue that target mass corrections remain\nunder control and do not endanger QCD factorization for coherent DVCS on\nnuclei. These results remove an important source of uncertainties due to the\nframe dependence and violation of electromagnetic Ward identities in the QCD\npredictions for the DVCS amplitudes in the leading-twist approximation.",
        "We consider projective Hyper-K\\\"ahler manifolds of dimension four that are\ndeformation equivalent to Hilbert squares of K3 surfaces. In case such a\nmanifold admits a divisorial contraction, the exceptional divisor is a conic\nbundle over a K3 surface. There are five types of such conic bundles. In case\nthe manifold has Picard rank two and has two (birational) divisorial\ncontractions we determine the types of these conic bundles. There are exactly\nseven cases. For the Fano varieties of cubic fourfolds there are only four\ncases and we provide examples of these.",
        "Studying the de Rham complex is a natural choice when working with problems\nin electromagnetics and fluid mechanics. By discretizing the complex correctly,\nit is possible to attain stable numerical methods to tackle these problems. An\nimportant consideration when constructing the discrete complex is that it must\npreserve the cohomology structure of the original one. This property is not\nguaranteed when the discrete function spaces chosen are hierarchical B-splines.\nResearch shows that a poor choice of refinement domains may give rise to\nspurious harmonic forms that ruin the accuracy of solutions, even for the\nsimplest partial differential equations. Another crucial aspect to consider in\nthe hierarchical setting is the notion of admissibility, as it is possible to\nobtain optimal convergence rates of numerical solutions by limiting the\nmulti-level interaction of basis functions. We will focus on the\ntwo-dimensional de Rham complex over the unit square $\\Omega \\subseteq\n\\mathbb{R}^2$. In this scenario, the discrete de Rham complex should be exact,\nand we provide both the theoretical and the algorithm-implementation framework\nto ensure this is the case. Moreover, we show that, under a common restriction,\nthe admissibility class of the first space of the discrete complex persists\nthroughout the remaining spaces. Finally, we include numerical results that\nmotivate the importance of the previous concerns for the vector Laplace and\nMaxwell eigenvalue problems.",
        "- An algorithm for calculating the radiation field of a charged point\nparticle performing a spiral motion in an infinite cylindrical waveguide with a\nmultilayer side wall is found. The number of layers and their filling is\narbitrary. The axis of the spiral is aligned with the axis of the waveguide, so\nthat the geometry of the problem has cylindrical symmetry. Explicit expressions\nfor modal frequency distributions and equations for resonant frequencies for\nsingle-layer and double-layer waveguides are given. Examples of graphical\nconstructions of modal frequency distributions of modes for single-layer\n(resistive), double-layer (metal-dielectric) and triple-layer (metal-dielectric\nwith internal NEG coating) waveguides are presented.",
        "The gyrokinetic particle-in-cell code PICLS is a full-f finite element tool\nto simulate turbulence in the tokamak scrape-off layer. During the previous\nyear, the capability of PICLS was extended to encompass electromagnetic\neffects. Successful tests using the method of manufactured solutions were\nconducted on the freshly added Amp\\`ere's-law-solver, and shear Alfv\\'en waves\nwere simulated to verify the new electromagnetic time step. However, as a code\nbased on the $p_{||}$-formulation of the gyrokinetic equations, PICLS is\naffected by the Amp\\`ere-cancellation problem. In order to bring higher-beta\nsimulations within reach of our computational capacity, we implemented the\nmixed-variable formulation with pullback-scheme in a similar fashion to, e.g.,\nEUTERPE, ORB5, or XGC. Here, we present the successful verification of the\ndifferent electromagnetic formulations of PICLS by simulating shear-Alfv\\'en\nwaves in a test setup designed to minimize kinetic effects.",
        "Recently, GeSn alloys have attracted much interest for direct-gap infrared\nphotonics and as potential topological materials which are compatible with the\nsemiconductor industry. However, for photonics, the high-Sn content required\nleads to low detectivity, associated with poor material quality, and the (>35%)\nSn required for topological properties have been out of reach experimentally.\nHere, we demonstrate that by patterning the Sn distribution within Ge, the\nelectronic properties have a far greater tunability than is possible with the\nrandom alloy. For the GeSn \\delta-digital alloy (DA) formed by confining Sn\natoms in atomic layer(s) along the [111] direction of Ge, we show that ~10% Sn\ncan lead to a triple-point semimetal. These findings are understood in terms of\nSn ordering causing spatial separation of Sn and Ge band edges, leading to band\ninversion. This mechanism can also lead to a weak topological insulator, Weyl\nsemimetal, and enables tunable direct bandgaps down to 2 meV, covering the\nentire infrared range. Our findings are generally applicable to other\nsemiconductors DAs and point to a new class of currently unexplored topological\nsystems accessible by epitaxy and establish the promise of low-Sn GeSn DAs for\napplication as infrared laser diodes and photodetectors in Si photonic\nintegrated circuits and infrared image sensors."
      ]
    }
  },
  {
    "id":2411.09469,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"The accuracy of colposcopic biopsy: Analyses from the placebo arm of the Gardasil clinical trials",
    "start_abstract":"We evaluated the overall agreement between colposcopically directed biopsies and definitive excisional specimens within context of three clinical trials. A total 737 women aged 16-45 who had a cervical biopsy taken 6 months before their therapy were included. Per-protocol, colposcopists to also obtain representative immediately therapy. Using adjudicated histological diagnoses, initial same day correlated with surgically excised specimens. The therapy, diagnoses was 42% (weighted kappa = 0.34) (95% CI: 0.29-0.39). underestimation intraepithelial neoplasia grade 2\/3 or adenocarcinoma in situ (CIN2-3\/AIS) CIN3\/AIS 26 42%, respectively. When allowing for one degree variance correlation, 92% CIN2-3\/AIS. specimen 56% 0.41) 0.36-0.47), CIN2-3\/AIS 57%. There significant associations when patients stratified by age, number biopsies, lesion size, presence human papillomavirus (HPV)16\/18 region. Of 178 diagnostic endocervical curettages performed, 14 (7.9%) found any HPV disease. Colposcopic accuracy improved CIN2 grouped as single predictive measure high-grade Colposcopy functioned well allowed one-degree difference surgical histologic interpretations, done practice. Taking more than colposcopic could improve patient management.",
    "start_categories":[
      "Clinical Trial"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b36"
      ],
      "title":[
        "CBAM: Convolutional block attention module"
      ],
      "abstract":[
        "We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Balancing the Budget: Understanding Trade-offs Between Supervised and\n  Preference-Based Finetuning",
        "Random Number Generation from Pulsars",
        "ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio\n  Chord Recognition",
        "Safe-VAR: Safe Visual Autoregressive Model for Text-to-Image Generative\n  Watermarking",
        "Optimizing Knowledge Distillation in Transformers: Enabling Multi-Head\n  Attention without Alignment Barriers",
        "ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code\n  Generation",
        "Evaluating the Impacts of Swapping on the US Decennial Census",
        "The Distributionally Robust Optimization Model of Sparse Principal\n  Component Analysis",
        "A class of anisotropic diffusion-transport equations in non-divergence\n  form",
        "Dark Distillation: Backdooring Distilled Datasets without Accessing Raw\n  Data",
        "Testing quantum gravity with dilute dipolar Bose gases",
        "The uncollapsed LaFe2As2 phase: compensated, highly doped,\n  electron-phonon coupled, iron-based superconductor",
        "Dimensions and metric dyadic cubes",
        "Histoires Morales: A French Dataset for Assessing Moral Alignment",
        "Generative Predictive Control: Flow Matching Policies for Dynamic and\n  Difficult-to-Demonstrate Tasks",
        "BEYONDWORDS is All You Need: Agentic Generative AI based Social Media\n  Themes Extractor",
        "Existence and uniqueness of control sets with a nonempty interior for\n  linear control systems on solvable groups",
        "Trustworthiness in Stochastic Systems: Towards Opening the Black Box",
        "Semantic Neural Radiance Fields for Multi-Date Satellite Data",
        "Non-Commutative fluid: an alternative source of cosmic acceleration",
        "On the Robustness of Cluster Clustering Covariance Calibration",
        "BRIDLE: Generalized Self-supervised Learning with Quantization",
        "Wavelet-Assisted Multi-Frequency Attention Network for Pansharpening",
        "Quasi-perfect spatiotemporal optical vortex with suppressed mode\n  degradation",
        "Generalizable Image Repair for Robust Visual Autonomous Racing",
        "CutPaste&Find: Efficient Multimodal Hallucination Detector with\n  Visual-aid Knowledge Base",
        "Examining Two Hop Reasoning Through Information Content Scaling",
        "Helix-mRNA: A Hybrid Foundation Model For Full Sequence mRNA\n  Therapeutics",
        "GPDFlow: Generative Multivariate Threshold Exceedance Modeling via\n  Normalizing Flows"
      ],
      "abstract":[
        "Post-training of Large Language Models often involves a pipeline of\nSupervised Finetuning (SFT) followed by Preference Finetuning (PFT) using\nmethods like Direct Preference Optimization. Both stages require annotated data\nthat are very different in structure and costs. We study how to optimally\nallocate a fixed training data budget between the two stages, through extensive\nexperiments spanning four diverse tasks, multiple model sizes and various data\nannotation costs. Our findings reveal that just SFT on the base model dominates\nperformance in low-data regimes ($<1,000$ annotated examples). With larger\ndata-budgets, we observe that a combination of SFT and PFT, often with\nincreasing portions allocated towards preference data yields optimal\nperformance. However, completely eliminating SFT and running PFT directly on\nthe base model yields suboptimal performance, described as the cold start\nproblem on tasks like mathematics. We observe that this is due to the\ndistribution shift arising from using DPO directly on the base model to elicit\nstep-by-step reasoning. This limitation can be effectively addressed by\nallocating even a small portion ($<10$%) of the budget to SFT first, resulting\nin performance improvements of $15-20$% on analytical benchmarks like GSM8k.\nThese results provide actionable insights for researchers and practitioners\noptimizing model development under budget constraints, where high-quality data\ncuration often represents a significant portion of the total costs of model\ndevelopment.",
        "Pulsars exhibit signals with precise inter-arrival times that are on the\norder of milliseconds to seconds depending on the individual pulsar. There is\nsubtle variation in the timing of pulsar signals, primarily due to the presence\nof gravitational waves, intrinsic variance in the period of the pulsar, and\nerrors in the realization of Terrestrial Time (TT). Traditionally, these\nvariations are dismissed as noise in high-precision timing experiments. In this\npaper, we show that these variations serve as a natural entropy source for the\ncreation of Random Number Generators (RNG). We also explore the effects of\nusing randomness extractors to increase the entropy of random bits extracted\nfrom Pulsar timing data. To evaluate the quality of the Pulsar RNG, we model\nits entropy as a $k$-source and use well-known cryptographic results to show\nits closeness to a theoretically ideal uniformly random source. To remain\nconsistent with prior work, we also show that the Pulsar RNG passes well-known\nstatistical tests such as the NIST test suite.",
        "Chord recognition serves as a critical task in music information retrieval\ndue to the abstract and descriptive nature of chords in music analysis. While\naudio chord recognition systems have achieved significant accuracy for small\nvocabularies (e.g., major\/minor chords), large-vocabulary chord recognition\nremains a challenging problem. This complexity also arises from the inherent\nlong-tail distribution of chords, where rare chord types are underrepresented\nin most datasets, leading to insufficient training samples. Effective chord\nrecognition requires leveraging contextual information from audio sequences,\nyet existing models, such as combinations of convolutional neural networks,\nbidirectional long short-term memory networks, and bidirectional transformers,\nface limitations in capturing long-term dependencies and exhibit suboptimal\nperformance on large-vocabulary chord recognition tasks. This work proposes\nChordFormer, a novel conformer-based architecture designed to tackle structural\nchord recognition (e.g., triads, bass, sevenths) for large vocabularies.\nChordFormer leverages conformer blocks that integrate convolutional neural\nnetworks with transformers, thus enabling the model to capture both local\npatterns and global dependencies effectively. By addressing challenges such as\nclass imbalance through a reweighted loss function and structured chord\nrepresentations, ChordFormer outperforms state-of-the-art models, achieving a\n2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy\non large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling\nclass imbalance, providing robust and balanced recognition across chord types.\nThis approach bridges the gap between theoretical music knowledge and practical\napplications, advancing the field of large-vocabulary chord recognition.",
        "With the success of autoregressive learning in large language models, it has\nbecome a dominant approach for text-to-image generation, offering high\nefficiency and visual quality. However, invisible watermarking for visual\nautoregressive (VAR) models remains underexplored, despite its importance in\nmisuse prevention. Existing watermarking methods, designed for diffusion\nmodels, often struggle to adapt to the sequential nature of VAR models. To\nbridge this gap, we propose Safe-VAR, the first watermarking framework\nspecifically designed for autoregressive text-to-image generation. Our study\nreveals that the timing of watermark injection significantly impacts generation\nquality, and watermarks of different complexities exhibit varying optimal\ninjection times. Motivated by this observation, we propose an Adaptive Scale\nInteraction Module, which dynamically determines the optimal watermark\nembedding strategy based on the watermark information and the visual\ncharacteristics of the generated image. This ensures watermark robustness while\nminimizing its impact on image quality. Furthermore, we introduce a Cross-Scale\nFusion mechanism, which integrates mixture of both heads and experts to\neffectively fuse multi-resolution features and handle complex interactions\nbetween image content and watermark patterns. Experimental results demonstrate\nthat Safe-VAR achieves state-of-the-art performance, significantly surpassing\nexisting counterparts regarding image quality, watermarking fidelity, and\nrobustness against perturbations. Moreover, our method exhibits strong\ngeneralization to an out-of-domain watermark dataset QR Codes.",
        "Knowledge distillation (KD) in transformers often faces challenges due to\nmisalignment in the number of attention heads between teacher and student\nmodels. Existing methods either require identical head counts or introduce\nprojectors to bridge dimensional gaps, limiting flexibility and efficiency. We\npropose Squeezing-Heads Distillation (SHD), a novel approach that enables\nseamless knowledge transfer between models with varying head counts by\ncompressing multi-head attention maps via efficient linear approximation.\nUnlike prior work, SHD eliminates alignment barriers without additional\nparameters or architectural modifications. Our method dynamically approximates\nthe combined effect of multiple teacher heads into fewer student heads,\npreserving fine-grained attention patterns while reducing redundancy.\nExperiments across language (LLaMA, GPT) and vision (DiT, MDT) generative and\nvision (DeiT) discriminative tasks demonstrate SHD's effectiveness: it\noutperforms logit-based and feature-alignment KD baselines, achieving\nstate-of-the-art results in image classification, image generation language\nfine-tuning, and language pre-training. The key innovations of flexible head\ncompression, projector-free design, and linear-time complexity make SHD a\nversatile and scalable solution for distilling modern transformers. This work\nbridges a critical gap in KD, enabling efficient deployment of compact models\nwithout compromising performance.",
        "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in chart understanding tasks. However, interpreting charts with\ntextual descriptions often leads to information loss, as it fails to fully\ncapture the dense information embedded in charts. In contrast, parsing charts\ninto code provides lossless representations that can effectively contain all\ncritical details. Although existing open-source MLLMs have achieved success in\nchart understanding tasks, they still face two major challenges when applied to\nchart-to-code tasks.: (1) Low executability and poor restoration of chart\ndetails in the generated code and (2) Lack of large-scale and diverse training\ndata. To address these challenges, we propose \\textbf{ChartCoder}, the first\ndedicated chart-to-code MLLM, which leverages Code LLMs as the language\nbackbone to enhance the executability of the generated code. Furthermore, we\nintroduce \\textbf{Chart2Code-160k}, the first large-scale and diverse dataset\nfor chart-to-code generation, and propose the \\textbf{Snippet-of-Thought (SoT)}\nmethod, which transforms direct chart-to-code generation data into step-by-step\ngeneration. Experiments demonstrate that ChartCoder, with only 7B parameters,\nsurpasses existing open-source MLLMs on chart-to-code benchmarks, achieving\nsuperior chart restoration and code excitability. Our code will be available at\nhttps:\/\/github.com\/thunlp\/ChartCoder.",
        "To meet its dual burdens of providing useful statistics and ensuring privacy\nof individual respondents, the US Census Bureau has for decades introduced some\nform of \"noise\" into published statistics. Initially, they used a method known\nas \"swapping\" (1990-2010). In 2020, they switched to an algorithm called\nTopDown that ensures a form of Differential Privacy. While the TopDown\nalgorithm has been made public, no implementation of swapping has been released\nand many details of the deployed swapping methodology deployed have been kept\nsecret. Further, the Bureau has not published (even a synthetic) \"original\"\ndataset and its swapped version. It is therefore difficult to evaluate the\neffects of swapping, and to compare these effects to those of other privacy\ntechnologies. To address these difficulties we describe and implement a\nparameterized swapping algorithm based on Census publications, court documents,\nand informal interviews with Census employees. With this implementation, we\ncharacterize the impacts of swapping on a range of statistical quantities of\ninterest. We provide intuition for the types of shifts induced by swapping and\ncompare against those introduced by TopDown. We find that even when swapping\nand TopDown introduce errors of similar magnitude, the direction in which\nstatistics are biased need not be the same across the two techniques. More\nbroadly, our implementation provides researchers with the tools to analyze and\npotentially correct for the impacts of disclosure avoidance systems on the\nquantities they study.",
        "We consider sparse principal component analysis (PCA) under a stochastic\nsetting where the underlying probability distribution of the random parameter\nis uncertain. This problem is formulated as a distributionally robust\noptimization (DRO) model based on a constructive approach to capturing\nuncertainty in the covariance matrix, which constitutes a nonsmooth constrained\nmin-max optimization problem. We further prove that the inner maximization\nproblem admits a closed-form solution, reformulating the original DRO model\ninto an equivalent minimization problem on the Stiefel manifold. This\ntransformation leads to a Riemannian optimization problem with intricate\nnonsmooth terms, a challenging formulation beyond the reach of existing\nalgorithms. To address this issue, we devise an efficient smoothing manifold\nproximal gradient algorithm. We prove the Riemannian gradient consistency and\nglobal convergence of our algorithm to a stationary point of the nonsmooth\nminimization problem. Moreover, we establish the iteration complexity of our\nalgorithm. Finally, numerical experiments are conducted to validate the\neffectiveness and scalability of our algorithm, as well as to highlight the\nnecessity and rationality of adopting the DRO model for sparse PCA.",
        "We generalize Einstein's probabilistic method for the Brownian motion to\nstudy compressible fluids in porous media. The multi-dimensional case is\nconsidered with general probability distribution functions. By relating the\nexpected displacement per unit time with the velocity of the fluid, we derive\nan anisotropic diffusion equation in non-divergence form that contains a\ntransport term. Under the Darcy law assumption, a corresponding nonlinear\npartial differential equations for the density function is obtained. The\nclassical solutions of this equation are studied, and the maximum and strong\nmaximum principles are established. We also obtain exponential decay estimates\nfor the solutions for all time, and particularly, their exponential convergence\nas time tends to infinity. Our analysis uses some transformations of the\nBernstein-Cole--Hopf type which are explicitly constructed even for very\ngeneral equation of state. Moreover, the Lemma of Growth in time is proved and\nutilized in order to achieve the above decaying estimates.",
        "Dataset distillation (DD) enhances training efficiency and reduces bandwidth\nby condensing large datasets into smaller synthetic ones. It enables models to\nachieve performance comparable to those trained on the raw full dataset and has\nbecome a widely adopted method for data sharing. However, security concerns in\nDD remain underexplored. Existing studies typically assume that malicious\nbehavior originates from dataset owners during the initial distillation\nprocess, where backdoors are injected into raw datasets. In contrast, this work\nis the first to address a more realistic and concerning threat: attackers may\nintercept the dataset distribution process, inject backdoors into the distilled\ndatasets, and redistribute them to users. While distilled datasets were\npreviously considered resistant to backdoor attacks, we demonstrate that they\nremain vulnerable to such attacks. Furthermore, we show that attackers do not\neven require access to any raw data to inject the backdoors successfully.\nSpecifically, our approach reconstructs conceptual archetypes for each class\nfrom the model trained on the distilled dataset. Backdoors are then injected\ninto these archetypes to update the distilled dataset. Moreover, we ensure the\nupdated dataset not only retains the backdoor but also preserves the original\noptimization trajectory, thus maintaining the knowledge of the raw dataset. To\nachieve this, a hybrid loss is designed to integrate backdoor information along\nthe benign optimization trajectory, ensuring that previously learned\ninformation is not forgotten. Extensive experiments demonstrate that distilled\ndatasets are highly vulnerable to backdoor attacks, with risks pervasive across\nvarious raw datasets, distillation methods, and downstream training strategies.\nMoreover, our attack method is efficient, capable of synthesizing a malicious\ndistilled dataset in under one minute in certain cases.",
        "We systematically investigate the effects of quantum gravity on the\nground-state properties of dilute homogeneous dipolar Bose gases using the\nHartree-Fock-Bogoliubov theory based on the generalized uncertainty principle.\nWe calculate quantum gravity corrections to the condensed fraction, the\nequation of state, the critical temperature and the superfluid fraction.\nImproved upper bounds on the generalized uncertainty principle parameters are\nfound. We compare our predictions with previous experimental and theoretical\nresults.",
        "The recently discovered LaFe2As2 superconducting compound, member of the 122\nfamily of iron pnictide superconductors, becomes superconducting below Tc=13K,\nyet its nominal doping apparently places it in the extreme overdoped limit,\nwhere superconductivity should be suppressed. In this work, we investigate the\nnormal state of magneto- and thermo-electric transport and specific heat of\nthis compound. The experimental data are consistent with the presence of highly\ncompensated electron and hole bands, with around 0.42 electrons per unit cell\njust above Tc, and high effective masses around 3m0. The temperature dependence\nof transport properties strongly resembles that of conventional\nsuperconductors, pointing to a key role of electron-phonon coupling. From these\nevidences, LaFe2As2 can be regarded as the connecting compound between\nunconventional and conventional superconductors.",
        "In this note, we provide equivalent definitions for fractal geometric\ndimensions through dyadic cube constructions. Given a metric space $X$ with\nfinite Assouad dimension, i.e., satisfying the doubling property, we show that\nthe construction of systems of dyadic cubes by Hyt\\\"onen-Kairema is compatible\nwith many dimensions. In particular, the Hausdorff, Minkowski, and Assouad\ndimensions can be equivalently expressed solely using dyadic cubes in the\naforementioned system. The same is true for the Assouad spectrum, a collection\nof dimensions introduced by Fraser-Yu.",
        "Aligning language models with human values is crucial, especially as they\nbecome more integrated into everyday life. While models are often adapted to\nuser preferences, it is equally important to ensure they align with moral norms\nand behaviours in real-world social situations. Despite significant progress in\nlanguages like English and Chinese, French has seen little attention in this\narea, leaving a gap in understanding how LLMs handle moral reasoning in this\nlanguage. To address this gap, we introduce Histoires Morales, a French dataset\nderived from Moral Stories, created through translation and subsequently\nrefined with the assistance of native speakers to guarantee grammatical\naccuracy and adaptation to the French cultural context. We also rely on\nannotations of the moral values within the dataset to ensure their alignment\nwith French norms. Histoires Morales covers a wide range of social situations,\nincluding differences in tipping practices, expressions of honesty in\nrelationships, and responsibilities toward animals. To foster future research,\nwe also conduct preliminary experiments on the alignment of multilingual models\non French and English data and the robustness of the alignment. We find that\nwhile LLMs are generally aligned with human moral norms by default, they can be\neasily influenced with user-preference optimization for both moral and immoral\ndata.",
        "Generative control policies have recently unlocked major progress in\nrobotics. These methods produce action sequences via diffusion or flow\nmatching, with training data provided by demonstrations. But despite enjoying\nconsiderable success on difficult manipulation problems, generative policies\ncome with two key limitations. First, behavior cloning requires expert\ndemonstrations, which can be time-consuming and expensive to obtain. Second,\nexisting methods are limited to relatively slow, quasi-static tasks. In this\npaper, we leverage a tight connection between sampling-based predictive control\nand generative modeling to address each of these issues. In particular, we\nintroduce generative predictive control, a supervised learning framework for\ntasks with fast dynamics that are easy to simulate but difficult to\ndemonstrate. We then show how trained flow-matching policies can be\nwarm-started at run-time, maintaining temporal consistency and enabling fast\nfeedback rates. We believe that generative predictive control offers a\ncomplementary approach to existing behavior cloning methods, and hope that it\npaves the way toward generalist policies that extend beyond quasi-static\ndemonstration-oriented tasks.",
        "Thematic analysis of social media posts provides a major understanding of\npublic discourse, yet traditional methods often struggle to capture the\ncomplexity and nuance of unstructured, large-scale text data. This study\nintroduces a novel methodology for thematic analysis that integrates tweet\nembeddings from pre-trained language models, dimensionality reduction using and\nmatrix factorization, and generative AI to identify and refine latent themes.\nOur approach clusters compressed tweet representations and employs generative\nAI to extract and articulate themes through an agentic Chain of Thought (CoT)\nprompting, with a secondary LLM for quality assurance. This methodology is\napplied to tweets from the autistic community, a group that increasingly uses\nsocial media to discuss their experiences and challenges. By automating the\nthematic extraction process, the aim is to uncover key insights while\nmaintaining the richness of the original discourse. This autism case study\ndemonstrates the utility of the proposed approach in improving thematic\nanalysis of social media data, offering a scalable and adaptable framework that\ncan be applied to diverse contexts. The results highlight the potential of\ncombining machine learning and Generative AI to enhance the depth and accuracy\nof theme identification in online communities.",
        "In this paper, we obtain weak conditions for the existence of a control set\nwith a nonempty interior for a linear control system on a solvable Lie group.\nWe show that the Lie algebra rank condition together with the compactness of\nthe nilpotent part of the generalized kernel of the drift are enough to assure\nthe existence of such a control set. Moreover, this control set is unique and\ncontains the whole generalized kernel in its closure.",
        "AI systems are increasingly tasked to complete responsibilities with\ndecreasing oversight. This delegation requires users to accept certain risks,\ntypically mitigated by perceived or actual alignment of values between humans\nand AI, leading to confidence that the system will act as intended. However,\nstochastic behavior by an AI system threatens to undermine alignment and\npotential trust. In this work, we take a philosophical perspective to the\ntension and potential conflict between stochasticity and trustworthiness. We\ndemonstrate how stochasticity complicates traditional methods of establishing\ntrust and evaluate two extant approaches to managing it: (1) eliminating\nuser-facing stochasticity to create deterministic experiences, and (2) allowing\nusers to independently control tolerances for stochasticity. We argue that both\napproaches are insufficient, as not all forms of stochasticity affect\ntrustworthiness in the same way or to the same degree. Instead, we introduce a\nnovel definition of stochasticity and propose latent value modeling for both AI\nsystems and users to better assess alignment. This work lays a foundational\nstep toward understanding how and when stochasticity impacts trustworthiness,\nenabling more precise trust calibration in complex AI systems, and underscoring\nthe importance of sociotechnical analyses to effectively address these\nchallenges.",
        "In this work we propose a satellite specific Neural Radiance Fields (NeRF)\nmodel capable to obtain a three-dimensional semantic representation (neural\nsemantic field) of the scene. The model derives the output from a set of\nmulti-date satellite images with corresponding pixel-wise semantic labels. We\ndemonstrate the robustness of our approach and its capability to improve noisy\ninput labels. We enhance the color prediction by utilizing the semantic\ninformation to address temporal image inconsistencies caused by non-stationary\ncategories such as vehicles. To facilitate further research in this domain, we\npresent a dataset comprising manually generated labels for popular multi-view\nsatellite images. Our code and dataset are available at\nhttps:\/\/github.com\/wagnva\/semantic-nerf-for-satellite-data.",
        "We have developed a Hubble function based on Newtonian Cosmology using\nnon-commutative fluid equations. Our Hubble function contains cosmic fluids\nwith the signature of a new cosmological parameter $\\sigma$, motivated by a\nnon-commutative Poisson bracket structure. Interestingly, this Hubble function\ndoes not include any external fluid content related to dark energy or the\nCosmological constant; the parameter $\\sigma$ acts as the source of accelerated\nexpansion. In this work, we aim to explain the phenomenon of the accelerating\nexpansion of the universe without \"dark energy\". Additionally, we have verified\nthe observational bounds for $\\sigma$ to assess its potential in explaining the\naccelerated expansion.",
        "Ongoing and upcoming wide-field surveys at different wavelengths will measure\nthe distribution of galaxy clusters with unprecedented precision, demanding\naccurate models for the two-point correlation function (2PCF) covariance. In\nthis work, we assess a semi-analytical framework for the cluster 2PCF\ncovariance that employs three nuisance parameters to account for non-Poissonian\nshot noise, residual uncertainties in the halo bias model, and subleading noise\nterms. We calibrate these parameters on a suite of fast approximate simulations\ngenerated by PINOCCHIO as well as full $N$-body simulations from OpenGADGET3.\nWe demonstrate that PINOCCHIO can reproduce the 2PCF covariance measured in\nOpenGADGET3 at the few percent level, provided the mass functions are carefully\nrescaled. Resolution tests confirm that high particle counts are necessary to\ncapture shot-noise corrections, especially at high redshifts. We perform the\nparameter calibration across multiple cosmological models, showing that one of\nthe nuisance parameters, the non-Poissonian shot-noise correction $\\alpha$,\ndepends mildly on the amplitude of matter fluctuations $\\sigma_8$. In contrast,\nthe remaining two parameters, $\\beta$ controlling the bias correction and\n$\\gamma$ controlling the secondary shot-noise correction, exhibit more\nsignificant variation with redshift and halo mass. Overall, our results\nunderscore the importance of calibrating covariance models on realistic mock\ncatalogs that replicate the selection function of forthcoming surveys and\nhighlight that approximate methods, when properly tuned, can effectively\ncomplement full $N$-body simulations for precision cluster cosmology.",
        "Self-supervised learning has been a powerful approach for learning meaningful\nrepresentations from unlabeled data across various domains, reducing the\nreliance on large labeled datasets. Inspired by BERT's success in capturing\ndeep bidirectional contexts in natural language processing, similar frameworks\nhave been adapted to other modalities such as audio, with models like BEATs\nextending the bidirectional training paradigm to audio signals using vector\nquantization (VQ). However, these frameworks face challenges, notably their\ndependence on a single codebook for quantization, which may not capture the\ncomplex, multifaceted nature of signals. In addition, inefficiencies in\ncodebook utilization lead to underutilized code vectors. To address these\nlimitations, we introduce BRIDLE (Bidirectional Residual Quantization\nInterleaved Discrete Learning Encoder), a self-supervised encoder pretraining\nframework that incorporates residual quantization (RQ) into the bidirectional\ntraining process, and is generalized for pretraining with audio, image, and\nvideo. Using multiple hierarchical codebooks, RQ enables fine-grained\ndiscretization in the latent space, enhancing representation quality. BRIDLE\ninvolves an interleaved training procedure between the encoder and tokenizer.\nWe evaluate BRIDLE on audio understanding tasks using classification\nbenchmarks, achieving state-of-the-art results, and demonstrate competitive\nperformance on image classification and video classification tasks, showing\nconsistent improvements over traditional VQ methods in downstream performance.",
        "Pansharpening aims to combine a high-resolution panchromatic (PAN) image with\na low-resolution multispectral (LRMS) image to produce a high-resolution\nmultispectral (HRMS) image. Although pansharpening in the frequency domain\noffers clear advantages, most existing methods either continue to operate\nsolely in the spatial domain or fail to fully exploit the benefits of the\nfrequency domain. To address this issue, we innovatively propose\nMulti-Frequency Fusion Attention (MFFA), which leverages wavelet transforms to\ncleanly separate frequencies and enable lossless reconstruction across\ndifferent frequency domains. Then, we generate Frequency-Query, Spatial-Key,\nand Fusion-Value based on the physical meanings represented by different\nfeatures, which enables a more effective capture of specific information in the\nfrequency domain. Additionally, we focus on the preservation of frequency\nfeatures across different operations. On a broader level, our network employs a\nwavelet pyramid to progressively fuse information across multiple scales.\nCompared to previous frequency domain approaches, our network better prevents\nconfusion and loss of different frequency features during the fusion process.\nQuantitative and qualitative experiments on multiple datasets demonstrate that\nour method outperforms existing approaches and shows significant generalization\ncapabilities for real-world scenarios.",
        "Spatiotemporal optical vortex (STOV) carrying transverse orbital angular\nmomentum (OAM) enriches the family of vortex beams and exhibit unique\nproperties. Typically, a high-order STOV with an intensity null degrades into\nmultiple first-order STOVs embedded within a single wave packet during\npropagation, a phenomenon known as time diffraction or mode degradation.\nHowever, this degradation limits the applicability of STOVs in specialized\nfields. Therefore, the generation of mode degradation-suppressed STOVs\n(MDS-STOVs) is of significant for both practical applications and theoretical\nstudies. Herein, we theoretically analyze the generation of MDS-STOVs by\nutilizing a conical phase to localize the energy of the STOV into a ring-shaped\nstructure. For MDS-STOVs with large topological charges (TCs), the ring-shaped\nprofile can be well-maintained, and the rapid expansion of the beam size with\nincreasing TC is significantly suppressed compared to conventional STOVs. As a\nresult, these MDS-STOVs can be regarded as quasi-perfect STOVs (QPSTOVs).\nFurthermore, QPSTOVs exhibit strong resistance to group delay dispersion (GDD),\neliminating the need for precise dispersion control and facilitating their\ngeneration and application. This work advances our understanding of the\nphysical properties of light carrying transverse OAM and opens up exciting\navenues for the application of STOVs in diverse fields, such as optical\ncommunication and quantum information processing.",
        "Vision-based autonomous racing relies on accurate perception for robust\ncontrol. However, image distribution changes caused by sensor noise, adverse\nweather, and dynamic lighting can degrade perception, leading to suboptimal\ncontrol decisions. Existing approaches, including domain adaptation and\nadversarial training, improve robustness but struggle to generalize to unseen\ncorruptions while introducing computational overhead. To address this\nchallenge, we propose a real-time image repair module that restores corrupted\nimages before they are used by the controller. Our method leverages generative\nadversarial models, specifically CycleGAN and pix2pix, for image repair.\nCycleGAN enables unpaired image-to-image translation to adapt to novel\ncorruptions, while pix2pix exploits paired image data when available to improve\nthe quality. To ensure alignment with control performance, we introduce a\ncontrol-focused loss function that prioritizes perceptual consistency in\nrepaired images. We evaluated our method in a simulated autonomous racing\nenvironment with various visual corruptions. The results show that our approach\nsignificantly improves performance compared to baselines, mitigating\ndistribution shift and enhancing controller reliability.",
        "Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal\nreasoning capabilities, but they remain susceptible to hallucination,\nparticularly object hallucination where non-existent objects or incorrect\nattributes are fabricated in generated descriptions. Existing detection methods\nachieve strong performance but rely heavily on expensive API calls and\niterative LVLM-based validation, making them impractical for large-scale or\noffline use. To address these limitations, we propose CutPaste\\&Find, a\nlightweight and training-free framework for detecting hallucinations in\nLVLM-generated outputs. Our approach leverages off-the-shelf visual and\nlinguistic modules to perform multi-step verification efficiently without\nrequiring LVLM inference. At the core of our framework is a Visual-aid\nKnowledge Base that encodes rich entity-attribute relationships and associated\nimage representations. We introduce a scaling factor to refine similarity\nscores, mitigating the issue of suboptimal alignment values even for\nground-truth image-text pairs. Comprehensive evaluations on benchmark datasets,\nincluding POPE and R-Bench, demonstrate that CutPaste\\&Find achieves\ncompetitive hallucination detection performance while being significantly more\nefficient and cost-effective than previous methods.",
        "Prior work has found that transformers have an inconsistent ability to learn\nto answer latent two-hop questions -- questions of the form \"Who is Bob's\nmother's boss?\" We study why this is the case by examining how transformers'\ncapacity to learn datasets of two-hop questions and answers (two-hop QA) scales\nwith their size, motivated by prior work on transformer knowledge capacity for\nsimple factual memorization. We find that capacity scaling and generalization\nboth support the hypothesis that latent two-hop QA requires transformers to\nlearn each fact twice, while two-hop QA with chain of thought does not. We also\nshow that with appropriate dataset parameters, it is possible to \"trap\" very\nsmall models in a regime where they memorize answers to two-hop questions\nindependently, even though they would perform better if they could learn to\nanswer them with function composition. Our findings show that measurement of\ncapacity scaling can complement existing interpretability methods, though there\nare challenges in using it for this purpose.",
        "mRNA-based vaccines have become a major focus in the pharmaceutical industry.\nThe coding sequence as well as the Untranslated Regions (UTRs) of an mRNA can\nstrongly influence translation efficiency, stability, degradation, and other\nfactors that collectively determine a vaccine's effectiveness. However,\noptimizing mRNA sequences for those properties remains a complex challenge.\nExisting deep learning models often focus solely on coding region optimization,\noverlooking the UTRs. We present Helix-mRNA, a structured state-space-based and\nattention hybrid model to address these challenges. In addition to a first\npre-training, a second pre-training stage allows us to specialise the model\nwith high-quality data. We employ single nucleotide tokenization of mRNA\nsequences with codon separation, ensuring prior biological and structural\ninformation from the original mRNA sequence is not lost. Our model, Helix-mRNA,\noutperforms existing methods in analysing both UTRs and coding region\nproperties. It can process sequences 6x longer than current approaches while\nusing only 10% of the parameters of existing foundation models. Its predictive\ncapabilities extend to all mRNA regions. We open-source the model\n(https:\/\/github.com\/helicalAI\/helical) and model weights\n(https:\/\/huggingface.co\/helical-ai\/helix-mRNA).",
        "The multivariate generalized Pareto distribution (mGPD) is a common method\nfor modeling extreme threshold exceedance probabilities in environmental and\nfinancial risk management. Despite its broad applicability, mGPD faces\nchallenges due to the infinite possible parametrizations of its dependence\nfunction, with only a few parametric models available in practice. To address\nthis limitation, we introduce GPDFlow, an innovative mGPD model that leverages\nnormalizing flows to flexibly represent the dependence structure. Unlike\ntraditional parametric mGPD approaches, GPDFlow does not impose explicit\nparametric assumptions on dependence, resulting in greater flexibility and\nenhanced performance. Additionally, GPDFlow allows direct inference of marginal\nparameters, providing insights into marginal tail behavior. We derive tail\ndependence coefficients for GPDFlow, including a bivariate formulation, a\n$d$-dimensional extension, and an alternative measure for partial exceedance\ndependence. A general relationship between the bivariate tail dependence\ncoefficient and the generative samples from normalizing flows is discussed.\nThrough simulations and a practical application analyzing the risk among five\nmajor US banks, we demonstrate that GPDFlow significantly improves modeling\naccuracy and flexibility compared to traditional parametric methods."
      ]
    }
  },
  {
    "id":2411.14752,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"\u201cApr\u00e8s Mois, Le D\u00e9luge\u201d: Preparing for the Coming Data Flood in the MRI-Guided Radiotherapy Era",
    "start_abstract":"Magnetic resonance imaging provides a sea of quantitative and semi-quantitative data. While radiation oncologists already navigate pool clinical (semantic) data, the tide will swell with advent hybrid MRI\/linear accelerator devices increasing interest in MRI-guided radiotherapy (MRIgRT), including adaptive MRIgRT. The variety MR sequences (of greater complexity than single parameter Hounsfield unit CT scanning routinely used radiotherapy), workflow fractionation, sheer quantity daily images acquired are challenges for scaling this technology. Biomedical informatics, which is science information biomedicine, can provide helpful insights looming transition. Funneling MRIgRT data into clinically meaningful streams requires committing to flow inter-institutional accessibility interoperability initiatives, standardizing dosimetry methods, streamlining linear workflow, MRI acquisition post-processing, current topic review attempt conceptually ford using informatics approaches as theoretical bridge.",
    "start_categories":[
      "Oncology"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation",
        "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
      ],
      "abstract":[
        "There has been exploding interest in embracing Transformer-based architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel networks, to prevent performance saturation on limited medical data, 4) Compound scaling at multiple levels (depth, width, kernel size) of MedNeXt. This leads to state-of-the-art performance on 4 tasks on CT and MRI modalities and varying dataset sizes, representing a modernized deep architecture for medical image segmentation. Our code is made publicly available at: https:\/\/github.com\/MIC-DKFZ\/MedNeXt.",
        "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "A Survey on Algorithmic Developments in Optimal Transport Problem with\n  Applications",
        "Multi-Lepton Jets from Quadruple $Z'$ via the Higgs Decay at LHC",
        "Human-Aided Trajectory Planning for Automated Vehicles through\n  Teleoperation and Arbitration Graphs",
        "Prototype Contrastive Consistency Learning for Semi-Supervised Medical\n  Image Segmentation",
        "A Conditional Point Cloud Diffusion Model for Deformable Liver Motion\n  Tracking Via a Single Arbitrarily-Angled X-ray Projection",
        "No Evidence for LLMs Being Useful in Problem Reframing",
        "Retracing the Cold Plasma Dispersion Law in Pulsar B0329+54: New\n  Insights into Frequency-Dependent Dispersion Measures",
        "KM-UNet KAN Mamba UNet for medical image segmentation",
        "MAPN: Enhancing Heterogeneous Sparse Graph Representation by Mamba-based\n  Asynchronous Aggregation",
        "Asymptotic Optimism of Random-Design Linear and Kernel Regression Models",
        "Some remarks on singular capillary cones with free boundary",
        "Characterization of Fractal Basins Using Deep Convolutional Neural\n  Networks",
        "Investigating Vulnerabilities of GPS Trip Data to Trajectory-User\n  Linking Attacks",
        "Simulating Raman Scattering Impairments with Depolarization Noise in\n  Quantum-Classical Links",
        "Finite Gr\\\"obner bases for quantum symmetric groups",
        "Fully-heavy tetraquarks in the vacuum and in a hot environment",
        "The GALAH survey: Improving chemical abundances using star clusters",
        "MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D\n  CT Image Synthesis",
        "PixelWorld: Towards Perceiving Everything as Pixels",
        "Numerical Solution and Errors Analysis of Iterative Method for a\n  Nonlinear Plate Bending Problem",
        "C2GM: Cascading Conditional Generation of Multi-scale Maps from Remote\n  Sensing Images Constrained by Geographic Features",
        "SLCGC: A lightweight Self-supervised Low-pass Contrastive Graph\n  Clustering Network for Hyperspectral Images",
        "Brief analysis of DeepSeek R1 and its implications for Generative AI",
        "$L^2$ decay estimates of weak solutions to 3D fractional MHD equations\n  in exterior domains",
        "Multi-agent Multi-armed Bandits with Minimum Reward Guarantee Fairness",
        "Energy Dispersion, Superconductivity and Magnetic Fluctuations in\n  Stacked Altermagnetism Materials",
        "Scalable Trajectory-User Linking with Dual-Stream Representation\n  Networks",
        "Counting two-step nilpotent wildly ramified extensions of function\n  fields",
        "Towards a Multimodal MRI-Based Foundation Model for Multi-Level Feature\n  Exploration in Segmentation, Molecular Subtyping, and Grading of Glioma"
      ],
      "abstract":[
        "Optimal Transport (OT) has established itself as a robust framework for\nquantifying differences between distributions, with applications that span\nfields such as machine learning, data science, and computer vision. This paper\noffers a detailed examination of the OT problem, beginning with its theoretical\nfoundations, including the classical formulations of Monge and Kantorovich and\ntheir extensions to modern computational techniques. It explores cutting-edge\nalgorithms, including Sinkhorn iterations, primal-dual strategies, and\nreduction-based approaches, emphasizing their efficiency and scalability in\naddressing high-dimensional problems. The paper also highlights emerging\ntrends, such as integrating OT into machine learning frameworks, the\ndevelopment of novel problem variants, and ongoing theoretical advancements.\nApplications of OT are presented across a range of domains, with particular\nattention to its innovative application in time series data analysis via\nOptimal Transport Warping (OTW), a robust alternative to methods like Dynamic\nTime Warping. Despite the significant progress made, challenges related to\nscalability, robustness, and ethical considerations remain, necessitating\nfurther research. The paper underscores OT's potential to bridge theoretical\ndepth and practical utility, fostering impactful advancements across diverse\ndisciplines.",
        "We investigate multi-lepton jet events from the decay of the 125 GeV Higgs\nboson ($h$) into quadruple new gauge bosons $(Z')$ at the LHC. Such an exotic\ndecay is realized via the process of $h \\to \\phi \\phi \\to Z'Z'Z'Z'$ with new\nscalar boson $\\phi$ in models with an additional $U(1)$ gauge symmetry. Charged\nleptons coming from the $Z'$ decay tend to be observed as lepton-jets rather\nthan isolated leptons when the masses of $Z'$ and $\\phi$ are smaller than\n${\\cal O}$(10) GeV, because of the highly-boosted effects. Performing the\nsignal and background analyses, we find that the branching ratio of $h \\to 4Z'$\nis maximally constrained to be smaller than of order $10^{-6}$ ($10^{-7}$) by\nusing the muonic-lepton jets assuming the integrated luminosity of 140\nfb$^{-1}$ (3000 fb$^{-1}$) at LHC. For lighter $Z'$ ($< 2m_\\mu$), we can use\nthe electronic-lepton jets instead of the muon-jets, by which the upper limit\non the branching ratio is obtained to be of order $10^{-6}$-$10^{-5}$. These\nbounds can be converted into the constraint on model parameters such as a\nmixing angle between $h$ and $\\phi$. It is shown that stronger bounds on the\nmixing angle are obtained in the dark photon case as compared with the previous\nconstraints given by flavor experiments and the Higgs decay $h \\to Z'Z'$ in the\nmass range of $m_{Z'}\\lesssim 10$ GeV.",
        "Teleoperation enables remote human support of automated vehicles in scenarios\nwhere the automation is not able to find an appropriate solution. Remote\nassistance concepts, where operators provide discrete inputs to aid specific\nautomation modules like planning, is gaining interest due to its reduced\nworkload on the human remote operator and improved safety. However, these\nconcepts are challenging to implement and maintain due to their deep\nintegration and interaction with the automated driving system. In this paper,\nwe propose a solution to facilitate the implementation of remote assistance\nconcepts that intervene on planning level and extend the operational design\ndomain of the vehicle at runtime. Using arbitration graphs, a modular\ndecision-making framework, we integrate remote assistance into an existing\nautomated driving system without modifying the original software components.\nOur simulative implementation demonstrates this approach in two use cases,\nallowing operators to adjust planner constraints and enable trajectory\ngeneration beyond nominal operational design domains.",
        "Medical image segmentation is a crucial task in medical image analysis, but\nit can be very challenging especially when there are less labeled data but with\nlarge unlabeled data. Contrastive learning has proven to be effective for\nmedical image segmentation in semi-supervised learning by constructing\ncontrastive samples from partial pixels. However, although previous contrastive\nlearning methods can mine semantic information from partial pixels within\nimages, they ignore the whole context information of unlabeled images, which is\nvery important to precise segmentation. In order to solve this problem, we\npropose a novel prototype contrastive learning method called Prototype\nContrastive Consistency Segmentation (PCCS) for semi-supervised medical image\nsegmentation. The core idea is to enforce the prototypes of the same semantic\nclass to be closer and push the prototypes in different semantic classes far\naway from each other. Specifically, we construct a signed distance map and an\nuncertainty map from unlabeled images. The signed distance map is used to\nconstruct prototypes for contrastive learning, and then we estimate the\nprototype uncertainty from the uncertainty map as trade-off among prototypes.\nIn order to obtain better prototypes, based on the student-teacher\narchitecture, a new mechanism named prototype updating prototype is designed to\nassist in updating the prototypes for contrastive learning. In addition, we\npropose an uncertainty-consistency loss to mine more reliable information from\nunlabeled data. Extensive experiments on medical image segmentation demonstrate\nthat PCCS achieves better segmentation performance than the state-of-the-art\nmethods. The code is available at https:\/\/github.com\/comphsh\/PCCS.",
        "Deformable liver motion tracking using a single X-ray projection enables\nreal-time motion monitoring and treatment intervention. We introduce a\nconditional point cloud diffusion model-based framework for accurate and robust\nliver motion tracking from arbitrarily angled single X-ray projections\n(PCD-Liver), which estimates volumetric liver motion by solving deformable\nvector fields (DVFs) of a prior liver surface point cloud based on a single\nX-ray image. The model is patient-specific and consists of two main components:\na rigid alignment model to estimate the liver's overall shifts and a\nconditional point cloud diffusion model that further corrects for liver surface\ndeformations. Conditioned on motion-encoded features extracted from a single\nX-ray projection via a geometry-informed feature pooling layer, the diffusion\nmodel iteratively solves detailed liver surface DVFs in a projection\nangle-agnostic manner. The liver surface motion estimated by PCD-Liver serves\nas a boundary condition for a U-Net-based biomechanical model to infer internal\nliver motion and localize liver tumors. A dataset of ten liver cancer patients\nwas used for evaluation. The accuracy of liver point cloud motion estimation\nwas assessed using root mean square error (RMSE) and 95th-percentile Hausdorff\ndistance (HD95), while liver tumor localization error was quantified using\ncenter-of-mass error (COME). The mean (standard deviation) RMSE, HD95, and COME\nof the prior liver or tumor before motion estimation were 8.86(1.51) mm,\n10.88(2.56) mm, and 9.41(3.08) mm, respectively. After PCD-Liver motion\nestimation, the corresponding values improved to 3.59(0.28) mm, 4.29(0.62) mm,\nand 3.45(0.96) mm. Under highly noisy conditions, PCD-Liver maintained stable\nperformance. This study presents an accurate and robust framework for\ndeformable liver motion estimation and tumor localization in image-guided\nradiotherapy.",
        "Problem reframing is a designerly activity wherein alternative perspectives\nare created to recast what a stated design problem is about. Generating\nalternative problem frames is challenging because it requires devising novel\nand useful perspectives that fit the given problem context. Large language\nmodels (LLMs) could assist this activity via their generative capability.\nHowever, it is not clear whether they can help designers produce high-quality\nframes. Therefore, we asked if there are benefits to working with LLMs. To this\nend, we compared three ways of using LLMs (N=280): 1) free-form, 2) direct\ngeneration, and 3) a structured approach informed by a theory of reframing. We\nfound that using LLMs does not help improve the quality of problem frames. In\nfact, it increases the competence gap between experienced and inexperienced\ndesigners. Also, inexperienced ones perceived lower agency when working with\nLLMs. We conclude that there is no benefit to using LLMs in problem reframing\nand discuss possible factors for this lack of effect.",
        "Multiple studies have investigated potential frequency-dependent dispersion\nmeasures (DM) in PSR B0329+54, with sensitivities at levels of $10^{-3} \\,\n\\text{pc} \\, \\text{cm}^{-3}$ or higher, using frequencies below 1 GHz.\nUtilizing the extensive bandwidth of the upgraded Giant Meterwave Radio\nTelescope, we conducted simultaneous observations of this pulsar across a\nfrequency range of 300 to 1460 MHz. Our observations reveal a distinct point in\nthe pulse profile of PSR B0329+54 that appears to align remarkably well with\nthe cold-plasma dispersion law, resulting in a unique measured DM across the\nentire frequency range. In contrast, using times of arrival (ToAs) from widely\nadopted pulsar timing techniques (e.g., FFTFIT)-leads to frequency-dependent\nDMs. We investigated the potential causes of these frequency-dependent DMs in\nthis pulsar and their relationship with the underlying magnetic field geometry\ncorresponding to the radio emission. Our study reveals that all frequencies in\nthe range 300-1460 MHz originate from a region no larger than 204 km, and the\ndipolar magnetic-field geometry model indicates that the emission region is\ncentered at $\\sim$800 km from the star. This is the tightest constraint on the\nsize of the emission region reported so far for PSR B0329+54 at the given\nfrequencies, and it is at least five times more stringent than the existing\nemission height constraints based on the dipolar geometry model.",
        "Medical image segmentation is a critical task in medical imaging analysis.\nTraditional CNN-based methods struggle with modeling long-range dependencies,\nwhile Transformer-based models, despite their success, suffer from quadratic\ncomputational complexity. To address these limitations, we propose KM-UNet, a\nnovel U-shaped network architecture that combines the strengths of\nKolmogorov-Arnold Networks (KANs) and state-space models (SSMs). KM-UNet\nleverages the Kolmogorov-Arnold representation theorem for efficient feature\nrepresentation and SSMs for scalable long-range modeling, achieving a balance\nbetween accuracy and computational efficiency. We evaluate KM-UNet on five\nbenchmark datasets: ISIC17, ISIC18, CVC, BUSI, and GLAS. Experimental results\ndemonstrate that KM-UNet achieves competitive performance compared to\nstate-of-the-art methods in medical image segmentation tasks. To the best of\nour knowledge, KM-UNet is the first medical image segmentation framework\nintegrating KANs and SSMs. This work provides a valuable baseline and new\ninsights for the development of more efficient and interpretable medical image\nsegmentation systems. The code is open source at\nhttps:\/\/github.com\/2760613195\/KM_UNet\n  Keywords:KAN,Manba, state-space models,UNet, Medical image segmentation, Deep\nlearning",
        "Graph neural networks (GNNs) have become the state of the art for various\ngraph-related tasks and are particularly prominent in heterogeneous graphs\n(HetGs). However, several issues plague this paradigm: first, the difficulty in\nfully utilizing long-range information, known as over-squashing; second, the\ntendency for excessive message-passing layers to produce indistinguishable\nrepresentations, referred to as over-smoothing; and finally, the inadequacy of\nconventional MPNNs to train effectively on large sparse graphs. To address\nthese challenges in deep neural networks for large-scale heterogeneous graphs,\nthis paper introduces the Mamba-based Asynchronous Propagation Network (MAPN),\nwhich enhances the representation of heterogeneous sparse graphs. MAPN consists\nof two primary components: node sequence generation and semantic information\naggregation. Node sequences are initially generated based on meta-paths through\nrandom walks, which serve as the foundation for a spatial state model that\nextracts essential information from nodes at various distances. It then\nasynchronously aggregates semantic information across multiple hops and layers,\neffectively preserving unique node characteristics and mitigating issues\nrelated to deep network degradation. Extensive experiments across diverse\ndatasets demonstrate the effectiveness of MAPN in graph embeddings for various\ndownstream tasks underscoring its substantial benefits for graph representation\nin large sparse heterogeneous graphs.",
        "We derived the closed-form asymptotic optimism of linear regression models\nunder random designs, and generalizes it to kernel ridge regression. Using\nscaled asymptotic optimism as a generic predictive model complexity measure, we\nstudied the fundamental different behaviors of linear regression model, tangent\nkernel (NTK) regression model and three-layer fully connected neural networks\n(NN). Our contribution is two-fold: we provided theoretical ground for using\nscaled optimism as a model predictive complexity measure; and we show\nempirically that NN with ReLUs behaves differently from kernel models under\nthis measure. With resampling techniques, we can also compute the optimism for\nregression models with real data.",
        "We study minimizing singular cones with free boundary associated with the\ncapillarity problem. Precisely, we provide a stability criterion $\\`a$ la\nJerison-Savin for capillary hypersurfaces and show that, in dimensions up to\n$4$, minimizing cones with non-sign-changing mean curvature are flat. We apply\nthis criterion to minimizing capillary drops and, additionally, establish the\ninstability of non-trivial axially symmetric cones in dimensions up to $6$.\n  The main results are based on a Simons-type inequality for a class of convex,\nhomogeneous, symmetric functions of the principal curvatures, combined with a\nboundary condition specific to the capillary setting.",
        "Neural network models have recently demonstrated impressive prediction\nperformance in complex systems where chaos and unpredictability appear. In\nspite of the research efforts carried out on predicting future trajectories or\nimproving their accuracy compared to numerical methods, not sufficient work has\nbeen done by using deep learning techniques in which they characterize the\nunpredictability of chaotic systems or give a general view of the global\nunpredictability of a system. In this work we propose a novel approach based on\ndeep learning techniques to measure the fractal dimension of the basins of\nattraction of the Duffing oscillator for a variety of parameters. As a\nconsequence, we provide an algorithm capable of predicting fractal dimension\nmeasures as accurately as the conventional algorithm, but with a computation\nspeed about ten times faster.",
        "Open human mobility data is considered an essential basis for the profound\nresearch and analysis required for the transition to sustainable mobility and\nsustainable urban planning. Cycling data has especially been the focus of data\ncollection endeavors in recent years. Although privacy risks regarding location\ndata are widely known, practitioners often refrain from advanced privacy\nmechanisms to prevent utility losses. Removing user identifiers from trips is\nthereby deemed a major privacy gain, as it supposedly prevents linking single\ntrips to obtain entire movement patterns. In this paper, we propose a novel\nattack to reconstruct user identifiers in GPS trip datasets consisting of\nsingle trips, unlike previous ones that are dedicated to evaluating\ntrajectory-user linking in the context of check-in data. We evaluate the\nremaining privacy risk for users in such datasets and our empirical findings\nfrom two real-world datasets show that the risk of re-identification is\nsignificant even when personal identifiers have been removed, and that\ntruncation as a simple additional privacy mechanism may not be effective in\nprotecting user privacy. Further investigations indicate that users who\nfrequently visit locations that are only visited by a small number of others,\ntend to be more vulnerable to re-identification.",
        "We model spontaneous Raman scattering noise in polarization-encoded quantum\ncommunication channels co-propagating with classical signals using the\ndepolarization channel. Utilizing NetSquid simulations, we validate the model\nagainst demonstrations of qubit transmission, entanglement distribution, and\nteleportation.",
        "Non-commutative Gr\\\"obner bases of two-sided ideals are not necessarily\nfinite. Motivated by this, we provide a closed-form description of a finite and\nreduced Gr\\\"obner bases for the two-sided ideal used in the construction of\nWangs quantum symmetric group. In particular, this proves that the word problem\nfor quantum symmetric groups is decidable.",
        "We study the thermal behavior of quarkonia and fully-heavy tetraquark states\nassociated to the charmonium, bottomonium and bottom-charmonium mass spectra.\nThe starting point is the Schr\\\"odinger formalism with a vacuum Cornell-like\npotential. The spin-spin, spin-orbit and tensor contributions are also\nconsidered to describe the structure of the vacuum quarkonia $Q\\bar Q$ spectra\n($Q$ denoting $c,b$ quarks). The parameters of the model are fixed using the\nexperimental data of the $Q\\bar Q$ states. After that, this formalism is\nextended to the fully-heavy tetraquark states within the $1^3 S_1$ axial\ndiquark--$1^3 S_1$ axial antidiquark configuration $[QQ] [\\bar Q \\bar Q]$, and\ntheir vacuum mass spectra are obtained and compared to the experimental data\nrecently obtained. Our predictions support the interpretation of the $X(6600)$\n(or $X(6552)$), $X(6900)$ and $X(7200)$ states as the radially-excited\n$T_{4c}(n^1S_0)$ configurations with $n=2,3,4$. In the sequence, we evaluate\nthe mass spectra behavior in a thermal medium, by introducing a modified\ntemperature-dependent Cornell potential. As a consequence, this formalism\nenables us to get some insight into the dissociation mechanism of $[QQ] [\\bar Q\n\\bar Q]$ states caused by a thermal medium, and into the temperature range at\nwhich the tetraquark states might be formed. We find that these structures\ncannot be formed in the thermal medium when the system has a temperature higher\nthan about twice the critical temperature. These findings may be useful to\nbetter understand the features of the exotics in heavy-ion collisions.",
        "Large spectroscopic surveys aim to consistently compute stellar parameters of\nvery diverse stars while minimizing systematic errors. We explore the use of\nstellar clusters as benchmarks to verify the precision of spectroscopic\nparameters in the 4. data release (DR4) of the GALAH survey. We examine 58 open\nand globular clusters and associations to validate measurements of temperature,\ngravity, chemical abundances, and stellar ages. We focus on identifying\nsystematic errors and understanding trends between stellar parameters,\nparticularly temperature and chemical abundances. We identify trends by\nstacking measurements of chemical abundances against effective temperature and\nmodelling them with splines. We also refit spectra in three clusters with the\nSpectroscopy Made Easy and Korg packages to reproduce the trends in DR4 and to\nsearch for their origin by varying temperature and gravity priors, linelists,\nand spectral continuum. Trends are consistent between clusters of different\nages and metallicities, can reach amplitudes of ~0.5 dex and differ for dwarfs\nand giants. We use the derived trends to correct the DR4 abundances of 24 and\n31 chemical elements for dwarfs and giants, and publish a detrended catalogue.\nWhile the origin of the trends could not be pinpointed, we found that: i)\nphotometric priors affect derived abundances, ii) temperature, metallicity, and\ncontinuum levels are degenerate in spectral fitting, and it is hard to break\nthe degeneracy even by using independent measurements, iii) the completeness of\nthe linelist used in spectral synthesis is essential for cool stars, and iv)\ndifferent spectral fitting codes produce significantly different iron\nabundances for stars of all temperatures. We conclude that clusters can be used\nto characterise the systematic errors of parameters produced in large surveys,\nbut further research is needed to explain the origin of the trends.",
        "Advancements in AI for medical imaging offer significant potential. However,\ntheir applications are constrained by the limited availability of data and the\nreluctance of medical centers to share it due to patient privacy concerns.\nGenerative models present a promising solution by creating synthetic data as a\nsubstitute for real patient data. However, medical images are typically\nhigh-dimensional, and current state-of-the-art methods are often impractical\nfor computational resource-constrained healthcare environments. These models\nrely on data sub-sampling, raising doubts about their feasibility and\nreal-world applicability. Furthermore, many of these models are evaluated on\nquantitative metrics that alone can be misleading in assessing the image\nquality and clinical meaningfulness of the generated images. To address this,\nwe introduce MedLoRD, a generative diffusion model designed for computational\nresource-constrained environments. MedLoRD is capable of generating\nhigh-dimensional medical volumes with resolutions up to\n512$\\times$512$\\times$256, utilizing GPUs with only 24GB VRAM, which are\ncommonly found in standard desktop workstations. MedLoRD is evaluated across\nmultiple modalities, including Coronary Computed Tomography Angiography and\nLung Computed Tomography datasets. Extensive evaluations through radiological\nevaluation, relative regional volume analysis, adherence to conditional masks,\nand downstream tasks show that MedLoRD generates high-fidelity images closely\nadhering to segmentation mask conditions, surpassing the capabilities of\ncurrent state-of-the-art generative models for medical image synthesis in\ncomputational resource-constrained environments.",
        "Existing foundation models typically process visual input as pixels and\ntextual input as tokens, a paradigm that contrasts with human perception, where\nboth modalities are processed in a unified manner. With the rise of embodied\nand agentic AI, where inputs primarily come from camera pixels, the need for a\nunified perception framework becomes increasingly evident. In this paper, we\npropose to unify all modalities (text, tables, code, diagrams, images, etc) as\npixel inputs, i.e. \"Perceive Everything as Pixels\" (PEAP). We introduce\nPixelWorld, a novel evaluation suite that unifies all the mentioned modalities\ninto pixel space to gauge the existing models' performance. Our findings show\nthat (1) PEAP outperforms baseline with token-based input in multimodal\ndatasets, benefiting from unified input for better disambiguation, (2)\nsignificant declines in reasoning and coding capabilities across all models\nwhen processing pixel-based input, underscoring the need to enhance foundation\nmodels' perceptual abilities, (3) larger models can maintain strong performance\non non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer\nsignificant performance degradation, (4) the attention pattern of PEAP is\nhighly aligned with text token input, (5) PEAP can be accelerated significantly\nby exploiting the spatial sparsity. We conclude that the existing frontier\nmodels are competent in pixel perception, however, there is still headroom for\nimprovement. Our code, dataset will be released upon acceptance.",
        "This paper uses the HCT finite element method and mesh adaptation technology\nto solve the nonlinear plate bending problem and conducts error analysis on the\niterative method, including a priori and a posteriori error estimates. Our\ninvestigation exploits Hermite finite elements such as BELL and\nHSIEH-CLOUGH-TOCHER (HCT) triangles for conforming finite element\ndiscretization. Then, the existence and uniqueness of the approximation\nsolution are proven by using a variant of the Brezzi-Rappaz-Raviart theorem. We\nsolve the approximation problem through a fixed-point strategy and an iterative\nalgorithm, and study the convergence of the iterative algorithm, and provide\nthe convergence conditions. An optimal a priori error estimation has been\nestablished. We construct a posteriori error indicators by distinguishing\nbetween discretization and linearization errors and prove their reliability and\noptimality. A numerical test is carried out and the results obtained confirm\nthose established theoreticall.",
        "Multi-scale maps are essential representations of surveying and cartographic\nresults, serving as fundamental components of geographic services. Current\nimage generation networks can quickly produce map tiles from remote-sensing\nimages. However, generative models designed for natural images often focus on\ntexture features, neglecting the unique characteristics of remote-sensing\nfeatures and the scale attributes of tile maps. This limitation in generative\nmodels impairs the accurate representation of geographic information, and the\nquality of tile map generation still needs improvement. Diffusion models have\ndemonstrated remarkable success in various image generation tasks, highlighting\ntheir potential to address this challenge. This paper presents C2GM, a novel\nframework for generating multi-scale tile maps through conditional guided\ndiffusion and multi-scale cascade generation. Specifically, we implement a\nconditional feature fusion encoder to extract object priors from remote sensing\nimages and cascade reference double branch input, ensuring an accurate\nrepresentation of complex features. Low-level generated tiles act as\nconstraints for high-level map generation, enhancing visual continuity.\nMoreover, we incorporate map scale modality information using CLIP to simulate\nthe relationship between map scale and cartographic generalization in tile\nmaps. Extensive experimental evaluations demonstrate that C2GM consistently\nachieves the state-of-the-art (SOTA) performance on all metrics, facilitating\nthe rapid and effective generation of multi-scale large-format maps for\nemergency response and remote mapping applications.",
        "Self-supervised hyperspectral image (HSI) clustering remains a fundamental\nyet challenging task due to the absence of labeled data and the inherent\ncomplexity of spatial-spectral interactions. While recent advancements have\nexplored innovative approaches, existing methods face critical limitations in\nclustering accuracy, feature discriminability, computational efficiency, and\nrobustness to noise, hindering their practical deployment. In this paper, a\nself-supervised efficient low-pass contrastive graph clustering (SLCGC) is\nintroduced for HSIs. Our approach begins with homogeneous region generation,\nwhich aggregates pixels into spectrally consistent regions to preserve local\nspatial-spectral coherence while drastically reducing graph complexity. We then\nconstruct a structural graph using an adjacency matrix A and introduce a\nlow-pass graph denoising mechanism to suppress high-frequency noise in the\ngraph topology, ensuring stable feature propagation. A dual-branch graph\ncontrastive learning module is developed, where Gaussian noise perturbations\ngenerate augmented views through two multilayer perceptrons (MLPs), and a\ncross-view contrastive loss enforces structural consistency between views to\nlearn noise-invariant representations. Finally, latent embeddings optimized by\nthis process are clustered via K-means. Extensive experiments and repeated\ncomparative analysis have verified that our SLCGC contains high clustering\naccuracy, low computational complexity, and strong robustness. The code source\nwill be available at https:\/\/github.com\/DY-HYX.",
        "In late January 2025, DeepSeek released their new reasoning model (DeepSeek\nR1); which was developed at a fraction of the cost yet remains competitive with\nOpenAI's models, despite the US's GPU export ban. This report discusses the\nmodel, and what its release means for the field of Generative AI more widely.\nWe briefly discuss other models released from China in recent weeks, their\nsimilarities; innovative use of Mixture of Experts (MoE), Reinforcement\nLearning (RL) and clever engineering appear to be key factors in the\ncapabilities of these models. This think piece has been written to a tight\ntimescale, providing broad coverage of the topic, and serves as introductory\nmaterial for those looking to understand the model's technical advancements, as\nwell as its place in the ecosystem. Several further areas of research are\nidentified.",
        "Consider three-dimensional fractional MHD equations in an exterior domain\nwith the Dirichlet boundary condition assumed. Asymptotic behaviours of weak\nsolutions to the three-dimensional exterior fractional MHD equations are\nstudied. $L^2$ decay estimates of the weak solutions are obtained.",
        "We investigate the problem of maximizing social welfare while ensuring\nfairness in a multi-agent multi-armed bandit (MA-MAB) setting. In this problem,\na centralized decision-maker takes actions over time, generating random rewards\nfor various agents. Our goal is to maximize the sum of expected cumulative\nrewards, a.k.a. social welfare, while ensuring that each agent receives an\nexpected reward that is at least a constant fraction of the maximum possible\nexpected reward.\n  Our proposed algorithm, RewardFairUCB, leverages the Upper Confidence Bound\n(UCB) technique to achieve sublinear regret bounds for both fairness and social\nwelfare. The fairness regret measures the positive difference between the\nminimum reward guarantee and the expected reward of a given policy, whereas the\nsocial welfare regret measures the difference between the social welfare of the\noptimal fair policy and that of the given policy.\n  We show that RewardFairUCB algorithm achieves instance-independent social\nwelfare regret guarantees of $\\tilde{O}(T^{1\/2})$ and a fairness regret upper\nbound of $\\tilde{O}(T^{3\/4})$. We also give the lower bound of\n$\\Omega(\\sqrt{T})$ for both social welfare and fairness regret. We evaluate\nRewardFairUCB's performance against various baseline and heuristic algorithms\nusing simulated data and real world data, highlighting trade-offs between\nfairness and social welfare regrets.",
        "Recently, altermagnetism (AM) has emerged as a new category of magnetism,\nalongside conventional antiferromagnetism (AFM) and ferromagnetism (FM). In an\nAM, superconductivity (SC) is faced with a dilemma that the spin-polarized\nbands, induced by the broken time reversal (T ) symmetry, dominantly supports\nspin-triplet pairing. In contrast, AM spin fluctuations routinely facilitate\nspin-singlet pairing as in AFM. Consequently, unconventional SC is either\nabsent or weak in AM materials. Here, we propose that stacking 2D AM materials\ncould resolve this dilemma. Stacked 2D materials have yielded a variety of new\nelectronic properties by altering the symmetries inherent in the monolayer. In\na 2D anisotropic Hubbard model, we investigate the general energy dispersions\nof both single-layer and stacked AM materials. We demonstrate that AM sheet\nstacking can alter the original symmetries, consequently affecting the energy\ndispersion. The interlayer magnetic coupling enhances the low q magnetic\nfluctuations. T symmetry is restored in the AA stacking with an\nantiferromagnetic interlayer coupling, and then both the energy dispersion and\npairing interaction are in favor of spin-singlet SC. The ferromagnetic\ninterlayer coupling in the AB stacking not only recovers T symmetry but also\nsupports spin-triplet pairing. It is further anticipated that twisted bilayer\nAM sheets could exhibit additional novel electronic properties, including\ntopology, flat bands, and collective excitations. Our work illustrates that\nstacking sheets of AM materials could open up a unique research domain in\nexploring novel quantum phenomena and offer a fertile ground for potential\nelectronic applications.",
        "Trajectory-user linking (TUL) aims to match anonymous trajectories to the\nmost likely users who generated them, offering benefits for a wide range of\nreal-world spatio-temporal applications. However, existing TUL methods are\nlimited by high model complexity and poor learning of the effective\nrepresentations of trajectories, rendering them ineffective in handling\nlarge-scale user trajectory data. In this work, we propose a novel\n$\\underline{Scal}$abl$\\underline{e}$ Trajectory-User Linking with dual-stream\nrepresentation networks for large-scale $\\underline{TUL}$ problem, named\nScaleTUL. Specifically, ScaleTUL generates two views using temporal and spatial\naugmentations to exploit supervised contrastive learning framework to\neffectively capture the irregularities of trajectories. In each view, a\ndual-stream trajectory encoder, consisting of a long-term encoder and a\nshort-term encoder, is designed to learn unified trajectory representations\nthat fuse different temporal-spatial dependencies. Then, a TUL layer is used to\nassociate the trajectories with the corresponding users in the representation\nspace using a two-stage training model. Experimental results on check-in\nmobility datasets from three real-world cities and the nationwide U.S.\ndemonstrate the superiority of ScaleTUL over state-of-the-art baselines for\nlarge-scale TUL tasks.",
        "We study the asymptotic distribution of wildly ramified extensions of\nfunction fields in characteristic $p > 2$, focusing on (certain) $p$-groups of\nnilpotency class at most $2$. Rather than the discriminant, we count extensions\naccording to an invariant describing the last jump in the ramification\nfiltration at each place. We prove a local-global principle relating the\ndistribution of extensions over global function fields to their distribution\nover local fields, leading to an asymptotic formula for the number of\nextensions with a given global last-jump invariant. A key ingredient is\nAbrashkin's nilpotent Artin-Schreier theory, which lets us parametrize\nextensions and obtain bounds on the ramification of local extensions by\nestimating the number of solutions to certain polynomial equations over finite\nfields.",
        "Accurate, noninvasive glioma characterization is crucial for effective\nclinical management. Traditional methods, dependent on invasive tissue\nsampling, often fail to capture the spatial heterogeneity of the tumor. While\ndeep learning has improved segmentation and molecular profiling, few approaches\nsimultaneously integrate tumor morphology and molecular features. Foundation\ndeep learning models, which learn robust, task-agnostic representations from\nlarge-scale datasets, hold great promise but remain underutilized in glioma\nimaging biomarkers. We propose the Multi-Task SWIN-UNETR (MTS-UNET) model, a\nnovel foundation-based framework built on the BrainSegFounder model, pretrained\non large-scale neuroimaging data. MTS-UNET simultaneously performs glioma\nsegmentation, histological grading, and molecular subtyping (IDH mutation and\n1p\/19q co-deletion). It incorporates two key modules: Tumor-Aware Feature\nEncoding (TAFE) for multi-scale, tumor-focused feature extraction and\nCross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch\nsignals associated with IDH mutation. The model was trained and validated on a\ndiverse, multi-center cohort of 2,249 glioma patients from seven public\ndatasets. MTS-UNET achieved a mean Dice score of 84% for segmentation, along\nwith AUCs of 90.58% for IDH mutation, 69.22% for 1p\/19q co-deletion prediction,\nand 87.54% for grading, significantly outperforming baseline models (p<=0.05).\nAblation studies validated the essential contributions of the TAFE and CMD\nmodules and demonstrated the robustness of the framework. The foundation-based\nMTS-UNET model effectively integrates tumor segmentation with multi-level\nclassification, exhibiting strong generalizability across diverse MRI datasets.\nThis framework shows significant potential for advancing noninvasive,\npersonalized glioma management by improving predictive accuracy and\ninterpretability."
      ]
    }
  },
  {
    "id":2411.14752,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation",
    "start_abstract":"There has been exploding interest in embracing Transformer-based architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel networks, to prevent performance saturation on limited medical data, 4) Compound scaling at multiple levels (depth, width, kernel size) of MedNeXt. This leads to state-of-the-art performance on 4 tasks on CT and MRI modalities and varying dataset sizes, representing a modernized deep architecture for medical image segmentation. Our code is made publicly available at: https:\/\/github.com\/MIC-DKFZ\/MedNeXt.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "\u201cApr\u00e8s Mois, Le D\u00e9luge\u201d: Preparing for the Coming Data Flood in the MRI-Guided Radiotherapy Era"
      ],
      "abstract":[
        "Magnetic resonance imaging provides a sea of quantitative and semi-quantitative data. While radiation oncologists already navigate pool clinical (semantic) data, the tide will swell with advent hybrid MRI\/linear accelerator devices increasing interest in MRI-guided radiotherapy (MRIgRT), including adaptive MRIgRT. The variety MR sequences (of greater complexity than single parameter Hounsfield unit CT scanning routinely used radiotherapy), workflow fractionation, sheer quantity daily images acquired are challenges for scaling this technology. Biomedical informatics, which is science information biomedicine, can provide helpful insights looming transition. Funneling MRIgRT data into clinically meaningful streams requires committing to flow inter-institutional accessibility interoperability initiatives, standardizing dosimetry methods, streamlining linear workflow, MRI acquisition post-processing, current topic review attempt conceptually ford using informatics approaches as theoretical bridge."
      ],
      "categories":[
        "Oncology"
      ]
    },
    "list":{
      "title":[
        "Soliquidy: a descriptor for atomic geometrical confusion",
        "Development of the Timing System for the X-Ray Imaging and Spectroscopy\n  Mission",
        "Statistical Challenges in Analyzing Migrant Backgrounds Among University\n  Students: a Case Study from Italy",
        "On the equivalence between galaxy angular correlation function and power\n  spectrum in constraining primordial non-Gaussianity",
        "Two-loop light-quark Electroweak corrections to Higgs boson pair\n  production in gluon fusion",
        "The Change of Variable Formula Integrals, do they have equal value?",
        "Molecular laser cooling using serrodynes: Implementation,\n  characterization and prospects",
        "Optimal control in combination therapy for heterogeneous cell\n  populations with drug synergies",
        "The surface binding and energy issues in rational design of the\n  separation membrane of Li||S batteries",
        "Rigidity of anti-de Sitter (2+1)-spacetimes with convex boundary near\n  the Fuchsian locus",
        "Transferable Foundation Models for Geometric Tasks on Point Cloud\n  Representations: Geometric Neural Operators",
        "First generation 4H-SiC LGAD production and its performance evaluation",
        "Quadratic BSDEs with Singular Generators and Unbounded Terminal\n  Conditions: Theory and Applications",
        "Game of grounds",
        "Sharp stability for critical points of the Sobolev inequality in the\n  absence of bubbling",
        "Ideal MHD. Part II: Rigidity from infinity for ideal Alfv\\'en waves in\n  3D thin domains",
        "Thin Spectra for Periodic and Ergodic Word Models",
        "Peaky Finders: Characterizing Double-Peaked Type IIb Supernovae in\n  Large-Scale Live-Stream Photometric Surveys",
        "Benefits of Early Stopping in Gradient Descent for Overparameterized\n  Logistic Regression",
        "Combining Physics and Mathematics Learning: A Taylor Series Analysis of\n  an Oscillating Magnetic Field",
        "Exceptional field theories",
        "A single-component regularity criterion and Inviscid limit of axially\n  symmetric MHD-Boussinesq system",
        "Sophomore's dream function: asymptotics, complex plane behavior and\n  relation to the error function",
        "Quantum critical electro-optic and piezo-electric nonlinearities",
        "Ion-kinetic-energy sampling in a 22-pole trap using ring-electrode\n  evaporation",
        "The category of anyon sectors for non-abelian quantum double models",
        "Averaging method for quasi-periodic response solutions",
        "First-principles Investigation of Exceptional Coarsening-resistant\n  V-Sc(Al2Cu)4 Nanoprecipitates in Al-Cu-Mg-Ag-Sc Alloys",
        "Counterdiabatic-influenced Floquet-engineering: State preparation,\n  annealing and learning the adiabatic gauge potential"
      ],
      "abstract":[
        "Tailoring material properties often requires understanding the solidification\nprocess. Herein, we introduce the geometric descriptor Soliquidy, which\nnumerically captures the Euclidean transport cost between the translationally\ndisordered versus ordered states of a materials. As a testbed, we apply\nSoliquidy to the classification of glass-forming metal alloys. By extending and\ncombining an experimental library of metallic thin-films (glass\/no-glass) with\nthe aflow.org computational database (geometrical and energetic information of\nmixtures) we found that the combination of Soliquity and formation enthalpies\ngenerates an effective classifier for glass formation. Such classifier is then\nused to tackle a public dataset of metallic glasses showing that the\nglass-agnostic assumptions of Soliquity can be useful for understanding\nkinetically-controlled phase transitions.",
        "This paper describes the development, design, ground verification, and\nin-orbit verification, performance measurement, and calibration of the timing\nsystem for the X-Ray Imaging and Spectroscopy Mission (XRISM). The scientific\ngoals of the mission require an absolute timing accuracy of 1.0~ms. All\ncomponents of the timing system were designed and verified to be within the\ntiming error budgets, which were assigned by component to meet the\nrequirements. After the launch of XRISM, the timing capability of the\nground-tuned timing system was verified using the millisecond pulsar\nPSR~B1937+21 during the commissioning period, and the timing jitter of the bus\nand the ground component were found to be below $15~\\mu$s compared to the NICER\n(Neutron star Interior Composition ExploreR) profile. During the performance\nverification and calibration period, simultaneous observations of the Crab\npulsar by XRISM, NuSTAR (Nuclear Spectroscopic Telescope Array), and NICER were\nmade to measure the absolute timing offset of the system, showing that the\narrival time of the main pulse with XRISM was aligned with that of NICER and\nNuSTAR to within $200~\\mu$s. In conclusion, the absolute timing accuracy of the\nbus and the ground component of the XRISM timing system meets the timing error\nbudget of $500~\\mu$s.",
        "The methodological issues and statistical complexities of analyzing\nuniversity students with migrant backgrounds is explored, focusing on Italian\ndata from the University of Milano-Bicocca. With the increasing size of migrant\npopulations and the growth of the second and middle generations, the need has\nrisen for deeper knowledge of the various strata of this population, including\nuniversity students with migrant backgrounds. This presents challenges due to\ninconsistent recording in university datasets. By leveraging both\nadministrative records and an original targeted survey we propose a methodology\nto fully identify the study population of students with migrant histories, and\nto distinguish relevant subpopulations within it such as second-generation born\nin Italy. Traditional logistic regression and machine learning random forest\nmodels are used and compared to predict migrant status. The primary\ncontribution lies in creating an expanded administrative dataset enriched with\nindicators of students' migrant backgrounds and status. The expanded dataset\nprovides a critical foundation for analyzing the characteristics of students\nwith migration histories across all variables routinely registered in the\nadministrative data set. Additionally, findings highlight the presence of\nselection bias in the targeted survey data, underscoring the need of further\nresearch.",
        "We investigate the angular power spectrum ($C_\\ell)$ and angular correlation\nfunction ($w(\\theta)$) of galaxy number density field in the presence of the\nlocal-type primordial non-Gaussianity (PNG), explicitly accounting for the\nintegral constraint in an all-sky survey. We show that the PNG signature in\n$C_{\\ell}$ is confined to low multipoles in the linear regime, whereas its\nsignature in $w(\\theta)$ extends across a wide range of angular scales,\nincluding those below the nonlinear scale. Therefore, the equivalence between\n$C_\\ell$ and $w(\\theta)$ can be violated when scale cuts of multipoles or\nangular scales -- for example, to mitigate systematic effects -- are applied in\nthe analysis. Assuming samples of photometric galaxies divided into multiple\nredshift bins in the range $0<z<7$, we forecast the precision of constraining\nthe PNG parameter ($f_{\\rm NL}$) from the hypothetical measurements of $C_\\ell$\nor $w(\\theta)$ assuming different scale cuts in the multipoles or angular\nscales, respectively. Our results imply that the PNG information can be\nextracted from $w(\\theta)$ on relatively small angular scales such as $\\lesssim\n10$ degree for a high-redshift galaxy sample or from $w(\\theta)$ measured in a\nsurvey with partial area coverage.",
        "We compute two-loop electroweak corrections to double Higgs boson production\nin gluon fusion mediated by light quarks in a fully analytical way. We\ndetermine a basis of master integrals satisfying canonical differential\nequations in $\\mathrm{d}\\log$ form, enhanced by subsequent rotations to remove\nunnecessary functions that do not appear in the analytic expressions of the\namplitudes. We determine the integration constants by matching our expressions\nto the large mass expansion limit of the canonical integrals. We express the\nsolution of differential equations in terms of Chen iterated integrals up to\ntranscendental weight six over logarithmic kernels with algebraic arguments,\nand further decompose them by employing a basis of uniform weight functions. By\nderiving differential equations for such basis, we provide numerical results as\nwell as routines for optimised numerical evaluations.",
        "Assuming that the two integrals in the Change of Variable Formula for the\nunidimensional Riemann integral are finite, one can ask if they have equal\nvalue. We give a positive answer to this question. The proof is very easy to\nfollow and to keep in mind. An example is given.",
        "An important effort is currently underway to extend optical cycling and laser\ncooling to more molecular species. Significant challenges arise in particular\nwhen multiple nuclear spins give rise to complex, resolved hyperfine spectra,\nas is the case for several molecular species relevant to precision tests of\nfundamental symmetries. We provide a detailed introduction to the use of\noptical spectra generated via serrodyne waveforms to address this complexity.\nWe discuss our experimental implementation of these serrodynes, characterize\ntheir properties, and outline procedures to find optimized sideband\nconfigurations that generate strong laser cooling forces. We demonstrate the\napplication of these techniques to barium monofluoride molecules and explore\ntheir prospects for the cooling of other species relevant to the study of\nfundamental physics.",
        "Cell heterogeneity plays an important role in patient responses to drug\ntreatments. In many cancers, it is associated with poor treatment outcomes.\nMany modern drug combination therapies aim to exploit cell heterogeneity, but\ndetermining how to optimise responses from heterogeneous cell populations while\naccounting for multi-drug synergies remains a challenge. In this work, we\nintroduce and analyse a general optimal control framework that can be used to\nmodel the treatment response of multiple cell populations that are treated with\nmultiple drugs that mutually interact. In this framework, we model the effect\nof multiple drugs on the cell populations using a system of coupled semi-linear\nordinary differential equations and derive general results for the optimal\nsolutions. We then apply this framework to three canonical examples and discuss\nthe wider question of how to relate mathematical optimality to clinically\nobservable outcomes, introducing a systematic approach to propose qualitatively\ndifferent classes of drug dosing inspired by optimal control.",
        "Lithium-sulfur batteries (LSBs) represent one of the most promising\nnext-generation energy storage technologies, offering exceptionally high energy\ndensities. However, their widespread adoption remains hindered by challenges\nsuch as sluggish conversion reactions and the dissolution of lithium\npolysulfides, which lead to poor cycling stability and reduced performance.\nWhile significant efforts have been made to address these limitations, the\nenergy storage capabilities of LSBs in practical devices remain far from\nachieving their full potential. This report delves into recent advancements in\nthe rational design of separation membranes for LSBs, focusing on addressing\nfundamental issues related to surface binding and surface energy interactions\nwithin materials science. By examining the functionalization and optimization\nof separation membranes, we aim to highlight strategies that can guide the\ndevelopment of more robust and efficient LSBs, bringing them closer to\npractical implementation.",
        "We prove that globally hyperbolic compact anti-de Sitter (2+1)-spacetimes\nwith strictly convex spacelike boundary that is either smooth or polyhedral and\nwhose holonomy is close to Fuchsian are determined by the induced metric on the\nboundary.",
        "We introduce methods for obtaining pretrained Geometric Neural Operators\n(GNPs) that can serve as basal foundation models for use in obtaining geometric\nfeatures. These can be used within data processing pipelines for machine\nlearning tasks and numerical methods. We show how our GNPs can be trained to\nlearn robust latent representations for the differential geometry of\npoint-clouds to provide estimates of metric, curvature, and other shape-related\nfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimate\nthe geometric properties of surfaces of arbitrary shape and topologies with\nrobustness in the presence of noise, (ii) to approximate solutions of geometric\npartial differential equations (PDEs) on manifolds, and (iii) to solve\nequations for shape deformations such as curvature driven flows. We also\nrelease a package of the codes and weights for using our pre-trained GNPs for\nprocessing point cloud representations. This allows for incorporating our\npre-trained GNPs as components for reuse within existing and new data\nprocessing pipelines. The GNPs also can be used as part of numerical solvers\ninvolving geometry or as part of methods for performing inference and other\ngeometric tasks.",
        "This contribution will delve into the design and performance of the newly\nproduced Silicon Carbide Low Gain Avalanche Detectors (4H-SiC LGADs) and\nprovide a comprehensive summary of their measured characteristics. This\nincludes an analysis of the detector's performance, temperature stability, and\nthe effectiveness of the internal gain layer in improving signal generation.\n  The 4H-SiC is re-emerging as a strong candidate for the next generation of\nsemiconductor detectors. This material offers several advantages, including\nhigh radiation tolerance and the ability to operate over a wide range of\ntemperatures without significant annealing effects. However, the signals\ngenerated by minimum ionizing particles in the 4H-SiC detector are lower\ncompared to the signal produced by standard silicon detectors due to their\nhigher bandgap energy. This is addressed by implementing a charge\nmultiplication layer, which results in the intrinsic gain of the device.\n  The presented 4H-SiC LGADs produced by onsemi are specifically designed and\noptimized for fabrication on the n-type substrate\/epi wafer with the gain layer\nimplanted approximately $1~\\mathrm{\\mu m}$ below the surface. The first\niteration of these LGAD structures was manufactured in early 2024 and since\nthen has been subjected to laboratory evaluation. The measured properties of\nthese detectors align well with the predictions arising from the extensive TCAD\nsimulation studies.",
        "We investigate a class of quadratic backward stochastic differential\nequations (BSDEs) with generators singular in $ y $. First, we establish the\nexistence of solutions and a comparison theorem, thereby extending results in\nthe literature. Additionally, we analyze the stability property and the\nFeynman-Kac formula, and prove the uniqueness of viscosity solutions for the\ncorresponding singular semilinear partial differential equations (PDEs).\nFinally, we demonstrate applications in the context of robust control linked to\nstochastic differential utility and certainty equivalent based on\n$g$-expectation. In these applications, the coefficient of the quadratic term\nin the generator captures the level of ambiguity aversion and the coefficient\nof absolute risk aversion, respectively.",
        "In this paper, we propose to connect Prawitz's theory of grounds with\nGirard's Ludics. This connection is carried out on two levels. On a more\nphilosophical one, we highlight some differences between Prawitz's and Girard's\napproaches, but we also argue that they share some basic ideas about proofs and\ndeduction. On a more formal one, we sketch an indicative translation of\nPrawitz's theory grounds into Girard's Ludics relative to the implicational\nfragment of propositional intuitionistic logic. This may allow for a dialogical\nreading of Prawitz's ground-theoretic approach. Moreover, it becomes possible\nto provide a formal definition of a notion of ground-candidate introduced by\nCozzo.",
        "When $u$ is close to a single Talenti bubble $v$ of the $p$-Sobolev\ninequality, we show that\n  \\begin{equation*}\n  \\|Du-Dv\\|_{L^p(\\mathbb{R}^n)}^{\\max\\{1,p-1\\}}\\le C \\|-{\\rm\ndiv}(|Du|^{p-2}Du)-|u|^{p^*-2}u\\|_{W^{-1,q}(\\mathbb{R}^n)}, \\end{equation*}\nwhere $C=C(n,p)>0$. This estimate provides a sharp stability estimate for the\nStruwe-type decomposition in the single bubble case, generalizing the result of\nCiraolo, Figalli, and Maggi \\cite{CFM2018} (focusing on the case $p=2$) to the\narbitrary $p$. Also, in the Sobolev setting, this answers an open problem\nraised by Zhou and Zou in \\cite[Remark 1.17]{ZZ2023}.",
        "This paper concerns the rigidity from infinity for Alfv\\'en waves governed by\nideal incompressible magnetohydrodynamic equations subjected to strong\nbackground magnetic fields along the $x_1$-axis in 3D thin domains\n$\\Omega_\\delta=\\mathbb{R}^2\\times(-\\delta,\\delta)$ with $\\delta\\in(0,1]$ and\nslip boundary conditions. We show that in any thin domain $\\Omega_\\delta$,\nAlfv\\'en waves must vanish identically if their scattering fields vanish at\ninfinities. As an application, the rigidity of Alfv\\'en waves in\n$\\Omega_{\\delta}$, propagating along the horizontal direction, can be\napproximated by the rigidity of Alfv\\'en waves in $\\mathbb{R}^2$ when $\\delta$\nis sufficiently small. Our proof relies on the uniform (with respect to\n$\\delta$) weighted energy estimates with a position parameter in weights to\ntrack the center of Alfv\\'en waves. The key issues in the analysis include\ndealing with the nonlinear nature of Alfv\\'en waves and the geometry of thin\ndomains.",
        "We establish a new and simple criterion that suffices to generate many\nspectral gaps for periodic word models. This leads to new examples of ergodic\nSchr\\\"odinger operators with Cantor spectra having zero Hausdorff dimension\nthat simultaneously may have arbitrarily small supremum norm together with\narbitrarily long runs on which the potential vanishes.",
        "We present the first photometric population study of double-peaked Type IIb\nsupernovae (SNe IIb). SNe IIb are produced from the core-collapse of massive\nstars whose outermost Hydrogen layer has been partially stripped prior to\nexplosion. These double-peaked light curves, consisting of a shock-cooling\nemission peak (SCE) followed by the main nickel-powered peak, contain more\ncrucial information about the progenitor system than the typical single-peaked\nlight curves. We compiled and analyzed a sample of 14 spectroscopically\nconfirmed SNe IIb -- including previously unpublished and re-classified -- with\npublicly available photometric observations, discovered between 2018--2022,\nfrom the ZTF and ATLAS surveys. We developed and fit a piecewise linear model,\nreferred to as the ``lightning bolt model,'' to describe the early-time\nbehavior of these objects to measure population statistics. Notably, we find\nthe SCE peak lasts, on average, fewer than five days above half-maximum light\nwith a mean rise time of $2.07\\pm1.0$ and $1.1\\pm0.8$ mags\/day in the g- and\nr-band respectively. These SCE rise rates are over 10x faster than -- and last\nonly a third the duration of -- the rise to the nickel-powered peak. These rise\ntimes are comparable to those of fast blue optical transient (FBOT) events and\nwe discuss the implications in the text. Finally, we present a proof-of-concept\nalert filter, using the ANTARES broker, to demonstrate how to translate these\npopulation statistics into simple and effective filters to find potential\ndouble-peaked SNe IIb in large-scale survey alert streams, like the imminent\nVera C. Rubin Observatory Legacy Survey of Space and Time (Rubin LSST).",
        "In overparameterized logistic regression, gradient descent (GD) iterates\ndiverge in norm while converging in direction to the maximum $\\ell_2$-margin\nsolution -- a phenomenon known as the implicit bias of GD. This work\ninvestigates additional regularization effects induced by early stopping in\nwell-specified high-dimensional logistic regression. We first demonstrate that\nthe excess logistic risk vanishes for early-stopped GD but diverges to infinity\nfor GD iterates at convergence. This suggests that early-stopped GD is\nwell-calibrated, whereas asymptotic GD is statistically inconsistent. Second,\nwe show that to attain a small excess zero-one risk, polynomially many samples\nare sufficient for early-stopped GD, while exponentially many samples are\nnecessary for any interpolating estimator, including asymptotic GD. This\nseparation underscores the statistical benefits of early stopping in the\noverparameterized regime. Finally, we establish nonasymptotic bounds on the\nnorm and angular differences between early-stopped GD and $\\ell_2$-regularized\nempirical risk minimizer, thereby connecting the implicit regularization of GD\nwith explicit $\\ell_2$-regularization.",
        "In this work, we present a simple and low-cost experiment designed to study\nthe oscillations of the magnetic field created by a cylindrical magnet under\ntwo different conditions: far and short distances from the magnetic sensor. A\nTaylor series expansion of the magnetic field function has been done to study\nthe convergence of the polynomial series to the real field in both situations.\nTo carry out the experiment, a small cylindrical magnet has been attached to an\noscillating and well-known spring-mass system. The resulting oscillating\nmagnetic field has been registered with the smartphone by using the\nmagnetometer sensor. A very good agreement has been obtained between the\ntheoretical model for the magnetic field and the experimental data collected\nwith the sensor located near and far from a cylindrical magnet and along its\nlongitudinal axis.",
        "We review exceptional field theories as the duality-covariant reformulation\nof maximal supergravity theories in ten and eleven dimensions, that make the\nunderlying exceptional symmetries explicit. Beyond their structural role in\nunifying the various maximal supergravities, we illustrate how they also\nprovide access to very efficient techniques for tackling concrete computational\nproblems in supergravity.",
        "In this paper, we first give a critical BKM-type blow-up criterion that only\ninvolves the horizontal swirl component of the velocity for the inviscid\naxially symmetric MHD-Boussinesq system. Moreover, we consider the inviscid\nlimit of the viscous MHD-Boussinesq system, and the convergence rate for the\nviscosity coefficient tending to zero is obtained.",
        "Sophomore's dream sum $S=\\sum_{n=1}^\\infty n^{-n}$ is extended to the\nfunction $f(t,a)=t\\int_{0}^{1}(ax)^{-tx}dx$ with $f(1,1)=S$. Asymptotic\nbehavior for a large $|t|$ is obtained, which is exponential for $t>0$ and\n$t<0,a>1$, and inverse-logarithmic for $t<0,a<1$. An advanced approximation\nincludes a half-derivative of the exponent and is expressed in terms of the\nerror function. This approach provides excellent interpolation description in\nthe complex plane. The function $f(t,a)$ demonstrates for $a>1$ oscillating\nbehavior along the imaginary axis with slowly increasing amplitude and the\nperiod of $2\\pi iea$, modulation by high-frequency oscillations being present.\nAlso, $f(t,a)$ has non-trivial zeros in the left complex half-plane with Im$t_n\n\\simeq 2(n-1\/8)\\pi e\/a$ for $a>1$. The results obtained describe analytical\nintegration of the function $x^{tx}$.",
        "Electro-optics, the tuning of optical properties of materials with electric\nfields, is key to a multitude of quantum and classical photonics applications.\nHowever, a major obstacle preventing many emerging use cases is inefficient\nmodulation in cryogenic environments, as traditional tuning mechanisms degrade\nat low temperatures. Guided by the connection between phase transitions and\nnonlinearity, we identify the quantum paraelectric perovskite SrTiO$_3$ (STO)\nas the strongest cryogenic electro-optic photonic material. As a result of the\nunique quantum paraelectric phase of STO, we demonstrate a dynamically tunable\nlinear Pockels coefficient ($r_{33}$) exceeding 500 pm\/V at $T=5$ K, and study\nits full temperature and bias dependence. We also measure an enhanced\npiezo-electric coefficient ($d_{33}$) above 90 pC\/N. Both of these coefficients\nexceed all previously reported values for cryogenic materials, including\nlithium niobate ($r_{33}\\approx24$ pm\/V) and barium titanate\n($r_{42}\\approx170$ pm\/V). Furthermore, by tuning STO towards \\textit{quantum\ncriticality} with oxygen isotope substitution we more than double the optical\nand piezo-electric nonlinearities, demonstrating a linear Pockels coefficient\nabove 1100 pm\/V. Our results probe the link between quantum phase transitions,\ndielectric susceptibility, and optical nonlinearities, unlocking opportunities\nin cryogenic optical and mechanical systems, and provide a framework for\ndiscovering new nonlinear materials.",
        "We present an experimental method for the characterization of the kinetic\nenergies of ions confined in a 22-pole radio frequency trap by inducing a small\npotential barrier using the surrounding ring electrodes, allowing the selective\nextraction of ions. Energy sampling experiments have been performed on buffer\ngas thermalized He$^+$ ions at trap temperatures between 10-180 K, resulting in\ndistinct extraction curves as a function of the potential barrier, and a\ndifferentiated behavior depending on the escape time from the trap. The\nexperiments are complemented by Monte Carlo simulations of the ion trajectories\ninside the calculated trap potential and allow us to investigate the properties\nof the sampling method, the role of ion motion coupling, and the impact of\nresidual buffer gas collisions on the observed results. The technique has also\nbeen successfully applied to identify energetic H$_3^+$ ions produced in an\nexothermic reaction inside the trap. Upon calibration, this method can provide\nrelative kinetic energy distributions or be used to filter the maximum desired\nkinetic energy of the ions inside the trap.",
        "We study Kitaev's quantum double model for arbitrary finite gauge group in\ninfinite volume, using an operator-algebraic approach. The quantum double model\nhosts anyonic excitations which can be identified with equivalence classes of\n`localized and transportable endomorphisms', which produce anyonic excitations\nfrom the ground state. Following the Doplicher--Haag--Roberts (DHR) sector\ntheory from AQFT, we organize these endomorphisms into a braided monoidal\ncategory capturing the fusion and braiding properties of the anyons. We show\nthat this category is equivalent to $\\mathrm{Rep}_f \\mathcal{D}(G)$, the\nrepresentation category of the quantum double of $G$. This establishes for the\nfirst time the full DHR structure for a class of 2d quantum lattice models with\nnon-abelian anyons.",
        "In this paper, we present an averaging method for obtaining quasi-periodic\nresponse solutions in perturbed real analytic quasi-periodic systems with\nDiophantine frequency vectors. Assuming that the averaged system possesses a\nnon-degenerate equilibrium and the eigenvalues of the linearized matrix are\npairwise distinct, we show that the original system admits a quasi-periodic\nresponse solution for the parameter belonging to a Cantorian set. The proof is\nbased on the KAM techniques, and this averaging method can be extended to the\nsecond-order systems. It is worth mentioning that our results do not require\nthe equilibrium point to be hyperbolic, which means that the eigenvalues of the\nlinearized matrix of the averaging system may be purely imaginary.",
        "Aluminum-copper-magnesium-sliver (Al-Cu-Mg-Ag) alloys are extensively\nutilized in aerospace industries due to the formation of Omega\nnano-plates.However, the rapid coarsening of these nano-plates above 475 K\nrestricts their application at elevated temperatures.When introducing scandium\n(Sc) to these alloys, the service temperature of the resultant alloys can reach\nan unprecedented 675 K, attributed to the in situ formation of a\ncoarsening-resistant V-Sc(Al2Cu)4 phase within the Omega nano-plates. However,\nthe fundamental thermodynamic properties and mechanisms behind the remarkable\ncoarsening resistance of V nano-plates remain unexplored.Here, we employ\nfirst-principles calculations to investigate the phase stability of\nV-Sc(Al2Cu)4 phase, the basic kinetic features of V phase formation within\nOmega nano-plates, and the origins of the extremely high thermal stability of V\nnano-plates. Our results indicate that V-Sc(Al2Cu)4 is meta-stable and\nthermodynamically tends to evolve into a stable ScAl7Cu5 phase. We also\ndemonstrate that kinetic factors are mainly responsible for the temperature\ndependence of V phase formation. Notably, the formation of V-Sc(Al2Cu)4 within\nOmega nano-plates modifies the Kagome lattice in the shell layer of the Omega\nnano-plates, inhibiting further thickening of V nano-plates through the\nthickening pathway of Omega nano-plates. This interface transition leads to the\nexceptional coarsening resistance of the V nano-plates. Moreover, we also\nscreened 14 promising element substitutions for Sc. These findings are\nanticipated to accelerate the development of high-performance Al alloys with\nsuperior heat resistance.",
        "Counterdiabatic driving, which enforces adiabatic evolution in arbitrary\ntimescales, can be realised by engineering a Floquet Hamiltonian which\noscillates between the Hamiltonian and its derivative requiring no additional\ncontrol terms. However, the coefficients of the Floquet Hamiltoinan require\nknowledge of the counterdiabatic terms, which can be difficult to derive\noutside of a limited set of examples. We introduce a new hybrid technique for\nthe control of quantum systems, Counterdiabatic-influenced Floquet-engineering\nor CAFFEINE for short. CAFFEINE parameterises the Floquet Hamiltonian for\ncounterdiabatic driving and utilises numerical quantum optimal control in order\nto obtain the desired target state. This removes the need to both obtain and\nimplement counterdiabatic terms, however, it does require the ability to\nquickly oscillate each term in the Hamiltonian. If this oscillation is\npossible, then CAFFEINE provides a framework to implement quantum annealing\nprotocols and general quantum state preparation. We illustrate this through\noptimisation of two numerical examples of preparing a Bell state with two\nqubits and performing annealing protocols for the one-dimensional Ising model.\nBeyond this, we also illustrate CAFFEINE's capabilities to learn the\ncounterdiabatic terms, which can potentially be used as a probe of quantum\nchaos and the geometry of quantum dynamics."
      ]
    }
  },
  {
    "id":2411.06785,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"RNA-Seq: a revolutionary tool for transcriptomics",
    "start_abstract":"RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "White-Box Transformers via Sparse Rate Reduction"
      ],
      "abstract":[
        "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "On the Fly Adaptation of Behavior Tree-Based Policies through\n  Reinforcement Learning",
        "A novel inversion algorithm for weak gravitational lensing using\n  quasi-conformal geometry",
        "Synthetic $\\pi$-flux system in 2D superconducting qubit array with\n  tunable coupling",
        "A Note on Exact State Visit Probabilities in Two-State Markov Chains",
        "Erd\\H{o}s's integer dilation approximation problem and GCD graphs",
        "Cracking Vector Search Indexes",
        "Computing Game Symmetries and Equilibria That Respect Them",
        "Revealing quantum operator scrambling via measuring Holevo information\n  on digital quantum simulators",
        "It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate\n  Mutually via Selective Rationale Optimisation",
        "A Geometric Perspective for High-Dimensional Multiplex Graphs",
        "Employee Turnover Prediction: A Cross-component Attention Transformer\n  with Consideration of Competitor Influence and Contagious Effect",
        "Towards High-performance Spiking Transformers from ANN to SNN Conversion",
        "Cohort-attention Evaluation Metric against Tied Data: Studying\n  Performance of Classification Models in Cancer Detection",
        "Stability, periodic orbits and KAM tori in the dynamics of the three\n  fixed centers problem",
        "The Unbearable Lightness of Prompting: A Critical Reflection on the\n  Environmental Impact of genAI use in Design Education",
        "Simultaneous extension of generalized BT-inverses and core-EP inverses",
        "Maximal Magic for Two-qubit States",
        "Electronic and optical excitations of K-Sb and Na-Sb crystals",
        "Bridging Simulation and Reality: A 3D Clustering-Based Deep Learning\n  Model for UAV-Based RF Source Localization",
        "Challenges and Trends in Egocentric Vision: A Survey",
        "Efficient Over-parameterized Matrix Sensing from Noisy Measurements via\n  Alternating Preconditioned Gradient Descent",
        "Towards Interactive Deepfake Analysis",
        "Low-temperature magnetic behaviour on the triangular lattice in\n  hexagonal Ba$_3$Tb(BO$_3$)$_3$",
        "Crystal tensor properties of magnetic materials with and without\n  spin-orbit coupling. Application of spin point groups as approximate\n  symmetries",
        "FeatureGS: Eigenvalue-Feature Optimization in 3D Gaussian Splatting for\n  Geometrically Accurate and Artifact-Reduced Reconstruction",
        "Intervals in Dyck paths and the wreath conjecture",
        "Bandit Optimal Transport",
        "LongSafety: Evaluating Long-Context Safety of Large Language Models",
        "Monitoring Reasoning Models for Misbehavior and the Risks of Promoting\n  Obfuscation"
      ],
      "abstract":[
        "With the rising demand for flexible manufacturing, robots are increasingly\nexpected to operate in dynamic environments where local -- such as slight\noffsets or size differences in workpieces -- are common. We propose to address\nthe problem of adapting robot behaviors to these task variations with a\nsample-efficient hierarchical reinforcement learning approach adapting Behavior\nTree (BT)-based policies. We maintain the core BT properties as an\ninterpretable, modular framework for structuring reactive behaviors, but extend\ntheir use beyond static tasks by inherently accommodating local task\nvariations. To show the efficiency and effectiveness of our approach, we\nconduct experiments both in simulation and on a Franka Emika Panda 7-DoF, with\nthe manipulator adapting to different obstacle avoidance and pivoting tasks.",
        "One of the challenges in weak gravitational lensing by galaxies and clusters\nis to infer the projected mass density distribution from gravitational lensing\nmeasurements, which is known as inversion problem. We introduce a novel\ntheoretical approach to solve the inversion problem. The cornerstone of the\nproposed method lies in a complex formalism that describes the lens mapping as\nquasi-conformal mapping with the Beltrami coefficient given by the negative of\nthe reduced shear, which is, in principle, observable from the image\nellipticities. We propose an algorithm called QCLens that is based on this\ncomplex formalism. QCLens computes the underlying quasi-conformal mapping with\na finite element approach by reducing the problem to two elliptic partial\ndifferential equations solely depending on the reduced shear field.\nExperimental results for both the Schwarzschild and singular isothermal lens\ndemonstrate the agreement of our proposed method with the analytically\ncomputable solutions.",
        "Flat-band systems provide an ideal platform for exploring exotic quantum\nphenomena, where the strongly suppressed kinetic energy in these flat energy\nbands suggests the potential for exotic phases driven by geometric structure,\ndisorder, and interactions. While intriguing phenomena and physical mechanisms\nhave been unveiled in theoretical models, synthesizing such systems within\nscalable quantum platforms remains challenging. Here, we present the\nexperimental realization of a $\\pi$-flux rhombic system using a two-dimensional\nsuperconducting qubit array with tunable coupling. We experimentally observe\ncharacteristic dynamics, e.g., $\\pi$-flux driven destructive interference, and\ndemonstrate the protocol for eigenstate preparation in this rhombic array with\ncoupler-assisted flux. Our results provide future possibilities for exploring\nthe interplay of geometry, interactions, and quantum information encoding in\nsuch degenerate systems.",
        "In this note we derive the exact probability that a specific state in a\ntwo-state Markov chain is visited exactly $k$ times after $N$ transitions. We\nprovide a closed-form solution for $\\mathbb{P}(N_l = k \\mid N)$, considering\ninitial state probabilities and transition dynamics. The solution corrects and\nextends prior incomplete results, offering a rigorous framework for enumerating\nstate transitions. Numerical simulations validate the derived expressions,\ndemonstrating their applicability in stochastic modeling.",
        "Let $\\mathcal{A}\\subset\\mathbb{R}_{\\geqslant1}$ be a countable set such that\n$\\limsup_{x\\to\\infty}\\frac{1}{\\log\nx}\\sum_{\\alpha\\in\\mathcal{A}\\cap[1,x]}\\frac{1}{\\alpha}>0$. We prove that, for\nevery $\\varepsilon>0$, there exist infinitely many pairs $(\\alpha, \\beta)\\in\n\\mathcal{A}^2$ such that $\\alpha\\neq \\beta$ and $|n\\alpha-\\beta| <\\varepsilon$\nfor some positive integer $n$. This resolves a problem of Erd\\H{o}s from 1948.\nA critical role in the proof is played by the machinery of GCD graphs, which\nwere introduced by the first author and by James Maynard in their work on the\nDuffin--Schaeffer conjecture in Diophantine approximation.",
        "Retrieval Augmented Generation (RAG) uses vector databases to expand the\nexpertise of an LLM model without having to retrain it. This idea can be\napplied over data lakes, leading to the notion of embeddings data lakes, i.e.,\na pool of vector databases ready to be used by RAGs. The key component in these\nsystems is the indexes enabling Approximated Nearest Neighbor Search (ANNS).\nHowever, in data lakes, one cannot realistically expect to build indexes for\nevery possible dataset. In this paper, we propose an adaptive, partition-based\nindex, CrackIVF, that performs much better than up-front index building.\nCrackIVF starts answering queries by near brute force search and only expands\nas it sees enough queries. It does so by progressively adapting the index to\nthe query workload. That way, queries can be answered right away without having\nto build a full index first. After seeing enough queries, CrackIVF will produce\nan index comparable to the best of those built using conventional techniques.\nAs the experimental evaluation shows, CrackIVF can often answer more than 1\nmillion queries before other approaches have even built the index and can start\nanswering queries immediately, achieving 10-1000x faster initialization times.\nThis makes it ideal when working with cold data or infrequently used data or as\na way to bootstrap access to unseen datasets.",
        "Strategic interactions can be represented more concisely, and analyzed and\nsolved more efficiently, if we are aware of the symmetries within the\nmultiagent system. Symmetries also have conceptual implications, for example\nfor equilibrium selection. We study the computational complexity of identifying\nand using symmetries. Using the classical framework of normal-form games, we\nconsider game symmetries that can be across some or all players and\/or actions.\nWe find a strong connection between game symmetries and graph automorphisms,\nyielding graph automorphism and graph isomorphism completeness results for\ncharacterizing the symmetries present in a game. On the other hand, we also\nshow that the problem becomes polynomial-time solvable when we restrict the\nconsideration of actions in one of two ways.\n  Next, we investigate when exactly game symmetries can be successfully\nleveraged for Nash equilibrium computation. We show that finding a Nash\nequilibrium that respects a given set of symmetries is PPAD- and CLS-complete\nin general-sum and team games respectively -- that is, exactly as hard as\nBrouwer fixed point and gradient descent problems. Finally, we present\npolynomial-time methods for the special cases where we are aware of a vast\nnumber of symmetries, or where the game is two-player zero-sum and we do not\neven know the symmetries.",
        "Quantum operator scrambling describes the spreading of local operators into\nthe whole system in the picture of Heisenberg evolution, which is often\nquantified by the operator size growth. Here we propose a measure of quantum\noperator scrambling via Holevo information of operators, by taking its capacity\nto distinguish operator information locally. We show that the operator size is\nclosely related to a special kind of Holevo information of operators. Moreover,\nwe propose a feasible protocol for measuring Holevo information of operators on\ndigital quantum simulators based on random states. Our numerical simulations\nshow that the integrable system can be told apart from the chaotic system by\nmeasuring the spatial-temporal patterns of Holevo information. Furthermore, we\nfind that error mitigation is required to restore the time-oscillation behavior\nof Holevo information for the integrable system, a crucial feature distinct\nfrom the chaotic one. Our work provides a new perspective to understand the\ninformation scrambling and quantum chaos from aspects of Holevo information of\noperators.",
        "Very large language models (LLMs) such as GPT-4 have shown the ability to\nhandle complex tasks by generating and self-refining step-by-step rationales.\nSmaller language models (SLMs), typically with < 13B parameters, have been\nimproved by using the data generated from very-large LMs through knowledge\ndistillation. However, various practical constraints such as API costs,\ncopyright, legal and ethical policies restrict using large (often opaque)\nmodels to train smaller models for commercial use. Limited success has been\nachieved at improving the ability of an SLM to explore the space of possible\nrationales and evaluate them by itself through self-deliberation. To address\nthis, we propose COALITION, a trainable framework that facilitates interaction\nbetween two variants of the same SLM and trains them to generate and refine\nrationales optimized for the end-task. The variants exhibit different behaviors\nto produce a set of diverse candidate rationales during the generation and\nrefinement steps. The model is then trained via Selective Rationale\nOptimization (SRO) to prefer generating rationale candidates that maximize the\nlikelihood of producing the ground-truth answer. During inference, COALITION\nemploys a controller to select the suitable variant for generating and refining\nthe rationales. On five different datasets covering mathematical problems,\ncommonsense reasoning, and natural language inference, COALITION outperforms\nseveral baselines by up to 5%. Our ablation studies reveal that\ncross-communication between the two variants performs better than using the\nsingle model to self-refine the rationales. We also demonstrate the\napplicability of COALITION for LMs of varying scales (4B to 14B parameters) and\nmodel families (Mistral, Llama, Qwen, Phi). We release the code for this work\nat https:\/\/github.com\/Sohanpatnaik106\/coalition.",
        "High-dimensional multiplex graphs are characterized by their high number of\ncomplementary and divergent dimensions. The existence of multiple hierarchical\nlatent relations between the graph dimensions poses significant challenges to\nembedding methods. In particular, the geometric distortions that might occur in\nthe representational space have been overlooked in the literature. This work\nstudies the problem of high-dimensional multiplex graph embedding from a\ngeometric perspective. We find that the node representations reside on highly\ncurved manifolds, thus rendering their exploitation more challenging for\ndownstream tasks. Moreover, our study reveals that increasing the number of\ngraph dimensions can cause further distortions to the highly curved manifolds.\nTo address this problem, we propose a novel multiplex graph embedding method\nthat harnesses hierarchical dimension embedding and Hyperbolic Graph Neural\nNetworks. The proposed approach hierarchically extracts hyperbolic node\nrepresentations that reside on Riemannian manifolds while gradually learning\nfewer and more expressive latent dimensions of the multiplex graph.\nExperimental results on real-world high-dimensional multiplex graphs show that\nthe synergy between hierarchical and hyperbolic embeddings incurs much fewer\ngeometric distortions and brings notable improvements over state-of-the-art\napproaches on downstream tasks.",
        "Employee turnover refers to an individual's termination of employment from\nthe current organization. It is one of the most persistent challenges for\nfirms, especially those ones in Information Technology (IT) industry that\nconfront high turnover rates. Effective prediction of potential employee\nturnovers benefits multiple stakeholders such as firms and online recruiters.\nPrior studies have focused on either the turnover prediction within a single\nfirm or the aggregated employee movement among firms. How to predict the\nindividual employees' turnovers among multiple firms has gained little\nattention in literature, and thus remains a great research challenge. In this\nstudy, we propose a novel deep learning approach based on job embeddedness\ntheory to predict the turnovers of individual employees across different firms.\nThrough extensive experimental evaluations using a real-world dataset, our\ndeveloped method demonstrates superior performance over several\nstate-of-the-art benchmark methods. Additionally, we estimate the cost saving\nfor recruiters by using our turnover prediction solution and interpret the\nattributions of various driving factors to employee's turnover to showcase its\npractical business value.",
        "Spiking neural networks (SNNs) show great potential due to their energy\nefficiency, fast processing capabilities, and robustness. There are two main\napproaches to constructing SNNs. Direct training methods require much memory,\nwhile conversion methods offer a simpler and more efficient option. However,\ncurrent conversion methods mainly focus on converting convolutional neural\nnetworks (CNNs) to SNNs. Converting Transformers to SNN is challenging because\nof the presence of non-linear modules. In this paper, we propose an Expectation\nCompensation Module to preserve the accuracy of the conversion. The core idea\nis to use information from the previous T time-steps to calculate the expected\noutput at time-step T. We also propose a Multi-Threshold Neuron and the\ncorresponding Parallel Parameter normalization to address the challenge of\nlarge time steps needed for high accuracy, aiming to reduce network latency and\npower consumption. Our experimental results demonstrate that our approach\nachieves state-of-the-art performance. For example, we achieve a top-1 accuracy\nof 88.60\\% with only a 1\\% loss in accuracy using 4 time steps while consuming\nonly 35\\% of the original power of the Transformer. To our knowledge, this is\nthe first successful Artificial Neural Network (ANN) to SNN conversion for\nSpiking Transformers that achieves high accuracy, low latency, and low power\nconsumption on complex datasets. The source codes of the proposed method are\navailable at https:\/\/github.com\/h-z-h-cell\/Transformer-to-SNN-ECMT.",
        "Artificial intelligence (AI) has significantly improved medical screening\naccuracy, particularly in cancer detection and risk assessment. However,\ntraditional classification metrics often fail to account for imbalanced data,\nvarying performance across cohorts, and patient-level inconsistencies, leading\nto biased evaluations. We propose the Cohort-Attention Evaluation Metrics (CAT)\nframework to address these challenges. CAT introduces patient-level assessment,\nentropy-based distribution weighting, and cohort-weighted sensitivity and\nspecificity. Key metrics like CATSensitivity (CATSen), CATSpecificity (CATSpe),\nand CATMean ensure balanced and fair evaluation across diverse populations.\nThis approach enhances predictive reliability, fairness, and interpretability,\nproviding a robust evaluation method for AI-driven medical screening models.",
        "We investigate the motion in space of an infinitesimal particle in the\ngravitational field generated by three primary bodies positioned at the\nvertices of a fixed equilateral triangle. We assume that the distances between\nthe primaries are small compared to their separation from the particle. By\napplying a Lie-Deprit normalization, we simplify the Hamiltonian, relegating\nboth the mean anomaly and the argument of periapisis to third-order terms or\nhigher. After reducing out the symmetries associated with the Kepler flow and\nthe central action of the angular momentum, we examine the relative equilibria\nin the first and second reduced spaces. We are able to identify the conditions\nfor the existence of circular periodic orbits and KAM tori, thus providing\ninsight into the system's long-term stability and dynamic structure.",
        "Design educators are finding ways to support students in skillfully using\nGenAI tools in their practices while encouraging the critical scrutiny of the\nethical and social issues around these technologies. However, the issue of\nenvironmental sustainability remains unaddressed. There is a lack of both\nresources to grasp the environmental costs of genAI in education and a lack of\nshared practices for engaging with the issue. This paper critically reflects on\nthe energy costs of using genAI in design education, using a workshop held in\n2023 with 49 students as a motivating example. Through this reflection, we\ndevelop a set of five alternative stances, with related actions, that support\nthe conscious use of genAI in design education. The work contributes to the\nfield of design and HCI by bringing together ways for educators to reflect on\ntheir practices, informing the future development of educational programs\naround genAI.",
        "In this paper we introduce the generalized inverse of complex square matrix\nwith respect to other matrix having same size. Some of its representations,\nproperties and characterizations are obtained. Also some new representation\nmatrices of W-weighted BT-inverse and W-weighted core-EP inverse are determined\nas well as characterizations of generalized inverses A A^\\odagger,\nA^{odagger,W}, A^\\diamond, A^{\\diamond,W}.",
        "Magic is a quantum resource essential for universal quantum computation and\nrepresents the deviation of quantum states from those that can be simulated\nefficiently using classical algorithms. Using the Stabilizer R\\'enyi Entropy\n(SRE), we investigate two-qubit states with maximal magic, which are most\ndistinct from classical simulability, and provide strong numerical evidence\nthat the maximal second order SRE is $\\log (16\/7)\\approx 0.827$, establishing a\ntighter bound than the prior $\\log(5\/2)\\approx 0.916$. We identity 480 states\nsaturating the new bound, which turn out to be the fiducial states for the\nmutually unbiased bases (MUBs) generated by the orbits of the Weyl-Heisenberg\n(WH) group, and conjecture that WH-MUBs are the maximal magic states for\n$n$-qubit, when $n\\neq 1$ and 3. We also reveal a striking interplay between\nmagic and entanglement: the entanglement of maximal magic states is restricted\nto two possible values, $1\/2$ and $1\/\\sqrt{2}$, as quantified by the\nconcurrence; none is maximally entangled.",
        "Recent advances in experimental techniques and computational methods have\nsignificantly expanded the family of alkali antimonides, a class of\nsemiconducting materials used as photocathodes in particle accelerators,\nunveiling new crystal structures and stoichiometries with improved stability\nand quantum efficiency. This work investigates the electronic and optical\nproperties of eight Na- and K-based alkali antimonide binary crystals with 3:1\nand 1:1 alkali-to-antimony ratios, which were predicted to be stable in a\nrecent high-throughput screening study. Employing density functional theory and\nmany-body perturbation theory, we find that all systems exhibit direct band\ngaps, except for monoclinic Na$_8$Sb$_8$, which has a nearly degenerate\nindirect gap. Optical spectra are characterized by near-infrared absorption\nonsets and intense visible excitations. Our analysis highlights the significant\nrole of electron-hole correlations, particularly in K-based compounds, leading\nto exciton binding energies above 100~meV and sharper absorption peaks. An\nin-depth analysis of the electronic contributions to the excited states\nprovides additional insight into the role of excitonic effects. By shedding\nlight on the fundamental properties of alkali antimonide binary crystals, our\nresults are relevant for the design and optimization of next-generation\nelectron sources for particle accelerators.",
        "Localization of radio frequency (RF) sources has critical applications,\nincluding search and rescue, jammer detection, and monitoring of hostile\nactivities. Unmanned aerial vehicles (UAVs) offer significant advantages for RF\nsource localization (RFSL) over terrestrial methods, leveraging autonomous 3D\nnavigation and improved signal capture at higher altitudes. Recent advancements\nin deep learning (DL) have further enhanced localization accuracy, particularly\nfor outdoor scenarios. DL models often face challenges in real-world\nperformance, as they are typically trained on simulated datasets that fail to\nreplicate real-world conditions fully. To address this, we first propose the\nEnhanced Two-Ray propagation model, reducing the simulation-to-reality gap by\nimproving the accuracy of propagation environment modeling. For RFSL, we\npropose the 3D Cluster-Based RealAdaptRNet, a DL-based method leveraging 3D\nclustering-based feature extraction for robust localization. Experimental\nresults demonstrate that the proposed Enhanced Two-Ray model provides superior\naccuracy in simulating real-world propagation scenarios compared to\nconventional free-space and two-ray models. Notably, the 3D Cluster-Based\nRealAdaptRNet, trained entirely on simulated datasets, achieves exceptional\nperformance when validated in real-world environments using the AERPAW physical\ntestbed, with an average localization error of 18.2 m. The proposed approach is\ncomputationally efficient, utilizing 33.5 times fewer parameters, and\ndemonstrates strong generalization capabilities across diverse trajectories,\nmaking it highly suitable for real-world applications.",
        "With the rapid development of artificial intelligence technologies and\nwearable devices, egocentric vision understanding has emerged as a new and\nchallenging research direction, gradually attracting widespread attention from\nboth academia and industry. Egocentric vision captures visual and multimodal\ndata through cameras or sensors worn on the human body, offering a unique\nperspective that simulates human visual experiences. This paper provides a\ncomprehensive survey of the research on egocentric vision understanding,\nsystematically analyzing the components of egocentric scenes and categorizing\nthe tasks into four main areas: subject understanding, object understanding,\nenvironment understanding, and hybrid understanding. We explore in detail the\nsub-tasks within each category. We also summarize the main challenges and\ntrends currently existing in the field. Furthermore, this paper presents an\noverview of high-quality egocentric vision datasets, offering valuable\nresources for future research. By summarizing the latest advancements, we\nanticipate the broad applications of egocentric vision technologies in fields\nsuch as augmented reality, virtual reality, and embodied intelligence, and\npropose future research directions based on the latest developments in the\nfield.",
        "We consider the noisy matrix sensing problem in the over-parameterization\nsetting, where the estimated rank $r$ is larger than the true rank $r_\\star$.\nSpecifically, our main objective is to recover a matrix $ X_\\star \\in\n\\mathbb{R}^{n_1 \\times n_2} $ with rank $ r_\\star $ from noisy measurements\nusing an over-parameterized factorized form $ LR^\\top $, where $ L \\in\n\\mathbb{R}^{n_1 \\times r}, \\, R \\in \\mathbb{R}^{n_2 \\times r} $ and $\n\\min\\{n_1, n_2\\} \\ge r > r_\\star $, with the true rank $ r_\\star $ being\nunknown. Recently, preconditioning methods have been proposed to accelerate the\nconvergence of matrix sensing problem compared to vanilla gradient descent,\nincorporating preconditioning terms $ (L^\\top L + \\lambda I)^{-1} $ and $\n(R^\\top R + \\lambda I)^{-1} $ into the original gradient. However, these\nmethods require careful tuning of the damping parameter $\\lambda$ and are\nsensitive to initial points and step size. To address these limitations, we\npropose the alternating preconditioned gradient descent (APGD) algorithm, which\nalternately updates the two factor matrices, eliminating the need for the\ndamping parameter and enabling faster convergence with larger step sizes. We\ntheoretically prove that APGD achieves near-optimal error convergence at a\nlinear rate, starting from arbitrary random initializations. Through extensive\nexperiments, we validate our theoretical results and demonstrate that APGD\noutperforms other methods, achieving the fastest convergence rate. Notably,\nboth our theoretical analysis and experimental results illustrate that APGD\ndoes not rely on the initialization procedure, making it more practical and\nversatile.",
        "Existing deepfake analysis methods are primarily based on discriminative\nmodels, which significantly limit their application scenarios. This paper aims\nto explore interactive deepfake analysis by performing instruction tuning on\nmulti-modal large language models (MLLMs). This will face challenges such as\nthe lack of datasets and benchmarks, and low training efficiency. To address\nthese issues, we introduce (1) a GPT-assisted data construction process\nresulting in an instruction-following dataset called DFA-Instruct, (2) a\nbenchmark named DFA-Bench, designed to comprehensively evaluate the\ncapabilities of MLLMs in deepfake detection, deepfake classification, and\nartifact description, and (3) construct an interactive deepfake analysis system\ncalled DFA-GPT, as a strong baseline for the community, with the Low-Rank\nAdaptation (LoRA) module. The dataset and code will be made available at\nhttps:\/\/github.com\/lxq1000\/DFA-Instruct to facilitate further research.",
        "The hexagonal polymorph of Ba$_3$Tb(BO$_3$)$_3$ contains Tb$^{3+}$ ions on a\nquasi-2D triangular lattice, resulting in geometric magnetic frustration.\nPowder samples of Ba$_3$Tb(BO$_3$)$_3$ have been investigated using specific\nheat, powder neutron diffraction (PND), inelastic neutron scattering (INS) and\nmuon-spin relaxation spectroscopy ($\\mu$SR). No long-range magnetic ordering is\nobserved down to the lowest measured temperatures of 75 mK in PND and specific\nheat data and 1.5 K in the $\\mu$SR data. Modelling the INS spectrum using a\npoint charge model suggests that the ground state is a singlet with a low-lying\ndoublet on each of the two crystallographically independent Tb$^{3+}$ sites and\nthat both the Tb ions display weak XY single-ion anisotropy.",
        "Spin space groups, formed by operations where the rotation of the spins is\nindependent of the accompanying operation acting on the crystal structure, are\nappropriate groups to describe the symmetry of magnetic structures with null\nspin-orbit coupling. Their corresponding spin point groups are the symmetry\ngroups to be considered for deriving the symmetry constraints on the form of\nthe crystal tensor properties of such idealized structures. These groups can\nalso be taken as approximate symmetries (with some restrictions) of real\nmagnetic structures, where spin-orbit and magnetic anisotropy are however\npresent. Here we formalize the invariance transformation properties that must\nsatisfy the most important crystal tensors under a spin point group. This is\ndone using modified Jahn symbols, which generalize those applicable to ordinary\nmagnetic point groups [Gallego et al., Acta Cryst. (2019) A{\\bf 75}, 438-447].\nThe analysis includes not only equilibrium tensors, but also transport, optical\nand non-linear optical susceptibility tensors. The constraints imposed by spin\ncollinearity and coplanarity within the spin group formalism on a series of\nrepresentative tensors are discussed and compiled. As illustrative examples,\nthe defined tensor invariance equations have been applied to some known\nmagnetic structures, showing the differences of the symmetry-adapted form of\nsome relevant tensors, when considered under the constraints of its spin point\ngroup or its magnetic point group. This comparison, with the spin point group\nimplying additional constraints in the tensor form, may allow to distinguish\nthose magnetic-related properties that can be solely attributed to spin-orbit\ncoupling from those that are expected to be present even under negligible\nspin-orbit effects.",
        "3D Gaussian Splatting (3DGS) has emerged as a powerful approach for 3D scene\nreconstruction using 3D Gaussians. However, neither the centers nor surfaces of\nthe Gaussians are accurately aligned to the object surface, complicating their\ndirect use in point cloud and mesh reconstruction. Additionally, 3DGS typically\nproduces floater artifacts, increasing the number of Gaussians and storage\nrequirements. To address these issues, we present FeatureGS, which incorporates\nan additional geometric loss term based on an eigenvalue-derived 3D shape\nfeature into the optimization process of 3DGS. The goal is to improve geometric\naccuracy and enhance properties of planar surfaces with reduced structural\nentropy in local 3D neighborhoods.We present four alternative formulations for\nthe geometric loss term based on 'planarity' of Gaussians, as well as\n'planarity', 'omnivariance', and 'eigenentropy' of Gaussian neighborhoods. We\nprovide quantitative and qualitative evaluations on 15 scenes of the DTU\nbenchmark dataset focusing on following key aspects: Geometric accuracy and\nartifact-reduction, measured by the Chamfer distance, and memory efficiency,\nevaluated by the total number of Gaussians. Additionally, rendering quality is\nmonitored by Peak Signal-to-Noise Ratio. FeatureGS achieves a 30 % improvement\nin geometric accuracy, reduces the number of Gaussians by 90 %, and suppresses\nfloater artifacts, while maintaining comparable photometric rendering quality.\nThe geometric loss with 'planarity' from Gaussians provides the highest\ngeometric accuracy, while 'omnivariance' in Gaussian neighborhoods reduces\nfloater artifacts and number of Gaussians the most. This makes FeatureGS a\nstrong method for geometrically accurate, artifact-reduced and memory-efficient\n3D scene reconstruction, enabling the direct use of Gaussian centers for\ngeometric representation.",
        "Let $\\iota_{k}(m,l)$ denote the total number of intervals of length $m$\nacross all Dyck paths of semilength $k$ such that each interval contains\nprecisely $l$ falls. We give the formula for $\\iota_{k}(m,l)$ and show that\n$\\iota_{k}(k,l)=\\binom{k}{l}^2$. Motivated by this, we propose two stronger\nvariants of the wreath conjecture due to Baranyai for $n=2k+1$.",
        "Despite the impressive progress in statistical Optimal Transport (OT) in\nrecent years, there has been little interest in the study of the\n\\emph{sequential learning} of OT. Surprisingly so, as this problem is both\npractically motivated and a challenging extension of existing settings such as\nlinear bandits. This article considers (for the first time) the stochastic\nbandit problem of learning to solve generic Kantorovich and entropic OT\nproblems from repeated interactions when the marginals are known but the cost\nis unknown. We provide $\\tilde{\\mathcal O}(\\sqrt{T})$ regret algorithms for\nboth problems by extending linear bandits on Hilbert spaces. These results\nprovide a reduction to infinite-dimensional linear bandits. To deal with the\ndimension, we provide a method to exploit the intrinsic regularity of the cost\nto learn, yielding corresponding regret bounds which interpolate between\n$\\tilde{\\mathcal O}(\\sqrt{T})$ and $\\tilde{\\mathcal O}(T)$.",
        "As Large Language Models (LLMs) continue to advance in understanding and\ngenerating long sequences, new safety concerns have been introduced through the\nlong context. However, the safety of LLMs in long-context tasks remains\nunder-explored, leaving a significant gap in both evaluation and improvement of\ntheir safety. To address this, we introduce LongSafety, the first comprehensive\nbenchmark specifically designed to evaluate LLM safety in open-ended\nlong-context tasks. LongSafety encompasses 7 categories of safety issues and 6\nuser-oriented long-context tasks, with a total of 1,543 test cases, averaging\n5,424 words per context. Our evaluation towards 16 representative LLMs reveals\nsignificant safety vulnerabilities, with most models achieving safety rates\nbelow 55%. Our findings also indicate that strong safety performance in\nshort-context scenarios does not necessarily correlate with safety in\nlong-context tasks, emphasizing the unique challenges and urgency of improving\nlong-context safety. Moreover, through extensive analysis, we identify\nchallenging safety issues and task types for long-context models. Furthermore,\nwe find that relevant context and extended input sequences can exacerbate\nsafety risks in long-context scenarios, highlighting the critical need for\nongoing attention to long-context safety challenges. Our code and data are\navailable at https:\/\/github.com\/thu-coai\/LongSafety.",
        "Mitigating reward hacking--where AI systems misbehave due to flaws or\nmisspecifications in their learning objectives--remains a key challenge in\nconstructing capable and aligned models. We show that we can monitor a frontier\nreasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding\nenvironments by using another LLM that observes the model's chain-of-thought\n(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent\nactions and outputs alone, and we further found that a LLM weaker than o3-mini,\nnamely GPT-4o, can effectively monitor a stronger model. Because CoT monitors\ncan be effective at detecting exploits, it is natural to ask whether those\nexploits can be suppressed by incorporating a CoT monitor directly into the\nagent's training objective. While we show that integrating CoT monitors into\nthe reinforcement learning reward can indeed produce more capable and more\naligned agents in the low optimization regime, we find that with too much\noptimization, agents learn obfuscated reward hacking, hiding their intent\nwithin the CoT while still exhibiting a significant rate of reward hacking.\nBecause it is difficult to tell when CoTs have become obfuscated, it may be\nnecessary to pay a monitorability tax by not applying strong optimization\npressures directly to the chain-of-thought, ensuring that CoTs remain\nmonitorable and useful for detecting misaligned behavior."
      ]
    }
  },
  {
    "id":2411.06785,
    "research_type":"applied",
    "start_id":"b25",
    "start_title":"White-Box Transformers via Sparse Rate Reduction",
    "start_abstract":"In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "RNA-Seq: a revolutionary tool for transcriptomics"
      ],
      "abstract":[
        "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Impact of transverse strain on linear, transitional and self-similar\n  turbulent mixing layers",
        "Dynamics of the general $Q$-tensor model interacting with a rigid body",
        "Magnetic mirror stars in five dimensions",
        "Quantum Electrodynamics from Quantum Cellular Automata, and the Tension\n  Between Symmetry, Locality and Positive Energy",
        "Oddities in the Entanglement Scaling of the Quantum Six-Vertex Model",
        "The Impact of Stellar Flares on the Atmospheric Escape of Exoplanets\n  orbiting M stars I: Insights from the AU Mic System",
        "Gradient-Based Optimization of Core-Shell Particles with Discrete\n  Materials for Directional Scattering",
        "Effects of GaAs Buffer Layer on Structural, Magnetic, and Transport\n  Properties of Magnetic Topological Insulators\n  Cr$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$ and\n  V$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$ Films",
        "Unlocking ultra-deep wide-field imaging with sidereal visibility\n  averaging",
        "Enhanced Electromechanical Properties of Solution-Processed\n  K$_{0.5}$Na$_{0.5}$NbO$_{3}$ Thin Films",
        "Enhancing Efficiency of Local Projections Estimation with Volatility\n  Clustering in High-Frequency Data",
        "Nice q-analogs of orthogonal polynomials with nice moments: Some simple\n  examples",
        "Euclid Quick Data Release (Q1): First visual morphology catalogue",
        "Table-top three-dimensional photoemission orbital tomography with a\n  femtosecond extreme ultraviolet light source",
        "Predicting the spectrum and decay constants of positive-parity\n  heavy-strange mesons using domain-wall fermions",
        "Topological Operations Around Exceptional Points via Shortcuts to\n  Adiabaticity",
        "Time-periodic driving of a bath-coupled open quantum gas of light",
        "Effective multipliers for weights whose log are H\\\"older continuous.\n  Application to the cost of fast boundary controls for the 1D Schr{\\\"o}dinger\n  equation",
        "OneForecast: A Universal Framework for Global and Regional Weather\n  Forecasting",
        "Green's function estimates for time measurable parabolic operators on\n  polyhedrons and polyhedral cones",
        "Left invertible quasi-isometric liftings",
        "Finite-Sample Analysis of Policy Evaluation for Robust Average Reward\n  Reinforcement Learning",
        "Integrability and charge transport in asymmetric quantum-circuit\n  geometries",
        "Class Imbalance in Anomaly Detection: Learning from an Exactly Solvable\n  Model",
        "New Proofs of the Explicit Formulas of Arakawa--Kaneko Zeta Values and\n  Kaneko--Tsumura $\\eta$- and $\\psi$- Values",
        "Selective band interaction and long-range hopping in a structured\n  environment with giant atoms",
        "Introducing APERTURE: A GPU-based General Relativistic Particle-in-Cell\n  Simulation Framework",
        "Bridging Voting and Deliberation with Algorithms: Field Insights from\n  vTaiwan and Kultur Komitee",
        "Quantum computation via Floquet-tailored Rydberg interactions"
      ],
      "abstract":[
        "The growth of interfacial instabilities such as the Rayleigh-Taylor (RTI) and\nRichtmyer-Meshkov instability (RMI) are modified when developing in convergent\ngeometries. Whilst these modifications are usually quantified by the\ncompression rate and convergence rate of the mixing layer, an alternative\nframework is proposed, describing the evolution of the mixing layer through the\neffects of the mean strain rates experienced by the mixing layer. An\ninvestigation into the effect of the transverse strain rate on the mixing layer\ndevelopment is conducted through application of transverse strain rates in\nplanar geometry. A model for the linear regime in planar geometry with\ntransverse strain rate is derived, with equivalent solutions to convergent\ngeometry, and validated with two-dimensional simulations demonstrating the\namplification of the instability growth under transverse compression. The\neffect of the transverse strain rate on the transitional-to-turbulent mixing\nlayer is investigated with implicit large eddy simulation based on the\nmulti-mode quarter-scale $\\theta$-group case by Thornber et al. (Phys. Fluids,\nvol. 29, 2017, 105107). The mixing layer's growth exhibits the opposite trend\nto the linear regime model, with reduced growth under transverse compression.\nThe effect of shear-production under transverse compression causes the mixing\nlayer to become more mixed and the turbulent kinetic energy is increasingly\ndominated by the transverse directions, deviating from the unstrained\nself-similar state. The mixing layer width is able to be predicted by adjusting\nthe buoyancy-drag model by Youngs & Thornber (Physica D, vol. 410, 2020,\n132517) to utilise a drag length scale that scales with the transverse\nexpansion.",
        "In this article, the fluid-rigid body interaction problem of nematic liquid\ncrystals described by the general Beris-Edwards $Q$-tensor model is studied. It\nis proved first that the total energy of this problem decreases in time. The\nassociated mathematical problem is a quasilinear mixed-order system with moving\nboundary. After the transformation to a fixed domain, a monolithic approach\nbased on the added mass operator and lifting arguments is employed to establish\nthe maximal $L^p$-regularity of the linearized problem in an anisotropic ground\nspace. This paves the way for the local strong well-posedness for large data\nand global strong well-posedness for small data of the interaction problem.",
        "We discuss a class of solutions of multidimensional gravity which are\nformally related to black-hole solutions but can observationally look like\ncompact stars whose surface reflects back all particles or signals getting\nthere. Some particular examples of such solutions are presented and studied,\nincluding those with a magnetic field in Maxwell or nonlinear electrodynamics\n(NED) in five dimensions. For NED as a possible source for magnetic mirror\nstars, we formulate a methodology of solving the 5D Einstein-NED equations and\npoint out the conditions under which there always exist mirror star solutions.\nWe also note that some of the Einstein-Maxwell solutions under consideration\nare discussed in the literature and called ``topological stars'' due to the\ncircular topology of the fifth dimension.",
        "We show that free QED is equivalent to the continuous-space-and-time limit of\nFermi and Bose lattice quantum cellular automata theories derived from quantum\nrandom walks satisfying simple symmetry and unitarity conditions. In doing so\nwe define the Fermi and Bose theories in a unified manner using the usual\nfermion internal space but a boson internal space that is six-dimensional. We\nshow that the reduction to a two-dimensional boson internal space (two helicity\nstates arising from spin-1 plus the photon transversality condition) comes from\nrestricting the quantum cellular automaton theory to positive energies. We\nbriefly examine common symmetries of quantum cellular automata, and how\ntime-reversal symmetry demands the existence of negative-energy solutions.\nThese solutions produce a tension in coupling the Fermi and Bose theories, in\nwhich the strong locality of quantum cellular automata seems to require a\nnonzero amplitude to produce negative-energy states, leading to an unphysical\ncascade of negative-energy particles. However, we show in a 1D model that by\nextending interactions over a larger (but finite) range it is possible to\nexponentially suppress the production of negative-energy particles to the point\nwhere they can be neglected.",
        "We investigate the entanglement properties of the Quantum Six-Vertex Model on\na cylinder, focusing on the Shannon-Renyi entropy in the limit of Renyi order\n$n = \\infty$. This entropy, calculated from the ground state amplitudes of the\nequivalent XXZ spin-1\/2 chain, allows us to determine the Renyi entanglement\nentropy of the corresponding Rokhsar-Kivelson wavefunctions, which describe the\nground states of certain conformal quantum critical points. Our analysis\nreveals a novel logarithmic correction to the expected entanglement scaling\nwhen the system size is odd. This anomaly arises from the geometric frustration\nof spin configurations imposed by periodic boundary conditions on odd-sized\nchains. We demonstrate that the scaling prefactor of this logarithmic term is\ndirectly related to the compactification radius of the low-energy bosonic field\ntheory description, or equivalently, the Luttinger parameter. Thus, this\ncorrection provides a direct probe of the underlying Conformal Field Theory\n(CFT) describing the critical point. Our findings highlight the crucial role of\nsystem size parity in determining the entanglement properties of this model and\noffer insights into the interplay between geometry, frustration, and\ncriticality.",
        "The X-rays and Extreme Ultraviolet (XUV) emission from M stars can drive the\natmospheric escape on planets orbiting them. M stars are also known for their\nfrequent emission of stellar flares, which will increase the high-energy flux\nreceived by their orbiting planets. To understand how stellar flares impact the\nprimordial atmospheres of planets orbiting young M stars, we use UV\nspectroscopic data of flares from the Habitable Zones and M dwarf Activity\nacross Time (HAZMAT) and Measurements of the Ultraviolet Spectral\nCharacteristics of Low-mass Exoplanetary Systems (MUSCLES) programs as a proxy\nto the XUV flare emission. Using the software package VPLanet, we simulate the\nyoung AU Mic planetary system composed of two Neptune-sized and one Earth-sized\nplanet orbiting a 23-Myr-old M1 star. Our findings show that the Earth-sized\nplanet AU Mic d should be in the process of losing completely its atmosphere in\nthe next couple million years, solely due to the quiescent emission, with\nflares not significantly contributing to its atmospheric escape due to the\nsmall size of AU mic d and its close-in distance from the star. However, our\nresults indicate that flares would play a crucial role for such planets further\naway, in the habitable zone (i.e. 0.2935 AU) of AU Mic-like stars during the\npost-saturation phase, accelerating the total atmospheric loss process by a few\nbillion years. For planets between 0.365 AU and the HZ outer edge, the\nadditional XUV from flares is necessary to deplete primordial atmospheres fully\nsince the quiescent emission alone is insufficient.",
        "Designing nanophotonic structures traditionally grapples with the\ncomplexities of discrete parameters, such as real materials, often resorting to\ncostly global optimization methods. This paper introduces an approach that\nleverages generative deep learning to map discrete parameter sets into a\ncontinuous latent space, enabling direct gradient-based optimization. For\nscenarios with non-differentiable physics evaluation functions, a neural\nnetwork is employed as a differentiable surrogate model. The efficacy of this\nmethodology is demonstrated by optimizing the directional scattering properties\nof core-shell nanoparticles composed of a selection of realistic materials. We\nderive suggestions for core-shell geometries with strong forward scattering and\nminimized backscattering. Our findings reveal significant improvements in\ncomputational efficiency and performance when compared to global optimization\ntechniques. Beyond nanophotonics design problems, this framework holds promise\nfor broad applications across all types of inverse problems constrained by\ndiscrete variables.",
        "Here, we study the effects of a GaAs buffer layer on the structural,\nmagnetic, and transport properties of Cr$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$\nmagnetic topological insulator thin films and compare them with those of\nV$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$, which we recently reported. Similar to\nthe case of V$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$, growth on a GaAs buffer\nlayer leads to some distinctly different properties than direct growth on InP\nsubstrates. These include improved interface quality confirmed by transmission\nelectron microscopy, enhanced magnetic coercive fields, and smaller resistivity\npeaks at the magnetization reversals. Furthermore, the Bi-ratio dependence of\nthe carrier density reveals that the interface property also affects the Fermi\nlevel. These results demonstrate the importance of the buffer layer in\ncontrolling the electronic properties of the magnetic topological insulator\nfilms.",
        "Producing ultra-deep high-angular-resolution images with current and\nnext-generation radio interferometers introduces significant computational\nchallenges. In particular, the imaging is so demanding that processing large\ndatasets, accumulated over hundreds of hours on the same pointing, is likely\ninfeasible in the current data reduction schemes. In this paper, we revisit a\nsolution to this problem that was considered in the past but is not being used\nin modern software: sidereal visibility averaging (SVA). This technique\ncombines individual observations taken at different sidereal days into one much\nsmaller dataset by averaging visibilities at similar baseline coordinates. We\npresent our method and validated it using four separate 8-hour observations of\nthe ELAIS-N1 deep field, taken with the International LOw Frequency ARray\n(LOFAR) Telescope (ILT) at 140~MHz. Additionally, we assessed the accuracy\nconstraints imposed by Earth's orbital motion relative to the observed pointing\nwhen combining multiple datasets. We find, with four observations, data volume\nreductions of a factor of 1.8 and computational time improvements of a factor\nof 1.6 compared to standard imaging. These factors will increase when more\nobservations are combined with SVA. For instance, with 3000~hours of LOFAR data\naimed at achieving sensitivities of the order of {\\mu}Jy\/beam at sub-arcsecond\nresolutions, we estimate data volume reductions of up to a factor of 169 and a\n14-fold decrease in computing time using our current algorithm. This\nadvancement for imaging large deep interferometric datasets will benefit\ncurrent generation instruments, such as LOFAR, and upcoming instruments such as\nthe Square Kilometre Array (SKA), provided the calibrated visibility data of\nthe individual observations are retained.",
        "K$_{0.5}$Na$_{0.5}$NbO$_{3}$ is among the most promising lead-free\npiezoelectrics. While its sputtered films match the performance of the champion\npiezoelectric Pb(Zr,Ti)O$_{3}$, processing of high-quality, reproducible, and\ntime-stable solution-processed K$_{0.5}$Na$_{0.5}$NbO$_{3}$ films remains\nchallenging. Here, we report 1 $\\mu$m-thick Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films prepared through a chemical solution\ndeposition process, which have perfectly dense microstructure and uniform\ncomposition across their thickness. The films exhibit a high transverse\npiezoelectric coefficient (e$_{31,f}$ = -14.8 C\/m$^{2}$), high dielectric\npermittivity (${\\epsilon}_{r}$ = 920), low dielectric losses (tan${\\delta}$ =\n0.05) and can withstand electric fields up to at least 1 MV\/cm. The functional\nproperties show excellent stability over time, and the synthesis process is\nreproducible. The results demonstrate the high potential of Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films to become a replacement for lead-based\nPb(Zr,Ti)O$_{3}$ films in piezoelectric applications.",
        "This paper advances the local projections (LP) method by addressing its\ninefficiency in high-frequency economic and financial data with volatility\nclustering. We incorporate a generalized autoregressive conditional\nheteroskedasticity (GARCH) process to resolve serial correlation issues and\nextend the model with GARCH-X and GARCH-HAR structures. Monte Carlo simulations\nshow that exploiting serial dependence in LP error structures improves\nefficiency across forecast horizons, remains robust to persistent volatility,\nand yields greater gains as sample size increases. Our findings contribute to\nrefining LP estimation, enhancing its applicability in analyzing economic\ninterventions and financial market dynamics.",
        "In this note I collect some typical examples of orthogonal polynomials with\nsimple moments where both moments and orthogonal polynomials have nice\nq-analogs.",
        "We present a detailed visual morphology catalogue for Euclid's Quick Release\n1 (Q1). Our catalogue includes galaxy features such as bars, spiral arms, and\nongoing mergers, for the 378000 bright ($I_E < 20.5$) or extended (area $\\geq\n700\\,$pixels) galaxies in Q1. The catalogue was created by finetuning the\nZoobot galaxy foundation models on annotations from an intensive one month\ncampaign by Galaxy Zoo volunteers. Our measurements are fully automated and\nhence fully scaleable. This catalogue is the first 0.4% of the approximately\n100 million galaxies where Euclid will ultimately resolve detailed morphology.",
        "Following electronic processes in molecules and materials at the level of the\nquantum mechanical electron wavefunction with angstrom-level spatial resolution\nand with full access to its femtosecond temporal dynamics is at the heart of\nultrafast condensed matter physics. A breakthrough invention allowing\nexperimental access to electron wavefunctions was the reconstruction of\nmolecular orbitals from angle-resolved photoelectron spectroscopy data in 2009,\ntermed photoemission orbital tomography (POT). This invention puts ultrafast\nthree-dimensional (3D) POT in reach, with many new prospects for the study of\nultrafast light-matter interaction, femtochemistry and photo-induced phase\ntransitions. Here, we develop a synergistic experimental-algorithmic approach\nto realize the first 3D-POT experiment using a short-pulse extreme ultraviolet\nlight source. We combine a new variant of photoelectron spectroscopy, namely\nultrafast momentum microscopy, with a table-top spectrally-tunable\nhigh-harmonic generation light source and a tailored algorithm for efficient 3D\nreconstruction from sparse, undersampled data. This combination dramatically\nspeeds up the experimental data acquisition, while at the same time reducing\nthe sampling requirements to achieve complete 3D information. We demonstrate\nthe power of this approach by full 3D imaging of the frontier orbitals of a\nprototypical organic semiconductor absorbed on pristine Ag(110).",
        "We present a lattice-QCD calculation of the masses and decay constants of the\npositive-parity heavy-strange mesons $D^*_{s0}$, $D_{s1}$, $B^*_{s0}$, and\n$B_{s1}$. The calculations are performed with domain-wall fermions for the\nlight and strange quarks and an anisotropic clover action for the charm and\nbottom quarks. We use seven different RBC\/UKQCD ensembles with pion masses\nranging from a near-physical 139 MeV up to 431 MeV. We consider two different\nanalysis types, with or without two-meson operators at the source. We observe\nthe expected below-threshold ground states. The fits without the two-meson\noperators appear to be more stable, but may overestimate the ground-state\nenergies, while preliminary fits with two-meson operators at the source only\nappear to underestimate the ground-state energies.",
        "The existence of singularities in the spectrum of non-Hermitian Hamiltonians\nleads to a non-trivial spectral topology which can be exploited to generate\ntopological operations. However, their implementation has remained elusive due\nto the difficulty of generating a true adiabatic evolution. Here, we develop\nfast, robust control protocols that generate a desired topological operation.\nOur strategy relies on shortcuts to adiabaticity, but is not a trivial\nextension. The presence of spectral singularities renders the strategy\ndeveloped for Hermitian Hamiltonians impractical as it will lead to faulty\ncontrol protocols. Moreover, due to the dynamics sensitivity to parameter\nuncertainties, not all shortcuts to adiabaticity can be used in a realistic\nsetting. We illustrate our method in the context of a two-mode non-Hermitian\nHamiltonian and discuss why in general celebrated shortcuts to adiabaticiy like\ntransitionless driving and superadiabatic transitionless driving are not\nappropriate control protocols for non-Hermitian systems.",
        "We study the frequency-resolved density response of a photon Bose-Einstein\ncondensate coupled to a bath of dye molecules by time-periodic driving. By\nmonitoring the photon number dynamics for different drive frequencies, we\nobtain the spectral response of the condensate in a phase-sensitive way. We\nfind that as the photon number increases, the response of the coupled\ncondensate-bath system transitions from overdamped to resonant behavior,\nindicating a transition from closed to open system dynamics. Our spectroscopy\nmethod paves the way for studies of collective excitations in complex\ndriven-dissipative systems.",
        "We give a simple proof of the Beurling-Malliavin multiplier theorem (BM1) in\nthe particular case of weights that verify the usual finite logarithmic\nintegral condition and such that their log are H{\\\"o}lder continuous with\nexponent less than 1. Our proof has the advantage to give an explicit version\nof BM1, in the sense that one can give precise estimates from below and above\nfor the multiplier, in terms of the exponential type we want to reach, and the\nconstants appearing in the H{\\\"o}lder condition of our weights. The same ideas\ncan be applied to a particular weight, that will lead to an improvement on the\nestimation of the cost of fast boundary controls for the 1D Schr{\\\"o}dinger\nequation on a segment. Our proof is mainly based on the use of a modified\nHilbert transform together with its link with the harmonic extension in the\ncomplex upper half plane and some modified conjugate harmonic extension in the\nupper half plane.",
        "Accurate weather forecasts are important for disaster prevention,\nagricultural planning, and water resource management. Traditional numerical\nweather prediction (NWP) methods offer physically interpretable high-accuracy\npredictions but are computationally expensive and fail to fully leverage\nrapidly growing historical data. In recent years, deep learning methods have\nmade significant progress in weather forecasting, but challenges remain, such\nas balancing global and regional high-resolution forecasts, excessive smoothing\nin extreme event predictions, and insufficient dynamic system modeling. To\naddress these issues, this paper proposes a global-regional nested weather\nforecasting framework based on graph neural networks (GNNs). By combining a\ndynamic system perspective with multi-grid theory, we construct a multi-scale\ngraph structure and densify the target region to capture local high-frequency\nfeatures. We introduce an adaptive information propagation mechanism, using\ndynamic gating units to deeply integrate node and edge features for more\naccurate extreme event forecasting. For high-resolution regional forecasts, we\npropose a neural nested grid method to mitigate boundary information loss.\nExperimental results show that the proposed method performs excellently across\nglobal to regional scales and short-term to long-term forecasts, especially in\nextreme event predictions (e.g., typhoons), significantly improving forecast\naccuracy. Our codes are available at https:\/\/github.com\/YuanGao-YG\/OneForecast.",
        "We provide Green's function estimates for parabolic operators on polyhedrons\nand polyhedral cones in $\\mathbb{R}^3$. These estimates incorporate mixed\nweights, which include appropriate powers of the distances to the vertices, the\nedges, and the boundary of the domains. The allowable ranges for the weight\nparameters are explicitly determined by the geometry of the domains.",
        "Quasi-isometric liftings similar to isometries, for the operators similar to\ncontractions in Hilbert spaces, are investigated. The existence of such\nliftings is established, and their applications are explored for specific\noperator classes, including quasicontractions. A particular focus is placed on\noperators that admit left invertible minimal quasi-isometric liftings. These\noperators are characterized within the framework of $A$-contractions, and the\nmatrix structures of their liftings are analyzed, highlighting parallels with\nthe isometric liftings of contractions.",
        "We present the first finite-sample analysis for policy evaluation in robust\naverage-reward Markov Decision Processes (MDPs). Prior works in this setting\nhave established only asymptotic convergence guarantees, leaving open the\nquestion of sample complexity. In this work, we address this gap by\nestablishing that the robust Bellman operator is a contraction under the span\nsemi-norm, and developing a stochastic approximation framework with controlled\nbias. Our approach builds upon Multi-Level Monte Carlo (MLMC) techniques to\nestimate the robust Bellman operator efficiently. To overcome the infinite\nexpected sample complexity inherent in standard MLMC, we introduce a truncation\nmechanism based on a geometric distribution, ensuring a finite constant sample\ncomplexity while maintaining a small bias that decays exponentially with the\ntruncation level. Our method achieves the order-optimal sample complexity of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ for robust policy evaluation and robust\naverage reward estimation, marking a significant advancement in robust\nreinforcement learning theory.",
        "We revisit the integrability of quantum circuits constructed from two-qubit\nunitary gates $U$ that satisfy the Yang-Baxter equation. A brickwork\narrangement of $U$ typically corresponds to an integrable Trotterization of\nsome Hamiltonian dynamics. Here, we consider more general circuit geometries\nwhich include circuits without any nontrivial space periodicity. We show that\nany time-periodic quantum circuit in which $U$ is applied to each pair of\nneighbouring qubits exactly once per period remains integrable. We further\ngeneralize this framework to circuits with time-varying two-qubit gates. The\nspatial arrangement of gates in the integrable circuits considered herein can\nbreak the space-reflection symmetry even when $U$ itself is symmetric. By\nanalyzing the dynamical spin susceptibility on ballistic hydrodynamic scale, we\ninvestigate how an asymmetric arrangement of gates affects the spin transport.\nWhile it induces nonzero higher odd moments in the dynamical spin\nsusceptibility, the first moment, which corresponds to a drift in the spreading\nof correlations, remains zero. We explain this within a quasiparticle picture\nwhich suggests that a nonzero drift necessitates gates acting on distinct\ndegrees of freedom.",
        "Class imbalance (CI) is a longstanding problem in machine learning, slowing\ndown training and reducing performances. Although empirical remedies exist, it\nis often unclear which ones work best and when, due to the lack of an\noverarching theory. We address a common case of imbalance, that of anomaly (or\noutlier) detection. We provide a theoretical framework to analyze, interpret\nand address CI. It is based on an exact solution of the teacher-student\nperceptron model, through replica theory. Within this framework, one can\ndistinguish several sources of CI: either intrinsic, train or test imbalance.\nOur analysis reveals that the optimal train imbalance is generally different\nfrom 50%, with a non trivial dependence on the intrinsic imbalance, the\nabundance of data and on the noise in the learning. Moreover, there is a\ncrossover between a small noise training regime where results are independent\nof the noise level to a high noise regime where performances quickly degrade\nwith noise. Our results challenge some of the conventional wisdom on CI and\noffer practical guidelines to address it.",
        "In this paper, we establish some new identities of integrals involving\nmultiple polylogarithm functions and their level two analogues in terms of\nHurwitz-type multiple zeta (star) values. Using these identities, we provide\nnew proofs of the explicit formulas of Arakawa--Kaneko zeta values,\nKaneko--Tsumura $\\eta$- and $\\psi$-values, and also give a formula for double\n$T$-values.",
        "Giant atoms, which couple to the environment at multiple discrete points,\nexhibit various nontrivial phenomena in quantum optics due to their nonlocal\ncouplings. In this study, we propose a one-dimensional cross-stitch ladder\nlattice featuring both a dispersive band and a flat band. By modulating the\nrelative phase between the coupling points, the giant atom selectively\ninteracts with either band. First, we analyze the scenario where the dispersive\nand flat bands intersect at two points, and the atomic frequency lies within\nthe band. Unlike the small atom, which simultaneously interacts with both\nbands, a single giant atom with a controllable phase interacts exclusively with\nthe dispersive or flat band. Second, in the bandgap regime, where two atoms\ninteract through bound-state overlaps manifesting as dipole-dipole\ninteractions, we demonstrate that giant atoms enable deterministic long-range\nhopping and energy exchange with higher fidelity compared to small atoms. These\nfindings provide promising applications in quantum information processing,\noffering enhanced controllability and selectivity for quantum systems and\ndevices.",
        "Low-luminosity Active Galactic Nuclei (AGN) are believed to be surrounded by\na collisionless, highly magnetized accretion flow. As a result,\nParticle-in-Cell simulations are the best tools to study the immediate vicinity\nof the event horizons of these supermassive black holes. We present a GPU-based\ngeneral relativistic particle-in-cell (GRPIC) code framework called Aperture.\nAperture is developed in C++, with compute kernels written in CUDA and HIP to\ntake advantage of the massive acceleration modern GPUs enable. The code is\norganized in a fully modular way, allowing easy extensions to new physics\nproblems. In this paper, we describe in detail the particle pusher, field\nsolver, and charge-conserving current deposition algorithms employed in\nAperture, and present test cases to validate their correctness. Then, we apply\nthe code to study spark gaps and plasma injection in black hole magnetospheres.\nWe find that the apparent location and time-evolution of the gap depend on the\nobserver. Our results reconcile the previous conflicting findings from 1D and\n2D simulations in the literature.",
        "Democratic processes increasingly aim to integrate large-scale voting with\nface-to-face deliberation, addressing the challenge of reconciling individual\npreferences with collective decision-making. This work introduces new methods\nthat use algorithms and computational tools to bridge online voting with\nface-to-face deliberation, tested in two real-world scenarios: Kultur Komitee\n2024 (KK24) and vTaiwan. These case studies highlight the practical\napplications and impacts of the proposed methods.\n  We present three key contributions: (1) Radial Clustering for Preference\nBased Subgroups, which enables both in-depth and broad discussions in\ndeliberative settings by computing homogeneous and heterogeneous group\ncompositions with balanced and adjustable group sizes; (2) Human-in-the-loop\nMES, a practical method that enhances the Method of Equal Shares (MES)\nalgorithm with real-time digital feedback. This builds algorithmic trust by\ngiving participants full control over how much decision-making is delegated to\nthe voting aggregation algorithm as compared to deliberation; and (3) the\nReadTheRoom deliberation method, which uses opinion space mapping to identify\nagreement and divergence, along with spectrum-based preference visualisation to\ntrack opinion shifts during deliberation. This approach enhances transparency\nby clarifying collective sentiment and fosters collaboration by encouraging\nparticipants to engage constructively with differing perspectives.\n  By introducing these actionable frameworks, this research extends in-person\ndeliberation with scalable digital methods that address the complexities of\nmodern decision-making in participatory processes.",
        "Rydberg atoms stand out as a highly promising platform for realizing quantum\ncomputation with significant advantages in constructing high-fidelity quantum\ngates. Floquet frequency modulation (FFM), in Rydberg-atom systems, provides a\nunique platform for achieving precise quantum control and uncovering exotic\nphysical phenomena, paving the way for innovative methodologies in quantum\ndynamics research. This work introduces a method to realize controlled\narbitrary phase gates in Rydberg atoms by manipulating system dynamics using\nFFM. Notably, this method eliminates the need for laser addressing of\nindividual atoms, significantly enhancing convenience for future practical\napplications. Furthermore, this approach can be integrated with soft quantum\ncontrol strategies to enhance the fidelity and robustness of the resultant\ncontrolled-phase gates. Finally, as an example, this methodology is applied in\nGrover-Long algorithm to search target items with zero failure rate,\ndemonstrating its substantial significance for future quantum information\nprocessing applications. This work leveraging Rydberg atoms and Floquet\nfrequency modulation may herald a new era of scalable and reliable quantum\ncomputing."
      ]
    }
  },
  {
    "id":2412.00868,
    "research_type":"applied",
    "start_id":"b22",
    "start_title":"Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models",
    "start_abstract":"The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm.",
    "start_categories":[
      "stat.ME"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "How resilient are language models to text perturbations"
      ],
      "abstract":[
        "Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Apparent violations of the second law in the quantum-classical dynamics\n  of interacting levitated nanoparticles",
        "Does dark matter fall in the same way as standard model particles? A\n  direct constraint of Euler's equation with cosmological data",
        "A countable Boolean algebra that is Reichenbach's common cause complete",
        "Exploring Sentiment Manipulation by LLM-Enabled Intelligent Trading\n  Agents",
        "Effects of galactic environment on size and dark matter content in\n  low-mass galaxies",
        "Lattice-Based Pruning in Recurrent Neural Networks via Poset Modeling",
        "Lipschitz Decompositions of Finite $\\ell_{p}$ Metrics",
        "DIN-CTS: Low-Complexity Depthwise-Inception Neural Network with\n  Contrastive Training Strategy for Deepfake Speech Detection",
        "Recent open heavy flavor studies for the Electron-Ion Collider",
        "An Investigation of FP8 Across Accelerators for LLM Inference",
        "ASTRAL: Automated Safety Testing of Large Language Models",
        "Scalable and Site-Specific Frequency Tuning of Two-Level System Defects\n  in Superconducting Qubit Arrays",
        "Fabrication of Soft and Comfortable Pressure-Sensing Shoe Sole for\n  Intuitive Monitoring of Human Quality Gaits",
        "Cognitive Performance Measurements and the Impact of Sleep Quality Using\n  Wearable and Mobile Sensors",
        "High-throughput Discovery of Anti-gap Semiconductors",
        "AI-Facilitated Collective Judgements",
        "Predictions for Bottomonium from a Relativistic Screened Potential Model",
        "Analysis of harmonic average method for interface problems with\n  discontinuous solutions and fluxes",
        "Ground-Optimized 4D Radar-Inertial Odometry via Continuous Velocity\n  Integration using Gaussian Process",
        "GraphTEN: Graph Enhanced Texture Encoding Network",
        "Anomalous nuclear effects on ion charge state distribution in helium gas",
        "Prediction-Powered E-Values",
        "Minimal Shortfall Strategies for Liquidation of a Basket of Stocks using\n  Reinforcement Learning",
        "Responsible Artificial Intelligence (RAI) in U.S. Federal Government :\n  Principles, Policies, and Practices",
        "A Transfer Learning Framework for Anomaly Detection in Multivariate IoT\n  Traffic Data",
        "Code Readability in the Age of Large Language Models: An Industrial Case\n  Study from Atlassian",
        "Propulsion of flapping foils undergoing in-plane clap-and-fling and\n  deviation motions",
        "Update on the isospin breaking corrections to the HVP with C-periodic\n  boundary conditions",
        "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents"
      ],
      "abstract":[
        "Random exchanges of energy arise naturally in stochastic systems. As a\nconsequence, apparent violations of the second law of thermodynamics can occur,\nas it holds true on average. Here we investigate the occurrence of these\napparent violations -- termed free lunches -- in a quantum-classical system\ncomprised of levitated nanoparticles exchanging energy via the Coulomb\ninteraction. We consider different initial states for the quantum system, and\nhow these exert work and fluctuations upon the classical particle affecting the\nprobability of free lunches. With that, we initiate the study of hybrid\nquantum-classical systems through the lens of stochastic thermodynamics.",
        "Since dark matter particles have never been directly detected, we do not know\nhow they move, and in particular we do not know how they fall inside\ngravitational potential wells. Usually it is assumed that dark matter only\ninteracts gravitationally with itself and with particles of the standard model,\nand therefore that its motion is governed by Euler's equation. In this paper,\nwe test this assumption for the first time at cosmological scales, by combining\nmeasurements of galaxy velocities with measurements of gravitational potential\nwells, encoded in the Weyl potential. We find that current data are consistent\nwith Euler's equation at redshifts $z\\in [0.3,0.8]$, and we place constraints\non the strength of a potential fifth force, which would alter the way dark\nmatter particles fall. We find that a positive fifth force cannot exceed 7% of\nthe gravitational interaction strength, while a negative fifth force is limited\nto 21%. The coming generation of surveys, including the Legacy Survey of Space\nand Time (LSST) of the Vera C. Rubin Observatory and the Dark Energy\nSpectroscopic Instrument (DESI) will drastically improve the constraints,\nallowing to constrain a departure from pure gravitational interaction at the\nlevel of 2%.",
        "The common cause completeness (CCC) is a philosophical principle that asserts\nthat if we consider two positively correlated events then it evokes a common\ncause. The principle is due to H. Reichenbach and has been largely studied in\nBoolean algebras and elsewhere.The results published so far bring about a\nquestion whether there is a small (countable) Boolean algebra with CCC. In this\nnote we construct such an example.",
        "Companies across all economic sectors continue to deploy large language\nmodels at a rapid pace. Reinforcement learning is experiencing a resurgence of\ninterest due to its association with the fine-tuning of language models from\nhuman feedback. Tool-chain language models control task-specific agents; if the\nconverse has not already appeared, it soon will. In this paper, we present what\nwe believe is the first investigation of an intelligent trading agent based on\ncontinuous deep reinforcement learning that also controls a large language\nmodel with which it can post to a social media feed observed by other traders.\nWe empirically investigate the performance and impact of such an agent in a\nsimulated financial market, finding that it learns to optimize its total\nreward, and thereby augment its profit, by manipulating the sentiment of the\nposts it produces. The paper concludes with discussion, limitations, and\nsuggestions for future work.",
        "We utilize the cosmological volume simulation, FIREbox, to investigate how a\ngalaxy's environment influences its size and dark matter content. Our study\nfocuses on approximately 1,200 galaxies (886 central and 332 satellite halos)\nin the low-mass regime, with stellar masses between $10^6$ to $10^9$\n$M_{\\odot}$. We analyze the size-mass relation ($r_{50} - M_{\\star}$), inner\ndark matter mass-stellar mass ($M^{50}_{\\rm DM} - M_{\\star}$) relation, and the\nhalo mass-stellar mass ($M_{\\rm halo} - M_{\\star}$) relation. At fixed stellar\nmass, we find the galaxies experiencing stronger tidal influences, indicated by\nhigher Perturbation Indices (PI $>$ 1) are generally larger and have lower\nmasses relative to their counterparts with lower Perturbation Indices (PI $<$\n1). Applying a Random Forest regression model, we show that both the\nenvironment (PI) and halo mass ($M_{rm halo}$) are significant predictors of a\ngalaxy's relative size and dark matter content. Notably, because $M_{\\rm halo}$\nis also strongly affected by the environment, our findings indicate that\nenvironmental conditions not only influence galactic sizes and relative inner\ndark matter content directly, but also indirectly through their impact on halo\nmass. Our results highlight a critical interplay between environmental factors\nand halo mass in shaping galaxy properties, affirming the environment as a\nfundamental driver in galaxy formation and evolution.",
        "Recurrent neural networks (RNNs) are central to sequence modeling tasks, yet\ntheir high computational complexity poses challenges for scalability and\nreal-time deployment. Traditional pruning techniques, predominantly based on\nweight magnitudes, often overlook the intrinsic structural properties of these\nnetworks. We introduce a novel framework that models RNNs as partially ordered\nsets (posets) and constructs corresponding dependency lattices. By identifying\nmeet irreducible neurons, our lattice-based pruning algorithm selectively\nretains critical connections while eliminating redundant ones. The method is\nimplemented using both binary and continuous-valued adjacency matrices to\ncapture different aspects of network connectivity. Evaluated on the MNIST\ndataset, our approach exhibits a clear trade-off between sparsity and\nclassification accuracy. Moderate pruning maintains accuracy above 98%, while\naggressive pruning achieves higher sparsity with only a modest performance\ndecline. Unlike conventional magnitude-based pruning, our method leverages the\nstructural organization of RNNs, resulting in more effective preservation of\nfunctional connectivity and improved efficiency in multilayer networks with\ntop-down feedback. The proposed lattice-based pruning framework offers a\nrigorous and scalable approach for reducing RNN complexity while sustaining\nrobust performance, paving the way for more efficient hierarchical models in\nboth machine learning and computational neuroscience.",
        "Lipschitz decomposition is a useful tool in the design of efficient\nalgorithms involving metric spaces. While many bounds are known for different\nfamilies of finite metrics, the optimal parameters for $n$-point subsets of\n$\\ell_p$, for $p > 2$, remained open, see e.g. [Naor, SODA 2017]. We make\nsignificant progress on this question and establish the bound\n$\\beta=O(\\log^{1-1\/p} n)$. Building on prior work, we demonstrate applications\nof this result to two problems, high-dimensional geometric spanners and\ndistance labeling schemes. In addition, we sharpen a related decomposition\nbound for $1<p<2$, due to Filtser and Neiman [Algorithmica 2022].",
        "In this paper, we propose a deep neural network approach for deepfake speech\ndetection (DSD) based on a lowcomplexity Depthwise-Inception Network (DIN)\ntrained with a contrastive training strategy (CTS). In this framework, input\naudio recordings are first transformed into spectrograms using Short-Time\nFourier Transform (STFT) and Linear Filter (LF), which are then used to train\nthe DIN. Once trained, the DIN processes bonafide utterances to extract audio\nembeddings, which are used to construct a Gaussian distribution representing\ngenuine speech. Deepfake detection is then performed by computing the distance\nbetween a test utterance and this distribution to determine whether the\nutterance is fake or bonafide. To evaluate our proposed systems, we conducted\nextensive experiments on the benchmark dataset of ASVspoof 2019 LA. The\nexperimental results demonstrate the effectiveness of combining the\nDepthwise-Inception Network with the contrastive learning strategy in\ndistinguishing between fake and bonafide utterances. We achieved Equal Error\nRate (EER), Accuracy (Acc.), F1, AUC scores of 4.6%, 95.4%, 97.3%, and 98.9%\nrespectively using a single, low-complexity DIN with just 1.77 M parameters and\n985 M FLOPS on short audio segments (4 seconds). Furthermore, our proposed\nsystem outperforms the single-system submissions in the ASVspoof 2019 LA\nchallenge, showcasing its potential for real-time applications.",
        "The future Electron-Ion Collider (EIC) will operate a series of\nhigh-luminosity high-energy electron+proton ($e+p$) and electron+nucleus\n($\\textit{e + A}$) collisions to study several fundamental questions in the\nhigh energy and nuclear physics field. Heavy flavor hadron and jet production\nat the EIC plays an important role in exploring both potential modification on\nthe initial-state nuclear parton distribution functions (nPDFs) and final-state\nparton propagation and hadronization processes under different nuclear medium\nconditions. The current design of the EIC ePIC detector has good performance of\nvertex and track reconstruction, particle identification and energy\ndetermination in the pseudorapidity region of $-3.5<\\eta<3.5$, which will\nenable a series of high precision heavy flavor hadron and jet measurements.\nLatest simulation studies of the projected nuclear modification factor $R_{eA}$\nof heavy flavor jets and heavy flavor hadron inside jets in $e+p$ and\n$\\textit{e + Au}$ collisions at $\\sqrt{s} =$ 28.6 GeV and 63.2 GeV as well as\nthe projected statistical accuracy of inclusive and differential charm baryon\nover meson ratio measurements in $e+p$ collisions will be presented. The\nimpacts of these proposed EIC measurements on constraining the heavy quark\npropagation properties in cold nuclear medium and exploring the heavy quark\nhadronization process will be discussed.",
        "The introduction of 8-bit floating-point (FP8) computation units in modern AI\naccelerators has generated significant interest in FP8-based large language\nmodel (LLM) inference. Unlike 16-bit floating-point formats, FP8 in deep\nlearning requires a shared scaling factor. Additionally, while E4M3 and E5M2\nare well-defined at the individual value level, their scaling and accumulation\nmethods remain unspecified and vary across hardware and software\nimplementations. As a result, FP8 behaves more like a quantization format than\na standard numeric representation. In this work, we provide the first\ncomprehensive analysis of FP8 computation and acceleration on two AI\naccelerators: the NVIDIA H100 and Intel Gaudi 2. Our findings highlight that\nthe Gaudi 2, by leveraging FP8, achieves higher throughput-to-power efficiency\nduring LLM inference, offering valuable insights into the practical\nimplications of FP8 adoption for datacenter-scale LLM serving.",
        "Large Language Models (LLMs) have recently gained attention due to their\nability to understand and generate sophisticated human-like content. However,\nensuring their safety is paramount as they might provide harmful and unsafe\nresponses. Existing LLM testing frameworks address various safety-related\nconcerns (e.g., drugs, terrorism, animal abuse) but often face challenges due\nto unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool\nthat automates the generation and execution of test cases (i.e., prompts) for\ntesting the safety of LLMs. First, we introduce a novel black-box coverage\ncriterion to generate balanced and diverse unsafe test inputs across a diverse\nset of safety categories as well as linguistic writing characteristics (i.e.,\ndifferent style and persuasive writing techniques). Second, we propose an\nLLM-based approach that leverages Retrieval Augmented Generation (RAG),\nfew-shot prompting strategies and web browsing to generate up-to-date test\ninputs. Lastly, similar to current LLM test automation techniques, we leverage\nLLMs as test oracles to distinguish between safe and unsafe test outputs,\nallowing a fully automated testing approach. We conduct an extensive evaluation\non well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms\nother LLMs when acting as the test oracle, accurately detecting unsafe\nresponses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs\nthat are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard);\nii) the results confirm that our approach can uncover nearly twice as many\nunsafe LLM behaviors with the same number of test inputs compared to currently\nused static datasets; and iii) our black-box coverage criterion combined with\nweb browsing can effectively guide the LLM on generating up-to-date unsafe test\ninputs, significantly increasing the number of unsafe LLM behaviors.",
        "State-of-the-art superconducting quantum processors containing tens to\nhundreds of qubits have demonstrated the building blocks for realizing\nfault-tolerant quantum computation. Nonetheless, a fundamental barrier to\nscaling further is the prevalence of fluctuating quantum two-level system (TLS)\ndefects that can couple resonantly to qubits, causing excess decoherence and\nenhanced gate errors. Here we introduce a scalable architecture for\nsite-specific and in-situ manipulation of TLS frequencies out of the spectral\nvicinity of our qubits. Our method is resource efficient, combining TLS\nfrequency tuning and universal single qubit control into a single on-chip\ncontrol line per qubit. We independently control each qubit's dissipative\nenvironment to dynamically improve both qubit coherence times and single qubit\ngate fidelities -- with a constant time overhead that does not scale with the\ndevice size. Over a period of 40 hours across 6 qubits, we demonstrate a $36\\%$\nimprovement in average single qubit error rates and a $17\\%$ improvement in\naverage energy relaxation times. Critically, we realize a 4-fold suppression in\nthe occurrence of TLS-induced performance outliers, and a complete reduction of\nsimultaneous outlier events. These results mark a significant step toward\novercoming the challenges that TLS defects pose to scaling superconducting\nquantum processors.",
        "The study discusses the design and fabrication of flexible pressure sensors\nusing Ecoflex\/Graphene composites. The fabricated sensor is used for the\napplication of intuitive monitoring of human quality gaits and implementation\nof the soft and comfortable shoe sole for rehabilitation of the patients with\nfoot disorder is also taken into consideration. The sensor is fabricated using\nmolding and casting technique by sandwiching the thin film Ecoflex\/Graphene\ncomposites between the copper (Cu) electrodes with the dimension of 15 x 15 mm2\nwith high sensitivity. There are five pressure sensors integrated in the shoe\nsole, a sensor at the forefoot, three sensors at the midfoot and one sensor at\nthe lower foot (heel). The behavior of the sensor is negative piezoresistive in\nwhich the resistance decreases as the pressure increases. The sensors are\nembedded in a soft and comfortable shoe sole and then integrated with a laptop\nor mobile application to monitor and analyze human gait in real-time.\nFurthermore, a dedicated Graphical User Interface (GUI) is designed to read the\ndata. The pressure sensors are integrated with ESP32 microcontroller which\nwirelessly transmit data to the GUI and smart phones which could be further\nused in the intuitive monitoring, rehabilitation of the patients with foot\ndisorder or neuromotor diseases.",
        "Human cognitive performance is an underlying factor in most of our daily\nlives, and numerous factors influence cognitive performance. In this work, we\ninvestigate how changes in sleep quality influence cognitive performance,\nmeasured from a dataset collected during a 2-month field study. We collected\ncognitive performance data (alertness) with the Psychomotor Vigilance Task\n(PVT), mobile keyboard typing metrics from participants' smartphones, and sleep\nquality metrics through a wearable sleep tracking ring. Our findings highlight\nthat specific sleep metrics like night-time heart rate, sleep latency, sleep\ntiming, sleep restfulness, and overall sleep quantity significantly influence\ncognitive performance. To strengthen the current research on cognitive\nmeasurements, we introduce smartphone typing metrics as a proxy or a\ncomplementary method for continuous passive measurement of cognitive\nperformance. Together, our findings contribute to ubiquitous computing via a\nlongitudinal case study with a novel wearable device, the resulting findings on\nthe association between sleep and cognitive function, and the introduction of\nsmartphone keyboard typing as a proxy of cognitive function.",
        "Conventional semiconductors typically have bonding states near the valence\nband maximum (VBM) and antibonding states near the conduction band minimum\n(CBM). Semiconductors with the opposite electronic configuration, namely an\nantibonding VBM and a bonding CBM, are here termed ``anti-gap semiconductors\".\nThey have been theoretically proposed to exhibit excellent optoelectronic\nproperties because of their strong tolerance to defects. However, no anti-gap\nsemiconductors have been identified so far, despite a known list of\nsemiconductors with an antibonding VBM. Here, we use high-throughput\ncomputation to identify over 100 anti-gap semiconductors. From this group, we\nanalyze the transition metal dichalcogenide MX$_2$ (M=Hf, Zr; X=S, Se) family\nin detail. In addition to verifying their defect tolerance for both electrons\nand holes using first-principles simulations, we also discovered that\nphotoexcitation of charge carriers can lead to significant lattice stiffening\nand increased thermal conductivity in anti-gap semiconductors, which can be\npotentially used as photo-driven thermal switches. Our work analyzes the\nformation of the anti-gap electronic structure and showcases their unusual\nphotoinduced lattice dynamics that can have a potential impact on their\nphotophysical applications.",
        "This article unpacks the design choices behind longstanding and newly\nproposed computational frameworks aimed at finding common grounds across\ncollective preferences and examines their potential future impacts, both\ntechnically and normatively. It begins by situating AI-assisted preference\nelicitation within the historical role of opinion polls, emphasizing that\npreferences are shaped by the decision-making context and are seldom\nobjectively captured. With that caveat in mind, we explore AI-facilitated\ncollective judgment as a discovery tool for fostering reasonable\nrepresentations of a collective will, sense-making, and agreement-seeking. At\nthe same time, we caution against dangerously misguided uses, such as enabling\nbinding decisions, fostering gradual disempowerment or post-rationalizing\npolitical outcomes.",
        "In this work, a comprehensive analysis of the mass spectra and decay\nproperties of bottomonium states using a relativistic screened potential model\nis carried out. The mass spectrum, decay constants, $E1$ transitions, $M1$\ntransitions, and annihilation decay widths are evaluated. The interpretation of\n$\\Upsilon(10355)$, $\\Upsilon(10580)$,$\\Upsilon(10860)$, and $\\Upsilon(11020)$\nas $S-D$ mixed bottomonium states are analysed. The $\\Upsilon(10355)$ state is\nconsidered to be $3S-2D$, $\\Upsilon(10580)$ state is considered to be $4S-3D$\nmixed state, the $\\Upsilon(10753)$ is obtained as purely $\\Upsilon_{1}(3D)$\nbottomonium state, and the $\\Upsilon(10860)$ and $\\Upsilon(11020)$ are deemed\nto be $5S-4D$ mixed states.",
        "Harmonic average method has been widely utilized to deal with heterogeneous\ncoefficients in solving differential equations. One remarkable advantage of the\nharmonic averaging method is that no derivative of the coefficient is needed.\nFurthermore, the coefficient matrix of the finite difference equations is an\nM-matrix which guarantees the stability of the algorithm. It has been\nnumerically observed but not theoretically proved that the method produces\nsecond order pointwise accuracy when the solution and flux are continuous even\nif the coefficient has finite discontinuities for which the method is\ninconsistent ($O(1)$ in the local truncation errors). It has been believed that\nthere are some fortunate error cancellations. The harmonic average method does\nnot converge when the solution or the flux has finite discontinuities. In this\npaper, not only we rigorously prove the second order convergence of the\nharmonic averaging method for one-dimensional interface problem when the\ncoefficient has a finite discontinuities and the solution and the flux are\ncontinuous, but also proposed an {\\em improved harmonic average method} that is\nalso second order accurate (in the $L^{\\infty}$ norm), which allows\ndiscontinuous solutions and fluxes along with the discontinuous coefficients.\nThe key in the convergence proof is the construction of the Green's function.\nThe proof shows how the error cancellations occur in a subtle way. Numerical\nexperiments in both 1D and 2D confirmed the theoretical proof of the improved\nharmonic average method.",
        "Radar ensures robust sensing capabilities in adverse weather conditions, yet\nchallenges remain due to its high inherent noise level. Existing radar odometry\nhas overcome these challenges with strategies such as filtering spurious\npoints, exploiting Doppler velocity, or integrating with inertial measurements.\nThis paper presents two novel improvements beyond the existing radar-inertial\nodometry: ground-optimized noise filtering and continuous velocity\npreintegration. Despite the widespread use of ground planes in LiDAR odometry,\nimprecise ground point distributions of radar measurements cause naive plane\nfitting to fail. Unlike plane fitting in LiDAR, we introduce a zone-based\nuncertainty-aware ground modeling specifically designed for radar. Secondly, we\nnote that radar velocity measurements can be better combined with IMU for a\nmore accurate preintegration in radar-inertial odometry. Existing methods often\nignore temporal discrepancies between radar and IMU by simplifying the\ncomplexities of asynchronous data streams with discretized propagation models.\nTackling this issue, we leverage GP and formulate a continuous preintegration\nmethod for tightly integrating 3-DOF linear velocity with IMU, facilitating\nfull 6-DOF motion directly from the raw measurements. Our approach demonstrates\nremarkable performance (less than 1% vertical drift) in public datasets with\nmeticulous conditions, illustrating substantial improvement in elevation\naccuracy. The code will be released as open source for the community:\nhttps:\/\/github.com\/wooseongY\/Go-RIO.",
        "Texture recognition is a fundamental problem in computer vision and pattern\nrecognition. Recent progress leverages feature aggregation into discriminative\ndescriptions based on convolutional neural networks (CNNs). However, modeling\nnon-local context relations through visual primitives remains challenging due\nto the variability and randomness of texture primitives in spatial\ndistributions. In this paper, we propose a graph-enhanced texture encoding\nnetwork (GraphTEN) designed to capture both local and global features of\ntexture primitives. GraphTEN models global associations through fully connected\ngraphs and captures cross-scale dependencies of texture primitives via\nbipartite graphs. Additionally, we introduce a patch encoding module that\nutilizes a codebook to achieve an orderless representation of texture by\nencoding multi-scale patch features into a unified feature space. The proposed\nGraphTEN achieves superior performance compared to state-of-the-art methods\nacross five publicly available datasets.",
        "The influence of isotope differences on ion charge state yield ratios has\nnever been studied in detail, having been considered negligible. However, we\nhave observed anomalous ion charge state distributions in the thermalization of\nenergetic atomic ions in helium gas; the charge state distributions varied\nbetween not only isotopes but also between nuclear states within the same\nnuclide. The magnitude of the observed results suggests that this anomaly is a\nuniversal phenomenon that cannot be explained by the framework of the known\nisotope effects. Nuclear spin and deformation could be key to unraveling this,\nbut the mechanisms remain an open question.",
        "Quality statistical inference requires a sufficient amount of data, which can\nbe missing or hard to obtain. To this end, prediction-powered inference has\nrisen as a promising methodology, but existing approaches are largely limited\nto Z-estimation problems such as inference of means and quantiles. In this\npaper, we apply ideas of prediction-powered inference to e-values. By doing so,\nwe inherit all the usual benefits of e-values -- such as anytime-validity,\npost-hoc validity and versatile sequential inference -- as well as greatly\nexpand the set of inferences achievable in a prediction-powered manner. In\nparticular, we show that every inference procedure that can be framed in terms\nof e-values has a prediction-powered counterpart, given by our method. We\nshowcase the effectiveness of our framework across a wide range of inference\ntasks, from simple hypothesis testing and confidence intervals to more involved\nprocedures for change-point detection and causal discovery, which were out of\nreach of previous techniques. Our approach is modular and easily integrable\ninto existing algorithms, making it a compelling choice for practical\napplications.",
        "This paper studies the ubiquitous problem of liquidating large quantities of\nhighly correlated stocks, a task frequently encountered by institutional\ninvestors and proprietary trading firms. Traditional methods in this setting\nsuffer from the curse of dimensionality, making them impractical for\nhigh-dimensional problems. In this work, we propose a novel method based on\nstochastic optimal control to optimally tackle this complex multidimensional\nproblem. The proposed method minimizes the overall execution shortfall of\nhighly correlated stocks using a reinforcement learning approach. We rigorously\nestablish the convergence of our optimal trading strategy and present an\nimplementation of our algorithm using intra-day market data.",
        "Artificial intelligence (AI) and machine learning (ML) have made tremendous\nadvancements in the past decades. From simple recommendation systems to more\ncomplex tumor identification systems, AI\/ML systems have been utilized in a\nplethora of applications. This rapid growth of AI\/ML and its proliferation in\nnumerous private and public sector applications, while successful, has also\nopened new challenges and obstacles for regulators. With almost little to no\nhuman involvement required for some of the new decision-making AI\/ML systems,\nthere is now a pressing need to ensure the responsible use of these systems.\nParticularly in federal government use-cases, the use of AI technologies must\nbe carefully governed by appropriate transparency and accountability\nmechanisms. This has given rise to new interdisciplinary fields of AI research\nsuch as \\textit{Responsible AI (RAI)}. In this position paper we provide a\nbrief overview of development in RAI and discuss some of the motivating\nprinciples commonly explored in the field. An overview of the current\nregulatory landscape relating to AI is also discussed with analysis of\ndifferent Executive Orders, policies and frameworks. We then present examples\nof how federal agencies are aiming for the responsible use of AI, specifically\nwe present use-case examples of different projects and research from the Census\nBureau on implementing the responsible use of AI. We also provide a brief\noverview for a Responsible AI Assessment Toolkit currently under-development\naimed at helping federal agencies operationalize RAI principles. Finally, a\nrobust discussion on how different policies\/regulations map to RAI principles,\nalong with challenges and opportunities for regulation\/governance of\nresponsible AI within the federal government is presented.",
        "In recent years, rapid technological advancements and expanded Internet\naccess have led to a significant rise in anomalies within network traffic and\ntime-series data. Prompt detection of these irregularities is crucial for\nensuring service quality, preventing financial losses, and maintaining robust\nsecurity standards. While machine learning algorithms have shown promise in\nachieving high accuracy for anomaly detection, their performance is often\nconstrained by the specific conditions of their training data. A persistent\nchallenge in this domain is the scarcity of labeled data for anomaly detection\nin time-series datasets. This limitation hampers the training efficacy of both\ntraditional machine learning and advanced deep learning models. To address\nthis, unsupervised transfer learning emerges as a viable solution, leveraging\nunlabeled data from a source domain to identify anomalies in an unlabeled\ntarget domain. However, many existing approaches still depend on a small amount\nof labeled data from the target domain. To overcome these constraints, we\npropose a transfer learning-based model for anomaly detection in multivariate\ntime-series datasets. Unlike conventional methods, our approach does not\nrequire labeled data in either the source or target domains. Empirical\nevaluations on novel intrusion detection datasets demonstrate that our model\noutperforms existing techniques in accurately identifying anomalies within an\nentirely unlabeled target domain.",
        "Programmers spend a significant amount of time reading code during the\nsoftware development process. This trend is amplified by the emergence of large\nlanguage models (LLMs) that automatically generate code. However, little is\nknown about the readability of the LLM-generated code and whether it is still\nimportant from practitioners' perspectives in this new era. In this paper, we\nconduct a survey to explore the practitioners' perspectives on code readability\nin the age of LLMs and investigate the readability of our LLM-based software\ndevelopment agents framework, HULA, by comparing its generated code with\nhuman-written code in real-world scenarios. Overall, the findings underscore\nthat (1) readability remains a critical aspect of software development; (2) the\nreadability of our LLM-generated code is comparable to human-written code,\nfostering the establishment of appropriate trust and driving the broad adoption\nof our LLM-powered software development platform.",
        "This study examines the performance of two flapping flat-plate foils\ninteracting with each other while generating thrust at a Reynolds number of 800\nthrough two-dimensional numerical simulations. These fluid dynamics simulations\nwere conducted with a commercial computational fluid dynamics solver\nimplementing a finite-volume method and an overset mesh capability. The foils\nperformed a combined motion involving pitching, heaving, and deviation. Both\nfoils exhibit similar movements, with one foil mirroring the other. The heaving\nand pitching motions occur at the same frequency but with a phase shift between\nthem. The effects of varying the phase shift and the minimum spacing between\nthe foils during motion were first explored. The study revealed that a maximum\nefficiency of 0.542 can be achieved by using two foils, representing an\nincrease of 13.5% relative to the optimal single-foil case. Then, the impacts\nof the deviation motion were investigated. The deviation motion was introduced\nwith a frequency twice as fast as the other motions, and a phase shift relative\nto the heaving motion. The other parameters such as the minimum spacing between\nthe foils, the heaving and pitching amplitudes, and the frequency were those of\nthe best configuration without deviation. The numerical simulations\ndemonstrated that deviation can increase efficiency further to a value of\n0.560, a relative increase of 3.95%.",
        "In the RC$^\\star$ collaboration, we simulate lattice QCD+QED using\n$C-$periodic spatial boundary conditions to ensure that locality, gauge\ninvariance, and translational invariance are preserved throughout the\ncalculation. We present our progress in computing isospin-breaking (IB)\ncorrections to the leading hadronic contribution to $(g-2)_\\mu$. We compare two\nways of including the IB corrections: the RM123 method and dynamical QCD+QED\nsimulations, both with $C-$periodic boundary conditions. The two calculations\nare performed at $\\beta=3.24$ with four flavours of $\\mathcal{O}(a)-$improved\nWilson fermions; the QCD ensemble features $SU(3)-$symmetric sea quarks plus\ncharm, while down and strange quarks are degenerate in QCD+QED gauge ensembles.",
        "Multimodal Large Language Models (MLLMs) have shown significant advancements,\nproviding a promising future for embodied agents. Existing benchmarks for\nevaluating MLLMs primarily utilize static images or videos, limiting\nassessments to non-interactive scenarios. Meanwhile, existing embodied AI\nbenchmarks are task-specific and not diverse enough, which do not adequately\nevaluate the embodied capabilities of MLLMs. To address this, we propose\nEmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs\nwith embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied\n3D scenes, each of which is rigorously selected and annotated. It covers a\nbroad spectrum of existing embodied AI tasks with significantly enhanced\ndiversity, all within a unified simulation and evaluation framework tailored\nfor MLLMs. The tasks are organized into five categories: navigation, object\ninteraction, social interaction, attribute question answering, and spatial\nquestion answering to assess different capabilities of the agents. We evaluated\nthe state-of-the-art MLLMs on EmbodiedEval and found that they have a\nsignificant shortfall compared to human level on embodied tasks. Our analysis\ndemonstrates the limitations of existing MLLMs in embodied capabilities,\nproviding insights for their future development. We open-source all evaluation\ndata and simulation framework at https:\/\/github.com\/thunlp\/EmbodiedEval."
      ]
    }
  },
  {
    "id":2412.00868,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"How resilient are language models to text perturbations",
    "start_abstract":"Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b22"
      ],
      "title":[
        "Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models"
      ],
      "abstract":[
        "The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."
      ],
      "categories":[
        "stat.ME"
      ]
    },
    "list":{
      "title":[
        "Qmod: Expressive High-Level Quantum Modeling",
        "On almost Gallai colourings in complete graphs",
        "Assessment of spectral phases of non-Hermitian quantum systems through\n  complex and singular values",
        "Bifurcations and stability of synchronized solutions in the Kuramoto\n  model with uniformly spaced natural frequencies",
        "Construction of exact refinements for the two-dimensional HB\/THB-spline\n  de Rham complex",
        "AI-powered virtual tissues from spatial proteomics for clinical\n  diagnostics and biomedical discovery",
        "Exact calculation of spectral properties of a particle interacting with\n  a one-dimensional Fermi gas in optical lattices",
        "Independent transversal blow-up of graphs",
        "WISDOM Project -- XXII. A 5% precision CO-dynamical supermassive black\n  hole mass measurement in the galaxy NGC 383",
        "Effects of Ru-doping on the magnetism of Ag3LiIr2O6, a candidate Kitaev\n  quantum spin liquid",
        "A Bayesian Record Linkage Approach to Applications in Tree Demography\n  Using Overlapping LiDAR Scans",
        "Multipoint stress mixed finite element methods for elasticity on cuboid\n  grids",
        "$q$-deformation of random partitions, determinantal structure, and\n  Riemann-Hilbert problem",
        "Efficient parameter inference in networked dynamical systems via steady\n  states: A surrogate objective function approach integrating mean-field and\n  nonlinear least squares",
        "A note on Arveson's hyperrigidity and non-degenerate C*-correspondences",
        "Explicit polynomial bounds on Dehn functions of subgroups of hyperbolic\n  groups",
        "Transfer Learning for Individualized Treatment Rules: Application to\n  Sepsis Patients Data from eICU-CRD and MIMIC-III Databases",
        "Two-Loop Master Integrals for Mixed QCD-EW Corrections to $gg \\to H$\n  Through $\\mathcal{O}(\\epsilon^2)$",
        "Towards Efficient PCSEL Design: A Data-Driven Approach for Design\n  Insights",
        "FOGGIE X: Characterizing the Small-Scale Structure of the CGM and its\n  Imprint on Observables",
        "A new Lagrangian approach to optimal control of second-order systems",
        "On Bass' conjecture of the small Davenport constant",
        "Individual causal effect estimation accounting for latent disease state\n  modification among bipolar participants in mobile health studies",
        "Recent Developments in Stochastic Inflation",
        "Engineering nonlinear Hall effect in bilayer graphene\/black phosphorus\n  heterostructures",
        "Matrix Time Series Modeling: A Hybrid Framework Combining Autoregression\n  and Common Factors",
        "Model-based time super-sampling of turbulent flow field sequences",
        "Transverse Nucleon Single-Spin Asymmetry for Single-Inclusive Hadron and\n  Jet Production at NLO Accuracy",
        "Revisiting the Extraction of Coupling Strength for Polaron Hopping from\n  $ab~initio$ Approach"
      ],
      "abstract":[
        "Quantum computing hardware is advancing at a rapid pace, yet the lack of\nhigh-level programming abstractions remains a serious bottleneck in the\ndevelopment of new applications. Widely used frameworks still rely on\ngate-level circuit descriptions, causing the algorithm's functional intent to\nbecome lost in low-level implementation details, and hindering flexibility and\nreuse. While various high-level quantum programming languages have emerged in\nrecent years - offering a significant step toward higher abstraction - many\nstill lack support for classical-like expression syntax, and native constructs\nfor useful quantum algorithmic idioms. This paper presents Qmod, a high-level\nquantum programming language designed to capture algorithmic intent in natural\nterms while delegating implementation decisions to automation. Qmod introduces\nquantum numeric variables and expressions, including digital fixed-point\narithmetic tuned for compact representations and optimal resource usage. Beyond\ndigital encoding, Qmod also supports non-digital expression modes - phase and\namplitude encoding - frequently exploited by quantum algorithms to achieve\ncomputational advantages. We describe the language's constructs, demonstrate\npractical usage examples, and outline future work on evaluating Qmod across a\nbroader set of use cases.",
        "For $t \\in \\mathbb{N}$, we say that a colouring of $E(K_n)$ is\n$\\textit{almost}$ $t$-$\\textit{Gallai}$ if no two rainbow $t$-cliques share an\nedge. Motivated by a lemma of Berkowitz on bounding the modulus of the\ncharacteristic function of clique counts in random graphs, we study the maximum\nnumber $\\tau_t(n)$ of rainbow $t$-cliques in an almost $t$-Gallai colouring of\n$E(K_n)$. For every $t \\ge 4$, we show that $n^{2-o(1)} \\leq \\tau_t(n) =\no(n^2)$. For $t=3$, surprisingly, the behaviour is substantially different. Our\nmain result establishes that $$\\left ( \\frac{1}{2}-o(1) \\right ) n\\log n \\le\n\\tau_3(n) = O\\big (n^{\\sqrt{2}}\\log n \\big ),$$ which gives the first\nnon-trivial improvements over the simple lower and upper bounds. Our proof\ncombines various applications of the probabilistic method and a generalisation\nof the edge-isoperimetric inequality for the hypercube.",
        "Chaotic behavior or lack thereof in non-Hermitian systems is often diagnosed\nvia spectral analysis of associated complex eigenvalues. Very recently,\nsingular values of the associated non-Hermitian systems have been proposed as\nan effective measure to study dissipative quantum chaos. Motivated by the rich\nproperties of non-Hermitian power-law banded random matrices and its promise as\na platform to study localized and delocalized phases in non-Hermitian systems,\nwe make an in-depth study to assess different spectral phases of these matrices\nthrough the lens of both complex eigenvalues and singular values. Remarkably,\nthe results from complex spectra and singular value analysis are seemingly\ndifferent, thereby necessitating caution while identifying different phases. We\nalso exemplify our findings by studying a non-Hermitian Hamiltonian with a\ncomplex on-site disorder. Our work indicates that systems, where disorder is\npresent both in the Hermitian and non-Hermitian segments of a Hamiltonian, are\nsensitive to the specific diagnostic tool that needs to be employed to study\nquantum chaos.",
        "We consider the classical Kuramoto model (KM) with natural frequencies and\nits continuum limit (CL), and discuss the existence of synchronized solutions\nand their bifurcations and stability. We specifically assume that the frequency\nfunction is symmetric and linear in the CL, so that the natural frequencies are\nevenly spaced in the KM. We show that in the KM, $O(2^n)$ one-parameter\nfamilies of synchronized solutions are born and $O(2^n)$ {saddle-node and}\npitchfork bifurcations occur at least, when the node number $n$ is odd and\ntends to infinity. Moreover, we prove that the family of synchronized solutions\nobtained in the previous work suffers a saddle-node bifurcation at which its\nstability changes from asymptotically stable to unstable and the other families\nof synchronized solutions are unstable in the KM. For the CL, we show that the\none-parameter family of synchronized solutions obtained in the previous work is\nthe only continuous one and there exist uncountably many one-parameter families\nof noncontinuous synchronized solutions and that the former is asymptotically\nstable and the latter are unstable.",
        "Studying the de Rham complex is a natural choice when working with problems\nin electromagnetics and fluid mechanics. By discretizing the complex correctly,\nit is possible to attain stable numerical methods to tackle these problems. An\nimportant consideration when constructing the discrete complex is that it must\npreserve the cohomology structure of the original one. This property is not\nguaranteed when the discrete function spaces chosen are hierarchical B-splines.\nResearch shows that a poor choice of refinement domains may give rise to\nspurious harmonic forms that ruin the accuracy of solutions, even for the\nsimplest partial differential equations. Another crucial aspect to consider in\nthe hierarchical setting is the notion of admissibility, as it is possible to\nobtain optimal convergence rates of numerical solutions by limiting the\nmulti-level interaction of basis functions. We will focus on the\ntwo-dimensional de Rham complex over the unit square $\\Omega \\subseteq\n\\mathbb{R}^2$. In this scenario, the discrete de Rham complex should be exact,\nand we provide both the theoretical and the algorithm-implementation framework\nto ensure this is the case. Moreover, we show that, under a common restriction,\nthe admissibility class of the first space of the discrete complex persists\nthroughout the remaining spaces. Finally, we include numerical results that\nmotivate the importance of the previous concerns for the vector Laplace and\nMaxwell eigenvalue problems.",
        "Spatial proteomics technologies have transformed our understanding of complex\ntissue architectures by enabling simultaneous analysis of multiple molecular\nmarkers and their spatial organization. The high dimensionality of these data,\nvarying marker combinations across experiments and heterogeneous study designs\npose unique challenges for computational analysis. Here, we present Virtual\nTissues (VirTues), a foundation model framework for biological tissues that\noperates across the molecular, cellular and tissue scale. VirTues introduces\ninnovations in transformer architecture design, including a novel tokenization\nscheme that captures both spatial and marker dimensions, and attention\nmechanisms that scale to high-dimensional multiplex data while maintaining\ninterpretability. Trained on diverse cancer and non-cancer tissue datasets,\nVirTues demonstrates strong generalization capabilities without task-specific\nfine-tuning, enabling cross-study analysis and novel marker integration. As a\ngeneralist model, VirTues outperforms existing approaches across clinical\ndiagnostics, biological discovery and patient case retrieval tasks, while\nproviding insights into tissue function and disease mechanisms.",
        "By using the exact Bethe wavefunctions of the one-dimensional Hubbard model\nwith $N$ spin-up fermions and one spin-down impurity, we derive an analytic\nexpression of the impurity form factor, in the form of a determinant of a\n$(N+1)$ by $(N+1)$ matrix. This analytic expression enables us to exactly\ncalculate spectral properties of one-dimensional Fermi polarons in lattices,\nwhen the masses of the impurity particle and the Fermi bath are equal. We\npresent the impurity spectral function as functions of the on-site interaction\nstrength and the filling factor of the Fermi bath, and discuss the origin of\nFermi singularities in the spectral function at small momentum and the\nemergence of polaron quasiparticles at large momentum near the boundary of\nBrillouin zone. Our analytic expression of the impurity form factors pave the\nway to exploring the intriguing dynamics of a particle interacting with a Fermi\nbath. Our exact predictions on the impurity spectral function could be directly\nexamined in cold-atom laboratories by using the radio-frequency spectroscopy\nand Ramsey spectroscopy.",
        "In an $r$-partite graph, an independent transversal of size $s$ (ITS)\nconsists of $s$ vertices from each part forming an independent set. Motivated\nby a question from Bollob\\'as, Erd\\H{o}s, and Szemer\\'edi (1975), Di Braccio\nand Illingworth (2024) inquired about the minimum degree needed to ensure an $n\n\\times \\cdots \\times n$ $r$-partite graph contains $K_r(s)$, a complete\n$r$-partite graph with $s$ vertices in each part. We reformulate this as\nfinding the smallest $n$ such that any $n \\times \\cdots \\times n$ $r$-partite\ngraph with maximum degree $\\Delta$ has an ITS. For any $\\varepsilon>0$, we\nprove the existence of a $\\gamma>0$ ensuring that if $G$ is a multipartite\ngraph partitioned as $(V_1, V_2, \\ldots, V_r)$, where the average degree of\neach part $V_i$ is at most $D$, the maximum degree of any vertex to any part\n$V_i$ is at most $\\gamma D$, and the size of each part $V_i$ is at least $(s +\n\\varepsilon)D$, then $G$ possesses an ITS. The constraint $(s + \\varepsilon)D$\non the part size is tight. This extends results of Loh and Sudakov (2007),\nGlock and Sudakov (2022), and Kang and Kelly (2022). We also show that any $n\n\\times \\cdots \\times n$ $r$-partite graph with minimum degree at least\n$\\left(r-1-\\frac{1}{2s^2}\\right)n$ contains $K_r(s)$ and provide a relative\nTur\\'an-type result. Additionally, this paper explores counting ITSs in\nmultipartite graphs.",
        "We present a measurement of the supermassive black hole (SMBH) mass of the\nnearby lenticular galaxy NGC 383, based on Atacama Large\nMillimeter\/sub-millimeter Array (ALMA) observations of the $^{12}$CO(2-1)\nemission line with an angular resolution of $0.''050\\times0.''024$\n($\\approx16\\times8$ pc$^2$). These observations spatially resolve the nuclear\nmolecular gas disc down to $\\approx41,300$ Schwarzschild radii and the SMBH\nsphere of influence by a factor of $\\approx24$ radially, better than any other\nSMBH mass measurement using molecular gas to date. The high resolution enables\nus to probe material with a maximum circular velocity of $\\approx1040$ km\/s,\neven higher than those of the highest-resolution SMBH mass measurements using\nmegamasers. We detect a clear Keplerian increase (from the outside in) of the\nline-of-sight rotation velocities, a slight offset between the gas disc\nkinematic (i.e. the position of the SMBH) and morphological (i.e. the centre of\nthe molecular gas emission) centres, an asymmetry of the innermost rotation\nvelocity peaks and evidence for a mild position angle warp and\/or non-circular\nmotions within the central $\\approx0.''3$. By forward modelling the mass\ndistribution and ALMA data cube, we infer a SMBH mass of\n$(3.58\\pm0.19)\\times10^9$ M$_\\odot$ ($1\\sigma$ confidence interval), more\nprecise ($5\\%$) but consistent within $\\approx1.4\\sigma$ with the previous\nmeasurement using lower-resolution molecular gas data. Our measurement\nemphasises the importance of high spatial resolution observations for precise\nSMBH mass determinations.",
        "We report our investigations on Ag3LiIr1.4Ru0.6O6, which results from the Ru\nsubstitution in the Kitaev quantum spin liquid candidate Ag3LiIr2O6. It\ncrystallizes in the monoclinic C2\/m space group like its parent compound,\nAg3LiIr2O6. Our susceptibility measurements reveal an effective moment = 2.6\nmuB, which is higher than the moments of the parent compound and less than that\nof the Ru-analog (Ag3LiRu2O6), suggesting the presence of magnetic Ir4+ (Jeff=\n1\/2) and Ru4+ (S=1). Bulk magnetic susceptibility suggests long-range order\n(LRO)at T~20 K, whereas no clear signature is present in the heat capacity.\nLikewise, there is a loss of the 7Li NMR spectral intensity around T~20 K as\nexpected at the onset of LRO, but a complete wipe-out is not seen in contrast\nto the result in Ag3LiIr2O6. There is also a T~20 K anomaly in the 7Li NMR\nrelaxation rate and also a fall in the 7Li NMR shift with decreasing\ntemperature. These results suggest LRO at T~20 K in Ag3LiIr1.4Ru0.6O6. However,\nat low-T below 10 K, we observe a power law variation in magnetic heat capacity\nand spin lattice relaxation rate, temperature-independent-7K, and no further\nloss of the 7Li NMR spectral intensity. These results might suggest the\npersistence or stabilisation of a quantum spin liquid-like phase, perhaps from\na fraction of the sample in Ag3LiIr1.4Ru0.6O6 below 10 K. Our muon spin\nrelaxation measurements suggest ordering around 20 K, consistent with our other\nprobes. It appears that the main effect of Ru-substitution is to shift the LRO\nto a higher temperature in comparison with Ag3LiIr2O6, though there are\nsignatures of a novel phase below about 10 K.",
        "In the information age, it has become increasingly common for data containing\nrecords about overlapping individuals to be distributed across multiple\nsources, making it necessary to identify which records refer to the same\nindividual. The goal of record linkage is to estimate this unknown structure in\nthe absence of a unique identifiable attribute. We introduce a Bayesian\nhierarchical record linkage model for spatial location data motivated by the\nestimation of individual specific growth-size curves for conifer species using\ndata derived from overlapping LiDAR scans. Annual tree growth may be estimated\ndependent upon correctly identifying unique individuals across scans in the\npresence of noise. We formalize a two-stage modeling framework, connecting the\nrecord linkage model and a flexible downstream individual tree growth model,\nthat provides robust uncertainty quantification and propagation through both\nstages of the modeling pipeline via an extension of the linkage-averaging\napproach of Sadinle (2018). In this paper, we discuss the two-stage model\nformulation, outline the computational strategies required to achieve\nscalability, assess the model performance on simulated data, and fit the model\nto a bi-temporal dataset derived from LiDAR scans of the Upper Gunnison\nWatershed provided by the Rocky Mountain Biological Laboratory to assess the\nimpact of key topographic covariates on the growth behavior of conifer species\nin the Southern Rocky Mountains (USA).",
        "We develop multipoint stress mixed finite element methods for linear\nelasticity with weak stress symmetry on cuboid grids, which can be reduced to a\nsymmetric and positive definite cell-centered system. The methods employ the\nlowest-order enhanced Raviart-Thomas finite element space for the stress and\npiecewise constant displacement. The vertex quadrature rule is employed to\nlocalize the interaction of stress degrees of freedom, enabling local stress\nelimination around each vertex. We introduce two methods. The first method uses\na piecewise constant rotation, resulting in a cell-centered system for the\ndisplacement and rotation. The second method employs a continuous piecewise\ntrilinear rotation and the vertex quadrature rule for the asymmetry bilinear\nforms, allowing for further elimination of the rotation and resulting in a\ncell-centered system for the displacement only. Stability and error analysis is\nperformed for both methods. For the stability analysis of the second method, a\nnew auxiliary H-curl conforming matrix-valued space is constructed, which forms\nan exact sequence with the stress space. A matrix-matrix inf-sup condition is\nshown for the curl of this auxiliary space and the trilinear rotation space.\nFirst-order convergence is established for all variables in their natural\nnorms, as well as second-order superconvergence of the displacement at the cell\ncenters. Numerical results are presented to verify the theory.",
        "We study $q$-deformation of the probability measure on partitions, i.e.,\n$q$-deformed random partitions. We in particular consider the $q$-Plancherel\nmeasure and show a determinantal formula for the correlation function using a\n$q$-deformation of the discrete Bessel kernel. We also investigate\nRiemann-Hilbert problems associated with the corresponding orthogonal\npolynomials and obtain $q$-Painlev{\\'e} equations from the $q$-difference Lax\nformalism.",
        "In networked dynamical systems, inferring governing parameters is crucial for\npredicting nodal dynamics, such as gene expression levels, species abundance,\nor population density. While many parameter estimation techniques rely on\ntime-series data, particularly systems that converge over extreme time ranges,\nonly noisy steady-state data is available, requiring a new approach to infer\ndynamical parameters from noisy observations of steady states. However, the\ntraditional optimization process is computationally demanding, requiring\nrepeated simulation of coupled ordinary differential equations (ODEs). To\novercome these limitations, we introduce a surrogate objective function that\nleverages decoupled equations to compute steady states, significantly reducing\ncomputational complexity. Furthermore, by optimizing the surrogate objective\nfunction, we obtain steady states that more accurately approximate the ground\ntruth than noisy observations and predict future equilibria when topology\nchanges. We empirically demonstrate the effectiveness of the proposed method\nacross ecological, gene regulatory, and epidemic networks. Our approach\nprovides an efficient and effective way to estimate parameters from\nsteady-state data and has the potential to improve predictions in networked\ndynamical systems.",
        "We revisit the results of Kim, and of Katsoulis and Ramsey concerning\nhyperrigidity for non-degenerate C*-correspondences. We show that the tensor\nalgebra is hyperrigid, if and only if Katsura's ideal acts non-degenerately, if\nand only if Katsura's ideal acts non-degenerately under any representation.\nThis gives a positive answer to the question of Katsoulis and Ramsey, showing\nthat their necessary condition and their sufficient condition for hyperrigidity\nof the tensor algebra are equivalent. Non-degeneracy of the left action of\nKatsura's ideal was also shown by Kim to be equivalent to hyperrigidity for the\nselfadjoint operator space associated with the C*-correspondence, and our\napproach provides a simplified proof of this result as well. In the process we\nrevisit Arveson's criterion connecting maximality with the unique extension\nproperty and hyperrigidity, in conjunction with the work of Salomon on\ngenerating sets.",
        "In 1999 Brady constructed the first example of a non-hyperbolic finitely\npresented subgroup of a hyperbolic group by fibring a non-positively curved\ncube complex over the circle. We show that his example has Dehn function\nbounded above by $n^{96}$. This provides the first explicit polynomial upper\nbound on the Dehn function of a finitely presented non-hyperbolic subgroup of a\nhyperbolic group. We also determine the precise hyperbolicity constant for the\n$1$-skeleton of the universal cover of the cube complex in Brady's construction\nwith respect to the $4$-point condition for hyperbolicity.",
        "Modern precision medicine aims to utilize real-world data to provide the best\ntreatment for an individual patient. An individualized treatment rule (ITR)\nmaps each patient's characteristics to a recommended treatment scheme that\nmaximizes the expected outcome of the patient. A challenge precision medicine\nfaces is population heterogeneity, as studies on treatment effects are often\nconducted on source populations that differ from the populations of interest in\nterms of the distribution of patient characteristics. Our research goal is to\nexplore a transfer learning algorithm that aims to address the population\nheterogeneity problem and obtain targeted, optimal, and interpretable ITRs. The\nalgorithm incorporates a calibrated augmented inverse probability weighting\n(CAIPW) estimator for the average treatment effect (ATE) and employs value\nfunction maximization for the target population using Genetic Algorithm (GA) to\nproduce our desired ITR. To demonstrate its practical utility, we apply this\ntransfer learning algorithm to two large medical databases, Electronic\nIntensive Care Unit Collaborative Research Database (eICU-CRD) and Medical\nInformation Mart for Intensive Care III (MIMIC-III). We first identify the\nimportant covariates, treatment options, and outcomes of interest based on the\ntwo databases, and then estimate the optimal linear ITRs for patients with\nsepsis. Our research introduces and applies new techniques for data fusion to\nobtain data-driven ITRs that cater to patients' individual medical needs in a\npopulation of interest. By emphasizing generalizability and personalized\ndecision-making, this methodology extends its potential application beyond\nmedicine to fields such as marketing, technology, social sciences, and\neducation.",
        "We consider mixed strong-electroweak corrections to Higgs production via\ngluon fusion, in which the Higgs boson couples to the top quark. Using the\nmethod of differential equations, we compute all of the master integrals that\ncontribute to this process at two loops through $\\mathcal{O}(\\epsilon^2)$ in\nthe dimensional regularization parameter $\\epsilon = (d-4)\/2$, keeping full\nanalytic dependence on the top quark, Higgs, W, and Z boson masses. We present\nthe results for these master integrals in terms of iterated integrals whose\nkernels depend on elliptic curves.",
        "We present a data-driven design approach for photonic crystals to achieve\nhigh efficiency in photonic crystal surface-emitting lasers (PCSELs). By\ndiscretizing the photonic crystal structure into a grid, we enable the\ngeneration of arbitrary lattice designs. Multiple fully connected layers\ncombined with a position embedding module extract essential features from the\nphotonic crystal designs, while coupled-wave theory (CWT) is used to evaluate\nthe efficiency (based on the ratio of surface-emitting to edge-emitting\nresonant) and quality factor Q. We introduce the Neural Networks (NNs) model to\nevaluate the structures, and to find a better performance design according to\nthe evaluation result. The model achieves high prediction accuracy, with\nPearson correlation coefficients of 0.780 for SEE and 0.887 for the\nlog-transformed Q. Additionally, we perform Shapley value analysis to identify\nthe most important Fourier coefficients, providing insights into the factors\nthat impact the performance of PCSEL designs. Our work speeds up the design\nprocess and offers valuable guidance for optimizing high-performance PCSELs,\nsupporting the development of fully photonic design automation (PDA).",
        "One of the main unknowns in galaxy evolution is how gas flows into and out of\ngalaxies in the circumgalactic medium (CGM). Studies observing the CGM in\nabsorption using multiple or extended background objects suggest a high degree\nof variation on relatively small ($\\lesssim 1$ kpc) spatial scales. Similarly,\nhigh-resolution simulations generally exhibit small-scale substructure in the\ngas around galaxies. We examine the small-scale structure of the $z = 1$ CGM\nusing simulations from the FOGGIE (Figuring Out Gas & Galaxies in Enzo)\nproject. We select gaseous substructures (\"clumps\") by their local overdensity\nand investigate their physical properties, including temperature, metallicity,\nand kinematics with respect to the galaxy and the nearby surroundings. FOGGIE\nresolves clumps down to sphericalized radii $R \\sim 0.25$ kpc at $z = 1$. The\ndistribution of clumps peaks at $\\sim 10^5$ $\\rm M_{\\odot}$ and $10^{4}$ K,\nconsistent with relatively condensed, cool gas with a slight preference for\ninflow-like velocities. Many clumps show internal temperature and density\nvariations, and thus internally varying ionization levels for key diagnostic\nions such as HI, MgII, and OVI. The average metallicity in clumps is about a\nfactor 1.5--2$\\times$ lower in metallicity than nearby gas, suggesting that the\nmetals are not well-mixed between structured and diffuse CGM, which may have\nimplications for observational metallicity estimations of dense CGM clouds. We\nestimate the survivability of CGM clumps and find that structures larger than\n0.5 kpc are generally long-lived. Finally, we qualitatively compare the\nsimulated cloud properties to Milky Way high-velocity clouds.",
        "In this work, we propose and study a new approach to formulate the optimal\ncontrol problem of second-order differential equations, with a particular\ninterest in those derived from force-controlled Lagrangian systems. The\nformulation results in a new hyperregular control Langrangian and, thus, a new\ncontrol Hamiltonian whose equations of motion provide necessary optimality\nconditions. We compare this approach to Pontryagin's maximum principle (PMP) in\nthis setting, providing geometric insight into their relation. This leads us to\ndefine an extended Tulczyjew's triple with controls. Moreover, we study the\nrelationship between Noether symmetries of this new formulation and those of\nthe PMP.",
        "Let $G$ be a finite group. The small Davenport constant $\\mathsf d(G)$ of $G$\nis the maximal integer $\\ell$ such that there is a sequence of length $\\ell$\nover $G$ which has no nonempty product-one subsequence. In 2007, Bass\nconjectured that $\\mathsf d(G_{m,n})=m+n-2$, where $G_{m,n}=\\langle x, y|\nx^m=y^n=1, x^{-1}yx=y^s\\rangle$, and $s$ has order $m$ modulo $n$. In this\npaper, we confirm the conjecture for any group $G_{m,n}$ with additional\nconditions that $s$ has order $m$ modulo $q$, for every prime divisor $q$ of\n$n$. Moreover, we solve the associated inverse problem characterizing the\nstructure of any product-one free sequence with extremal length $\\mathsf\nd(G_{m,n})$. Our results generalize some obtained theorems on this problem.",
        "Individuals with bipolar disorder tend to cycle through disease states such\nas depression and mania. The heterogeneous nature of disease across states\ncomplicates the evaluation of interventions for bipolar disorder patients, as\nvaried interventional success is observed within and across individuals. In\nfact, we hypothesize that disease state acts as an effect modifier for the\ncausal effect of a given intervention on health outcomes. To address this\ndilemma, we propose an N-of-1 approach using an adapted autoregressive hidden\nMarkov model, applied to longitudinal mobile health data collected from\nindividuals with bipolar disorder. This method allows us to identify a latent\nvariable from mobile health data to be treated as an effect modifier between\nthe exposure and outcome of interest while allowing for missing data in the\noutcome. A counterfactual approach is employed for causal inference and to\nobtain a g-formula estimator to recover said effect. The performance of the\nproposed method is compared with a naive approach across extensive simulations\nand application to a multi-year smartphone study of bipolar patients,\nevaluating the individual effect of digital social activity on sleep duration\nacross different latent disease states.",
        "This article is dedicated to the memory of Alexei Starobinsky. I begin with\nsome recollections of him and then review the generalization of his wonderful\nstochastic formalism from scalar potential models to theories which interact\nwith fermions and photons, and finally to theories with derivative interactions\nsuch as nonlinear sigma models and gravity. This entails effective potentials\ngenerated by the usual field-dependent masses, as well as by field-dependent\nfield strengths, and by field-dependent Hubble parameters. I also discuss\nsecular loop corrections which cannot be captured by stochastic techniques.",
        "Two-dimensional van der Waals materials offer a highly tunable platform for\ngenerating emergent quantum phenomena through symmetry breaking.\nStacking-induced symmetry breaking at interfaces provides an effective method\nto modulate their electronic properties for functional devices. Here, we\nstrategically stack bilayer graphene with black phosphorus, a low-symmetry\nsemiconductor, to break the symmetries and induce the nonlinear Hall effect\n(NLHE) that can persist up to room temperature. Intriguingly, it is found the\nNLHE undergoes sign reversals by varying the electrical displacement field\nunder fixed carrier density. The scaling analysis reveals that the sign\nreversal of the NLHE is contributed from both the Berry curvature dipole (BCD)\nand extrinsic scatterings. The displacement field-induced sign reversal of the\nBCD indicates asymmetric distributions of Berry curvature hot spots across\ndifferent Fermi pockets in bilayer graphene. Our findings suggest that symmetry\nengineering of van der Waals heterostructures is promising for room-temperature\napplications based on nonlinear quantum devices, such as high-frequency\nrectifiers and wireless charging.",
        "Matrix-valued time series analysis has gained prominence in econometrics and\nfinance due to the increasing availability of high-dimensional data with\ninherent matrix structures. Traditional approaches, such as Matrix\nAutoregressive (MAR) models and Dynamic Matrix Factor (DMF) models, often\nimpose restrictive assumptions that may not align with real-world data\ncomplexities. To address this gap, we propose a novel Matrix Autoregressive\nwith Common Factors (MARCF) model, which bridges the gap between MAR and DMF\nframeworks by introducing common bases between predictor and response\nsubspaces. The MARCF model achieves significant dimension reduction and enables\na more flexible and interpretable factor representation of dynamic\nrelationships. We develop a computationally efficient estimator and a gradient\ndescent algorithm. Theoretical guarantees for computational and statistical\nconvergence are provided, and extensive simulations demonstrate the robustness\nand accuracy of the model. Applied to a multinational macroeconomic dataset,\nthe MARCF model outperforms existing methods in forecasting and provides\nmeaningful insights into the interplay between countries and economic factors.",
        "We propose a novel method for model-based time super-sampling of turbulent\nflow fields. The key enabler is the identification of an empirical Galerkin\nmodel from the projection of the Navier-Stokes equations on a data-tailored\nbasis. The basis is obtained from a Proper Orthogonal Decomposition (POD) of\nthe measured fields. Time super-sampling is thus achieved by a time-marching\nintegration of the identified dynamical system, taking the original snapshots\nas initial conditions. Temporal continuity of the reconstructed velocity fields\nis achieved through a forward-backwards integration between consecutive\nmeasured Particle Image Velocimetry measurements of a turbulent jet flow. The\nresults are compared with the interpolation of the POD temporal coefficients\nand the low-order reconstruction of data measured at a higher sampling rate. In\nboth cases, the results obtained show the ability of the method to reconstruct\nthe dynamics of the flow with small errors during several flow characteristic\ntimes.",
        "We investigate the single-spin asymmetry for the single-inclusive production\nof hadrons and jets in collisions of transversely polarized nucleons and\nunpolarized leptons, $\\ell N^\\uparrow \\to (h\\,\\mathrm{or\\,jet})X$. We compute\nthe spin-dependent cross section within collinear twist-3 factorization in\nperturbative QCD at next-to-leading order (NLO) accuracy. In this approach,\nmultiparton correlations generate a non-vanishing effect. For the present\npaper, we focus on correlations in the nucleon initial-state rather than in the\nfragmentation process. We explicitly verify that collinear twist-3\nfactorization is valid at the one-loop level. Our analytical results show that\nat NLO the relevant multiparton correlation functions in the nucleon are probed\non their full support in momentum fractions. Our numerical analysis for\ncollisions at the Electron-Ion Collider indicates that the NLO corrections can\nbe large and are sensitive to the functional form of the twist-3 correlation\nfunctions.",
        "Accurately determining the coupling strength between polaron states is\nessential to describe charge-hopping transport in materials. In this work, we\nrevisit methodologies to extract coupling strengths using ab initio approaches.\nOur findings underscore the critical role of incorporating anharmonic effects\nin the model Hamiltonian when analyzing total energy variations along reaction\ncoordinates. Furthermore, we demonstrate that coupling strength extraction\nbased on total energy calculated from ab initio approaches is fundamentally\nlimited due to the stabilization of diabatic states, which do not involve\ncoupling strength. We demonstrate that such limitation exists in both DFT+U and\nHSE hybrid functionals, which are widely used in the study of polaron\ntransport. Instead, we suggest extracting coupling strength directly from the\nelectronic structure of a system in neutral conditions. The neutral condition\navoids overestimating coupling strength due to additional energy splitting\nbetween bonding and anti-bonding states from the charging energy. This study\nhighlights the limitations of existing methods and introduces a robust\nframework for accurately extracting coupling parameters, paving the way for\nimproved modeling of charge transport in complex materials."
      ]
    }
  }
]