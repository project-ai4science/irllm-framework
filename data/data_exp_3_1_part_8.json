[
  {
    "id":2411.1945,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "start_abstract":"It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products.",
    "start_categories":[
      "physics.gen-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Auto-Encoding Variational Bayes"
      ],
      "abstract":[
        "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "ALPET: Active Few-shot Learning for Citation Worthiness Detection in\n  Low-Resource Wikipedia Languages",
        "Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline\n  Comparison for Semantic Similarity",
        "ResBench: Benchmarking LLM-Generated FPGA Designs with Resource\n  Awareness",
        "D3PO: Preference-Based Alignment of Discrete Diffusion Models",
        "Pre-Equalization Aided Grant-Free Massive Access in Massive MIMO System",
        "MSV-Mamba: A Multiscale Vision Mamba Network for Echocardiography\n  Segmentation",
        "A stochastic programming approach for the scheduling of medical\n  interpreting service under uncertainty",
        "Extinction and Extirpation Conditions in Coalescent and Ecotonal\n  Metacommunities",
        "A dynamical proof of Matui's absorption theorem",
        "Semantic Communications Services within Generalist Operated Networks",
        "Unified Approaches in Self-Supervised Event Stream Modeling: Progress\n  and Prospects",
        "A note on finiteness properties of vertex stabilisers",
        "Universality of Packing Dimension Estimates for Spectral Measures of\n  Quasiperiodic Operators: Monotone Potentials",
        "GameFactory: Creating New Games with Generative Interactive Videos",
        "Handling Uncertainty in Health Data using Generative Algorithms",
        "Graph Neural Networks for Efficient AC Power Flow Prediction in Power\n  Grids",
        "First Token Probability Guided RAG for Telecom Question Answering",
        "Optimal Traffic Allocation for Multi-Slot Sponsored Search: Balance of\n  Efficiency and Fairness",
        "Local time-integration for Friedrichs' systems",
        "On the Resolution of Partial Differential Equations for Lattice\n  Structures on Smooth Manifolds",
        "Grothendieck rings of localizations of the integers as ordered abelian\n  groups",
        "Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models\n  for Wireless Networks",
        "I2V3D: Controllable image-to-video generation with 3D guidance",
        "Spin-$s$ $Q$-systems: Twist and Open Boundaries",
        "Counterexamples to a conjecture of Adams",
        "Serenade: A Singing Style Conversion Framework Based On Audio Infilling",
        "Comparison of CNN-based deep learning architectures for unsteady CFD\n  acceleration on small datasets",
        "Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented\n  Manipulation",
        "Radial symmetry and sharp asymptotic behaviors of nonnegative solutions\n  to weighted doubly $D^{1,p}$-critical quasi-linear nonlocal elliptic\n  equations with Hardy potential"
      ],
      "abstract":[
        "Citation Worthiness Detection (CWD) consists in determining which sentences,\nwithin an article or collection, should be backed up with a citation to\nvalidate the information it provides. This study, introduces ALPET, a framework\ncombining Active Learning (AL) and Pattern-Exploiting Training (PET), to\nenhance CWD for languages with limited data resources. Applied to Catalan,\nBasque, and Albanian Wikipedia datasets, ALPET outperforms the existing CCW\nbaseline while reducing the amount of labeled data in some cases above 80\\%.\nALPET's performance plateaus after 300 labeled samples, showing it suitability\nfor low-resource scenarios where large, labeled datasets are not common. While\nspecific active learning query strategies, like those employing K-Means\nclustering, can offer advantages, their effectiveness is not universal and\noften yields marginal gains over random sampling, particularly with smaller\ndatasets. This suggests that random sampling, despite its simplicity, remains a\nstrong baseline for CWD in constraint resource environments. Overall, ALPET's\nability to achieve high performance with fewer labeled samples makes it a\npromising tool for enhancing the verifiability of online content in\nlow-resource language settings.",
        "This paper investigates the efficacy of RWKV, a novel language model\narchitecture known for its linear attention mechanism, for generating sentence\nembeddings in a zero-shot setting. I conduct a layer-wise analysis to evaluate\nthe semantic similarity captured by embeddings from different hidden layers of\na pre-trained RWKV model. The performance is assessed on the Microsoft Research\nParaphrase Corpus (MRPC) dataset using Spearman correlation and compared\nagainst a GloVe-based baseline. My results indicate that while RWKV embeddings\ncapture some semantic relatedness, they underperform compared to the GloVe\nbaseline in terms of Spearman correlation. I also analyze the inference time\nand GPU memory usage, highlighting the computational trade-offs associated with\nRWKV embeddings. The findings suggest that while RWKV offers potential\nadvantages in terms of linear scaling, its zero-shot sentence embedding quality\nfor semantic similarity tasks requires further investigation and potential\ntask-specific fine-tuning to match or exceed simpler baselines.",
        "Field-Programmable Gate Arrays (FPGAs) are widely used in modern hardware\ndesign, yet writing Hardware Description Language (HDL) code for FPGA\nimplementation remains a complex and time-consuming task. Large Language Models\n(LLMs) have emerged as a promising tool for HDL generation, but existing\nbenchmarks for LLM-based code generation primarily focus on functional\ncorrectness while overlooking hardware resource usage. Furthermore, current\nbenchmarks offer limited diversity and do not fully represent the wide range of\nreal-world FPGA applications. To address these shortcomings, we introduce\nResBench, the first resource-focused benchmark explicitly designed to\ndistinguish between resource-optimized and inefficient LLM-generated HDL code.\nResBench consists of 56 problems across 12 categories, covering applications\nfrom finite state machines to financial computing. Our open-source evaluation\nframework automatically tests LLMs by generating Verilog code, verifying\ncorrectness, and measuring resource usage. The experiments, which primarily\nanalyze Lookup Table (LUT) usage, reveal significant differences among LLMs,\ndemonstrating ResBench's capability to identify models that generate more\nresource-optimized FPGA designs.",
        "Diffusion models have achieved state-of-the-art performance across multiple\ndomains, with recent advancements extending their applicability to discrete\ndata. However, aligning discrete diffusion models with task-specific\npreferences remains challenging, particularly in scenarios where explicit\nreward functions are unavailable. In this work, we introduce Discrete Diffusion\nDPO (D3PO), the first adaptation of Direct Preference Optimization (DPO) to\ndiscrete diffusion models formulated as continuous-time Markov chains. Our\napproach derives a novel loss function that directly fine-tunes the generative\nprocess using preference data while preserving fidelity to a reference\ndistribution. We validate D3PO on a structured binary sequence generation task,\ndemonstrating that the method effectively aligns model outputs with preferences\nwhile maintaining structural validity. Our results highlight that D3PO enables\ncontrolled fine-tuning without requiring explicit reward models, making it a\npractical alternative to reinforcement learning-based approaches. Future\nresearch will explore extending D3PO to more complex generative tasks,\nincluding language modeling and protein sequence generation, as well as\ninvestigating alternative noise schedules, such as uniform noising, to enhance\nflexibility across different applications.",
        "The spatial diversity and multiplexing advantages of massive\nmulti-input-multi-output (mMIMO) can significantly improve the capacity of\nmassive non-orthogonal multiple access (NOMA) in machine type communications.\nHowever, state-of-the-art grant-free massive NOMA schemes for mMIMO systems\nrequire accurate estimation of random access channels to perform activity\ndetection and the following coherent data demodulation, which suffers from\nexcessive pilot overhead and access latency. To address this, we propose a\npre-equalization aided grant-free massive access scheme for mMIMO systems,\nwhere an iterative detection scheme is conceived. Specifically, the base\nstation (BS) firstly activates one of its antennas (i.e., beacon antenna) to\nbroadcast a beacon signal, which facilitates the user equipment (UEs) to\nperform downlink channel estimation and pre-equalize the uplink random access\nsignal with respect to the channels associated with the beacon antenna. During\nthe uplink transmission stage, the BS detects UEs' activity and data by using\nthe proposed iterative detection algorithm, which consists of three modules:\ncoarse data detection (DD), data-aided channel estimation (CE), and fine DD. In\nthe proposed algorithm, the joint activity and DD is firstly performed based on\nthe signals received by the beacon antenna. Subsequently, the DD is further\nrefined by iteratively performing data-aided CE module and fine DD module using\nsignals received by all BS antennas. Our simulation results demonstrate that\nthe proposed scheme outperforms state-of-the-art mMIMO-based grant-free massive\nNOMA schemes with the same access latency. Simulation codes are provided to\nreproduce the results in this article: https:\/\/github.com\/owenwang517\/tvt-2025.",
        "Ultrasound imaging frequently encounters challenges, such as those related to\nelevated noise levels, diminished spatiotemporal resolution, and the complexity\nof anatomical structures. These factors significantly hinder the model's\nability to accurately capture and analyze structural relationships and dynamic\npatterns across various regions of the heart. Mamba, an emerging model, is one\nof the most cutting-edge approaches that is widely applied to diverse vision\nand language tasks. To this end, this paper introduces a U-shaped deep learning\nmodel incorporating a large-window Mamba scale (LMS) module and a hierarchical\nfeature fusion approach for echocardiographic segmentation. First, a cascaded\nresidual block serves as an encoder and is employed to incrementally extract\nmultiscale detailed features. Second, a large-window multiscale mamba module is\nintegrated into the decoder to capture global dependencies across regions and\nenhance the segmentation capability for complex anatomical structures.\nFurthermore, our model introduces auxiliary losses at each decoder layer and\nemploys a dual attention mechanism to fuse multilayer features both spatially\nand across channels. This approach enhances segmentation performance and\naccuracy in delineating complex anatomical structures. Finally, the\nexperimental results using the EchoNet-Dynamic and CAMUS datasets demonstrate\nthat the model outperforms other methods in terms of both accuracy and\nrobustness. For the segmentation of the left ventricular endocardium\n(${LV}_{endo}$), the model achieved optimal values of 95.01 and 93.36,\nrespectively, while for the left ventricular epicardium (${LV}_{epi}$), values\nof 87.35 and 87.80, respectively, were achieved. This represents an improvement\nranging between 0.54 and 1.11 compared with the best-performing model.",
        "Limited English Proficiency (LEP) patients face higher risks of adverse\nhealth outcomes due to communication barriers, making timely medical\ninterpreting services essential for mitigating those risks. This paper\naddresses the scheduling of medical interpreting services under uncertainty.\nThe problem is formulated as a two-stage stochastic programming model that\naccounts for uncertainties in emergency patients' arrival and service time. The\nmodel handles the hiring decisions of part-time interpreters and the assignment\nof full-time and hired part-time interpreters. The objective is to minimize the\ntotal cost, which encompasses full-time interpreters' overtime cost, the fixed\nand variable costs of part-time interpreters, and the penalty cost for not\nserving LEP patients on time. The model is solved using the Sample Average\nApproximation (SAA) algorithm. To overcome the computational burden of the SAA\nalgorithm, a Tabu Search (TS) algorithm was used to solve the model. A\nreal-life case study is used to validate and evaluate the proposed solution\nalgorithms. The results demonstrate the effectiveness of the proposed\nstochastic programming-based solutions in concurrently reducing both the total\ncost and the waiting time. Further, sensitivity analysis reveals how the\nincrease in some key parameters, such as the arrival rate of emergency patients\nwith LEP, impacts scheduling outcomes.",
        "Here we present extinction, extirpation and coexistence conditions where \/\nwhen two communities combine. We consider one specific model where two\ncommunities coalesce, and another model where the communities coexist side by\nside, blending in a transitionary zone called the ecotone. Specifically, (1) we\nanalytically calculate the shifts in abundances as a function of mixing\nstrength. (2) Obtain a critical value for the mixing strength leading to\nextinction. (3) Derive an inequality condition for full coexistent mixing. (4)\nfind how the individual communities penetrate into one other as a function of\nmixing strength. (5) derive the conditions for one species to cross the ecotone\nand invade an neighboring community and (6) conditions for a native species to\nget extirpated. Lastly, (7) we spatially investigate the species richness\nwithin the ecotone and derive a condition that determines whether the ecotone\nwill have higher or lower richness compared to its surrounding habitats.",
        "We give a dynamical, relatively elementary proof of an \"absorption theorem\"\nwhich is closely related to a well-known result due to Matui. The construction\nis in the spirit of an earlier joint work of the author and S. Robert. In an\nappendix we explain how to use this result to correct the dynamical proof given\nby Melleray--Robert of a classification theorem for orbit equivalence of\nminimal ample groups due to Giordano, Putnam and Skau (the original argument\nhad a gap).",
        "This paper addresses the challenge of integrating semantic communication\nprinciples into operated networks, traditionally optimized based on\nnetwork-centric metrics rather than application-specific needs. Operated\nnetworks strongly adhere to the principle of ``separation of concerns\", which\nemphasizes a clear distinction between network operation and application.\nDespite the initial perceived incompatibility between semantic communication\nand the principles of operated networks, this paper provides solutions to\nreconcile them. The foundations of these solutions include the adoption of\nnon-arbitrary semantic representations as a standard encoding for\ncommunications, the establishment of a standard interface between the\napplication and network, and a dedicated network control plane. These enable\nthe application to describe the data typology and the nature of the task, and\nto agree upon a transmission scheme tailored to the supported task. Through\nthree scenarios involving an application transmitting text representations, we\nillustrate the implementation of the proposal and demonstrate the potential of\nthe approach.",
        "The proliferation of digital interactions across diverse domains, such as\nhealthcare, e-commerce, gaming, and finance, has resulted in the generation of\nvast volumes of event stream (ES) data. ES data comprises continuous sequences\nof timestamped events that encapsulate detailed contextual information relevant\nto each domain. While ES data holds significant potential for extracting\nactionable insights and enhancing decision-making, its effective utilization is\nhindered by challenges such as the scarcity of labeled data and the fragmented\nnature of existing research efforts. Self-Supervised Learning (SSL) has emerged\nas a promising paradigm to address these challenges by enabling the extraction\nof meaningful representations from unlabeled ES data. In this survey, we\nsystematically review and synthesize SSL methodologies tailored for ES modeling\nacross multiple domains, bridging the gaps between domain-specific approaches\nthat have traditionally operated in isolation. We present a comprehensive\ntaxonomy of SSL techniques, encompassing both predictive and contrastive\nparadigms, and analyze their applicability and effectiveness within different\napplication contexts. Furthermore, we identify critical gaps in current\nresearch and propose a future research agenda aimed at developing scalable,\ndomain-agnostic SSL frameworks for ES modeling. By unifying disparate research\nefforts and highlighting cross-domain synergies, this survey aims to accelerate\ninnovation, improve reproducibility, and expand the applicability of SSL to\ndiverse real-world ES challenges.",
        "We prove a criterion for the geometric and algebraic finiteness properties of\nvertex stabilisers of $G$-CW-complexes, given the finiteness properties of the\ngroup $G$ and of the stabilisers of positive dimensional cells. This\ngeneralises a result of Haglund--Wise for groups acting on trees to higher\ndimensions. As an application, for $n\\ge 2$, we deduce the existence of\nuncountably many quasi-isometry classes of one-ended groups that are of type\n$\\mathsf{FP}_n$ and not of type $\\mathsf{FP}_{n+1}$.",
        "Let $H$ be a quasiperiodic Schr\\\"{o}dinger operator generated by a monotone\npotential, as defined in [16]. Following [20], we study the connection between\nthe Lyapunov exponent $L\\left(E\\right)$, arithmetic properties of the frequency\n$\\alpha$, and certain fractal-dimensional properties of the spectral measures\nof $H$.",
        "Generative game engines have the potential to revolutionize game development\nby autonomously creating new content and reducing manual workload. However,\nexisting video-based game generation methods fail to address the critical\nchallenge of scene generalization, limiting their applicability to existing\ngames with fixed styles and scenes. In this paper, we present GameFactory, a\nframework focused on exploring scene generalization in game video generation.\nTo enable the creation of entirely new and diverse games, we leverage\npre-trained video diffusion models trained on open-domain video data. To bridge\nthe domain gap between open-domain priors and small-scale game dataset, we\npropose a multi-phase training strategy that decouples game style learning from\naction control, preserving open-domain generalization while achieving action\ncontrollability. Using Minecraft as our data source, we release GF-Minecraft, a\nhigh-quality and diversity action-annotated video dataset for research.\nFurthermore, we extend our framework to enable autoregressive\naction-controllable game video generation, allowing the production of\nunlimited-length interactive game videos. Experimental results demonstrate that\nGameFactory effectively generates open-domain, diverse, and action-controllable\ngame videos, representing a significant step forward in AI-driven game\ngeneration. Our dataset and project page are publicly available at\n\\url{https:\/\/vvictoryuki.github.io\/gamefactory\/}.",
        "Understanding and managing uncertainty is crucial in machine learning,\nespecially in high-stakes domains like healthcare, where class imbalance can\nimpact predictions. This paper introduces RIGA, a novel pipeline that mitigates\nclass imbalance using generative AI. By converting tabular healthcare data into\nimages, RIGA leverages models like cGAN, VQVAE, and VQGAN to generate balanced\nsamples, improving classification performance. These representations are\nprocessed by CNNs and later transformed back into tabular format for seamless\nintegration. This approach enhances traditional classifiers like XGBoost,\nimproves Bayesian structure learning, and strengthens ML model robustness by\ngenerating realistic synthetic data for underrepresented classes.",
        "This paper proposes a novel approach using Graph Neural Networks (GNNs) to\nsolve the AC Power Flow problem in power grids. AC OPF is essential for\nminimizing generation costs while meeting the operational constraints of the\ngrid. Traditional solvers struggle with scalability, especially in large\nsystems with renewable energy sources. Our approach models the power grid as a\ngraph, where buses are nodes and transmission lines are edges. We explore\ndifferent GNN architectures, including GCN, GAT, SAGEConv, and GraphConv to\npredict AC power flow solutions efficiently. Our experiments on IEEE test\nsystems show that GNNs can accurately predict power flow solutions and scale to\nlarger systems, outperforming traditional solvers in terms of computation time.\nThis work highlights the potential of GNNs for real-time power grid management,\nwith future plans to apply the model to even larger grid systems.",
        "Large Language Models (LLMs) have garnered significant attention for their\nimpressive general-purpose capabilities. For applications requiring intricate\ndomain knowledge, Retrieval-Augmented Generation (RAG) has shown a distinct\nadvantage in incorporating domain-specific information into LLMs. However,\nexisting RAG research has not fully addressed the challenges of Multiple Choice\nQuestion Answering (MCQA) in telecommunications, particularly in terms of\nretrieval quality and mitigating hallucinations. To tackle these challenges, we\npropose a novel first token probability guided RAG framework. This framework\nleverages confidence scores to optimize key hyperparameters, such as chunk\nnumber and chunk window size, while dynamically adjusting the context. Our\nmethod starts by retrieving the most relevant chunks and generates a single\ntoken as the potential answer. The probabilities of all options are then\nnormalized to serve as confidence scores, which guide the dynamic adjustment of\nthe context. By iteratively optimizing the hyperparameters based on these\nconfidence scores, we can continuously improve RAG performance. We conducted\nexperiments to validate the effectiveness of our framework, demonstrating its\npotential to enhance accuracy in domain-specific MCQA tasks.",
        "The majority of online marketplaces offer promotion programs to sellers to\nacquire additional customers for their products. These programs typically allow\nsellers to allocate advertising budgets to promote their products, with higher\nbudgets generally correlating to improve ad performance. Auction mechanisms\nwith budget pacing are commonly employed to implement such ad systems. While\nauctions deliver satisfactory average effectiveness, ad performance under\nallocated budgets can be unfair in practice.\n  To address this issue, we propose a novel ad allocation model that departs\nfrom traditional auction mechanics. Our approach focuses on solving a global\noptimization problem that balances traffic allocation while considering\nplatform efficiency and fairness constraints.\n  This study presents the following contributions. First, we introduce a\nfairness metric based on the Gini index. Second, we formulate the optimization\nproblem incorporating efficiency and fairness objectives. Third, we offer an\nonline algorithm to solve this optimization problem. Finally, we demonstrate\nthat our approach achieves superior fairness compared to baseline auction-based\nalgorithms without sacrificing efficiency. We contend that our proposed method\ncan be effectively applied in real-time ad allocation scenarios and as an\noffline benchmark for evaluating the fairness-efficiency trade-off of existing\nauction-based systems.",
        "In this paper, we address the full discretization of Friedrichs' systems with\na two-field structure, such as Maxwell's equations or the acoustic wave\nequation in div-grad form, cf. [14]. We focus on a discontinuous Galerkin space\ndiscretization applied to a locally refined mesh or a small region with high\nwave speed. This results in a stiff system of ordinary differential equations,\nwhere the stiffness is mainly caused by a small region of the spatial mesh.\nWhen using explicit time-integration schemes, the time step size is severely\nrestricted by a few spatial elements, leading to a loss of efficiency. As a\nremedy, we propose and analyze a general leapfrog-based scheme which is\nmotivated by [5]. The new, fully explicit, local time-integration method\nfilters the stiff part of the system in such a way that its CFL condition is\nsignificantly weaker than that of the leapfrog scheme while its computational\ncost is only slightly larger. For this scheme, the filter function is a\nsuitably scaled and shifted Chebyshev polynomial. While our main interest is in\nexplicit local-time stepping schemes, the filter functions can be much more\ngeneral, for instance, a certain rational function leads to the locally\nimplicit method, proposed and analyzed in [24]. Our analysis provides\nsufficient conditions on the filter function to ensure full order of\nconvergence in space and second order in time for the whole class of local\ntime-integration schemes.",
        "This paper explores the embedding of lattice structures $L \\subseteq\n\\mathbb{R}^n$ into smooth manifolds $M \\subseteq \\mathbb{R}^n$ through a\nrigorous mathematical framework. Building upon the foundational results\nestablished in \"Embedding of a Discrete Lattice Structure in a Smooth\nManifold,\" this work investigates the existence and solvability of partial\ndifferential equations (PDEs) governing the embedding process. The primary aim\nis to derive and analyze solutions to these PDEs while preserving the geometric\nand topological properties of $L$ and $M$.\n  The solutions are shown to exist under initial boundary conditions, with the\ngeometric structure of $M$ and the discrete topology of $L$ playing crucial\nroles in ensuring well-posedness and regularity.\n  This paper provides a detailed exposition of the mathematical interplay\nbetween discrete and continuous spaces, offering novel insights into embedding\ntheory and the geometry of manifolds interacting with discrete substructures.",
        "Let $\\mathbb{Z}_S$ be the ring generated by the inverses of all elements of a\nnon-empty proper subset $S$ of integer primes. We show that the ring generated\nby the values of unary definable sets in the model-theoretic Grothendieck ring\nof $(\\mathbb{Z}_S;+,<)$ is a quotient of $(\\mathbb{Z}\/q\\mathbb{Z})[T]\/(T+T^2)$,\nwhere $q$ is the largest odd integer that divides $p-1$ for all $p \\notin S$.\n  This implies that the Grothendieck ring of $(\\mathbb{Z}_S;+,<)$ is trivial in\nvarious salient cases, for example when $S$ is finite, or when $S$ does not\ncontain any prime of the form $2^n+1$, $n\\in \\mathbb{N}$.",
        "This paper proposes a novel framework for real-time adaptive-bitrate video\nstreaming by integrating latent diffusion models (LDMs) within the FFmpeg\ntechniques. This solution addresses the challenges of high bandwidth usage,\nstorage inefficiencies, and quality of experience (QoE) degradation associated\nwith traditional constant bitrate streaming (CBS) and adaptive bitrate\nstreaming (ABS). The proposed approach leverages LDMs to compress I-frames into\na latent space, offering significant storage and semantic transmission savings\nwithout sacrificing high visual quality. While it keeps B-frames and P-frames\nas adjustment metadata to ensure efficient video reconstruction at the user\nside, the proposed framework is complemented with the most state-of-the-art\ndenoising and video frame interpolation (VFI) techniques. These techniques\nmitigate semantic ambiguity and restore temporal coherence between frames, even\nin noisy wireless communication environments. Experimental results demonstrate\nthe proposed method achieves high-quality video streaming with optimized\nbandwidth usage, outperforming state-of-the-art solutions in terms of QoE and\nresource efficiency. This work opens new possibilities for scalable real-time\nvideo streaming in 5G and future post-5G networks.",
        "We present I2V3D, a novel framework for animating static images into dynamic\nvideos with precise 3D control, leveraging the strengths of both 3D geometry\nguidance and advanced generative models. Our approach combines the precision of\na computer graphics pipeline, enabling accurate control over elements such as\ncamera movement, object rotation, and character animation, with the visual\nfidelity of generative AI to produce high-quality videos from coarsely rendered\ninputs. To support animations with any initial start point and extended\nsequences, we adopt a two-stage generation process guided by 3D geometry: 1)\n3D-Guided Keyframe Generation, where a customized image diffusion model refines\nrendered keyframes to ensure consistency and quality, and 2) 3D-Guided Video\nInterpolation, a training-free approach that generates smooth, high-quality\nvideo frames between keyframes using bidirectional guidance. Experimental\nresults highlight the effectiveness of our framework in producing controllable,\nhigh-quality animations from single input images by harmonizing 3D geometry\nwith generative models. The code for our framework will be publicly released.",
        "In integrable spin chains, the spectral problem can be solved by the method\nof Bethe ansatz, which transforms the problem of diagonalization of the\nHamiltonian into the problem of solving a set of algebraic equations named\nBethe equations. In this work, we systematically investigate the spin-$s$ XXX\nchain with twisted and open boundary conditions using the rational $Q$-system,\nwhich is a powerful tool to solve Bethe equations. We establish basic\nframeworks of the rational $Q$-system and confirm its completeness numerically\nin both cases. For twisted boundaries, we investigate the polynomiality\nconditions of the rational $Q$-system and derive physical conditions for\nsingular solutions of Bethe equations. For open boundaries, we uncover novel\nphenomena such as hidden symmetries and magnetic strings under specific\nboundary parameters. Hidden symmetries lead to the appearance of extra\ndegeneracies in the Hilbert space, while the magnetic string is a novel type of\nexact string configuration, whose length depends on the boundary magnetic\nfields. These findings, supported by both analytical and numerical evidences,\noffer new insights into the interplay between symmetries and boundary\nconditions.",
        "For any odd prime $p$ and any integer $n>0$ with $p^2|n$, we show that the\nmod $p$ cohomology ring of the classifying space of the projective unitary\ngroup $PU(n)$ is not completely detected by elementary abelian $p$-subgroups,\nproviding counterexamples to a conjecture due to J. F. Adams. We also give an\napplication involving Milnor operations and Brown-Peterson cohomology.",
        "We propose Serenade, a novel framework for the singing style conversion (SSC)\ntask. Although singer identity conversion has made great strides in the\nprevious years, converting the singing style of a singer has been an unexplored\nresearch area. We find three main challenges in SSC: modeling the target style,\ndisentangling source style, and retaining the source melody. To model the\ntarget singing style, we use an audio infilling task by predicting a masked\nsegment of the target mel-spectrogram with a flow-matching model using the\ncomplement of the masked target mel-spectrogram along with disentangled\nacoustic features. On the other hand, to disentangle the source singing style,\nwe use a cyclic training approach, where we use synthetic converted samples as\nsource inputs and reconstruct the original source mel-spectrogram as a target.\nFinally, to retain the source melody better, we investigate a post-processing\nmodule using a source-filter-based vocoder and resynthesize the converted\nwaveforms using the original F0 patterns. Our results showed that the Serenade\nframework can handle generalized SSC tasks with the best overall similarity\nscore, especially in modeling breathy and mixed singing styles. Moreover,\nalthough resynthesizing with the original F0 patterns alleviated out-of-tune\nsinging and improved naturalness, we found a slight tradeoff in similarity due\nto not changing the F0 patterns into the target style.",
        "CFD acceleration for virtual nuclear reactors or digital twin technology is a\nprimary goal in the nuclear industry. This study compares advanced\nconvolutional neural network (CNN) architectures for accelerating unsteady\ncomputational fluid dynamics (CFD) simulations using small datasets based on a\nchallenging natural convection flow dataset. The advanced architectures such as\nautoencoders, UNet, and ConvLSTM-UNet, were evaluated under identical\nconditions to determine their predictive accuracy and robustness in\nautoregressive time-series predictions. ConvLSTM-UNet consistently outperformed\nother models, particularly in difference value calculation, achieving lower\nmaximum errors and stable residuals. However, error accumulation remains a\nchallenge, limiting reliable predictions to approximately 10 timesteps. This\nhighlights the need for enhanced strategies to improve long-term prediction\nstability. The novelty of this work lies in its fair comparison of\nstate-of-the-art CNN models within the RePIT framework, demonstrating their\npotential for accelerating CFD simulations while identifying limitations under\nsmall data conditions. Future research will focus on exploring alternative\nmodels, such as graph neural networks and implicit neural representations.\nThese efforts aim to develop a robust hybrid approach for long-term unsteady\nCFD acceleration, contributing to practical applications in virtual nuclear\nreactor.",
        "Object affordance reasoning, the ability to infer object functionalities\nbased on physical properties, is fundamental for task-oriented planning and\nactivities in both humans and Artificial Intelligence (AI). This capability,\nrequired for planning and executing daily activities in a task-oriented manner,\nrelies on commonsense knowledge of object physics and functionalities,\nextending beyond simple object recognition. Current computational models for\naffordance reasoning from perception lack generalizability, limiting their\napplicability in novel scenarios. Meanwhile, comprehensive Large Language\nModels (LLMs) with emerging reasoning capabilities are challenging to deploy on\nlocal devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a\nlarge-scale dataset comprising 1,496 tasks and 119k images, designed to enhance\nthe generalizability of affordance reasoning from perception. Utilizing this\ndataset, we develop Afford-X, an end-to-end trainable affordance reasoning\nmodel that incorporates Verb Attention and Bi-Fusion modules to improve\nmulti-modal understanding. This model achieves up to a 12.1% performance\nimprovement over the best-reported results from non-LLM methods, while also\ndemonstrating a 1.2% enhancement compared to our previous conference paper.\nAdditionally, it maintains a compact 187M parameter size and infers nearly 50\ntimes faster than the GPT-4V API. Our work demonstrates the potential for\nefficient, generalizable affordance reasoning models that can be deployed on\nlocal devices for task-oriented manipulations. We showcase Afford-X's\neffectiveness in enabling task-oriented manipulations for robots across various\ntasks and environments, underscoring its efficiency and broad implications for\nadvancing robotics and AI systems in real-world applications.",
        "In this paper, we mainly consider nonnegative weak solutions $u\\in\nD^{1,p}(\\R^{N})$ to the doubly $D^{1,p}(\\R^{N})$-critical nonlocal quasi-linear\nSchr\\\"{o}dinger-Hartree equation: \\begin{align*} -\\Delta_p u- \\mu\n\\frac{u^{p-1}}{|x|^p}=\\left(|x|^{-2p}\\ast |u|^{p}\\right)|u|^{p-2}u \\qquad\n&\\mbox{in} \\,\\, \\mathbb{R}^N, \\end{align*} where $N\\geq3$, $0\\leq\\mu<\n\\bar{\\mu}:=\\left( (N-p)\/p \\right)^p$ and $1<p<\\frac{N}{2}$. When $\\mu>0$, due\nto appearance of the Hardy potential, the equation has singularity at\n$0\\in\\mathbb{R}^{N}$ and hence is not translation invariant, so sharp\nasymptotic estimates near the origin must be involved. First, we establish\nregularity and the sharp estimates on asymptotic behaviors near the origin and\nthe infinity for any positive solution $u\\in D^{1,p}(\\R^{N})$ (and $|\\nabla\nu|$) to more general equation $-\\triangle_p u - \\mu\n\\frac{1}{|x|^p}u^{p-1}=V(x)\\frac{1}{|x|^s}u^{p-1}$ with $N\\geq2$, $0\\leq\\mu<\n\\bar{\\mu}$, $1<p<N$, $0\\leq s < p$ and $0\\leq V(x)\\in L^\\frac{N}{p-s}(\\R^N)$.\nThen, as a consequence, we can apply the method of moving planes to prove that\nall the nontrivial nonnegative solutions in $D^{1,p}(\\R^{N})$ are radially\nsymmetric and strictly radially decreasing about the origin\n$0\\in\\mathbb{R}^{N}$. The sharp asymptotic estimates and radial symmetry for\nmore general weighted doubly $D^{1,p}$-critical nonlocal quasi-linear equations\nwere also derived. Our results extend the results in \\cite{DLL} from the\nspecial case $\\mu=0$ to general cases $0\\leq\\mu<\\bar{\\mu}$."
      ]
    }
  },
  {
    "id":2411.1945,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Auto-Encoding Variational Bayes",
    "start_abstract":"How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
      ],
      "abstract":[
        "It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products."
      ],
      "categories":[
        "physics.gen-ph"
      ]
    },
    "list":{
      "title":[
        "Isometric Gelfand transforms of complete Nevanlinna-Pick spaces",
        "On extensivity of morphisms",
        "On Branch-and-Price for Project Scheduling",
        "Extreme Shape Coexistence Observed in $^{70}$Co",
        "Learning Memory and Material Dependent Constitutive Laws",
        "Exploring the Technology Landscape through Topic Modeling, Expert\n  Involvement, and Reinforcement Learning",
        "Models for the Eremenko-Lyubich class",
        "Subcode Ensemble Decoding of Linear Block Codes",
        "How does non-metricity affect particle creation and evaporation in\n  bumblebee gravity?",
        "A mesh-free hybrid Chebyshev-Tucker tensor format with applications to\n  multi-particle modelling",
        "Investigating Parameter-Efficiency of Hybrid QuGANs Based on Geometric\n  Properties of Generated Sea Route Graphs",
        "Complex potential and open system applications in heavy-ions and cold\n  atoms",
        "A Pristine-UNIONS view on the Galaxy: Kinematics of the distant spur\n  feature of the Sagittarius stream traced by Blue Horizontal Branch stars",
        "Implementation and verification of coherent error suppression using\n  randomized compiling for Grover's algorithm on a trapped-ion device",
        "Dynkin Systems and the One-Point Geometry",
        "Tri-layer SiN-on-Si 8x8 Optical Switches with Thermo-optic and\n  Electro-optic Actuators",
        "Sub-GeV dark matter and nano-Hertz gravitational waves from a\n  classically conformal dark sector",
        "A Framework for Stochastic Fairness in Dominant Resource Allocation with\n  Cloud Computing Applications",
        "Approximating non-Gaussian Bayesian partitions with normalising flows:\n  statistics, inference and application to cosmology",
        "Accurate myocardial T1 mapping at 5T using an improved MOLLI method: A\n  validation study",
        "On the Spectral Analysis of Power Graph of Dihedral Groups",
        "A Formalism for Calibrating the Instrumental Polarization of Radio\n  Interferometric Arrays at Meter Wavelengths using Unpolarized Sky: A\n  Demonstration using the MWA Observations",
        "Metalens array for complex-valued optical discrete Fourier transform",
        "The physics of oscillating surfaces and sounds",
        "Reconfigurable Intelligent Surfaces-Assisted Integrated Access and\n  Backhaul",
        "Classification of Homogeneous Local Representations of the Singular\n  Braid Monoid",
        "Approximation of Permutation Invariant Polynomials by Transformers:\n  Efficient Construction in Column-Size",
        "Which Features are Best for Successor Features?",
        "Observation of the $\\Omega$(2012) baryon at the LHC"
      ],
      "abstract":[
        "We show that any complete Nevanlinna-Pick space whose multiplier algebra has\nisometric Gelfand transform (or commutative C*-envelope) is essentially the\nHardy space on the open unit disk.",
        "Extensivity of a category may be described as a property of coproducts in the\ncategory, namely, that they are disjoint and universal. An alternative\nviewpoint is that it is a property of morphisms in a category. This paper\nexplores this point of view through a natural notion of extensive and\ncoextensive morphism. Through these notions, topics in universal algebra, such\nas the strict refinement and Fraser-Horn properties, take categorical form and\nthereby enjoy the benefits of categorical generalisation. On the other hand,\nthe universal algebraic theory surrounding these topics inspire categorical\nresults. One such result we establish in this paper is that a Barr-exact\ncategory is coextensive if and only if every split monomorphism in the category\nis coextensive.",
        "Integer programs for resource-constrained project scheduling problems are\nnotoriously hard to solve due to their weak linear relaxations. Several papers\nhave proposed reformulating project scheduling problems via Dantzig-Wolfe\ndecomposition to strengthen their linear relaxation and decompose large problem\ninstances. The reformulation gives rise to a master problem that has a large\nnumber of variables. Therefore, the master problem is solved by a column\ngeneration procedure embedded in a branching framework, also known as\nbranch-and-price. While branch-and-price has been successfully applied to many\nproblem classes, it turns out to be ineffective for most project scheduling\nproblems. This paper identifies drivers of the ineffectiveness by analyzing the\nstructure of the reformulated problem and the strength of different branching\nschemes. Our analysis shows that the reformulated problem has an unfavorable\nstructure for column generation: It is highly degenerate, slowing down the\nconvergence of column generation, and for many project scheduling problems, it\nyields the same or only slightly stronger linear relaxations as classical\nformulations at the expense of large increases in runtime. Our computational\nexperiments complement our theoretical findings.",
        "The shape of the atomic nucleus is a property which underpins our\nunderstanding of nuclear systems, impacts the limits of nuclear existence, and\nenables probes of physics beyond the Standard Model. Nuclei can adopt a variety\nof shapes, including spheres, axially deformed spheroids, and pear shapes. In\nsome regions of the nuclear chart where a spherical nucleus would naively be\nexpected, deformed nuclear states can result from collective action of\nconstituent protons and neutrons. In a small subset of nuclei both spherical\nand deformed nuclear states have been experimentally observed, a phenomenon\ntermed shape coexistence. We present spectroscopic evidence for the coexistence\nof $J^{\\pi}=1+$ spherical and deformed states in $^{70}$Co, separated by less\nthan 275~keV. This close degeneracy of levels with the same $J^{\\pi}$ and\ndifferent shapes demonstrates an extreme example of shape coexistence resulting\nfrom the interplay of independent particle motion and collective behavior in\nhighly unstable nuclear systems and identifies the Co isotopes as a transition\npoint between deformed ground states observed in the Cr isotopes and spherical\nconfigurations observed in the closed-shell Ni isotopes.",
        "The theory of homogenization provides a systematic approach to the derivation\nof macroscale constitutive laws, obviating the need to repeatedly resolve\ncomplex microstructure. However, the unit cell problem that defines the\nconstitutive model is typically not amenable to explicit evaluation. It is\ntherefore of interest to learn constitutive models from data generated by the\nunit cell problem. Many viscoelastic and elastoviscoplastic materials are\ncharacterized by memory-dependent constitutive laws. In order to amortize the\ncomputational investment in finding such memory-dependent constitutive laws, it\nis desirable to learn their dependence on the material microstructure. While\nprior work has addressed learning memory dependence and material dependence\nseparately, their joint learning has not been considered. This paper focuses on\nthe joint learning problem and proposes a novel neural operator framework to\naddress it.\n  In order to provide firm foundations, the homogenization problem for linear\nKelvin-Voigt viscoelastic materials is studied. The theoretical properties of\nthe cell problem in this Kelvin-Voigt setting are used to motivate the proposed\ngeneral neural operator framework; these theoretical properties are also used\nto prove a universal approximation theorem for the learned macroscale\nconstitutive model. This formulation of learnable constitutive models is then\ndeployed beyond the Kelvin-Voigt setting. Numerical experiments are presented\nshowing that the resulting data-driven methodology accurately learns history-\nand microstructure-dependent linear viscoelastic and nonlinear\nelastoviscoplastic constitutive models, and numerical results also demonstrate\nthat the resulting constitutive models can be deployed in macroscale simulation\nof material deformation.",
        "In today's rapidly evolving technological landscape, organizations face the\nchallenge of integrating external insights into their decision-making processes\nto stay competitive. To address this issue, this study proposes a method that\ncombines topic modeling, expert knowledge inputs, and reinforcement learning\n(RL) to enhance the detection of technological changes. The method has four\nmain steps: (1) Build a relevant topic model, starting with textual data like\ndocuments and reports to find key themes. (2) Create aspect-based topic models.\nExperts use curated keywords to build models that showcase key domain-specific\naspects. (3) Iterative analysis and RL driven refinement: We examine metrics\nsuch as topic magnitude, similarity, entropy shifts, and how models change over\ntime. We optimize topic selection with RL. Our reward function balances the\ndiversity and similarity of the topics. (4) Synthesis and operational\nintegration: Each iteration provides insights. In the final phase, the experts\ncheck these insights and reach new conclusions. These conclusions are designed\nfor use in the firm's operational processes. The application is tested by\nforecasting trends in quantum communication. Results demonstrate the method's\neffectiveness in identifying, ranking, and tracking trends that align with\nexpert input, providing a robust tool for exploring evolving technological\nlandscapes. This research offers a scalable and adaptive solution for\norganizations to make informed strategic decisions in dynamic environments.",
        "If $f$ is in the Eremenko-Lyubich class (transcendental entire functions with\nbounded singular set) then $\\Omega= \\{ z: |f(z)| > R\\}$ and $f|_\\Omega$ must\nsatisfy certain simple topological conditions when $R$ is sufficiently large. A\nmodel $(\\Omega, F)$ is an open set $\\Omega$ and a holomorphic function $F$ on\n$\\Omega$ that satisfy these same conditions. We show that any model can be\napproximated by an Eremenko-Lyubich function in a precise sense. In many cases,\nthis allows the construction of functions in the Eremenko-Lyubich with a\ndesired property to be reduced to the construction of a model with that\nproperty, and this is often much easier to do.",
        "Low-density parity-check (LDPC) codes together with belief propagation (BP)\ndecoding yield exceptional error correction capabilities in the large block\nlength regime. Yet, there remains a gap between BP decoding and maximum\nlikelihood decoding for short block length LDPC codes. In this context,\nensemble decoding schemes yield both reduced latency and good error rates. In\nthis paper, we propose subcode ensemble decoding (SCED), which employs an\nensemble of decodings on different subcodes of the code. To ensure that all\ncodewords are decodable, we use the concept of linear coverings and explore\napproaches for sampling suitable ensembles for short block length LDPC codes.\nMonte-Carlo simulations conducted for three LDPC codes demonstrate that SCED\nimproves decoding performance compared to stand-alone decoding and automorphism\nensemble decoding. In particular, in contrast to existing schemes, e.g.,\nmultiple bases belief propagation and automorphism ensemble decoding, SCED does\nnot require the NP-complete search for low-weight dual codewords or knowledge\nof the automorphism group of the code, which is often unknown.",
        "In this work, we analyze the impact of non-metricity on particle creation and\nthe evaporation process of black holes within the framework of bumblebee\ngravity. In general lines, we compare black holes in the metric formalism [1]\nand the metric-affine approach [2]. Initially, we focus on bosonic particle\nmodes to investigate Hawking radiation. Using the Klein-Gordon equation, we\ncompute the Bogoliubov coefficients and derive the Hawking temperature.\nSubsequently, we examine Hawking radiation as a tunneling process, resolving\ndivergent integrals through the residue method. The analysis is then extended\nto fermionic particle modes, also within the tunneling framework. Particle\ncreation densities are calculated for both bosonic and fermionic cases.\nAdditionally, greybody bounds are estimated for bosonic and fermionic\nparticles. Finally, we explore the evaporation process, considering the final\nstate of the black holes. In a general panorama, non-metricity in bumblebee\ngravity raises particle density for bosons while reducing it for fermions,\nincreases greybody factors (for both bosons and fermions), amplifies the\nemission rate, and accelerates the evaporation process.",
        "In this paper, we introduce a mesh-free two-level hybrid Tucker tensor format\nfor approximation of multivariate functions, which combines the product\nChebyshev interpolation with the ALS-based Tucker decomposition of the tensor\nof Chebyshev coefficients. It allows to avoid the expenses of the\nrank-structured approximation of function-related tensors defined on large\nspacial grids, while benefiting from the Tucker decomposition of the rather\nsmall core tensor of Chebyshev coefficients. This leads to nearly optimal\nTucker rank parameters which are close to the results for well established\nTucker-ALS algorithm applied to the large grid-based tensors. These rank\nparameters inherited from the Tucker-ALS decomposition of the coefficient\ntensor can be much less than the polynomial degrees of the initial Chebyshev\ninterpolant via function independent basis set. Furthermore, the tensor product\nChebyshev polynomials discretized on a tensor grid leads to a low-rank\ntwo-level orthogonal algebraic Tucker tensor that approximates the initial\nfunction with controllable accuracy. It is shown that our techniques could be\ngainfully applied to the long-range part of the electrostatic potential of\nmulti-particle systems approximated in the range-separated tensor format. Error\nand complexity estimates of the proposed methods are presented. We demonstrate\nthe efficiency of the suggested method numerically on examples of the\nlong-range components of multi-particle interaction potentials generated by 3D\nNewton kernel for large bio-molecule systems and lattice-type compounds.",
        "The demand for artificially generated data for the development, training and\ntesting of new algorithms is omnipresent. Quantum computing (QC), does offer\nthe hope that its inherent probabilistic functionality can be utilised in this\nfield of generative artificial intelligence. In this study, we use\nquantum-classical hybrid generative adversarial networks (QuGANs) to\nartificially generate graphs of shipping routes. We create a training dataset\nbased on real shipping data and investigate to what extent QuGANs are able to\nlearn and reproduce inherent distributions and geometric features of this data.\nWe compare hybrid QuGANs with classical Generative Adversarial Networks (GANs),\nwith a special focus on their parameter efficiency. Our results indicate that\nQuGANs are indeed able to quickly learn and represent underlying geometric\nproperties and distributions, although they seem to have difficulties in\nintroducing variance into the sampled data. Compared to classical GANs of\ngreater size, measured in the number of parameters used, some QuGANs show\nsimilar result quality. Our reference to concrete use cases, such as the\ngeneration of shipping data, provides an illustrative example and demonstrate\nthe potential and diversity in which QC can be used.",
        "Since the discovery of the complex potential of quarkonium at high\ntemperatures, quarkonium has been regarded as an open quantum system in the\nquark-gluon plasma. Recently, a similar issue regarding in-medium bound states\nof impurities has also emerged in particle physics and cold atomic physics. We\nwill provide an overview of recent advancements in understanding key quantities\nsuch as complex potential and transport coefficients for heavy impurities in\nfinite temperature QCD and cold atomic systems.",
        "Providing a detailed picture of the Sagittarius (Sgr) stream offers important\nconstraints on the build-up of the Galactic halo as well as its gravitational\npotential at large radii. While several attempts have been made to model the\nstructure of the Sgr stream, no model has yet been able to match all the\nfeatures observed for the stream. Moreover, for several of these features,\nobservational characterisation of their properties is rather limited,\nparticularly at large distances. The aim of this work is to investigate the\nkinematics of the Sgr stream outermost spur feature using blue horizontal\nbranch (BHB) stars. Candidate BHB stars were selected by combining two\napproaches; one capitalising on Pan-STARRS1 3$\\Pi$ griz and u photometry taken\nas part of UNIONS, the other using Pristine Survey CaHK and SDSS ugr\nphotometry. Follow-up optical spectra are obtained using ESO\/VLT\/FORS2 to\nconfirm their BHB nature and obtain line-of-sight (LOS) velocities. Of our 25\ncandidates, 20 stars can be confirmed as bona fide BHB stars. Their LOS\nvelocities, together with the 3D positions of these stars qualitatively match\nwell with Sgr model predictions and trace the outer apocentre of the trailing\narm and its spur feature very nicely. The quantitative offsets that are found\nbetween our data and the different models can be used to provide information\nabout the Galactic gravitational potential at large distances. We present a\nfirst, tentative, analysis in this direction, showing that the model of\nVasiliev et al. (2021) would provide better agreement with our observations if\nthe enclosed mass of the Milky Way within 100 kpc were lowered to\n$(5.3\\!\\pm\\!0.4)\\!\\times\\!10^{11}$ M$_\\odot$ (versus\n$(5.6\\!\\pm\\!0.4)\\!\\times\\!10^{11}$ M$_\\odot$). Our selection of BHB stars\nprovides a new view on the outermost structure in 3D positions and LOS\nvelocities of the Sgr debris.",
        "In near-term quantum computations that do not employ error correction, noise\ncan proliferate rapidly, corrupting the quantum state and making results\nunreliable. These errors originate from both decoherence and control\nimprecision. The latter can manifest as coherent noise that is especially\ndetrimental. Here, we study the impact of coherent errors and their mitigation\nunder standard error-reduction techniques, both theoretically and\nexperimentally on a trapped-ion quantum computer. As a representative case\nstudy, we implement a range of Grover's algorithm circuits containing up to 10\nqubits and 26 two-qubit gates. We demonstrate the effectiveness of randomized\ncompiling (RC) and algorithm error detection (ED), where the latter is realized\nvia post-selection on ancillary qubits that ideally return to the ground state\nat the end of each circuit. Our results highlight a synergetic effect:\ncombining RC and ED yields the largest reductions in errors, indicating that\nthese methods can work together to extend the capabilities of near-term quantum\ndevices for moderately deep circuits.",
        "In this note I demonstrate that the collection of Dynkin systems on finite\nsets assembles into a Connes-Consani $\\mathbb{F}_1$-module, with the collection\nof partitions of finite sets as a sub-module. The underlying simplicial set of\nthis $\\mathbb{F}_1$-module is shown to be isomorphic to the delooping of the\nKrasner hyperfield $\\mathbb{K}$, where $1+1=\\{0,1\\}$. The face and degeneracy\nmaps of the underlying simplicial set of the $\\mathbb{F}_1$-module of\npartitions correspond to merging partition blocks and introducing singleton\nblocks, respectively. I also show that the $\\mathbb{F}_1$-module of partitions\ncannot correspond to a set with a binary operation (even partially defined or\nmultivalued) under the ``Eilenberg-MacLane'' embedding. These results imply\nthat the $n$-fold sum of the Dynkin $\\mathbb{F}_1$-module with itself is\nisomorphic to the $\\mathbb{F}_1$-module of the discrete projective geometry on\n$n$ points.",
        "We present two spatial-multiplexed switch-and-select (S&S) 8x8 optical\nswitches incorporating a tri-layer SiN-on-Si platform, one equipped with\nthermo-optic (T-O) and the other electro-optic (E-O) switching elements. To the\nbest of our knowledge, the electro-optic switch fabric is the first-of-its-kind\ndevice assembled in such a multi-layer platform. The shuffle between the\nmultiplexer and demultiplexer array is established via a tri-layer Si-SiN-SiN\nstructure, creating a three-dimensional crossing-free photonic shuffle network.\nAt the same time, the implementation of the S&S topology can effectively\nsuppress the first-order crosstalk. The measured on-chip losses for the T-O\nswitch range from 2.1 to 11.5 dB, with a 5.2 dB average, while the E-O device\nexhibits losses between 8.7 to 19.6 dB, with a 15.1 dB average. Both switches\ndemonstrate ultra-low crosstalk, with measured ranges of 38.9 to 50.8 dB and\n42.8 to 51.9 dB, for the T-O and E-O devices respectively. The switching times\nare 17.6 us for the T-O switch and 5.9 ns with the E-O actuated one. These\nperformance metrics highlight the potential of these switches for\nnext-generation data center applications.",
        "Strong first-order phase transitions in a dark sector offer a compelling\nexplanation for the stochastic gravitational wave background in the nano-Hertz\nrange recently detected by pulsar timing arrays (PTAs). We explore the\npossibility that such a phase transition at the same time gives mass to a\nstable fermion that accounts for the observed dark matter abundance and leads\nto testable effects in laboratory experiments. Concretely, we consider a\nclassically conformal dark sector with a hidden $U(1)^\\prime$ gauge symmetry\nthat couples to the Standard Model via kinetic mixing. Since the PTA signal\nrequires a phase transition in the MeV temperature range, spontaneous symmetry\nbreaking gives rise to a sub-GeV dark matter candidate that couples to the\nStandard Model via a dark photon mediator and obtains its relic abundance via\nannihilations into electrons and dark Higgs bosons. Such a scenario is tightly\nconstrained by laboratory searches for dark photons and cosmological\nconstraints on the decays of dark Higgs bosons after the phase transition. We\nshow that viable parameter regions can be found both for the case that the dark\nHiggs bosons remain in equilibrium with the Standard Model and that they\ndecouple and only decay much later. In the latter case, the parameter regions\npreferred by the PTA signal and the dark matter relic abundance can be fully\nexplored by future beam-dump experiments searching for missing energy.",
        "Allocation of limited resources under uncertain requirements often\nnecessitates fairness considerations, with applications in computer systems,\nhealth systems, and humanitarian logistics. This paper introduces a\ndistributionally robust (DR) stochastic fairness framework for multi-resource\nallocation, leveraging rough estimates of the mean and variance of resource\nrequirement distributions. The framework employs a sampled approximation DR\n(SA-DR) model to develop the concept of stochastic fairness, satisfying key\nproperties such as stochastic Pareto efficiency, stochastic sharing incentive,\nand stochastic envy-freeness under suitable conditions. We show the convergence\nof the SA-DR model to the DR model and propose a finitely convergent algorithm\nto solve the SA-DR model. We empirically evaluate the performance of our\nmoment-based SA-DR model -- which uses only rough estimates of the mean and\nvariance of the resource requirement distribution -- against alternative\nresource allocation models under varying levels of information availability. We\ndemonstrate that our moment-based partial-information SA-DR model can achieve\nperformance closer to the full-information model than the worst-case\ninformation model. Convergence of the sampled approximation model and\ncomparisons across models are illustrated using data from cloud computing\napplications.",
        "Subject of this paper is the simplification of Markov chain Monte Carlo\nsampling as used in Bayesian statistical inference by means of normalising\nflows, a machine learning method which is able to construct an invertible and\ndifferentiable transformation between Gaussian and non-Gaussian random\ndistributions. We use normalising flows to compute Bayesian partition functions\nfor non-Gaussian distributions and show how normalising flows can be employed\nin finding analytical expressions for posterior distributions beyond the\nGaussian limit. Flows offer advantages for the numerical evaluation of the\npartition function itself, as well as for cumulants and for the information\nentropy. We demonstrate how normalising flows in conjunction with Bayes\npartitions can be used in inference problems in cosmology and apply them to the\nposterior distribution for the matter density $\\Omega_m$ and a dark energy\nequation of state parameter $w_0$ on the basis of supernova data.",
        "Purpose: To develop 5T-SRIS, an improved 5T myocardial T1 mapping method\nbased on MOLLI, which addresses limitations in inversion efficiency, readout\nperturbations, and imperfect magnetization recovery. Methods: The proposed\n5T-SRIS method is based on a modified 5-(3)-3 MOLLI sequence with ECG gating\nand gradient echo readout. To improve inversion efficiency at 5T, the inversion\npulse was redesigned using adiabatic hyperbolic secant (HSn) and\ntangent\/hyperbolic tangent (Tan\/Tanh) pulses. Signal evolution was modeled\nrecursively with inversion efficiency and a correction factor (C) to correct\ninversion imperfections, and T1 values were estimated via nonlinear\noptimization. The method was validated in phantom studies, as well as in 21\nhealthy volunteers and 9 patients at 5T. Results: The optimized IR pulse based\non the tangent\/hyperbolic tangent pulse was found to outperform the\nconventional hyperbolic secant IR pulse at the 5T scanner. This optimized IR\npulse achieves an average inversion factor of 0.9014within a B0 range of 250Hz\nand a B1 range of -50% to 20%. Phantom studies show that the 5T-SRIS achieved\nhigh accuracy with errors within 5%. In vivo studies with 21 healthy\nvolunteers, the native myocardial T1 values were 1468 ms (apex), 1514 ms\n(middle), and 1545 ms (base). In vivo studies with 9 heart patients, the native\nmyocardial T1 values were 1484 ms (apex), 1532 ms (middle), and 1581 ms (base).\nAnd the post myocardial T1 values were 669 ms (apex), 698 ms (middle), and 675\nms (base). Conclusion: The 5T-SRIS technique is robust and suitable for\nclinical cardiac imaging. This study demonstrates its feasibility for accurate\nmyocardial T1 mapping at 5T, despite challenges related to magnetic field\ninhomogeneity. Keywords: Myocardial T1 mapping, 5T, improved MOLLI, 5T-SRIS",
        "The power graph \\( \\mathcal{G}_G \\) of a group \\( G \\) is a graph whose\nvertex set is \\( G \\), and two elements \\( x, y \\in G \\) are adjacent if one is\nan integral power of the other. In this paper, we determine the adjacency,\nLaplacian, and signless Laplacian spectra of the power graph of the dihedral\ngroup \\( D_{2pq} \\), where \\( p \\) and \\( q \\) are distinct primes. Our\nfindings demonstrate that the results of Romdhini et al. [2024], published in\nthe \\textit{European Journal of Pure and Applied Mathematics}, do not hold\nuniversally for all \\( n \\geq 3 \\). Our analysis demonstrates that their\nresults hold true exclusively when \\( n = p^m \\) where \\( p \\) is a prime\nnumber and \\( m \\) is a positive integer. The research examines their\nmethodology via explicit counterexamples to expose its boundaries and establish\ncorrected results. This study improves past research by expanding the spectrum\nevaluation of power graphs linked to dihedral groups.",
        "Calibration of instrumental polarization is critical for measuring polarized\nradio emissions from astrophysical sources to extract the magnetic field\ninformation in astrophysical, heliospheric, and terrestrial plasmas. At meter\nwavelengths, calibration of radio polarimetric observations is particularly\nchallenging because of the scarcity of bright polarized sources due to\nsignificant Faraday depolarization. Here, we present a novel formalism for\npolarization calibration using an unpolarized sky model. The formalism is\nspecifically designed for wide-field, low-frequency instruments like the\nMurchison Widefield Array (MWA), the LOw Frequency ARray (LOFAR), New Extension\nin Nan\\c{c}ay Upgrading LoFAR (NenuFAR), Owens Valley Radio Observatory - Long\nWavelength Array (OVRO-LWA), low-frequency telescope of the Square Kilometre\nArray Observatory (SKAO-low), etc. By leveraging the apparent polarization of\nthe unpolarized sky induced by the polarized primary beam of the radio\ntelescope, this method avoids dependence on bright polarized calibrators. It is\nalso immune to ionospheric Faraday rotation. The validation of the approach via\nMWA observations confirms the accuracy of the method. This formalism provides a\nrobust framework for low-frequency polarization calibration. It addresses the\nlongstanding calibration challenges and advances the field of low-frequency\npolarimetry by enabling polarization studies of astrophysical radio sources.",
        "Photonic computing has emerged as a promising platform for accelerating\ncomputational tasks with high degrees of parallelism, such as image processing\nand neural network. We present meta-DFT (discrete Fourier transform), a single\nlayer metasurface device, designed to perform optical complex-to-complex DFT\nwith O(N) time complexity. One critical challenge in free-space analog optical\ncomputing is to control the measurement error. Our scheme addresses this issue\nby focusing light on spatially separated focal points and reconstructing the\ncomplex phase, which enable error correction. We systematically evaluate the\ndevice's performance using input vectors with random complex amplitudes and\nphases, to demonstrate its robust accuracy. Our findings pave the way towards\nadvancement of metasurface-based computation, offering a robust framework that\nis readily extensible to an arbitrary complex-valued matrix-vector\nmultiplication (MVM).",
        "The longitudinal oscillations of air columns composed of contractions and\nrarefaction make up sound. Sound amplification is widely used in medical,\nelectronic and communication fields. A simplistic technique for producing and\namplifying can be rewarding. In this study, we investigate a simplistic DIY\nspeaker configuration that can be utilized for sound creation and modulation by\nimplementing response of magnets and a solenoid to an oscillating input signal.\nWe use steady state solution of forced simple harmonic oscillator with damping\nparameters to analyze our design and show its characteristic frequencies. We\npresent an analytical way of obtaining optimal parameters of the setup to\ntheoretically obtain experimental characteristic frequencies and provide an\nin-depth investigation of the setup.",
        "In this paper, we study the impact of reconfigurable intelligent surfaces\n(RISs) on the coverage extension of integrated access and backhaul (IAB)\nnetworks. Particularly, using a finite stochastic geometry model, with random\ndistributions of user equipments (UEs) in a finite region, and planned\nhierachical architecture for IAB, we study the service coverage probability\ndefined as the probability of the event that the UEs' minimum rate requirements\nare satisfied. We present comparisons between different cases including\nIAB-only, IAB assisted with RIS for backhaul as well as IAB assisted by network\ncontrolled repeaters (NCRs). Our investigations focus on wide-area IAB assisted\nwith RIS through the lens of different design architectures and deployments,\nrevealing both conflicts and synergies for minimizing the effect of tree\nfoliage over seasonal changes. Our simulation results reveal both opportunities\nand challenges towards the implementation of RIS in IAB.",
        "For a natural number $n$, denote by $B_n$ the braid group on $n$ strings and\nby $SM_n$ the singular braid monoid on $n$ strings. $SM_n$ is one of the most\nimportant extensions of $B_n$. In [13], Y. Mikhalchishina classified all\nhomogeneous $2$-local representations of $B_n$ for all $n \\geq 3$. In this\narticle, we extend the result of Mikhalchishina in two ways. First, we classify\nall homogeneous $3$-local representations of $B_n$ for all $n \\geq 4$. Second,\nwe classify all homogeneous $2$-local representations of $SM_n$ for all $n\\geq\n2$ and all homogeneous $3$-local representations of $SM_n$ for all $n\\geq 4$.",
        "Transformers are a type of neural network that have demonstrated remarkable\nperformance across various domains, particularly in natural language processing\ntasks. Motivated by this success, research on the theoretical understanding of\ntransformers has garnered significant attention. A notable example is the\nmathematical analysis of their approximation power, which validates the\nempirical expressive capability of transformers. In this study, we investigate\nthe ability of transformers to approximate column-symmetric polynomials, an\nextension of symmetric polynomials that take matrices as input. Consequently,\nwe establish an explicit relationship between the size of the transformer\nnetwork and its approximation capability, leveraging the parameter efficiency\nof transformers and their compatibility with symmetry by focusing on the\nalgebraic properties of symmetric polynomials.",
        "In reinforcement learning, universal successor features (SFs) are a way to\nprovide zero-shot adaptation to new tasks at test time: they provide optimal\npolicies for all downstream reward functions lying in the linear span of a set\nof base features. But it is unclear what constitutes a good set of base\nfeatures, that could be useful for a wide set of downstream tasks beyond their\nlinear span. Laplacian eigenfunctions (the eigenfunctions of\n$\\Delta+\\Delta^\\ast$ with $\\Delta$ the Laplacian operator of some reference\npolicy and $\\Delta^\\ast$ that of the time-reversed dynamics) have been argued\nto play a role, and offer good empirical performance.\n  Here, for the first time, we identify the optimal base features based on an\nobjective criterion of downstream performance, in a non-tautological way\nwithout assuming the downstream tasks are linear in the features. We do this\nfor three generic classes of downstream tasks: reaching a random goal state,\ndense random Gaussian rewards, and random ``scattered'' sparse rewards. The\nfeatures yielding optimal expected downstream performance turn out to be the\n\\emph{same} for these three task families. They do not coincide with Laplacian\neigenfunctions in general, though they can be expressed from $\\Delta$: in the\nsimplest case (deterministic environment and decay factor $\\gamma$ close to\n$1$), they are the eigenfunctions of $\\Delta^{-1}+(\\Delta^{-1})^\\ast$.\n  We obtain these results under an assumption of large behavior cloning\nregularization with respect to a reference policy, a setting often used for\noffline RL. Along the way, we get new insights into\nKL-regularized\\option{natural} policy gradient, and into the lack of SF\ninformation in the norm of Bellman gaps.",
        "A signal consistent with the $\\Omega$(2012) baryon has been observed with a\nsignificance of $15\\sigma$ in pp collisions at $\\sqrt{s} = 13$ TeV at the LHC.\nIn this paper, the analysis technique is described and measurements of the mass\nand width of the $\\Omega$(2012) are reported, along with the first measurement\nof its transverse-momentum spectrum and yield. This paper corroborates the\nobservation by Belle of this excited $\\Omega$ state and the observation that\nthe $\\Omega$(2012) has a rather narrow width for a strongly decaying resonance.\nThe yield measurement is combined with a statistical thermal model calculation\nof strange baryon yield ratios to obtain estimates of the $\\Omega{\\rm\n(2012)}^{-} \\rightarrow \\Xi\\overline{\\rm K}$ branching ratios. These results\nwill improve our understanding of the internal structure and mass spectrum of\nexcited baryon states and serve as a baseline for searches regarding\nmodifications of these properties in high-temperature media."
      ]
    }
  },
  {
    "id":2411.05237,
    "research_type":"basic",
    "start_id":"b0",
    "start_title":"Factors Influencing Physicians' Clinical Decision-making at Upazila Health Complexes in Bangladesh",
    "start_abstract":"Selecting the most appropriate treatment for each patient is key activity in patient-physician encounters and providing healthcare services. Achieving desirable clinical goals mostly depends on making right decision at time any setting. But little known about physicians' decision-making primary care setting Bangladesh. Therefore, this study explored factors that influence decisions prescribing medications, ordering pathologic tests, counseling patients, average length of visits a consultation session, referral patients to other physicians or hospitals by Upazila Health Complexes (UHCs) country. It also structure social networks their association with process.This was cross-sectional descriptive used data collected from 85 physicians. The respondents, who work UHCs Rajshahi Division, were selected purposively. analyzed statistics including frequency, percentage, one-way analysis variance, linear regression understand relationships among variables.The results reveal multiple visits, referring UHCs. Most prescribe drugs keeping mind purchasing capacity. Risk violence patients' relatives better management are two decisions. professional personal play an influential role process. found dedicate 16.17 minutes session. influenced various distance between residence workplace, level education, number colleagues whom they have regular contact can seek help.The yielded some novel insights complexity everyday tasks would be interest public health researchers policy makers.",
    "start_categories":[
      "Healthcare"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Interactive Teaching Algorithms for Inverse Reinforcement Learning"
      ],
      "abstract":[
        "We study the problem of inverse reinforcement learning (IRL) with added twist that learner is assisted by a helpful teacher. More formally, we tackle following algorithmic question: How could teacher provide an informative sequence demonstrations to IRL speed up process? present interactive teaching framework where adaptively chooses next demonstration based on learner's current policy. In particular, design algorithms for two concrete settings: omniscient setting has full knowledge about dynamics and blackbox minimal knowledge. Then, sequential variant popular MCE-IRL prove convergence guarantees our algorithm in setting. Extensive experiments car driving simulator environment show progress can be speeded drastically as compared uninformative"
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Hamilton-Jacobi equations involving a Caputo time-fractional derivative",
        "Uniting Quantum Processing Nodes of Cavity-coupled Ions with Rare-earth\n  Quantum Repeaters Using Single-photon Pulse Shaping Based on Atomic Frequency\n  Comb",
        "Orderable Thompson-like groups arising from Ore categories",
        "Less Biased Noise Scale Estimation for Threshold-Robust RANSAC",
        "Dynami-CAL GraphNet: A Physics-Informed Graph Neural Network Conserving\n  Linear and Angular Momentum for Dynamical Systems",
        "Non-(strong) ergodicity criteria for discrete time Markov chains on\n  general state spaces",
        "Benchmark on Peer Review Toxic Detection: A Challenging Task with a New\n  Dataset",
        "Noise avalanche and its quantum quenching in bosonic chains with random\n  off-diagonal disorder",
        "MIRROR: Multi-Modal Pathological Self-Supervised Representation Learning\n  via Modality Alignment and Retention",
        "Towards Data-Efficient Pretraining for Atomic Property Prediction",
        "Oscillations Make Neural Networks Robust to Quantization",
        "Pulling Back Theorem for Generalizing the Diagonal Averaging Principle\n  in Symplectic Geometry Mode Decomposition and Singular Spectrum Analysis",
        "Self-Adaptive Ising Machines for Constrained Optimization",
        "A $p$-adic Gross-Zagier formula for twisted triple product $p$-adic\n  $L$-functions attached to finite slope families",
        "From High-Entropy Alloys to Alloys with High Entropy: A New Paradigm in\n  Materials Science and Engineering for Advancing Sustainable Metallurgy",
        "CVKAN: Complex-Valued Kolmogorov-Arnold Networks",
        "Fair densest subgraph across multiple graphs",
        "QBIOL: A quantum bioelectrochemical software based on point stochastic\n  processes",
        "Existence and Design of Target Output Controllers",
        "The Extraordinary Long-lasting Infrared Echo of PS16dtm Reveals an\n  Extremely Energetic Nuclear Outburst",
        "ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant\n  Tightness",
        "Evolution of X-ray Gas in SN 1987A from 2007 to 2021: Ring Fading and\n  Ejecta Brightening Unveiled through Differential Emission Measure Analysis",
        "Ab initio thermal conductivity of Ge$_x$Sn$_{1-x}$O$_2$ alloys",
        "New Stress-dependent Elastic Wave Velocity Models for Reservoir Rocks\n  with Applications",
        "Many-body effects of two-level systems in superconducting qubits",
        "GRIP: A General Robotic Incremental Potential Contact Simulation Dataset\n  for Unified Deformable-Rigid Coupled Grasping",
        "TimelineKGQA: A Comprehensive Question-Answer Pair Generator for\n  Temporal Knowledge Graphs",
        "InfoBridge: Mutual Information estimation via Bridge Matching",
        "Kise-Manitow's Hand in Space: Securing Communication and Connections in\n  Space"
      ],
      "abstract":[
        "We prove a representation formula of intrinsic Hopf-Lax type for subsolutions\nto Hamilton-Jacobi equations involving a Caputo time-fractional derivative.",
        "We present an architecture for remotely connecting cavity-coupled trapped\nions via a quantum repeater based on rare-earth-doped crystals. The main\nchallenge for its realization lies in interfacing these two physical platforms,\nwhich produce photons with a typical temporal mismatch of one or two orders of\nmagnitude. To address this, we propose an efficient protocol that enables\ncustom temporal reshaping of single-photon pulses whilst preserving purity. Our\napproach is to modify a commonly used memory protocol, called atomic frequency\ncomb, for systems exhibiting inhomogeneous broadening like rare-earth-doped\ncrystals. Our results offer a viable solution for uniting quantum processing\nnodes with a quantum repeater backbone.",
        "We give sufficient conditions for left- and bi-orderability of fundamental\ngroups of Ore categories in terms of indirect factors, including Thompson\ngroups and many of their generalizations. Besides recovering known results, we\nprove that braided groups of fractions of digit rewriting systems (which\ngeneralize braided Thompson groups to the wider setting of topological full\ngroups of edge shift) are left-orderable, and that their purely braided\ncounterparts are bi-orderable. In particular, the braided Houghton groups are\nleft-orderable.",
        "The gold-standard for robustly estimating relative pose through image\nmatching is RANSAC. While RANSAC is powerful, it requires setting the inlier\nthreshold that determines whether the error of a correspondence under an\nestimated model is sufficiently small to be included in its consensus set.\nSetting this threshold is typically done by hand, and is difficult to tune\nwithout a access to ground truth data. Thus, a method capable of automatically\ndetermining the optimal threshold would be desirable. In this paper we revisit\ninlier noise scale estimation, which is an attractive approach as the inlier\nnoise scale is linear to the optimal threshold. We revisit the noise scale\nestimation method SIMFIT and find bias in the estimate of the noise scale. In\nparticular, we fix underestimates from using the same data for fitting the\nmodel as estimating the inlier noise, and from not taking the threshold itself\ninto account. Secondly, since the optimal threshold within a scene is\napproximately constant we propose a multi-pair extension of SIMFIT++, by\nfiltering of estimates, which improves results. Our approach yields robust\nperformance across a range of thresholds, shown in Figure 1.",
        "Accurate, interpretable, and real-time modeling of multi-body dynamical\nsystems is essential for predicting behaviors and inferring physical properties\nin natural and engineered environments. Traditional physics-based models face\nscalability challenges and are computationally demanding, while data-driven\napproaches like Graph Neural Networks (GNNs) often lack physical consistency,\ninterpretability, and generalization. In this paper, we propose Dynami-CAL\nGraphNet, a Physics-Informed Graph Neural Network that integrates the learning\ncapabilities of GNNs with physics-based inductive biases to address these\nlimitations. Dynami-CAL GraphNet enforces pairwise conservation of linear and\nangular momentum for interacting nodes using edge-local reference frames that\nare equivariant to rotational symmetries, invariant to translations, and\nequivariant to node permutations. This design ensures physically consistent\npredictions of node dynamics while offering interpretable, edge-wise linear and\nangular impulses resulting from pairwise interactions. Evaluated on a 3D\ngranular system with inelastic collisions, Dynami-CAL GraphNet demonstrates\nstable error accumulation over extended rollouts, effective extrapolations to\nunseen configurations, and robust handling of heterogeneous interactions and\nexternal forces. Dynami-CAL GraphNet offers significant advantages in fields\nrequiring accurate, interpretable, and real-time modeling of complex multi-body\ndynamical systems, such as robotics, aerospace engineering, and materials\nscience. By providing physically consistent and scalable predictions that\nadhere to fundamental conservation laws, it enables the inference of forces and\nmoments while efficiently handling heterogeneous interactions and external\nforces.",
        "For discrete time Markov chains on general state spaces, we provide criteria\nfor non-ergodicity and non-strong ergodicity. By taking advantage of minimal\nnon-negative solution theory, our criteria are in terms of the existence of\nsolutions to inequalities involving the one step transition semigroup of the\nchain. Based on Dynkin's formula, Lyapunov-type conditions for non-strong\nergodicity are also obtained.",
        "Peer review is crucial for advancing and improving science through\nconstructive criticism. However, toxic feedback can discourage authors and\nhinder scientific progress. This work explores an important but underexplored\narea: detecting toxicity in peer reviews. We first define toxicity in peer\nreviews across four distinct categories and curate a dataset of peer reviews\nfrom the OpenReview platform, annotated by human experts according to these\ndefinitions. Leveraging this dataset, we benchmark a variety of models,\nincluding a dedicated toxicity detection model, a sentiment analysis model,\nseveral open-source large language models (LLMs), and two closed-source LLMs.\nOur experiments explore the impact of different prompt granularities, from\ncoarse to fine-grained instructions, on model performance. Notably,\nstate-of-the-art LLMs like GPT-4 exhibit low alignment with human judgments\nunder simple prompts but achieve improved alignment with detailed instructions.\nMoreover, the model's confidence score is a good indicator of better alignment\nwith human judgments. For example, GPT-4 achieves a Cohen's Kappa score of 0.56\nwith human judgments, which increases to 0.63 when using only predictions with\na confidence score higher than 95%. Overall, our dataset and benchmarks\nunderscore the need for continued research to enhance toxicity detection\ncapabilities of LLMs. By addressing this issue, our work aims to contribute to\na healthy and responsible environment for constructive academic discourse and\nscientific collaboration.",
        "Here we discuss a phenomenon of sharp increase in the photon number noise at\ninitial stages of propagation in tight-binding bosonic chains with off-diagonal\ndisorder. Such a \"noise avalanche\" occurs under classical coherent excitation\nof waveguides and leads to high super-thermal photon bunching. Additional\nclassical excitation slows but cannot quench this noise avalanche. However, an\nadditional single-photon excitation stops the avalanche.",
        "Histopathology and transcriptomics are fundamental modalities in oncology,\nencapsulating the morphological and molecular aspects of the disease.\nMulti-modal self-supervised learning has demonstrated remarkable potential in\nlearning pathological representations by integrating diverse data sources.\nConventional multi-modal integration methods primarily emphasize modality\nalignment, while paying insufficient attention to retaining the\nmodality-specific structures. However, unlike conventional scenarios where\nmulti-modal inputs share highly overlapping features, histopathology and\ntranscriptomics exhibit pronounced heterogeneity, offering orthogonal yet\ncomplementary insights. Histopathology provides morphological and spatial\ncontext, elucidating tissue architecture and cellular topology, whereas\ntranscriptomics delineates molecular signatures through gene expression\npatterns. This inherent disparity introduces a major challenge in aligning them\nwhile maintaining modality-specific fidelity. To address these challenges, we\npresent MIRROR, a novel multi-modal representation learning method designed to\nfoster both modality alignment and retention. MIRROR employs dedicated encoders\nto extract comprehensive features for each modality, which is further\ncomplemented by a modality alignment module to achieve seamless integration\nbetween phenotype patterns and molecular profiles. Furthermore, a modality\nretention module safeguards unique attributes from each modality, while a style\nclustering module mitigates redundancy and enhances disease-relevant\ninformation by modeling and aligning consistent pathological signatures within\na clustering space. Extensive evaluations on TCGA cohorts for cancer subtyping\nand survival analysis highlight MIRROR's superior performance, demonstrating\nits effectiveness in constructing comprehensive oncological feature\nrepresentations and benefiting the cancer diagnosis.",
        "This paper challenges the recent paradigm in atomic property prediction that\nlinks progress to growing dataset sizes and computational resources. We show\nthat pretraining on a carefully selected, task-relevant dataset can match or\neven surpass large-scale pretraining, while using as little as 1\/24th of the\ncomputational cost. We introduce the Chemical Similarity Index (CSI), a novel\nmetric inspired by computer vision's Fr\\'echet Inception Distance, for\nmolecular graphs which quantifies the alignment between upstream pretraining\ndatasets and downstream tasks. By selecting the most relevant dataset with\nminimal CSI distance, we show that models pretrained on a smaller, focused\ndataset consistently outperform those pretrained on massive, mixed datasets\nsuch as JMP, even when those larger datasets include the relevant dataset.\nCounterintuitively, we also find that indiscriminately adding more data can\ndegrade model performance when the additional data poorly aligns with the task\nat hand. Our findings highlight that quality often outperforms quantity in\npretraining for atomic property prediction.",
        "We challenge the prevailing view that oscillations in Quantization Aware\nTraining (QAT) are merely undesirable artifacts caused by the Straight-Through\nEstimator (STE). Through theoretical analysis of QAT in linear models, we\ndemonstrate that the gradient of the loss function can be decomposed into two\nterms: the original full-precision loss and a term that causes quantization\noscillations. Based on these insights, we propose a novel regularization method\nthat induces oscillations to improve quantization robustness. Contrary to\ntraditional methods that focuses on minimizing the effects of oscillations, our\napproach leverages the beneficial aspects of weight oscillations to preserve\nmodel performance under quantization. Our empirical results on ResNet-18 and\nTiny ViT demonstrate that this counter-intuitive strategy matches QAT accuracy\nat >= 3-bit weight quantization, while maintaining close to full precision\naccuracy at bits greater than the target bit. Our work therefore provides a new\nperspective on model preparation for quantization, particularly for finding\nweights that are robust to changes in the bit of the quantizer -- an area where\ncurrent methods struggle to match the accuracy of QAT at specific bits.",
        "The symplectic geometry mode decomposition (SGMD) is a powerful method for\nanalyzing time sequences. The SGMD is based on the upper conversion via\nembedding and down conversion via diagonal averaging principle (DAP) inherited\nfrom the singular spectrum analysis (SSA). However, there are two defects in\nthe DAP: it just hold for the time delay $\\tau=1$ in the trajectory matrix and\nit fails for the time sequence of type-1 with the form $X=\\{x[n]\\}^N_{n=1}$. In\norder to overcome these disadvantages, the inverse step for embedding is\nexplored with binary Diophantine equation in number theory. The contributions\nof this work lie in three aspects: firstly, the pulling back theorem is\nproposed and proved, which state the general formula for converting the\ncomponent of trajectory matrix to the component of time sequence for the\ngeneral representation of time sequence and for any time delay $\\tau\\ge 1$;\nsecondly a unified framework for decomposing both the deterministic and random\ntime sequences into multiple modes is presented and explained; finally, the\nguidance of configuring the time delay is suggested, namely the time delay\nshould be selected in a limited range via balancing the efficiency of matrix\ncomputation and accuracy of state estimation. It could be expected that the\npulling back theorem will help the researchers and engineers to deepen the\nunderstanding of the theory and extend the applications of the SGMD and SSA in\nanalyzing time sequences.",
        "Ising machines (IM) are physics-inspired alternatives to von Neumann\narchitectures for solving hard optimization tasks. By mapping binary variables\nto coupled Ising spins, IMs can naturally solve unconstrained combinatorial\noptimization problems such as finding maximum cuts in graphs. However, despite\ntheir importance in practical applications, constrained problems remain\nchallenging to solve for IMs that require large quadratic energy penalties to\nensure the correspondence between energy ground states and constrained optimal\nsolutions. To relax this requirement, we propose a self-adaptive IM that\niteratively shapes its energy landscape using a Lagrange relaxation of\nconstraints and avoids prior tuning of penalties. Using a probabilistic-bit\n(p-bit) IM emulated in software, we benchmark our algorithm with\nmultidimensional knapsack problems (MKP) and quadratic knapsack problems (QKP),\nthe latter being an Ising problem with linear constraints. For QKP with 300\nvariables, the proposed algorithm finds better solutions than state-of-the-art\nIMs such as Fujitsu's Digital Annealer and requires 7,500x fewer samples. Our\nresults show that adapting the energy landscape during the search can speed up\nIMs for constrained optimization.",
        "Our main objective in the present paper is to generalise the work of\nBlanco-Chac\\'{o}n and Fornea on the $p$-adic Gross-Zagier formula for twisted\ntriple product $p$-aidc $L$-function. We extend their main result to the case\nof finite slope families of Hilbert modular forms and also allow the prime $p$\nto be inert in the real quadratic field $L$.",
        "The development of high-entropy alloys (HEAs) has marked a paradigm shift in\nalloy design, moving away from traditional methods that prioritize a dominant\nbase metal enhanced by minor elements. HEAs instead incorporate multiple\nalloying elements with no single dominant component, broadening the scope of\nalloy design. This shift has led to the creation of diverse alloys with high\nentropy (AHEs) families, including high-entropy steels, superalloys, and\nintermetallics, each highlighting the need to consider additional factors such\nas stacking fault energy (SFE), lattice misfit, and anti-phase boundary energy\n(APBE) due to their significant influence on microstructure and performance.\nLeveraging multiple elements in alloying opens up promising possibilities for\ndeveloping new alloys from multi-component scrap and electronic waste, reducing\nreliance on critical metals and emphasizing the need for advanced data\ngeneration techniques. With the vast possibilities offered by these\nmulti-component feedstocks, modelling and Artificial Intelligence based tools\nare essential to efficiently explore and optimize new alloys, supporting\nsustainable progress in metallurgy. These advancements call for a reimagined\nalloy design framework, emphasizing robust data acquisition, alternative design\nparameters, and advanced computational tools over traditional\ncomposition-focused methodologies.",
        "In this work we propose CVKAN, a complex-valued KAN, to join the intrinsic\ninterpretability of KANs and the advantages of Complex-Valued Neural Networks\n(CVNNs). We show how to transfer a KAN and the necessary associated mechanisms\ninto the complex domain. To confirm that CVKAN meets expectations we conduct\nexperiments on symbolic complex-valued function fitting and physically\nmeaningful formulae as well as on a more realistic dataset from knot theory.\nOur proposed CVKAN is more stable and performs on par or better than\nreal-valued KANs while requiring less parameters and a shallower network\narchitecture, making it more explainable.",
        "Many real-world networks can be modeled as graphs. Finding dense subgraphs is\na key problem in graph mining with applications in diverse domains. In this\npaper, we consider two variants of the densest subgraph problem where multiple\ngraph snapshots are given and the goal is to find a fair densest subgraph\nwithout over-representing the density among the graph snapshots. More formally,\ngiven a set of graphs and input parameter $\\alpha$, we find a dense subgraph\nmaximizing the sum of densities across snapshots such that the difference\nbetween the maximum and minimum induced density is at most $\\alpha$. We prove\nthat this problem is NP-hard and present an integer programming based, exact\nalgorithm and a practical polynomial-time heuristic. We also consider a\nminimization variant where given an input parameter $\\sigma$, we find a dense\nsubgraph which minimizes the difference between the maximum and minimum density\nwhile inducing a total density of at least $\\sigma$ across the graph snapshots.\nWe prove the NP-hardness of the problem and propose two algorithms: an\nexponential time algorithm based on integer programming and a greedy algorithm.\nWe present an extensive experimental study that shows that our algorithms can\nfind the ground truth in synthetic dataset and produce good results in\nreal-world datasets. Finally, we present case studies that show the usefulness\nof our problem.",
        "Bioelectrochemistry is crucial for understanding biological functions and\ndriving applications in synthetic biology, healthcare, and catalysis. However,\ncurrent simulation methods fail to capture both the stochastic nature of\nmolecular motion and electron transfer across the relevant picosecond-to-minute\ntimescales. We present QBIOL, a web-accessible software that integrates\nmolecular dynamics, applied mathematics, GPU programming, and quantum charge\ntransport to address this challenge. QBIOL enables quantitative stochastic\nelectron transfer simulations and has the potential to reproduce numerically\nany (bio) electrochemical experiments. We illustrate this potential by\ncomparing our simulations with experimental data on the current generated by\nelectrode-attached redox-labeled DNA, or by nanoconfined redox species, in\nresponse to a variety of electrical excitation waveforms, configurations of\ninterest in biosensing and catalysis. The adaptable architecture of QBIOL\nextends to the development of devices for quantum and molecular technologies,\npositioning our software as a powerful tool for enabling new research in this\nrapidly evolving field.",
        "This paper introduces new conditions for target output controllability and\nprovides existence conditions for placing a specific number of poles with a\ntarget output controller. Additionally, an algorithm is presented for the\ndesign of a target output controller. Controllability of the system under\nconsideration is not required for designing target output controllers in this\ncontext. The findings in this paper extend the principles of full state\nfeedback control. Moreover, we present conditions for static output feedback\ncontrol under specific constraints. Several numerical examples are provided to\nillustrate the results.",
        "PS16dtm is one of the earliest reported candidate tidal disruption events\n(TDEs) in active galactic nuclei (AGNs) and displays a remarkably bright and\nlong-lived infrared (IR) echo revealed by multi-epoch photometry from the\nWide-field Infrared Survey Explorer (WISE). After a rapid rise in the first\nyear, the echo remains persistently at a high state from July 2017 to July\n2024, the latest epoch, and keeps an almost constant color. We have fitted the\nextraordinary IR emission with a refined dust echo model by taking into account\nthe dust sublimation process. The fitting suggests that an extremely giant dust\nstructure with a new inner radius of $\\sim1.6$ pc and an ultra-high peak\nbolometric luminosity, i.e., $\\sim6\\times10^{46} \\rm erg~s^{-1}$ for typical\n0.1$\\mu$m-sized silicate grain, is required to account for the IR echo. This\nwork highlights the distinctive value of IR echoes in measuring the accurate\nintrinsic bolometric luminosity, and thus the total radiated energy of TDEs,\nwhich could be severely underestimated by traditional methods, i.e. probably by\nmore than an order of magnitude in PS16dtm. Such large energetic output\ncompared to normal TDEs could be boosted by the pre-existing accretion disk and\ngas clouds around the black hole. Our model can be validated in the near future\nby IR time-domain surveys such as Near-Earth Object (NEO) Surveyor, given the\nrecent retirement of WISE. In addition, the potential for spatially resolving a\nreceding dusty torus after a TDE could also be an exciting subject in the era\nof advanced IR interferometry.",
        "Fitting a body to a 3D clothed human point cloud is a common yet challenging\ntask. Traditional optimization-based approaches use multi-stage pipelines that\nare sensitive to pose initialization, while recent learning-based methods often\nstruggle with generalization across diverse poses and garment types. We propose\nEquivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline\nthat estimates cloth-to-body surface mapping through locally approximate SE(3)\nequivariance, encoding tightness as displacement vectors from the cloth surface\nto the underlying body. Following this mapping, pose-invariant body features\nregress sparse body markers, simplifying clothed human fitting into an\ninner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show\nthat ETCH significantly outperforms state-of-the-art methods -- both\ntightness-agnostic and tightness-aware -- in body fitting accuracy on loose\nclothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant\ntightness design can even reduce directional errors by (67.2% ~ 89.8%) in\none-shot (or out-of-distribution) settings. Qualitative results demonstrate\nstrong generalization of ETCH, regardless of challenging poses, unseen shapes,\nloose clothing, and non-rigid dynamics. We will release the code and models\nsoon for research purposes at https:\/\/boqian-li.github.io\/ETCH\/.",
        "As the nearest supernova (SN) observed since Kepler's SN of 1604, SN 1987A\nprovides an unprecedented opportunity to study in detail the early evolution of\nsupernova remnants (SNRs). Despite extensive studies through both observations\nand simulations, there is still an urgent need for a more effective approach to\nintegrate the results from two sides. In this study, we conducted a detailed\ndifferential emission measure (DEM) analysis on the XMM-Newton observations\ntaken in 2007 to 2021 to characterize the continuous temperature structure of\nSN 1987A, which can be better compared with simulations. The X-ray plasma\nexhibit a temperature distribution with a major peak at $\\sim0.5$-$1$ keV and a\nhigh-temperature tail extending to $\\gtrsim5$ keV. The emission measure (EM) of\nthe major peak started to decline around 2014, while the EM of the tail\ncontinued increasing and appears to have formed a secondary peak at $\\sim3$-$5$\nkeV in recent years. Our DEM results consistent well with simulations, which\nhelp to further identify the major peak as originating from the equatorial ring\nand the secondary peak as arising from the newly shocked ejecta. Together with\nthe simulations, our DEM analysis reveals recent fading of the ring and\nbrightening of the ejecta in X-rays from SN 1987A. Additionally, we observed a\nrecent decrease in the centroid energy of Fe K line, providing further evidence\nof newly shocked ejecta.",
        "Rutile GeO2 is an emerging ultra-wide band gap semiconductor (UWBG) that has\ndemonstrated excellent potential for applications in power electronic devices.\nAlloys of rutile SnO2, a well-established UWBG semiconducting oxide, with GeO2\nare promising for tuning the material properties for applications. The thermal\nconductivity, in particular, is a key property which is significantly impacted\nby alloy disorder, but which is also essential in assessing the operation and\ndegradation of materials in high-power electronic applications. Here, we\npresent first-principles calculations of the thermal conductivity of rutile\nGeO2, SnO2, and their alloys, and quantify the effects of scattering by alloy\ndisorder, temperature, and isotope mass distribution. We show that the\nrelatively high thermal conductivity of the binary compounds is reduced by\nalloying, grain boundaries, and isotope disorder. However, we also find that\nthe room-temperature thermal conductivity of the alloys is still comparable to\nor surpasses the values for beta-Ga2O3, an established UWBG semiconducting\noxide. Our findings provide a roadmap for the codesign of the thermal\nproperties of rutile GexSn1-xO2 alloys for electronic device applications.",
        "This study presents new elastic velocity-effective stress laws for reservoir\nrocks. These models are grounded in previously established correlations between\nelastic modulus and porosity, which incorporate critical porosity. The accuracy\nof the models is validated against wave velocities from 38 core samples,\nyielding coefficients of determination ($\\mathrm{R}^2$) of 0.9994 for\ncompressional wave and 0.9985 for shear wave. A sensitivity analysis reveals\nthat the maximum uncertainties for compressional and shear waves are less than\n$\\pm$5.5% and $\\pm$7.5%, respectively. To demonstrate the applicability of the\nproposed models, a case study was conducted on three wells in the Northern\nCarnarvon Basin, where the new elastic wave velocity-effective stress laws\nproduced reliable predictions for velocity logs in the studied formations. The\nrelationships reported herein may prove beneficial for hydrocarbon exploration,\nproduction, and ensuring drilling safety in both unconventional and\nconventional fields.",
        "Superconducting qubits are often adversely affected by two-level systems\n(TLSs) within the Josephson junction, which contribute to decoherence and\nsubsequently limit the performance of the qubit. By treating the TLS as a soft\n(i.e., low-frequency) bosonic mode localized in real space, we find that a\nsingle TLS in either the amorphous oxide surface or the superconducting bulk\nmay result in a localized \"hot spot\" of amplified Josephson energy. Such\namplification is shown to have a non-negligible effect on the $T_1$ time of\ncertain superconducting qubits, regardless of whether or not the TLS is on\nresonance with the qubit frequency. With this study, we identify sources of\ndecoherence unique to the superconducting element of superconducting quantum\ndevices.",
        "Grasping is fundamental to robotic manipulation, and recent advances in\nlarge-scale grasping datasets have provided essential training data and\nevaluation benchmarks, accelerating the development of learning-based methods\nfor robust object grasping. However, most existing datasets exclude deformable\nbodies due to the lack of scalable, robust simulation pipelines, limiting the\ndevelopment of generalizable models for compliant grippers and soft\nmanipulands. To address these challenges, we present GRIP, a General Robotic\nIncremental Potential contact simulation dataset for universal grasping. GRIP\nleverages an optimized Incremental Potential Contact (IPC)-based simulator for\nmulti-environment data generation, achieving up to 48x speedup while ensuring\nefficient, intersection- and inversion-free simulations for compliant grippers\nand deformable objects. Our fully automated pipeline generates and evaluates\ndiverse grasp interactions across 1,200 objects and 100,000 grasp poses,\nincorporating both soft and rigid grippers. The GRIP dataset enables\napplications such as neural grasp generation and stress field prediction.",
        "Question answering over temporal knowledge graphs (TKGs) is crucial for\nunderstanding evolving facts and relationships, yet its development is hindered\nby limited datasets and difficulties in generating custom QA pairs. We propose\na novel categorization framework based on timeline-context relationships, along\nwith \\textbf{TimelineKGQA}, a universal temporal QA generator applicable to any\nTKGs. The code is available at: \\url{https:\/\/github.com\/PascalSun\/TimelineKGQA}\nas an open source Python package.",
        "Diffusion bridge models have recently become a powerful tool in the field of\ngenerative modeling. In this work, we leverage their power to address another\nimportant problem in machine learning and information theory - the estimation\nof the mutual information (MI) between two random variables. We show that by\nusing the theory of diffusion bridges, one can construct an unbiased estimator\nfor data posing difficulties for conventional MI estimators. We showcase the\nperformance of our estimator on a series of standard MI estimation benchmarks.",
        "The increasing complexity of space systems, coupled with their critical\noperational roles, demands a robust, scalable, and sustainable security\nframework. This paper presents a novel system-of-systems approach for the\nupcoming Lunar Gateway. We demonstrate the application of the\nsecure-by-component approach to the two earliest deployed systems in the\nGateway, emphasizing critical security controls both internally and for\nexternal communication and connections. Additionally, we present a phased\napproach for the integration of Canadarm3, addressing the unique security\nchallenges that arise from both inter-system interactions and the arm's\nautonomous capabilities."
      ]
    }
  },
  {
    "id":2411.05237,
    "research_type":"basic",
    "start_id":"b3",
    "start_title":"Interactive Teaching Algorithms for Inverse Reinforcement Learning",
    "start_abstract":"We study the problem of inverse reinforcement learning (IRL) with added twist that learner is assisted by a helpful teacher. More formally, we tackle following algorithmic question: How could teacher provide an informative sequence demonstrations to IRL speed up process? present interactive teaching framework where adaptively chooses next demonstration based on learner's current policy. In particular, design algorithms for two concrete settings: omniscient setting has full knowledge about dynamics and blackbox minimal knowledge. Then, sequential variant popular MCE-IRL prove convergence guarantees our algorithm in setting. Extensive experiments car driving simulator environment show progress can be speeded drastically as compared uninformative",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Factors Influencing Physicians' Clinical Decision-making at Upazila Health Complexes in Bangladesh"
      ],
      "abstract":[
        "Selecting the most appropriate treatment for each patient is key activity in patient-physician encounters and providing healthcare services. Achieving desirable clinical goals mostly depends on making right decision at time any setting. But little known about physicians' decision-making primary care setting Bangladesh. Therefore, this study explored factors that influence decisions prescribing medications, ordering pathologic tests, counseling patients, average length of visits a consultation session, referral patients to other physicians or hospitals by Upazila Health Complexes (UHCs) country. It also structure social networks their association with process.This was cross-sectional descriptive used data collected from 85 physicians. The respondents, who work UHCs Rajshahi Division, were selected purposively. analyzed statistics including frequency, percentage, one-way analysis variance, linear regression understand relationships among variables.The results reveal multiple visits, referring UHCs. Most prescribe drugs keeping mind purchasing capacity. Risk violence patients' relatives better management are two decisions. professional personal play an influential role process. found dedicate 16.17 minutes session. influenced various distance between residence workplace, level education, number colleagues whom they have regular contact can seek help.The yielded some novel insights complexity everyday tasks would be interest public health researchers policy makers."
      ],
      "categories":[
        "Healthcare"
      ]
    },
    "list":{
      "title":[
        "Cavity-enhanced solid-state nuclear spin gyroscope",
        "Strong-damping limit of quantum Brownian motion in a disordered\n  environment",
        "Non-Hermitian electron-positron annihilation under thermal effects",
        "PLMP -- Point-Line Minimal Problems for Projective SfM",
        "JWST ASPIRE: How Did Galaxies Complete Reionization? Evidence for Excess\n  IGM Transmission around ${\\rm [O\\,{\\scriptstyle III}]}$ Emitters during\n  Reionization",
        "The General Position Problem: A Survey",
        "Edgeworth Expansion for Semi-hard Triplet Loss",
        "Solution of Uncertain Multiobjective Optimization Problems by Using\n  Nonlinear Conjugate Gradient Method",
        "Relatively non-degenerate integrated decay estimates for massless Vlasov\n  fields on Schwarzschild spacetimes",
        "Mass Matrix Rules and the Flat Pattern of Quarks",
        "Imperfect detectors for adversarial tasks with applications to quantum\n  key distribution",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Enhancing Offline Model-Based RL via Active Model Selection: A Bayesian\n  Optimization Perspective",
        "Data-Driven Sequential Sampling for Tail Risk Mitigation",
        "Characterizing the Conformational States of G Protein Coupled Receptors\n  Generated with AlphaFold",
        "Counting Frobenius Pseudoprimes",
        "Discovery of a Highly Anisotropic Type-II Ferromagnetic Weyl State\n  Exhibiting a 3D Quantum Hall Effect",
        "On the Bogomolov-Positselski Conjecture",
        "On Robust Aggregation for Distributed Data",
        "Strong Long-Wave Infrared Optical Response in a Topological\n  Semiconductor with a Mexican Hat Band Structure",
        "Topologically protected edge states in time photonic crystals with\n  chiral symmetry",
        "Probing the ringdown perturbation in binary black hole coalescences with\n  an improved quasi-normal mode extraction algorithm",
        "The extended Dirichlet space and criticality theory for nonlinear\n  Dirichlet forms",
        "Accelerating Equity: Overcoming the Gender Gap in VC Funding",
        "Explicit Codes approaching Generalized Singleton Bound using Expanders",
        "Entropic bottlenecks to nematic ordering in an $RP^{2}$ apolar spin\n  model",
        "Random attraction in TASEP with time-varying hopping rates",
        "The $\\ell_{\\infty}$ Directed Spanning Forest",
        "Variational and nonvariational solutions for double phase variable\n  exponent problems"
      ],
      "abstract":[
        "Solid-state quantum sensors based on ensembles of nitrogen-vacancy (NV)\ncenters in diamond have emerged as powerful tools for precise sensing\napplications. Nuclear spin sensors are particularly well-suited for\napplications requiring long coherence times, such as inertial sensing, but\nremain underexplored due to control complexity and limited optical readout\nefficiency. In this work, we propose cooperative cavity quantum electrodynamic\n(cQED) coupling to achieve efficient nuclear spin readout. Unlike previous cQED\nmethods used to enhance electron spin readout, here we employ two-field\ninterference in the NV hyperfine subspace to directly probe the nuclear spin\ntransitions. We model the nuclear spin NV-cQED system (nNV-cQED) and observe\nseveral distinct regimes, including electromagnetically induced transparency,\nmasing without inversion, and oscillatory behavior. We then evaluate the\nnNV-cQED system as an inertial sensor, indicating a rotation sensitivity\nimproved by three orders of magnitude compared to previous solid-state spin\ndemonstrations. Furthermore, we show that the NV electron spin can be\nsimultaneously used as a comagnetometer, and the four crystallographic axes of\nNVs can be employed for vector resolution in a single nNV-cQED system. These\nresults showcase the applications of two-field interference using the nNV-cQED\nplatform, providing critical insights into the manipulation and control of\nquantum states in hybrid NV systems and unlocking new possibilities for\nhigh-performance quantum sensing.",
        "We consider a microscopic model of an inhomogeneous environment where an\narbitrary quantum system is locally coupled to a harmonic bath via a\nfinite-range interaction. We show that in the overdamped regime the position\ndistribution obeys a classical Kramers-Moyal equation that involves an infinite\nnumber of higher derivatives, implying that the finite bath correlation length\nleads to non-Gaussian Markovian noise. We analytically solve the equation for a\nharmonically bound particle and analyze its non-Gaussian diffusion as well as\nits steady-state properties.",
        "In this paper we examine the thermal effects into the $e^{+}e^{-}\\to\n\\ell^{+}\\ell^{-}$ scattering in a non-hermitian extension of QED. We compute\nthe thermal contributions to this scattering cross-section within the Thermo\nField Dynamics approach. In order to highlight the non-hermitian effects we\nhave considered some limits of interest: i) zero-temperature limit and\nhigh-energy limit and ii) high-temperature regime. Since this type of\nscattering possesses accurate experimental data for the cross-section (for muon\nand tau at the final state) it can be used to set stringent bounds upon the\nnon-hermitian parameters.",
        "We completely classify all minimal problems for Structure-from-Motion (SfM)\nwhere arrangements of points and lines are fully observed by multiple\nuncalibrated pinhole cameras. We find 291 minimal problems, 73 of which have\nunique solutions and can thus be solved linearly. Two of the linear problems\nallow an arbitrary number of views, while all other minimal problems have at\nmost 9 cameras. All minimal problems have at most 7 points and at most 12\nlines. We compute the number of solutions of each minimal problem, as this\ngives a measurement of the problem's intrinsic difficulty, and find that these\nnumber are relatively low (e.g., when comparing with minimal problems for\ncalibrated cameras). Finally, by exploring stabilizer subgroups of\nsubarrangements, we develop a geometric and systematic way to 1) factorize\nminimal problems into smaller problems, 2) identify minimal problems in\nunderconstrained problems, and 3) formally prove non-minimality.",
        "The spatial correlation between galaxies and the Ly$\\alpha$ forest provides\ninsights into how galaxies reionized the Universe. Here, we present initial\nresults on the spatial cross-correlation between [OIII] emitters and Ly$\\alpha$\nforest at 5.4<z<6.5 from the JWST ASPIRE NIRCam\/F356W Grism Spectroscopic\nSurvey in z>6.5 QSO fields. Using data from five QSO fields, we find $2\\sigma$\nevidence for excess Ly$\\alpha$ forest transmission at ~20-40 cMpc around [OIII]\nemitters at z=5.86, indicating that [OIII] emitters reside within a highly\nionized IGM. At smaller scales, the Ly$\\alpha$ forest is preferentially\nabsorbed, suggesting gas overdensities around [OIII] emitters. Comparing with\nmodels including THESAN simulations, we interpret the observed\ncross-correlation as evidence for significant large-scale fluctuations of the\nIGM and the late end of reionization at z<6, characterized by ionized bubbles\nover 50 cMpc around [OIII] emitters. The required UV background necessitates an\nunseen population of faint galaxies around the [OIII] emitters. Furthermore, we\nfind that the number of observed [OIII] emitters near individual transmission\nspikes is insufficient to sustain reionization in their surroundings, even\nassuming all [OIII] emitters harbour AGN with 100 % LyC escape fractions.\nDespite broad agreement, a careful analysis of ASPIRE and THESAN, using the\nobserved host halo mass from the clustering of [OIII] emitters, suggests that\nthe simulations underpredict the observed excess IGM transmission around [OIII]\nemitters, challenging our model of reionization. Potential solutions include\nlarger ionized bubbles at z<6, more enhanced large-scale UV background or\ntemperature fluctuations of the IGM, and possibly a patchy early onset of\nreionization at z>10. Current observational errors are dominated by cosmic\nvariance, meaning future analyses of more QSO fields from JWST will improve the\nresults.",
        "Inspired by a chessboard puzzle of Dudeney, the general position problem in\ngraph theory asks for the largest sets $S$ of vertices in a graph such that no\nthree elements of $S$ lie on a common shortest path. The number of vertices in\nsuch a largest set is the general position number of the graph. This paper\nprovides a survey of this rapidly growing problem, which now has an extensive\nliterature. We cover exact results for various graph classes and the behaviour\nof the general position number under various graph products and operations. We\nalso discuss interesting variations of the general position problem, for\nexample variants corresponding to different graph convexities, as well as\ndynamic, fractional, colouring and game versions of the problem.",
        "We develop a higher-order asymptotic analysis for the semi-hard triplet loss\nusing the Edgeworth expansion. It is known that this loss function enforces\nthat embeddings of similar samples are close while those of dissimilar samples\nare separated by a specified margin. By refining the classical central limit\ntheorem, our approach quantifies the impact of the margin parameter and the\nskewness of the underlying data distribution on the loss behavior. In\nparticular, we derive explicit Edgeworth expansions that reveal first-order\ncorrections in terms of the third cumulant, thereby characterizing non-Gaussian\neffects present in the distribution of distance differences between\nanchor-positive and anchor-negative pairs. Our findings provide detailed\ninsight into the sensitivity of the semi-hard triplet loss to its parameters\nand offer guidance for choosing the margin to ensure training stability.",
        "This paper introduces a nonlinear conjugate gradient method (NCGM) for\naddressing the robust counterpart of uncertain multiobjective optimization\nproblems (UMOPs). Here, the robust counterpart is defined as the minimum across\nobjective-wise worst-case scenarios. There are some drawbacks to using\nscalarization techniques to solve the robust counterparts of UMOPs, such as the\npre-specification and restrictions of weights, and function importance that is\nunknown beforehand. NCGM is free from any kind of priori chosen scalars or\nordering information of objective functions as accepted in scalarization\nmethods. With the help of NCGM, we determine the critical point for the robust\ncounterpart of UMOP, which is the robust critical point for UMOP. To tackle\nthis robust counterpart using the NCGM, the approach involves constructing and\nsolving a subproblem to determine a descent direction. Subsequently, a new\ndirection is derived based on parameter selection methods such as\nFletcher-Reeves, conjugate descent, Dai-Yuan, Polak-Ribi$\\grave{e}$re-Polyak,\nand Hestenes-Stiefel. An Armijo-type inexact line search is employed to\nidentify an appropriate step length. Utilizing descent direction and step\nlength, a sequence is generated, and convergence of the proposed method is\nestablished. The effectiveness of the proposed method is verified and compared\nagainst an existing method using a set of test problems.",
        "In this article, we make use of a weight function capturing the concentration\nphenomenon of unstable future-trapped causal geodesics. A projection $V_+$, on\nthe tangent space of the null-shell, of the associated symplectic gradient\nturns out to enjoy good commutation properties with the massless Vlasov\noperator. This reflects that $V_+f$ decays exponentially locally near the\nphoton sphere, for any smooth solution $f$ to the massless Vlasov equation.\n  By identifying a well-chosen modification of $V_+$, we are able to construct\na $W_{x,p}^{1,1}$ weighted norm for which any smooth solution to the massless\nVlasov equation verifies an integrated local energy decay estimate without\nrelative degeneration. Together with the $r^p$-weighted energy method of\nDafermos--Rodnianski, we establish time decay for the energy norm. This norm\nallows for the control of the energy-momentum tensor $\\mathrm{T}[f]$ as well as\nall its first order derivatives.\n  The method developed in this paper is in particular compatible with\napproaches recently developed for the study of quasi-linear wave equations on\nblack hole spacetimes.",
        "Seeking mass patterns is a key to decoding the unknown flavor puzzles in\nparticle physics. Inspired by quark hierarchical masses, the mass matrix can\nuniversally be factorized into a family-diagonal phase matrix $K_L^q$ and a\nreal symmetric matrix $M_N^q$ characterized by only two parameters. The\nfactorized structure provides model-independent rules to the mass matrix. We\ndemonstrate that the large $\\delta_{CP}$ naturally arises from the degeneracy\nof the first two quark families in the mass hierarchy limit. As an application,\nthe flat pattern with elements close to unity in the matrix is checked by\nfitting quark masses and the CKM mixing. It achieves a precise description of\nflavor structure with minimal parameters.",
        "Security analyses in quantum key distribution (QKD) and other adversarial\nquantum tasks often assume perfect device models. However, real-world\nimplementations often deviate from these models. Thus, it is important to\ndevelop security proofs that account for such deviations from ideality. In this\nwork, we develop a general framework for analysing imperfect threshold\ndetectors, treating uncharacterised device parameters such as dark counts and\ndetection efficiencies as adversarially controlled within some ranges. This\napproach enables a rigorous worst-case analysis, ensuring security proofs\nremain valid under realistic conditions. Our results strengthen the connection\nbetween theoretical security and practical implementations by introducing a\nflexible framework for integrating detector imperfections into adversarial\nquantum protocols.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "Offline model-based reinforcement learning (MBRL) serves as a competitive\nframework that can learn well-performing policies solely from pre-collected\ndata with the help of learned dynamics models. To fully unleash the power of\noffline MBRL, model selection plays a pivotal role in determining the dynamics\nmodel utilized for downstream policy learning. However, offline MBRL\nconventionally relies on validation or off-policy evaluation, which are rather\ninaccurate due to the inherent distribution shift in offline RL. To tackle\nthis, we propose BOMS, an active model selection framework that enhances model\nselection in offline MBRL with only a small online interaction budget, through\nthe lens of Bayesian optimization (BO). Specifically, we recast model selection\nas BO and enable probabilistic inference in BOMS by proposing a novel\nmodel-induced kernel, which is theoretically grounded and computationally\nefficient. Through extensive experiments, we show that BOMS improves over the\nbaseline methods with a small amount of online interaction comparable to only\n$1\\%$-$2.5\\%$ of offline training data on various RL tasks.",
        "Given a finite collection of stochastic alternatives, we study the problem of\nsequentially allocating a fixed sampling budget to identify the optimal\nalternative with a high probability, where the optimal alternative is defined\nas the one with the smallest value of extreme tail risk. We particularly\nconsider a situation where these alternatives generate heavy-tailed losses\nwhose probability distributions are unknown and may not admit any specific\nparametric representation. In this setup, we propose data-driven sequential\nsampling policies that maximize the rate at which the likelihood of falsely\nselecting suboptimal alternatives decays to zero. We rigorously demonstrate the\nsuperiority of the proposed methods over existing approaches, which is further\nvalidated via numerical studies.",
        "G-Protein Coupled Receptors (GPCRs) are integral to numerous physiological\nprocesses and are the target of approximately one-third of FDA-approved\ntherapeutics. Despite their significance, only a limited subset of GPCRs has\nbeen successfully targeted, primarily due to challenges in accurately modeling\ntheir structures. AlphaFold, a state-of-the-art deep learning model, has\ndemonstrated remarkable capability in predicting protein structures with high\naccuracy. This study conducts an evaluation of AlphaFold performance in\npredicting GPCR structures and their conformational states by comparing its\npredictions to experimentally determined structures using metrics such as\naverage deformation between alpha carbon atoms and the Helix 3 - Helix 6\n(H3-H6) distance. Our analysis reveals that both AlphaFold 2 (AF2) and\nAlphaFold 3 (AF3) produce more accurate predictions for GPCRs in inactive\nconformations, with lower activity levels correlating with smaller\ndeformations. Conversely, higher activity levels are associated with increased\nvariability in AlphaFold performance due to difficulties with accurately\npredicting conformational changes upon GPCR activation and ligand binding.\nAdditionally, AlphaFold performance varies across different GPCR classes,\ninfluenced by the availability and quality of training data as well as the\nstructural complexity and diversity of the receptors. These findings\ndemonstrate the potential of AlphaFold in advancing drug discovery efforts,\nwhile also highlighting the necessity for continued refinement to enhance\npredictive accuracy for active conformations.",
        "We generalize the work of Erdos-Pomerance and Fiori-Shallue on counting\nFrobenius pseudoprimes from the cases of degree one and two respectively to\narbitrary degree. More specifically we provide formulas for counting the number\nof false witnesses for a number $n$ with respect to Grantham's Frobenius\nprimality test. We also provide conditional assymptotic lower bounds on the\naverage number of Frobenius pseudoprimes and assymptotic upper bounds on the\nsame.",
        "Topological semimetals, particularly Weyl semimetals (WSMs), are crucial\nplatforms for exploring emergent quantum phenomena due to their unique\nelectronic structures and potential to transition into various topological\nphases. In this study, we report the discovery of a ferromagnetic (FM) type-II\nWSM in Mn(Bi1-xSbx)4Te7, which exhibits a remarkable three-dimensional (3D)\nquantum Hall effect (QHE). By precisely tuning the chemical potential through\nSb doping, we obtained samples with the Fermi level near the charge neutrality\npoint for x = ~ 0.27. This was confirmed by spectroscopy measurements (ARPES\nand STS), and these samples showed strong quantum oscillations along with a key\ntransport signature of a Weyl state - chiral anomaly, and Fermi surface\nreconstruction driven by FM ordering. Our theoretical analysis indicates that\nthis Weyl state evolves from a parent nodal ring state, where higher-order\nk-terms split the nodal line into type-II Weyl nodes. The Weyl state exhibits\nsignificant anisotropy, characterized by a pronounced reduction in Fermi\nvelocity along the kz-axis, likely accounting for the observed 3D QHE. These\nresults not only highlight the exceptional tunability of the Mn(Bi1-xSbx)4Te7\nsystem, where precise control of the chemical potential and magnetic properties\nopens access to novel quantum phases, but also advance the understanding of FM\nWSMs.",
        "Let $p$ be a prime, we say that a Kummerian oriented pro-$p$ group\n$(G,\\theta)$ has the Bogomolov-Positselski property if $I_\\theta(G)$ is a free\npro-$p$ group. We give a new criterion for an oriented pro-$p$ group to have\nthe Bogomolov-Positselski property based on previous work by Positselski\n(arXiv:1405.0965) and Quadrelli and Weigel (arXiv:2103.12438) linking their\nseemingly unrelated approaches and thereby answering a question posed by\nQuadrelli and Weigel.\n  Under further assumptions, we derive two additional criteria. The first of\nwhich strongly resembles an analogue of the Merkujev-Suslin theorem. The second\nallows to relax the conditions given by Positselski in Theorem 2 of\narXiv:1405.0965. In addition, we show how to make those weaker assumptions\ncomputationally effective in some special cases.",
        "When data are stored across multiple locations, directly pooling all the data\ntogether for statistical analysis may be impossible due to communication costs\nand privacy concerns. Distributed computing systems allow the analysis of such\ndata, by getting local servers to separately process their own statistical\nanalyses and using a central processor to aggregate the local statistical\nresults. Naive aggregation of local statistics using simple or weighted\naverages, is vulnerable to contamination within a distributed computing system.\nThis paper develops and investigates a Huber-type aggregation method for\nlocally computed M-estimators to handle contamination in the local estimates.\nOur implementation of this aggregation method requires estimating the\nasymptotic variance-covariance matrix of the M-estimator, which we accomplish\nusing a robust spatial median approach. Theoretically, the Huber-type\naggregation achieves the same convergence rate as if all the data were pooled.\nWe establish its asymptotic normality for making inferences, including\njustifying a two-step approach for detecting contamination in the distributed\ncomputing system. Extensive simulation studies are conducted to validate the\ntheoretical results and the usefulness of our proposed approach is demonstrated\non U.S. airline data.",
        "Light sources and photodetectors operating in the far- to mid-infrared\n(FIR\/MIR) band ($8$-$12~\\rm \\mu m$, $0.1$-$0.15~\\rm eV$) remain relatively\npoorly developed compared to their counterparts operating in the visible and\nnear-infrared ranges, despite extensive application potential for thermal\nimaging, standoff sensing, and other technologies. This is attributable in part\nto the lack of narrow-gap materials ($<0.1~\\rm eV$) with high optical gain and\nabsorption. In this work, a narrow-gap semiconductor, $\\rm Pb_{0.7}Sn_{0.3}Se$,\nis demonstrated to exhibit an optical response $>10\\times$ larger than that of\n$\\rm Hg_{x}Cd_{1-x}Te$ (MCT), the dominant material for FIR\/MIR photodetectors.\nA previous theoretical investigation indicated that chalcogen $p$ and metal $d$\nband inversion in this material creates a Mexican hat band structure (MHBS),\nwhich results in a dramatic increase in the joint density of states at the\noptical transition edge compared to typical semiconductors. This prediction is\nexperimentally validated here using single-crystal specimens of $\\rm\nPb_{0.7}Sn_{0.3}Se$ measured using temperature-dependent spectroscopic\nellipsometry over a wavelength range of $1.7$-$20~\\rm \\mu m$ ($0.73$-$0.062~\\rm\neV$). These measurements demonstrate a large enhancement in extinction\ncoefficient and refractive index characteristic of a MHBS in the vicinity of\nthe absorption edge, in agreement with theoretical predictions. The realization\nof topological semiconductors with a MHBS is expected to lead to\nhigh-efficiency detectors operating in the FIR\/MID range.",
        "Time photonic crystals are media in which their electromagnetic parameters\nare modulated periodically in time, showing promising applications in\nnon-resonant lasers and particle accelerators, among others. Traditionally\nutilized to study space photonic crystals, topological band theory has also\nbeen translated recently to analyze time photonic crystals with time inversion\nsymmetry, enabling the construction of the temporal version of topological edge\nstates. However, temporal disorder can readily break time inversion symmetry in\npractice, hence likely destroying the edge states associated with this type of\ntime photonic crystals. To overcome this limitation, here we propose a new\nclass of time photonic crystals presenting chiral symmetry instead, whose edge\nstates exhibit superior robustness over the time-reversal-symmetry-protected\ncounterparts. Our time photonic crystal is equivalent to a temporal version of\nthe Su-Schrieffer-Heeger model, and the chiral symmetry of this type of time\nphotonic crystals quantizes the winding number defined in the Bloch frequency\nband. Remarkably, random temporal disorders do not impact the eigenfrequencies\nof these chiral-symmetry-protected edge states, while instead enhancing their\ntemporal localizations. Our findings thus provide a promising paradigm to\ncontrol field amplification with exceptional robustness as well as being a\nfeasible platform to investigate various topological phases in time-varying\nmedia.",
        "Using gravitational waves to probe the geometry of the ringing remnant black\nhole formed in a binary black hole coalescence is a well-established way to\ntest Einstein's theory of general relativity. However, doing so requires\nknowledge of when the predictions of black hole perturbation theory, i.e.,\nquasi-normal modes (QNMs), are a valid description of the emitted gravitational\nwave as well as what the amplitudes of these excitations are. In this work, we\ndevelop an algorithm to systematically extract QNMs from the ringdown of black\nhole merger simulations. Our algorithm improves upon previous ones in three\nways: it fits over the two-sphere, enabling a complete model of the strain; it\nperforms a reverse-search in time for QNMs using a more robust nonlinear least\nsquares routine called \\texttt{VarPro}; and it checks the variance of QNM\namplitudes, which we refer to as ``stability'', over an interval matching the\nnatural time scale of each QNM. Using this algorithm, we not only demonstrate\nthe stability of a multitude of QNMs and their overtones across the parameter\nspace of quasi-circular, non-precessing binary black holes, but we also\nidentify new quadratic QNMs that may be detectable in the near future using\nground-based interferometers. Furthermore, we provide evidence which suggests\nthat the source of remnant black hole perturbations is roughly independent of\nthe overtone index in a given angular harmonic across binary parameter space,\nat least for overtones with $n\\leq2$. This finding may hint at the\nspatiotemporal structure of ringdown perturbations in black hole coalescences,\nas well as the regime of validity of perturbation theory in the ringdown of\nthese events. Our algorithm is made publicly available at the following GitHub\nrepository: https:\/\/github.com\/keefemitman\/qnmfinder.",
        "In this paper we establish the existence of the extended Dirichlet space for\nnonlinear Dirichlet forms under mild conditions. We employ it to introduce and\ncharacterize criticality (recurrence) and subcriticality (transience) and\nestablish basics of a potential theory.",
        "We examine the growing gender gap in venture capital funding, focusing on\naccelerator programs in the U.S. We collect a unique dataset with detailed\ninformation on accelerators and startups. Using a two-stage methodology, we\nfirst estimate a matching model between startups and accelerators, and then use\nits output to analyze the gender gap in post-graduation outcomes through a\ncontrol function approach. Our results show that female-founded startups face a\nsignificant funding disadvantage, primarily due to relocation challenges tied\nto family obligations. However, larger cohorts and higher-quality accelerators\nhelp reduce this gap by offering female founders better networking\nopportunities and mentorship.",
        "We construct a new family of explicit codes that are list decodable to\ncapacity and achieve an optimal list size of $O(\\frac{1}{\\epsilon})$. In\ncontrast to existing explicit constructions of codes achieving list decoding\ncapacity, our arguments do not rely on algebraic structure but utilize simple\ncombinatorial properties of expander graphs.\n  Our construction is based on a celebrated distance amplification procedure\ndue to Alon, Edmonds, and Luby [FOCS'95], which transforms any high-rate code\ninto one with near-optimal rate-distance tradeoff. We generalize it to show\nthat the same procedure can be used to transform any high-rate code into one\nthat achieves list decoding capacity. Our proof can be interpreted as a\n\"local-to-global\" phenomenon for (a slight strengthening of) the generalized\nSingleton bound. Using this construction, for every $R, \\epsilon \\in (0,1)$ and\n$k \\in \\mathbb{N}^+$, we obtain an \\emph{explicit} family of codes $\\mathcal{C}\n\\subseteq \\Sigma^n$, with rate $R$ such that,\n  - They achieve the $\\epsilon$-relaxed generalized Singleton bound: for any $g\n\\in \\Sigma^n$ and any list $\\mathcal{H}$ of at most $k$ codewords, we have, \\[\n\\underset{h \\in \\mathcal{H}}{\\mathbb{E}} [\\Delta(g,h)] ~\\geq~\n\\frac{|\\mathcal{H}|-1}{|\\mathcal{H}|} \\cdot (1 - R - \\epsilon). \\]\n  - The alphabet size is a constant depending only on $\\epsilon$ and $k$.\n  - They can be list decoded up to radius $\\frac{k-1}{k}(1-R-\\epsilon)$, in\ntime $n^{O_{k,\\epsilon}(1)}$.\n  As a corollary of our result, we also obtain the first explicit construction\nof LDPC codes achieving list decoding capacity, and in fact arbitrarily close\nto the generalized Singleton bound.",
        "The Lebwohl-Lasher model of liquid crystals with (d = 2, n = 3) describes\ninteracting apolar spins, with an $RP^{2}$ order-parameter topology.\nSimulations with a modified Wang-Landau Monte Carlo protocol, that includes a\ndensity of states (DoS) factor, had previously found a zero latent-heat\ntransition at $T=T_{n}$ to a novel nematic order, coexisting with unbound\ndefects whose binding is completed only on cooling. We find through this\nentropically augmented MC protocol, that there is a deep dip in the DoS at an\nenergy preceding global ordering, reflecting 'sparse' intermediate\nconfigurations, or entropy barriers. The narrow entropic bottleneck induces a\ncusp in the initially rising nematic correlation length, at a micro-canonical\n'precursor' temperature $T=T_{p}$. A finite-scale cooperativity of defects and\nnematic clusters penetrates the bottleneck at $T_{p}$ to enable a third-order\nphase transition at a lower $T_{n}$: a rare pathway, overlooked by energy-only\nacceptance protocols.",
        "The totally asymmetric simple exclusion principle (TASEP) is a fundamental\nmodel in nonequilibrium statistical mechanics. It describes the stochastic\nunidirectional movement of particles along a 1D chain of ordered sites. We\nconsider the continuous-time version of TASEP with a finite number of sites and\nwith time-varying hopping rates between the sites. We show how to formulate\nthis model as a nonautonomous random dynamical system (NRDS) with a finite\nstate-space. We provide conditions guaranteeing that random pullback and\nforward attractors of such an NRDS exist and consist of singletons. In the\ncontext of the nonautonomous TASEP these conditions imply almost sure\nsynchronization of the individual random paths. This implies in particular that\nperturbations that change the state of the particles along the chain are\n\"filtered out\" in the long run. We demonstrate that the required conditions are\ntight by providing examples where these conditions do not hold and consequently\nthe forward attractor does not exist or the pullback attractor is not a\nsingleton. The results in this paper generalize our earlier results for\nautonomous TASEP in https:\/\/doi.org\/10.1137\/20M131446X and contain these as a\nspecial case.",
        "We study the $\\ell_{\\infty}$\\textit{ directed spanning forest}(DSF), which is\na directed forest with vertex set given by a homogeneous Poisson point process\nsuch that each Poisson point connects to the nearest Poisson point (in\n$\\ell_{\\infty}$ distance) with a strictly larger $y$-coordinate. In this paper,\nwe prove that the $\\ell_{\\infty}$ DSF is connected and we find optimal\nestimates on the tail distribution of coalescing time of two $\\ell_{\\infty}$\nDSF paths. Similar estimates were earlier obtained in \\cite{coupier20212d} for\nthe $\\ell_2$ (Euclidean) DSF and showed that when properly scaled, it converges\nin distribution to the Brownian web. The geometry of $\\ell_\\infty$ balls compel\nus to develop new argument.",
        "In this article, we examine two double-phase variable exponent problems, each\nformulated within a distinct framework. The first problem is non-variational,\nas the nonlinear term may depend on the gradient of the solution. The first\nmain result establishes an existence property from the nonlinear monotone\noperator theory given by Browder and Minty. The second problem is set up within\na variational framework, where we employ a well-known critical point result by\nBonanno and Chinn\\`{\\i}. In both cases, we demonstrate the existence of at\nleast one nontrivial solution. To illustrate the practical application of the\nmain results, we provide examples for each problem."
      ]
    }
  },
  {
    "id":2412.09927,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Three-dimensional nanomagnetism",
    "start_abstract":"Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities.",
    "start_categories":[
      "cond-mat.mes-hall"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "MagNet: machine learning enhanced three-dimensional magnetic reconstruction"
      ],
      "abstract":[
        "Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "The Factorizable Feigin-Frenkel center",
        "A BERT Based Hybrid Recommendation System For Academic Collaboration",
        "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding\n  and Expert Reasoning Abilities",
        "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and\n  Attacks",
        "Quantum Feature-Empowered Deep Classification for Fast Mangrove Mapping",
        "Antenna Position and Beamforming Optimization for Movable Antenna\n  Enabled ISAC: Optimal Solutions and Efficient Algorithms",
        "Do Unit Proofs Work? An Empirical Study of Compositional Bounded Model\n  Checking for Memory Safety Verification",
        "A New Statistical Approach to the Performance Analysis of Vision-based\n  Localization",
        "PAID: A Framework of Product-Centric Advertising Image Design",
        "How much should we care about what others know? Jump signals in optimal\n  investment under relative performance concerns",
        "Optimal PMU Placement for Kalman Filtering of DAE Power System Models",
        "CubeDiff: Repurposing Diffusion-Based Image Models for Panorama\n  Generation",
        "Bias Analysis of Experiments for Multi-Item Multi-Period Inventory\n  Control Policies",
        "Robust Phantom-Assisted Framework for Multi-Person Localization and\n  Vital Signs Monitoring Using MIMO FMCW Radar",
        "On the existence of twisted Shalika periods: the Archimedean case",
        "On reflected isotropic stable processes",
        "A Comprehensive Survey on Long Context Language Modeling",
        "Design and Implementation of a Dual Uncrewed Surface Vessel Platform for\n  Bathymetry Research under High-flow Conditions",
        "AI Load Dynamics--A Power Electronics Perspective",
        "Looking into the Future of Health-Care Services: Can Life-Like Agents\n  Change the Future of Health-Care Services?",
        "Enhancing Vision-Language Compositional Understanding with Multimodal\n  Synthetic Data",
        "Bounded conciseness in the space of marked groups",
        "CRDT-Based Game State Synchronization in Peer-to-Peer VR",
        "Exercises on the Kepler ellipses through a fixed point in space, after\n  Otto Laporte",
        "Simplifying Formal Proof-Generating Models with ChatGPT and Basic\n  Searching Techniques",
        "FairDiverse: A Comprehensive Toolkit for Fair and Diverse Information\n  Retrieval Algorithms",
        "Minimum Time Strategies for a Differential Drive Robot Escaping from a\n  Circular Detection Region",
        "Intelligent Reflecting Surface-Aided Electromagnetic Stealth over\n  Extended Regions",
        "Continuity of Hausdorff Dimension at Hopf Bifurcation"
      ],
      "abstract":[
        "We prove a factorizable version of the Feigin-Frenkel theorem on the center\nof the completed enveloping algebra of the affine Kac-Moody algebra attached to\na simple Lie algebra at the critical level. On any smooth curve C we consider a\nsheaf of complete topological Lie algebras whose fiber at any point is the\nusual affine algebra at the critical level and consider its sheaf of completed\nenveloping algebras. We show that the center of this sheaf is a factorization\nalgebra and establish that it is canonically isomorphic, in a factorizable\nmanner, with the factorization algebra of functions on Opers on the pointed\ndisk.",
        "Universities serve as a hub for academic collaboration, promoting the\nexchange of diverse ideas and perspectives among students and faculty through\ninterdisciplinary dialogue. However, as universities expand in size,\nconventional networking approaches via student chapters, class groups, and\nfaculty committees become cumbersome. To address this challenge, an\nacademia-specific profile recommendation system is proposed to connect\nlike-minded stakeholders within any university community. This study evaluates\nthree techniques: Term Frequency-Inverse Document Frequency (TF-IDF),\nBidirectional Encoder Representations from Transformers (BERT), and a hybrid\napproach to generate effective recommendations. Due to the unlabelled nature of\nthe dataset, Affinity Propagation cluster-based relabelling is performed to\nunderstand the grouping of similar profiles. The hybrid model demonstrated\nsuperior performance, evidenced by its similarity score, Silhouette score,\nDavies-Bouldin index, and Normalized Discounted Cumulative Gain (NDCG),\nachieving an optimal balance between diversity and relevance in\nrecommendations. Furthermore, the optimal model has been implemented as a\nmobile application, which dynamically suggests relevant profiles based on\nusers' skills and collaboration interests, incorporating contextual\nunderstanding. The potential impact of this application is significant, as it\npromises to enhance networking opportunities within large academic institutions\nthrough the deployment of intelligent recommendation systems.",
        "Understanding and reasoning over non-speech sounds and music are crucial for\nboth humans and AI agents to interact effectively with their environments. In\nthis paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM)\nwith advanced audio understanding and reasoning capabilities. AF2 leverages (i)\na custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio\nreasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves\nstate-of-the-art performance with only a 3B parameter small language model,\nsurpassing large open-source and proprietary models across over 20 benchmarks.\nNext, for the first time, we extend audio understanding to long audio segments\n(30 secs to 5 mins) and propose LongAudio, a large and novel dataset for\ntraining ALMs on long audio captioning and question-answering tasks.\nFine-tuning AF2 on LongAudio leads to exceptional performance on our proposed\nLongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio\nunderstanding capabilities. We conduct extensive ablation studies to confirm\nthe efficacy of our approach. Project Website:\nhttps:\/\/research.nvidia.com\/labs\/adlr\/AF2\/.",
        "Embodied AI systems, including robots and autonomous vehicles, are\nincreasingly integrated into real-world applications, where they encounter a\nrange of vulnerabilities stemming from both environmental and system-level\nfactors. These vulnerabilities manifest through sensor spoofing, adversarial\nattacks, and failures in task and motion planning, posing significant\nchallenges to robustness and safety. Despite the growing body of research,\nexisting reviews rarely focus specifically on the unique safety and security\nchallenges of embodied AI systems. Most prior work either addresses general AI\nvulnerabilities or focuses on isolated aspects, lacking a dedicated and unified\nframework tailored to embodied AI. This survey fills this critical gap by: (1)\ncategorizing vulnerabilities specific to embodied AI into exogenous (e.g.,\nphysical attacks, cybersecurity threats) and endogenous (e.g., sensor failures,\nsoftware flaws) origins; (2) systematically analyzing adversarial attack\nparadigms unique to embodied AI, with a focus on their impact on perception,\ndecision-making, and embodied interaction; (3) investigating attack vectors\ntargeting large vision-language models (LVLMs) and large language models (LLMs)\nwithin embodied systems, such as jailbreak attacks and instruction\nmisinterpretation; (4) evaluating robustness challenges in algorithms for\nembodied perception, decision-making, and task planning; and (5) proposing\ntargeted strategies to enhance the safety and reliability of embodied AI\nsystems. By integrating these dimensions, we provide a comprehensive framework\nfor understanding the interplay between vulnerabilities and safety in embodied\nAI.",
        "A mangrove mapping (MM) algorithm is an essential classification tool for\nenvironmental monitoring. The recent literature shows that compared with other\nindex-based MM methods that treat pixels as spatially independent,\nconvolutional neural networks (CNNs) are crucial for leveraging spatial\ncontinuity information, leading to improved classification performance. In this\nwork, we go a step further to show that quantum features provide radically new\ninformation for CNN to further upgrade the classification results. Simply\nspeaking, CNN computes affine-mapping features, while quantum neural network\n(QNN) offers unitary-computing features, thereby offering a fresh perspective\nin the final decision-making (classification). To address the challenging MM\nproblem, we design an entangled spatial-spectral quantum feature extraction\nmodule. Notably, to ensure that the quantum features contribute genuinely novel\ninformation (unaffected by traditional CNN features), we design a separate\nnetwork track consisting solely of quantum neurons with built-in\ninterpretability. The extracted pure quantum information is then fused with\ntraditional feature information to jointly make the final decision. The\nproposed quantum-empowered deep network (QEDNet) is very lightweight, so the\nimprovement does come from the cooperation between CNN and QNN (rather than\nparameter augmentation). Extensive experiments will be conducted to demonstrate\nthe superiority of QEDNet.",
        "In this paper, we propose an integrated sensing and communication (ISAC)\nsystem enabled by movable antennas (MAs), which can dynamically adjust antenna\npositions to enhance both sensing and communication performance for future\nwireless networks. To characterize the benefits of MA-enabled ISAC systems, we\nfirst derive the Cram\\'er-Rao bound (CRB) for angle estimation error, which is\nthen minimized for optimizing the antenna position vector (APV) and beamforming\ndesign, subject to a pre-defined signal-to-noise ratio (SNR) constraint to\nensure the communication performance. In particular, for the case with receive\nMAs only, we provide a closed-form optimal antenna position solution, and show\nthat employing MAs over conventional fixed-position antennas (FPAs) can achieve\na sensing performance gain upper-bounded by 4.77 dB. On the other hand, for the\ncase with transmit MAs only, we develop a boundary traversal breadth-first\nsearch (BT-BFS) algorithm to obtain the global optimal solution in the\nline-of-sight (LoS) channel scenario, along with a lower-complexity boundary\ntraversal depth-first search (BT-DFS) algorithm to find a local optimal\nsolution efficiently. While in the scenario with non-LoS (NLoS) channels, a\nmajorization-minimization (MM) based Rosen's gradient projection (RGP)\nalgorithm with an efficient initialization method is proposed to obtain\nstationary solutions for the considered problem, which can be extended to the\ngeneral case with both transmit and receive MAs. Extensive numerical results\nare presented to verify the effectiveness of the proposed algorithms, and\ndemonstrate the superiority of the considered MA-enabled ISAC system over\nconventional ISAC systems with FPAs in terms of sensing and communication\nperformance trade-off.",
        "Memory safety defects pose a major threat to software reliability, enabling\ncyberattacks, outages, and crashes. To mitigate these risks, organizations\nadopt Compositional Bounded Model Checking (BMC), using unit proofs to formally\nverify memory safety. However, methods for creating unit proofs vary across\norganizations and are inconsistent within the same project, leading to errors\nand missed defects. In addition, unit proofing remains understudied, with no\nsystematic development methods or empirical evaluations.\n  This work presents the first empirical study on unit proofing for memory\nsafety verification. We introduce a systematic method for creating unit proofs\nthat leverages verification feedback and objective criteria. Using this\napproach, we develop 73 unit proofs for four embedded operating systems and\nevaluate their effectiveness, characteristics, cost, and generalizability. Our\nresults show unit proofs are cost-effective, detecting 74\\% of recreated\ndefects, with an additional 9\\% found with increased BMC bounds, and 19 new\ndefects exposed. We also found that embedded software requires small unit\nproofs, which can be developed in 87 minutes and executed in 61 minutes on\naverage. These findings provide practical guidance for engineers and empirical\ndata to inform tooling design.",
        "Many modern wireless devices with accurate positioning needs also have access\nto vision sensors, such as a camera, radar, and Light Detection and Ranging\n(LiDAR). In scenarios where wireless-based positioning is either inaccurate or\nunavailable, using information from vision sensors becomes highly desirable for\ndetermining the precise location of the wireless device. Specifically, vision\ndata can be used to estimate distances between the target (where the sensors\nare mounted) and nearby landmarks. However, a significant challenge in\npositioning using these measurements is the inability to uniquely identify\nwhich specific landmark is visible in the data. For instance, when the target\nis located close to a lamppost, it becomes challenging to precisely identify\nthe specific lamppost (among several in the region) that is near the target.\nThis work proposes a new framework for target localization using range\nmeasurements to multiple proximate landmarks. The geometric constraints\nintroduced by these measurements are utilized to narrow down candidate landmark\ncombinations corresponding to the range measurements and, consequently, the\ntarget's location on a map. By modeling landmarks as a marked Poisson point\nprocess (PPP), we show that three noise-free range measurements are sufficient\nto uniquely determine the correct combination of landmarks in a two-dimensional\nplane. For noisy measurements, we provide a mathematical characterization of\nthe probability of correctly identifying the observed landmark combination\nbased on a novel joint distribution of key random variables. Our results\ndemonstrate that the landmark combination can be identified using ranges, even\nwhen individual landmarks are visually indistinguishable.",
        "Creating visually appealing advertising images is often a labor-intensive and\ntime-consuming process. Is it possible to automatically generate such images\nusing only basic product information--specifically, a product foreground image,\ntaglines, and a target size? Existing methods mainly focus on parts of the\nproblem and fail to provide a comprehensive solution. To address this gap, we\npropose a novel multistage framework called Product-Centric Advertising Image\nDesign (PAID). It consists of four sequential stages to highlight product\nforegrounds and taglines while achieving overall image aesthetics: prompt\ngeneration, layout generation, background image generation, and graphics\nrendering. Different expert models are designed and trained for the first three\nstages: First, we use a visual language model (VLM) to generate background\nprompts that match the products. Next, a VLM-based layout generation model\narranges the placement of product foregrounds, graphic elements (taglines and\ndecorative underlays), and various nongraphic elements (objects from the\nbackground prompt). Following this, we train an SDXL-based image generation\nmodel that can simultaneously accept prompts, layouts, and foreground controls.\nTo support the PAID framework, we create corresponding datasets with over\n50,000 labeled images. Extensive experimental results and online A\/B tests\ndemonstrate that PAID can produce more visually appealing advertising images.",
        "We present a multi-agent and mean-field formulation of a game between\ninvestors who receive private signals informing their investment decisions and\nwho interact through relative performance concerns. A key tool in our model is\na Poisson random measure which drives jumps in both market prices and signal\nprocesses and thus captures common and idiosyncratic noise. Upon receiving a\njump signal, an investor evaluates not only the signal's implications for stock\nprice movements but also its implications for the signals received by her peers\nand for their subsequent investment decisions. A crucial aspect of this\nassessment is the distribution of investor types in the economy. These types\ndetermine their risk aversion, performance concerns, and the quality and\nquantity of their signals. We demonstrate how these factors are reflected in\nthe corresponding HJB equations, characterizing an agent's optimal response to\nher peers' signal-based strategies. The existence of equilibria in both the\nmulti-agent and mean-field game is established using Schauder's Fixed Point\nTheorem under suitable conditions on investor characteristics, particularly\ntheir signal processes. Finally, we present numerical case studies that\nillustrate these equilibria from a financial-economic perspective. This allows\nus to address questions such as how much investors should care about the\ninformation known by their peers.",
        "Optimal sensor placement is essential for minimizing costs and ensuring\naccurate state estimation in power systems. This paper introduces a novel\nmethod for optimal sensor placement for dynamic state estimation of power\nsystems modeled by differential-algebraic equations. The method identifies\noptimal sensor locations by minimizing the steady-state covariance matrix of\nthe Kalman filter, thus minimizing the error of joint differential and\nalgebraic state estimation. The problem is reformulated as a mixed-integer\nsemidefinite program and effectively solved using off-the-shelf numerical\nsolvers. Numerical results demonstrate the merits of the proposed approach by\nbenchmarking its performance in phasor measurement unit placement in comparison\nto greedy algorithms.",
        "We introduce a novel method for generating 360{\\deg} panoramas from text\nprompts or images. Our approach leverages recent advances in 3D generation by\nemploying multi-view diffusion models to jointly synthesize the six faces of a\ncubemap. Unlike previous methods that rely on processing equirectangular\nprojections or autoregressive generation, our method treats each face as a\nstandard perspective image, simplifying the generation process and enabling the\nuse of existing multi-view diffusion models. We demonstrate that these models\ncan be adapted to produce high-quality cubemaps without requiring\ncorrespondence-aware attention layers. Our model allows for fine-grained text\ncontrol, generates high resolution panorama images and generalizes well beyond\nits training set, whilst achieving state-of-the-art results, both qualitatively\nand quantitatively. Project page: https:\/\/cubediff.github.io\/",
        "Randomized experiments, or A\/B testing, are the gold standard for evaluating\ninterventions but are underutilized in the area of inventory management. This\nstudy addresses this gap by analyzing A\/B testing strategies in multi-item,\nmulti-period inventory systems with lost sales and capacity constraints. We\nexamine switchback experiments, item-level randomization, pairwise\nrandomization, and staggered rollouts, analyzing their biases theoretically and\ncomparing them through numerical experiments. Our findings provide actionable\nguidance for selecting experimental designs across various contexts in\ninventory management.",
        "With the rising prevalence of cardiovascular and respiratory disorders and an\naging global population, healthcare systems face increasing pressure to adopt\nefficient, non-contact vital sign monitoring (NCVSM) solutions. This study\nintroduces a robust framework for multi-person localization and vital signs\nmonitoring, using multiple-input-multiple-output frequency-modulated continuous\nwave radar, addressing challenges in real-world, cluttered environments. Two\nkey contributions are presented. First, a custom hardware phantom was developed\nto simulate multi-person NCVSM scenarios, utilizing recorded thoracic impedance\nsignals to replicate realistic cardiopulmonary dynamics. The phantom's design\nfacilitates repeatable and rapid validation of radar systems and algorithms\nunder diverse conditions to accelerate deployment in human monitoring. Second,\naided by the phantom, we designed a robust algorithm for multi-person\nlocalization utilizing joint sparsity and cardiopulmonary properties, alongside\nharmonics-resilient dictionary-based vital signs estimation, to mitigate\ninterfering respiration harmonics. Additionally, an adaptive signal refinement\nprocedure is introduced to enhance the accuracy of continuous NCVSM by\nleveraging the continuity of the estimates. Performance was validated and\ncompared to existing techniques through 12 phantom trials and 12 human trials,\nincluding both single- and multi-person scenarios, demonstrating superior\nlocalization and NCVSM performance. For example, in multi-person human trials,\nour method achieved average respiration rate estimation accuracies of 94.14%,\n98.12%, and 98.69% within error thresholds of 2, 3, and 4 breaths per minute,\nrespectively, and heart rate accuracies of 87.10%, 94.12%, and 95.54% within\nthe same thresholds. These results highlight the potential of this framework\nfor reliable multi-person NCVSM in healthcare and IoT applications.",
        "Let $\\K$ be an archimedean local field. We investigate the existence of the\ntwisted Shalika functionals on irreducible admissible smooth representations of\n$\\GL_{2n}(\\K)$ in terms of their L-parameters. As part of our proof, we\nestablish a Hochschild-Serre spectral sequence for nilpotent normal subgroups\nand a Kunneth formula in the framework of Schwartz homology. We also prove the\nanalogous result for twisted linear periods using theta correspondence. The\nexistence of twisted Shalika functionals on representations of\n$\\GL_{2n}^{+}(\\R)$ is also studied, which is of independent interest.",
        "We build two types of isotropic stable processes reflected in a strongly\nconvex bounded domain $\\mathcal{D}$. In both cases, when the process tries to\njump across the boundary, it is stopped at the unique point where\n$\\partial\\mathcal{D}$ intersects the line segment defined by the attempted\njump. It then leaves the boundary either continuously (for the first type) or\nby a power-law distributed jump (for the second type). The construction of\nthese processes is done via an It\\^o synthesis: we concatenate their excursions\nin the domain, which are obtained by translating, rotating and stopping the\nexcursions of some stable processes reflected in the half-space. The key\ningredient in this procedure is the construction of the boundary processes,\ni.e. the processes time-changed by their local time on the boundary, which\nsolve stochastic differential equations driven by some Poisson measures of\nexcursions. The well-posedness of these boundary processes relies on delicate\nestimates involving some geometric inequalities and the laws of the undershoot\nand overshoot of the excursion when it leaves the domain. After having\nconstructed the processes, we show that they are Markov and Feller, we study\ntheir infinitesimal generator and write down the reflected fractional heat\nequations satisfied by their time-marginals.",
        "Efficient processing of long contexts has been a persistent pursuit in\nNatural Language Processing. With the growing number of long documents,\ndialogues, and other textual data, it is important to develop Long Context\nLanguage Models (LCLMs) that can process and analyze extensive inputs in an\neffective and efficient way. In this paper, we present a comprehensive survey\non recent advances in long-context modeling for large language models. Our\nsurvey is structured around three key aspects: how to obtain effective and\nefficient LCLMs, how to train and deploy LCLMs efficiently, and how to evaluate\nand analyze LCLMs comprehensively. For the first aspect, we discuss data\nstrategies, architectural designs, and workflow approaches oriented with long\ncontext processing. For the second aspect, we provide a detailed examination of\nthe infrastructure required for LCLM training and inference. For the third\naspect, we present evaluation paradigms for long-context comprehension and\nlong-form generation, as well as behavioral analysis and mechanism\ninterpretability of LCLMs. Beyond these three key aspects, we thoroughly\nexplore the diverse application scenarios where existing LCLMs have been\ndeployed and outline promising future development directions. This survey\nprovides an up-to-date review of the literature on long-context LLMs, which we\nwish to serve as a valuable resource for both researchers and engineers. An\nassociated GitHub repository collecting the latest papers and repos is\navailable at:\n\\href{https:\/\/github.com\/LCLM-Horizon\/A-Comprehensive-Survey-For-Long-Context-Language-Modeling}{\\color[RGB]{175,36,67}{LCLM-Horizon}}.",
        "Bathymetry, the study of underwater topography, relies on sonar mapping of\nsubmerged structures. These measurements, critical for infrastructure health\nmonitoring, often require expensive instrumentation. The high financial risk\nassociated with sensor damage or vessel loss creates a reluctance to deploy\nuncrewed surface vessels (USVs) for bathymetry. However, the crewed-boat\nbathymetry operations, are costly, pose hazards to personnel, and frequently\nfail to achieve the stable conditions necessary for bathymetry data collection,\nespecially under high currents. Further research is essential to advance\nautonomous control, navigation, and data processing technologies, with a\nparticular focus on bathymetry. There is a notable lack of accessible hardware\nplatforms that allow for integrated research in both bathymetry-focused\nautonomous control and navigation, as well as data evaluation and processing.\nThis paper addresses this gap through the design and implementation of two\ncomplementary USV systems tailored for uncrewed bathymetry research. This\nincludes a low-cost USV for Navigation And Control research (NAC-USV) and a\nsecond, high-end USV equipped with a high-resolution multi-beam sonar and the\nassociated hardware for Bathymetry data quality Evaluation and Post-processing\nresearch (BEP-USV). The NAC-USV facilitates the investigation of autonomous,\nfail-safe navigation and control, emphasizing the stability requirements for\nhigh-quality bathymetry data collection while minimizing the risk to equipment.\nThe BEP-USV, which mirrors the NAC-USV hardware, is then used for additional\ncontrol validation and in-depth exploration of bathymetry data evaluation and\npost-processing methodologies. We detail the design and implementation of both\nsystems, and open source the design. Furthermore, we demonstrate the system's\neffectiveness in a range of operational scenarios.",
        "As AI-driven computing infrastructures rapidly scale, discussions around data\ncenter design often emphasize energy consumption, water and electricity usage,\nworkload scheduling, and thermal management. However, these perspectives often\noverlook the critical interplay between AI-specific load transients and power\nelectronics. This paper addresses that gap by examining how large-scale AI\nworkloads impose unique demands on power conversion chains and, in turn, how\nthe power electronics themselves shape the dynamic behavior of AI-based\ninfrastructure. We illustrate the fundamental constraints imposed by\nmulti-stage power conversion architectures and highlight the key role of\nfinal-stage modules in defining realistic power slew rates for GPU clusters.\nOur analysis shows that traditional designs, optimized for slower-varying or\nCPU-centric workloads, may not adequately accommodate the rapid load ramps and\ndrops characteristic of AI accelerators. To bridge this gap, we present\ninsights into advanced converter topologies, hierarchical control methods, and\nenergy buffering techniques that collectively enable robust and efficient power\ndelivery. By emphasizing the bidirectional influence between AI workloads and\npower electronics, we hope this work can set a good starting point and offer\npractical design considerations to ensure future exascale-capable data centers\ncan meet the stringent performance, reliability, and scalability requirements\nof next-generation AI deployments.",
        "Time constraints on doctor patient interaction and restricted access to\nspecialists under the managed care system led to increasingly referring to\ncomputers as a medical information source and a self-health-care management\ntool. However, research show that less than 40% of information seekers\nindicated that online information helped them to make a decision about their\nhealth. Searching multiple web sites that need basic computer skills, lack of\ninteraction and no face to face interaction in most search engines and some\nsocial issues, led us to develop a specialized life-like agent that would\novercome mentioned problems.",
        "Despite impressive advancements in various multimodal tasks, vision-language\nmodels (VLMs) still struggle with compositional understanding due to limited\nexposure to training samples that contain subtle variations within paired\nexamples. With advances in multimodal generative models, a natural solution is\nto generate synthetic samples with subtle variations for training VLMs.\nHowever, generating and training on synthetic samples with subtle variations\npresents two challenges: difficulty in accurately creating precise variations\nand inconsistency in cross-modal alignment quality. To address these\nchallenges, we propose SVD-GT (Subtle Variation Data Generation and Training),\nwhich integrates image feature injection into a text-to-image generative model\nto enhance the quality of synthetic variations and employs an adaptive margin\nloss to differentiate samples using adaptive margins, which help filter out\npotentially incorrect synthetic samples and focus the learning on informative\nhard samples. Evaluations on four compositional understanding benchmarks\ndemonstrate that SVD-GT significantly improves the compositionality of VLMs,\nboosting the average accuracy of CLIP by over 8% across all benchmarks and\noutperforming state-of-the-art methods by 2% on three benchmarks.",
        "We prove that bounded conciseness is a closed property in the space of marked\ngroups. As a consequence, we reformulate a conjecture of Fern\\'andez-Alcober\nand Shumyatsky [7] about conciseness in the class of residually finite groups.",
        "Virtual presence demands ultra-low latency, a factor that centralized\narchitectures, by their nature, cannot minimize. Local peer-to-peer\narchitectures offer a compelling alternative, but also pose unique challenges\nin terms of network infrastructure. This paper introduces a prototype\nleveraging Conflict-Free Replicated Data Types (CRDTs) to enable real-time\ncollaboration in a shared virtual environment. Using this prototype, we\ninvestigate latency, synchronization, and the challenges of decentralized\ncoordination in dynamic non-Byzantine contexts. We aim to question prevailing\nassumptions about decentralized architectures and explore the practical\npotential of P2P in advancing virtual presence. This work challenges the\nconstraints of mediated networks and highlights the potential of decentralized\narchitectures to redefine collaboration and interaction in digital spaces.",
        "This article has a twofold purpose. On the one hand I would like to draw\nattention to some nice exercises on the Kepler laws, due to Otto Laporte from\n1970. Our discussion here has a more geometric flavour than the original\nanalytic approach of Laporte. On the other hand it serves as an addendum to a\npaper of mine from 1998 on the quantum integrability of the Kovalevsky top.\nLater I learned that this integrability result had been obtained already long\nbefore by Laporte in 1933.",
        "The challenge of formal proof generation has a rich history, but with modern\ntechniques, we may finally be at the stage of making actual progress in\nreal-life mathematical problems. This paper explores the integration of ChatGPT\nand basic searching techniques to simplify generating formal proofs, with a\nparticular focus on the miniF2F dataset. We demonstrate how combining a large\nlanguage model like ChatGPT with a formal language such as Lean, which has the\nadded advantage of being verifiable, enhances the efficiency and accessibility\nof formal proof generation. Despite its simplicity, our best-performing\nLean-based model surpasses all known benchmarks with a 31.15% pass rate. We\nextend our experiments to include other datasets and employ alternative\nlanguage models, showcasing our models' comparable performance in diverse\nsettings and allowing for a more nuanced analysis of our results. Our findings\noffer insights into AI-assisted formal proof generation, suggesting a promising\ndirection for future research in formal mathematical proof.",
        "In modern information retrieval (IR). achieving more than just accuracy is\nessential to sustaining a healthy ecosystem, especially when addressing\nfairness and diversity considerations. To meet these needs, various datasets,\nalgorithms, and evaluation frameworks have been introduced. However, these\nalgorithms are often tested across diverse metrics, datasets, and experimental\nsetups, leading to inconsistencies and difficulties in direct comparisons. This\nhighlights the need for a comprehensive IR toolkit that enables standardized\nevaluation of fairness- and diversity-aware algorithms across different IR\ntasks. To address this challenge, we present FairDiverse, an open-source and\nstandardized toolkit. FairDiverse offers a framework for integrating fair and\ndiverse methods, including pre-processing, in-processing, and post-processing\ntechniques, at different stages of the IR pipeline. The toolkit supports the\nevaluation of 28 fairness and diversity algorithms across 16 base models,\ncovering two core IR tasks (search and recommendation) thereby establishing a\ncomprehensive benchmark. Moreover, FairDiverse is highly extensible, providing\nmultiple APIs that empower IR researchers to swiftly develop and evaluate their\nown fairness and diversity aware models, while ensuring fair comparisons with\nexisting baselines. The project is open-sourced and available on\nhttps:\/\/github.com\/XuChen0427\/FairDiverse.",
        "A Differential Drive Robot (DDR) located inside a circular detection region\nin the plane wants to escape from it in minimum time. Various robotics\napplications can be modeled like the previous problem, such as a DDR escaping\nas soon as possible from a forbidden\/dangerous region in the plane or running\nout from the sensor footprint of an unmanned vehicle flying at a constant\naltitude. In this paper, we find the motion strategies to accomplish its goal\nunder two scenarios. In one, the detection region moves slower than the DDR and\nseeks to prevent escape; in another, its position is fixed. We formulate the\nproblem as a zero-sum pursuit-evasion game, and using differential games\ntheory, we compute the players' time-optimal motion strategies. Given the DDR's\nspeed advantage, it can always escape by translating away from the center of\nthe detection region at maximum speed. In this work, we show that the previous\nstrategy could be optimal in some cases; however, other motion strategies\nemerge based on the player's speed ratio and the players' initial\nconfigurations.",
        "Compared to traditional electromagnetic stealth (ES) materials, which are\neffective only within specific frequencies and orientations, intelligent\nreflecting surface (IRS) technology introduces a novel paradigm for achieving\ndynamic and adaptive ES by adapting its reflection pattern in real time to\nneutralize radar probing signals echoed back from the target. In this letter,\nwe study an IRS-aided ES system mounted on an aerial target to evade radar\ndetection admist uncertain\/moving radar positions over an extended area.\nSpecifically, we aim to optimize the IRS's passive reflection to minimize the\nmaximum received signal-to-noise ratio (SNR) of the target echo signal in the\narea. A semi-closed-form solution is derived by first discretizing the\ncontinuous spatial frequency deviation to approximate the semi-infinite\nreflection gain constraint and then leveraging the Lagrange dual method.\nSimulation results are provided to validate that the proposed IRS-aided ES\nstrategy can consistently reduce the reflection gains for radars located across\na large region.",
        "We investigate the continuity of Hausdorff dimension and box dimension (limit\ncapacity) of non-hyperbolic repellers of diffeomorphisms derived from\ntransitive Anosov diffeomophisms through a Hopf bifurcation studied by Horita\nand Viana (see Discret. Contin. Dyn. Syst., 13 (2005), 1125-1137). Here, we\nextend their work showing that both dimensions are continuous at paremeter\nbifurcation. In the proof, we consider maps with holes introduced by Horita and\nViana in Journal of Statistical Physics 105(2001), 835-862 and further\ndeveloped by Dysman in Journal of Statistical Physics 120(2005),479-509,\nrelating the Hausdorff dimension with the volume of the hole."
      ]
    }
  },
  {
    "id":2412.09927,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"MagNet: machine learning enhanced three-dimensional magnetic reconstruction",
    "start_abstract":"Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Three-dimensional nanomagnetism"
      ],
      "abstract":[
        "Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities."
      ],
      "categories":[
        "cond-mat.mes-hall"
      ]
    },
    "list":{
      "title":[
        "A new framework for Ljusternik-Schnirelmann theory and its application\n  to planar Choquard equations",
        "Implicit Generative Modeling by Kernel Similarity Matching",
        "k-Sample inference via Multimarginal Optimal Transport",
        "A Drinfeld Presentation of the Queer Super-Yangian",
        "Implicit Bias in Matrix Factorization and its Explicit Realization in a\n  New Architecture",
        "An experimental technique for measuring radial coherence",
        "Probing prethermal nonergodicity through measurement outcomes of\n  monitored quantum dynamics",
        "Four Total Eclipsing Contact Binary Systems: The First Photometric Light\n  Curve Solutions Employing TESS and Gaia Surveys",
        "Time-Variant Vector Field Visualization for Magnetic Fields of Neutron\n  Star Simulations",
        "Quantum Birkhoff Normal Form in the $\\sigma$-Bruno-R\\\"{u}ssmann\n  non-resonant condition",
        "From de Bruijn graphs to variation graphs-relationships between\n  pangenome models",
        "High-accuracy multi-ion spectroscopy with mixed-species Coulomb crystals",
        "A Liouville-type theorem for the p-Laplacian on complete non-compact\n  Riemannian manifolds",
        "Gradient Flows and the Curvature of Theory Space",
        "Causal survival analysis, Estimation of the Average Treatment Effect\n  (ATE): Practical Recommendations",
        "Clustering Does Not Always Imply Latent Geometry",
        "Gravitomagnetic tidal response of relativistic stars in partially\n  screened scalar-tensor theories",
        "Stabilizing open photon condensates by ghost-attractor dynamics",
        "Scalar Field Fluctuations and the Production of Dark Matter",
        "The Interplay between Dust Dynamics and Turbulence Induced by the\n  Vertical Shear Instability",
        "Photogalvanic Shift Currents in BiFeO3 --LaFeO3 Superlattices",
        "Quasi-Large Hole Polarons in BiVO4-Implications for Photocatalysis and\n  Solar Energy Conversion",
        "Satire: Computing Rigorous Bounds for Floating-Point Rounding Error in\n  Mixed-Precision Loop-Free Programs",
        "Mapping the $\\Lambda_{\\rm s}$CDM scenario to $f(T)$ modified gravity:\n  Effects on structure growth rate",
        "Tensor network method for solving the Ising model with a magnetic field",
        "Study of hadron interactions and compositeness",
        "Quantum Reservoir Computing and Risk Bounds",
        "A novel metric for species vulnerability and coexistence in\n  spatially-extended ecosystems",
        "Microchip semiconductor membrane external-cavity surface-emitting laser"
      ],
      "abstract":[
        "We consider the planar logarithmic Choquard equation $$- \\Delta u + a(x)u +\n(\\log|\\cdot| \\ast u^2)u = 0,\\qquad \\text{in } \\mathbb{R}^2$$ in the strongly\nindefinite and possibly degenerate setting where no sign condition is imposed\non the linear potential $a \\in L^\\infty(\\mathbb{R}^2)$. In particular, we shall\nprove the existence of a sequence of high energy solutions to this problem in\nthe case where $a$ is invariant under $\\mathbb{Z}^2$-translations.\n  The result extends to a more general $G$-equivariant setting, for which we\ndevelop a new variational approach which allows us to find critical points of\nLjusternik-Schnirelmann type. In particular, our method resolves the problem\nthat the energy functional $\\Phi$ associated with the logarithmic Choquard\nequation is only defined on a subspace $X \\subset H^1(\\mathbb{R}^2)$ with the\nproperty that $\\|\\cdot\\|_X$ is not translation invariant. The new approach is\nbased on a new $G$-equivariant version of the Cerami condition and on\ndeformation arguments adapted to a family of suitably constructed scalar\nproducts $\\langle \\cdot, \\cdot \\rangle_u$, $u \\in X$ with the $G$-equivariance\nproperty $\\langle g \\ast v , g \\ast w \\rangle_{g \\ast u} = \\langle v , w\n\\rangle_u.$",
        "Understanding how the brain encodes stimuli has been a fundamental problem in\ncomputational neuroscience. Insights into this problem have led to the design\nand development of artificial neural networks that learn representations by\nincorporating brain-like learning abilities. Recently, learning representations\nby capturing similarity between input samples has been studied to tackle this\nproblem. This approach, however, has thus far been used to only learn\ndownstream features from an input and has not been studied in the context of a\ngenerative paradigm, where one can map the representations back to the input\nspace, incorporating not only bottom-up interactions (stimuli to latent) but\nalso learning features in a top-down manner (latent to stimuli). We investigate\na kernel similarity matching framework for generative modeling. Starting with a\nmodified sparse coding objective for learning representations proposed in prior\nwork, we demonstrate that representation learning in this context is equivalent\nto maximizing similarity between the input kernel and a latent kernel. We show\nthat an implicit generative model arises from learning the kernel structure in\nthe latent space and show how the framework can be adapted to learn manifold\nstructures, potentially providing insights as to how task representations can\nbe encoded in the brain. To solve the objective, we propose a novel Alternate\nDirection Method of Multipliers (ADMM) based algorithm and discuss the\ninterpretation of the optimization process. Finally, we discuss how this\nrepresentation learning problem can lead towards a biologically plausible\narchitecture to learn the model parameters that ties together representation\nlearning using similarity matching (a bottom-up approach) with predictive\ncoding (a top-down approach).",
        "This paper proposes a Multimarginal Optimal Transport ($MOT$) approach for\nsimultaneously comparing $k\\geq 2$ measures supported on finite subsets of\n$\\mathbb{R}^d$, $d \\geq 1$. We derive asymptotic distributions of the optimal\nvalue of the empirical $MOT$ program under the null hypothesis that all $k$\nmeasures are same, and the alternative hypothesis that at least two measures\nare different. We use these results to construct the test of the null\nhypothesis and provide consistency and power guarantees of this $k$-sample\ntest. We consistently estimate asymptotic distributions using bootstrap, and\npropose a low complexity linear program to approximate the test cut-off. We\ndemonstrate the advantages of our approach on synthetic and real datasets,\nincluding the real data on cancers in the United States in 2004 - 2020.",
        "We introduce a Drinfeld presentation for the super-Yangian\n$\\mathrm{Y}(\\mathfrak{q}_n)$ associated with the queer Lie superalgebra\n$\\mathfrak{q}_n$. The Drinfeld generators of $\\mathrm{Y}(\\mathfrak{q}_n)$ are\nobtained by a block version Gauss decomposition of the generator matrix in its\nRTT presentation, and the Drinfeld relations are explicitly computed by\nutilizing a block version of its RTT relations.",
        "Gradient descent for matrix factorization is known to exhibit an implicit\nbias toward approximately low-rank solutions. While existing theories often\nassume the boundedness of iterates, empirically the bias persists even with\nunbounded sequences. We thus hypothesize that implicit bias is driven by\ndivergent dynamics markedly different from the convergent dynamics for data\nfitting. Using this perspective, we introduce a new factorization model:\n$X\\approx UDV^\\top$, where $U$ and $V$ are constrained within norm balls, while\n$D$ is a diagonal factor allowing the model to span the entire search space.\nOur experiments reveal that this model exhibits a strong implicit bias\nregardless of initialization and step size, yielding truly (rather than\napproximately) low-rank solutions. Furthermore, drawing parallels between\nmatrix factorization and neural networks, we propose a novel neural network\nmodel featuring constrained layers and diagonal components. This model achieves\nstrong performance across various regression and classification tasks while\nfinding low-rank solutions, resulting in efficient and lightweight networks.",
        "Coherence refers to correlations between field vibrations at two separate\npoints in degrees of freedom such as space, time, and polarisation. In the\ncontext of space, coherence theory has been formulated between two transverse\npositions which can be described either in the cartesian coordinates or in the\ncylindrical coordinates. When expressed in cylindrical coordinates, spatial\ncoherence is described in terms of azimuthal and radial coordinates. The\ndescription of spatial coherence in radial degree of freedom has been\nformulated only recently in JOSA A 40, 411 (2023). In the present article, we\ndemonstrate an efficient experimental technique for measuring radial coherence,\nand we report measurement of radial coherence of two different types of\nradially partially coherent optical fields.",
        "Projective measurements are a key element in quantum physics and enable rich\nphenomena in monitored quantum dynamics. Here, we show that the measurement\noutcomes, recorded during monitored dynamics, can provide crucial information\nabout the properties of the monitored dynamical system itself. We demonstrate\nthis for a Floquet model of many-body localization, where we find that the\nprethermal many-body localized regime becomes unstable against rare\nmeasurements, yielding an unusual enhancement of quantum entanglement. Through\nan unsupervised learning and mutual information analysis on the classical\ndataset of measurement outcomes, we find that the information loss in the\nsystem, reflected by the increased entanglement, is compensated by an emergent\nstructure in this classical dataset. Our findings highlight the crucial role of\nmeasurements and corresponding classical outcomes in capturing prethermal\nnonergodicity, offering a promising perspective for applications to other\nmonitored quantum dynamics.",
        "We presented the first photometric light curve solutions of four W Ursae\nMajoris (W UMa)-type contact binary systems. This investigation utilized\nphotometric data from the Transiting Exoplanet Survey Satellite (TESS) and Gaia\nData Release 3 (DR3). We used the PHysics Of Eclipsing BinariEs (PHOEBE) Python\ncode and the Markov Chain Monte Carlo (MCMC) method for these light curve\nsolutions. Only TIC 249064185 among the target systems needed a cold starspot\nto be included in the analysis. Based on the estimated mass ratios for these\ntotal eclipse systems, three of them are categorized as low mass ratio contact\nbinary stars. The absolute parameters of the systems were estimated using the\nGaia DR3 parallax method and the orbital period and semi-major axis ($P-a$)\nempirical relationship. We defined that TIC 318015356 and TIC 55522736 systems\nare A-subtypes, while TIC 249064185 and TIC 397984843 are W-subtypes, depending\non each component's effective temperature and mass. We estimated the initial\nmasses of the stars, the mass lost by the binary system, and the systems' ages.\nWe displayed star positions in the mass-radius, mass-luminosity, and total\nmass-orbital angular momentum diagrams. In addition, our findings indicate a\ngood agreement with the mass-temperature empirical parameter relationship for\nthe primary stars.",
        "We present a novel visualization application designed to explore the\ntime-dependent development of magnetic fields of neutron stars. The strongest\nmagnetic fields in the universe can be found within neutron stars, potentially\nplaying a role in initiating astrophysical jets and facilitating the outflow of\nneutron-rich matter, ultimately resulting in the production of heavy elements\nduring binary neutron star mergers. Since such effects may be dependent on the\nstrength and configuration of the magnetic field, the formation and parameters\nof such fields are part of current research in astrophysics. Magnetic fields\nare investigated using simulations in which various initial configurations are\ntested. However, the long-term configuration is an open question, and current\nsimulations do not achieve a stable magnetic field. Neutron star simulations\nproduce data quantities in the range of several terabytes, which are both\nspatially in 3D and temporally resolved. Our tool enables physicists to\ninteractively explore the generated data. We first convert the data in a\npre-processing step and then we combine sparse vector field visualization using\nstreamlines with dense vector field visualization using line integral\nconvolution. We provide several methods to interact with the data responsively.\nThis allows the user to intuitively investigate data-specific issues.\nFurthermore, diverse visualization techniques facilitate individual exploration\nof the data and enable real-time processing of specific domain tasks, like the\ninvestigation of the time-dependent evolution of the magnetic field. In a\nqualitative study, domain experts tested the tool, and the usability was\nqueried. Experts rated the tool very positively and recommended it for their\ndaily work.",
        "The aim of this paper is to construct a Gevrey quantum Birkhoff normal form\nfor the $h$-differential operator $P_{h}(t),$ where $\nt\\in(-\\frac{1}{2},\\frac{1}{2})$, in the neighborhood of the union $\\Lambda$ of\nKAM tori. This construction commences from an appropriate Birkhoff normal form\nof $H$ around $\\Lambda$ and proceeds under the $\\sigma$-Bruno-R\\\"{u}ssmann\ncondition with $\\sigma>1$.",
        "Pangenomes serve as a framework for joint analysis of genomes of related\norganisms. Several pangenome models were proposed, offering different\nfunctionalities, applications provided by available tools, their efficiency\netc. Among them, two graph-based models are particularly widely used: variation\ngraphs and de Bruijn graphs. In the current paper we propose an axiomatization\nof the desirable properties of a graph representation of a collection of\nstrings. We show the relationship between variation graphs satisfying these\ncriteria and de Bruijn graphs. This relationship can be used to efficiently\nbuild a variation graph representing a given set of genomes, transfer\nannotations between both models, compare the results of analyzes based on each\nmodel etc.",
        "Multi-ion optical clocks offer the possibility of overcoming the low\nsignal-to-noise ratio of single-ion clocks, while still providing low\nsystematic uncertainties. We present simultaneous spectroscopy of up to four\n${}^{115}$In${}^+$ clock ions in a linear Coulomb crystal, sympathetically\ncooled with ${}^{172}$Yb${}^+$ ions. In first clock comparisons, we see\nagreement below $1\\times10^{-17}$ with results obtained using a single In${}^+$\nion, for which we have evaluated the systematic uncertainty to be\n$2.5\\times10^{-18}$. Operation with four clock ions reduces the instability\nfrom $1.6\\times10^{-15}\/\\sqrt{t\/(1\\;\\mathrm{s})}$ to\n$9.2\\times10^{-16}\/\\sqrt{t\/(1\\;\\mathrm{s})}$. We derive a model for\ndecay-related dead time during state preparation, which matches the observed\nscaling of instability with clock ion number $N$, and indicates that\n$1\/\\sqrt{N}$ scaling can be achieved with the addition of a repump laser.",
        "A Liouville-type result for the p-Laplacian on complete Riemannian manifolds\nis proved. As an application are present some results concerning complete\nnon-compact hypersurfaces immersed in a suitable warped product manifold.",
        "The metric and potential associated with the gradient property of\nrenormalisation group flow in multiscalar models in $d=4-\\varepsilon$\ndimensions are studied. The metric is identified with the Zamolodchikov metric\nof nearly marginal operators on the sphere. An explicit form for the associated\nRicci scalar in $d=4-\\varepsilon$ is derived, which shows that the space of\nmultiscalar field theories is curved. The potential is identified with a\nquantity $\\widetilde{F}$ that was previously proposed as a weakly monotonic\nfunction interpolating between the $a$-theorem in four dimensions and the\n$F$-theorem in three dimensions. This implies that the $\\widetilde{F}$-theorem\ncan be extended perturbatively to a theorem about gradient flow in\n$d=4-\\varepsilon$.",
        "Causal survival analysis combines survival analysis and causal inference to\nevaluate the effect of a treatment or intervention on a time-to-event outcome,\nsuch as survival time. It offers an alternative to relying solely on Cox models\nfor assessing these effects. In this paper, we present a comprehensive review\nof estimators for the average treatment effect measured with the restricted\nmean survival time, including regression-based methods, weighting approaches,\nand hybrid techniques. We investigate their theoretical properties and compare\ntheir performance through extensive numerical experiments. Our analysis focuses\non the finite-sample behavior of these estimators, the influence of nuisance\nparameter selection, and their robustness and stability under model\nmisspecification. By bridging theoretical insights with practical evaluation,\nwe aim to equip practitioners with both state-of-the-art implementations of\nthese methods and practical guidelines for selecting appropriate estimators for\ntreatment effect estimation. Among the approaches considered, G-formula\ntwo-learners, AIPCW-AIPTW, Buckley-James estimators, and causal survival\nforests emerge as particularly promising.",
        "The latent space approach to complex networks has revealed fundamental\nprinciples and symmetries, enabling geometric methods. However, the conditions\nunder which network topology implies geometricity remain unclear. We provide a\nmathematical proof and empirical evidence showing that the multiscale\nself-similarity of complex networks is a crucial factor in implying latent\ngeometry. Using degree-thresholding renormalization, we prove that any random\nscale-free graph in a $d$-dimensional homogeneous and isotropic manifold is\nself-similar when interactions are pairwise. Hence, both clustering and\nself-similarity are required to imply geometricity. Our findings highlight that\ncorrelated links can lead to finite clustering without self-similarity, and\ntherefore without inherent latent geometry. The implications are significant\nfor network mapping and ensemble equivalence between graphs and continuous\nspaces.",
        "In scalar-tensor theories beyond Horndeski, the Vainshtein screening\nmechanism is only partially effective inside astrophysical bodies. We\ninvestigate the potential to detect this partial breaking of Vainshtein\nscreening through the tidal response of fluid bodies. Specifically, we\ncalculate the gravitomagnetic tidal Love numbers and analyze how deviations\nfrom general relativity depend on parameters governing the breaking of\nVainshtein screening in the weak-gravity regime. For fixed parameter values,\nthe relative deviations increase with higher multipoles and larger compactness.\nHowever, we demonstrate that these parameters alone are insufficient to fully\ncharacterize the tidal response of relativistic bodies in scalar-tensor\ntheories beyond Horndeski.",
        "We study the temporal, driven-dissipative dynamics of open photon\nBose-Einstein condensates (BEC) in a dye-filled microcavity, taking the\ncondensate amplitude and the noncondensed fluctuations into account on the same\nfooting by means of a cumulant expansion within the Lindblad formalism. The\nfluctuations fundamentally alter the dynamics in that the BEC always dephases\nto zero for sufficiently long time. However, a ghost-attractor, although it is\noutside of the physically accessible configuration space, attracts the dynamics\nand leads to a plateau-like stabilization of the BEC for an exponentially long\ntime, consistent with experiments. We also show that the photon BEC and the\nlasing state are separated by a true phase transition, since they are\ncharacterized by different fixed points. The ghost-attractor nonequilibrium\nstabilization mechanism is alternative to prethermalization and may possibly be\nrealized on other dynamical platforms as well.",
        "One of the simplest possible candidates for dark matter is a stable scalar\nsinglet beyond the Standard Model. If its mass is below the Hubble scale during\ninflation, long-wavelength modes of this scalar will be excited during\ninflation, and their subsequent evolution may lead to the correct relic density\nof dark matter. In this work, we provide a comprehensive analysis of the\nevolution of a spectator scalar. We examine three cases: (1) a non-interacting\nmassive scalar, (2) a massive scalar with self-interactions of the form\n$\\lambda_\\chi \\chi^p$, and (3) a massive scalar coupled to the inflaton $\\phi$\nthrough an interaction term of the form $\\sigma_{n,m} \\phi^n \\chi^m$. In all\ncases, we assume minimal coupling to gravity and compare these results with the\nproduction of short-wavelength modes arising from single graviton exchange. The\nevolution is tracked during the reheating phase. Our findings are summarized\nusing $(m_\\chi, T_{\\rm RH})$ parameter planes, where $m_\\chi$ is the mass of\nthe scalar field and $T_{\\rm RH}$ is the reheating temperature after inflation.\nThe non-interacting scalar is highly constrained, requiring $m_\\chi > 3 \\times\n10^{12}~\\rm {GeV}$ and $ T_{\\rm RH} \\lesssim 7~\\text{TeV}$ for an inflationary\npotential with a quadratic minimum. However, when self-interactions or\ncouplings to the inflaton are included, the viable parameter space expands\nconsiderably. In these cases, sub-GeV and even sub-eV scalar masses can yield\nthe correct relic abundance, opening new possibilities for light dark matter\ncandidates. In all cases, we also impose additional constraints arising from\nthe production of isocurvature fluctuations, the prevention of a secondary\ninflationary phase triggered by the spectator field, and the fragmentation of\nscalar condensates.",
        "The interaction between gas and dust in protoplanetary disks (PPDs) plays a\ncrucial role in setting the stage of planet formation. In particular, the\nstreaming instability (SI) is well recognized as the mechanism for planetesimal\nformation out of this interaction. The outer region of PPDs is likely subject\nto the vertical shear instability (VSI), representing a major source of disk\nturbulence characterized by vertical corrugation that leads to strong dust\nstirring. In the meantime, the VSI turbulence in 3D generates vortices through\nthe Rossby wave instability (RWI), which can trap dust and thereby promote dust\nconcentration. In this study, we use the multifluid dust module in Athena++ to\nconduct 2D axisymmetric global simulations of PPDs with mesh refinement and 3D\nglobal simulations with modest resolution. In 2D, the VSI corrugation mode is\nweakened by dust back-reaction, while the SI can still survive regardless of\ninitial conditions. Dust clumping occurs and is seeded by VSI-induced zonal\nflows. In 3D, dust can settle even more with increased dusty buoyancy,\nsuppressing the VSI corrugation mode. Meanwhile, dust back-reaction enhances\ndust concentration in RWI vortices, though higher resolution is needed to\nassess dust clumping.",
        "Designing materials with controlled photovoltaic response may lead to\nimproved solar cells or photosensors. In this regard, ferroelectric\nsuperlattices have emerged as a rich platform to engineer functional\nproperties. In addition, ferroelectrics are naturally endowed with a bulk\nphotovoltaic response stemming from non-thermalized photoexcited carriers,\nwhich can overcome the fundamental limits of current solar cells. Yet, their\nphotovoltaic output has been limited by poor optical absorption and poor charge\ncollection or photo-excited carrier mean free path. We use Density Functional\nTheory and Wannierization to compute the so-called Bulk Photovoltaic shift\ncurrent and the optical properties of BiFeO3\/LaFeO3 superlattices. We show\nthat, by stacking these two materials, not only the optical absorption is\nimproved at larger wavelengths (due to LaFeO3 smaller bandgap), but the\nphotovolgavanic shift current is also enhanced compared to that of pure BiFeO3\n, by suppressing the destructive interferences occurring between different\nwavelengths.",
        "Bismuth vanadate (BiVO4 BVO) is a promising photocatalyst for solar energy\nconversion, but its efficiency is limited by small polaron formation. However,\nsome physical properties of BVO deviate from typical small polaron behavior.\nUsing the state-of-the-art first-principles calculations, we demonstrate that\nBVO forms a quasi-large hole polaron with a radius around 2 nm, resembling free\ncarriers with high mobility. This polaron is stabilized primarily by acoustic\nphonon modes, creating a shallow trap state near the valence band maximum,\nwhich prolongs its lifetime. Simultaneously, it retains a redox potential\ncomparable to that of free carriers. We propose that such large polarons\nexplain the superior properties of BVO and other transition metal oxide\nphotocatalysts. Tuning phonon modes to stabilize large polarons offers a\npromising strategy for designing materials with enhanced solar energy\nconversion efficiency.",
        "Techniques that rigorously bound the overall rounding error exhibited by a\nnumerical program are of significant interest for communities developing\nnumerical software. However, there are few available tools today that can be\nused to rigorously bound errors in programs that employ conditional statements\n(a basic need) as well as mixed-precision arithmetic (a direction of\nsignificant future interest) employing global optimization in error analysis.\nIn this paper, we present a new tool that fills this void while also employing\nan abstraction-guided optimization approach to allow designers to trade\nerror-bound tightness for gains in analysis time -- useful when searching for\ndesign alternatives. We first present the basic rigorous analysis framework of\nSatire and then show how to extend it to incorporate abstractions,\nconditionals, and mixed-precision arithmetic. We begin by describing Satire's\ndesign and its performance on a collection of benchmark examples. We then\ndescribe these aspects of Satire: (1) how the error-bound and tool execution\ntime vary with the abstraction level; (2) the additional machinery to handle\nconditional expression branches, including defining the concepts of instability\njumps and instability window widths and measuring these quantities; and (3) how\nthe error changes when a mix of precision values are used. To showcase how\n\\satire can add value during design, we start with a Conjugate Gradient solver\nand demonstrate how its step size and search direction are affected by\ndifferent precision settings. Satire is freely available for evaluation, and\ncan be used during the design of numerical routines to effect design tradeoffs\nguided by rigorous empirical error guarantees.",
        "The concept of a rapidly sign-switching cosmological constant, interpreted as\na mirror AdS-dS transition in the late universe and known as the $\\Lambda_{\\rm\ns}$CDM, has significantly improved the fit to observational data, offering a\npromising framework for alleviating major cosmological tensions such as the\n$H_0$ and $S_8$ tensions. However, when considered within general relativity,\nthis scenario does not predict any effects on the evolution of the matter\ndensity contrast beyond modifications to the background functions. In this\nwork, we propose a new gravitational model in which the background dynamics\npredicted by the $\\Lambda_{\\rm s}$CDM framework are mapped into $f(T)$ gravity,\ndubbed $f(T)-\\Lambda_{\\rm s}$CDM, rendering the models indistinguishable at the\nbackground level. However, in this new scenario, the sign-switching\ncosmological constant dynamics modify the evolution of linear matter\nperturbations through an effective gravitational constant, $G_{\\rm eff}$. We\ninvestigate the evolution of the growth rate and derive new observational\nconstraints for this scenario using RSD measurements. We also present new\nconstraints in the standard $\\Lambda_{\\rm s}$CDM case, incorporating the latest\nType Ia supernovae data samples available in the literature, along with BAO\ndata from DESI. Our findings indicate that the new corrections expected at the\nlinear perturbative level, as revealed through RSD samples, can provide\nsignificant evidence in favor of this new scenario. Additionally, this model\nmay be an excellent candidate for resolving the current $S_8$ tension.",
        "We study the two-dimensional square lattice Ising ferromagnet and\nantiferromagnet with a magnetic field by using tensor network method. Focusing\non the role of guage fixing, we present the partition function in terms of a\ntensor network. The tensor has a different symmetry property for ferromagnets\nand antiferromagnets. The tensor network of the partition function is\ninterpreted as a multiple product of the one-dimensional quantum Hamiltonian.\nWe perform infinite density matrix renormalization group to contract the\ntwo-dimensional tensor network. We present the numerical result of\nmagnetization and entanglement entropy for the Ising ferromagnet and\nantiferromagnet side by side. In order to determine the critical line in the\nparameter space of temperature and magnetic field, we use the half-chain\nentanglement entropy of the one-dimensional quantum state. The entanglement\nentropy precisely indicates the critical line forming the parabolic shape for\nthe antiferromagnetic case, but shows the critical point for the ferromagnetic\ncase.",
        "Plenty of hadrons have been established experimentally, yet the\nnonperturbative nature of the strong interaction complicates a comprehensive\nunderstanding of their internal structure, particularly for exotic hadrons that\nextend beyond conventional mesons and baryons. One prominent candidate for the\ninternal structure of exotic hadrons is the hadronic molecule, a loosely bound\nsystem of hadrons analogous to atomic nuclei. Understanding such systems\nrequires precise knowledge of hadron interactions, which traditional scattering\nexperiments struggle to provide, especially in the low-energy region. Recent\nadvances, including precise baryon-baryon interaction measurements, femtoscopy\ntechniques that probe momentum correlations between particles, and\nfirst-principles lattice QCD calculations, have significantly improved our\nunderstanding of hadron interactions. Here, we review these recent\ndevelopments, demonstrate the successful application of femtoscopy to\nantikaon-nucleon interactions, utilize the concept of compositeness to quantify\nthe hadronic molecular component of the Lambda(1405), and discuss both\nexperimental and theoretical prospects, including future studies at J-PARC.",
        "We propose a way to bound the generalisation errors of several classes of\nquantum reservoirs using the Rademacher complexity. We give specific,\nparameter-dependent bounds for two particular quantum reservoir classes. We\nanalyse how the generalisation bounds scale with growing numbers of qubits.\nApplying our results to classes with polynomial readout functions, we find that\nthe risk bounds converge in the number of training samples. The explicit\ndependence on the quantum reservoir and readout parameters in our bounds can be\nused to control the generalisation error to a certain extent. It should be\nnoted that the bounds scale exponentially with the number of qubits $n$. The\nupper bounds on the Rademacher complexity can be applied to other reservoir\nclasses that fulfill a few hypotheses on the quantum dynamics and the readout\nfunction.",
        "We develop a theoretical framework to understand the persistence and\ncoexistence of competitive species in a spatially explicit metacommunity model\nwith a heterogeneous dispersal kernel. Our analysis, based on methods from the\nphysics of disordered systems and non-Gaussian dynamical mean field theory,\nreveals that species coexistence is governed by a single key parameter, which\nwe term competitive balance. From competitive balance, we derive a novel metric\nto quantitatively assess the vulnerability of a species, showing that abundance\nalone is not sufficient to determine it. Rather, a species' vulnerability\ncrucially depends on the state of the metacommunity as a whole. We test our\ntheory by analyzing two distinct tropical forest datasets, finding excellent\nagreement with our theoretical predictions. A key step in our analysis is the\nintroduction of a new quantity - the competitive score - which disentangles the\nabundance distribution and enables us to circumvent the challenge of estimating\nboth the colonization kernel and the joint abundance distribution. Our findings\nprovide novel and fundamental insights into the ecosystem-level trade-offs\nunderlying macroecological patterns and introduce a robust approach for\nestimating extinction risks.",
        "We demonstrate the first microchip semiconductor membrane external-cavity\nsurface-emitting laser. This compact type of laser consists solely of a\nsemiconductor gain region present as a micron-thin membrane, sandwiched between\ntwo transparent heat spreaders. The heat spreaders have a highly reflective\ncoating on their outer facets, which assembles the laser's plane-parallel\nsolid-state cavity with a total length of just ~1 mm. One of the coatings with\nslightly reduced reflectivity acts as outcoupling mirror. The microchip\nmembrane external-cavity surface-emitting laser (microchip MECSEL) is optically\npumped with a standard fiber-coupled diode laser module emitting at 808 nm and\nstabilizes itself due to an occurring thermal lens. More than one watt of\ncontinuous wave output power around 1123 nm and a record value in fitted slope\nefficiency of ~51.4 % with MECSELs, while maintaining excellent beam quality\n(TEM_00, M^2 < 1.05), is demonstrated. Important properties of semiconductor\nlasers such as the efficiency, beam quality, and polarization were\ninvestigated. Further, this setup was used to characterize the thermal lens and\nit's dependence on the absorbed pump power in the microchip MECSEL. Such\nsystems represent an attractive solution, when high-power output at\ncustomizable emission wavelength with excellent beam quality is needed in\ncombination with very compact built size."
      ]
    }
  },
  {
    "id":2412.19927,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Model-free simulations of turbulent reactive flows",
    "start_abstract":"A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given.",
    "start_categories":[
      "physics.flu-dyn"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b40"
      ],
      "title":[
        "Physics guided neural networks for spatio-temporal superresolution of turbulent flows"
      ],
      "abstract":[
        "Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Continuous spectrum-shrinking maps and applications to preserver\n  problems",
        "Aligning LLMs with Domain Invariant Reward Models",
        "People Reduce Workers' Compensation for Using Artificial Intelligence\n  (AI)",
        "DnD Filter: Differentiable State Estimation for Dynamic Systems using\n  Diffusion Models",
        "The Commutators of $n$-dimensional Rough Fractional Hardy Operators on\n  Two Weighted Grand Herz-Morrey Spaces with Variable Exponents",
        "Hypernetwork-based approach for optimal composition design in partially\n  controlled multi-agent systems",
        "Science mapping of the Revista General de Informacion y Documentacion\n  (2005-2022)",
        "Error norm estimates for the block conjugate gradient algorithm",
        "TFBS-Finder: Deep Learning-based Model with DNABERT and Convolutional\n  Networks to Predict Transcription Factor Binding Sites",
        "Scalable Video Conferencing Using SDN Principles",
        "Mining Diamonds in labeled Transition Systems",
        "TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of\n  Tools",
        "Monte-Carlo based non-line-of-sight underwater wireless optical\n  communication channel modeling and system performance analysis under\n  turbulence",
        "AIoT-based smart traffic management system",
        "Phase space analysis of CCDM cosmologies",
        "Understanding Abandonment and Slowdown Dynamics in the Maven Ecosystem",
        "Lifts of Brauer characters in characteristic two, II",
        "From Retrieval to Generation: Comparing Different Approaches",
        "Lagrangian chaos and unique ergodicity for stochastic primitive\n  equations",
        "Making the Peers' Subjective Well-being Visible Impairs\n  Cooperator-centered Experimental Social Networks",
        "Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models",
        "Negativity in Self-Admitted Technical Debt: How Sentiment Influences\n  Prioritization",
        "$H^\\infty$-control for a class of boundary controlled hyperbolic PDEs",
        "Self-supervised Normality Learning and Divergence Vector-guided Model\n  Merging for Zero-shot Congenital Heart Disease Detection in Fetal Ultrasound\n  Videos",
        "Random Subwords and Billiard Walks in Affine Weyl Groups",
        "Kise-Manitow's Hand in Space: Securing Communication and Connections in\n  Space",
        "Higher-Order Belief in Incomplete Information MAIDs",
        "Forward-backward Contention Resolution Schemes for Fair Rationing",
        "A Bridge to Nowhere: A Healthcare Case Study for Non-Reformist Design"
      ],
      "abstract":[
        "For a positive integer $n$ let $\\mathcal{X}_n$ be either the algebra $M_n$ of\n$n \\times n$ complex matrices, the set $N_n$ of all $n \\times n$ normal\nmatrices, or any of the matrix Lie groups $\\mathrm{GL}(n)$, $\\mathrm{SL}(n)$\nand $\\mathrm{U}(n)$. We first give a short and elementary argument that for two\npositive integers $m$ and $n$ there exists a continuous spectrum-shrinking map\n$\\phi : \\mathcal{X}_n \\to M_m$ (i.e.\\ $\\mathrm{sp}(\\phi(X))\\subseteq\n\\mathrm{sp}(X)$ for all $X \\in \\mathcal{X}_n$) if and only if $n$ divides $m$.\nMoreover, in that case we have the equality of characteristic polynomials\n$k_{\\phi(X)}(\\cdot) = k_{X}(\\cdot)^\\frac{m}{n}$ for all $X \\in \\mathcal{X}_n$,\nwhich in particular shows that $\\phi$ preserves spectra. Using this we show\nthat whenever $n \\geq 3$, any continuous commutativity preserving and\nspectrum-shrinking map $\\phi : \\mathcal{X}_n \\to M_n$ is of the form\n$\\phi(\\cdot)=T(\\cdot)T^{-1}$ or $\\phi(\\cdot)=T(\\cdot)^tT^{-1}$, for some $T\\in\n\\mathrm{GL}(n)$. The analogous results fail for the special unitary group\n$\\mathrm{SU}(n)$, and slightly more elaborate versions hold for the spaces of\nsemisimple elements in either $\\mathrm{GL}(n)$ or $\\mathrm{SL}(n)$, where a\nqualitatively new (and surprising) phenomenon arises: the map sending\n$SNS^{-1}$ to $S^{-1}NS$ for positive invertible $S$ and normal $N$ is also an\nexample. As a consequence, we also recover (a strengthened version of)\n\\v{S}emrl's influential characterization of Jordan automorphisms of $M_n$ via\npreserving properties.",
        "Aligning large language models (LLMs) to human preferences is challenging in\ndomains where preference data is unavailable. We address the problem of\nlearning reward models for such target domains by leveraging feedback collected\nfrom simpler source domains, where human preferences are easier to obtain. Our\nkey insight is that, while domains may differ significantly, human preferences\nconvey \\emph{domain-agnostic} concepts that can be effectively captured by a\nreward model. We propose \\method, a framework that trains domain-invariant\nreward models by optimizing a dual loss: a domain loss that minimizes the\ndivergence between source and target distribution, and a source loss that\noptimizes preferences on the source domain. We show \\method is a general\napproach that we evaluate and analyze across 4 distinct settings: (1)\nCross-lingual transfer (accuracy: $0.621 \\rightarrow 0.661$), (2)\nClean-to-noisy (accuracy: $0.671 \\rightarrow 0.703$), (3) Few-shot-to-full\ntransfer (accuracy: $0.845 \\rightarrow 0.920$), and (4) Simple-to-complex tasks\ntransfer (correlation: $0.508 \\rightarrow 0.556$). Our code, models and data\nare available at \\url{https:\/\/github.com\/portal-cornell\/dial}.",
        "We investigate whether and why people might reduce compensation for workers\nwho use AI tools. Across 10 studies (N = 3,346), participants consistently\nlowered compensation for workers who used AI tools. This \"AI Penalization\"\neffect was robust across (1) different types of work and worker statuses and\nworker statuses (e.g., full-time, part-time, or freelance), (2) different forms\nof compensation (e.g., required payments or optional bonuses) and their timing,\n(3) various methods of eliciting compensation (e.g., slider scale, multiple\nchoice, and numeric entry), and (4) conditions where workers' output quality\nwas held constant, subject to varying inferences, or controlled for. Moreover,\nthe effect emerged not only in hypothetical compensation scenarios (Studies\n1-5) but also with real gig workers and real monetary compensation (Study 6).\nPeople reduced compensation for workers using AI tools because they believed\nthese workers deserved less credit than those who did not use AI (Studies 3 and\n4). This effect weakened when it is less permissible to reduce worker\ncompensation, such as when employment contracts provide stricter constraints\n(Study 4). Our findings suggest that adoption of AI tools in the workplace may\nexacerbate inequality among workers, as those protected by structured contracts\nface less vulnerability to compensation reductions, while those without such\nprotections risk greater financial penalties for using AI.",
        "This paper proposes the DnD Filter, a differentiable filter that utilizes\ndiffusion models for state estimation of dynamic systems. Unlike conventional\ndifferentiable filters, which often impose restrictive assumptions on process\nnoise (e.g., Gaussianity), DnD Filter enables a nonlinear state update without\nsuch constraints by conditioning a diffusion model on both the predicted state\nand observational data, capitalizing on its ability to approximate complex\ndistributions. We validate its effectiveness on both a simulated task and a\nreal-world visual odometry task, where DnD Filter consistently outperforms\nexisting baselines. Specifically, it achieves a 25\\% improvement in estimation\naccuracy on the visual odometry task compared to state-of-the-art\ndifferentiable filters, and even surpasses differentiable smoothers that\nutilize future measurements. To the best of our knowledge, DnD Filter\nrepresents the first successful attempt to leverage diffusion models for state\nestimation, offering a flexible and powerful framework for nonlinear estimation\nunder noisy measurements.",
        "In this paper, we obtain the boundedness of $m$th order commutators generated\nby the $n$-dimensional fractional Hardy operator with rough kernel and its\nadjoint operator with BMO functions on two weighted grand Herz-Morrey spaces\nwith variable exponents. Replacing Lipschitz functions with BMO functions the\ncorresponding result is also given.",
        "Partially Controlled Multi-Agent Systems (PCMAS) are comprised of\ncontrollable agents, managed by a system designer, and uncontrollable agents,\noperating autonomously. This study addresses an optimal composition design\nproblem in PCMAS, which involves the system designer's problem, determining the\noptimal number and policies of controllable agents, and the uncontrollable\nagents' problem, identifying their best-response policies. Solving this\nbi-level optimization problem is computationally intensive, as it requires\nrepeatedly solving multi-agent reinforcement learning problems under various\ncompositions for both types of agents. To address these challenges, we propose\na novel hypernetwork-based framework that jointly optimizes the system's\ncomposition and agent policies. Unlike traditional methods that train separate\npolicy networks for each composition, the proposed framework generates policies\nfor both controllable and uncontrollable agents through a unified hypernetwork.\nThis approach enables efficient information sharing across similar\nconfigurations, thereby reducing computational overhead. Additional\nimprovements are achieved by incorporating reward parameter optimization and\nmean action networks. Using real-world New York City taxi data, we demonstrate\nthat our framework outperforms existing methods in approximating equilibrium\npolicies. Our experimental results show significant improvements in key\nperformance metrics, such as order response rate and served demand,\nhighlighting the practical utility of controlling agents and their potential to\nenhance decision-making in PCMAS.",
        "A study of the Revista General de Informacion y Documentacion, from 2005 to\n2022. The objective is aimed at qualifying the structure of the research field\nand assessing the trajectory of the thematic areas covered. Applying as\nmethodology the analysis of co-words, the construction of bibliometric networks\nand the creation of scientific maps. 514 documents are extracted from the Web\nof Science (WoS) database. The keywords assigned by the authors of the\ndocuments are selected and divided into three subperiods: 2005-2010, 2011-2016\nand 2017-2022. In the results, 1701 author keywords and 37 bibliometric\nnetworks are obtained. In the period 2005-2010, the structure of the research\nfield is represented on the scientific map with very few central and\nspecialized topics, considering an initial and underdeveloped organization. In\nthe period 2011-2016, the structure of the research field is distributed on the\nscientific map with a more varied number of central and specialized topics, but\nstill insufficient, considering an organization in the process of development.\nIn the period 2017-2022, the structure of the research field is shown on the\nmap with all kinds of family of topics (central, specialized, transversal,\nemerging or disappearing), being valued as a dynamic, complex and heterogeneous\norganization. Regarding the evolution of the thematic areas, the map shows\nsolid progress between the last two periods. The morphology of the thematic\nfield treated in RGID is outlined in three phases: foundation, process of\ndevelopment and consolidation.",
        "In the book [Meurant and Tichy, SIAM, 2024] we discussed the estimation of\nerror norms in the conjugate gradient (CG) algorithm for solving linear systems\n$Ax=b$ with a symmetric positive definite matrix $A$, where $b$ and $x$ are\nvectors. In this paper, we generalize the most important formulas for\nestimating the $A$-norm of the error to the block case. First, we discuss in\ndetail the derivation of various variants of the block CG (BCG) algorithm from\nthe block Lanczos algorithm. We then consider BCG and derive the related block\nGauss and block Gauss-Radau quadrature rules. We show how to obtain lower and\nupper bounds on the $A$-norm of the error of each system, both in terms of the\nquantities computed in BCG and in terms of the underlying block Lanczos\nalgorithm. Numerical experiments demonstrate the behavior of the bounds in\npractical computations.",
        "Transcription factors are proteins that regulate the expression of genes by\nbinding to specific genomic regions known as Transcription Factor Binding Sites\n(TFBSs), typically located in the promoter regions of those genes. Accurate\nprediction of these binding sites is essential for understanding the complex\ngene regulatory networks underlying various cellular functions. In this regard,\nmany deep learning models have been developed for such prediction, but there is\nstill scope of improvement. In this work, we have developed a deep learning\nmodel which uses pre-trained DNABERT, a Convolutional Neural Network (CNN)\nmodule, a Modified Convolutional Block Attention Module (MCBAM), a Multi-Scale\nConvolutions with Attention (MSCA) module and an output module. The pre-trained\nDNABERT is used for sequence embedding, thereby capturing the long-term\ndependencies in the DNA sequences while the CNN, MCBAM and MSCA modules are\nuseful in extracting higher-order local features. TFBS-Finder is trained and\ntested on 165 ENCODE ChIP-seq datasets. We have also performed ablation studies\nas well as cross-cell line validations and comparisons with other models. The\nexperimental results show the superiority of the proposed method in predicting\nTFBSs compared to the existing methodologies. The codes and the relevant\ndatasets are publicly available at\nhttps:\/\/github.com\/NimishaGhosh\/TFBS-Finder\/.",
        "Video-conferencing applications face an unwavering surge in traffic,\nstressing their underlying infrastructure in unprecedented ways. This paper\nrethinks the key building block for conferencing infrastructures -- selective\nforwarding units (SFUs). SFUs relay and adapt media streams between\nparticipants and, today, run in software on general-purpose servers. Our main\ninsight, discerned from dissecting the operation of production SFU servers, is\nthat SFUs largely mimic traditional packet-processing operations such as\ndropping and forwarding. Guided by this, we present Scallop, an SDN-inspired\nSFU that decouples video-conferencing applications into a hardware-based data\nplane for latency-sensitive and frequent media operations, and a software\ncontrol plane for the (infrequent) remaining tasks, such as analyzing feedback\nsignals. Our Tofino-based implementation fully supports WebRTC and delivers\n7-210 times improved scaling over a 32-core commodity server, while reaping\nperformance improvements by cutting forwarding-induced latency by 26 times.",
        "Labeled transition systems can be a great way to visualize the complex\nbehavior of parallel and communicating systems. However, if, during a\nparticular timeframe, no synchronization or communication between processes\noccurs, then multiple parallel sequences of actions are able to interleave\narbitrarily, and the resulting graph quickly becomes too complex for the human\neye to understand easily. With that in mind, we propose an exact formalization\nof these arbitrary interleavings, and an algorithm to find all said\ninterleavings in deterministic LTSs, to reduce the visual complexity of labeled\ntransition systems.",
        "Precision therapeutics require multimodal adaptive models that generate\npersonalized treatment recommendations. We introduce TxAgent, an AI agent that\nleverages multi-step reasoning and real-time biomedical knowledge retrieval\nacross a toolbox of 211 tools to analyze drug interactions, contraindications,\nand patient-specific treatment strategies. TxAgent evaluates how drugs interact\nat molecular, pharmacokinetic, and clinical levels, identifies\ncontraindications based on patient comorbidities and concurrent medications,\nand tailors treatment strategies to individual patient characteristics. It\nretrieves and synthesizes evidence from multiple biomedical sources, assesses\ninteractions between drugs and patient conditions, and refines treatment\nrecommendations through iterative reasoning. It selects tools based on task\nobjectives and executes structured function calls to solve therapeutic tasks\nthat require clinical reasoning and cross-source validation. The ToolUniverse\nconsolidates 211 tools from trusted sources, including all US FDA-approved\ndrugs since 1939 and validated clinical insights from Open Targets. TxAgent\noutperforms leading LLMs, tool-use models, and reasoning agents across five new\nbenchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC,\ncovering 3,168 drug reasoning tasks and 456 personalized treatment scenarios.\nIt achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing\nGPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning.\nTxAgent generalizes across drug name variants and descriptions. By integrating\nmulti-step inference, real-time knowledge grounding, and tool-assisted\ndecision-making, TxAgent ensures that treatment recommendations align with\nestablished clinical guidelines and real-world evidence, reducing the risk of\nadverse events and improving therapeutic decision-making.",
        "Compared with line-of-sight (LOS) communication, nonline-of-sight (NLOS)\nunderwater wireless optical communication (UWOC) systems have garnered\nextensive attention because of their heightened suitability for the intricate\nand dynamic underwater environment. In the NLOS channel, photons can reach the\nreceiver by sea surface reflection or particle scattering. However, research\nlacks comprehensive channel models that incorporate sea surface reflection and\nparticle scattering. Moreover, the presence of ocean turbulence introduces\nrandom fluctuations in the received optical signal based on the average light\nintensity. Consequently, this paper adopts the Monte Carlo simulation method\n(MCS) to solve the fading-free impulse response of the joint\nreflection-scattering channel. Furthermore, a weighted double gamma function\n(WDGF) is proposed to characterize the channel impulse response (CIR). Based on\nthe closed CIR model, the average bit error rate and the performance of the\ninterruption probability of the UWOC system under turbulence are analyzed. The\nconclusions obtained are intended to assist in the design and performance\nevaluation of NLOS UWOC systems.",
        "This paper presents a novel AI-based smart traffic management system\nde-signed to optimize traffic flow and reduce congestion in urban environments.\nBy analysing live footage from existing CCTV cameras, this approach eliminates\nthe need for additional hardware, thereby minimizing both deployment costs and\nongoing maintenance expenses. The AI model processes live video feeds to\naccurately count vehicles and assess traffic density, allowing for adaptive\nsignal control that prioritizes directions with higher traffic volumes. This\nreal-time adaptability ensures smoother traffic flow, reduces congestion, and\nminimizes waiting times for drivers. Additionally, the proposed system is\nsimulated using PyGame to evaluate its performance under various traffic\nconditions. The simulation results demonstrate that the AI-based system\nout-performs traditional static traffic light systems by 34%, leading to\nsignificant improvements in traffic flow efficiency. The use of AI to optimize\ntraffic signals can play a crucial role in addressing urban traffic challenges,\noffering a cost-effective, scalable, and efficient solution for modern cities.\nThis innovative system represents a key advancement in the field of smart city\ninfra-structure and intelligent transportation systems.",
        "We perform a detailed investigation of the CCDM (creation of cold dark\nmatter) cosmologies using the powerful techniques of qualitative analysis of\ndynamical systems. Considering a wide variety of the creation rates ranging\nfrom constant to dynamical, we examine the nature of critical points and their\nstability obtained from the individual scenario consisting of only cold dark\nmatter, or cold dark matter plus a second fluid with constant equation of\nstate. According to our analyses, these scenarios predict unstable dark matter\ndominated critical points, stable accelerating attractors dominated either by\ndark matter or the second fluid, scaling attractors in which both dark matter\nand the second fluid co-exist. Along with these critical points, these\nscenarios also indicate the possibility of decelerating attractors or\ndecelerating scaling attractors in the future which are new results in this\ndirection. These altogether suggest that CCDM cosmologies are viable\nalternatives to the mainstream cosmological models.",
        "The sustainability of libraries is critical for modern software development,\nyet many libraries face abandonment, posing significant risks to dependent\nprojects. This study explores the prevalence and patterns of library\nabandonment in the Maven ecosystem. We investigate abandonment trends over the\npast decade, revealing that approximately one in four libraries fail to survive\nbeyond their creation year. We also analyze the release activities of\nlibraries, focusing on their lifespan and release speed, and analyze the\nevolution of these metrics within the lifespan of libraries. We find that while\nslow release speed and relatively long periods of inactivity are often\nprecursors to abandonment, some abandoned libraries exhibit bursts of high\nfrequent release activity late in their life cycle. Our findings contribute to\na new understanding of library abandonment dynamics and offer insights for\npractitioners to identify and mitigate risks in software ecosystems.",
        "In 2007, J. P. Cossey conjectured that if $G$ is a finite $p$-solvable group\nand $\\varphi$ is an irreducible Brauer character of $G$ with vertex $Q$, then\nthe number of lifts of $\\varphi$ is at most $|Q:Q'|$. In this paper we\nrevisited Cossey's conjecture for $p=2$ from the perspective of Navarro\nvertices and obtained a new way to count the number of lifts of $\\varphi$. Some\napplications were given.",
        "Knowledge-intensive tasks, particularly open-domain question answering\n(ODQA), document reranking, and retrieval-augmented language modeling, require\na balance between retrieval accuracy and generative flexibility. Traditional\nretrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently\nretrieve from large corpora but often lack semantic depth. Generative models\nlike GPT-4-o provide richer contextual understanding but face challenges in\nmaintaining factual consistency. In this work, we conduct a systematic\nevaluation of retrieval-based, generation-based, and hybrid models, with a\nprimary focus on their performance in ODQA and related retrieval-augmented\ntasks. Our results show that dense retrievers, particularly DPR, achieve strong\nperformance in ODQA with a top-1 accuracy of 50.17\\% on NQ, while hybrid models\nimprove nDCG@10 scores on BEIR from 43.42 (BM25) to 52.59, demonstrating their\nstrength in document reranking. Additionally, we analyze language modeling\ntasks using WikiText-103, showing that retrieval-based approaches like BM25\nachieve lower perplexity compared to generative and hybrid methods,\nhighlighting their utility in retrieval-augmented generation. By providing\ndetailed comparisons and practical insights into the conditions where each\napproach excels, we aim to facilitate future optimizations in retrieval,\nreranking, and generative models for ODQA and related knowledge-intensive\napplications.",
        "We show that the Lagrangian flow associated with the stochastic 3D primitive\nequations (PEs) with non-degenerate noise is chaotic, i.e., the corresponding\ntop Lyapunov exponent is strictly positive almost surely. This result builds on\nthe landmark work by Bedrossian, Blumenthal, and Punshon-Smith on Lagrangian\nchaos in stochastic fluid mechanics. Our primary contribution is establishing\nan instance where Lagrangian chaos can be proven for a fluid flow with\nsupercritical energy, a key characteristic of 3D fluid dynamics. For the 3D\nPEs, establishing the existence of the top Lyapunov exponent is already a\nchallenging task. We address this difficulty by deriving new estimates for the\ninvariant measures of the 3D PEs, which capture the anisotropic smoothing in\nthe dynamics of the PEs. As a by-product of our results, we also obtain the\nfirst uniqueness result for invariant measures of stochastic PEs.",
        "Past experiments show that reputation or the knowledge of peers' past\ncooperation can enhance cooperation in human social networks. On the other\nhand, the knowledge of peers' wealth undermines cooperativeness, and that of\npeers' interconnectedness and network structure does not affect it. However, it\nis unknown if making peers' subjective well-being (SWB) available or visible in\nsocial networks may enhance or undermine cooperation. Therefore, we implemented\nonline network experiments (N = 662 in 50 networked groups with 15 rounds of\ninteractions), in which study participants cooperated with or defected against\nconnected peers through Public Goods Game, made and cut social ties with\nothers, and rated their SWB. We manipulated the visibility of connected peers'\nSWB (25 visible vs. 25 invisible SWB networked groups) while keeping the\nconnected peers' reputation and in-game wealth visible. Results show that\nmaking the peers\/ SWB visible did not alter overall cooperativeness, wealth,\ninter-connectedness, or SWB. In contrast, the visible SWB networked groups\nexhibited a higher number of communities and lower transitivity (the proportion\nof the cases where a peer of a peer is also a peer) than the invisible SWB\nnetworked groups. These phenomena are explained by an altered decision-making\npattern in the visible SWB networks: cooperators were less likely to connect\nwith cooperators and more likely to connect with defectors, and consequently,\ncooperators could not maintain their popularity or stay in the center of the\nnetworks.",
        "Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.",
        "Self-Admitted Technical Debt, or SATD, is a self-admission of technical debt\npresent in a software system. To effectively manage SATD, developers need to\nestimate its priority and assess the effort required to fix the described\ntechnical debt. About a quarter of descriptions of SATD in software systems\nexpress some form of negativity or negative emotions when describing technical\ndebt. In this paper, we report on an experiment conducted with 59 respondents\nto study whether negativity expressed in the description of SATD\n\\textbf{actually} affects the prioritization of SATD. The respondents are a mix\nof professional developers and students, and in the experiment, we asked\nparticipants to prioritize four vignettes: two expressing negativity and two\nexpressing neutral sentiment. To ensure realism, vignettes were based on\nexisting SATD. We find that negativity causes between one-third and half of\ndevelopers to prioritize SATD, in which negativity is expressed as having more\npriority. Developers affected by negativity when prioritizing SATD are twice as\nlikely to increase their estimation of urgency and 1.5 times as likely to\nincrease their estimation of importance and effort for SATD compared to the\nlikelihood of decreasing these prioritization scores. Our findings show how\ndevelopers actively use negativity in SATD to determine how urgently a\nparticular instance of TD should be addressed. However, our study also\ndescribes a gap in the actions and belief of developers. Even if 33% to 50% use\nnegativity to prioritize SATD, 67% of developers believe that using negativity\nas a proxy for priority is unacceptable. Therefore, we would not recommend\nusing negativity as a proxy for priority. However, we also recognize that\ndevelopers might unavoidably express negativity when describing technical debt.",
        "A solution to the suboptimal $H^\\infty$-control problem is given for a class\nof hyperbolic partial differential equations (PDEs). The first result of this\nmanuscript shows that the considered class of PDEs admits an equivalent\nrepresentation as an infinite-dimensional discrete-time system. Taking\nadvantage of this, this manuscript shows that it is equivalent to solve the\nsuboptimal $H^\\infty$-control problem for a finite-dimensional discrete-time\nsystem whose matrices are derived from the PDEs. After computing the solution\nto this much simpler problem, the solution to the original problem can be\ndeduced easily. In particular, the optimal compensator solution to the\nsuboptimal $H^\\infty$-control problem is governed by a set of hyperbolic PDEs,\nactuated and observed at the boundary. We illustrate our results with a\nboundary controlled and boundary observed vibrating string.",
        "Congenital Heart Disease (CHD) is one of the leading causes of fetal\nmortality, yet the scarcity of labeled CHD data and strict privacy regulations\nsurrounding fetal ultrasound (US) imaging present significant challenges for\nthe development of deep learning-based models for CHD detection. Centralised\ncollection of large real-world datasets for rare conditions, such as CHD, from\nlarge populations requires significant co-ordination and resource. In addition,\ndata governance rules increasingly prevent data sharing between sites. To\naddress these challenges, we introduce, for the first time, a novel\nprivacy-preserving, zero-shot CHD detection framework that formulates CHD\ndetection as a normality modeling problem integrated with model merging. In our\nframework dubbed Sparse Tube Ultrasound Distillation (STUD), each hospital site\nfirst trains a sparse video tube-based self-supervised video anomaly detection\n(VAD) model on normal fetal heart US clips with self-distillation loss. This\nenables site-specific models to independently learn the distribution of healthy\ncases. To aggregate knowledge across the decentralized models while maintaining\nprivacy, we propose a Divergence Vector-Guided Model Merging approach,\nDivMerge, that combines site-specific models into a single VAD model without\ndata exchange. Our approach preserves domain-agnostic rich spatio-temporal\nrepresentations, ensuring generalization to unseen CHD cases. We evaluated our\napproach on real-world fetal US data collected from 5 hospital sites. Our\nmerged model outperformed site-specific models by 23.77% and 30.13% in accuracy\nand F1-score respectively on external test sets.",
        "Let $W$ be an irreducible affine Weyl group, and let $\\mathsf{b}$ be a finite\nword over the alphabet of simple reflections of $W$. Fix a probability\n$p\\in(0,1)$. For each integer $K\\geq 0$, let $\\mathsf{sub}_p(\\mathsf{b}^K)$ be\nthe random subword of $\\mathsf{b}^K$ obtained by deleting each letter\nindependently with probability $1-p$. Let $v_p(\\mathsf{b}^K)$ be the element of\n$W$ represented by $\\mathsf{sub}_p(\\mathsf{b}^K)$. One can view\n$v_p(\\mathsf{b}^K)$ geometrically as a random alcove; in many cases, this\nalcove can be seen as the location after a certain amount of time of a random\nbilliard trajectory that, upon hitting a hyperplane in the Coxeter arrangement\nof $W$, reflects off of the hyperplane with probability $1-p$. We show that the\nasymptotic distribution of $v_p(\\mathsf{b}^K)$ is a central spherical\nmultivariate normal distribution with some variance $\\sigma_{\\mathsf{b}}^2$\ndepending on $\\mathsf{b}$ and $p$. We provide a formula to compute\n$\\sigma_{\\mathsf{b}}^2$ that is remarkably simple when $\\mathsf{b}$ contains\nonly one occurrence of the simple reflection that is not in the associated\nfinite Weyl group. As a corollary, we provide an asymptotic formula for\n$\\mathbb{E}[\\ell(v_p(\\mathsf{b}^K))]$, the expected Coxeter length of\n$v_p(\\mathsf{b}^K)$. For example, when $W=\\widetilde A_{r}$ and $\\mathsf{b}$\ncontains each simple reflection exactly once, we find that\n\\[\\lim_{K\\to\\infty}\\frac{1}{\\sqrt{K}}\\mathbb{E}[\\ell(v_p(\\mathsf{b}^K))]=\\sqrt{\\frac{2}{\\pi}r(r+1)\\frac{p}{1-p}}.\\]",
        "The increasing complexity of space systems, coupled with their critical\noperational roles, demands a robust, scalable, and sustainable security\nframework. This paper presents a novel system-of-systems approach for the\nupcoming Lunar Gateway. We demonstrate the application of the\nsecure-by-component approach to the two earliest deployed systems in the\nGateway, emphasizing critical security controls both internally and for\nexternal communication and connections. Additionally, we present a phased\napproach for the integration of Canadarm3, addressing the unique security\nchallenges that arise from both inter-system interactions and the arm's\nautonomous capabilities.",
        "Multi-agent influence diagrams (MAIDs) are probabilistic graphical models\nwhich represent strategic interactions between agents. MAIDs are equivalent to\nextensive form games (EFGs) but have a more compact and informative structure.\nHowever, MAIDs cannot, in general, represent settings of incomplete information\n-- wherein agents have different beliefs about the game being played, and\ndifferent beliefs about each-other's beliefs. In this paper, we introduce\nincomplete information MAIDs (II-MAIDs). We define both infinite and\nfinite-depth II-MAIDs and prove an equivalence relation to EFGs with incomplete\ninformation and no common prior over types. We prove that II-MAIDs inherit\nclassical equilibria concepts via this equivalence, but note that these\nsolution concepts are often unrealistic in the setting with no common prior\nbecause they violate common knowledge of rationality. We define a more\nrealistic solution concept based on recursive best-response. Throughout, we\ndescribe an example with a hypothetical AI agent undergoing evaluation to\nillustrate the applicability of II-MAIDs.",
        "We use contention resolution schemes (CRS) to derive algorithms for the fair\nrationing of a single resource when agents have stochastic demands. We aim to\nprovide ex-ante guarantees on the level of service provided to each agent, who\nmay measure service in different ways (Type-I, II, or III), calling for CRS\nunder different feasibility constraints (rank-1 matroid or knapsack). We are\nparticularly interested in two-order CRS where the agents are equally likely to\narrive in a known forward order or its reverse, which is motivated by online\nrationing at food banks.\n  In particular, we derive a two-order CRS for rank-1 matroids with guarantee\n$1\/(1+e^{-1\/2})\\approx 0.622$, which we prove is tight. This improves upon the\n$1\/2$ guarantee that is best-possible under a single order (Alaei, SIAM J.\nComput. 2014), while achieving separation with the $1-1\/e\\approx 0.632$\nguarantee that is possible for random-order CRS (Lee and Singla, ESA 2018).\nBecause CRS guarantees imply prophet inequalities, this also beats the\ntwo-order prophet inequality with ratio $(\\sqrt{5}-1)\/2\\approx 0.618$ from\n(Arsenis, SODA 2021), which was tight for single-threshold policies. Rank-1\nmatroids suffice to provide guarantees under Type-II or III service, but Type-I\nservice requires knapsack. Accordingly, we derive a two-order CRS for knapsack\nwith guarantee $1\/3$, improving upon the $1\/(3+e^{-2})\\approx 0.319$ guarantee\nthat is best-possible under a single order (Jiang et al., SODA 2022). To our\nknowledge, $1\/3$ provides the best-known guarantee for knapsack CRS even in the\noffline setting. Finally, we provide an upper bound of $1\/(2+e^{-1})\\approx\n0.422$ for two-order knapsack CRS, strictly smaller than the upper bound of\n$(1-e^{-2})\/2\\approx0.432$ for random-order knapsack CRS.",
        "In the face of intensified datafication and automation in public sector\nindustries, frameworks like design justice and the feminist practice of refusal\nprovide help to identify and mitigate structural harm and challenge inequities\nreproduced in digitized infrastructures. This paper applies those frameworks to\nemerging efforts across the U.S. healthcare industry to automate prior\nauthorization -- a process whereby insurance companies determine whether a\ntreatment or service is 'medically necessary' before agreeing to cover it.\nFederal regulatory interventions turn to datafication and automation to reduce\nthe harms of this widely unpopular process shown to delay vital treatments and\ncreate immense administrative burden for healthcare providers and patients.\nThis paper explores emerging prior authorization reforms as a case study,\napplying the frameworks of design justice and refusal to highlight the inherent\nconservatism of interventions oriented towards improving the user experience of\nextractive systems. I further explore how the abolitionist framework of\nnon-reformist reform helps to clarify alternative interventions that would\nmitigate the harms of prior authorization in ways that do not reproduce or\nextend the power of insurance companies. I propose a set of four tenets for\nnonreformist design to mitigate structural harms and advance design justice in\na broad set of domains."
      ]
    }
  },
  {
    "id":2412.19927,
    "research_type":"applied",
    "start_id":"b40",
    "start_title":"Physics guided neural networks for spatio-temporal superresolution of turbulent flows",
    "start_abstract":"Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Model-free simulations of turbulent reactive flows"
      ],
      "abstract":[
        "A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given."
      ],
      "categories":[
        "physics.flu-dyn"
      ]
    },
    "list":{
      "title":[
        "To Hedge or Not to Hedge: Optimal Strategies for Stochastic Trade Flow\n  Management",
        "Freelance Holography, Part II: Moving Boundary in Gauge\/Gravity\n  Correspondence",
        "Dynamically Learning to Integrate in Recurrent Neural Networks",
        "Automated DC Voltage and DC Resistance Real-time Multiple Standard for\n  Artifact Calibration of Calibrators and multimeters",
        "Two almost planetary mass survivors of common envelope evolution",
        "Electrochemically induced hyperfluorescence based on the formation of\n  charge-transfer excimers",
        "Computation of generalised magnetic coordinates asymptotically close to\n  the separatrix",
        "The diffuse extragalactic gamma-ray background radiation: star-forming\n  galaxies are not the dominant component",
        "Tight Analysis of Difference-of-Convex Algorithm (DCA) Improves\n  Convergence Rates for Proximal Gradient Descent",
        "An Open Source Validation System for Continuous Arterial Blood Pressure\n  Measuring Sensors",
        "Updated analysis of neutron magnetic form factor and the nucleon\n  transverse densities",
        "Power residue symbols and the exponential local-global principle",
        "In-medium bottomonium properties from lattice NRQCD calculations with\n  extended meson operators",
        "MXMap: A Multivariate Cross Mapping Framework for Causal Discovery in\n  Dynamical Systems",
        "Adsorption Behavior of Greenhouse Gases on Carbon Nanobelts: A\n  Semi-Empirical Tight-Binding Approach for Environmental Application",
        "Atomic Altermagnetism",
        "Positive weighted partitions generated by double series",
        "Running vacuum and H^4-inflation",
        "Interplay between tidal flows and magnetic fields in nonlinear\n  simulations of stellar and planetary convective envelopes",
        "Complementary signatures of $\\alpha-$attractor inflation in CMB and\n  cosmic string Gravitational Waves",
        "Improved Two-source Extractors against Quantum Side Information",
        "Exceptional Topology of Non-Hermitian Brillouin Klein Bottles",
        "Blob velocities and sizes in the Alcator C-Mod scrape-off layer for\n  ohmic and high confinement mode plasmas",
        "Power-Efficient Over-the-Air Aggregation with Receive Beamforming for\n  Federated Learning",
        "Reed-Solomon Codes Against Insertions and Deletions: Full-Length and\n  Rate-$1\/2$ Codes",
        "Causality constraints on radiative transfer",
        "Analysis of Learning-based Offshore Wind Power Prediction Models with\n  Various Feature Combinations",
        "Local Quantum Mechanical Prediction of the Singlet State",
        "A Fast and Robust Reformulation of the UVN-Flash Problem via Direct\n  Entropy Maximization"
      ],
      "abstract":[
        "This paper addresses the trade-off between internalisation and\nexternalisation in the management of stochastic trade flows. We consider agents\nwho must absorb flows and manage risk by deciding whether to warehouse it or\nhedge in the market, thereby incurring transaction costs and market impact.\nUnlike market makers, these agents cannot skew their quotes to attract\noffsetting flows and deter risk-increasing ones, leading to a fundamentally\ndifferent problem. Within the Almgren-Chriss framework, we derive\nalmost-closed-form solutions in the case of quadratic execution costs, while\nmore general cases require numerical methods. In particular, we discuss the\nchallenges posed by artificial boundary conditions when using classical\ngrid-based numerical PDE techniques and propose reinforcement learning methods\nas an alternative.",
        "We continue developing the freelance holography program, formulating\ngauge\/gravity correspondence where the gravity side is formulated on a space\nbounded by a generic timelike codimension-one surface inside AdS and arbitrary\nboundary conditions are imposed on the gravity fields on the surface. Our\nanalysis is performed within the Covariant Phase Space Formalism (CPSF). We\ndiscuss how a given boundary condition on the bulk fields on a generic boundary\nevolves as we move the boundary to another boundary inside AdS and work out how\nthis evolution is encoded in deformations of the holographic boundary theory.\nOur analyses here extend the extensively studied T$\\bar{\\text{T}}$-deformation\nby relaxing the boundary conditions at asymptotic AdS or at the cutoff surface\nto be any arbitrary one (besides Dirichlet). We discuss some of the\nimplications of our general freelance holography setting.",
        "Learning to remember over long timescales is fundamentally challenging for\nrecurrent neural networks (RNNs). While much prior work has explored why RNNs\nstruggle to learn long timescales and how to mitigate this, we still lack a\nclear understanding of the dynamics involved when RNNs learn long timescales\nvia gradient descent. Here we build a mathematical theory of the learning\ndynamics of linear RNNs trained to integrate white noise. We show that when the\ninitial recurrent weights are small, the dynamics of learning are described by\na low-dimensional system that tracks a single outlier eigenvalue of the\nrecurrent weights. This reveals the precise manner in which the long timescale\nassociated with white noise integration is learned. We extend our analyses to\nRNNs learning a damped oscillatory filter, and find rich dynamical equations\nfor the evolution of a conjugate pair of outlier eigenvalues. Taken together,\nour analyses build a rich mathematical framework for studying dynamical\nlearning problems salient for both machine learning and neuroscience.",
        "An automated temperature-controlled electrical DC voltage and DC resistance\nmultiple reference standard (MRS) has been developed by Measurements\nInternational (MI) with the scientific support from the Istituto Nazionale di\nRicerca Metrologica (INRIM). The MRS includes a 10 V, a 1 {\\Omega}, and a 10\nk{\\Omega} standards selectable via a switch unit. This setup allows the\nartifact calibration of high-end calibrators and multimeters used in\nlow-frequency electrical measurements. The two resistors are high-stability\nstandards from MI, while the 10 V standard is based on a low-noise circuit\ndeveloped by INRIM in collaboration with MI. A key innovation is the internal\nreal-time clock calendar, which displays the calibration values of the MRS\nstandards and their updated values internally calculated. This ensures reliable\nuse of the MRS standards over extended periods between calibrations,\neffectively minimizing uncertainties due to their drift. The standards are\nhoused in a thermal box, minimizing temperature variations. The MRS standards\nmeet the uncertainty requirements defined by calibrators and multimeters\nmanufacturers for artifact calibration and can also serve as laboratory\nreferences or travelling standards for interlaboratory comparisons (ILCs). MI\nis currently commercializing the MRS.",
        "White dwarfs are often found in close binaries with stellar or even\nsubstellar companions. It is generally thought that these compact binaries form\nvia common envelope evolution, triggered by the progenitor of the white dwarf\nexpanding after it evolved off the main-sequence and engulfing its companion.\nTo date, a handful of white dwarfs in compact binaries with substellar\ncompanions have been found, typically with masses greater than around 50\nM$_\\mathrm{Jup}$. Here we report the discovery of two eclipsing white dwarf\nplus brown dwarf binaries containing very low mass brown dwarfs. ZTF J1828+2308\nconsists of a hot ($15900\\pm75$ K) $0.610\\pm0.004$ M$_{\\odot}$ white dwarf in a\n2.7 hour binary with a $0.0186\\pm0.0008$ M$_{\\odot}$ ($19.5\\pm0.8$\nM$_\\mathrm{Jup}$) brown dwarf. ZTF J1230$-$2655 contains a cool ($10000\\pm110$\nK) $0.65\\pm0.02$ M$_{\\odot}$ white dwarf in a 5.7 hour binary with a companion\nthat has a mass of less than 0.0211 M$_{\\odot}$ (22.1 M$_\\mathrm{Jup}$). While\nthe brown dwarf in ZTF J1828+2308 has a radius consistent with its mass and\nage, ZTF J1230$-$2655 contains a roughly 20 per cent overinflated brown dwarf\nfor its age. We are only able to reconstruct the common envelope phase for\neither system if it occurred after the first thermal pulse, when the white\ndwarf progenitor had already lost a significant fraction of its original mass.\nThis is true even for very high common envelope ejection efficiencies\n($\\alpha_\\mathrm{CE}\\sim 1$), unless both systems have extremely low\nmetallicities. It may be that the lowest mass companions can only survive a\ncommon envelope phase if it occurs at this very late stage.",
        "Despite the extensive use of electrochemiluminescence in sensing\napplications, its potential in lighting and display technology has been\nconstrained by the low luminance and short operational lifetime of\nelectrochemiluminescence devices (ECLDs). Here, we demonstrate a substantial\nenhancement in the luminance, efficiency, and operational longevity of ECLDs by\nintroducing electrochemically induced hyperfluorescence (ECiHF) via\nelectrogeneration of charge-transfer (CT) excimers and subsequent energy\ntransfer to fluorescent acceptors. By assuming a double-decker arrangement of\nthe electron donor and acceptor groups, the molecule TpAT-tFFO supports\nsolution-state thermally activated delayed fluorescence from a CT excimer state\nand efficient energy transfer to the rubrene dye TBRb. Optimized ECLDs based on\nthis material combination achieve an unprecedented luminance of 6,220 cd\/m2 and\ntheir operational lifetime (LT50) at an initial luminance of 100 cd\/m2 exceeds\n20 minutes, more than 10-fold longer than other ECLDs with meaningful\nefficiency or brightness. We identify energy level alignment between the\nexcimer and the emitter as a crucial factor for efficient ECiHF. In mixtures\nwith energy gaps > 0.5 eV, electron transfer results in reduced performance and\nrenders the operation strongly dependent on applied voltage and frequency. By\ncontrast, spectroelectrochemical analysis reveals that devices with favorable\nenergy level alignment operate on a pure excimer mechanism across a wide range\nof frequencies. These findings highlight the innovative potential of ECiHF in\nimproving the performance of ECLD, which can be widely applied in future\ncommercial lighting solutions.",
        "Integrals to calculate generalised magnetic coordinates from an input\nmagnetic flux function asymptotically close to the separatrix are presented,\nand implemented in the GPEC\/DCON code suite. These integrals allow\ncharacterisation of the magnetic equilibrium of a diverted tokamak, in magnetic\ncoordinates, arbitrarily close to the last closed flux surface, avoiding the\nnumerical issues associated with calculating diverging field line integrals\nnear a magnetic x-point. These methods provide an important first step in the\ndevelopment of robust asymptotic equilibrium behaviour for spectral 3D MHD\ncodes at the separatrix.",
        "Star-forming galaxies (SFGs) are considered to be an important component of\nthe diffuse extragalactic gamma-ray background (EGB) radiation observed in 0.1\n-- 820 GeV, but their quantitative contribution has not yet been precisely\ndetermined. In this study, we aim to provide the currently most reliable\nestimate of the contribution of SFGs based on careful calibration with\ngamma-ray luminosities of nearby galaxies and physical quantities (star\nformation rate, stellar mass, and size) of galaxies observed by high-redshift\ngalaxy surveys. Our calculations are based on the latest database of particle\ncollision cross-sections and energy spectra of secondary particles, and take\ninto account not only hadronic but also leptonic processes with various\nradiation fields in a galaxy. We find that SFGs are not the dominant component\nof the unresolved EGB measured by Fermi; the largest contribution is around 50%\n-- 60% in the 1 -- 10 GeV region, and the contribution falls rapidly in lower\nand higher energy ranges. This result appears to contradict a previous study,\nwhich claimed that SFGs are the dominant component of the unresolved EGB, and\nthe origin of the discrepancy is examined. In calculations of cosmic-ray\nproduction, propagation, and interaction in a galaxy, we try models developed\nby two independent groups and find that they have little impact on EGB.",
        "We investigate a difference-of-convex (DC) formulation where the second term\nis allowed to be weakly convex. We examine the precise behavior of a single\niteration of the difference-of-convex algorithm (DCA), providing a tight\ncharacterization of the objective function decrease, distinguishing between six\ndistinct parameter regimes.\n  Our proofs, inspired by the performance estimation framework, are notably\nsimplified compared to related prior research. We subsequently derive sublinear\nconvergence rates for the DCA towards critical points, assuming at least one of\nthe functions is smooth.\n  Additionally, we explore the underexamined equivalence between proximal\ngradient descent (PGD) and DCA iterations, demonstrating how DCA, a\nparameter-free algorithm, without the need for a stepsize, serves as a tool for\nstudying the exact convergence rates of PGD.",
        "Measuring the blood pressure waveform is becoming a more frequently studied\narea. The development of sensor technologies opens many new ways to be able to\nmeasure high-quality signals. The development of such an aim-specific sensor\ncan be time-consuming, expensive, and difficult to test or validate with known\nand consistent waveforms. In this paper, we present an open source blood\npressure waveform simulator with an open source Python validation package to\nreduce development costs for early-stage sensor development and research. The\nsimulator mainly consists of 3D printed parts which technology has become a\nwidely available and cheap solution. The core part of the simulator is a 3D\nprinted cam that can be generated based on real blood pressure waveforms. The\nvalidation framework can create a detailed comparison between the signal\nwaveform used to design the cam and the measured time series from the sensor\nbeing validated. The presented simulator proved to be robust and accurate in\nshort- and long-term use, as it produced the signal waveform consistently and\naccurately. To validate this solution, a 3D force sensor was used, which was\nproven earlier to be able to measure high-quality blood pressure waveforms on\nthe radial artery at the wrist. The results showed high similarity between the\nmeasured and the nominal waveforms, meaning that comparing the normalized\nsignals, the RMSE value ranged from $0.0276 \\pm 0.0047$ to $0.0212 \\pm 0.0023$,\nand the Pearson correlation ranged from $0.9933 \\pm 0.0027$ to $0.9978 \\pm\n0.0005$. Our validation framework is available at\nhttps:\/\/github.com\/repat8\/cam-bpw-sim. Our hardware framework, which allows\nreproduction of the presented solution, is available at\nhttps:\/\/github.com\/repat8\/cam-bpw-sim-hardware. The entire design is an open\nsource project and was developed using free software.",
        "We provide an updated global extraction of the neutron magnetic form factor,\nincluding new extractions from $^3$H-$^3$He comparisons at Jefferson Lab. Our\nnew global fit addresses discrepancies between previous data sets at modest\nmomentum transfer by separating the uncertainties from world data into\nnormalization and uncorrelated uncertainties. We use this updated global fit,\nalong with previous fits for the other form factors, to extract the neutron and\nproton transverse charge and magnetization densities and their uncertainties.",
        "The exponential local-global principle, or Skolem conjecture, says: Suppose\nthat \\(b\\) is a positive integer, and that the sequence \\((u_{n})_{n =\n-\\infty}^{\\infty}\\) is such that every term is in \\(\\mathbb{Z}[1\/b]\\), the\nlinear recurrence \\(u_{n + d} = a_{1}u_{n + d - 1} + \\cdots + a_{d}u_{n}\\)\nholds for all integers \\(n\\), and every root of \\(x^{d} - a_{1}x^{d - 1} -\na_{2}x^{d - 2} - \\cdots - a_{d}\\) is nonzero and simple; then there is no zero\nterm \\(u_{n}\\) if and only if, for some integer \\(m\\) that is larger than \\(1\\)\nand relatively prime to \\(b\\), every term \\(u_{n}\\) is not in\n\\(m\\mathbb{Z}[1\/b]\\).\n  Particular cases of the conjecture are known, but the general conjecture is\nopen. This paper proves some apparently new quadratic and degenerate cubic\ncases of the exponential local-global principle via power residue symbols.\n  This work was presented at the Stellenbosch Number Theory Conference 2025 in\nJanuary 2025 at Stellenbosch University; much of the work was also presented at\nthe 67th Annual Congress of the South African Mathematical Society in December\n2024 at the University of Pretoria.",
        "We calculate the temperature dependence of bottomonium correlators in\n(2+1)-flavor lattice QCD with the aim to constrain in-medium properties of\nbottomonia at high temperature. The lattice calculations are performed using\nHISQ action with physical strange quark mass and light quark masses twenty\ntimes smaller than the strange quark mass at two lattice spacings $a=0.0493$ fm\nand $0.0602$ fm, and temporal extents $N_{\\tau}=16-30$, corresponding to the\ntemperatures $T=133-250$ MeV. We use a tadpole-improved NRQCD action including\nspin-dependent $v^6$ corrections for the heavy quarks and extended meson\noperators in order to be sensitive to in-medium properties of the bottomonium\nstates of interest. We find that within estimated errors the bottomonium masses\ndo not change compared to their vacuum values for all temperatures under our\nconsideration; however, we find different nonzero widths for the various\nbottomonium states.",
        "Convergent Cross Mapping (CCM) is a powerful method for detecting causality\nin coupled nonlinear dynamical systems, providing a model-free approach to\ncapture dynamic causal interactions. Partial Cross Mapping (PCM) was introduced\nas an extension of CCM to address indirect causality in three-variable systems\nby comparing cross-mapping quality between direct cause-effect mapping and\nindirect mapping through an intermediate conditioning variable. However, PCM\nremains limited to univariate delay embeddings in its cross-mapping processes.\nIn this work, we extend PCM to the multivariate setting, introducing multiPCM,\nwhich leverages multivariate embeddings to more effectively distinguish\nindirect causal relationships. We further propose a multivariate cross-mapping\nframework (MXMap) for causal discovery in dynamical systems. This two-phase\nframework combines (1) pairwise CCM tests to establish an initial causal graph\nand (2) multiPCM to refine the graph by pruning indirect causal connections.\nThrough experiments on simulated data and the ERA5 Reanalysis weather dataset,\nwe demonstrate the effectiveness of MXMap. Additionally, MXMap is compared\nagainst several baseline methods, showing advantages in accuracy and causal\ngraph refinement.",
        "This research investigates the adsorption characteristics of carbon nanobelts\n(CNB) and Mobius carbon nanobelts (MCNB) interacting with various greenhouse\ngases, including NH3, CO2, CO, H2S, CH4, CH3OH, NO2, NO, and COCl2. The study\nemploys semi-empirical tight-binding calculations via xTB software,\ncomplemented by topological analysis using MULTIWFN software. Comparative\nanalysis reveals MCNB's superior adsorption properties, particularly for\nspecific gases. Notable adsorption energies for MCNB were measured at -1.595eV,\n-0.669eV, and -0.637eV for NO, COCl2, and NO2, respectively, significantly\nexceeding the corresponding CNB values of -0.636eV, -0.449eV, and -0.438eV. The\ninvestigation of desorption kinetics demonstrates rapid recovery times\n(sub-millisecond) for most gas-nanobelt interactions, with the notable\nexception of the MCNB+NO system, which exhibits persistent bonding. Topological\nanalysis confirms chemisorption mechanisms for NO, COCl2, and NO2 on both\nnanobelt variants, characterized by complex hybridizations of covalent and\nnon-covalent interactions. Molecular dynamics simulations conducted in both\npacked configurations and dry air mixtures demonstrate the nanobelts' effective\ngas-attracting properties, maintaining consistent capture performance across\ndifferent environmental conditions. These findings establish carbon nanobelts,\nparticularly the Mobius configuration, as promising candidates for greenhouse\ngas capture technologies, offering potential applications in environmental\nremediation and climate change mitigation strategies.",
        "Altermagnetism has been recently experimentally verified by photoemission\nmapping of the spin order in momentum space in MnTe and CrSb, which feature two\nanisotropic sublattices with antiparallel magnetic dipole moments. In this\nwork, we explicitly demonstrate the presence of an even-parity ferroically\nordered non-dipolar spin density on the atomic sites, i.e. atomic\naltermagnetism, in MnTe, $La_2O_3Mn_2Se_2$ and $Ba_2CaOsO_6$. We do so through\nspin-symmetry analysis and partial-wave decomposition of the spin density\nobtained by first-principles calculations. In MnTe we show a ferroically\nordered g-wave form factor in the spin density around the Mn site. In the\n$A_2O_3M_2Se_2$ family (A= La, Sr and M= Mn, Fe, Co), we show that there is a\nferroically ordered d-wave form factor coexisting with the antiferroic magnetic\ndipoles in the M site, while the O site shows no dipole but a pure d-wave\natomic spin density. In the Mott-insulating candidate $Ba_2CaOsO_6$, as a key\nresult, we reveal a pure form of atomic altermagnetism - absent of any dipolar\nsublattice order. This highlights that the altermagnetic order can exist\nwithout a N\\'eel vector formed by antiferroic dipole moments on an even number\nof crystal sublattices, underlining its distinction from collinear N\\'eel\nantiferromagnetic order. Our calculations predict that $La_2O_3Mn_2Se_2$ and\n$Ba_2CaOsO_6$ can exhibit giant spin-splitter angles of up to 42{\\deg} and\n26{\\deg} respectively, thus demonstrating the possibility of large\naltermagnetic responses without requiring the staggered N\\'eel order of local\ndipole moments.",
        "We investigate some weighted integer partitions whose generating functions\nare double-series. We will establish closed formulas for these $q$-double\nseries and deduce that their coefficients are non-negative. This leads to\ninequalities among integer partitions.",
        "Recent studies of QFT in cosmological spacetime indicate that the speeding up\nof the present universe may not just be associated with a rigid cosmological\nterm but with a running one that evolves with the expansion rate:\n$\\Lambda=\\Lambda(H)$. This running is inherited from the cosmic evolution of\nthe vacuum energy density (VED), $\\rho_{\\rm vac}$, which is sensitive to\nquantum effects in curved spacetime that ultimately trigger that running. The\nVED is a function of the Hubble rate and its time derivatives: $\\rho_{\\rm\nvac}=\\rho_{\\rm vac}(H, \\dot{H},\\ddot{H},...)$. Two nearby points of the cosmic\nevolution during the FLRW epoch are smoothly related as $\\delta\\rho_{\\rm\nvac}\\sim {\\cal O}(H^2)$. In the very early universe, in contrast, the higher\npowers of the Hubble rate take over and bring about a period of fast inflation.\nThey originate from quantum effects on the effective action of vacuum, which we\ncompute. Herein we focus on the lowest possible power for inflation to occur:\n$H^4$. During the inflationary phase, $H$ remains approximately constant and\nvery large. Subsequently, the universe enters the usual FLRW radiation epoch.\nThis new mechanism (`RVM-inflation') is not based on any supplementary\n`inflaton' field, it is fueled by pure QFT effects on the dynamical background\nand is different from Starobinsky's inflation, in which $H$ is never constant.",
        "Stars and planets in close systems are magnetised but the influence of\nmagnetic fields on their tidal responses (and vice versa) and dissipation rates\nhas not been well explored. We present exploratory nonlinear\nmagnetohydrodynamical (MHD) simulations of tidally-excited inertial waves in\nconvective envelopes. These waves probably provide the dominant contribution to\ntidal dissipation in several astrophysical settings, including tidal\ncircularisation of solar-type binary stars and hot Jupiters, and orbital\nmigration of the moons of Jupiter and Saturn. We model convective envelopes as\nincompressible magnetised fluids in spherical shells harbouring an initially\n(rotationally-aligned) dipolar magnetic field. We find that depending on its\nstrength (quantified by its Lehnert number Le) and the magnetic Prandtl number\nPm, the magnetic field can either deeply modify the tidal response or be\nsubstantially altered by tidal flows. Simulations with small Le exhibit strong\ntidally-generated differential rotation (zonal flows) for sufficiently large\ntidal amplitudes, such that both the amplitude and topology of the initial\nmagnetic field are tidally impacted. In contrast, strong magnetic fields can\ninhibit these zonal flows through large-scale magnetic torques, and by Maxwell\nstresses arising from magneto-rotational instability, which we identify and\ncharacterise in our simulations, along with the role of torsional Alfv\\'en\nwaves. Without tidally-driven zonal flows, the resulting tidal dissipation is\nclose to the linear predictions. We quantify the transition Le as a function of\nPm, finding it to be comparable to realistic values found in solar-like stars,\nsuch that we predict complex interactions between tidal flows and magnetic\nfields.",
        "When cosmic strings are formed during inflation, they regrow to reach a\nscaling regime, leaving distinct imprints on the stochastic gravitational wave\nbackground (SGWB). Such signatures, associated with specific primordial\nfeatures, can be detected by upcoming gravitational wave observatories, such as\nthe LISA and Einstein Telescope (ET). Our analysis explores scenarios in which\ncosmic strings form either before or during inflation. We examine how the\nnumber of e-folds experienced by cosmic strings during inflation correlates\nwith the predictions of inflationary models observable in cosmic microwave\nbackground (CMB) measurements. This correlation provides a testable link\nbetween inflationary physics and the associated gravitational wave signals in a\ncomplementary manner. Focusing on $\\alpha$-attractor models of inflation, with\nthe Polynomial $\\alpha$-attractor serving as an illustrative example, we find\nconstraints, for instance, on the spectral index $n_s$ to $0.962 \\lesssim n_s\n\\lesssim 0.972$ for polynomial exponent $n=1$, $0.956 \\lesssim n_s \\lesssim\n0.968$ for $n=2$, $0.954 \\lesssim n_s \\lesssim 0.965$ for $n=3$, and $0.963\n\\lesssim n_s \\lesssim 0.964$ for $n=4$, which along with the GW signals from\nLISA, are capable of detecting local cosmic strings that have experienced $\\sim\n34 - 47$ e-folds of inflation consistent with current Planck data and are also\ntestable in upcoming CMB experiments such as LiteBIRD and CMB-S4.",
        "Two-source extractors aim to extract randomness from two independent sources\nof weak randomness. It has been shown that any two-source extractor which is\nsecure against classical side information remains secure against quantum side\ninformation. Unfortunately, this generic reduction comes with a significant\npenalty to the performance of the extractor. In this paper, we show that the\ntwo-source extractor from Dodis et al. performs equally well against quantum\nside information as in the classical realm, surpassing previously known results\nabout this extractor. Additionally, we derive a new quantum XOR-Lemma which\nallows us to re-derive the generic reduction but also allows for improvements\nfor a large class of extractors.",
        "Exceptional points (EPs) are prominent non-Hermitian band degeneracies that\ngive rise to a variety of intriguing and unconventional phenomena. Similar to\nWeyl and Dirac points, EPs carry topological charges and comply with the\ncelebrated fermion doubling theorems in lattices. Beyond these characteristics,\nEPs exhibit more exotic topological properties, particularly non-Abelian\nbraiding topologies not seen in conventional degeneracies. Here, we investigate\nthese foundational concepts of EPs in two-dimensional non-Hermitian lattices\nwhere the fundamental domain of the Brillouin zone is a Klein bottle, rather\nthan a torus assumed in previous studies. We find that EPs do not necessarily\nappear in pairs with opposite topological charges in the Brillouin Klein\nbottle, thus violating the fermion doubling theorem. The violation occurs\nbecause, without crossing the boundary, the sum of the topological charges of\nEPs is in fact an even number rather than zero. Moreover, we uncover unique\nbraiding topologies of EPs that cannot be captured by existing theories.\nSpecifically, the composite braidings around all EPs equals the braiding along\nthe boundary of the Brillouin Klein bottle. This novel braiding topology\nfurther confirms the failure of the fermion doubling theorem, and allows us to\nexplore the non-Abelian braidings of EPs beyond the scope of topological\ncharges. Our work highlights the fundamental role of Brillouin-zone topology in\nnon-Hermitian systems.",
        "An improved time delay estimation method is used to calculate the velocity of\ncross-field blob motion in the scrape-off layer of Alcator C-Mod for an ohmic\nand two high confinement (H-mode) plasmas; an edge localized mode free and an\nenhanced D-alpha H-mode. The gas puff imaging data analysis results are\ninterpreted in the framework of a stochastic model that describes the\nfluctuations as a super-position of uncorrelated blob-like structures. In all\nconfinement modes investigated, the scrape-off layer is dominated by large\namplitude, blob-like filaments moving radially outwards with velocities in the\nrange from 400 to 1000 m\/s. Blobs in high confinement mode plasmas have similar\nvelocities and sizes as in ohmic plasma, which is consistent with the close\nsimilarity of conditionally averaged burst shapes and frequency spectra for the\nconfinement modes investigated.",
        "This paper studies power-efficient uplink transmission design for federated\nlearning (FL) that employs over-the-air analog aggregation and multi-antenna\nbeamforming at the server. We jointly optimize device transmit weights and\nreceive beamforming at each FL communication round to minimize the total device\ntransmit power while ensuring convergence in FL training. Through our\nconvergence analysis, we establish sufficient conditions on the aggregation\nerror to guarantee FL training convergence. Utilizing these conditions, we\nreformulate the power minimization problem into a unique bi-convex structure\nthat contains a transmit beamforming optimization subproblem and a receive\nbeamforming feasibility subproblem. Despite this unconventional structure, we\npropose a novel alternating optimization approach that guarantees monotonic\ndecrease of the objective value, to allow convergence to a partial optimum. We\nfurther consider imperfect channel state information (CSI), which requires\naccounting for the channel estimation errors in the power minimization problem\nand FL convergence analysis. We propose a CSI-error-aware joint beamforming\nalgorithm, which can substantially outperform one that does not account for\nchannel estimation errors. Simulation with canonical classification datasets\ndemonstrates that our proposed methods achieve significant power reduction\ncompared to existing benchmarks across a wide range of parameter settings,\nwhile attaining the same target accuracy under the same convergence rate.",
        "The performance of Reed-Solomon codes (RS codes, for short) in the presence\nof insertion and deletion errors has been studied recently in several papers.\nIn this work, we further study this intriguing mathematical problem, focusing\non two regimes. First, we study the question of how well full-length RS codes\nperform against insertions and deletions. For 2-dimensional RS codes, we fully\ncharacterize which codes cannot correct even a single insertion or deletion and\nshow that (almost) all 2-dimensional RS codes correct at least $1$ insertion or\ndeletion error. Moreover, for large enough field size $q$, and for any $k \\ge\n2$, we show that there exists a full-length $k$-dimensional RS code that\ncorrects $q\/10k$ insertions and deletions. Second, we focus on rate $1\/2$ RS\ncodes that can correct a single insertion or deletion error. We present a\npolynomial time algorithm that constructs such codes for $q = O(k^4)$. This\nresult matches the existential bound given in \\cite{con2023reed}.",
        "The standard formula, due to Spiegel, for the smoothing of temperature\nfluctuations by radiative transfer is unstable in relativity. This is due to\nthe fact that Spiegel neglected the transit time of light, thereby allowing the\ntransport coefficients to move outside the convex geometry compatible with\ncausality (the \"hydrohedron\"). Here, we fix this pathology. First, we prove\nthat the linearized radiative transfer equations are causal and covariantly\nstable by construction. Then, we repeat Spiegel's calculation accounting for\nthe finite speed of photons. We find that the full transfer problem can be\nsolved analytically. All the infinite (exact) transport coefficients arising\nfrom it fall inside the hydrohedron. Our analysis also accounts for isotropic\nscattering.",
        "Accurate wind speed prediction is crucial for designing and selecting sites\nfor offshore wind farms. This paper investigates the effectiveness of various\nmachine learning models in predicting offshore wind power for a site near the\nGulf of Mexico by analyzing meteorological data. After collecting and\npreprocessing meteorological data, nine different input feature combinations\nwere designed to assess their impact on wind power predictions at multiple\nheights. The results show that using wind speed as the output feature improves\nprediction accuracy by approximately 10% compared to using wind power as the\noutput. In addition, the improvement of multi-feature input compared with\nsingle-feature input is not obvious mainly due to the poor correlation among\nkey features and limited generalization ability of models. These findings\nunderscore the importance of selecting appropriate output features and\nhighlight considerations for using machine learning in wind power forecasting,\noffering insights that could guide future wind power prediction models and\nconversion techniques.",
        "We deduce the quantum mechanical prediction of $-{\\bf a}\\cdot{\\bf b}$ for the\nsinglet spin state employing local measurement functions following Bell's\napproach. Our derivation is corroborated through a computational simulation\nconducted via the Mathematica programming environment.",
        "We investigate the phase equilibrium problem for multicomponent mixtures\nunder specified internal energy (U), volume (V), and mole numbers (N1,N2, . . .\n,Nn), commonly known as the UVN-flash problem. While conventional phase\nequilibrium calculations typically use pressure-temperature-mole number (PTN)\nspecifications, the UVN formulation is essential for dynamic simulations of\nclosed systems and energy balance computations. Existing approaches, including\nthose based on iterative pressure-temperature updates and direct entropy\nmaximization, suffer from computational inefficiencies due to nested iterations\nand reliance on inner Newton solvers. In this work, we present a novel\nreformulation of the UVN-flash problem as a direct entropy maximization problem\nthat eliminates the need for inner Newton iterations, addressing key\ncomputational bottlenecks. We derive two new novel formulations: 1) a\nformulation based on entropy and internal energy and (2) an alternative\nformulation based on Helmholtz free energy. We begin with a stability analysis\nframework, followed by a reformulation of the UVN flash problem in natural\nvariables. We then introduce our novel approach and discuss the numerical\nmethods used, including gradient and Hessian computations. The proposed method\nis validated against benchmark cases, demonstrating improved efficiency and\nrobustness."
      ]
    }
  },
  {
    "id":2412.10196,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Quality assurance procedures for mass spectrometry untargeted metabolomics. a review",
    "start_abstract":"Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "XGBoost: A Scalable Tree Boosting System"
      ],
      "abstract":[
        "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Target Selection for the Redshift-Limited WAVES-Wide with Machine\n  Learning",
        "Auxiliary Discrminator Sequence Generative Adversarial Networks\n  (ADSeqGAN) for Few Sample Molecule Generation",
        "Competing Effects of Local Solvation Structures on Chemical Shift\n  Changes of Liquid Electrolyte",
        "Diffusion Models for Cayley Graphs",
        "Funzac at CoMeDi Shared Task: Modeling Annotator Disagreement from\n  Word-In-Context Perspectives",
        "A Comprehensive Reanalysis of K2-18 b's JWST NIRISS+NIRSpec Transmission\n  Spectrum",
        "Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles",
        "Equilibrium Moment Analysis of It\\^o SDEs",
        "NaFM: Pre-training a Foundation Model for Small-Molecule Natural\n  Products",
        "DAST: Difficulty-Adaptive Slow-Thinking for Large Reasoning Models",
        "Hypersurfaces passing through the Galois orbit of a point",
        "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs",
        "Optimal joint reconstruction from CMB observations: application to\n  cosmic birefringence, patchy reionization and CMB lensing",
        "Wireless Network Topology Inference: A Markov Chains Approach",
        "Learning the RoPEs: Better 2D and 3D Position Encodings with STRING",
        "Specific Aspects of Intellectual Property Management in the\n  Knowledge-Based Economy",
        "Challenges and Innovations in LLM-Powered Fake News Detection: A\n  Synthesis of Approaches and Future Directions",
        "GSBA$^K$: $top$-$K$ Geometric Score-based Black-box Attack",
        "Reducing Hallucinations in Language Model-based SPARQL Query Generation\n  Using Post-Generation Memory Retrieval",
        "Monolayer transition metal dichalcogenides under finite-pulse polarized\n  radiation",
        "A Fair Federated Learning Framework for Collaborative Network Traffic\n  Prediction and Resource Allocation",
        "Highly correlated electronic state in a ferrimagnetic quadruple\n  perovskite CuCu$_3$Fe$_2$Re$_2$O$_{12}$",
        "Large Model Empowered Metaverse: State-of-the-Art, Challenges and\n  Opportunities",
        "Probing muonic force with periastron advance in binary pulsar systems",
        "$\\Lambda$(1405) in the flavor SU(3) limit using a separable potential in\n  the HAL QCD method",
        "Wettability and sp2\/sp3 ratio effects on supercapacitor performance of\n  N-doped hydrogenated amorphous Carbon Nanofoam",
        "Kinks of fractional $\\phi^4$ models: existence, uniqueness,\n  monotonicity, stability, and sharp asymptotics",
        "Inter3D: A Benchmark and Strong Baseline for Human-Interactive 3D Object\n  Reconstruction",
        "Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented\n  Conversational AI Through Accountability Modeling"
      ],
      "abstract":[
        "The forthcoming Wide Area Vista Extragalactic Survey (WAVES) on the 4-metre\nMulti-Object Spectroscopic Telescope (4MOST) has a key science goal of probing\nthe halo mass function to lower limits than possible with previous surveys. For\nthat purpose, in its Wide component, galaxies targetted by WAVES will be\nflux-limited to $Z<21.1$ mag and will cover the redshift range of $z<0.2$, at a\nspectroscopic success rate of $\\sim95\\%$. Meeting this completeness\nrequirement, when the redshift is unknown a priori, is a challenge. We solve\nthis problem with supervised machine learning to predict the probability of a\ngalaxy falling within the WAVES-Wide redshift limit, rather than estimate each\nobject's redshift. This is done by training an XGBoost tree-based classifier to\ndecide if a galaxy should be a target or not. Our photometric data come from\n9-band VST+VISTA observations, including KiDS+VIKING surveys. The redshift\nlabels for calibration are derived from an extensive spectroscopic sample\noverlapping with KiDS and ancillary fields. Our current results indicate that\nwith our approach, we should be able to achieve the completeness of $\\sim95\\%$,\nwhich is the WAVES success criterion.",
        "In this work, we introduce Auxiliary Discriminator Sequence Generative\nAdversarial Networks (ADSeqGAN), a novel approach for molecular generation in\nsmall-sample datasets. Traditional generative models often struggle with\nlimited training data, particularly in drug discovery, where molecular datasets\nfor specific therapeutic targets, such as nucleic acids binders and central\nnervous system (CNS) drugs, are scarce. ADSeqGAN addresses this challenge by\nintegrating an auxiliary random forest classifier as an additional\ndiscriminator into the GAN framework, significantly improves molecular\ngeneration quality and class specificity.\n  Our method incorporates pretrained generator and Wasserstein distance to\nenhance training stability and diversity. We evaluate ADSeqGAN on a dataset\ncomprising nucleic acid-targeting and protein-targeting small molecules,\ndemonstrating its superior ability to generate nucleic acid binders compared to\nbaseline models such as SeqGAN, ORGAN, and MolGPT. Through an oversampling\nstrategy, ADSeqGAN also significantly improves CNS drug generation, achieving a\nhigher yield than traditional de novo models. Critical assessments, including\ndocking simulations and molecular property analysis, confirm that\nADSeqGAN-generated molecules exhibit strong binding affinities, enhanced\nchemical diversity, and improved synthetic feasibility.\n  Overall, ADSeqGAN presents a novel framework for generative molecular design\nin data-scarce scenarios, offering potential applications in computational drug\ndiscovery. We have demonstrated the successful applications of ADSeqGAN in\ngenerating synthetic nucleic acid-targeting and CNS drugs in this work.",
        "Understanding the solvation structure of electrolytes is critical for\noptimizing the electrochemical performance of rechargeable batteries, as it\ndirectly influences properties such as ionic conductivity, viscosity, and\nelectrochemical stability. The highly complex structures and strong\ninteractions in high-concentration electrolytes make accurate modeling and\ninterpretation of their ``structure-property\" relationships even more\nchallenging with spectroscopic methods. In this study, we present a machine\nlearning-based approach to predict dynamic $^7$Li NMR chemical shifts in\nLiFSI\/DME electrolyte solutions. Additionally, we provide a comprehensive\nstructural analysis to interpret the observed chemical shift behavior in our\nexperiments, particularly the abrupt changes in $^7$Li chemical shifts at high\nconcentrations. Using advanced modeling techniques, we quantitatively establish\nthe relationship between molecular structure and NMR spectra, offering critical\ninsights into solvation structure assignments. Our findings reveal the\ncoexistence of two competing local solvation structures that shift in dominance\nas electrolyte concentration approaches the concentrated limit, leading to\nanomalous reverse of $^7$Li NMR chemical shift in our experiment. This work\nprovides a detailed molecular-level understanding of the intricate solvation\nstructures probed by NMR spectroscopy, leading the way for enhanced electrolyte\ndesign.",
        "We review the problem of finding paths in Cayley graphs of groups and group\nactions, using the Rubik's cube as an example, and we list several more\nexamples of significant mathematical interest. We then show how to formulate\nthese problems in the framework of diffusion models. The exploration of the\ngraph is carried out by the forward process, while finding the target nodes is\ndone by the inverse backward process. This systematizes the discussion and\nsuggests many generalizations. To improve exploration, we propose a ``reversed\nscore'' ansatz which substantially improves over previous comparable\nalgorithms.",
        "In this work, we evaluate annotator disagreement in Word-in-Context (WiC)\ntasks exploring the relationship between contextual meaning and disagreement as\npart of the CoMeDi shared task competition. While prior studies have modeled\ndisagreement by analyzing annotator attributes with single-sentence inputs,\nthis shared task incorporates WiC to bridge the gap between sentence-level\nsemantic representation and annotator judgment variability. We describe three\ndifferent methods that we developed for the shared task, including a feature\nenrichment approach that combines concatenation, element-wise differences,\nproducts, and cosine similarity, Euclidean and Manhattan distances to extend\ncontextual embedding representations, a transformation by Adapter blocks to\nobtain task-specific representations of contextual embeddings, and classifiers\nof varying complexities, including ensembles. The comparison of our methods\ndemonstrates improved performance for methods that include enriched and\ntask-specfic features. While the performance of our method falls short in\ncomparison to the best system in subtask 1 (OGWiC), it is competitive to the\nofficial evaluation results in subtask 2 (DisWiC).",
        "Sub-Neptunes are the most common type of planet in our galaxy. Interior\nstructure models suggest that the coldest sub-Neptunes could host liquid water\noceans underneath their hydrogen envelopes - sometimes called 'hycean' planets.\nJWST transmission spectra of the $\\sim$ 250 K sub-Neptune K2-18 b were recently\nused to report detections of CH$_4$ and CO$_2$, alongside weaker evidence of\n(CH$_3$)$_2$S (dimethyl sulfide, or DMS). Atmospheric CO$_2$ was interpreted as\nevidence for a liquid water ocean, while DMS was highlighted as a potential\nbiomarker. However, these notable claims were derived using a single data\nreduction and retrieval modeling framework, which did not allow for standard\nrobustness tests. Here we present a comprehensive reanalysis of K2-18 b's JWST\nNIRISS SOSS and NIRSpec G395H transmission spectra, including the first\nanalysis of the second-order NIRISS SOSS data. We incorporate multiple\nwell-tested data reduction pipelines and retrieval codes, spanning 60 different\ndata treatments and over 250 atmospheric retrievals. We confirm the detection\nof CH$_4$ ($\\approx$ 4$\\sigma$), with a volume mixing ratio of log CH$_4$ =\n$-1.15^{+0.40}_{-0.52}$, but we find no statistically significant or reliable\nevidence for CO$_2$ or DMS. Finally, we quantify the observed atmospheric\ncomposition using photochemical-climate and interior models, demonstrating that\nour revised composition of K2-18 b can be explained by an oxygen-poor\nmini-Neptune without requiring a liquid water surface or life.",
        "User simulators are crucial for replicating human interactions with dialogue\nsystems, supporting both collaborative training and automatic evaluation,\nespecially for large language models (LLMs). However, existing simulators often\nrely solely on text utterances, missing implicit user traits such as\npersonality, speaking style, and goals. In contrast, persona-based methods lack\ngeneralizability, as they depend on predefined profiles of famous individuals\nor archetypes. To address these challenges, we propose User Simulator with\nimplicit Profiles (USP), a framework that infers implicit user profiles from\nhuman-machine conversations and uses them to generate more personalized and\nrealistic dialogues. We first develop an LLM-driven extractor with a\ncomprehensive profile schema. Then, we refine the simulation through\nconditional supervised fine-tuning and reinforcement learning with cycle\nconsistency, optimizing it at both the utterance and conversation levels.\nFinally, we adopt a diverse profile sampler to capture the distribution of\nreal-world user profiles. Experimental results demonstrate that USP outperforms\nstrong baselines in terms of authenticity and diversity while achieving\ncomparable performance in consistency. Furthermore, dynamic multi-turn\nevaluations based on USP strongly align with mainstream benchmarks,\ndemonstrating its effectiveness in real-world applications.",
        "Stochastic differential equations have proved to be a valuable governing\nframework for many real-world systems which exhibit ``noise'' or randomness in\ntheir evolution. One quality of interest in such systems is the shape of their\nequilibrium probability distribution, if such a thing exists. In some cases a\nstraightforward integral equation may yield this steady-state distribution, but\nin other cases the equilibrium distribution exists and yet that integral\nequation diverges. Here we establish a new equilibrium-analysis technique based\non the logic of finite-timestep simulation which allows us to glean information\nabout the equilibrium regardless -- in particular, a relationship between the\nraw moments of the equilibrium distribution. We utilize this technique to\nextract information about one such equilibrium resistant to direct definition.",
        "Natural products, as metabolites from microorganisms, animals, or plants,\nexhibit diverse biological activities, making them crucial for drug discovery.\nNowadays, existing deep learning methods for natural products research\nprimarily rely on supervised learning approaches designed for specific\ndownstream tasks. However, such one-model-for-a-task paradigm often lacks\ngeneralizability and leaves significant room for performance improvement.\nAdditionally, existing molecular characterization methods are not well-suited\nfor the unique tasks associated with natural products. To address these\nlimitations, we have pre-trained a foundation model for natural products based\non their unique properties. Our approach employs a novel pretraining strategy\nthat is especially tailored to natural products. By incorporating contrastive\nlearning and masked graph learning objectives, we emphasize evolutional\ninformation from molecular scaffolds while capturing side-chain information.\nOur framework achieves state-of-the-art (SOTA) results in various downstream\ntasks related to natural product mining and drug discovery. We first compare\ntaxonomy classification with synthesized molecule-focused baselines to\ndemonstrate that current models are inadequate for understanding natural\nsynthesis. Furthermore, by diving into a fine-grained analysis at both the gene\nand microbial levels, NaFM demonstrates the ability to capture evolutionary\ninformation. Eventually, our method is experimented with virtual screening,\nillustrating informative natural product representations that can lead to more\neffective identification of potential drug candidates.",
        "Recent advancements in slow-thinking reasoning models have shown exceptional\nperformance in complex reasoning tasks. However, these models often exhibit\noverthinking-generating redundant reasoning steps for simple problems, leading\nto excessive computational resource usage. While current mitigation strategies\nuniformly reduce reasoning tokens, they risk degrading performance on\nchallenging tasks that require extended reasoning. This paper introduces\nDifficulty-Adaptive Slow-Thinking (DAST), a novel framework that enables models\nto autonomously adjust the length of Chain-of-Thought(CoT) based on problem\ndifficulty. We first propose a Token Length Budget (TLB) metric to quantify\ndifficulty, then leveraging length-aware reward shaping and length preference\noptimization to implement DAST. DAST penalizes overlong responses for simple\ntasks while incentivizing sufficient reasoning for complex problems.\nExperiments on diverse datasets and model scales demonstrate that DAST\neffectively mitigates overthinking (reducing token usage by over 30\\% on\naverage) while preserving reasoning accuracy on complex problems.",
        "Asgarli, Ghioca, and Reichstein recently proved that if $K$ is a field with\n$|K|>2$, then for any positive integers $d$ and $n$, and separable field\nextension $L\/K$ with degree $m=\\binom{n+d}{d}$, there exists a point $P\\in\n\\mathbb{P}^n(L)$ which does not lie on any degree $d$ hypersurface defined over\n$K$. They asked whether the result holds when $|K| = 2$. We answer their\nquestion in the affirmative by combining various ideas from arithmetic\ngeometry. More generally, we show that for each positive integer $r$ and\nseparable field extension $L\/K$ with degree $r$, there exists a point $P \\in\n\\mathbb{P}^n(L)$ such that the vector space of degree $d$ forms over $K$ that\nvanish at $P$ has the expected dimension. We also discuss applications to\nlinear systems of hypersurfaces with special properties.",
        "Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or\nnonsensical information) when serving as AI assistants in various domains.\nSince hallucinations always come with truthful content in the LLM responses,\nprevious factuality alignment methods that conduct response-level preference\nlearning inevitably introduced noises during training. Therefore, this paper\nproposes a fine-grained factuality alignment method based on Direct Preference\nOptimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as\nmask signals, Mask-DPO only learns from factually correct sentences in the\npreferred samples and prevents the penalty on factual contents in the not\npreferred samples, which resolves the ambiguity in the preference learning.\nExtensive experimental results demonstrate that Mask-DPO can significantly\nimprove the factuality of LLMs responses to questions from both in-domain and\nout-of-domain datasets, although these questions and their corresponding topics\nare unseen during training. Only trained on the ANAH train set, the score of\nLlama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%,\neven surpassing the score of Llama3.1-70B-Instruct (53.44%), while its\nFactScore on the out-of-domain Biography dataset is also improved from 30.29%\nto 39.39%. We further study the generalization property of Mask-DPO using\ndifferent training sample scaling strategies and find that scaling the number\nof topics in the dataset is more effective than the number of questions. We\nprovide a hypothesis of what factual alignment is doing with LLMs, on the\nimplication of this phenomenon, and conduct proof-of-concept experiments to\nverify it. We hope the method and the findings pave the way for future research\non scaling factuality alignment.",
        "Line-of-sight distortions of the cosmic microwave background (CMB), including\ngravitational lensing, cosmic birefringence, and patchy screening, encode\ncrucial cosmological information. While quadratic estimators (QE) have been\nexcellent tools for extracting these signals, they become suboptimal for\ncurrent- and next-generation CMB surveys, failing to maximize signal-to-noise\nand suffering from bias contamination that standard bias-hardening techniques\ncannot mitigate. We present a joint maximum a posteriori framework that\nsimultaneously reconstructs multiple distortion fields while explicitly\naccounting for their mutual contamination. For cosmic birefringence searches,\nour method achieves up to 2.5 improvement in reconstruction noise compared to\nthe QE, while significantly reducing CMB lensing-induced biases up to a factor\nof 5. These gains in lensing biases manifest not only in deep polarization\nsurveys like CMB-S4 and SPT-3G, but also in higher-noise experiments like\nSimons Observatory, where our method reduces lensing-induced biases by a factor\nof two thanks to the power of delensing. Our code provides a first step to\nrobust analyses of CMB secondary anisotropies, their cross-correlations with\nlarge-scale structure, and ultimately enabling more sensitive searches for\nprimordial $B$-modes.",
        "In this work, we address the problem of inferring the topology of a wireless\nnetwork using limited observational data. Specifically, we assume that we can\ndetect when a node is transmitting, but no further information regarding the\ntransmission is available. We propose a novel network estimation procedure\ngrounded in the following abstract problem: estimating the parameters of a\nfinite discrete-time Markov chain by observing, at each time step, which states\nare visited by multiple ``anonymous'' copies of the chain. We develop a\nconsistent estimator that approximates the transition matrix of the chain in\nthe operator norm, with the number of required samples scaling roughly linearly\nwith the size of the state space. Applying this estimation procedure to\nwireless networks, our numerical experiments demonstrate that the proposed\nmethod accurately infers network topology across a wide range of parameters,\nconsistently outperforming transfer entropy, particularly under conditions of\nhigh network congestion.",
        "We introduce STRING: Separable Translationally Invariant Position Encodings.\nSTRING extends Rotary Position Encodings, a recently proposed and widely used\nalgorithm in large language models, via a unifying theoretical framework.\nImportantly, STRING still provides exact translation invariance, including\ntoken coordinates of arbitrary dimensionality, whilst maintaining a low\ncomputational footprint. These properties are especially important in robotics,\nwhere efficient 3D token representation is key. We integrate STRING into Vision\nTransformers with RGB(-D) inputs (color plus optional depth), showing\nsubstantial gains, e.g. in open-vocabulary object detection and for robotics\ncontrollers. We complement our experiments with a rigorous mathematical\nanalysis, proving the universality of our methods.",
        "This paper addresses the issue of intellectual property management in the\nknowledge-based economy. The starting point in carrying out the study is the\npresentation of some concepts regarding in the first phase, the intellectual\ncapital. Arguments are made that the knowledge-based economy is a challenge for\nthe current century. The subject of intellectual property is approached through\nthe prism of a topical concept operationalized in the current global economic\ncontext. The main institutions that are directly related to this concept are\nmentioned. The topic of patents related to WOS indexed scientific papers is\nalso debated, along with a series of statistics and studies on the state of\npatent protection worldwide in the top fields. The last part of the paper\ncontains the conclusions and own points of view on the debated topic.",
        "The pervasiveness of the dissemination of fake news through social media\nplatforms poses critical risks to the trust of the general public, societal\nstability, and democratic institutions. This challenge calls for novel\nmethodologies in detection, which can keep pace with the dynamic and\nmulti-modal nature of misinformation. Recent works include powering the\ndetection using large language model advances in multimodal frameworks,\nmethodologies using graphs, and adversarial training in the literature of fake\nnews. Based on the different approaches which can bring success, some key\nhighlights will be underlined: enhanced LLM-improves accuracy through more\nadvanced semantics and cross-modality fusion for robust detections. The review\nfurther identifies critical gaps in adaptability to dynamic social media\ntrends, real-time, and cross-platform detection capabilities, as well as the\nethical challenges thrown up by the misuse of LLMs. Future directions underline\nthe development of style-agnostic models, cross-lingual detection frameworks,\nand robust policies with a view to mitigating LLM-driven misinformation. This\nsynthesis thus lays a concrete foundation for those researchers and\npractitioners committed to reinforcing fake news detection systems with\ncomplications that keep on growing in the digital landscape.",
        "Existing score-based adversarial attacks mainly focus on crafting $top$-1\nadversarial examples against classifiers with single-label classification.\nTheir attack success rate and query efficiency are often less than\nsatisfactory, particularly under small perturbation requirements; moreover, the\nvulnerability of classifiers with multi-label learning is yet to be studied. In\nthis paper, we propose a comprehensive surrogate free score-based attack, named\n\\b geometric \\b score-based \\b black-box \\b attack (GSBA$^K$), to craft\nadversarial examples in an aggressive $top$-$K$ setting for both untargeted and\ntargeted attacks, where the goal is to change the $top$-$K$ predictions of the\ntarget classifier. We introduce novel gradient-based methods to find a good\ninitial boundary point to attack. Our iterative method employs novel gradient\nestimation techniques, particularly effective in $top$-$K$ setting, on the\ndecision boundary to effectively exploit the geometry of the decision boundary.\nAdditionally, GSBA$^K$ can be used to attack against classifiers with $top$-$K$\nmulti-label learning. Extensive experimental results on ImageNet and PASCAL VOC\ndatasets validate the effectiveness of GSBA$^K$ in crafting $top$-$K$\nadversarial examples.",
        "The ability to generate SPARQL queries from natural language questions is\ncrucial for ensuring efficient and accurate retrieval of structured data from\nknowledge graphs (KG). While large language models (LLMs) have been widely\nadopted for SPARQL query generation, they are often susceptible to\nhallucinations and out-of-distribution errors when producing KG elements like\nUniform Resource Identifiers (URIs) based on internal parametric knowledge.\nThis often results in content that appears plausible but is factually\nincorrect, posing significant challenges for their use in real-world\ninformation retrieval (IR) applications. This has led to increased research\naimed at detecting and mitigating such errors. In this paper, we introduce PGMR\n(Post-Generation Memory Retrieval), a modular framework that incorporates a\nnon-parametric memory module to retrieve KG elements and enhance LLM-based\nSPARQL query generation. Our experimental results indicate that PGMR\nconsistently delivers strong performance across diverse datasets, data\ndistributions, and LLMs. Notably, PGMR significantly mitigates URI\nhallucinations, nearly eliminating the problem in several scenarios.",
        "Recent advances in time-resolved angle-resolved photoemission spectroscopy\nhave enabled access to ultrafast electron states and their spin dynamics in\nsolids. Atomically thin transition metal dichalcogenides are paradigmatic\ntwo-dimensional materials where electron momentum and spin degrees of freedom\nare coupled, being suitable candidates for time-resolved spectroscopy studies.\nIn this work, we present a thorough study of the electron dynamics when these\nmaterials are subject to an intense finite-pulse driving radiation. We extend\nthe scope of the conventional Floquet engineering and rely of the so-called\n$t-t^{\\prime}$ formalism to deal with driving fields described with two\ndistinct time scales, namely the envelope amplitude timescale and the time\nperiod of the external field. The interplay between the finite-pulse timescales\nand the intrinsic properties of the electrons gives rise to transient valley\npolarization and dynamical modifications of band structures, revealed by the\ntime-dependent circular dichroism of the sample.",
        "In the beyond 5G era, AI\/ML empowered realworld digital twins (DTs) will\nenable diverse network operators to collaboratively optimize their networks,\nultimately improving end-user experience. Although centralized AI-based\nlearning techniques have been shown to achieve significant network traffic\naccuracy, resulting in efficient network operations, they require sharing of\nsensitive data among operators, leading to privacy and security concerns.\nDistributed learning, and specifically federated learning (FL), that keeps data\nisolated at local clients, has emerged as an effective and promising solution\nfor mitigating such concerns. Federated learning poses, however, new challenges\nin ensuring fairness both in terms of collaborative training contributions from\nheterogeneous data and in mitigating bias in model predictions with respect to\nsensitive attributes. To address these challenges, a fair FL framework is\nproposed for collaborative network traffic prediction and resource allocation.\nTo demonstrate the effectiveness of the proposed approach, noniid and\nimbalanced federated datasets based on real-word traffic traces are utilized\nfor an elastic optical network. The assumption is that different optical nodes\nmay be managed by different operators. Fairness is evaluated according to the\ncoefficient of variations measure in terms of accuracy across the operators and\nin terms of quality-of-service across the connections (i.e., reflecting\nend-user experience). It is shown that fair traffic prediction across the\noperators result in fairer resource allocations across the connections.",
        "Recently synthesized quadruple perovskite CuCu$_3$Fe$_2$Re$_2$O$_{12}$\npossesses strong ferromagnetism and unusual electron properties, including\nenhanced electronic specific heat. Application of the first principles\nelectronic structure approaches unambiguously shows importance of the many-body\neffects in this compound. While CuCu$_3$Fe$_2$Re$_2$O$_{12}$ is half-metallic\nferrimagnet in the DFT+U method, in the density functional theory (DFT)\ncombined with the dynamical mean-field theory (DMFT) it appears to be a metal.\nStrong correlations lead to a renormalization of electronic spectrum and\nformation of incoherent states close to the Fermi level. Electronic specific\nheat and magnetic properties obtained in the DFT+DMFT approach are in better\nagreement with available experimental data than derived by other band structure\ntechniques.",
        "The Metaverse represents a transformative shift beyond traditional mobile\nInternet, creating an immersive, persistent digital ecosystem where users can\ninteract, socialize, and work within 3D virtual environments. Powered by large\nmodels such as ChatGPT and Sora, the Metaverse benefits from precise\nlarge-scale real-world modeling, automated multimodal content generation,\nrealistic avatars, and seamless natural language understanding, which enhance\nuser engagement and enable more personalized, intuitive interactions. However,\nchallenges remain, including limited scalability, constrained responsiveness,\nand low adaptability in dynamic environments. This paper investigates the\nintegration of large models within the Metaverse, examining their roles in\nenhancing user interaction, perception, content creation, and service quality.\nTo address existing challenges, we propose a generative AI-based framework for\noptimizing Metaverse rendering. This framework includes a cloud-edge-end\ncollaborative model to allocate rendering tasks with minimal latency, a\nmobility-aware pre-rendering mechanism that dynamically adjusts to user\nmovement, and a diffusion model-based adaptive rendering strategy to fine-tune\nvisual details. Experimental results demonstrate the effectiveness of our\napproach in enhancing rendering efficiency and reducing rendering overheads,\nadvancing large model deployment for a more responsive and immersive Metaverse.",
        "Pulsars, highly magnetized, rotating neutron stars, can have significant muon\nabundances in their dense cores, making them promising environments to probe\nultralight mediators coupled to muons. The precise measurement of periastron\nadvance in binary pulsar systems provides a sensitive probe of such long-range\nforces. In this work, we study the periastron advance constraints from binary\npulsar systems on the ultralight muonic mediators. We compute the muon number\nfraction in neutron stars, by properly taking into account the suppression\neffect of the long-range muonic force. We find that the periastron advance\nconstraints impose the most stringent constraints on ultralight muonic\nmediators in the mass range of $\\simeq(10^{-17},\\,2\\times10^{-15})$ eV, probing\nmuonic couplings as small as $\\mathcal{O}(10^{-21})$, which surpass the limits\nfrom LIGO\/Virgo gravitational wave measurements, by about an order of\nmagnitude.",
        "Using the HAL QCD method, we investigate S-wave meson-baryon interactions in\nsinglet and two octet channels in the flavor SU(3) limit, where the chiral\nunitary model predicts that a combination of bound-state poles in these\nchannels corresponds to $\\Lambda(1405)$. To avoid the singular behavior of the\nleading-order potentials of these channels in the derivative expansion, we\ninstead employ a separable potential in the time-dependent HAL QCD method. To\ncalculate all-to-all propagators in the three-point correlation functions\nincluding $\\Lambda$-baryon source operators with zero momentum, we employ the\nconventional stochastic estimation combined with the covariant approximation\naveraging. Separable potentials both in the singlet and octet channels show\nattraction without singular behavior. Our results of the corresponding phase\nshifts indicate that the attractive interaction in the singlet channel is\nstronger than that in the octet. Binding energies are consistent with the\nestimates from the two-point correlation function within one (two) sigma for\nthe singlet (octet) channel, and this ordering of binding energies is\nconsistent with the mass hierarchy suggested by the chiral unitary model.",
        "Pulsed laser-deposited amorphous carbon nanofoams could be a potential\ncandidate for electrochemical energy storage applications due to their\nproperties such as ultralightweight, huge volumetric void fractions, and\nco-existence of sp, sp2 and sp3 carbon hybridization. It is known that the\ncharge-storage of carbon nanostructures containing disordered sp2-domains is\ndetermined by the wettability, surface area, and porosity of carbon\nnanostructures. However, their charge-storage performance is limited to the\nareal capacitance of the order of a few mF\/cm2. We enhanced the supercapacitor\nperformance of nitrogen-doped amorphous carbon nanofoam by engineering its\nwettability and sp2-C\/sp3-C ratio by vacuum annealing. The specific capacitance\nwas enhanced around fifty times and the widened voltage of the device increased\nfrom 0.8 to 1.1 V compared to as-grown nanofoam. In addition, we examined for\nthe first time the initial increase in specific capacitance of the aqueous\nsymmetric supercapacitor with respect to the scan rate, employing in-situ\nmeasurements coupling Raman spectroscopy and electrochemistry. We attribute\nthis effect, although observed in previous literatures but unexplained, to the\nelectrochemical activation induced by structural changes during the charge\nstorage performance. This optimization of pulsed laser-deposited carbon\nnanofoam may open an avenue for fabricating lightweight and porous\nnanostructures for advanced macro-to-micro-supercapacitor devices.",
        "In the present work we construct kink solutions for different (parabolic and\nwave) variants of the fractional $\\phi^4$ model, in both the sub-Laplacian and\nsuper-Laplacian setting. We establish existence and monotonicity results (for\nthe sub - Laplacian case), along with sharp asymptotics which are corroborated\nthrough numerical computations. Importantly, in the sub-Laplacian regime, we\nprovide the explicit and numerically verifiable spectral condition, which\nguarantees uniqueness for odd kinks. We check numerically the relevant\ncondition to confirm the uniqueness of such solutions. In addition, we show\nasymptotic stability for the stationary kinks in the parabolic setting and\nalso, the spectral stability for the traveling kinks in the corresponding wave\nequation.",
        "Recent advancements in implicit 3D reconstruction methods, e.g., neural\nrendering fields and Gaussian splatting, have primarily focused on novel view\nsynthesis of static or dynamic objects with continuous motion states. However,\nthese approaches struggle to efficiently model a human-interactive object with\nn movable parts, requiring 2^n separate models to represent all discrete\nstates. To overcome this limitation, we propose Inter3D, a new benchmark and\napproach for novel state synthesis of human-interactive objects. We introduce a\nself-collected dataset featuring commonly encountered interactive objects and a\nnew evaluation pipeline, where only individual part states are observed during\ntraining, while part combination states remain unseen. We also propose a strong\nbaseline approach that leverages Space Discrepancy Tensors to efficiently\nmodelling all states of an object. To alleviate the impractical constraints on\ncamera trajectories across training states, we propose a Mutual State\nRegularization mechanism to enhance the spatial density consistency of movable\nparts. In addition, we explore two occupancy grid sampling strategies to\nfacilitate training efficiency. We conduct extensive experiments on the\nproposed benchmark, showcasing the challenges of the task and the superiority\nof our approach.",
        "Recent LLMs have enabled significant advancements for conversational agents.\nHowever, they are also well known to hallucinate, producing responses that seem\nplausible but are factually incorrect. On the other hand, users tend to\nover-rely on LLM-based AI agents, accepting AI's suggestion even when it is\nwrong. Adding positive friction, such as explanations or getting user\nconfirmations, has been proposed as a mitigation in AI-supported\ndecision-making systems. In this paper, we propose an accountability model for\nLLM-based task-oriented dialogue agents to address user overreliance via\nfriction turns in cases of model uncertainty and errors associated with\ndialogue state tracking (DST). The accountability model is an augmented LLM\nwith an additional accountability head that functions as a binary classifier to\npredict the relevant slots of the dialogue state mentioned in the conversation.\nWe perform our experiments with multiple backbone LLMs on two established\nbenchmarks (MultiWOZ and Snips). Our empirical findings demonstrate that the\nproposed approach not only enables reliable estimation of AI agent errors but\nalso guides the decoder in generating more accurate actions. We observe around\n3% absolute improvement in joint goal accuracy (JGA) of DST output by\nincorporating accountability heads into modern LLMs. Self-correcting the\ndetected errors further increases the JGA from 67.13 to 70.51, achieving\nstate-of-the-art DST performance. Finally, we show that error correction\nthrough user confirmations (friction turn) achieves a similar performance gain,\nhighlighting its potential to reduce user overreliance."
      ]
    }
  },
  {
    "id":2412.10196,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"XGBoost: A Scalable Tree Boosting System",
    "start_abstract":"Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Quality assurance procedures for mass spectrometry untargeted metabolomics. a review"
      ],
      "abstract":[
        "Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Stability of 2-class groups in the $\\mathbb{Z}_2$-extension of certain\n  real biquadratic fields",
        "Obstructions for Morin and fold maps: Stiefel-Whitney classes and Euler\n  characteristics of singularity loci",
        "Chiral broadband High Harmonic Generation Source by Vectorial\n  Time-Polarization-Gating",
        "Crystal skeletons: Combinatorics and axioms",
        "Quantum crystal spin Hall effect in two-dimensional altermagnetic\n  systems",
        "Random Variables, Conditional Independence and Categories of Abstract\n  Sample Spaces",
        "First-principles Hubbard parameters with automated and reproducible\n  workflows",
        "Constant-Overhead Fault-Tolerant Bell-Pair Distillation using High-Rate\n  Codes",
        "Bounded Dark Energy",
        "Probing Type II Seesaw Leptogenesis Through Lepton Flavor Violation",
        "Strategic Queues with Priority Classes",
        "Predicting Liquidity-Aware Bond Yields using Causal GANs and Deep\n  Reinforcement Learning with LLM Evaluation",
        "$\\mathrm{G}_2$-structures with torsion and the deformed Shatashvili-Vafa\n  vertex algebra",
        "A new class of non-stationary Gaussian fields with general smoothness on\n  metric graphs",
        "Isolating the hard core of phaseless inference: the Phase selection\n  formulation",
        "Automated DC Voltage and DC Resistance Real-time Multiple Standard for\n  Artifact Calibration of Calibrators and multimeters",
        "Probing Magnetism in Self-Assembled Organometallic Complexes using Kondo\n  Spectroscopy",
        "Bluetooth sensors in phyphox with Arduino and MicroPython -- Paving the\n  way from an idea to an experiment for teachers and learners",
        "Impact of radiative accelerations on the stellar characterization of\n  FGK-type stars using spectroscopic and seismic constraints",
        "Functional equation arising in behavioral sciences: solvability and\n  collocation scheme in H\\\"older spaces",
        "Rough Stochastic Pontryagin Maximum Principle and an Indirect Shooting\n  Method",
        "Universal Anyon Tunneling in a Chiral Luttinger Liquid",
        "Invariants recovering the reduction type of a hyperelliptic curve",
        "The Engel--Minkowski question-mark function",
        "A Full AGN Feedback Prescription for Numerical Models: Negative,\n  Positive and Hot Gas-Ejection Mode",
        "Critical Equations Involving Nonlocal Subelliptic Operators on\n  Stratified Lie Groups: Spectrum, Bifurcation and Multiplicity",
        "Partial-dual genus polynomial of graphs",
        "Well-posedness for the dNLS hierarchy",
        "Spectral distribution of the free Jacobi process with equal rank\n  projections"
      ],
      "abstract":[
        "Greenberg's conjecture on the stability of $\\ell$-class groups in the\ncyclotomic $\\mathbb{Z}_{\\ell}$-extension of a real field has been proven for\nvarious infinite families of real quadratic fields for the prime $\\ell=2$. In\nthis work, we consider an infinite family of real biquadratic fields $K$. With\nsome extensive use of elementary group theoretic and class field theoretic\narguments, we investigate the $2$-class groups of the $n$-th layers $K_n$ of\nthe cyclotomic $\\mathbb{Z}_2$-extension of $K$ and verify Greenberg's\nconjecture. We also relate capitulation of ideal classes of certain\nsub-extensions of $K_n$ to the relative sizes of the $2$-class groups.",
        "For a singularity type $\\eta$, let the $\\eta$-avoiding number of an\n$n$-dimensional manifold $M$ be the lowest $k$ for which there is a map\n$M\\to\\mathbb{R}^{n+k}$ without $\\eta$ type singular points. For instance, the\ncase of $\\eta=\\Sigma^1$ is the case of immersions, which has been extensively\nstudied in the case of real projective spaces. In this paper we study the\n$\\eta$-avoiding number for other singularity types. Our results come in two\nlevels: first we give an abstract reasoning that a non-zero cohomology class is\nsupported on the singularity locus $\\eta(f)$, proving that $\\eta(f)$ cannot be\nempty. Second, we interpret this obstruction as a non-zero invariant of the\nsingularity locus $\\eta(f)$ for generic $f$. The main technique that we employ\nis Sullivan's Stiefel-Whitney classes, which are mod 2, real analogues of the\nChern-Schwartz-MacPherson (CSM) classes. We introduce the Segre-Stiefel-Whitney\nclasses of a singularity ${\\rm s}^{\\rm sw}_\\eta$ whose lowest degree term is\nthe mod 2 Thom polynomial of $\\eta$. Using these techniques we compute some\nuniversal formulas for the Euler characteristic of a singularity locus.",
        "Chiral (highly helical) extreme ultraviolet (XUV) sources are pivotal for\ninvestigating chiroptical phenomena on the ultrafast electronic timescale.\nTable-top, coherent High Harmonic Generation (HHG)-based sources are\nparticularly well-suited for these studies. However, chiral materials, such as\norganic chiral molecules and solid-state magnetic materials, exhibit fine\nspectral features which necessitate broadband radiation for their complete\ninterrogation. The generation of radiation that is both broadband and helical\nthrough HHG presents a seemingly paradoxical challenge: while chiral HHG\nemission requires at least two recollisions occurring along different\ndirections in the polarization plane, the Floquet limit might already be\nreached with as few as three recollisions, resulting in a sparse spectrum\ncharacterized by pronounced discrete harmonic peaks. Here we propose a\nstraightforward scheme that enables the interrogation of fine spectral\nfeatures, in principle restricted only by the resolution of the XUV\nspectrometer, with chiral XUV light. Our method is based on using a vectorial\ntwo-color driver with close central-frequencies with slight symmetry breaking.\nIt integrates the time-gating and polarization-gating techniques to generate a\nvectorial driver which induces well-controlled bursts of recollisions,\noccurring along different directions in the polarization plane. The method\nsatisfies the dual requirements of an XUV source which is both broadband and\nhelical. We perform polarization scan and demonstrate that the broadband XUV\nradiation exhibits rapid modulations in its spectral ellipticity, and fast\nalternation in its spectral helicities. The phase of modulations could be\ncontrolled by introducing a slight symmetry breaking. This allows us to control\nand modulate the XUV polarization state, which should enable the detection of\nchiroptical signals with enhanced sensitivity.",
        "Crystal skeletons were introduced by Maas-Gari\\'epy in 2023 by contracting\nquasi-crystal components in a crystal graph. On the representation theoretic\nlevel, crystal skeletons model the expansion of Schur functions into Gessel's\nquasisymmetric functions. Motivated by questions of Schur positivity, we\nprovide a combinatorial description of crystal skeletons, and prove many new\nproperties, including a conjecture by Maas-Gari\\'epy that crystal skeletons\ngeneralize dual equivalence graphs. We then present a new axiomatic approach to\ncrystal skeletons. We give three versions of the axioms based on\n$GL_n$-branching, $S_n$-branching, and local axioms in analogy to the local\nStembridge axioms for crystals based on novel commutation relations.",
        "In the field of condensed matter physics, time-reversal symmetry provides the\nfoundation for a number of interesting quantum phenomena, in particular the\ntopological materials and the quantum spin Hall physics that have been\nextensively studied in recent years. Here, based on the first-principles\nelectronic-structure calculations, symmetry analysis, and model simulations, we\ndemonstrate that time-reversal symmetry is not fundamentally necessary for the\nquantum spin Hall effect. In altermagnetic materials, as an alternative, it can\nalso be protected by crystal symmetry, which can be referred to as the quantum\ncrystal spin Hall effect.",
        "Two high-level \"pictures\" of probability theory have emerged: one that takes\nas central the notion of random variable, and one that focuses on distributions\nand probability channels (Markov kernels). While the channel-based picture has\nbeen successfully axiomatized, and widely generalized, using the notion of\nMarkov category, the categorical semantics of the random variable picture\nremain less clear. Simpson's probability sheaves are a recent approach, in\nwhich probabilistic concepts like random variables are allowed vary over a site\nof sample spaces. Simpson has identified rich structure on these sites, most\nnotably an abstract notion of conditional independence, and given examples\nranging from probability over databases to nominal sets. We aim bring this\ndevelopment together with the generality and abstraction of Markov categories:\nWe show that for any suitable Markov category, a category of sample spaces can\nbe defined which satisfies Simpson's axioms, and that a theory of probability\nsheaves can be developed purely synthetically in this setting. We recover\nSimpson's examples in a uniform fashion from well-known Markov categories, and\nconsider further generalizations.",
        "We introduce an automated, flexible framework (aiida-hubbard) to\nself-consistently calculate Hubbard $U$ and $V$ parameters from\nfirst-principles. By leveraging density-functional perturbation theory, the\ncomputation of the Hubbard parameters is efficiently parallelized using\nmultiple concurrent and inexpensive primitive cell calculations. Furthermore,\nthe intersite $V$ parameters are defined on-the-fly during the iterative\nprocedure to account for atomic relaxations and diverse coordination\nenvironments. We demonstrate the scalability and reliability of the framework\nby computing in high-throughput fashion the self-consistent onsite $U$ and\nintersite $V$ parameters for 115 Li-containing bulk solids. Our analysis of the\nHubbard parameters calculated reveals a significant correlation of the onsite\n$U$ values on the oxidation state and coordination environment of the atom on\nwhich the Hubbard manifold is centered, while intersite $V$ values exhibit a\ngeneral decay with increasing interatomic distance. We find, e.g., that the\nnumerical values of $U$ for Fe and Mn 3d orbitals can vary up to 3 eV and 6 eV,\nrespectively; their distribution is characterized by typical shifts of about\n0.5 eV and 1.0 eV upon change in oxidation state, or local coordination\nenvironment. For the intersite $V$ a narrower spread is found, with values\nranging between 0.2 eV and 1.6 eV when considering transition metal and oxygen\ninteractions. This framework paves the way for the exploration of redox\nmaterials chemistry and high-throughput screening of $d$ and $f$ compounds\nacross diverse research areas, including the discovery and design of novel\nenergy storage materials, as well as other technologically-relevant\napplications.",
        "We present a fault-tolerant Bell-pair distillation scheme achieving constant\noverhead through high-rate quantum low-density parity-check (qLDPC) codes. Our\napproach maintains a constant distillation rate equal to the code rate - as\nhigh as $1\/3$ in our implementations - while requiring no additional overhead\nbeyond the physical qubits of the code. Full circuit-level analysis\ndemonstrates fault-tolerance for input Bell pair infidelities below a threshold\n$\\sim 5\\%$, readily achievable with near-term capabilities. Unlike previous\nproposals, our scheme keeps the output Bell pairs encoded in qLDPC codes at\neach node, eliminating decoding overhead and enabling direct use in distributed\nquantum applications through recent advances in qLDPC computation. These\nresults establish qLDPC-based distillation as a practical route toward\nresource-efficient quantum networks and distributed quantum computing.",
        "Recent cosmological observations suggest that the dark energy equation of\nstate may have changed in the latter stages of cosmic history. We introduce a\nquintessence scenario, termed bounded dark energy, capable of explaining this\nfeature in a technically natural way. Our approach is motivated from a\nbottom-up perspective, based on the concept of mirage cut-off, where we\ndemonstrate the stability of the quintessence potential against large quantum\ncorrections. At the same time, the bounded dark energy framework aligns well\nwith top-down considerations motivated from quantum gravity arguments. We\nemploy both human-driven insights and machine learning techniques to identify\nexplicit realizations of bounded dark energy models. We then perform an\nanalysis based on Markov Chain Monte-Carlo to assess their predictions against\nCMB, galaxy surveys, and supernova data, showing that bounded dark energy\nprovides a good fit to current observations. We also discuss how upcoming\nmeasurements can further test and refine our proposal.",
        "Lepton flavor violation (LFV) offers a powerful probe of physics beyond the\nStandard Model, particularly in models addressing neutrino masses and the\nbaryon asymmetry of the universe. In this study, we investigate LFV processes\nwithin the framework of type II seesaw leptogenesis, where the Standard Model\nis extended by an $SU(2)_L$ triplet Higgs field. We focus on key LFV processes\nincluding $\\mu^+\\to e^+\\gamma$, $\\mu^+ \\to e^+e^-e^+$, and $\\mu \\rightarrow e$\nconversion in nuclei, deriving stringent constraints on the parameter space\nfrom current experimental data. We scan the 3$\\sigma$ range of neutrino\noscillation parameters and identify the most conservative bounds consistent\nwith existing measurements. Our results reveal that the MEG experiment\ncurrently provides the strongest constraints in the normal ordering (NO)\nscenario, while the SINDRUM experiment offers comparable sensitivity in the\ninverted ordering (IO) case. Future experiments, such as MEG II, Mu3e, Mu2e,\nand COMET, are predicted to significantly improve the sensitivity, testing\nlarger regions of the parameter space. This work underscores the crucial role\nof LFV experiments in probing type II seesaw leptogenesis, providing an avenue\nto explore the connections between neutrino mass generation, baryogenesis, and\ninflation at experimentally accessible energy scales.",
        "We consider a strategic M\/M\/1 queueing model under a first-come-first-served\nregime, where customers are split into two classes and class $A$ has priority\nover class $B$. Customers can decide whether to join the queue or balk, and, in\ncase they have joined the queue, whether and when to renege. We study the\nequilibrium strategies and compare the equilibrium outcome and the social\noptimum in the two cases where the social optimum is or is not constrained by\npriority.",
        "Financial bond yield forecasting is challenging due to data scarcity,\nnonlinear macroeconomic dependencies, and evolving market conditions. In this\npaper, we propose a novel framework that leverages Causal Generative\nAdversarial Networks (CausalGANs) and Soft Actor-Critic (SAC) reinforcement\nlearning (RL) to generate high-fidelity synthetic bond yield data for four\nmajor bond categories (AAA, BAA, US10Y, Junk). By incorporating 12 key\nmacroeconomic variables, we ensure statistical fidelity by preserving essential\nmarket properties. To transform this market dependent synthetic data into\nactionable insights, we employ a finetuned Large Language Model (LLM)\nQwen2.5-7B that generates trading signals (BUY\/HOLD\/SELL), risk assessments,\nand volatility projections. We use automated, human and LLM evaluations, all of\nwhich demonstrate that our framework improves forecasting performance over\nexisting methods, with statistical validation via predictive accuracy, MAE\nevaluation(0.103%), profit\/loss evaluation (60% profit rate), LLM evaluation\n(3.37\/5) and expert assessments scoring 4.67 out of 5. The reinforcement\nlearning-enhanced synthetic data generation achieves the least Mean Absolute\nError of 0.103, demonstrating its effectiveness in replicating real-world bond\nmarket dynamics. We not only enhance data-driven trading strategies but also\nprovides a scalable, high-fidelity synthetic financial data pipeline for risk &\nvolatility management and investment decision-making. This work establishes a\nbridge between synthetic data generation, LLM driven financial forecasting, and\nlanguage model evaluation, contributing to AI-driven financial decision-making.",
        "We construct representations of the deformed Shatashvili-Vafa vertex algebra\n$\\mathrm{SV}_a$, with parameter $a \\in \\mathbb{C}$, as recently proposed in the\nphysics literature by Fiset and Gaberdiel. The geometric input for our\nconstruction are integrable $\\mathrm{G}_2$-structures with closed torsion,\nsolving the heterotic $\\mathrm{G}_2$ system with $\\alpha'=0$ on the group\nmanifolds $S^3\\times T^4$ and $S^3\\times S^3\\times S^1$. From considerations in\nstring theory, one expects the chiral algebra of these backgrounds to include\n$\\mathrm{SV}_a$, and we provide a mathematical realization of this expectation\nby obtaining embeddings of $\\mathrm{SV}_a$ in the corresponding superaffine\nvertex algebra and the chiral de Rham complex. In our examples, the parameter\n$a$ is proportional to the scalar torsion class of the $\\mathrm{G}_2$\nstructure, $a \\sim \\tau_0$, as expected from previous work in the\nsemi-classical limit by the second author, jointly with De la Ossa and\nMarchetto.",
        "The increasing availability of network data has driven the development of\nadvanced statistical models specifically designed for metric graphs, where\nGaussian processes play a pivotal role. While models such as Whittle-Mat\\'ern\nfields have been introduced, there remains a lack of practically applicable\noptions that accommodate flexible non-stationary covariance structures or\ngeneral smoothness. To address this gap, we propose a novel class of\ngeneralized Whittle-Mat\\'ern fields, which are rigorously defined on general\ncompact metric graphs and permit both non-stationarity and arbitrary\nsmoothness. We establish new regularity results for these fields, which extend\neven to the standard Whittle-Mat\\'ern case. Furthermore, we introduce a method\nto approximate the covariance operator of these processes by combining the\nfinite element method with a rational approximation of the operator's\nfractional power, enabling computationally efficient Bayesian inference for\nlarge datasets. Theoretical guarantees are provided by deriving explicit\nconvergence rates for the covariance approximation error, and the practical\nutility of our approach is demonstrated through simulation studies and an\napplication to traffic speed data, highlighting the flexibility and\neffectiveness of the proposed model class.",
        "Real-valued Phase retrieval is a non-convex continuous inference problem,\nwhere a high-dimensional signal is to be reconstructed from a dataset of\nsignless linear measurements. Focusing on the noiseless case, we aim to\ndisentangle the two distinct sub-tasks entailed in the Phase retrieval problem:\nthe hard combinatorial problem of retrieving the missing signs of the\nmeasurements, and the nested convex problem of regressing the input-output\nobservations to recover the hidden signal. To this end, we introduce and\nanalytically characterize a two-level formulation of the problem, called\n``Phase selection''. Within the Replica Theory framework, we perform a large\ndeviation analysis to characterize the minimum mean squared error achievable\nwith different guesses for the hidden signs. Moreover, we study the free-energy\nlandscape of the problem when both levels are optimized simultaneously, as a\nfunction of the dataset size. At low temperatures, in proximity to the\nBayes-optimal threshold -- previously derived in the context of Phase retrieval\n-- we detect the coexistence of two free-energy branches, one connected to the\nrandom initialization condition and a second to the signal. We derive the phase\ndiagram for a first-order transition after which the two branches merge.\nInterestingly, introducing an $L_2$ regularization in the regression sub-task\ncan anticipate the transition to lower dataset sizes, at the cost of a bias in\nthe signal reconstructions which can be removed by annealing the regularization\nintensity. Finally, we study the inference performance of three meta-heuristics\nin the context of Phase selection: Simulated Annealing, Approximate Message\nPassing, and Langevin Dynamics on the continuous relaxation of the sign\nvariables. With simultaneous annealing of the temperature and the $L_2$\nregularization, they are shown to approach the Bayes-optimal sample efficiency.",
        "An automated temperature-controlled electrical DC voltage and DC resistance\nmultiple reference standard (MRS) has been developed by Measurements\nInternational (MI) with the scientific support from the Istituto Nazionale di\nRicerca Metrologica (INRIM). The MRS includes a 10 V, a 1 {\\Omega}, and a 10\nk{\\Omega} standards selectable via a switch unit. This setup allows the\nartifact calibration of high-end calibrators and multimeters used in\nlow-frequency electrical measurements. The two resistors are high-stability\nstandards from MI, while the 10 V standard is based on a low-noise circuit\ndeveloped by INRIM in collaboration with MI. A key innovation is the internal\nreal-time clock calendar, which displays the calibration values of the MRS\nstandards and their updated values internally calculated. This ensures reliable\nuse of the MRS standards over extended periods between calibrations,\neffectively minimizing uncertainties due to their drift. The standards are\nhoused in a thermal box, minimizing temperature variations. The MRS standards\nmeet the uncertainty requirements defined by calibrators and multimeters\nmanufacturers for artifact calibration and can also serve as laboratory\nreferences or travelling standards for interlaboratory comparisons (ILCs). MI\nis currently commercializing the MRS.",
        "Control of individual spins at the atomic level holds great promise for\nminiaturized spintronics, quantum sensing, and quantum information processing.\nBoth single atomic and molecular spin centers are prime candidates for these\napplications and are often individually addressed and manipulated using\nscanning tunneling microscopy (STM). In this work, we present a hybrid approach\nand demonstrate a robust method for self-assembly of magnetic organometallic\ncomplexes consisting of individual iron (Fe) atoms and molecules on a silver\nsubstrate using STM. We employ two types of molecules, bis(dibenzoylmethane)\ncopper(II) [Cu(dbm)2] and iron phthalocyanine (FePc). We show that in both\ncases the Fe atoms preferentially attach underneath the benzene ring ligand of\nthe molecules, effectively forming an organometallic half-sandwich arene\ncomplex, Fe(C6H6), that is akin to the properties of metallocenes. In both\nsituations, a molecule can be combined with up to two Fe atoms. In addition, we\nobserve a change in the magnetic properties of the attached Fe atoms in\nscanning tunneling spectroscopy, revealing a distinct Kondo signature at the Fe\nsites. We explain the latter using density functional theory calculations, and\nfind that the bond formation between the Fe 3d-orbitals and the benzene\n{\\pi}-molecular orbitals creates a favorable situation for Kondo screening of\nthe d_xz- and d_yz-like orbitals. Thus, this work establishes a reliable design\nprinciple for forming hybrid organometallic complexes and simultaneous tuning\nof their atomic spin states.",
        "In order to extend the available sensors of smartphone experiments with cheap\nmicrocontroller-based external sensors, the smartphone experimentation app\n\"phyphox\" has been extended with a generic Bluetooth Low Energy interface.\nSince its application requires an in-depth understanding of the underlying\ntechnologies, the direct use of that interface for educational purposes is\nlimited. To avoid this difficulty, the functionality was encapsulated into an\nArduino and MicroPython library. With these, also educators and learners with\nonly rudimentary programming knowledge can integrate an app-based interface\ninto microcontroller projects with only few lines of code. This opens a wide\nrange of new learning opportunities, which are described exemplarily.",
        "Chemical transport mechanisms are fundamental processes in stellar evolution\nmodels. They are responsible for the chemical distribution, and their impact\ndetermines how accurately we can characterize stars. Radiative accelerations\nare one of these processes. They allow the accumulation of elements at\ndifferent depths in the star. We aim to assess the impact of radiative\naccelerations on the modeling of FGK-type stars and their impact on the\nprediction of surface abundances. To reduce the cost of the computation of\nradiative accelerations, we implemented the single-valued parameters (SVP)\nmethod in the stellar evolution code MESA. The SVP method is more efficient in\ncalculating radiative accelerations, which enables computations of large enough\ngrids of models for stellar characterization. Compared to models that include\natomic diffusion (with only gravitational settling), the inclusion of radiative\naccelerations has a small effect on the inference of fundamental properties,\nwith an impact of 2\\%, 0.7\\%, and 5\\% for mass, radius, and age. However, the\ntreatment of radiative accelerations is necessary to predict the chemical\ncomposition of and accurately characterize stars.",
        "We consider a generalization of a functional equation that models the\nlearning process in various animal species. The equation can be considered\nnonlocal, as it is built with a convex combination of the unknown function\nevaluated at mixed arguments. This makes the equation contain two terms with\nvanishing delays. We prove the existence and uniqueness of the solution in the\nH\\\"older space which is a natural function space to consider. In the second\npart of the paper, we devise an efficient numerical collocation method used to\nfind an approximation to the main problem. We prove the convergence of the\nscheme and, in passing, several properties of the linear interpolation operator\nacting on the H\\\"older space. Numerical simulations verify that the order of\nconvergence of the method (measured in the supremum norm) is equal to the order\nof H\\\"older continuity.",
        "We derive first-order Pontryagin optimality conditions for stochastic optimal\ncontrol with deterministic controls for systems modeled by rough differential\nequations (RDE) driven by Gaussian rough paths. This Pontryagin Maximum\nPrinciple (PMP) applies to systems following stochastic differential equations\n(SDE) driven by Brownian motion, yet it does not rely on forward-backward SDEs\nand involves the same Hamiltonian as the deterministic PMP. The proof consists\nof first deriving various integrable error bounds for solutions to nonlinear\nand linear RDEs by leveraging recent results on Gaussian rough paths. The PMP\nthen follows using standard techniques based on needle-like variations. As an\napplication, we propose the first indirect shooting method for nonlinear\nstochastic optimal control and show that it converges 10x faster than a direct\nmethod on a stabilization task.",
        "The edge modes of fractional quantum Hall liquids are described by chiral\nLuttinger liquid theory. Despite many years of experimental investigation\nfractional quantum Hall edge modes remain enigmatic with significant\ndiscrepancies between experimental observations and detailed predictions of\nchiral Luttinger liquid theory. Here we report measurements of tunneling\nconductance between counterpropagating edge modes at $\\nu=1\/3$ across a quantum\npoint contact fabricated on an AlGaAs\/GaAs heterostructure designed to promote\na sharp confinement potential. We present evidence for tunneling of anyons\nthrough an $\\nu=1\/3$ incompressible liquid that exhibits universal scaling\nbehavior with respect to temperature, source-drain bias, and barrier\ntransmission, as originally proposed by Wen[1,2]. We measure the tunneling\nexponent $g = 0.334 \\pm 0.001$, consistent with the scaling dimension $\\Delta =\ng\/2 = 1\/6$ for a Laughlin quasiparticle at the edge. When combined with\nmeasurements of the fractional charge $e^*=e\/3$ and the recently observed\nanyonic statistical angle $\\theta_a=\\frac{2\\pi}{3}$, the measured tunneling\nexponent fully characterizes the topological order of the primary Laughlin\nstate at $\\nu=1\/3$.",
        "Tate's algorithm tells us that for an elliptic curve $E$ over a local field\n$K$ of residue characteristic $\\geq 5$, $E\/K$ has potentially good reduction if\nand only if $\\text{ord}(j_E)\\geq 0$. It also tells us that when $E\/K$ is\nsemistable the dual graph of the special fibre of the minimal regular model of\n$E\/K^{\\text{unr}}$ can be recovered from $\\text{ord}(j_E)$. We generalise these\nresults to hyperelliptic curves of genus $g\\geq 2$ over local fields of odd\nresidue characteristic $K$ by defining a list of absolute invariants that\ndetermine the potential stable model of a genus $g$ hyperelliptic curve $C$.\nThey also determine the dual graph of the special fibre of the minimal regular\nmodel of $C\/K^{\\text{unr}}$ if $C\/K$ is semistable. This list depends only on\nthe genus of $C$, and the absolute invariants can be written in terms of the\ncoefficients of a Weierstrass equation for $C$. We explicitly describe the\nmethod by which the valuations of the invariants recover the dual graphs.\nAdditionally, we show by way of a counterexample that if $g \\geq 2$, there is\nno list of invariants whose valuations determine the dual graph of the special\nfibre of the minimal regular model of a genus $g$ hyperelliptic curve $C$ over\na local field $K$ of odd residue characteristic when $C$ is not assumed to be\nsemistable.",
        "The present article deals with properties of a certain function of the\nMinkowski type with arguments defined by Engel series. Differential, integral,\nand other properties of the function were considered.",
        "We build upon the state-of-the-art semi-analytic model \\texttt{FEGA24}\n(Formation and Evolution of GAlaxies, \\citealt{contini2024d}), which integrates\nthe latest prescriptions relevant to galaxy formation and evolution, alongside\na comprehensive AGN feedback model. This model incorporates three modes of\nfeedback: negative (preventing excessive cooling), positive (enhancing star\nformation), and hot gas ejection (expelling gas beyond the virial radius of\nhalos). These modes operate in a coordinated manner: the negative mode\nregulates the cooling process, the positive mode promotes bursts of star\nformation, and the hot gas ejection mode expels gas beyond the virial radius\nwhen the AGN is sufficiently powerful. Our updated semi-analytic model,\n\\texttt{FEGA25}, retains the qualitative and quantitative consistency of the\nanalyses presented in \\cite{contini2024d}, while delivering more robust\nresults. Notably, \\texttt{FEGA25} provides a more detailed characterization of\nthe fraction of red galaxies as a function of stellar mass, predicts a main\nsequence of star-forming galaxies more consistent with observations, and\nestimates the fraction of hot gas in halos closer to observed values. These\nfindings underscore the importance of a physical mechanism capable of ejecting\nhot gas beyond the virialized region of dark matter halos without significantly\naltering the stellar and cold gas components. Such a mechanism is crucial to\nensure the proper functioning of other processes, such as cooling and star\nformation. Since supernova feedback is already modeled at its maximum\nefficiency, AGN feedback emerges as the natural candidate for this role.",
        "In this paper, we explore the bifurcation phenomena and establish the\nexistence of multiple solutions for the nonlocal subelliptic Brezis-Nirenberg\nproblem:\n  \\begin{equation*}\n  \\begin{cases} (-\\Delta_{\\mathbb{G}})^s u= |u|^{2_s^*-2}u+\\lambda u \\quad\n&\\text{in}\\quad \\Omega, \\\\ u=0\\quad & \\text{in}\\quad \\mathbb{G}\\backslash\n\\Omega, \\end{cases}\n  \\end{equation*} where $(-\\Delta_{\\mathbb{G}})^s$ is the fractional\nsub-Laplacian on the stratified Lie group $\\mathbb{G}$ with homogeneous\ndimension $Q,$ $\\Omega$ is a open bounded subset of $\\mathbb{G},$ $s \\in\n(0,1)$, $Q> 2s,$ $2_s^*:=\\frac{2Q}{Q-2s}$ is subelliptic fractional Sobolev\ncritical exponent, $\\lambda>0$ is a real parameter. This work extends the\nseminal contributions of Cerami, Fortunato, and Struwe to nonlocal subelliptic\noperators on stratified Lie groups. A key component of our study involves\nanalyzing the subelliptic $(s, p)$-eigenvalue problem for the (nonlinear)\nfractional $p$-sub-Laplacian $(-\\Delta_{p,{\\mathbb{G}}})^s$ \\begin{align*}\n  (-\\Delta_{p,{\\mathbb{G}}})^s u&=\\lambda\n|u|^{p-2}u,~\\text{in}~\\Omega,\\nonumber\n  u&=0~\\text{ in }~{\\mathbb{G}}\\setminus\\Omega, \\end{align*} with\n$0<s<1<p<\\infty$ and $Q>ps$, over the fractional Folland-Stein-Sobolev spaces\non stratified Lie groups applying variational methods. Particularly, we prove\nthat the $(s, p)$-spectrum of $(-\\Delta_{p,{\\mathbb{G}}})^s$ is closed and the\nsecond eigenvalue $\\lambda_2(\\Omega)$ with\n$\\lambda_2(\\Omega)>\\lambda_1(\\Omega)$ is well-defined and provides a\nvariational characterization of $\\lambda_2(\\Omega)$. We emphasize that the\nresults obtained here are also novel for $\\mathbb{G}$ being the Heisenberg\ngroup.",
        "Recently, Chmutov introduced the partial duality of ribbon graphs, which can\nbe regarded as a generalization of the classical Euler-Poincar\\'e duality. The\npartial-dual genus polynomial $^\\partial\\varepsilon_G(z)$ is an enumeration of\nthe partial duals of $G$ by Euler genus. For an intersection graph derived from\na given chord diagram, the partial-dual genus polynomial can be defined by\nconsidering the ribbon graph associated to the chord diagram. In this paper, we\nprovide a combinatorial approach to the partial-dual genus polynomial in terms\nof intersection graphs without referring to chord diagrams. After extending the\ndefinition of the partial-dual genus polynomial from intersection graphs to all\ngraphs, we prove that it satisfies the four-term relation of graphs. This\nprovides an answer to a problem proposed by Chmutov.",
        "We prove well-posedness for higher-order equations in the so-called dNLS\nhierarchy (also known as part of the Kaup-Newell hierarchy) in almost critical\nFourier-Lebesgue and in modulation spaces. Leaning in on estimates proven by\nthe author in a previous instalment Adams (2024), where a similar\nwell-posedness theory was developed for the equations of the NLS hierarchy, we\nshow the $j$th equation in the dNLS hierarchy is locally well-posed for initial\ndata in $\\hat H^s_r(\\mathbb{R})$ for $s \\ge \\frac{1}{2} + \\frac{j-1}{r'}$ and\n$1 < r \\le 2$ and also in $M^s_{2, p}(\\mathbb{R})$ for $s \\ge \\frac{j}{2}$ and\n$2 \\le p < \\infty$. Supplementing our results with corresponding ill-posedness\nresults in Fourier-Lebesgue and modulation spaces shows optimality.\n  Our arguments are based on the Fourier restriction norm method in Bourgain\nspaces adapted to our data spaces and the gauge-transformation commonly\nassociated with the dNLS equation. For the latter we establish bi-Lipschitz\ncontinuity between appropriate modulation spaces and that even for higher-order\nequations `bad' cubic nonlinear terms are lifted from the equation.",
        "The free Jacobi process is the radial part of the compression of the free\nunitary Brownian motion by two free orthogonal projections. In this paper, we\ndetermine the characteristic curves of the partial differential equation\nsatisfied by its spectral distribution when both projections have the same rank\n$\\alpha \\in (0,1)$. Doing so leads for any fixed time $t >0$ to an expression\nof the moment generating function in a neighborhood of the origin, extending\nour previous results valid for $\\alpha = 1\/2$. Moreover, the obtained\ncharacteristic curves are encoded by an $\\alpha$-deformation of the\n$\\xi$-transform of the spectral distribution of the free unitary Brownian\nmotion, of which we study mapping properties. We also prove a dynamical version\nof a recent identity pointed out by T. Kunisky and relating the stationary\ndistributions of the free Jacobi processes corresponding to the set of\nparameters $(\\alpha, \\alpha)$ and $(1\/2,\\alpha)$. Actually, our dynamical\nversion relates the Cauchy-Stieltjes transforms of the densities of the finite\ntime spectral distributions."
      ]
    }
  },
  {
    "id":2412.00225,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"The Burgers equation",
    "start_abstract":"The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows.",
    "start_categories":[
      "math.AP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b17"
      ],
      "title":[
        "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"
      ],
      "abstract":[
        "We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Teach-to-Reason with Scoring: Self-Explainable Rationale-Driven\n  Multi-Trait Essay Scoring",
        "FlashSR: One-step Versatile Audio Super-resolution via Diffusion\n  Distillation",
        "Insights of Transitions to Thermoacoustic Instability in Inverse\n  Diffusion Flame using Multifractal Detrended Fluctuation Analysis",
        "Delta-WKV: A Novel Meta-in-Context Learner for MRI Super-Resolution",
        "AAS2RTO: Automated Alert Streams to Real-Time Observations: Preparing\n  for rapid follow-up of transient objects in the era of LSST",
        "\"Auntie, Please Don't Fall for Those Smooth Talkers\": How Chinese\n  Younger Family Members Safeguard Seniors from Online Fraud",
        "Complex Riemannian spacetime and singularity-free black holes and\n  cosmology",
        "Contrast-Free Myocardial Scar Segmentation in Cine MRI using Motion and\n  Texture Fusion",
        "Proof-Driven Clause Learning in Neural Network Verification",
        "The impact of baryons on the sparsity of simulated galaxy clusters from\n  The Three Hundred Project",
        "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching",
        "Efficient Language Modeling for Low-Resource Settings with Hybrid\n  RNN-Transformer Architectures",
        "Semi-supervised Anomaly Detection with Extremely Limited Labels in\n  Dynamic Graphs",
        "Quantifying the degree of hydrodynamic behaviour in heavy-ion collisions",
        "Funnelling super-resolution STED microscopy through multimode fibres",
        "QLIO: Quantized LiDAR-Inertial Odometry",
        "Geometric Iterative Approach for Efficient Inverse Kinematics and\n  Planning of Continuum Robots with a Floating Base Under Environment\n  Constraints",
        "Black Hole Evaporation in Loop Quantum Gravity",
        "FAIR: Facilitating Artificial Intelligence Resilience in Manufacturing\n  Industrial Internet",
        "Grasping in Uncertain Environments: A Case Study For Industrial Robotic\n  Recycling",
        "Implicit Communication in Human-Robot Collaborative Transport",
        "CENTS: Generating synthetic electricity consumption time series for rare\n  and unseen scenarios",
        "Thermal emission from bow shocks. III. Variable diffuse X-ray emission\n  from stellar-wind bow shocks driven by dynamical instabilities",
        "Computing the generalized plasma dispersion function for non-Maxwellian\n  plasmas, with applications to Thomson scattering",
        "Gaussian Random Fields as an Abstract Representation of Patient Metadata\n  for Multimodal Medical Image Segmentation",
        "Can You Move These Over There? An LLM-based VR Mover for Supporting\n  Object Manipulation",
        "Friction-Scaled Vibrotactile Feedback for Real-Time Slip Detection in\n  Manipulation using Robotic Sixth Finger",
        "Estimating Multi-chirp Parameters using Curvature-guided Langevin Monte\n  Carlo",
        "Data-Driven Distributionally Robust Mixed-Integer Control through Lifted\n  Control Policy"
      ],
      "abstract":[
        "Multi-trait automated essay scoring (AES) systems provide a fine-grained\nevaluation of an essay's diverse aspects. While they excel in scoring, prior\nsystems fail to explain why specific trait scores are assigned. This lack of\ntransparency leaves instructors and learners unconvinced of the AES outputs,\nhindering their practical use. To address this, we propose a self-explainable\nRationale-Driven Multi-trait automated Essay scoring (RaDME) framework. RaDME\nleverages the reasoning capabilities of large language models (LLMs) by\ndistilling them into a smaller yet effective scorer. This more manageable\nstudent model is optimized to sequentially generate a trait score followed by\nthe corresponding rationale, thereby inherently learning to select a more\njustifiable score by considering the subsequent rationale during training. Our\nfindings indicate that while LLMs underperform in direct AES tasks, they excel\nin rationale generation when provided with precise numerical scores. Thus,\nRaDME integrates the superior reasoning capacities of LLMs into the robust\nscoring accuracy of an optimized smaller model. Extensive experiments\ndemonstrate that RaDME achieves both accurate and adequate reasoning while\nsupporting high-quality multi-trait scoring, significantly enhancing the\ntransparency of AES.",
        "Versatile audio super-resolution (SR) is the challenging task of restoring\nhigh-frequency components from low-resolution audio with sampling rates between\n4kHz and 32kHz in various domains such as music, speech, and sound effects.\nPrevious diffusion-based SR methods suffer from slow inference due to the need\nfor a large number of sampling steps. In this paper, we introduce FlashSR, a\nsingle-step diffusion model for versatile audio super-resolution aimed at\nproducing 48kHz audio. FlashSR achieves fast inference by utilizing diffusion\ndistillation with three objectives: distillation loss, adversarial loss, and\ndistribution-matching distillation loss. We further enhance performance by\nproposing the SR Vocoder, which is specifically designed for SR models\noperating on mel-spectrograms. FlashSR demonstrates competitive performance\nwith the current state-of-the-art model in both objective and subjective\nevaluations while being approximately 22 times faster.",
        "The inverse diffusion flame (IDF) can experience thermoacoustic instability\ndue to variations in power input or flow conditions. However, the dynamical\ntransitions in IDF that lead to this instability when altering control\nparameters have not been thoroughly investigated. In this study, we explore the\ncontrol parameters through two different approaches and employ multifractal\ndetrended fluctuation analysis to characterize the transitions observed prior\nto the onset of thermoacoustic instability in the inverse diffusion flame. Our\nfindings reveal a loss of multifractality near the region associated with\nthermoacoustic instability, which suggests a more ordered behavior. We\ndetermine that the singularity exponent, the width of the multifractal\nspectrum, and the Hurst exponent are reliable indicators of thermoacoustic\ninstability and serve as effective classifiers of dynamical states in inverse\ndiffusion flames.",
        "Magnetic Resonance Imaging (MRI) Super-Resolution (SR) addresses the\nchallenges such as long scan times and expensive equipment by enhancing image\nresolution from low-quality inputs acquired in shorter scan times in clinical\nsettings. However, current SR techniques still have problems such as limited\nability to capture both local and global static patterns effectively and\nefficiently. To address these limitations, we propose Delta-WKV, a novel MRI\nsuper-resolution model that combines Meta-in-Context Learning (MiCL) with the\nDelta rule to better recognize both local and global patterns in MRI images.\nThis approach allows Delta-WKV to adjust weights dynamically during inference,\nimproving pattern recognition with fewer parameters and less computational\neffort, without using state-space modeling. Additionally, inspired by\nReceptance Weighted Key Value (RWKV), Delta-WKV uses a quad-directional\nscanning mechanism with time-mixing and channel-mixing structures to capture\nlong-range dependencies while maintaining high-frequency details. Tests on the\nIXI and fastMRI datasets show that Delta-WKV outperforms existing methods,\nimproving PSNR by 0.06 dB and SSIM by 0.001, while reducing training and\ninference times by over 15\\%. These results demonstrate its efficiency and\npotential for clinical use with large datasets and high-resolution imaging.",
        "The upcoming Vera C. Rubin Legacy Survey of Space and Time (LSST) will\ndiscover tens of thousands of astrophysical transients per night, far outpacing\navailable spectroscopic follow-up capabilities. Carefully prioritising\ncandidates for follow-up observations will maximise the scientific return from\nsmall telescopes with a single-object spectrograph. We introduce AAS2RTO, an\nastrophysical transient candidate prioritisation tool written in Python.\nAAS2RTO is flexible in that any number of criteria that consider observed\nproperties of transients can be implemented. The visibility of candidates from\na given observing site is also considered. The prioritised list of candidates\nprovided by AAS2RTO is continually updated when new transient data are made\navailable. Therefore, it can be applied to observing campaigns with a wide\nvariety of scientific motivations. AAS2RTO uses a greedy algorithm to\nprioritise candidates. Candidates are represented by a single numerical value,\nor `score'. Scores are computed by constructing simple numerical factors which\nindividually consider the competing facets of a candidate which make it\nsuitable for follow-up observation. AAS2RTO is currently configured to work\nprimarily with photometric data from the Zwicky Transient Facility (ZTF),\ndistributed by certified LSST community brokers. We provide an example of how\nAAS2RTO can be used by defining a set of criteria to prioritise observations of\ntype Ia supernovae (SNe Ia) close to peak brightness, in preparation for\nobservations with the spectrograph at the Danish-1.54m telescope. Using a\nsample of archival alerts from ZTF, we evaluate the criteria we have designed\nto estimate the number of SNe Ia that we will be able to observe with a 1.5m\ntelescope. Finally, we evaluate the performance of our criteria when applied to\nmock LSST observations of SNe Ia.",
        "Online fraud substantially harms individuals and seniors are\ndisproportionately targeted. While family is crucial for seniors, little\nresearch has empirically examined how they protect seniors against fraud. To\naddress this gap, we employed an inductive thematic analysis of 124 posts and\n16,872 comments on RedNote (Xiaohongshu), exploring the family support\necosystem for senior-targeted online fraud in China. We develop a taxonomy of\nsenior-targeted online fraud from a familial perspective, revealing younger\nmembers often spot frauds hard for seniors to detect, such as unusual charges.\nYounger family members fulfill multiple safeguarding roles, including\npreventative measures, fraud identification, fraud persuasion, loss recovery,\nand education. They also encounter numerous challenges, such as seniors'\nrefusal of help and considerable mental and financial stress. Drawing on these,\nwe develop a conceptual framework to characterize family support in\nsenior-targeted fraud, and outline implications for researchers and\npractitioners to consider the broader stakeholder ecosystem and cultural\naspects.",
        "An approach is presented to address singularities in general relativity using\na complex Riemannian spacetime extension. We demonstrate how this method can be\napplied to both black hole and cosmological singularities, specifically\nfocusing on the Schwarzschild and Kerr black holes and the\nFriedmann-Lema\\^itre-Robertson-Walker (FLRW) Big Bang cosmology. By extending\nthe relevant coordinates into the complex plane and carefully choosing\nintegration contours, we show that it is possible to regularize these\nsingularities, resulting in physically meaningful, singularity-free solutions\nwhen projected back onto real spacetime. The removal of the singularity at the\nBig Bang allows for a bounce cosmology. This approach offers a potential bridge\nbetween classical general relativity and quantum gravity effects, suggesting a\nway to resolve longstanding issues in gravitational physics without requiring a\nfull theory of quantum gravity.",
        "Late gadolinium enhancement MRI (LGE MRI) is the gold standard for the\ndetection of myocardial scars for post myocardial infarction (MI). LGE MRI\nrequires the injection of a contrast agent, which carries potential side\neffects and increases scanning time and patient discomfort. To address these\nissues, we propose a novel framework that combines cardiac motion observed in\ncine MRI with image texture information to segment the myocardium and scar\ntissue in the left ventricle. Cardiac motion tracking can be formulated as a\nfull cardiac image cycle registration problem, which can be solved via deep\nneural networks. Experimental results prove that the proposed method can\nachieve scar segmentation based on non-contrasted cine images with comparable\naccuracy to LGE MRI. This demonstrates its potential as an alternative to\ncontrast-enhanced techniques for scar detection.",
        "The widespread adoption of deep neural networks (DNNs) requires efficient\ntechniques for safety verification. Existing methods struggle to scale to\nreal-world DNNs, and tremendous efforts are being put into improving their\nscalability. In this work, we propose an approach for improving the scalability\nof DNN verifiers using Conflict-Driven Clause Learning (CDCL) -- an approach\nthat has proven highly successful in SAT and SMT solving. We present a novel\nalgorithm for deriving conflict clauses using UNSAT proofs, and propose several\noptimizations for expediting it. Our approach allows a modular integration of\nSAT solvers and DNN verifiers, and we implement it on top of an interface\ndesigned for this purpose. The evaluation of our implementation over several\nbenchmarks suggests a 2X--3X improvement over a similar approach, with specific\ncases outperforming the state of the art.",
        "Measurements of the sparsity of galaxy clusters can be used to probe the\ncosmological information encoded in the host dark matter halo profile, and\ninfer constraints on the cosmological model parameters. Key to the success of\nthese analyses is the control of potential sources of systematic uncertainty.\nAs an example, the presence of baryons can alter the cluster sparsity with\nrespect to predictions from N-body simulations. Similarly, a radial dependent\nmass bias, as in the case of masses inferred under the hydrostatic equilibrium\n(HE) hypothesis, can affect sparsity estimates. We examine the imprint of\nbaryonic processes on the sparsity statistics. Then, we investigate the\nrelation between cluster sparsities and gas mass fraction. Finally, we perform\na study of the impact of HE mass bias on sparsity measurements and the\nimplication on cosmological parameter inference analyses. We use catalogues of\nsimulated galaxy clusters from The Three Hundred project and run a comparative\nanalysis of the sparsity of clusters from N-body\/hydro simulations implementing\ndifferent feedback model scenarios. Sparsities which probe the mass profile\nacross a large radial range are affected by the presence of baryons in a way\nthat is particularly sensitive to astrophysical feedback, whereas those probing\nexclusively external cluster regions are less affected. In the former case, we\nfind the sparsities to be moderately correlated with measurements of the gas\nfraction in the inner cluster regions. We infer constraints on $S_8$ using\nsynthetic average sparsity measurements generated to evaluate the impact of\nbaryons, selection effects and HE bias. In the case of multiple sparsities\nthese lead to highly bias results. Hence, we calibrate linear bias models that\nenable us to correct for these effects and recover unbiased constraints that\nare significantly tighter than those inferred from single sparsity analyses.",
        "Recent advances in large language models have demonstrated remarkable\nreasoning capabilities through Chain of Thought (CoT) prompting, but often at\nthe cost of excessive verbosity in their intermediate outputs, which increases\ncomputational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting\nframework that combines cognitive-inspired reasoning paradigms with linguistic\nconstraints to minimize token usage while preserving reasoning accuracy. SoT is\ndesigned as a flexible framework that can incorporate any custom reasoning\nparadigms based on cognitive science, and we instantiate it with three such\nparadigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each\ntailored to different reasoning tasks and selected dynamically via a\nlightweight routing model. Through comprehensive evaluation across 15 reasoning\ndatasets with multiple languages and multimodal scenarios, we demonstrate that\nSoT achieves token reductions of 76% with negligible accuracy impact. In\ncertain domains like mathematical and multi-hop reasoning, it even improves\naccuracy while using significantly fewer tokens. Our code is publicly\navailable: https:\/\/www.github.com\/SimonAytes\/SoT.",
        "Transformer-based language models have recently been at the forefront of\nactive research in text generation. However, these models' advances come at the\nprice of prohibitive training costs, with parameter counts in the billions and\ncompute requirements measured in petaflop\/s-decades. In this paper, we\ninvestigate transformer-based architectures for improving model performance in\na low-data regime by selectively replacing attention layers with feed-forward\nand quasi-recurrent neural network layers. We test these architectures on the\nstandard Enwik8 and Wikitext-103 corpora. Our results show that our reduced\narchitectures outperform existing models with a comparable number of\nparameters, and obtain comparable performance to larger models while\nsignificantly reducing the number of parameters.",
        "Semi-supervised graph anomaly detection (GAD) has recently received\nincreasing attention, which aims to distinguish anomalous patterns from graphs\nunder the guidance of a moderate amount of labeled data and a large volume of\nunlabeled data. Although these proposed semi-supervised GAD methods have\nachieved great success, their superior performance will be seriously degraded\nwhen the provided labels are extremely limited due to some unpredictable\nfactors. Besides, the existing methods primarily focus on anomaly detection in\nstatic graphs, and little effort was paid to consider the continuous evolution\ncharacteristic of graphs over time (dynamic graphs). To address these\nchallenges, we propose a novel GAD framework (EL$^{2}$-DGAD) to tackle anomaly\ndetection problem in dynamic graphs with extremely limited labels.\nSpecifically, a transformer-based graph encoder model is designed to more\neffectively preserve evolving graph structures beyond the local neighborhood.\nThen, we incorporate an ego-context hypersphere classification loss to classify\ntemporal interactions according to their structure and temporal neighborhoods\nwhile ensuring the normal samples are mapped compactly against anomalous data.\nFinally, the above loss is further augmented with an ego-context contrasting\nmodule which utilizes unlabeled data to enhance model generalization. Extensive\nexperiments on four datasets and three label rates demonstrate the\neffectiveness of the proposed method in comparison to the existing GAD methods.",
        "Exploiting the first measurements of the same ion species in O+O collisons at\nRHIC and LHC, we propose an experimentally accessible observable to distinguish\nwhether collective behaviour builds up through a hydrodynamic expansion of a\nstrongly interacting QGP or through few rescatterings in a non-equilibrated\ndilute medium. Our procedure allows to disentangle the effects of the initial\nstate geometry and the dynamical response mechanism on the total resulting\nanisotropic flow. We validate the ability of our proposed observable to\ndiscriminate between systems with different interaction rates using results\nfrom event-by-event simulations in kinetic theory in the Relaxation Time\nApproximation (RTA). As a proof of concept, we extract the degree of\nhydrodynamization for Pb+Pb collisions at LHC from experimental data.",
        "Holographic multimode fibre endoscopes have recently shown their ability to\nunveil and monitor deep brain structures with sub-micrometre resolution,\nestablishing themselves as a minimally-invasive technology with promising\napplications in neurobiology. In this approach, holographic control of the\ninput light field entering the multimode fibres is achieved by means of\nwavefront shaping, usually treating the fibre as a complex medium. In contrast\nto other unpredictable and highly scattering complex media, multimode fibres\nfeature symmetries and strong correlations between their input and output\nfields. Both step-index and graded-index multimode fibres offer a specific set\nof such correlations which, when appropriately leveraged, enable generating\nhigh-quality focused pulses with minimal intermodal dispersion. With this, we\nfunnelled pulsed super-resolution STED microscopy with time-gated detection\nthrough a custom multimode fibre probe, combining the correlations of both\nmultimode fibre types. We demonstrate resolution improvements over 3-times\nbeyond the diffraction limit and showcase its applicability in bioimaging. This\nwork provides not only a solution for delivering short pulses through\nstep-index multimode fibre segments but also marks a step towards bringing\nadvanced super-resolution imaging techniques with virtually no depth\nlimitations.",
        "LiDAR-Inertial Odometry (LIO) is widely used for autonomous navigation, but\nits deployment on Size, Weight, and Power (SWaP)-constrained platforms remains\nchallenging due to the computational cost of processing dense point clouds.\nConventional LIO frameworks rely on a single onboard processor, leading to\ncomputational bottlenecks and high memory demands, making real-time execution\ndifficult on embedded systems. To address this, we propose QLIO, a\nmulti-processor distributed quantized LIO framework that reduces computational\nload and bandwidth consumption while maintaining localization accuracy. QLIO\nintroduces a quantized state estimation pipeline, where a co-processor\npre-processes LiDAR measurements, compressing point-to-plane residuals before\ntransmitting only essential features to the host processor. Additionally, an\nrQ-vector-based adaptive resampling strategy intelligently selects and\ncompresses key observations, further reducing computational redundancy.\nReal-world evaluations demonstrate that QLIO achieves a 14.1% reduction in\nper-observation residual data while preserving localization accuracy.\nFurthermore, we release an open-source implementation to facilitate further\nresearch and real-world deployment. These results establish QLIO as an\nefficient and scalable solution for real-time autonomous systems operating\nunder computational and bandwidth constraints.",
        "Continuum robots with floating bases demonstrate exceptional operational\ncapabilities in confined spaces, such as those encountered in medical surgeries\nand equipment maintenance. However, developing low-cost solutions for their\nmotion and planning problems remains a significant challenge in this field.\nThis paper investigates the application of geometric iterative strategy methods\nto continuum robots, and proposes the algorithm based on an improved two-layer\ngeometric iterative strategy for motion planning. First, we thoroughly study\nthe kinematics and effective workspace of a multi-segment tendon-driven\ncontinuum robot with a floating base. Then, generalized iterative algorithms\nfor solving arbitrary-segment continuum robots are proposed based on a series\nof problems such as initial arm shape dependence exhibited by similar methods\nwhen applied to continuum robots. Further, the task scenario is extended to a\nfollow-the-leader task considering environmental factors, and further extended\nalgorithm are proposed. Simulation comparison results with similar methods\ndemonstrate the effectiveness of the proposed method in eliminating the initial\narm shape dependence and improving the solution efficiency and accuracy. The\nexperimental results further demonstrate that the method based on improved\ntwo-layer geometric iteration can be used for motion planning task of a\ncontinuum robot with a floating base, under an average deviation of about 4 mm\nin the end position, an average orientation deviation of no more than 1 degree,\nand the reduction of average number of iterations and time cost is 127.4\niterations and 72.6 ms compared with similar methods, respectively.",
        "The conference \\emph{Black Holes Inside and Out} marked the 50th anniversary\nof Hawking's seminal paper on black hole radiance. It was clear already from\nHawking's analysis that a proper quantum gravity theory would be essential for\na more complete understanding of the evaporation process. This task was\nundertaken in Loop Quantum Gravity (LQG) two decades ago and by now the\nliterature on the subject is quite rich. The goal of this contribution is to\nsummarize a mainstream perspective that has emerged. The intended audience is\nthe broader gravitational physics community, rather than quantum gravity\nexperts. Therefore, the emphasis is on conceptual issues, especially on the key\nfeatures that distinguish the LQG approach, and on concrete results that\nunderlie the paradigm that has emerged. This is \\emph{not} meant to be an\nexhaustive review. Rather, it is a broad-brush stroke portrait of the present\nstatus. Further details can be found in the references listed.",
        "Artificial intelligence (AI) systems have been increasingly adopted in the\nManufacturing Industrial Internet (MII). Investigating and enabling the AI\nresilience is very important to alleviate profound impact of AI system failures\nin manufacturing and Industrial Internet of Things (IIoT) operations, leading\nto critical decision making. However, there is a wide knowledge gap in defining\nthe resilience of AI systems and analyzing potential root causes and\ncorresponding mitigation strategies. In this work, we propose a novel framework\nfor investigating the resilience of AI performance over time under hazard\nfactors in data quality, AI pipelines, and the cyber-physical layer. The\nproposed method can facilitate effective diagnosis and mitigation strategies to\nrecover AI performance based on a multimodal multi-head self latent attention\nmodel. The merits of the proposed method are elaborated using an MII testbed of\nconnected Aerosol Jet Printing (AJP) machines, fog nodes, and Cloud with\ninference tasks via AI pipelines.",
        "Autonomous robotic grasping of uncertain objects in uncertain environments is\nan impactful open challenge for the industries of the future. One such industry\nis the recycling of Waste Electrical and Electronic Equipment (WEEE) materials,\nin which electric devices are disassembled and readied for the recovery of raw\nmaterials. Since devices may contain hazardous materials and their disassembly\ninvolves heavy manual labor, robotic disassembly is a promising venue. However,\nsince devices may be damaged, dirty and unidentified, robotic disassembly is\nchallenging since object models are unavailable or cannot be relied upon. This\ncase study explores grasping strategies for industrial robotic disassembly of\nWEEE devices with uncertain vision data. We propose three grippers and\nappropriate tactile strategies for force-based manipulation that improves\ngrasping robustness. For each proposed gripper, we develop corresponding\nstrategies that can perform effectively in different grasping tasks and\nleverage the grippers design and unique strengths. Through experiments\nconducted in lab and factory settings for four different WEEE devices, we\ndemonstrate how object uncertainty may be overcome by tactile sensing and\ncompliant techniques, significantly increasing grasping success rates.",
        "We focus on human-robot collaborative transport, in which a robot and a user\ncollaboratively move an object to a goal pose. In the absence of explicit\ncommunication, this problem is challenging because it demands tight implicit\ncoordination between two heterogeneous agents, who have very different sensing,\nactuation, and reasoning capabilities. Our key insight is that the two agents\ncan coordinate fluently by encoding subtle, communicative signals into actions\nthat affect the state of the transported object. To this end, we design an\ninference mechanism that probabilistically maps observations of joint actions\nexecuted by the two agents to a set of joint strategies of workspace traversal.\nBased on this mechanism, we define a cost representing the human's uncertainty\nover the unfolding traversal strategy and introduce it into a model predictive\ncontroller that balances between uncertainty minimization and efficiency\nmaximization. We deploy our framework on a mobile manipulator (Hello Robot\nStretch) and evaluate it in a within-subjects lab study (N=24). We show that\nour framework enables greater team performance and empowers the robot to be\nperceived as a significantly more fluent and competent partner compared to\nbaselines lacking a communicative mechanism.",
        "Recent breakthroughs in large-scale generative modeling have demonstrated the\npotential of foundation models in domains such as natural language, computer\nvision, and protein structure prediction. However, their application in the\nenergy and smart grid sector remains limited due to the scarcity and\nheterogeneity of high-quality data. In this work, we propose a method for\ncreating high-fidelity electricity consumption time series data for rare and\nunseen context variables (e.g. location, building type, photovoltaics). Our\napproach, Context Encoding and Normalizing Time Series Generation, or CENTS,\nincludes three key innovations: (i) A context normalization approach that\nenables inverse transformation for time series context variables unseen during\ntraining, (ii) a novel context encoder to condition any state-of-the-art\ntime-series generator on arbitrary numbers and combinations of context\nvariables, (iii) a framework for training this context encoder jointly with a\ntime-series generator using an auxiliary context classification loss designed\nto increase expressivity of context embeddings and improve model performance.\nWe further provide a comprehensive overview of different evaluation metrics for\ngenerative time series models. Our results highlight the efficacy of the\nproposed method in generating realistic household-level electricity consumption\ndata, paving the way for training larger foundation models in the energy domain\non synthetic as well as real-world data.",
        "X-ray emission from wind-driven bow shocks is both difficult to measure and\npredict, but may give important insights into the energy budget of the hot\nphase of the ISM by quantifying mixing at the interface between hot and warm\ngas phases. We investigate the effect of magnetic fields and numerical\nresolution on predicted X-ray emission and other observable properties of bow\nshocks, to study convergence properties and assess robustness of predicted\nobservables from simulations. A suite of 2D and 3D HD and MHD simulations of\nbow shocks were run and analysed to generate synthetic emission maps and light\ncurves in X-ray and infrared emission. Resolving the Kelvin-Helmholtz (KH)\ninstability at the wind-ISM contact discontinuity is crucial for obtaining\nconverged results and for predicting X-ray emission and the properties of the\nhot shocked wind. When sufficient spatial resolution is used, we measure time\nvariation of X-ray emission of at least an order of magnitude on a timescale\ncomparable to the advection timescale of the wake downstream from the bow\nshock. Good correspondence is found between 2D and 3D simulations with\ncomparable resolution, and 3D simulations can achieve the required resolution\nwith reasonable computing resources. Development of the KH instability is\ninhibited for shear flows parallel to the ISM magnetic field, compared with\nwhat is seen in the perpendicular direction, resulting in synthetic IR emission\nmaps of bow shocks that are smooth when seen from one perspective but show\nstrong distortions from another. Measuring the X-ray morphology and luminosity\nin bow shocks may be useful for constraining mixing and energy-transfer rates\nbetween hot and warm gas phases of the ISM. Dynamical instabilities at the\nwind-ISM interface are a crucial ingredient in determining the properties of\nthe hot-gas phase in stellar bow-shocks, in particular to capture its time\ndependence.",
        "Kinetic plasma studies often require computing integrals of the velocity\ndistribution over a complex-valued pole. The standard method is to solve the\nintegral in the complex plane using the Plemelj theorem, resulting in the\nstandard plasma dispersion function for Maxwellian plasmas. For non-Maxwellian\nplasmas, the Plemelj theorem does not generalize to an analytic form, and\ncomputational methods must be used. In this paper, a new computational method\nis developed to accurately integrate a non-Maxwellian velocity distribution\nover an arbitrary set of complex valued poles. This method works by keeping the\nintegration contour on the real line, and applying a trapezoid rule-like\nintegration scheme over all discretized intervals. In intervals containing a\npole, the velocity distribution is linearly interpolated, and the analytic\nresult for the integral over a linear function is used. The integration scheme\nis validated by comparing its results to the analytic plasma dispersion\nfunction for Maxwellian distributions. We then show the utility of this method\nby computing the Thomson scattering spectra for several non-Maxwellian\ndistributions: the kappa, super Gaussian, and toroidal distributions. Thomson\nscattering is a valuable plasma diagnostic tool for both laboratory and space\nplasmas, but the technique relies on fitting measured wave spectra to a forward\nmodel, which typically assumes Maxwellian plasmas. Therefore, this integration\nmethod can expand the capabilities of Thomson scatter diagnostics to regimes\nwhere the plasma is non-Maxwellian, including high energy density plasmas,\nfrictionally heated plasmas in the ionosphere, and plasmas with a substantial\nsuprathermal electron tail.",
        "The growing rate of chronic wound occurrence, especially in patients with\ndiabetes, has become a concerning trend in recent years. Chronic wounds are\ndifficult and costly to treat, and have become a serious burden on health care\nsystems worldwide. Chronic wounds can have devastating consequences for the\npatient, with infection often leading to reduced quality of life and increased\nmortality risk. Innovative deep learning methods for the detection and\nmonitoring of such wounds have the potential to reduce the impact to both\npatient and clinician. We present a novel multimodal segmentation method which\nallows for the introduction of patient metadata into the training workflow\nwhereby the patient data are expressed as Gaussian random fields. Our results\nindicate that the proposed method improved performance when utilising multiple\nmodels, each trained on different metadata categories. Using the Diabetic Foot\nUlcer Challenge 2022 test set, when compared to the baseline results\n(intersection over union = 0.4670, Dice similarity coefficient = 0.5908) we\ndemonstrate improvements of +0.0220 and +0.0229 for intersection over union and\nDice similarity coefficient respectively. This paper presents the first study\nto focus on integrating patient data into a chronic wound segmentation\nworkflow. Our results show significant performance gains when training\nindividual models using specific metadata categories, followed by average\nmerging of prediction masks using distance transforms. All source code for this\nstudy is available at:\nhttps:\/\/github.com\/mmu-dermatology-research\/multimodal-grf",
        "In our daily lives, we can naturally convey instructions for the spatial\nmanipulation of objects using words and gestures. Transposing this form of\ninteraction into virtual reality (VR) object manipulation can be beneficial. We\npropose VR Mover, an LLM-empowered solution that can understand and interpret\nthe user's vocal instruction to support object manipulation. By simply pointing\nand speaking, the LLM can manipulate objects without structured input. Our user\nstudy demonstrates that VR Mover enhances user usability, overall experience\nand performance on multi-object manipulation, while also reducing workload and\narm fatigue. Users prefer the proposed natural interface for broad movements\nand may complementarily switch to gizmos or virtual hands for finer\nadjustments. These findings are believed to contribute to design implications\nfor future LLM-based object manipulation interfaces, highlighting the potential\nfor more intuitive and efficient user interactions in VR environments.",
        "The integration of extra-robotic limbs\/fingers to enhance and expand motor\nskills, particularly for grasping and manipulation, possesses significant\nchallenges. The grasping performance of existing limbs\/fingers is far inferior\nto that of human hands. Human hands can detect onset of slip through tactile\nfeedback originating from tactile receptors during the grasping process,\nenabling precise and automatic regulation of grip force. The frictional\ninformation is perceived by humans depending upon slip happening between finger\nand object. Enhancing this capability in extra-robotic limbs or fingers used by\nhumans is challenging. To address this challenge, this paper introduces novel\napproach to communicate frictional information to users through encoded\nvibrotactile cues. These cues are conveyed on onset of incipient slip thus\nallowing users to perceive friction and ultimately use this information to\nincrease force to avoid dropping of object. In a 2-alternative forced-choice\nprotocol, participants gripped and lifted a glass under three different\nfrictional conditions, applying a normal force of 3.5 N. After reaching this\nforce, glass was gradually released to induce slip. During this slipping phase,\nvibrations scaled according to static coefficient of friction were presented to\nusers, reflecting frictional conditions. The results suggested an accuracy of\n94.53 p\/m 3.05 (mean p\/mSD) in perceiving frictional information upon lifting\nobjects with varying friction. The results indicate effectiveness of using\nvibrotactile feedback for sensory feedback, allowing users of extra-robotic\nlimbs or fingers to perceive frictional information. This enables them to\nassess surface properties and adjust grip force according to frictional\nconditions, enhancing their ability to grasp, manipulate objects more\neffectively.",
        "This paper considers the problem of estimating chirp parameters from a noisy\nmixture of chirps. While a rich body of work exists in this area, challenges\nremain when extending these techniques to chirps of higher order polynomials.\nWe formulate this as a non-convex optimization problem and propose a modified\nLangevin Monte Carlo (LMC) sampler that exploits the average curvature of the\nobjective function to reliably find the minimizer. Results show that our\nCurvature-guided LMC (CG-LMC) algorithm is robust and succeeds even in low SNR\nregimes, making it viable for practical applications.",
        "This paper investigates the finite-horizon distributionally robust\nmixed-integer control (DRMIC) of uncertain linear systems. However, deriving an\noptimal causal feedback control policy to this DRMIC problem is computationally\nformidable for most ambiguity sets. To address the computational challenge, we\npropose a novel distributionally robust lifted control policy (DR-LCP) method\nto derive a high-quality approximate solution to this DRMIC problem for a rich\nclass of Wasserstein metric-based ambiguity sets, including the Wasserstein\nambiguity set and its variants. In theory, we analyze the asymptotic\nperformance and establish a tight non-asymptotic bound of the proposed method.\nIn numerical experiments, the proposed DR-LCP method empirically demonstrates\nsuperior performance compared with existing methods in the literature."
      ]
    }
  },
  {
    "id":2412.00225,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
    "start_abstract":"We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "The Burgers equation"
      ],
      "abstract":[
        "The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows."
      ],
      "categories":[
        "math.AP"
      ]
    },
    "list":{
      "title":[
        "On the test properties of the Frobenius endomorphism",
        "Twinning in ferromagnetic Heusler Rh2MnSb epitaxial thin films",
        "Cepheids in spectroscopic binary systems -- current status and recent\n  discoveries",
        "Efficient Diffusion Posterior Sampling for Noisy Inverse Problems",
        "Biglobal resolvent analysis of separated flow over a NACA0012 airfoil",
        "Efficient $d$-ary Cuckoo Hashing at High Load Factors by Bubbling Up",
        "Liquidity Competition Between Brokers and an Informed Trader",
        "A plastic damage model with mixed isotropic-kinematic hardening for\n  low-cycle fatigue in 7020 aluminum",
        "A Fast Decoding Algorithm for Generalized Reed-Solomon Codes and\n  Alternant Codes",
        "Dirac-type condition for Hamilton-generated graphs",
        "A free boundary approach to the quasistatic evolution of debonding\n  models",
        "The Lagrangian approach to the compressible primitive equations",
        "Chemistry in the Galactic Center",
        "On complex eigenvalues of a real nonsymmetric matrix",
        "Robust triple-q magnetic order with trainable spin vorticity in\n  Na$_2$Co$_2$TeO$_6$",
        "Optimal properties of tensor product of B-bases",
        "Validate Quantum State Preparation Programs",
        "Linear-Time User-Level DP-SCO via Robust Statistics",
        "Score-Preserving Targeted Maximum Likelihood Estimation",
        "Tomography of the Ophiuchus Molecular Cloud with Velocity Features in\n  C$_2$H $N=1-0$ spectra: A Pilot Study of Coherent Sub-structures",
        "Regularity of $3$-Path Ideals of Trees and Unicyclic Graphs",
        "Highly Uniform Magnetic and Electronic Environment in\n  Non-Centrosymmetric Superconductor LaRhGe$_3$",
        "A varifold-type estimation for data sampled on a rectifiable set",
        "Numerical Aspects of the Tensor Product Multilevel Method for\n  High-dimensional, Kernel-based Reconstruction on Sparse Grids",
        "The Computational Advantage of Depth: Learning High-Dimensional\n  Hierarchical Functions with Gradient Descent",
        "Dislocation correlations in GaN epitaxial films revealed by EBSD and XRD",
        "Flux homomorphism and bilinear form constructed from Shelukhin's\n  quasimorphism",
        "Ranking dynamics of urban mobility",
        "Why a Bose-Einstein condensate cannot exist in a system of interacting\n  bosons at ultrahigh temperatures"
      ],
      "abstract":[
        "In this paper, we prove two theorems concerning the test properties of the\nFrobenius endomorphism over commutative Noetherian local rings of prime\ncharacteristic $p$. Our first theorem generalizes a result of Funk-Marley on\nthe vanishing of Ext and Tor modules, while our second theorem generalizes one\nof our previous results on maximal Cohen-Macaulay tensor products. In these\nearlier results, we replace $^{e}R$ with a more general module $^{e}M$, where\n$R$ is a Cohen-Macaulay ring, $M$ is a Cohen-Macaulay $R$-module with full\nsupport, and $^{e}M$ is the module viewed as an $R$-module via the $e$-th\niteration of the Frobenius endomorphism. We also provide examples and present\napplications of our results, yielding new characterizations of the regularity\nof local rings.",
        "Epitaxially grown full Heusler alloy of Rh2MnSb thin films were prepared for\nthe first time using DC magnetron sputtering. The films were deposited on MgO\n[001] substrates with a deposition temperature of 600{\\deg}C, 700{\\deg}C, and\n800{\\deg}C. We report the structural, morphological, optical, magneto-optical,\nand magnetic properties of the films with a 200 nm nominal thickness. The\ngrown-at-600{\\deg}C film was close to stoichiometric and exhibited L21 ordering\ntypical for Heusler alloys. The single-phase Rh2MnSb film had a tetragonal\nstructure with lattice parameters close to the bulk material. X-ray\nphotoelectron spectroscopy revealed the metallic character of the film free\nfrom contamination. The tetragonal films exhibited discernible regular twinning\nwith the majority of twin domains with the c-axis perpendicular to the surface\ndue to a substrate constraint. The twin formation was studied by atomic force\nand transmission electron microscopy and by X-ray diffraction. Magnetic\nmeasurements showed TC of about 220-275 K and saturation magnetization of about\n55 emu\/g, close to the bulk material. Magneto-optical Kerr effect measurements\nof the film prepared at 600 {\\deg}C affirmed paramagnetic behavior at room\ntemperature and suggested the half-metallic behavior. The observed properties\nhighlight the potential for further investigations of Rh2MnSb's thin films,\nfocusing on compositional and structural control.",
        "We present a summary of the current knowledge about Cepheids in binary\nsystems. We focus on the most recent findings and discoveries, such as the\nhighly increasing number of confirmed and candidate spectroscopic binary\nCepheids and the progress in determining their physical parameters. This\nincludes new and newly analyzed binary Cepheids in the Milky Way and Magellanic\nClouds. We will provide an update on the project to increase the number of the\nmost valuable Cepheids in double-lined binary (SB2) systems from six to more\nthan 100. To date, we have confirmed 60 SB2 systems, including detecting a\nsignificant orbital motion for 37. We identified systems with orbital periods\nup to five times shorter than the shortest period reported before and systems\nwith mass ratios significantly different from unity (suggesting past binary\ninteractions, including merger events). Both features are essential to\nunderstanding how multiplicity affects the formation and destruction of Cepheid\nprogenitors and how this influences global Cepheid properties. We will also\npresent nine new systems composed of two Cepheids. Only one such double Cepheid\nsystem was known before.",
        "The pretrained diffusion model as a strong prior has been leveraged to\naddress inverse problems in a zero-shot manner without task-specific\nretraining. Different from the unconditional generation, the measurement-guided\ngeneration requires estimating the expectation of clean image given the current\nimage and the measurement. With the theoretical expectation expression, the\ncrucial task of solving inverse problems is to estimate the noisy likelihood\nfunction at the intermediate image sample. Using the Tweedie's formula and the\nknown noise model, the existing diffusion posterior sampling methods perform\ngradient descent step with backpropagation through the pretrained diffusion\nmodel. To alleviate the costly computation and intensive memory consumption of\nthe backpropagation, we propose an alternative maximum-a-posteriori (MAP)-based\nsurrogate estimator to the expectation. With this approach and further density\napproximation, the MAP estimator for linear inverse problem is the solution to\na traditional regularized optimization, of which the loss comprises of data\nfidelity term and the diffusion model related prior term. Integrating the MAP\nestimator into a general denoising diffusion implicit model (DDIM)-like\nsampler, we achieve the general solving framework for inverse problems. Our\napproach highly resembles the existing $\\Pi$GDM without the manifold projection\noperation of the gradient descent direction. The developed method is also\nextended to nonlinear JPEG decompression. The performance of the proposed\nposterior sampling is validated across a series of inverse problems, where both\nVP and VE SDE-based pretrained diffusion models are taken into consideration.",
        "The effects of Reynolds number across $Re=1000$, $2500$, $5000$, and $10000$\non separated flow over a two-dimensional NACA0012 airfoil at an angle of attack\nof $\\alpha=14^\\circ$ are investigated through the biglobal resolvent analysis.\nWe identify modal structures and energy amplifications over a range of\nfrequency, spanwise wavenumber, and discount parameter, providing insights\nacross various timescales. Using temporal discounting, we find that the shear\nlayer dynamics dominates over short time horizons, while the wake dynamics\nbecomes the primary amplification mechanism over long time horizons. Spanwise\neffects also appear over long time horizon, sustained by low frequencies. At a\nfixed timescale, we investigate the influence of Reynolds number on response\nand forcing mode structures, as well as the energy gain over different\nfrequencies. Across all Reynolds numbers, the response modes shift from\nwake-dominated structures at low frequencies to shear layer-dominated\nstructures at higher frequencies. The frequency at which the dominant mechanism\nchanges is independent of the Reynolds number. The response mode structures\nshow similarities across different Reynolds numbers, with local streamwise\nwavelengths only depending on frequency. Comparisons at a different angle of\nattack ($\\alpha=9^\\circ$) show that the transition from wake to shear layer\ndynamics with increasing frequency only occurs if the unsteady flow is\nthree-dimensional. We also study the dominant frequencies associated with wake\nand shear layer dynamics across the angles of attack and Reynolds numbers, and\npresent the characteristic scaling for each mechanism.",
        "A $d$-ary cuckoo hash table is an open-addressed hash table that stores each\nkey $x$ in one of $d$ random positions $h_1(x), h_2(x), \\ldots, h_d(x)$. In the\noffline setting, where all items are given and keys need only be matched to\nlocations, it is possible to support a load factor of $1 - \\epsilon$ while\nusing $d = \\lceil \\ln \\epsilon^{-1} + o(1) \\rceil$ hashes. The online setting,\nwhere keys are moved as new keys arrive sequentially, has the additional\nchallenge of the time to insert new keys, and it has not been known whether one\ncan use $d = O(\\ln \\epsilon^{-1})$ hashes to support $\\poly(\\epsilon^{-1})$\nexpected-time insertions.\n  In this paper, we introduce bubble-up cuckoo hashing, an implementation of\n$d$-ary cuckoo hashing that achieves all of the following properties\nsimultaneously:\n  (1) uses $d = \\lceil \\ln \\epsilon^{-1} + \\alpha \\rceil$ hash locations per\nitem for an arbitrarily small positive constant $\\alpha$.\n  (2) achieves expected insertion time $O(\\delta^{-1})$ for any insertion\ntaking place at load factor $1 - \\delta \\le 1 - \\epsilon$.\n  (3) achieves expected positive query time $O(1)$, independent of $d$ and\n$\\epsilon$.\n  The first two properties give an essentially optimal value of $d$ without\ncompromising insertion time. The third property is interesting even in the\noffline setting: it says that, even though \\emph{negative} queries must take\ntime $d$, positive queries can actually be implemented in $O(1)$ expected time,\neven when $d$ is large.",
        "We study a multi-agent setting in which brokers transact with an informed\ntrader. Through a sequential Stackelberg-type game, brokers manage trading\ncosts and adverse selection with an informed trader. In particular, supplying\nliquidity to the informed traders allows the brokers to speculate based on the\nflow information. They simultaneously attempt to minimize inventory risk and\ntrading costs with the lit market based on the informed order flow, also known\nas the internalization-externalization strategy. We solve in closed form for\nthe trading strategy that the informed trader uses with each broker and propose\na system of equations which classify the equilibrium strategies of the brokers.\nBy solving these equations numerically we may study the resulting strategies in\nequilibrium. Finally, we formulate a competitive game between brokers in order\nto determine the liquidity prices subject to precommitment supplied to the\ninformed trader and provide a numerical example in which the resulting\nequilibrium is not Pareto efficient.",
        "The paper at hand presents an in-depth investigation into the fatigue\nbehavior of the high-strength aluminum alloy EN AW-7020 T6 using both\nexperimental and numerical approaches. Two types of specimens are investigated:\na dog-bone specimen subjected to cyclic loading in a symmetric\nstrain-controlled regime, and a compact tension specimen subjected to repeated\nloading and unloading, which leads to damage growth from the notch tip.\nExperimental data from these tests are used to identify the different phases of\nfatigue. Subsequently, a plastic-damage model is developed, incorporating J2\nplasticity with Chaboche-type mixed isotropic-kinematic hardening. A detailed\ninvestigation reveals that the Chaboche model must be blended with a suitable\nisotropic hardening and combined with a proper damage growth model to\naccurately describe cyclic fatigue including large plastic strains up to\nfailure. Multiple back-stress components with independent properties are\nsuperimposed, and exponential isotropic hardening with saturation effects is\nintroduced to improve alignment with experimental results. For damage,\ndifferent stress splits are tested, with the deviatoric\/volumetric split\nproving successful in reproducing the desired degradation in peak stress and\nstiffness. A nonlinear activation function is introduced to ensure smooth\ntransitions between tension and compression. Two damage indices, one for the\ndeviatoric part and one for the volumetric part, are defined, each of which is\ngoverned by a distinct trilinear damage growth function. The governing\ndifferential equation of the problem is regularized by higher-order gradient\nterms to address the ill-posedness induced by softening. Finally, the\nplasticity model is calibrated using finite element simulations of the dog-bone\ntest and subsequently applied to the cyclic loading of the compact tension\nspecimen.",
        "In this paper, it is shown that the syndromes of generalized Reed-Solomon\n(GRS) codes and alternant codes can be characterized in terms of inverse fast\nFourier transform, regardless of code definitions. Then a fast decoding\nalgorithm is proposed, which has a computational complexity of $O(n\\log(n-k) +\n(n-k)\\log^2(n-k))$ for all $(n,k)$ GRS codes and $(n,k)$ alternant codes.\nParticularly, this provides a new decoding method for Goppa codes, which is an\nimportant subclass of alternant codes. When decoding the binary Goppa code with\nlength $8192$ and correction capability $128$, the new algorithm is nearly 10\ntimes faster than traditional methods. The decoding algorithm is suitable for\nthe McEliece cryptosystem, which is a candidate for post-quantum cryptography\ntechniques.",
        "The cycle space $\\mathcal{C}(G)$ of a graph $G$ is defined as the linear\nspace spanned by all cycles in $G$. For an integer $k\\ge 3$, let $\\mathcal{C}_k\n(G)$ denote the subspace of $\\mathcal{C}(G)$ generated by the cycles of length\nexactly $k$. A graph $G$ on $n$ vertices is called Hamilton-generated if\n$\\mathcal{C}_n (G) = \\mathcal{C}(G)$, meaning every cycle in $G$ is a symmetric\ndifference of some Hamilton cycles of $G$. %A necessary condition for this\nproperty is that $n$ must be odd. Heinig (European J. Combin., 2014) showed\nthat for any $\\sigma >0$ and sufficiently large odd $n$, every $n$-vertex graph\nwith minimum degree $(1+ \\sigma)n\/2$ is Hamilton-generated. He further posed\nthe question that whether the minimum degree requirement could be lowered to\nthe Dirac threshold $n\/2$. Recent progress by Christoph, Nenadov, and\nPetrova~(arXiv:2402.01447) reduced the minimum degree condition to $n\/2 + C$\nfor some large constant $C$. In this paper, we resolve Heinig's problem\ncompletely by proving that for sufficiently large odd $n$, every\nHamilton-connected graph $G$ on $n$ vertices with minimum degree at least\n$(n-1)\/2$ is Hamilton-generated. Moreover, this result is tight for the minimum\ndegree and the Hamilton-connected condition. The proof relies on the\nparity-switcher technique introduced by Christoph, et al in their recent work,\nas well as a classification lemma that strengthens a previous result by\nKrivelevich, Lee, and Sudakov~(Trans. Amer. Math. Soc., 2014).",
        "The mechanical process of progressively debonding an adhesive membrane from a\nsubstrate is described as a quasistatic variational evolution of sets and\nherein investigated. Existence of energetic solutions, based on global\nminimisers of a suitable functional together with an energy balance, is\nobtained within the natural class of open sets, improving and simplifying\nprevious results known in literature. The proposed approach relies on an\nequivalent reformulation of the model in terms of the celebrated one-phase\nBernoulli free boundary problem. This point of view allows performing the\nMinimizing Movements scheme in spaces of functions instead of the more\ncomplicated framework of sets. Nevertheless, in order to encompass\nirreversibility of the phenomenon, it remains crucial to keep track of the\ndebonded region at each discrete time-step, thus actually resulting in a\ncoupled algorithm.",
        "This article develops the hydrostatic Lagrangian approach to the compressible\nprimitive equations. A fundamental aspect in the analysis is the investigation\nof the compressible hydrostatic Lam\\'{e} and Stokes operators. Local strong\nwell-posedness for large data and global strong well-posedness for small data\nare established under various assumptions on the pressure law, both in the\npresence and absence of gravity.",
        "Gas and dust in the Galactic Center are subjected to energetic processing by\nintense UV radiation fields, widespread shocks, enhanced rates of cosmic-rays\nand X-rays, and strong magnetic fields. The Giant Molecular Clouds in the\nGalactic Center present a rich chemistry in a wide variety of chemical\ncompounds, some of which are prebiotic. We have conducted unbiased,\nultrasensitive and broadband spectral surveys toward the G+0.693-0.027\nmolecular cloud located in the Galactic Center, which have yielded the\ndiscovery of new complex organic molecules proposed as precursors of the\n\"building blocks\" of life. I will review our current understanding of the\nchemistry in Galactic Center molecular clouds, and summarize the recent\ndetections toward G+0.693-0.027 of key precursors of prebiotic chemistry. All\nthis suggests that the ISM is an important source of prebiotic material that\ncould have contributed to the process of the origin of life on Earth and\nelsewhere in the Universe.",
        "We consider real non-symmetric matrices and their factorisation as a product\nof real symmetric matrices. The number of complex eigenvalues of the original\nmatrix reveals restrictions on such factorisations as we shall prove.",
        "Recent studies suggest that the candidate Kitaev magnet Na$_2$Co$_2$TeO$_6$\npossesses novel triple-$\\mathbf{q}$ magnetic order instead of conventional\nsingle-$\\mathbf{q}$ zigzag order. Here we present dedicated experiments in\nsearch for distinct properties expected of the triple-$\\mathbf{q}$ order,\nnamely, insensitivity of the magnetic domains to weak $C_3$ symmetry-breaking\nfields and fictitious magnetic fields generated by the spin vorticity. In\nstructurally pristine single crystals, we show that $C_3$ symmetry-breaking\nin-plane uniaxial strains do not affect the order's magnetic neutron\ndiffraction signals. We further show that $\\mathbf{c}$-axis propagating light\nexhibits large Faraday rotations in the ordered state due to the spin\nvorticity, the sign of which can be trained via the system's ferrimagnetic\nmoment. These results are in favor of the triple-$\\mathbf{q}$ order in\nNa$_2$Co$_2$TeO$_6$ and reveal its unique emerging behavior.",
        "It is proved the optimal conditioning for the infinity norm of collocation\nmatrices of the tensor product of normalized B-bases among the tensor product\nof all normalized totally positive bases of the corresponding space of\nfunctions. Bounds for the minimal eigenvalue and singular value and\nillustrative numerical examples are also included.",
        "One of the key steps in quantum algorithms is to prepare an initial quantum\nsuperposition state with different kinds of features. These so-called state\npreparation algorithms are essential to the behavior of quantum algorithms, and\ncomplicated state preparation algorithms are difficult to develop correctly and\neffectively. This paper presents Pqasm: a high-assurance framework implemented\nwith the Coq proof assistant, allowing us to certify our Pqasm tool to\ncorrectly reflect quantum program behaviors. The key in the framework is to\nreduce the program correctness assurance of a program containing a quantum\nsuperposition state to the program correctness assurance for the program state\nwithout superposition. The reduction allows the development of an effective\ntesting framework for testing quantum state preparation algorithm\nimplementations on a classical computer - considered to be a hard problem with\nno clear solution until this point. We utilize the QuickChick property-based\ntesting framework to test state preparation programs. We evaluated the\neffectiveness of our approach over 5 case studies implemented using Pqasm; such\ncases are not even simulatable in the current quantum simulators.",
        "User-level differentially private stochastic convex optimization (DP-SCO) has\ngarnered significant attention due to the paramount importance of safeguarding\nuser privacy in modern large-scale machine learning applications. Current\nmethods, such as those based on differentially private stochastic gradient\ndescent (DP-SGD), often struggle with high noise accumulation and suboptimal\nutility due to the need to privatize every intermediate iterate. In this work,\nwe introduce a novel linear-time algorithm that leverages robust statistics,\nspecifically the median and trimmed mean, to overcome these challenges. Our\napproach uniquely bounds the sensitivity of all intermediate iterates of SGD\nwith gradient estimation based on robust statistics, thereby significantly\nreducing the gradient estimation noise for privacy purposes and enhancing the\nprivacy-utility trade-off. By sidestepping the repeated privatization required\nby previous methods, our algorithm not only achieves an improved theoretical\nprivacy-utility trade-off but also maintains computational efficiency. We\ncomplement our algorithm with an information-theoretic lower bound, showing\nthat our upper bound is optimal up to logarithmic factors and the dependence on\n$\\epsilon$. This work sets the stage for more robust and efficient\nprivacy-preserving techniques in machine learning, with implications for future\nresearch and application in the field.",
        "Targeted maximum likelihood estimators (TMLEs) are asymptotically optimal\namong regular, asymptotically linear estimators. In small samples, however, we\nmay be far from \"asymptopia\" and not reap the benefits of optimality. Here we\npropose a variant (score-preserving TMLE; SP-TMLE) that leverages an initial\nestimator defined as the solution of a large number of possibly data-dependent\nscore equations. Instead of targeting only the efficient influence function in\nthe TMLE update to knock out the plug-in bias, we also target the\nalready-solved scores. Solving additional scores reduces the remainder term in\nthe von-Mises expansion of our estimator because these scores may come close to\nspanning higher-order influence functions. The result is an estimator with\nbetter finite-sample performance. We demonstrate our approach in simulation\nstudies leveraging the (relaxed) highly adaptive lasso (HAL) as our initial\nestimator. These simulations show that in small samples SP-TMLE has reduced\nbias relative to plug-in HAL and reduced variance relative to vanilla TMLE,\nblending the advantages of the two approaches. We also observe improved\nestimation of standard errors in small samples.",
        "The C$_2$H $N=1-0$ transition was used to investigate the possible line of\nsight sub-structures from the dense and optically thick in $^{13}$CO $J=1-0$\nregions in the Ophiuchus star forming molecular cloud. With a 0.2 K or lower\nnoise, multi-peak spectra were obtained and then used for identifying\nsub-structures. There are clues, e.g., the core velocity dispersion remains\nunchanged with the increasing scale that this cloud has a mild thickness in the\nline of sight direction and a large amount of overlapping CO cores, as\nexpected, at least two coherent layers have been found. The integrated\nintensity maps of these two layers are different in shape and morphology.\nInferred from the point velocity dispersion, one sub-structure with a thickness\nof $\\sim 1$ pc was found, while other substructures were more likely to be\nfragments.",
        "Let $G$ be a simple graph and $I_3(G)$ be its $3$-path ideal in the\ncorresponding polynomial ring $R$. In this article, we prove that for an\narbitrary graph $G$, $reg(R\/I_3(G))$ is bounded below by $2\\nu_3(G)$, where\n$\\nu_3(G)$ denotes the $3$-path induced matching number of $G$. We give a class\nof graphs, namely, trees for which the lower bound is attained. Also, for a\nunicyclic graph $G$, we show that $reg(R\/I_3(G))\\leq 2\\nu_3(G)+2$ and provide\nan example that shows that the given upper bound is sharp.",
        "We report the results of $^{139}$La NMR measurements in the\nnon-centrosymmetric superconductor LaRhGe$_3$. This material crystallizes in a\ntetragonal structure without inversion symmetry and exhibits type-I\nsuperconductivity below 385 mK. We observed remarkably sharp NMR signals,\nindicating that the magnetic and electronic properties of the sample are\nextremely uniform in LaRhGe$_3$ despite the complex crystal structure. Our NMR\nresults indicate that LaRhGe$_3$ is a weakly correlated semimetal in the normal\nstate.",
        "We investigate the inference of varifold structures in a statistical\nframework: assuming that we have access to i.i.d. samples in $\\mathbb{R}^n$\nobtained from an underlying $d$--dimensional shape $S$ endowed with a possibly\nnon uniform density $\\theta$, we propose and analyse an estimator of the\nvarifold structure associated to $S$. The shape $S$ is assumed to be piecewise\n$C^{1,a}$ in a sense that allows for a singular set whose small enlargements\nare of small $d$--dimensional measure. The estimators are kernel--based both\nfor infering the density and the tangent spaces and the convergence result\nholds for the bounded Lipschitz distance between varifolds, in expectation and\nin a noiseless model. The mean convergence rate involves the dimension $d$ of\n$S$, its regularity through $a \\in (0, 1]$ and the regularity of the density\n$\\theta$.",
        "This paper investigates the approximation of functions with finite smoothness\ndefined on domains with a Cartesian product structure. The recently proposed\ntensor product multilevel method (TPML) combines Smolyak's sparse grid method\nwith a kernel-based residual correction technique. The contributions of this\npaper are twofold. First, we present two improvements on the TPML that reduce\nthe computational cost of point evaluations compared to a naive implementation.\nSecond, we provide numerical examples that demonstrate the effectiveness and\ninnovation of the TPML.",
        "Understanding the advantages of deep neural networks trained by gradient\ndescent (GD) compared to shallow models remains an open theoretical challenge.\nWhile the study of multi-index models with Gaussian data in high dimensions has\nprovided analytical insights into the benefits of GD-trained neural networks\nover kernels, the role of depth in improving sample complexity and\ngeneralization in GD-trained networks remains poorly understood. In this paper,\nwe introduce a class of target functions (single and multi-index Gaussian\nhierarchical targets) that incorporate a hierarchy of latent subspace\ndimensionalities. This framework enables us to analytically study the learning\ndynamics and generalization performance of deep networks compared to shallow\nones in the high-dimensional limit. Specifically, our main theorem shows that\nfeature learning with GD reduces the effective dimensionality, transforming a\nhigh-dimensional problem into a sequence of lower-dimensional ones. This\nenables learning the target function with drastically less samples than with\nshallow networks. While the results are proven in a controlled training\nsetting, we also discuss more common training procedures and argue that they\nlearn through the same mechanisms. These findings open the way to further\nquantitative studies of the crucial role of depth in learning hierarchical\nstructures with deep networks.",
        "Correlations between dislocations in crystals reduce the elastic energy via\nscreening of the strain by the surrounding dislocations. We study the\ncorrelations of threading dislocations in GaN epitaxial films with dislocation\ndensities of 5x10^8 cm^-2 and 1.8x10^10 cm^-2 by X-ray diffraction (XRD) in\nreciprocal space and by high-resolution electron backscatter diffraction (EBSD)\nin real space, where the strain is derived from a cross-correlation analysis of\nthe Kikuchi patterns. The measured XRD curves and EBSD strain and rotation maps\nare compared with Monte Carlo simulations within one and the same model for the\ndislocation distributions. The screening of the dislocation strains is provided\nby creating pairs of dislocations with opposite Burgers vectors, with the mean\ndistance between dislocations in a pair equal to the screening distance. The\npairs overlap and cannot be distinguished as separate dipoles. The\nEBSD-measured autocorrelation functions of the strain and rotation components\nfollow the expected logarithmic law for distances smaller than the screening\ndistances and become zero for larger distances, which is confirmed by the Monte\nCarlo simulations. Screening distances of 2 \\textmu m and 0.3 \\textmu m are\nobtained for the samples with low and high dislocation densities, respectively.\nThe dislocation strain is thus screened by only 4 neighboring dislocations.\nHigh-resolution EBSD allows for a more precise determination of the screening\ndistances than from fits of the XRD curves. In addition, an anisotropic\nresolution of the EBSD measurements is observed and quantified.",
        "Given a closed connected symplectic manifold $(M,\\omega)$, we construct an\nalternating $\\mathbb{R}$-bilinear form\n$\\mathfrak{b}=\\mathfrak{b}_{\\mu_{\\mathrm{Sh}}}$ on the real first cohomology of\n$M$ from Shelukhin's quasimorphism $\\mu_{\\mathrm{Sh}}$. Here\n$\\mu_{\\mathrm{Sh}}$ is defined on the universal cover of the group of\nHamiltonian diffeomorphisms on $(M,\\omega)$. This bilinear form is invariant\nunder the symplectic mapping class group action, and $\\mathfrak{b}$ yields a\nconstraint on the fluxes of commuting two elements in the group of\nsymplectomorphisms on $(M,\\omega)$. These results might be seen as an analog of\nRousseau's result for an open connected symplectic manifold, where he recovered\nthe symplectic pairing from the Calabi homomorphism. Furthermore,\n$\\mathfrak{b}$ controls the extendability of Shelukhin's quasimorphisms, as\nwell as the triviality of a characteristic class of Reznikov. To construct\n$\\mathfrak{b}$, we build general machinery for a group $G$ of producing a\nreal-valued $\\mathbb{Z}$-bilinear form $\\mathfrak{b}_{\\mu}$ from a\n$G$-invariant quasimorphism $\\mu$ on the commutator subgroup of $G$.",
        "Human mobility, a pivotal aspect of urban dynamics, displays a profound and\nmultifaceted relationship with urban sustainability. Despite considerable\nefforts analyzing mobility patterns over decades, the ranking dynamics of urban\nmobility has received limited attention. This study aims to contribute to the\nfield by investigating changes in rank and size of hourly inflows to various\nlocations across 60 Chinese cities throughout the day. We find that the\nrank-size distribution of hourly inflows over the course of the day is stable\nacross cities. To uncover the microdynamics beneath the stable aggregate\ndistribution amidst shifting location inflows, we analyzed consecutive-hour\ninflow size and ranking variations. Our findings reveal a dichotomy: locations\nwith higher daily average inflow display a clear monotonic trend, with more\npronounced increases or decreases in consecutive-hour inflow. In contrast,\nranking variations exhibit a non-monotonic pattern, distinguished by the\nstability of not only the top and bottom rankings but also those in\nmoderately-inflowed locations. Finally, we compare ranking dynamics across\ncities using a ranking metric, the rank turnover. The results advance our\nunderstanding of urban mobility dynamics, providing a basis for applications in\nurban planning and traffic engineering.",
        "It is well known that a Bose-Einstein (BE) condensate of atoms exists in a\nsystem of interacting Bose atoms at $T\\lesssim T^{(i)}_{c}$, where\n$T^{(i)}_{c}$ is the BE condensation temperature of an ideal gas. It is also\ngenerally accepted that BE condensation is impossible at ``ultrahigh''\ntemperatures $T\\gg T^{(i)}_{c}$. While the latter property has been\ntheoretically proven for an ideal gas, no such proof exists for an interacting\nsystem, to our knowledge. In this paper, we propose an approximate mathematical\nproof for a finite, nonrelativistic, periodic system of $N$ spinless\ninteracting bosons. The key point is that, at $T\\gg T^{(i)}_{c}$, the main\ncontribution to the occupation number\n$N_{0}=\\frac{1}{Z}\\sum_{\\wp}e^{-E_{\\wp}\/k_{B}T}\\langle\n\\Psi_{\\wp}|\\hat{a}^{+}_{\\mathbf{0}}\\hat{a}_{\\mathbf{0}}|\\Psi_{\\wp}\\rangle$,\ncorresponding to atoms with zero momentum, originates from the states\ncontaining $N$ elementary quasiparticles. These states do not contain the BE\ncondensate of zero-momentum atoms, implying that an ultrahigh temperature\nshould ``blur'' such a condensate."
      ]
    }
  },
  {
    "id":2411.05456,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"MRI segmentation of the human brain: challenges, methods, and applications",
    "start_abstract":"Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "U-net: Convolutional networks for biomedical image segmentation"
      ],
      "abstract":[
        "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Robust Evidence for Declining Disruptiveness: Assessing the Role of\n  Zero-Backward-Citation Works",
        "Coresets for Robust Clustering via Black-box Reductions to Vanilla Case",
        "Thermal Radiation Force and Torque on Moving Nanostructures with\n  Anisotropic Optical Response",
        "Efficient Transformed Gaussian Process State-Space Models for\n  Non-Stationary High-Dimensional Dynamical Systems",
        "Wheel-GINS: A GNSS\/INS Integrated Navigation System with a Wheel-mounted\n  IMU",
        "Electron-Chiral Phonon Coupling, Crystal Angular Momentum, and Phonon\n  Chirality",
        "Exploring the Hidden Reasoning Process of Large Language Models by\n  Misleading Them",
        "Characterizing the Burst Error Correction Ability of Quantum Cyclic\n  Codes",
        "Global Existence and Nonlinear Stability of Finite-Energy Solutions of\n  the Compressible Euler-Riesz Equations with Large Initial Data of Spherical\n  Symmetry",
        "Probing the hollowing transition of a shell-shaped BEC with collective\n  excitation",
        "Gender Dynamics in Software Engineering: Insights from Research on\n  Concurrency Bug Reproduction",
        "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time\n  Cognitive Task Solving and Reasoning in UAVs",
        "On connected subgraph arrangements",
        "Scale-wise Distillation of Diffusion Models",
        "Revealed Social Networks",
        "Derivation of the Gromeka Acceleration Vector for Dimensionless\n  Womersley Flow",
        "Anisotropic temperature-dependent lattice parameters and elastic\n  constants from first principles",
        "Tensor meson transition form factors in holographic QCD and the muon\n  $g-2$",
        "EXPRESS: An LLM-Generated Explainable Property Valuation System with\n  Neighbor Imputation",
        "Contrastive Similarity Learning for Market Forecasting: The ContraSim\n  Framework",
        "Pseudo-Hermitian physics from dynamically coupled macrospins",
        "Turan type inequalities for rational functions with pescribed poles and\n  restricted zeros",
        "Entanglement entropy evolution during gravitational collapse",
        "\"Who Has the Time?\": Understanding Receptivity to Health Chatbots among\n  Underserved Women in India",
        "Attention Distillation: A Unified Approach to Visual Characteristics\n  Transfer",
        "The Ultraviolet Problem in Supergravity",
        "Primordial Black Hole Hot Spots and Nucleosynthesis",
        "Separating the bulk and interface contribution of spin-orbit torque in\n  ferromagnet-Heavy metal bilayers tuned by variation of resistivity of heavy\n  metal",
        "Two-photon spectroscopy and a verification of the Kennard-Stepanov\n  relation in high-pressure two-species xenon-noble gas mixtures"
      ],
      "abstract":[
        "We respond to Holst et al.'s (HATWG) critique that the observed decline in\nscientific disruptiveness demonstrated in Park et al. (PLF) stems from\nincluding works with zero backward citations (0-bcites). Applying their own\nadvocated dataset, metric, and exclusion criteria, we demonstrate statistically\nand practically significant declines in disruptiveness that equal major\nbenchmark transformations in science. Notably, we show that HATWG's own\nregression model -- designed specifically to address their concerns about\n0-bcite works -- reveals highly significant declines for both papers (p<0.001)\nand patents (p<0.001), a finding they neither acknowledge nor interpret. Their\ncritique is undermined by methodological deficiencies, including reliance on\nvisual inspection without statistical assessment, and severe data quality\nissues in their SciSciNet dataset, which contains nearly three times more\n0-bcite papers than our original data. HATWG's departure from established\nscientometric practices -- notably their inclusion of document types and fields\nknown for poor metadata quality -- invalidates their conclusions. Monte Carlo\nsimulations and additional analyses using multiple disruptiveness measures\nacross datasets further validate the robustness of the declining trend. Our\nfindings collectively demonstrate that the observed decline in disruptiveness\nis not an artifact of 0-bcite works but represents a substantive change in\nscientific and technological innovation patterns.",
        "We devise $\\epsilon$-coresets for robust $(k,z)$-Clustering with $m$ outliers\nthrough black-box reductions to vanilla case. Given an $\\epsilon$-coreset\nconstruction for vanilla clustering with size $N$, we construct coresets of\nsize $N\\cdot \\mathrm{poly}\\log(km\\epsilon^{-1}) +\nO_z\\left(\\min\\{km\\epsilon^{-1}, m\\epsilon^{-2z}\\log^z(km\\epsilon^{-1})\n\\}\\right)$ for various metric spaces, where $O_z$ hides $2^{O(z\\log z)}$\nfactors. This increases the size of the vanilla coreset by a small\nmultiplicative factor of $\\mathrm{poly}\\log(km\\epsilon^{-1})$, and the additive\nterm is up to a $(\\epsilon^{-1}\\log (km))^{O(z)}$ factor to the size of the\noptimal robust coreset. Plugging in vanilla coreset results of [Cohen-Addad et\nal., STOC'21], we obtain the first coresets for $(k,z)$-Clustering with $m$\noutliers with size near-linear in $k$ while previous results have size at least\n$\\Omega(k^2)$ [Huang et al., ICLR'23; Huang et al., SODA'25].\n  Technically, we establish two conditions under which a vanilla coreset is as\nwell a robust coreset. The first condition requires the dataset to satisfy\nspecial structures - it can be broken into \"dense\" parts with bounded diameter.\nWe combine this with a new bounded-diameter decomposition that has only $O_z(km\n\\epsilon^{-1})$ non-dense points to obtain the $O_z(km \\epsilon^{-1})$ additive\nbound. Another condition requires the vanilla coreset to possess an extra\nsize-preserving property. We further give a black-box reduction that turns a\nvanilla coreset to the one satisfying the said size-preserving property,\nleading to the alternative $O_z(m\\epsilon^{-2z}\\log^{z}(km\\epsilon^{-1}))$\nadditive bound.\n  We also implement our reductions in the dynamic streaming setting and obtain\nthe first streaming algorithms for $k$-Median and $k$-Means with $m$ outliers,\nusing space $\\tilde{O}(k+m)\\cdot\\mathrm{poly}(d\\epsilon^{-1}\\log\\Delta)$ for\ninputs on the grid $[\\Delta]^d$.",
        "Nanoscale objects moving relative to a thermal radiation bath experience a\ndrag force due to the imbalance in their interaction with the blue- and\nredshifted components of the electromagnetic field. Here, we show that, in\naddition to this drag force, moving nanostructures with an anisotropic optical\nresponse experience a lateral force and a torque that substantially modify\ntheir trajectory. These phenomena emerge from the additional coupling between\nthe electromagnetic field components polarized parallel and perpendicular to\nthe trajectory, enabled by the anisotropic response of the nanostructure. This\nwork unveils the intricate dynamics of anisotropic nanostructures moving in a\nthermal radiation bath.",
        "Gaussian process state-space models (GPSSMs) have emerged as a powerful\nframework for modeling dynamical systems, offering interpretable uncertainty\nquantification and inherent regularization. However, existing GPSSMs face\nsignificant challenges in handling high-dimensional, non-stationary systems due\nto computational inefficiencies, limited scalability, and restrictive\nstationarity assumptions. In this paper, we propose an efficient transformed\nGaussian process state-space model (ETGPSSM) to address these limitations. Our\napproach leverages a single shared Gaussian process (GP) combined with\nnormalizing flows and Bayesian neural networks, enabling efficient modeling of\ncomplex, high-dimensional state transitions while preserving scalability. To\naddress the lack of closed-form expressions for the implicit process in the\ntransformed GP, we follow its generative process and introduce an efficient\nvariational inference algorithm, aided by the ensemble Kalman filter (EnKF), to\nenable computationally tractable learning and inference. Extensive empirical\nevaluations on synthetic and real-world datasets demonstrate the superior\nperformance of our ETGPSSM in system dynamics learning, high-dimensional state\nestimation, and time-series forecasting, outperforming existing GPSSMs and\nneural network-based methods in both accuracy and computational efficiency.",
        "A long-term accurate and robust localization system is essential for mobile\nrobots to operate efficiently outdoors. Recent studies have shown the\nsignificant advantages of the wheel-mounted inertial measurement unit\n(Wheel-IMU)-based dead reckoning system. However, it still drifts over extended\nperiods because of the absence of external correction signals. To achieve the\ngoal of long-term accurate localization, we propose Wheel-GINS, a Global\nNavigation Satellite System (GNSS)\/inertial navigation system (INS) integrated\nnavigation system using a Wheel-IMU. Wheel-GINS fuses the GNSS position\nmeasurement with the Wheel-IMU via an extended Kalman filter to limit the\nlong-term error drift and provide continuous state estimation when the GNSS\nsignal is blocked. Considering the specificities of the GNSS\/Wheel-IMU\nintegration, we conduct detailed modeling and online estimation of the\nWheel-IMU installation parameters, including the Wheel-IMU leverarm and\nmounting angle and the wheel radius error. Experimental results have shown that\nWheel-GINS outperforms the traditional GNSS\/Odometer\/INS integrated navigation\nsystem during GNSS outages. At the same time, Wheel-GINS can effectively\nestimate the Wheel-IMU installation parameters online and, consequently,\nimprove the localization accuracy and practicality of the system. The source\ncode of our implementation is publicly available\n(https:\/\/github.com\/i2Nav-WHU\/Wheel-GINS).",
        "We explicitly derive the wavefunctions of chiral phonons propagating along\nthe helical axis in chiral crystals and clarify the characteristics of\nelectron-phonon interactions in chiral helical crystals. In particular, we\nelucidate how the conservation of not only the crystal momentum (CM) but also\nthe crystal angular momentum (CAM) manifests in the interaction vertex. This\nformulation provides a microscopic framework for describing physical processes\ninvolving chiral phonons. Furthermore, we construct a phononic analogue of\nZilch, a known measure of chirality carried by light, and discuss its\nrelationship with phonon angular momentum.",
        "Large language models (LLMs) and Vision language models (VLMs) have been able\nto perform various forms of reasoning tasks in a wide range of scenarios, but\nare they truly engaging in task abstraction and rule-based reasoning beyond\nmere memorization and pattern matching? To answer this question, we propose a\nnovel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether\nLLMs\/VLMs perform abstract reasoning by altering their original understanding\nof fundamental rules. In particular, by constructing a dataset with math\nexpressions that contradict correct operation principles, we fine-tune the\nmodel to learn those contradictory rules and assess its generalization ability\non different test domains. Through a series of experiments, we find that\ncurrent LLMs\/VLMs are capable of effectively applying contradictory rules to\nsolve practical math word problems and math expressions represented by images,\nimplying the presence of an internal mechanism that abstracts before reasoning.",
        "Quantum burst error correction codes (QBECCs) are of great importance to deal\nwith the memory effect in quantum channels. As the most important family of\nQBECCs, quantum cyclic codes (QCCs) play a vital role in the correction of\nburst errors. In this work, we characterize the burst error correction ability\nof QCCs constructed from the Calderbank-Shor-Steane (CSS) and the Hermitian\nconstructions. We determine the burst error correction limit of QCCs and\nquantum Reed-Solomon codes with algorithms in polynomial-time complexities. As\na result, lots of QBECCs saturating the quantum Reiger bound are obtained. We\nshow that quantum Reed-Solomon codes have better burst error correction\nabilities than the previous results. At last, we give the quantum\nerror-trapping decoder (QETD) of QCCs for decoding burst errors. The decoder\nruns in linear time and can decode both degenerate and nondegenerate burst\nerrors. What's more, the numerical results show that QETD can decode much more\ndegenerate burst errors than the nondegenerate ones.",
        "The compressible Euler-Riesz equations are fundamental with wide applications\nin astrophysics, plasma physics, and mathematical biology. In this paper, we\nare concerned with the global existence and nonlinear stability of\nfinite-energy solutions of the multidimensional Euler-Riesz equations with\nlarge initial data of spherical symmetry. We consider both attractive and\nrepulsive interactions for a wide range of Riesz and logarithmic potentials for\ndimensions larger than or equal to two. This is achieved by the inviscid limit\nof the solutions of the corresponding Cauchy problem for the\nNavier-Stokes-Riesz equations. The strong convergence of the vanishing\nviscosity solutions is achieved through delicate uniform estimates in $L^p$. It\nis observed that, even if the attractive potential is super-Coulomb, no\nconcentration is formed near the origin in the inviscid limit. Moreover, we\nprove that the nonlinear stability of global finite-energy solutions for the\nEuler-Riesz equations is unconditional under a spherically symmetric\nperturbation around the steady solutions. Unlike the Coulomb case where the\npotential can be represented locally, the singularity and regularity of the\nnonlocal radial Riesz potential near the origin require careful analysis, which\nis a crucial step. Finally, unlike the Coulomb case, a Gr\\\"onwall type estimate\nis required to overcome the difficulty of the appearance of boundary terms in\nthe sub-Coulomb case and the singularity of the super-Coulomb potential.\nFurthermore, we prove the nonlinear stability of global finite-energy solutions\nfor the compressible Euler-Riesz equations around steady states by employing\nconcentration compactness arguments. Steady states properties are obtained by\nvariational arguments connecting to recent advances in aggregation-diffusion\nequations.",
        "We investigate the hollowing transition of a shell-shaped Bose-Einstein\ncondensate using collective excitations. The shell is created using an\nimmiscible dual-species BEC mixture, with its hollowness controlled by tuning\nthe repulsive interspecies interaction via a Feshbach resonance. Our results\nreveal two distinct monopole modes in which the two condensates oscillate\neither in-phase or out-of-phase. The spectrum of the out-of-phase mode exhibits\na non-monotonic dependence on the interspecies interaction, providing a clear\nsignature of the topology change from a filled to a hollow condensate.\nFurthermore, we find that the critical point of the hollowing transition\ndepends strongly on the number ratio of the two species. Our findings provide a\ndetailed understanding of the topology change in shell-shaped quantum gases and\npave the way for future study of quantum many-body phenomena in curved spaces.",
        "Reproducing concurrency bugs is a complex task due to their unpredictable\nbehavior. Researchers, regardless of gender, are contributing to automating\nthis complex task to aid software developers. While some studies have\ninvestigated gender roles in the broader software industry, limited research\nexists on gender representation specifically among researchers working in\nconcurrent bug reproduction. To address this gap, in this paper, we present a\nliterature review to assess the gender ratio in this field. We also explore\npotential variations in technique selection and bug-type focus across genders.\nOur findings indicate that female researchers are underrepresented compared to\ntheir male counterparts in this area, with a current male-to-female author\nratio of 29:6. Through this study, we emphasize the importance of fostering\ngender equity in software engineering research, ensuring a diversity of\nperspectives in the development of automated bug reproduction tools.",
        "This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA)\nmodel tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand\nadvanced cognitive abilities. Trained on a dataset comprising over 8,000\nsimulated flight trajectories across three key categories-Human Recognition,\nSymbol Understanding, and Reasoning-the model generates real-time 4D action\ncommands based on first-person visual inputs and textual instructions. To\nfurther enhance performance in intricate scenarios, we propose\nCognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM)\nreasoning module to simplify task directives prior to high-frequency control.\nExperimental evaluations using our open-source benchmark, CognitiveDroneBench,\nreveal that while a racing-oriented model (RaceVLA) achieves an overall success\nrate of 31.3%, the base CognitiveDrone model reaches 59.6%, and\nCognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate\nimprovements of up to 30% in critical cognitive tasks, underscoring the\neffectiveness of incorporating advanced reasoning capabilities into UAV control\nsystems. Our contributions include the development of a state-of-the-art VLA\nmodel for UAV control and the introduction of the first dedicated benchmark for\nassessing cognitive tasks in drone operations. The complete repository is\navailable at cognitivedrone.github.io",
        "Recently, Cuntz and K\\\"uhne introduced a particular class of hyperplane\narrangements stemming from a given graph $G$, so called connected subgraph\narrangements $A_G$. In this note we strengthen some of the result from their\nwork and prove new ones for members of this class. For instance, we show that\naspherical members withing this class stem from a rather restricted set of\ngraphs. Specifically, if $A_G$ is an aspherical connected subgraph arrangement,\nthen $A_G$ is free with the unique possible exception when the underlying graph\n$G$ is the complete graph on $4$ nodes.",
        "We present SwD, a scale-wise distillation framework for diffusion models\n(DMs), which effectively employs next-scale prediction ideas for\ndiffusion-based few-step generators. In more detail, SwD is inspired by the\nrecent insights relating diffusion processes to the implicit spectral\nautoregression. We suppose that DMs can initiate generation at lower data\nresolutions and gradually upscale the samples at each denoising step without\nloss in performance while significantly reducing computational costs. SwD\nnaturally integrates this idea into existing diffusion distillation methods\nbased on distribution matching. Also, we enrich the family of distribution\nmatching approaches by introducing a novel patch loss enforcing finer-grained\nsimilarity to the target distribution. When applied to state-of-the-art\ntext-to-image diffusion models, SwD approaches the inference times of two full\nresolution steps and significantly outperforms the counterparts under the same\ncomputation budget, as evidenced by automated metrics and human preference\nstudies.",
        "People are influenced by their peers when making decisions. In this paper, we\nstudy the linear-in-means model which is the standard empirical model of peer\neffects. As data on the underlying social network is often difficult to come\nby, we focus on data that only captures an agent's choices. Under exogenous\nagent participation variation, we study two questions. We first develop a\nrevealed preference style test for the linear-in-means model. We then study the\nidentification properties of the linear-in-means model. With sufficient\nparticipation variation, we show how an analyst is able to recover the\nunderlying network structure and social influence parameters from choice data.\nOur identification result holds when we allow the social network to vary across\ncontexts. To recover predictive power, we consider a refinement which allows us\nto extrapolate the underlying network structure across groups and provide a\ntest of this version of the model.",
        "This manuscript presents an analytical and theoretical investigation of the\nGromeka acceleration field in a dimensionless Womersley flow, derived through\nthe exact solution of the governing Navier-Stokes equations in phase space. By\ndecomposing the convective acceleration into rotational and nonrotational\ncomponents, the derivation highlights the dominant role of vorticity dynamics\nnear the wall, where steep velocity gradients interact with the oscillatory\naxial velocity to produce localized radial accelerations. The solution reveals\nthat the Gromeka acceleration mediates nonlinear interactions between\nharmonics, driving energy redistribution and boundary layer development under\nmulti-harmonic boundary conditions. Complementary analysis of the kinetic\nenergy gradient further delineates inertial effects, demonstrating their role\nin phase-dependent flow separation and reattachment. These findings provide a\ncomprehensive framework for understanding momentum transport and instability\ngeneration in pulsatile wall-bounded flows.",
        "The Quasi-harmonic Approximation (QHA) is a widely used method for\ncalculating the temperature dependence of lattice parameters and the thermal\nexpansion coefficients from first principles. However, applying QHA to\nanisotropic systems typically requires several dozens or even hundreds of\nphonon band structure calculations, leading to high computational costs. The\nZero Static Internal Stress Approximation (ZSISA) QHA method partly addresses\nsuch caveat, but the computational load of its implementation remains high, so\nthat its volumetric-only counterpart v-ZSISA-QHA is preferred. In this work, we\npresent an efficient implementation of the ZSISA-QHA, enabling its application\nacross a wide range of crystal structures under varying temperature (T) and\npressure (P) conditions. By incorporating second-order derivatives of the\nvibrational free energy with respect to lattice degrees of freedom, we\nsignificantly reduce the number of required phonon band structure calculations\nfor the determination of all lattice parameters and angles. For hexagonal,\ntrigonal, and tetragonal systems, only six phonon band structure calculations\nare needed, while 10, 15, and 28 calculations suffice for orthorhombic,\nmonoclinic, and triclinic systems, respectively. This method is tested for a\nvariety of non-cubic materials, from uniaxial ones like ZnO and CaCO3 to\nmonoclinic or triclinic materials such as ZrO2, HfO2, and Al2SiO5,\ndemonstrating a significant reduction in computational effort while maintaining\naccuracy in modeling anisotropic thermal expansion, unlike the v-ZSISA-QHA. The\nmethod is also applied to the first-principles calculation of\ntemperature-dependent elastic constants, with only up to six more phonon band\nstructure calculations, depending on the crystallographic system.",
        "Despite the prominence of tensor mesons in photon-photon collisions, until\nrecently their contribution to the hadronic light-by-light (HLBL) scattering\npart of the anomalous magnetic moment of the muon has been estimated at the\nlevel of only a few $10^{-12}$, with an almost negligible contribution to the\nerror budget of the Standard Model prediction. A recent reanalysis within the\ndispersive approach has found that after resolving the issue of kinematic\nsingularities in previous approaches, a larger result is obtained, a few\n$10^{-11}$, and with opposite sign as in previous results, when a simple quark\nmodel for the transition form factors is employed. In this paper, we present\nthe first complete evaluation of tensor meson contributions within a hard-wall\nmodel in holographic QCD, which reproduces surprisingly well mass, two-photon\nwidth, and the observed singly virtual transition form factors of the dominant\n$f_2(1270)$, requiring only that the energy-momentum tensor correlator is\nmatched to the leading OPE result of QCD. Due to a second structure function\nthat is absent in the quark model, the result for $a_\\mu$ turns out to be\npositive instead of negative, and also with a magnitude of a few $10^{-11}$. We\ndiscuss both pole and non-pole contributions arising from tensor meson\nexchanges in the holographic HLBL amplitude, finding that keeping all\ncontributions improves dramatically the convergence of a sum over excited\ntensor mesons and avoids unnaturally large contributions from the first few\nexcited modes at low energies. Moreover, we find that the infinite tower of\ntensor mesons permits to fill the gap in the symmetric longitudinal\nshort-distance constraint on the HLBL amplitude left by the contribution of\naxial vector mesons. Total $a_\\mu^\\mathrm{Tensor}$ contribution: $+12.4\\times\n10^{-11}$; with an $F_\\rho$ fit this is reduced slightly to $+11.1\\times\n10^{-11}$.",
        "The demand for property valuation has attracted significant attention from\nsellers, buyers, and customers applying for loans. Reviews of existing\napproaches have revealed shortcomings in terms of not being able to handle\nmissing value situations, as well as lacking interpretability, which means they\ncannot be used in real-world applications. To address these challenges, we\npropose an LLM-Generated EXplainable PRopErty valuation SyStem with neighbor\nimputation called EXPRESS, which provides the customizable missing value\nimputation technique, and addresses the opaqueness of prediction by providing\nthe feature-wise explanation generated by LLM. The dynamic nearest neighbor\nsearch finds similar properties depending on different application scenarios by\nproperty configuration set by users (e.g., house age as criteria for the house\nin rural areas, and locations for buildings in urban areas). Motivated by the\nhuman appraisal procedure, we generate feature-wise explanations to provide\nusers with a more intuitive understanding of the prediction results.",
        "We introduce the Contrastive Similarity Space Embedding Algorithm\n(ContraSim), a novel framework for uncovering the global semantic relationships\nbetween daily financial headlines and market movements. ContraSim operates in\ntwo key stages: (I) Weighted Headline Augmentation, which generates augmented\nfinancial headlines along with a semantic fine-grained similarity score, and\n(II) Weighted Self-Supervised Contrastive Learning (WSSCL), an extended version\nof classical self-supervised contrastive learning that uses the similarity\nmetric to create a refined weighted embedding space. This embedding space\nclusters semantically similar headlines together, facilitating deeper market\ninsights. Empirical results demonstrate that integrating ContraSim features\ninto financial forecasting tasks improves classification accuracy from WSJ\nheadlines by 7%. Moreover, leveraging an information density analysis, we find\nthat the similarity spaces constructed by ContraSim intrinsically cluster days\nwith homogeneous market movement directions, indicating that ContraSim captures\nmarket dynamics independent of ground truth labels. Additionally, ContraSim\nenables the identification of historical news days that closely resemble the\nheadlines of the current day, providing analysts with actionable insights to\npredict market trends by referencing analogous past events.",
        "We consider two classical macrospins with dynamical (frequency-dependent)\ncoupling, modeled by a generalized Landau-Lifshitz-Gilbert equation. We show\nthat, in the absence of local damping, the resulting dynamics are\npseudo-Hermitian. When two precessional modes hybridize near a crossing, the\nspectral behavior takes the form either of an anticrossing or level attraction,\nwith the latter formalized in terms of spontaneous $\\mathcal{PT}$-symmetry\nbreaking. Near equilibrium, mixing due to nondissipative interactions results\nin repulsion, while dissipative mixing results in attraction. In contrast, when\nthe fluctuating degrees of freedom form a free-energy saddle point, we find\nthat nondissipative interactions result in level attraction, while dissipative\ninteractions produce level repulsion. Accounting for the effects of local\nGilbert damping, we examine the cases in which approximate\n$\\mathcal{PT}$-symmetry breaking is still possible and determine the degree to\nwhich the qualitative spectral properties still persist.",
        "In this paper, we establish some inequalities for rational functions\n  with prescribed poles having s-fold zeros at origin and also show that\n  it implies some inequalities for polynomials and their polar derivatives.",
        "We investigate the dynamics of the ground state entanglement entropy for a\ndiscretized scalar field propagating within the Oppenheimer-Snyder collapse\nmetric. Starting from a well-controlled initial configuration, we follow the\nsystem as it evolves toward the formation of a horizon and, eventually, a\nsingularity. Our approach employs an Ermakov-like equation to determine the\ntime-dependent ground state of the field and calculates the resulting\nentanglement entropy by tracing out the degrees of freedom inside a spherical\nregion within the matter sphere. We find that the entanglement entropy exhibits\nnontrivial scaling and time dependence during collapse. Close to the horizon,\nthe entropy can deviate from the simple area law, reflecting the rapid changes\nin geometry and field configuration. Although the model is idealized, these\nresults provide insights into the generation and scaling of entanglement in the\npresence of realistic, dynamically evolving gravitational fields.",
        "Access to health information and services among women continues to be a major\nchallenge in many communities globally. In recent years, there has been a\ngrowing interest in the potential of chatbots to address this information and\naccess gap. We conducted interviews and focus group discussions with\nunderserved women in urban India to understand their receptivity towards the\nuse of chatbots for maternal and child health, as well as barriers to their\nadoption. Our findings uncover gaps in digital access and literacies, and\nperceived conflict with various responsibilities that women are burdened with,\nwhich shape their interactions with digital technology. Our paper offers\ninsights into the design of chatbots for community health that can meet the\nlived realities of women in underserved settings.",
        "Recent advances in generative diffusion models have shown a notable inherent\nunderstanding of image style and semantics. In this paper, we leverage the\nself-attention features from pretrained diffusion networks to transfer the\nvisual characteristics from a reference to generated images. Unlike previous\nwork that uses these features as plug-and-play attributes, we propose a novel\nattention distillation loss calculated between the ideal and current\nstylization results, based on which we optimize the synthesized image via\nbackpropagation in latent space. Next, we propose an improved Classifier\nGuidance that integrates attention distillation loss into the denoising\nsampling process, further accelerating the synthesis and enabling a broad range\nof image generation applications. Extensive experiments have demonstrated the\nextraordinary performance of our approach in transferring the examples' style,\nappearance, and texture to new images in synthesis. Code is available at\nhttps:\/\/github.com\/xugao97\/AttentionDistillation.",
        "We review the development of understanding for the problem of ultraviolet\ndivergences in supergravity. This history proceeds from initial constructions\nof counterterms invariant under the relevant degrees of local supersymmetry,\nthrough a deeper understanding of non-renormalisation theorems for the relevant\ndegrees of ``off-shell'' linearly realisable supersymmetry, and on to current\nunderstanding of limitations on counterterm eligibility based on the duality\nsymmetries of supergravity theories, as well as of the related structures\nemerging in superstring theory.",
        "Upon their evaporation via Hawking radiation, primordial black holes (PBHs)\nmay deposit energy in the ambient plasma on scales smaller than the typical\ndistance between two black holes, leading to the formation of hot spots around\nthem. We investigate how the corresponding rise of the local temperature during\nthe evaporation may act as a shield against the release of low-energy photons,\naffecting PBH's capacity to dissociate light nuclei after Big-Bang\nNucleosynthesis through photo-dissociation. We study the different ways PBH hot\nspots affect the flux of low-energy photons expected from PBH evaporation, and\nwe find that such effects can be particularly relevant to the physics of\nphoto-dissociation during Big-Bang Nucleosynthesis for PBHs with masses between\n$10^{11}$g and $3\\times 10^{12}$g. We emphasize that the magnitude of this\neffect is highly dependent on the specific shape of the temperature profile\naround PBHs and its time evolution. This underscores the necessity for a\ncomprehensive study of PBH hot spots and their dynamics in the future.",
        "Harmonic Hall measurements were conducted on a series of Ferromagnetic\nmetal\/Heavy metal (FM\/HM) bilayers with beta-Tungsten (W) as the HM and\nin-plane magnetized permalloy (Py) as the FM and the efficiencies of the two\northogonal components of the spin orbit-torque were extracted. Two sets of Hall\nbar-shaped devices were considered where the HM resistivity systematically\nvaried over a wide range (sim150-1000 muOmega-cm) while the FM layer remained\nthe same and each set having a different aspect ratio of voltage pickup line\nwidth and Hall bar width. Using numerical simulations of current distribution\nat the region between voltage pickup lines we have normalised the SOT\nefficiencies and examined their dependence. The current-induced spin-orbit\ntorque efficiency in ferromagnetic metal (FM)\/heavy metal (HM) bilayers is\nquantitatively investigated in this study.beta-W, known for its high spin-orbit\ncoupling, served as the HM layer, while Py, an FM with an in-plane magnetic\nanisotropy, comprised the other layer. We performed a thorough analysis of the\nsecond harmonic Hall resistance (R_{xy}^{2\\omega}) obtained from Py\/beta-W\nbilayer devices, systematically varying the resistivity (rho_W) of the beta-W\nlayer within the range of 200 to 1000 \\mu\\Omega-cm by employing a fixed current\ndensity (J_W\\sim0.8\\times10^{11} A\/m^2) through beta-W. Through this analysis,\nwe derived the Slonczewski-like efficiency (xi_{SL}) and field-like efficiency\n(\\xi_{FL}) as a function of rho_W. Notably, the device with a resistivity of\n980 muOmega-cm exhibited the highest xi_{SL}, yielding a value of -0.42 0.09.\nThese results highlight the promising potential of highly resistiv beta-W as a\nmaterial of interest in spintronics research.",
        "Between the absorption and the emission spectral lineshapes of dense atomic\nand molecular media, such as dye solutions and alkali-noble buffer gas mixtures\nat high pressure, in many cases there exists a universal scaling, the\nKennard-Stepanov relation, which is a manifestation of detailed balance. This\nrelation plays a crucial role in recent Bose-Einstein condensation experiments\nof visible-spectral-photons in e.g. dye-solution-filled optical microcavities.\nIt has recently been proposed to use high-pressure xenon-noble gas mixtures as\na thermalization medium for vacuum-ultraviolet regime photons, so as to extend\nthe achievable wavelength range of such Bose-Einstein-condensed optical sources\nfrom the visible to the vacuum-ultraviolet regime. In this work, we report\ntwo-photon excitation spectroscopy measurements of ground state ($5p^6$) xenon\natoms subject to up to 80bar of helium or krypton buffer gas pressure,\nrespectively, in the 220nm - 260nm wavelength range. The study of such\ntwo-photon spectra is of interest e.g. for the exploration of possible pumping\nschemes of a future vacuum-ultraviolet photon Bose-Einstein condensate. We have\nalso recorded absorption and emission spectra of the $5p^6 \\leftrightarrow\n5p^56s$ single-photon transition near 147nm wavelength of xenon atoms subject\nto 80bar of krypton buffer gas pressure. We find that the ratio of absorption\nand emission follows a Kennard-Stepanov scaling, which suggests that such gas\nmixtures are promising candidates as a thermalization medium for a\nBose-Einstein condensate of vacuum-ultraviolet photons."
      ]
    }
  },
  {
    "id":2411.05456,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"U-net: Convolutional networks for biomedical image segmentation",
    "start_abstract":"There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "MRI segmentation of the human brain: challenges, methods, and applications"
      ],
      "abstract":[
        "Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Operator Learning for Reconstructing Flow Fields from Sparse\n  Measurements: an Energy Transformer Approach",
        "Excitability and oscillations of active droplets",
        "Several classes of linear codes with few weights derived from Weil sums",
        "ByteQC: GPU-Accelerated Quantum Chemistry Package for Large-Scale\n  Systems",
        "What exactly has TabPFN learned to do?",
        "Amplification of turbulence through multiple planar shocks",
        "Diophantine approximation and the subspace theorem",
        "Optimal $L^p$-approximation of convex sets by convex subsets",
        "The Hierarchy of Saturating Matching Numbers",
        "Bayesianize Fuzziness in the Statistical Analysis of Fuzzy Data",
        "Escalation dynamics and the severity of wars",
        "Efficient and stable derivative-free Steffensen algorithm for root\n  finding",
        "Analytical Strategies and Winning Conditions for Elliptic-Orbit\n  Target-Attacker-Defender Game",
        "Parameter Estimation of State Space Models Using Particle Importance\n  Sampling",
        "A family of asymptotically bad wild towers of function fields",
        "Weak-Strong Uniqueness and the d'Alembert Paradox",
        "Tensor-structured PCG for finite difference solver of domain patterns in\n  ferroelectric material",
        "On the reproducibility of discrete-event simulation studies in health\n  research: an empirical study using open models",
        "System Architecture Optimization Strategies: Dealing with Expensive\n  Hierarchical Problems",
        "Tremblay-Turbiner-Winternitz (TTW) system at integer index $k$:\n  polynomial algebras of integrals",
        "VeloxQ: A Fast and Efficient QUBO Solver",
        "Zero Estimation Cost Strategy for Witsenhausen Counterexample with\n  Causal Encoder",
        "QE-CONVERSE: An open-source package for the Quantum ESPRESSO\n  distribution to compute non-perturbatively orbital magnetization from first\n  principles, including NMR chemical shifts and EPR parameters",
        "Optimal control over the full counting statistics in a non-adiabatic\n  pump",
        "X-ray cavities in TNG-Cluster: a direct comparison to observations",
        "Mass-manufactured Gradient Plasmonic Metasurfaces for Enhanced Mid-IR\n  Spectrochemical Analysis of Complex Biofluids",
        "Limiting behavior of mixed coherent systems with L\\'evy-frailty\n  Marshall-Olkin failure times",
        "An Equivalence Relation Classification of the Isomorphism Types of\n  Absolutely Indecomposable (resp. Absolutely Simple) Modules in Finite Group\n  Modular Representation Theory with a Green Theory Invariants \"Compatibility\"",
        "Convergence Analysis of EXTRA in Non-convex Distributed Optimization"
      ],
      "abstract":[
        "Machine learning methods have shown great success in various scientific\nareas, including fluid mechanics. However, reconstruction problems, where full\nvelocity fields must be recovered from partial observations, remain\nchallenging. In this paper, we propose a novel operator learning framework for\nsolving reconstruction problems by using the Energy Transformer (ET), an\narchitecture inspired by associative memory models. We formulate reconstruction\nas a mapping from incomplete observed data to full reconstructed fields. The\nmethod is validated on three fluid mechanics examples using diverse types of\ndata: (1) unsteady 2D vortex street in flow past a cylinder using simulation\ndata; (2) high-speed under-expanded impinging supersonic jets impingement using\nSchlieren imaging; and (3) 3D turbulent jet flow using particle tracking. The\nresults demonstrate the ability of ET to accurately reconstruct complex flow\nfields from highly incomplete data (90\\% missing), even for noisy experimental\nmeasurements, with fast training and inference on a single GPU. This work\nprovides a promising new direction for tackling reconstruction problems in\nfluid mechanics and other areas in mechanics, geophysics, weather prediction,\nand beyond.",
        "In living cells, cycles of formation and dissolution of liquid droplets can\nmediate biological functions such as DNA repair. However, the minimal\nphysicochemical prerequisite for such droplet oscillations remains elusive.\nHere, we present a simple model composed of only two independent chemical\ncomponents with their diffusive and chemical fluxes governed by non-equilibrium\nthermodynamics. There is turnover of fuel that maintains a chemical reaction\naway from equilibrium, leading to active droplets. We find that a single active\ndroplet undergoes a pitchfork-bifurcation in the droplet volume upon increasing\nthe fueling strength. Strikingly, the active droplet becomes excitable upon\nadding a further chemical reaction. For sufficient fueling, the system\nundergoes self-sustained oscillations cycling between droplet formation and\ndissolution. The minimal nature of our model suggests self-sustained active\ndroplets as functional modules for de novo life.",
        "Linear codes with few weights have applications in secret sharing,\nauthentication codes, association schemes and strongly regular graphs. In this\npaper, several classes of $t$-weight linear codes over ${\\mathbb F}_{q}$ are\npresented with the defining sets given by the intersection, difference and\nunion of two certain sets, where $t=3,4,5,6$ and $q$ is an odd prime power. By\nusing Weil sums and Gauss sums, the parameters and weight distributions of\nthese codes are determined completely. Moreover, three classes of optimal codes\nmeeting the Griesmer bound are obtained, and computer experiments show that\nmany (almost) optimal codes can be derived from our constructions.",
        "Applying quantum chemistry algorithms to large-scale systems requires\nsubstantial computational resources scaled with the system size and the desired\naccuracy. To address this, ByteQC, a fully-functional and efficient package for\nlarge-scale quantum chemistry simulations, has been open-sourced at\nhttps:\/\/github.com\/bytedance\/byteqc, leveraging recent advances in\ncomputational power and many-body algorithms.\n  Regarding computational power, several standard algorithms are efficiently\nimplemented on modern GPUs, ranging from mean-field calculations (Hartree-Fock\nand density functional theory) to post-Hartree-Fock methods such as\nM{\\o}ller-Plesset perturbation theory, random phase approximation, coupled\ncluster methods, and quantum Monte Carlo methods. For the algorithmic approach,\nwe also employ a quantum embedding method, which significantly expands the\ntractable system size while preserving high accuracy at the gold-standard\nlevel.\n  All these features have been systematically benchmarked. For standalone\nalgorithms, the benchmark results demonstrate up to a 60$\\times$ speedup when\ncompared to 100-core CPUs. Additionally, the tractable system sizes have been\nsignificantly expanded: 1,610 orbitals for coupled cluster with single and\ndouble excitations (1,380 orbitals with perturbative triple excitations),\n11,040 orbitals for M{\\o}ller-Plesset perturbation theory of second order,\n37,120 orbitals for mean-field calculations under open boundary conditions, and\nover 100,000 orbitals for periodic boundary conditions. For the advanced\nquantum embedding feature, two representative examples are demonstrated: the\nwater cluster problem (2,752 orbitals) and a water monomer adsorbed on a boron\nnitride surface (3,929 orbitals), achieving the gold-standard accuracy.",
        "TabPFN [Hollmann et al., 2023], a Transformer model pretrained to perform\nin-context learning on fresh tabular classification problems, was presented at\nthe last ICLR conference. To better understand its behavior, we treat it as a\nblack-box function approximator generator and observe its generated function\napproximations on a varied selection of training datasets. Exploring its\nlearned inductive biases in this manner, we observe behavior that is at turns\neither brilliant or baffling. We conclude this post with thoughts on how these\nresults might inform the development, evaluation, and application of prior-data\nfitted networks (PFNs) in the future.",
        "We study the amplification of isotropic, incompressible turbulence through\nmultiple planar, collisional shocks, using analytical linear theory. There are\ntwo limiting cases we explore. The first assumes shocks occur rapidly in time\nsuch that the turbulence does not evolve between shocks. Whereas the second\ncase allows enough time for turbulence to isotropize between each shock. For\nthe latter case, through a quasi-equation-of-state, we show that the weak\nmulti-shock limit is agnostic to the distinction between thermal and vortical\nturbulent pressures, like an isotropic volumetric compression. When turbulence\ndoes not return to isotropy between shocks, the generated anisotropy -- itself\na function of shock strength -- can feedback on amplification by further\nshocks, altering choices for maximal or minimal amplification. In addition for\nthis case, we find that amplification is sensitive to the shock ordering. We\nmap how choices of shock strength can impact these amplification differences\ndue to ordering, finding, for example, shock pairs which lead to identical mean\npost-shock fields (density, temperature, pressure) but maximally distinct\nturbulent amplification.",
        "Diophantine approximation explores how well irrational numbers can be\napproximated by rationals, with foundational results by Dirichlet, Hurwitz, and\nLiouville culminating in Roth's theorem. Schmidt's subspace theorem extends\nRoth's results to higher dimensions, with profound implications to Diophantine\nequations and transcendence theory. This article provides a self-contained and\naccessible exposition of Roth's theorem and Schlickewei's refinement of the\nsubspace theorem, with an emphasis on proofs. The arguments presented are\nclassical and approachable for readers with a background in algebraic number\ntheory, serving as a streamlined, yet condensed reference for these fundamental\nresults.",
        "Given a convex set $\\Omega$ of $\\mathbb{R}^n$, we consider the shape\noptimization problem of finding a convex subset $\\omega\\subset \\Omega$, of a\ngiven measure, minimizing the $p$-distance functional $$\\mathcal{J}_p(\\omega)\n:= \\left(\\int_{\\mathbb{S}^{n-1}} |h_\\Omega-h_\\omega|^p\nd\\mathcal{H}^{n-1}\\right)^{\\frac{1}{p}},$$ where $1 \\le p <\\infty$ and\n$h_\\omega$ and $h_\\Omega$ are the support functions of $\\omega$ and the fixed\ncontainer $\\Omega$, respectively.\n  We prove the existence of solutions and show that this minimization problem\n$\\Gamma$-converges, when $p$ tends to $+\\infty$, towards the problem of finding\na convex subset $\\omega\\subset \\Omega$, of a given measure, minimizing the\nHausdorff distance to the convex $\\Omega$.\n  In the planar case, we show that the free parts of the boundary of the\noptimal shapes, i.e., those that are in the interior of $\\Omega$, are given by\npolygonal lines.\n  Still in the $2-d$ setting, from a computational perspective, the classical\nmethod based on optimizing Fourier coefficients of support functions is not\nefficient, as it is unable to efficiently capture the presence of segments on\nthe boundary of optimal shapes. We subsequently propose a method combining\nFourier analysis and a recent numerical scheme, allowing to obtain accurate\nresults, as demonstrated through numerical experiments.",
        "In this paper, we study three matching problems all of which came up quite\nrecently in the field of machine teaching. The cost of a matching is defined in\nsuch a way that, for some formal model of teaching, it equals (or bounds) the\nnumber of labeled examples needed to solve a given teaching task. We show how\nthe cost parameters associated with these problems depend on each other and how\nthey are related to other well known combinatorial parameters (like, for\ninstance, the VC-dimension).",
        "Fuzzy data, prevalent in social sciences and other fields, capture\nuncertainties arising from subjective evaluations and measurement imprecision.\nDespite significant advancements in fuzzy statistics, a unified inferential\nregression-based framework remains undeveloped. Hence, we propose a novel\napproach for analyzing bounded fuzzy variables within a regression framework.\nBuilding on the premise that fuzzy data result from a process analogous to\nstatistical coarsening, we introduce a conditional probabilistic approach that\nlinks observed fuzzy statistics (e.g., mode, spread) to the underlying,\nunobserved statistical model, which depends on external covariates. The\ninferential problem is addressed using Approximate Bayesian methods, mainly\nthrough a Gibbs sampler incorporating a quadratic approximation of the\nposterior distribution. Simulation studies and applications involving external\nvalidations are employed to evaluate the effectiveness of the proposed approach\nfor fuzzy data analysis. By reintegrating fuzzy data analysis into a more\ntraditional statistical framework, this work provides a significant step toward\nenhancing the interpretability and applicability of fuzzy statistical methods\nin many applicative contexts.",
        "Although very large wars remain an enduring threat in global politics, we\nlack a clear understanding of how some wars become large and costly, while most\ndo not. There are three possibilities: large conflicts start with and maintain\nintense fighting, they persist over a long duration, or they escalate in\nintensity over time. Using detailed within-conflict data on civil and\ninterstate wars 1946--2008, we show that escalation dynamics -- variations in\nfighting intensity within an armed conflict -- play a fundamental role in\nproducing large conflicts and are a generic feature of both civil and\ninterstate wars. However, civil wars tend to deescalate when they become very\nlarge, limiting their overall severity, while interstate wars exhibit a\npersistent risk of continual escalation. A non-parametric model demonstrates\nthat this distinction in escalation dynamics can explain the differences in the\nhistorical sizes of civil vs. interstate wars, and explain Richardson's Law\ngoverning the frequency and severity of interstate conflicts over the past 200\nyears. Escalation dynamics also drive enormous uncertainty in forecasting the\neventual sizes of both hypothetical and ongoing civil wars, indicating a need\nto better understand the causes of escalation and deescalation within\nconflicts. The close relationship between the size, and hence the cost, of an\narmed conflict and its potential for escalation has broad implications for\ntheories of conflict onset or termination and for risk assessment in\ninternational relations.",
        "We explore a family of numerical methods, based on the Steffensen divided\ndifference iterative algorithm, that do not evaluate the derivative of the\nobjective functions. The family of methods achieves second-order convergence\nwith two function evaluations per iteration with marginal additional\ncomputational cost. An important side benefit of the method is the improvement\nin stability for different initial conditions compared to the vanilla\nSteffensen method. We present numerical results for scalar functions, fields,\nand scalar fields. This family of methods outperforms the Steffensen method\nwith respect to standard quantitative metrics in most cases.",
        "This paper proposes an analytical framework for the orbital\nTarget-Attacker-Defender game with a non-maneuvering target along elliptic\norbits. Focusing on the linear quadratic game, we derive an analytical solution\nto the matrix Riccati equation, which yields analytical Nash-equilibrium\nstrategies for all players. Based on the analytical strategies, we derive the\nanalytical form of the necessary and sufficient winning conditions for the\nattacker. The simulation results show good consistency between the analytical\nand numerical methods, exhibiting 0.004$\\%$ relative error in the cost\nfunction. The analytical method achieves over 99.9$\\%$ reduction in CPU time\ncompared to the conventional numerical method, strengthening the advantage of\ndeveloping the analytical strategies. Furthermore, we verify the proposed\nwinning conditions and investigate the effects of eccentricity on the game\noutcomes. Our analysis reveals that for games with hovering initial states, the\ninitial position of the defender should be constrained inside a mathematically\ndefinable set to ensure that the attacker wins the game. This constrained set\nfurthermore permits geometric interpretation through our proposed framework.\nThis work establishes the analytical framework for orbital\nTarget-Attacker-Defender games, providing fundamental insights into the\nsolution analysis of the game.",
        "State-space models have been used in many applications, including\neconometrics, engineering, medical research, etc. The maximum likelihood\nestimation (MLE) of the static parameter of general state-space models is not\nstraightforward because the likelihood function is intractable. It is popular\nto use the sequential Monte Carlo(SMC) method to perform gradient ascent\noptimisation in either offline or online fashion. One problem with existing\nonline SMC methods for MLE is that the score estimators are inconsistent, i.e.\nthe bias does not vanish with increasing particle size. In this paper, two SMC\nalgorithms are proposed based on an importance sampling weight function to use\neach set of generated particles more efficiently. The first one is an offline\nalgorithm that locally approximates the likelihood function using importance\nsampling, where the locality is adapted by the effective sample size (ESS). The\nsecond one is a semi-online algorithm that has a computational cost linear in\nthe particle size and uses score estimators that are consistent. We study its\nconsistency and asymptotic normality. Their computational superiority is\nillustrated in numerical studies for long time series.",
        "In a previous work general conditions were given to prove the infiniteness of\nthe genus of certain towers of function fields over a perfect field. It was\nshown that many examples where particular cases of those general results. In\nthis paper the genus of a family of wild towers of function fields will be\nconsidered together with a result with less restrictive sufficient conditions\nfor a wild tower to have infinite genus.",
        "We prove conditional weak-strong uniqueness of the potential Euler solution\nfor external flow around a smooth body in three space dimensions, within the\nclass of viscosity weak solutions with the same initial data. Our sufficient\ncondition is the vanishing of the streamwise component of the skin friction in\nthe inviscid limit, somewhat weaker than the condition of Bardos-Titi in\nbounded domains. Because global-in-time existence of the smooth potential\nsolution leads back to the d'Alembert paradox, we argue that weak-strong\nuniqueness is not a valid criterion for \"relevant\" notions of generalized Euler\nsolution and that our condition is likely to be violated in the inviscid limit.\nWe prove also that the Drivas-Nguyen condition on uniform continuity at the\nwall of the normal velocity component implies weak-strong uniqueness within the\ngeneral class of admissible weak Euler solutions in bounded domains.",
        "This paper presents a case study of application of the preconditioned method\nof conjugate gradients (CG) on a problem with operator resembling the structure\nof sum of Kronecker products. In particular, we are solving the Poisson's\nequation on a sample of homogeneous isotropic ferroelectric material of cuboid\nshape, where the Laplacian is discretized by finite difference. We present\nseveral preconditioners that fits the Kronecker structure and thus can be\nefficiently implemented and applied. Preconditioner based on the Moore--Penrose\npseudoinverse is extremely efficient for this particular problem, and also\napplicable (if we are able to store the dense right-hand side of our problem).\nWe briefly analyze the computational cost of the method and individual\npreconditioners, and illustrate effectiveness of the chosen one by numerical\nexperiments.\n  Although we describe our method as preconditioned CG with pseudoinverse-based\npreconditioner, it can also be seen as pseudoinverse-based direct solver with\niterative refinement by CG iteration.\n  This work is motivated by real application, the method was already\nimplemented in C\/C++ code Ferrodo2 and first results were published in Physical\nReview B 107(9) (2023), paper id 094102.",
        "Reproducibility of computational research is critical for ensuring\ntransparency, reliability and reusability. Challenges with computational\nreproducibility have been documented in several fields, but healthcare\ndiscrete-event simulation (DES) models have not been thoroughly examined in\nthis context. This study assessed the computational reproducibility of eight\npublished healthcare DES models (Python or R), selected to represent diverse\ncontexts, complexities, and years of publication. Repositories and articles\nwere also assessed against guidelines and reporting standards, offering\ninsights into their relationship with reproducibility success. Reproducing\nresults required up to 28 hours of troubleshooting per model, with 50% fully\nreproduced and 50% partially reproduced (12.5% to 94.1% of reported outcomes).\nKey barriers included the absence of open licences, discrepancies between\nreported and coded parameters, and missing code to produce model outputs, run\nscenarios, and generate tables and figures. Addressing these issues would often\nrequire relatively little effort from authors: adding an open licence and\nsharing all materials used to produce the article. Actionable recommendations\nare proposed to enhance reproducibility practices for simulation modellers and\nreviewers.",
        "Choosing the right system architecture for the problem at hand is challenging\ndue to the large design space and high uncertainty in the early stage of the\ndesign process. Formulating the architecting process as an optimization problem\nmay mitigate some of these challenges. This work investigates strategies for\nsolving System Architecture Optimization (SAO) problems: expensive, black-box,\nhierarchical, mixed-discrete, constrained, multi-objective problems that may be\nsubject to hidden constraints. Imputation ratio, correction ratio, correction\nfraction, and max rate diversity metrics are defined for characterizing hierar\nchical design spaces. This work considers two classes of optimization\nalgorithms for SAO: Multi-Objective Evolutionary Algorithms (MOEA) such as\nNSGA-II, and Bayesian Optimization (BO) algorithms. A new Gaussian process\nkernel is presented that enables modeling hierarchical categorical variables,\nextending previous work on modeling continuous and integer hierarchical\nvariables. Next, a hierarchical sampling algorithm that uses design space\nhierarchy to group design vectors by active design variables is developed.\nThen, it is demonstrated that integrating more hierarchy information in the\noptimization algorithms yields better optimization results for BO algorithms.\nSeveral realistic single-objective and multi-objective test problems are used\nfor investigations. Finally, the BO algorithm is applied to a jet engine\narchitecture optimization problem. This work shows that the developed BO\nalgorithm can effectively solve the problem with one order of magnitude less\nfunction evaluations than NSGA-II. The algorithms and problems used in this\nwork are implemented in the open-source Python library SBArchOpt.",
        "An infinite 3-parametric family of superintegrable and exactly-solvable\nquantum models on a plane, admitting separation of variables in polar\ncoordinates, marked by integer index $k$ was introduced in Journ Phys A 42\n(2009) 242001 and was called in literature the TTW system. In this paper it is\nconjectured that the Hamiltonian and both integrals of TTW system have hidden\nalgebra $g^{(k)}$ - it was checked for $k=1,2,3,4$ - having its\nfinite-dimensional representation spaces as the invariant subspaces. It is\nchecked that for $k=1,2,3,4$ that the Hamiltonian $H$, two integrals ${\\cal\nI}_{1,2}$ and their commutator ${\\cal I}_{12} = [{\\cal I}_1,{\\cal I}_2]$ are\nfour generating elements of the polynomial algebra of integrals of the order\n$(k+1)$: $[{\\cal I}_1,{\\cal I}_{12}] = P_{k+1}(H, {\\cal I}_{1,2},{\\cal\nI}_{12})$, $[{\\cal I}_2,{\\cal I}_{12}] = Q_{k+1}(H, {\\cal I}_{1,2},{\\cal\nI}_{12})$, where $P_{k+1},Q_{k+1}$ are polynomials of degree $(k+1)$ written in\nterms of ordered monomials of $H, {\\cal I}_{1,2},{\\cal I}_{12}$. This implies\nthat polynomial algebra of integrals is subalgebra of $g^{(k)}$. It is\nconjectured that all is true for any integer $k$.",
        "We introduce VeloxQ, a fast and efficient solver for Quadratic Unconstrained\nBinary Optimization (QUBO) problems, which are central to numerous real-world\noptimization tasks. Unlike other physics-inspired approaches to optimization\nproblems, such as quantum annealing and quantum computing, VeloxQ does not\nrequire substantial progress of technology to unlock its full potential. We\nbenchmark VeloxQ against the state-of-the-art QUBO solvers based on emerging\ntechnologies. Our comparison includes quantum annealers, specifically D-Wave's\nAdvantage, and Advantage2 prototype platforms, the digital-quantum algorithm\ndesigned to solve Higher-Order Unconstrained Binary Optimization (HUBO)\ndeveloped by Kipu Quantum, physics-inspired algorithms: Simulated Bifurcation\nand Parallel Annealing and an algorithm based on tropical tensor networks. We\nalso take into account modern developments of conventional algorithms: Branch\nand Bound algorithm, an optimal implementation of the brute-force algorithm and\nBEIT QUBO solver. Our results show that VeloxQ not only matches but often\nsurpasses the mentioned solvers in solution quality and runtime. Additionally,\nVeloxQ demonstrates excellent scalability being the only solver capable of\nsolving large-scale optimization problems, including up to $2\\times 10^{8}$\nsparsely connected variables, that are currently intractable for its\ncompetitors. These findings position VeloxQ as a powerful and practical tool\nfor tackling large-scale QUBO and HUBO problems, offering a compelling\nalternative to existing quantum and classical optimization methods.",
        "We propose a zero estimation cost (ZEC) scheme for causal-encoding\nnoncausal-decoding vector-valued Witsenhausen counterexample based on the\ncoordination coding result. In contrast to source coding, our goal is to\ncommunicate a controlled system state. The introduced ZEC scheme is a joint\ncontrol-communication approach that transforms the system state into a sequence\nthat can be efficiently communicated using block coding. Numerical results show\nthat our approach significantly reduces the power required for achieving\nzero-estimation-cost state reconstruction at the decoder. In the second part,\nwe introduce a more general non-zero estimation cost (Non-ZEC) scheme. We\nobserve numerically that the Non-ZEC scheme operates as a time-sharing\nmechanism between the two-point strategy and the ZEC scheme. Overall, by\nleveraging block-coding gain, our proposed methods substantially improve the\npower-estimation trade-off for Witsenhausen counterexample.",
        "Orbital magnetization, a key property arising from the orbital motion of\nelectrons, plays a crucial role in determining the magnetic behavior of\nmolecules and solids. Despite its straightforward calculation in finite\nsystems, the computation in periodic systems poses challenges due to the\nill-defined position operator and surface current contributions. The modern\ntheory of orbital magnetization, implemented in the Density Functional Theory\n(DFT) framework, offers an accurate solution via the \"converse approach.\" Here,\nwe introduce QE-CONVERSE, a refactored, modular implementation of this method,\nreplacing outdated routines from Quantum ESPRESSO (version 3.2). QE-CONVERSE\nintegrates modern computational libraries like scaLAPACK and ELPA, enhancing\nscalability, especially for large supercell calculations. This work focuses on\nproviding the community with a reliable, accurate orbital magnetization package\nfor properties such as Electron Paramagnetic Resonance (EPR) g-tensors and\nNuclear Magnetic Resonance (NMR) chemical shifts, particularly where\nperturbative methods fail. We demonstrate QE-CONVERSE's effectiveness with\nbenchmark cases, including the NMR shifts of ${}^{27}$Al in alumina and\n${}^{17}$O and ${}^{29}$Si in $\\alpha$-quartz, as well as the EPR g-tensor for\n$_{ }^{n}\\Sigma(n\\geq 2)$ radicals and nitrogen defects in silicon. Results\nshow excellent agreement with theoretical and experimental data, with improved\naccuracy in EPR calculations over linear response methods. QE-CONVERSE is fully\ncompatible with recent Quantum ESPRESSO versions, enabling new possibilities\nfor studying complex materials",
        "We introduce a systematic procedure based on optimal control theory to\naddress the full counting statistics of particle transport in a stochastic\nsystem. Our approach enhances the performance of a Thouless pump in the\nnon-adiabatic regime by simultaneously optimizing the average pumping rate\nwhile minimizing noise. We demonstrate our optimization procedure on a\nparadigmatic model for the electronic transport through a quantum dot, both in\nthe limit of vanishing Coulomb interaction and in the interacting regime. Our\nmethod enables independent control of the moments associated with charge and\nspin transfer, allowing for the enhancement of spin current with minimal charge\ncurrent or the independent tuning of spin and charge fluctuations. These\nresults underscore the versatility of our approach, which can be applied to a\nbroad class of stochastic systems.",
        "The TNG-Cluster magnetohydrodynamic cosmological simulations, produce a\ndiverse population of X-ray cavities in the intracluster medium (ICM) of\nsimulated galaxy clusters. These arise from episodic, high velocity, kinetic\nenergy injections from the central active supermassive black hole (AGN, SMBH).\nHere, we present the first comprehensive comparative analysis of X-ray cavities\nin TNG-Cluster with observational data. First, we select a volume-limited\nsample of 35 real clusters ($z \\leq 0.071$, M$_\\text{500c}$ = 10$^{14-14.8}$\nM$_\\odot$) observed with the Chandra X-ray Observatory, identify 3 analogs for\neach in TNG-Cluster (total of 105) and generate mock Chandra images using same\nexposure times as their observed counterparts. We identify X-ray cavities and\nmeasure their properties in both datasets using identical techniques, ensuring\na direct, apples-to-apples comparison. Our analysis reveals that both samples\nhave a similar fraction of X-ray cavities (35-43 per cent). They exhibit\ncomparable sizes and morphologies, although the sizes of simulated X-ray\ncavities still attached to the SMBH are somewhat larger in TNG-Cluster -- a\nscarcity at $< 10$ kpc. The area of TNG X-ray cavities increases as they rise\nin the ICM, consistent with the trend seen in the observational sample. The\ncavity powers, estimated using observational techniques, show good agreement\nbetween the two samples (10$^{42-45}$ erg.s$^{-1}$), suggesting that X-ray\ncavities in the simulation are an important heating mechanism in cluster cores.\nOverall, the rather simple AGN feedback model of TNG, with no model choices\nmade to reproduce X-ray morphological features, and without cosmic rays,\ncreates a quantitatively realistic population of X-ray cavities at cluster\nscales.",
        "Mid-infrared spectroscopy offers powerful label-free molecular analysis\ncapabilities but faces significant challenges when analyzing complex biological\nsamples. Here, we present a transformative surface-enhanced infrared absorption\nspectroscopy (SEIRAS) platform that overcomes fundamental limitations through\nkey innovations. First, we demonstrate high-throughput wafer-scale fabrication\nof mid-IR plasmonic micro-hole-array (MHA) metasurfaces on free-standing\nsilicon nitride membranes, yielding approximately 400 sensor chips per 6-inch\nwafer. Second, our gradient MHA metasurface design supports spectrally cascaded\nplasmonic modes, generating over 400 sharp resonance peaks across the 1200-2000\ncm-1 fingerprint region. This approach enables comprehensive molecular\nfingerprinting using simple imaging optics in transmission mode. Third, we\nvalidate our SEIRAS platform using a model polymer system and clinical\nperitoneal fluid samples from ovarian cancer patients, demonstrating its\ncapability to resolve complex molecular signatures in real biological\nspecimens. The platform's dense spectral coverage ensures optimal on-resonance\nenhancement across the broad fingerprint region, revealing previously obscured\nvibrational bands that conventional IR spectroscopy cannot distinguish. By\ncombining high-throughput fabrication with simplified optical readout and the\ncapability to analyze complex biological samples, our work establishes a\nfoundation for translating SEIRAS technology into practical biomedical\napplications.",
        "In this paper we show a limit result for the reliability function of a system\n-- that is, the probability that the whole system is still operational after a\ncertain given time -- when the number of components of the system grows to\ninfinity. More specifically, we consider a sequence of mixed coherent systems\nwhose components are homogeneous and non-repairable, with failure-times\ngoverned by a L\\'evy-frailty Marshall-Olkin (LFMO) distribution -- a\ndistribution that allows simultaneous component failures. We show that under\nintegrability conditions the reliability function converges to the probability\nof a first-passage time of a L\\'evy subordinator process. To the best of our\nknowledge, this is the first result to tackle the asymptotic behavior of the\nreliability function as the number of components of the system grows. To\nillustrate our approach, we give an example of a parametric family of\nreliability functions where the system failure time converges in distribution\nto an exponential random variable, and give computational experiments testing\nconvergence.",
        "Let $G$ be a finite group, let $F$ be a field of prime $p$ order and let\n$\\bar{F}$ be an algebraic closure of $F$. In this context, we develop an\nequivalence relation on the set of isomorphic types of indecomposable (resp.\nsimple) $\\bar{F}G$-modules. Note that here all indecomposable (resp. simple)\nmodules are absolutely indecomposable (resp. simple). Also we show that members\nof the same equivalence have related Green Theory invariants. This work\ncompletes and extends previous results of the author.",
        "Optimization problems involving the minimization of a finite sum of smooth,\npossibly non-convex functions arise in numerous applications. To achieve a\nconsensus solution over a network, distributed optimization algorithms, such as\n\\textbf{EXTRA} (decentralized exact first-order algorithm), have been proposed\nto address these challenges. In this paper, we analyze the convergence\nproperties of \\textbf{EXTRA} in the context of smooth, non-convex optimization.\nBy interpreting its updates as a nonlinear dynamical system, we show novel\ninsights into its convergence properties. Specifically, i) \\textbf{EXTRA}\nconverges to a consensual first-order stationary point of the global objective\nwith a sublinear rate; and ii) \\textbf{EXTRA} avoids convergence to consensual\nstrict saddle points, offering second-order guarantees that ensure robustness.\nThese findings provide a deeper understanding of \\textbf{EXTRA} in a non-convex\ncontext."
      ]
    }
  },
  {
    "id":2411.05213,
    "research_type":"basic",
    "start_id":"b1",
    "start_title":"Microreactors gain wider use as alternative to batch production",
    "start_abstract":"The microreactors are gaining wide use among the pharmaceuticals and chemical companies as an alternative to batch production. They not only offers a flexible approach to continuous processing, but promises to save much of the time and effort consumed while expanding the chemistries at commercial scale. Most of the ten global pharma and chemical companies have acquired the Cytos Lab System,, a microreactor product developed by Cellular Process Chemistry Systems GmbH (CPC). Microreactors are comprised of plates with distinct channels in the submillimeter range, providing high surface-to-volume ratio, ultra fast mixing and high degree of control at all levels of production.",
    "start_categories":[
      "q-bio.OT"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Effect of bioclogging in porous media on complex conductivity signatures"
      ],
      "abstract":[
        "Flow through sand columns inoculated with Pseudomonas aeruginosa were used to investigate the effect of bioclogging on complex conductivity and flow transport properties. Complex (0.1\u20131000 Hz), bulk hydraulic (K), volumetric rate (Q), dispersivity (D), microbial cell concentrations monitored over time. Environmental scanning electron microscope images sands obtained at end experiment. Bioclogging resulting from increases in concentration production exopolymeric substances (EPS) had a large impact imaginary ( \u03c3 \u2033), K, Q, D, porosity (\u03a6). Changes electrical properties developed three stages: an initial stage 1 no significant changes all measured parameters (Days 1\u20138), which we attribute reversible irreversible attachment cells sand. In 2a 9\u201316), caused by growth biomass either as microcolonies filling pore throats and\/or uniform covering surfaces resulted maximum decrease Q but only moderate \u2033. 2b 17\u201324), EPS increase biofilm thickness higher \u2033 compared 2a. 3 25\u201332), reached quasi steady state insignificant are observed parameters. The results this study suggest that can provide complimentary information assessment porous media."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation",
        "AI Load Dynamics--A Power Electronics Perspective",
        "In-sensor 24 classes HAR under 850 Bytes",
        "MapGS: Generalizable Pretraining and Data Augmentation for Online\n  Mapping via Novel View Synthesis",
        "MOVE: A Mixture-of-Vision-Encoders Approach for Domain-Focused\n  Vision-Language Processing",
        "Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation",
        "A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for\n  General Industrial Process Tasks Based on Large Language Model",
        "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models",
        "A Novel Control Strategy for Offset Points Tracking in the Context of\n  Agricultural Robotics",
        "EFT & Species Scale: Friends or foes?",
        "ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code\n  Generation",
        "Intuitionistic modal logics: new and simpler decidability proofs for FIK\n  and LIK",
        "PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via\n  Multimodal LLM Agents",
        "Enhancing Disinformation Detection with Explainable AI and Named Entity\n  Replacement",
        "Enhancing Highway Safety: Accident Detection on the A9 Test Stretch\n  Using Roadside Sensors",
        "Ray-Tracing Channel Modeling for LEO Satellite-to-Ground Communication\n  Systems",
        "An Amplitude-Encoding-Based Classical-Quantum Transfer Learning\n  framework: Outperforming Classical Methods in Image Recognition",
        "Monocular Person Localization under Camera Ego-motion",
        "Translational diffusion in supercooled water at and near the glass\n  transition temperature -- 136 K",
        "NICER, NuSTAR and Insight-HXMT views to the newly discovered black hole\n  X-ray binary Swift J1727.8--1613",
        "Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool\n  for Perceived Socio-Economic Bias in LLMs",
        "Analysis of the Effect of Bars on Environmental Dependence of Disc\n  Galaxies with MaNGA Survey Data",
        "The Human-AI Handshake Framework: A Bidirectional Approach to Human-AI\n  Collaboration",
        "Optimal Transport Barycenter via Nonconvex-Concave Minimax Optimization",
        "Towards understanding structure-function relationships in random fiber\n  networks",
        "GPU-accelerated Subcycling Time Integration with the Einstein Toolkit",
        "Sparsity learning via structured functional factor augmentation",
        "Crosscap states with tunable entanglement as exact eigenstates of local\n  spin chain Hamiltonians",
        "Vietoris-Rips complexes of torus grids"
      ],
      "abstract":[
        "Long-term, open-domain dialogue capabilities are essential for chatbots\naiming to recall past interactions and demonstrate emotional intelligence (EI).\nYet, most existing research relies on synthetic, LLM-generated data, leaving\nopen questions about real-world conversational patterns. To address this gap,\nwe introduce REALTALK, a 21-day corpus of authentic messaging app dialogues,\nproviding a direct benchmark against genuine human interactions.\n  We first conduct a dataset analysis, focusing on EI attributes and persona\nconsistency to understand the unique challenges posed by real-world dialogues.\nBy comparing with LLM-generated conversations, we highlight key differences,\nincluding diverse emotional expressions and variations in persona stability\nthat synthetic dialogues often fail to capture.\n  Building on these insights, we introduce two benchmark tasks: (1) persona\nsimulation where a model continues a conversation on behalf of a specific user\ngiven prior dialogue context; and (2) memory probing where a model answers\ntargeted questions requiring long-term memory of past interactions.\n  Our findings reveal that models struggle to simulate a user solely from\ndialogue history, while fine-tuning on specific user chats improves persona\nemulation. Additionally, existing models face significant challenges in\nrecalling and leveraging long-term context within real-world conversations.",
        "As AI-driven computing infrastructures rapidly scale, discussions around data\ncenter design often emphasize energy consumption, water and electricity usage,\nworkload scheduling, and thermal management. However, these perspectives often\noverlook the critical interplay between AI-specific load transients and power\nelectronics. This paper addresses that gap by examining how large-scale AI\nworkloads impose unique demands on power conversion chains and, in turn, how\nthe power electronics themselves shape the dynamic behavior of AI-based\ninfrastructure. We illustrate the fundamental constraints imposed by\nmulti-stage power conversion architectures and highlight the key role of\nfinal-stage modules in defining realistic power slew rates for GPU clusters.\nOur analysis shows that traditional designs, optimized for slower-varying or\nCPU-centric workloads, may not adequately accommodate the rapid load ramps and\ndrops characteristic of AI accelerators. To bridge this gap, we present\ninsights into advanced converter topologies, hierarchical control methods, and\nenergy buffering techniques that collectively enable robust and efficient power\ndelivery. By emphasizing the bidirectional influence between AI workloads and\npower electronics, we hope this work can set a good starting point and offer\npractical design considerations to ensure future exascale-capable data centers\ncan meet the stringent performance, reliability, and scalability requirements\nof next-generation AI deployments.",
        "The year 2023 was a key year for tinyML unleashing a new age of intelligent\nsensors pushing intelligence from the MCU into the source of the data at the\nsensor level, enabling them to perform sophisticated algorithms and machine\nlearning models in real-time. This study presents an innovative approach to\nHuman Activity Recognition (HAR) using Intelligent Sensor Processing Units\n(ISPUs), demonstrating the feasibility of deploying complex machine learning\nmodels directly on ultra-constrained sensor hardware. We developed a 24-class\nHAR model achieving 85\\% accuracy while operating within an 850-byte stack\nmemory limit. The model processes accelerometer and gyroscope data in real\ntime, reducing latency, enhancing data privacy, and consuming only 0.5 mA of\npower. To address memory constraints, we employed incremental class injection\nand feature optimization techniques, enabling scalability without compromising\nperformance. This work underscores the transformative potential of on-sensor\nprocessing for applications in healthcare, predictive maintenance, and smart\nenvironments, while introducing a publicly available, diverse HAR dataset for\nfurther research. Future efforts will explore advanced compression techniques\nand broader IoT integration to push the boundaries of TinyML on constrained\ndevices.",
        "Online mapping reduces the reliance of autonomous vehicles on high-definition\n(HD) maps, significantly enhancing scalability. However, recent advancements\noften overlook cross-sensor configuration generalization, leading to\nperformance degradation when models are deployed on vehicles with different\ncamera intrinsics and extrinsics. With the rapid evolution of novel view\nsynthesis methods, we investigate the extent to which these techniques can be\nleveraged to address the sensor configuration generalization challenge. We\npropose a novel framework leveraging Gaussian splatting to reconstruct scenes\nand render camera images in target sensor configurations. The target config\nsensor data, along with labels mapped to the target config, are used to train\nonline mapping models. Our proposed framework on the nuScenes and Argoverse 2\ndatasets demonstrates a performance improvement of 18% through effective\ndataset augmentation, achieves faster convergence and efficient training, and\nexceeds state-of-the-art performance when using only 25% of the original\ntraining data. This enables data reuse and reduces the need for laborious data\nlabeling. Project page at https:\/\/henryzhangzhy.github.io\/mapgs.",
        "Multimodal language models (MLMs) integrate visual and textual information by\ncoupling a vision encoder with a large language model through the specific\nadapter. While existing approaches commonly rely on a single pre-trained vision\nencoder, there is a great variability of specialized encoders that can boost\nmodel's performance in distinct domains. In this work, we propose MOVE (Mixture\nof Vision Encoders) a simple yet effective approach to leverage multiple\npre-trained encoders for specialized multimodal tasks. MOVE automatically\nroutes inputs to the most appropriate encoder among candidates such as Unichat,\nInternViT, and Texify, thereby enhancing performance across a diverse set of\nbenchmarks, including ChartQA, MMBench, and MMMU. Experimental results\ndemonstrate that MOVE achieves competitive accuracy without incurring the\ncomplexities of image slicing for high-resolution images.",
        "The Qwen 2.5 3B base model was fine-tuned to generate contextually rich and\nengaging movie dialogue, leveraging the Cornell Movie-Dialog Corpus, a curated\ndataset of movie conversations. Due to the limitations in GPU computing and\nVRAM, the training process began with the 0.5B model progressively scaling up\nto the 1.5B and 3B versions as efficiency improvements were implemented. The\nQwen 2.5 series, developed by Alibaba Group, stands at the forefront of small\nopen-source pre-trained models, particularly excelling in creative tasks\ncompared to alternatives like Meta's Llama 3.2 and Google's Gemma. Results\ndemonstrate the ability of small models to produce high-quality, realistic\ndialogue, offering a promising approach for real-time, context-sensitive\nconversation generation.",
        "Data-driven soft sensors (DDSS) have become mainstream methods for predicting\nkey performance indicators in process industries. However, DDSS development\nrequires complex and costly customized designs tailored to various tasks during\nthe modeling process. Moreover, DDSS are constrained to a single structured\ndata modality, limiting their ability to incorporate additional contextual\nknowledge. Furthermore, DDSSs' limited representation learning leads to weak\npredictive performance with scarce data. To address these challenges, we\npropose a general framework named LLM-TKESS (large language model for\ntext-based knowledge-embedded soft sensing), harnessing the powerful general\nproblem-solving capabilities, cross-modal knowledge transfer abilities, and\nfew-shot capabilities of LLM for enhanced soft sensing modeling. Specifically,\nan auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's\npotential for capturing temporal relationships within series and spatial\nsemantic relationships among auxiliary variables. Then, we propose a two-stage\nfine-tuning alignment strategy: in the first stage, employing\nparameter-efficient fine-tuning through autoregressive training adjusts LLM to\nrapidly accommodate process variable data, resulting in a soft sensing\nfoundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM\nto various downstream tasks without modifying its architecture. Then, we\npropose two text-based knowledge-embedded soft sensors, integrating new natural\nlanguage modalities to overcome the limitations of pure structured data models.\nFurthermore, benefiting from LLM's pre-existing world knowledge, our model\ndemonstrates outstanding predictive capabilities in small sample conditions.\nUsing the thermal deformation of air preheater rotor as a case study, we\nvalidate through extensive experiments that LLM-TKESS exhibits outstanding\nperformance.",
        "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. Experimental results on multiple datasets demonstrate\nthat AdEval effectively reduces the impact of data contamination on evaluation\noutcomes, enhancing both the fairness and reliability of the evaluation\nprocess.",
        "In this paper, we present a novel method to control a rigidly connected\nlocation on the vehicle, such as a point on the implement in case of\nagricultural tasks. Agricultural robots are transforming modern farming by\nenabling precise and efficient operations, replacing humans in arduous tasks\nwhile reducing the use of chemicals. Traditionnaly, path_following algorithms\nare designed to guide the vehicle's center along a predefined trajetory.\nHowever, since the actual agronomic task is performed by the implement, it is\nessential to control a specific point on the implement itself rather than\nvehicle's center. As such, we present in this paper two approaches for\nachieving the control of an offset point on the robot. The first approach\nadapts existing control laws, initially inteded for rear axle's midpoint, to\nmanage the desired lateral deviation. The second approach employs backstepping\ncontrol techniques to create a control law that directly targets the implement.\nWe conduct real-world experiments, highlighting the limitations of traditional\napproaches for offset points control, and demonstrating the strengths and\nweaknesses of the proposed methods.",
        "Recently the notion that quantum gravity effects could manifest at scales\nmuch lower than the Planck scale has seen an intense Swamplandish revival.\nDozens of works have explored how the so-called species scale -- at which an\neffective description of gravity must break down -- relates to String Theory\nand the Swampland conjectures. In particular, the interplay between this scale\nand the abundant towers of states becoming lighter in asymptotic regions of\nmoduli spaces has proved to be key in understanding the real scale of quantum\ngravity. Nevertheless concerns have been raised regarding the validity of using\ninfinite towers of states when estimating this scale within Effective Field\nTheory and, more precisely, the consistency of cutting the tower part way\nthrough in a framework that relies on a clear separation of scales. In this\nwork we take an EFT point-of-view and provide a detailed perturbative\nderivation of the species scale -- by computing the 1-loop graviton propagator\nin the presence of many fields -- thereby clarifying common sources of\nconfusion in the literature. Not only do we clarify the setup, assumptions and\nregimes of validity of the result, but more importantly apply the same methods\nto an infinite tower of states. We show how each state in the tower contributes\nto the species scale and how the procedure of counting only ''light fields''\ncan be compatible with not cutting the tower, thereby maintaining the harmony\nbetween infinite towers and EFTs even in the context of the species scale.",
        "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in chart understanding tasks. However, interpreting charts with\ntextual descriptions often leads to information loss, as it fails to fully\ncapture the dense information embedded in charts. In contrast, parsing charts\ninto code provides lossless representations that can effectively contain all\ncritical details. Although existing open-source MLLMs have achieved success in\nchart understanding tasks, they still face two major challenges when applied to\nchart-to-code tasks.: (1) Low executability and poor restoration of chart\ndetails in the generated code and (2) Lack of large-scale and diverse training\ndata. To address these challenges, we propose \\textbf{ChartCoder}, the first\ndedicated chart-to-code MLLM, which leverages Code LLMs as the language\nbackbone to enhance the executability of the generated code. Furthermore, we\nintroduce \\textbf{Chart2Code-160k}, the first large-scale and diverse dataset\nfor chart-to-code generation, and propose the \\textbf{Snippet-of-Thought (SoT)}\nmethod, which transforms direct chart-to-code generation data into step-by-step\ngeneration. Experiments demonstrate that ChartCoder, with only 7B parameters,\nsurpasses existing open-source MLLMs on chart-to-code benchmarks, achieving\nsuperior chart restoration and code excitability. Our code will be available at\nhttps:\/\/github.com\/thunlp\/ChartCoder.",
        "In this note, by integrating ideas concerning terminating tableaux-based\nprocedures in modal logics and finite frame property of intuitionistic modal\nlogic IK, we provide new and simpler decidability proofs for FIK and LIK.",
        "Chart visualizations, while essential for data interpretation and\ncommunication, are predominantly accessible only as images in PDFs, lacking\nsource data tables and stylistic information. To enable effective editing of\ncharts in PDFs or digital scans, we present PlotEdit, a novel multi-agent\nframework for natural language-driven end-to-end chart image editing via\nself-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1)\nChart2Table for data table extraction, (2) Chart2Vision for style attribute\nidentification, (3) Chart2Code for retrieving rendering code, (4) Instruction\nDecomposition Agent for parsing user requests into executable steps, and (5)\nMultimodal Editing Agent for implementing nuanced chart component modifications\n- all coordinated through multimodal feedback to maintain visual fidelity.\nPlotEdit outperforms existing baselines on the ChartCraft dataset across style,\nlayout, format, and data-centric edits, enhancing accessibility for visually\nchallenged users and improving novice productivity.",
        "The automatic detection of disinformation presents a significant challenge in\nthe field of natural language processing. This task addresses a multifaceted\nsocietal and communication issue, which needs approaches that extend beyond the\nidentification of general linguistic patterns through data-driven algorithms.\nIn this research work, we hypothesise that text classification methods are not\nable to capture the nuances of disinformation and they often ground their\ndecision in superfluous features. Hence, we apply a post-hoc explainability\nmethod (SHAP, SHapley Additive exPlanations) to identify spurious elements with\nhigh impact on the classification models. Our findings show that\nnon-informative elements (e.g., URLs and emoticons) should be removed and named\nentities (e.g., Rwanda) should be pseudo-anonymized before training to avoid\nmodels' bias and increase their generalization capabilities. We evaluate this\nmethodology with internal dataset and external dataset before and after\napplying extended data preprocessing and named entity replacement. The results\nshow that our proposal enhances on average the performance of a disinformation\nclassification method with external test data in 65.78% without a significant\ndecrease of the internal test performance.",
        "Road traffic injuries are the leading cause of death for people aged 5-29,\nresulting in about 1.19 million deaths each year. To reduce these fatalities,\nit is essential to address human errors like speeding, drunk driving, and\ndistractions. Additionally, faster accident detection and quicker medical\nresponse can help save lives. We propose an accident detection framework that\ncombines a rule-based approach with a learning-based one. We introduce a\ndataset of real-world highway accidents featuring high-speed crash sequences.\nIt includes 294,924 labeled 2D boxes, 93,012 labeled 3D boxes, and track IDs\nacross 48,144 frames captured at 10 Hz using four roadside cameras and LiDAR\nsensors. The dataset covers ten object classes and is released in the OpenLABEL\nformat. Our experiments and analysis demonstrate the reliability of our method.",
        "Based on the vision of global coverage for sixth-generation (6G) wireless\ncommunication systems, the low earth orbit (LEO) satellite-to-ground channel\nmodel for urban scenarios has emerged as highly important for the system\ndesign. In this paper, we propose an LEO satellite-to-ground channel model\nthrough shooting and bouncing rays (SBR) algorithm to analyze the channel\ncharacteristics. The orbit of LEO is modeled by the simplified general\nperturbations 4 (SGP4), and an accurate celestial model is applied to calculate\nthe Doppler shift of multipath in a transmission time window of LEO\nsatellite-to-ground communications. Channel characteristics of LEO\nsatellite-to-ground communications such as the root-mean-square (RMS) delay\nspread, the Doppler shift, and the received power at different times are\nobtained. The simulation results show that the received power is only\nsignificantly noticeable in the transmission time window when the satellite is\nclose to the receiver. Proposed model validates the effectiveness of\nray-tracing in actual LEO satellite-to-ground communication scenarios and\nextends the calculation of the Doppler shift.",
        "The classical-quantum transfer learning (CQTL) method is introduced to\naddress the challenge of training large-scale, high-resolution image data on a\nlimited number of qubits (ranging from tens to hundreds) in the current Noisy\nIntermediate-Scale quantum (NISQ) era. existing CQTL frameworks have been\ndemonstrate quantum advantages with a small number of parameters (around 50),\nbut the performance of quantum neural networks is sensitive to the number of\nparameters. Currently, there is a lack of exploration into larger-scale quantum\ncircuits with more parameters. This paper proposes an amplitude-encoding-based\nclassical-quantum transfer learning (AE-CQTL) framework, accompanied by an\neffective learning algorithm. The AE-CQTL framework multiplies the parameters\nof quantum circuits by using multi-layer ansatz. Based on the AE-CQTL\nframework, we designed and implemented two CQTL neural network models: Transfer\nlearning Quantum Neural Network (TLQNN) and Transfer Learning Quantum\nConvolutional Neural Network (TLQCNN). Both models significantly expand the\nparameter capacity of quantum circuits, elevating the parameter scale from a\nfew dozen to over one hundred parameters. In cross-experiments with three\nbenchmark datasets (MNIST, Fashion-MNIST and CIFAR10) and three source models\n(ResNet18, ResNet50 and DenseNet121), TLQNN and TLQCNN have exceeded the\nbenchmark classical classifier in multiple performance metrics, including\naccuracy, convergence, stability, and generalization capability. Our work\ncontributes to advancing the application of classical-quantum transfer learning\non larger-scale quantum devices in future.",
        "Localizing a person from a moving monocular camera is critical for\nHuman-Robot Interaction (HRI). To estimate the 3D human position from a 2D\nimage, existing methods either depend on the geometric assumption of a fixed\ncamera or use a position regression model trained on datasets containing little\ncamera ego-motion. These methods are vulnerable to fierce camera ego-motion,\nresulting in inaccurate person localization. We consider person localization as\na part of a pose estimation problem. By representing a human with a four-point\nmodel, our method jointly estimates the 2D camera attitude and the person's 3D\nlocation through optimization. Evaluations on both public datasets and real\nrobot experiments demonstrate our method outperforms baselines in person\nlocalization accuracy. Our method is further implemented into a\nperson-following system and deployed on an agile quadruped robot.",
        "The properties of amorphous solid water at and near the calorimetric glass\ntransition temperature, $T_{g}$, of 136 K have been debated for years. One\nhypothesis is that water turns into a \"true\" liquid at $T_{g}$ (i.e., it\nbecomes ergodic) and exhibits all the characteristics of an ergodic liquid,\nincluding translational diffusion. A competing hypothesis is that only\nrotational motion becomes active at $T_{g}$, while the \"real\" glass transition\nin water is at a considerably higher temperature. To address this dispute, we\nhave investigated the diffusive mixing in nanoscale water films, with\nthicknesses up to ~100 nm, using infrared (IR) spectroscopy. The experiments\nused films that were composed of at least 90% $H_{2}O$ with $D_{2}O$ making up\nthe balance and were conducted in conditions where H\/D exchange was essentially\neliminated. Because the IR spectra of multilayer $D_{2}O$ films (e.g.,\nthicknesses of ~3 - 6 nm) embedded within thick $H_{2}O$ films are distinct\nfrom the spectrum of isolated $D_{2}O$ molecules within $H_{2}O$, the diffusive\nmixing of (initially) isotopically layered water films could be followed as a\nfunction of annealing time and temperature. The results show that water films\nwith total thicknesses ranging from ~20 to 100 nm diffusively mixed prior to\ncrystallization for temperatures between 120 and 144 K. The translational\ndiffusion had an Arrhenius temperature dependence with an activation energy of\n40.8 kJ\/mol, which indicates that water at and near $T_{g}$ is a strong liquid.\nThe measured diffusion coefficient at 136 K is 6.25 x 10$^{-21} m^{2}\/s$.",
        "Swift J1727.8--1613 is a black hole X-ray binary newly discovered in 2023. We\nperform spectral analysis with simultaneous Insight-HXMT, NICER and NuSTAR\nobservations when the source was approaching to the hard intermediate state.\nSuch a joint view reveals an additional hard component apart from the normally\nobserved hard component with reflection in the spectrum, to be distinguished\nfrom the usual black hole X-ray binary systems. By including this extra\ncomponent in the spectrum, we have measured a high spin of\n$0.98^{+0.02}_{-0.07}$ and an inclination of around $40^{+1.2}_{-0.8}$ degrees,\nwhich is consistent with NICER results reported before. However, we find that\nthe additional spectral component can not be exclusively determined due to the\nmodel degeneracy. Accordingly, a possible jet\/corona configuration is adjusted\nto account for the spectral fitting with different model trials. The extra\ncomponent may originate either from a relativistic jet or a jet base\/corona\nunderneath a slow jet.",
        "We introduce an Item Response Theory (IRT)-based framework to detect and\nquantify socioeconomic bias in large language models (LLMs) without relying on\nsubjective human judgments. Unlike traditional methods, IRT accounts for item\ndifficulty, improving ideological bias estimation. We fine-tune two LLM\nfamilies (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct\nideological positions and introduce a two-stage approach: (1) modeling response\navoidance and (2) estimating perceived bias in answered responses. Our results\nshow that off-the-shelf LLMs often avoid ideological engagement rather than\nexhibit bias, challenging prior claims of partisanship. This empirically\nvalidated framework enhances AI alignment research and promotes fairer AI\ngovernance.",
        "Bars are fundamental structures in disc galaxies, although their role in\ngalaxy evolution is still not fully known. This study investigates the effect\nof the presence of bars on the environmental dependence of disc galaxies'\nproperties using the volume-limited sample from Mapping Nearby Galaxies at APO\n(MaNGA) survey. The disc galaxies with and without bars samples were obtained\nusing the Galaxy Zoo 2 project then assigned into field and group sub-samples.\nThese sub-samples were used to compare the stellar mass, star formation rate,\n$g-r$ colour, concentration index and gas phase metallicity, and their\nrelationships between field and group environments. Then these are used to\ninvestigate if there is an existence of any difference between galaxies with\nand without bars. A one-to-one correspondence between field and group galaxies'\nproperties were observed, and a strong dependence on the environment for\nproperties of unbarred galaxies was observed when compared to barred. The\nstellar mass against star formation rate, $g-r$ colour against concentration\nindex and stellar mass against gas phase metallicity of unbarred galaxies\nstrongly depend on environment while for barred these relations weakly depend\non environment. The study concludes that bars in disc galaxies decrease the\ndependence of analysed properties and its relations on the environment.",
        "Human-AI collaboration is evolving from a tool-based perspective to a\npartnership model where AI systems complement and enhance human capabilities.\nTraditional approaches often limit AI to a supportive role, missing the\npotential for reciprocal relationships where both human and AI inputs\ncontribute to shared goals. Although Human-Centered AI (HcAI) frameworks\nemphasize transparency, ethics, and user experience, they often lack mechanisms\nfor genuine, dynamic collaboration. The \"Human-AI Handshake Model\" addresses\nthis gap by introducing a bi-directional, adaptive framework with five key\nattributes: information exchange, mutual learning, validation, feedback, and\nmutual capability augmentation. These attributes foster balanced interaction,\nenabling AI to act as a responsive partner, evolving with users over time.\nHuman enablers like user experience and trust, alongside AI enablers such as\nexplainability and responsibility, facilitate this collaboration, while shared\nvalues of ethics and co-evolution ensure sustainable growth. Distinct from\nexisting frameworks, this model is reflected in tools like GitHub Copilot and\nChatGPT, which support bi-directional learning and transparency. Challenges\nremain, including maintaining ethical standards and ensuring effective user\noversight. Future research will explore these challenges, aiming to create a\ntruly collaborative human-AI partnership that leverages the strengths of both\nto achieve outcomes beyond what either could accomplish alone.",
        "The optimal transport barycenter (a.k.a. Wasserstein barycenter) is a\nfundamental notion of averaging that extends from the Euclidean space to the\nWasserstein space of probability distributions. Computation of the\nunregularized barycenter for discretized probability distributions on point\nclouds is a challenging task when the domain dimension $d > 1$. Most practical\nalgorithms for approximating the barycenter problem are based on entropic\nregularization. In this paper, we introduce a nearly linear time $O(m \\log{m})$\nand linear space complexity $O(m)$ primal-dual algorithm, the\nWasserstein-Descent $\\dot{\\mathbb{H}}^1$-Ascent (WDHA) algorithm, for computing\nthe exact barycenter when the input probability density functions are\ndiscretized on an $m$-point grid. The key success of the WDHA algorithm hinges\non alternating between two different yet closely related Wasserstein and\nSobolev optimization geometries for the primal barycenter and dual Kantorovich\npotential subproblems. Under reasonable assumptions, we establish the\nconvergence rate and iteration complexity of WDHA to its stationary point when\nthe step size is appropriately chosen. Superior computational efficacy,\nscalability, and accuracy over the existing Sinkhorn-type algorithms are\ndemonstrated on high-resolution (e.g., $1024 \\times 1024$ images) 2D synthetic\nand real data.",
        "Random fiber networks form the structural foundation of numerous biological\ntissues and engineered materials, from living tissue in the human body to\neveryday materials like fabric and paper. From a mechanics perspective,\nunderstanding the structure-function relationships of random fiber networks is\nparticularly interesting because when external force is applied to these\nnetworks, only a small subset of fibers will actually carry the majority of the\nload. Specifically, these load-bearing fibers propagate through the network to\nform load paths, also called force chains. However, the relationship between\nfiber network geometric structure, force chains, and the overall mechanical\nbehavior of random fiber network structures remains poorly understood. To this\nend, we implement a finite element model of random fiber networks with\ngeometrically exact beam elements, and use this model to explore random fiber\nnetwork mechanical behavior. Our focus is twofold. First, we explore the\nmechanical behavior of single fiber chains and random fiber networks. Second,\nwe propose and validate an interpretable analytical approach to predicting\nfiber network mechanics from structural information alone. Key findings include\ninsight into the critical strain-stiffening transition point for single fiber\nchains and fiber networks, and a connection between force chains and the\ndistance-weighted graph shortest paths that arise by treating fiber networks as\nspatial graph structures. This work marks an important step towards mapping the\nstructure-function relationships of random fiber networks undergoing large\ndeformations. Additionally, with our code distributed under open-source\nlicenses, we hope that future researchers can directly build on our work to\naddress related problems beyond the scope defined here.",
        "Adaptive Mesh Refinement (AMR) with subcycling in time enables different grid\nlevels to advance using their own time steps, ensuring finer grids employ\nsmaller steps for accuracy while coarser grids take larger steps to improve\ncomputational efficiency. We present the development, validation, and\nperformance analysis of a subcycling in time algorithm implemented within the\nCarpetX driver in the Einstein Toolkit framework. This new approach\nsignificantly improves upon the previous subcycling implementation in the\nCarpet driver by achieving higher-order convergence -- fourth order in time\ninstead of second order -- and enhanced scaling performance. The key innovation\nlies in optimizing the exchange of ghost points at refinement boundaries,\nlimiting it to the same number as those at inter-process boundaries using dense\noutput from coarser levels, thereby reducing computational and communication\noverhead compared to the implementation in Carpet, which required a larger\nnumber of buffer zones.\n  To validate the algorithm, we first demonstrate its fourth-order convergence\nusing a scalar wave test. We then apply the algorithm to binary black hole\n(BBH) simulations, confirming its robustness and accuracy in a realistic\nastrophysical scenario. The results show excellent agreement with the\nwell-established LazEv code. Scaling tests on CPU (Frontera) and GPU (Vista)\nclusters reveal significant performance gains, with the new implementation\nachieving improved speed and scalability compared to the Carpet-based version.",
        "As one of the most powerful tools for examining the association between\nfunctional covariates and a response, the functional regression model has been\nwidely adopted in various interdisciplinary studies. Usually, a limited number\nof functional covariates are assumed in a functional linear regression model.\nNevertheless, correlations may exist between functional covariates in\nhigh-dimensional functional linear regression models, which brings significant\nstatistical challenges to statistical inference and functional variable\nselection. In this article, a novel functional factor augmentation structure\n(fFAS) is proposed for multivariate functional series, and a multivariate\nfunctional factor augmentation selection model (fFASM) is further proposed to\ndeal with issues arising from variable selection of correlated functional\ncovariates. Theoretical justifications for the proposed fFAS are provided, and\nstatistical inference results of the proposed fFASM are established. Numerical\ninvestigations support the superb performance of the novel fFASM model in terms\nof estimation accuracy and selection consistency.",
        "It has been observed recently that various spin chain Hamiltonians admit\nspecial zero energy \"crosscap\" eigenstates. These states are made up of\nmaximally entangled Bell pairs prepared on antipodal sites of a periodic chain.\nWe generalize the states by allowing the antipodal pairs to have non-maximal,\ntunable entanglement. We give sufficient conditions for such states to be exact\nzero energy eigenstates of a local Hamiltonian. The conditions are naturally\nsatisfied in many models which have a global U(1) symmetry. These models\ninclude well known integrable models such as the XX model, the Bariev model,\nthe folded XXZ model, and also a variety of non-integrable models. Using the\nzero-energy crosscap states we also derive a family of exact zero modes with\nsub-volume law entanglement.",
        "We study the topology of Vietoris--Rips complexes of finite grids on the\ntorus. Let $T_{n,n}$ be the grid of $n\\times n$ points on the flat torus\n$S^1\\times S^1$, equipped with the $l^1$ metric. Let $\\mathrm{VR}(T_{n,n};k)$\nbe the Vietoris--Rips simplicial complex of this torus grid at scale $k\\ge 0$.\nFor $n\\ge 7$ and small scales $2\\le k\\le \\frac{n-1}{3}$, the complex\n$\\mathrm{VR}(T_{n,n};k)$ is homotopy equivalent to the torus. For large scales\n$k\\ge 2\\lfloor\\frac{n}{2}\\rfloor$, the complex $\\mathrm{VR}(T_{n,n};k)$ is a\nsimplex and hence contractible. Interesting topology arises over intermediate\nscales $\\frac{n-1}{3}<k<2\\lfloor\\frac{n}{2}\\rfloor$. For example, we prove that\n$\\mathrm{VR}(T_{2n,2n};2n-1)\\cong S^{2n^2-1}$ for $n\\ge 2$, that\n$\\mathrm{VR}(T_{3n,3n};n)\\simeq\\vee^{6n^2-1}S^2$ for $n\\ge 2$, and that\n$\\mathrm{VR}(T_{3n-1,3n-1};n)\\simeq \\bigvee_{6n-3} S^2\\vee \\bigvee_{6n-2}S^3$\nfor $n\\geq 3$. Based on homology computations, we conjecture that\n$\\mathrm{VR}(T_{n,n};k)$ is homotopy equivalent to a $3$-sphere for a countable\nfamily of $(n,k)$ pairs, and we prove this for $(n,k)=(7,4)$."
      ]
    }
  },
  {
    "id":2411.05213,
    "research_type":"basic",
    "start_id":"b8",
    "start_title":"How flocculation can explain coexistence in the chemostat",
    "start_abstract":"We study a chemostat model in which two microbial species grow on single resource. show that coexistence is possible when the would normally win exclusive competition aggregates flocs. Our mathematical analysis exploits fact flocculation fast compared to biological growth, common hypothesis floc models. A numerical shows validity of this approach large parameter range. indicate how our yields mechanistic justification for so-called density-dependent growth.",
    "start_categories":[
      "q-bio.OT"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Effect of bioclogging in porous media on complex conductivity signatures"
      ],
      "abstract":[
        "Flow through sand columns inoculated with Pseudomonas aeruginosa were used to investigate the effect of bioclogging on complex conductivity and flow transport properties. Complex (0.1\u20131000 Hz), bulk hydraulic (K), volumetric rate (Q), dispersivity (D), microbial cell concentrations monitored over time. Environmental scanning electron microscope images sands obtained at end experiment. Bioclogging resulting from increases in concentration production exopolymeric substances (EPS) had a large impact imaginary ( \u03c3 \u2033), K, Q, D, porosity (\u03a6). Changes electrical properties developed three stages: an initial stage 1 no significant changes all measured parameters (Days 1\u20138), which we attribute reversible irreversible attachment cells sand. In 2a 9\u201316), caused by growth biomass either as microcolonies filling pore throats and\/or uniform covering surfaces resulted maximum decrease Q but only moderate \u2033. 2b 17\u201324), EPS increase biofilm thickness higher \u2033 compared 2a. 3 25\u201332), reached quasi steady state insignificant are observed parameters. The results this study suggest that can provide complimentary information assessment porous media."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation",
        "AI Load Dynamics--A Power Electronics Perspective",
        "In-sensor 24 classes HAR under 850 Bytes",
        "MapGS: Generalizable Pretraining and Data Augmentation for Online\n  Mapping via Novel View Synthesis",
        "MOVE: A Mixture-of-Vision-Encoders Approach for Domain-Focused\n  Vision-Language Processing",
        "Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation",
        "A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for\n  General Industrial Process Tasks Based on Large Language Model",
        "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models",
        "A Novel Control Strategy for Offset Points Tracking in the Context of\n  Agricultural Robotics",
        "EFT & Species Scale: Friends or foes?",
        "ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code\n  Generation",
        "Intuitionistic modal logics: new and simpler decidability proofs for FIK\n  and LIK",
        "PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via\n  Multimodal LLM Agents",
        "Enhancing Disinformation Detection with Explainable AI and Named Entity\n  Replacement",
        "Enhancing Highway Safety: Accident Detection on the A9 Test Stretch\n  Using Roadside Sensors",
        "Ray-Tracing Channel Modeling for LEO Satellite-to-Ground Communication\n  Systems",
        "An Amplitude-Encoding-Based Classical-Quantum Transfer Learning\n  framework: Outperforming Classical Methods in Image Recognition",
        "Monocular Person Localization under Camera Ego-motion",
        "Translational diffusion in supercooled water at and near the glass\n  transition temperature -- 136 K",
        "NICER, NuSTAR and Insight-HXMT views to the newly discovered black hole\n  X-ray binary Swift J1727.8--1613",
        "Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool\n  for Perceived Socio-Economic Bias in LLMs",
        "Analysis of the Effect of Bars on Environmental Dependence of Disc\n  Galaxies with MaNGA Survey Data",
        "The Human-AI Handshake Framework: A Bidirectional Approach to Human-AI\n  Collaboration",
        "Optimal Transport Barycenter via Nonconvex-Concave Minimax Optimization",
        "Towards understanding structure-function relationships in random fiber\n  networks",
        "GPU-accelerated Subcycling Time Integration with the Einstein Toolkit",
        "Sparsity learning via structured functional factor augmentation",
        "Crosscap states with tunable entanglement as exact eigenstates of local\n  spin chain Hamiltonians",
        "Vietoris-Rips complexes of torus grids"
      ],
      "abstract":[
        "Long-term, open-domain dialogue capabilities are essential for chatbots\naiming to recall past interactions and demonstrate emotional intelligence (EI).\nYet, most existing research relies on synthetic, LLM-generated data, leaving\nopen questions about real-world conversational patterns. To address this gap,\nwe introduce REALTALK, a 21-day corpus of authentic messaging app dialogues,\nproviding a direct benchmark against genuine human interactions.\n  We first conduct a dataset analysis, focusing on EI attributes and persona\nconsistency to understand the unique challenges posed by real-world dialogues.\nBy comparing with LLM-generated conversations, we highlight key differences,\nincluding diverse emotional expressions and variations in persona stability\nthat synthetic dialogues often fail to capture.\n  Building on these insights, we introduce two benchmark tasks: (1) persona\nsimulation where a model continues a conversation on behalf of a specific user\ngiven prior dialogue context; and (2) memory probing where a model answers\ntargeted questions requiring long-term memory of past interactions.\n  Our findings reveal that models struggle to simulate a user solely from\ndialogue history, while fine-tuning on specific user chats improves persona\nemulation. Additionally, existing models face significant challenges in\nrecalling and leveraging long-term context within real-world conversations.",
        "As AI-driven computing infrastructures rapidly scale, discussions around data\ncenter design often emphasize energy consumption, water and electricity usage,\nworkload scheduling, and thermal management. However, these perspectives often\noverlook the critical interplay between AI-specific load transients and power\nelectronics. This paper addresses that gap by examining how large-scale AI\nworkloads impose unique demands on power conversion chains and, in turn, how\nthe power electronics themselves shape the dynamic behavior of AI-based\ninfrastructure. We illustrate the fundamental constraints imposed by\nmulti-stage power conversion architectures and highlight the key role of\nfinal-stage modules in defining realistic power slew rates for GPU clusters.\nOur analysis shows that traditional designs, optimized for slower-varying or\nCPU-centric workloads, may not adequately accommodate the rapid load ramps and\ndrops characteristic of AI accelerators. To bridge this gap, we present\ninsights into advanced converter topologies, hierarchical control methods, and\nenergy buffering techniques that collectively enable robust and efficient power\ndelivery. By emphasizing the bidirectional influence between AI workloads and\npower electronics, we hope this work can set a good starting point and offer\npractical design considerations to ensure future exascale-capable data centers\ncan meet the stringent performance, reliability, and scalability requirements\nof next-generation AI deployments.",
        "The year 2023 was a key year for tinyML unleashing a new age of intelligent\nsensors pushing intelligence from the MCU into the source of the data at the\nsensor level, enabling them to perform sophisticated algorithms and machine\nlearning models in real-time. This study presents an innovative approach to\nHuman Activity Recognition (HAR) using Intelligent Sensor Processing Units\n(ISPUs), demonstrating the feasibility of deploying complex machine learning\nmodels directly on ultra-constrained sensor hardware. We developed a 24-class\nHAR model achieving 85\\% accuracy while operating within an 850-byte stack\nmemory limit. The model processes accelerometer and gyroscope data in real\ntime, reducing latency, enhancing data privacy, and consuming only 0.5 mA of\npower. To address memory constraints, we employed incremental class injection\nand feature optimization techniques, enabling scalability without compromising\nperformance. This work underscores the transformative potential of on-sensor\nprocessing for applications in healthcare, predictive maintenance, and smart\nenvironments, while introducing a publicly available, diverse HAR dataset for\nfurther research. Future efforts will explore advanced compression techniques\nand broader IoT integration to push the boundaries of TinyML on constrained\ndevices.",
        "Online mapping reduces the reliance of autonomous vehicles on high-definition\n(HD) maps, significantly enhancing scalability. However, recent advancements\noften overlook cross-sensor configuration generalization, leading to\nperformance degradation when models are deployed on vehicles with different\ncamera intrinsics and extrinsics. With the rapid evolution of novel view\nsynthesis methods, we investigate the extent to which these techniques can be\nleveraged to address the sensor configuration generalization challenge. We\npropose a novel framework leveraging Gaussian splatting to reconstruct scenes\nand render camera images in target sensor configurations. The target config\nsensor data, along with labels mapped to the target config, are used to train\nonline mapping models. Our proposed framework on the nuScenes and Argoverse 2\ndatasets demonstrates a performance improvement of 18% through effective\ndataset augmentation, achieves faster convergence and efficient training, and\nexceeds state-of-the-art performance when using only 25% of the original\ntraining data. This enables data reuse and reduces the need for laborious data\nlabeling. Project page at https:\/\/henryzhangzhy.github.io\/mapgs.",
        "Multimodal language models (MLMs) integrate visual and textual information by\ncoupling a vision encoder with a large language model through the specific\nadapter. While existing approaches commonly rely on a single pre-trained vision\nencoder, there is a great variability of specialized encoders that can boost\nmodel's performance in distinct domains. In this work, we propose MOVE (Mixture\nof Vision Encoders) a simple yet effective approach to leverage multiple\npre-trained encoders for specialized multimodal tasks. MOVE automatically\nroutes inputs to the most appropriate encoder among candidates such as Unichat,\nInternViT, and Texify, thereby enhancing performance across a diverse set of\nbenchmarks, including ChartQA, MMBench, and MMMU. Experimental results\ndemonstrate that MOVE achieves competitive accuracy without incurring the\ncomplexities of image slicing for high-resolution images.",
        "The Qwen 2.5 3B base model was fine-tuned to generate contextually rich and\nengaging movie dialogue, leveraging the Cornell Movie-Dialog Corpus, a curated\ndataset of movie conversations. Due to the limitations in GPU computing and\nVRAM, the training process began with the 0.5B model progressively scaling up\nto the 1.5B and 3B versions as efficiency improvements were implemented. The\nQwen 2.5 series, developed by Alibaba Group, stands at the forefront of small\nopen-source pre-trained models, particularly excelling in creative tasks\ncompared to alternatives like Meta's Llama 3.2 and Google's Gemma. Results\ndemonstrate the ability of small models to produce high-quality, realistic\ndialogue, offering a promising approach for real-time, context-sensitive\nconversation generation.",
        "Data-driven soft sensors (DDSS) have become mainstream methods for predicting\nkey performance indicators in process industries. However, DDSS development\nrequires complex and costly customized designs tailored to various tasks during\nthe modeling process. Moreover, DDSS are constrained to a single structured\ndata modality, limiting their ability to incorporate additional contextual\nknowledge. Furthermore, DDSSs' limited representation learning leads to weak\npredictive performance with scarce data. To address these challenges, we\npropose a general framework named LLM-TKESS (large language model for\ntext-based knowledge-embedded soft sensing), harnessing the powerful general\nproblem-solving capabilities, cross-modal knowledge transfer abilities, and\nfew-shot capabilities of LLM for enhanced soft sensing modeling. Specifically,\nan auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's\npotential for capturing temporal relationships within series and spatial\nsemantic relationships among auxiliary variables. Then, we propose a two-stage\nfine-tuning alignment strategy: in the first stage, employing\nparameter-efficient fine-tuning through autoregressive training adjusts LLM to\nrapidly accommodate process variable data, resulting in a soft sensing\nfoundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM\nto various downstream tasks without modifying its architecture. Then, we\npropose two text-based knowledge-embedded soft sensors, integrating new natural\nlanguage modalities to overcome the limitations of pure structured data models.\nFurthermore, benefiting from LLM's pre-existing world knowledge, our model\ndemonstrates outstanding predictive capabilities in small sample conditions.\nUsing the thermal deformation of air preheater rotor as a case study, we\nvalidate through extensive experiments that LLM-TKESS exhibits outstanding\nperformance.",
        "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. Experimental results on multiple datasets demonstrate\nthat AdEval effectively reduces the impact of data contamination on evaluation\noutcomes, enhancing both the fairness and reliability of the evaluation\nprocess.",
        "In this paper, we present a novel method to control a rigidly connected\nlocation on the vehicle, such as a point on the implement in case of\nagricultural tasks. Agricultural robots are transforming modern farming by\nenabling precise and efficient operations, replacing humans in arduous tasks\nwhile reducing the use of chemicals. Traditionnaly, path_following algorithms\nare designed to guide the vehicle's center along a predefined trajetory.\nHowever, since the actual agronomic task is performed by the implement, it is\nessential to control a specific point on the implement itself rather than\nvehicle's center. As such, we present in this paper two approaches for\nachieving the control of an offset point on the robot. The first approach\nadapts existing control laws, initially inteded for rear axle's midpoint, to\nmanage the desired lateral deviation. The second approach employs backstepping\ncontrol techniques to create a control law that directly targets the implement.\nWe conduct real-world experiments, highlighting the limitations of traditional\napproaches for offset points control, and demonstrating the strengths and\nweaknesses of the proposed methods.",
        "Recently the notion that quantum gravity effects could manifest at scales\nmuch lower than the Planck scale has seen an intense Swamplandish revival.\nDozens of works have explored how the so-called species scale -- at which an\neffective description of gravity must break down -- relates to String Theory\nand the Swampland conjectures. In particular, the interplay between this scale\nand the abundant towers of states becoming lighter in asymptotic regions of\nmoduli spaces has proved to be key in understanding the real scale of quantum\ngravity. Nevertheless concerns have been raised regarding the validity of using\ninfinite towers of states when estimating this scale within Effective Field\nTheory and, more precisely, the consistency of cutting the tower part way\nthrough in a framework that relies on a clear separation of scales. In this\nwork we take an EFT point-of-view and provide a detailed perturbative\nderivation of the species scale -- by computing the 1-loop graviton propagator\nin the presence of many fields -- thereby clarifying common sources of\nconfusion in the literature. Not only do we clarify the setup, assumptions and\nregimes of validity of the result, but more importantly apply the same methods\nto an infinite tower of states. We show how each state in the tower contributes\nto the species scale and how the procedure of counting only ''light fields''\ncan be compatible with not cutting the tower, thereby maintaining the harmony\nbetween infinite towers and EFTs even in the context of the species scale.",
        "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in chart understanding tasks. However, interpreting charts with\ntextual descriptions often leads to information loss, as it fails to fully\ncapture the dense information embedded in charts. In contrast, parsing charts\ninto code provides lossless representations that can effectively contain all\ncritical details. Although existing open-source MLLMs have achieved success in\nchart understanding tasks, they still face two major challenges when applied to\nchart-to-code tasks.: (1) Low executability and poor restoration of chart\ndetails in the generated code and (2) Lack of large-scale and diverse training\ndata. To address these challenges, we propose \\textbf{ChartCoder}, the first\ndedicated chart-to-code MLLM, which leverages Code LLMs as the language\nbackbone to enhance the executability of the generated code. Furthermore, we\nintroduce \\textbf{Chart2Code-160k}, the first large-scale and diverse dataset\nfor chart-to-code generation, and propose the \\textbf{Snippet-of-Thought (SoT)}\nmethod, which transforms direct chart-to-code generation data into step-by-step\ngeneration. Experiments demonstrate that ChartCoder, with only 7B parameters,\nsurpasses existing open-source MLLMs on chart-to-code benchmarks, achieving\nsuperior chart restoration and code excitability. Our code will be available at\nhttps:\/\/github.com\/thunlp\/ChartCoder.",
        "In this note, by integrating ideas concerning terminating tableaux-based\nprocedures in modal logics and finite frame property of intuitionistic modal\nlogic IK, we provide new and simpler decidability proofs for FIK and LIK.",
        "Chart visualizations, while essential for data interpretation and\ncommunication, are predominantly accessible only as images in PDFs, lacking\nsource data tables and stylistic information. To enable effective editing of\ncharts in PDFs or digital scans, we present PlotEdit, a novel multi-agent\nframework for natural language-driven end-to-end chart image editing via\nself-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1)\nChart2Table for data table extraction, (2) Chart2Vision for style attribute\nidentification, (3) Chart2Code for retrieving rendering code, (4) Instruction\nDecomposition Agent for parsing user requests into executable steps, and (5)\nMultimodal Editing Agent for implementing nuanced chart component modifications\n- all coordinated through multimodal feedback to maintain visual fidelity.\nPlotEdit outperforms existing baselines on the ChartCraft dataset across style,\nlayout, format, and data-centric edits, enhancing accessibility for visually\nchallenged users and improving novice productivity.",
        "The automatic detection of disinformation presents a significant challenge in\nthe field of natural language processing. This task addresses a multifaceted\nsocietal and communication issue, which needs approaches that extend beyond the\nidentification of general linguistic patterns through data-driven algorithms.\nIn this research work, we hypothesise that text classification methods are not\nable to capture the nuances of disinformation and they often ground their\ndecision in superfluous features. Hence, we apply a post-hoc explainability\nmethod (SHAP, SHapley Additive exPlanations) to identify spurious elements with\nhigh impact on the classification models. Our findings show that\nnon-informative elements (e.g., URLs and emoticons) should be removed and named\nentities (e.g., Rwanda) should be pseudo-anonymized before training to avoid\nmodels' bias and increase their generalization capabilities. We evaluate this\nmethodology with internal dataset and external dataset before and after\napplying extended data preprocessing and named entity replacement. The results\nshow that our proposal enhances on average the performance of a disinformation\nclassification method with external test data in 65.78% without a significant\ndecrease of the internal test performance.",
        "Road traffic injuries are the leading cause of death for people aged 5-29,\nresulting in about 1.19 million deaths each year. To reduce these fatalities,\nit is essential to address human errors like speeding, drunk driving, and\ndistractions. Additionally, faster accident detection and quicker medical\nresponse can help save lives. We propose an accident detection framework that\ncombines a rule-based approach with a learning-based one. We introduce a\ndataset of real-world highway accidents featuring high-speed crash sequences.\nIt includes 294,924 labeled 2D boxes, 93,012 labeled 3D boxes, and track IDs\nacross 48,144 frames captured at 10 Hz using four roadside cameras and LiDAR\nsensors. The dataset covers ten object classes and is released in the OpenLABEL\nformat. Our experiments and analysis demonstrate the reliability of our method.",
        "Based on the vision of global coverage for sixth-generation (6G) wireless\ncommunication systems, the low earth orbit (LEO) satellite-to-ground channel\nmodel for urban scenarios has emerged as highly important for the system\ndesign. In this paper, we propose an LEO satellite-to-ground channel model\nthrough shooting and bouncing rays (SBR) algorithm to analyze the channel\ncharacteristics. The orbit of LEO is modeled by the simplified general\nperturbations 4 (SGP4), and an accurate celestial model is applied to calculate\nthe Doppler shift of multipath in a transmission time window of LEO\nsatellite-to-ground communications. Channel characteristics of LEO\nsatellite-to-ground communications such as the root-mean-square (RMS) delay\nspread, the Doppler shift, and the received power at different times are\nobtained. The simulation results show that the received power is only\nsignificantly noticeable in the transmission time window when the satellite is\nclose to the receiver. Proposed model validates the effectiveness of\nray-tracing in actual LEO satellite-to-ground communication scenarios and\nextends the calculation of the Doppler shift.",
        "The classical-quantum transfer learning (CQTL) method is introduced to\naddress the challenge of training large-scale, high-resolution image data on a\nlimited number of qubits (ranging from tens to hundreds) in the current Noisy\nIntermediate-Scale quantum (NISQ) era. existing CQTL frameworks have been\ndemonstrate quantum advantages with a small number of parameters (around 50),\nbut the performance of quantum neural networks is sensitive to the number of\nparameters. Currently, there is a lack of exploration into larger-scale quantum\ncircuits with more parameters. This paper proposes an amplitude-encoding-based\nclassical-quantum transfer learning (AE-CQTL) framework, accompanied by an\neffective learning algorithm. The AE-CQTL framework multiplies the parameters\nof quantum circuits by using multi-layer ansatz. Based on the AE-CQTL\nframework, we designed and implemented two CQTL neural network models: Transfer\nlearning Quantum Neural Network (TLQNN) and Transfer Learning Quantum\nConvolutional Neural Network (TLQCNN). Both models significantly expand the\nparameter capacity of quantum circuits, elevating the parameter scale from a\nfew dozen to over one hundred parameters. In cross-experiments with three\nbenchmark datasets (MNIST, Fashion-MNIST and CIFAR10) and three source models\n(ResNet18, ResNet50 and DenseNet121), TLQNN and TLQCNN have exceeded the\nbenchmark classical classifier in multiple performance metrics, including\naccuracy, convergence, stability, and generalization capability. Our work\ncontributes to advancing the application of classical-quantum transfer learning\non larger-scale quantum devices in future.",
        "Localizing a person from a moving monocular camera is critical for\nHuman-Robot Interaction (HRI). To estimate the 3D human position from a 2D\nimage, existing methods either depend on the geometric assumption of a fixed\ncamera or use a position regression model trained on datasets containing little\ncamera ego-motion. These methods are vulnerable to fierce camera ego-motion,\nresulting in inaccurate person localization. We consider person localization as\na part of a pose estimation problem. By representing a human with a four-point\nmodel, our method jointly estimates the 2D camera attitude and the person's 3D\nlocation through optimization. Evaluations on both public datasets and real\nrobot experiments demonstrate our method outperforms baselines in person\nlocalization accuracy. Our method is further implemented into a\nperson-following system and deployed on an agile quadruped robot.",
        "The properties of amorphous solid water at and near the calorimetric glass\ntransition temperature, $T_{g}$, of 136 K have been debated for years. One\nhypothesis is that water turns into a \"true\" liquid at $T_{g}$ (i.e., it\nbecomes ergodic) and exhibits all the characteristics of an ergodic liquid,\nincluding translational diffusion. A competing hypothesis is that only\nrotational motion becomes active at $T_{g}$, while the \"real\" glass transition\nin water is at a considerably higher temperature. To address this dispute, we\nhave investigated the diffusive mixing in nanoscale water films, with\nthicknesses up to ~100 nm, using infrared (IR) spectroscopy. The experiments\nused films that were composed of at least 90% $H_{2}O$ with $D_{2}O$ making up\nthe balance and were conducted in conditions where H\/D exchange was essentially\neliminated. Because the IR spectra of multilayer $D_{2}O$ films (e.g.,\nthicknesses of ~3 - 6 nm) embedded within thick $H_{2}O$ films are distinct\nfrom the spectrum of isolated $D_{2}O$ molecules within $H_{2}O$, the diffusive\nmixing of (initially) isotopically layered water films could be followed as a\nfunction of annealing time and temperature. The results show that water films\nwith total thicknesses ranging from ~20 to 100 nm diffusively mixed prior to\ncrystallization for temperatures between 120 and 144 K. The translational\ndiffusion had an Arrhenius temperature dependence with an activation energy of\n40.8 kJ\/mol, which indicates that water at and near $T_{g}$ is a strong liquid.\nThe measured diffusion coefficient at 136 K is 6.25 x 10$^{-21} m^{2}\/s$.",
        "Swift J1727.8--1613 is a black hole X-ray binary newly discovered in 2023. We\nperform spectral analysis with simultaneous Insight-HXMT, NICER and NuSTAR\nobservations when the source was approaching to the hard intermediate state.\nSuch a joint view reveals an additional hard component apart from the normally\nobserved hard component with reflection in the spectrum, to be distinguished\nfrom the usual black hole X-ray binary systems. By including this extra\ncomponent in the spectrum, we have measured a high spin of\n$0.98^{+0.02}_{-0.07}$ and an inclination of around $40^{+1.2}_{-0.8}$ degrees,\nwhich is consistent with NICER results reported before. However, we find that\nthe additional spectral component can not be exclusively determined due to the\nmodel degeneracy. Accordingly, a possible jet\/corona configuration is adjusted\nto account for the spectral fitting with different model trials. The extra\ncomponent may originate either from a relativistic jet or a jet base\/corona\nunderneath a slow jet.",
        "We introduce an Item Response Theory (IRT)-based framework to detect and\nquantify socioeconomic bias in large language models (LLMs) without relying on\nsubjective human judgments. Unlike traditional methods, IRT accounts for item\ndifficulty, improving ideological bias estimation. We fine-tune two LLM\nfamilies (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct\nideological positions and introduce a two-stage approach: (1) modeling response\navoidance and (2) estimating perceived bias in answered responses. Our results\nshow that off-the-shelf LLMs often avoid ideological engagement rather than\nexhibit bias, challenging prior claims of partisanship. This empirically\nvalidated framework enhances AI alignment research and promotes fairer AI\ngovernance.",
        "Bars are fundamental structures in disc galaxies, although their role in\ngalaxy evolution is still not fully known. This study investigates the effect\nof the presence of bars on the environmental dependence of disc galaxies'\nproperties using the volume-limited sample from Mapping Nearby Galaxies at APO\n(MaNGA) survey. The disc galaxies with and without bars samples were obtained\nusing the Galaxy Zoo 2 project then assigned into field and group sub-samples.\nThese sub-samples were used to compare the stellar mass, star formation rate,\n$g-r$ colour, concentration index and gas phase metallicity, and their\nrelationships between field and group environments. Then these are used to\ninvestigate if there is an existence of any difference between galaxies with\nand without bars. A one-to-one correspondence between field and group galaxies'\nproperties were observed, and a strong dependence on the environment for\nproperties of unbarred galaxies was observed when compared to barred. The\nstellar mass against star formation rate, $g-r$ colour against concentration\nindex and stellar mass against gas phase metallicity of unbarred galaxies\nstrongly depend on environment while for barred these relations weakly depend\non environment. The study concludes that bars in disc galaxies decrease the\ndependence of analysed properties and its relations on the environment.",
        "Human-AI collaboration is evolving from a tool-based perspective to a\npartnership model where AI systems complement and enhance human capabilities.\nTraditional approaches often limit AI to a supportive role, missing the\npotential for reciprocal relationships where both human and AI inputs\ncontribute to shared goals. Although Human-Centered AI (HcAI) frameworks\nemphasize transparency, ethics, and user experience, they often lack mechanisms\nfor genuine, dynamic collaboration. The \"Human-AI Handshake Model\" addresses\nthis gap by introducing a bi-directional, adaptive framework with five key\nattributes: information exchange, mutual learning, validation, feedback, and\nmutual capability augmentation. These attributes foster balanced interaction,\nenabling AI to act as a responsive partner, evolving with users over time.\nHuman enablers like user experience and trust, alongside AI enablers such as\nexplainability and responsibility, facilitate this collaboration, while shared\nvalues of ethics and co-evolution ensure sustainable growth. Distinct from\nexisting frameworks, this model is reflected in tools like GitHub Copilot and\nChatGPT, which support bi-directional learning and transparency. Challenges\nremain, including maintaining ethical standards and ensuring effective user\noversight. Future research will explore these challenges, aiming to create a\ntruly collaborative human-AI partnership that leverages the strengths of both\nto achieve outcomes beyond what either could accomplish alone.",
        "The optimal transport barycenter (a.k.a. Wasserstein barycenter) is a\nfundamental notion of averaging that extends from the Euclidean space to the\nWasserstein space of probability distributions. Computation of the\nunregularized barycenter for discretized probability distributions on point\nclouds is a challenging task when the domain dimension $d > 1$. Most practical\nalgorithms for approximating the barycenter problem are based on entropic\nregularization. In this paper, we introduce a nearly linear time $O(m \\log{m})$\nand linear space complexity $O(m)$ primal-dual algorithm, the\nWasserstein-Descent $\\dot{\\mathbb{H}}^1$-Ascent (WDHA) algorithm, for computing\nthe exact barycenter when the input probability density functions are\ndiscretized on an $m$-point grid. The key success of the WDHA algorithm hinges\non alternating between two different yet closely related Wasserstein and\nSobolev optimization geometries for the primal barycenter and dual Kantorovich\npotential subproblems. Under reasonable assumptions, we establish the\nconvergence rate and iteration complexity of WDHA to its stationary point when\nthe step size is appropriately chosen. Superior computational efficacy,\nscalability, and accuracy over the existing Sinkhorn-type algorithms are\ndemonstrated on high-resolution (e.g., $1024 \\times 1024$ images) 2D synthetic\nand real data.",
        "Random fiber networks form the structural foundation of numerous biological\ntissues and engineered materials, from living tissue in the human body to\neveryday materials like fabric and paper. From a mechanics perspective,\nunderstanding the structure-function relationships of random fiber networks is\nparticularly interesting because when external force is applied to these\nnetworks, only a small subset of fibers will actually carry the majority of the\nload. Specifically, these load-bearing fibers propagate through the network to\nform load paths, also called force chains. However, the relationship between\nfiber network geometric structure, force chains, and the overall mechanical\nbehavior of random fiber network structures remains poorly understood. To this\nend, we implement a finite element model of random fiber networks with\ngeometrically exact beam elements, and use this model to explore random fiber\nnetwork mechanical behavior. Our focus is twofold. First, we explore the\nmechanical behavior of single fiber chains and random fiber networks. Second,\nwe propose and validate an interpretable analytical approach to predicting\nfiber network mechanics from structural information alone. Key findings include\ninsight into the critical strain-stiffening transition point for single fiber\nchains and fiber networks, and a connection between force chains and the\ndistance-weighted graph shortest paths that arise by treating fiber networks as\nspatial graph structures. This work marks an important step towards mapping the\nstructure-function relationships of random fiber networks undergoing large\ndeformations. Additionally, with our code distributed under open-source\nlicenses, we hope that future researchers can directly build on our work to\naddress related problems beyond the scope defined here.",
        "Adaptive Mesh Refinement (AMR) with subcycling in time enables different grid\nlevels to advance using their own time steps, ensuring finer grids employ\nsmaller steps for accuracy while coarser grids take larger steps to improve\ncomputational efficiency. We present the development, validation, and\nperformance analysis of a subcycling in time algorithm implemented within the\nCarpetX driver in the Einstein Toolkit framework. This new approach\nsignificantly improves upon the previous subcycling implementation in the\nCarpet driver by achieving higher-order convergence -- fourth order in time\ninstead of second order -- and enhanced scaling performance. The key innovation\nlies in optimizing the exchange of ghost points at refinement boundaries,\nlimiting it to the same number as those at inter-process boundaries using dense\noutput from coarser levels, thereby reducing computational and communication\noverhead compared to the implementation in Carpet, which required a larger\nnumber of buffer zones.\n  To validate the algorithm, we first demonstrate its fourth-order convergence\nusing a scalar wave test. We then apply the algorithm to binary black hole\n(BBH) simulations, confirming its robustness and accuracy in a realistic\nastrophysical scenario. The results show excellent agreement with the\nwell-established LazEv code. Scaling tests on CPU (Frontera) and GPU (Vista)\nclusters reveal significant performance gains, with the new implementation\nachieving improved speed and scalability compared to the Carpet-based version.",
        "As one of the most powerful tools for examining the association between\nfunctional covariates and a response, the functional regression model has been\nwidely adopted in various interdisciplinary studies. Usually, a limited number\nof functional covariates are assumed in a functional linear regression model.\nNevertheless, correlations may exist between functional covariates in\nhigh-dimensional functional linear regression models, which brings significant\nstatistical challenges to statistical inference and functional variable\nselection. In this article, a novel functional factor augmentation structure\n(fFAS) is proposed for multivariate functional series, and a multivariate\nfunctional factor augmentation selection model (fFASM) is further proposed to\ndeal with issues arising from variable selection of correlated functional\ncovariates. Theoretical justifications for the proposed fFAS are provided, and\nstatistical inference results of the proposed fFASM are established. Numerical\ninvestigations support the superb performance of the novel fFASM model in terms\nof estimation accuracy and selection consistency.",
        "It has been observed recently that various spin chain Hamiltonians admit\nspecial zero energy \"crosscap\" eigenstates. These states are made up of\nmaximally entangled Bell pairs prepared on antipodal sites of a periodic chain.\nWe generalize the states by allowing the antipodal pairs to have non-maximal,\ntunable entanglement. We give sufficient conditions for such states to be exact\nzero energy eigenstates of a local Hamiltonian. The conditions are naturally\nsatisfied in many models which have a global U(1) symmetry. These models\ninclude well known integrable models such as the XX model, the Bariev model,\nthe folded XXZ model, and also a variety of non-integrable models. Using the\nzero-energy crosscap states we also derive a family of exact zero modes with\nsub-volume law entanglement.",
        "We study the topology of Vietoris--Rips complexes of finite grids on the\ntorus. Let $T_{n,n}$ be the grid of $n\\times n$ points on the flat torus\n$S^1\\times S^1$, equipped with the $l^1$ metric. Let $\\mathrm{VR}(T_{n,n};k)$\nbe the Vietoris--Rips simplicial complex of this torus grid at scale $k\\ge 0$.\nFor $n\\ge 7$ and small scales $2\\le k\\le \\frac{n-1}{3}$, the complex\n$\\mathrm{VR}(T_{n,n};k)$ is homotopy equivalent to the torus. For large scales\n$k\\ge 2\\lfloor\\frac{n}{2}\\rfloor$, the complex $\\mathrm{VR}(T_{n,n};k)$ is a\nsimplex and hence contractible. Interesting topology arises over intermediate\nscales $\\frac{n-1}{3}<k<2\\lfloor\\frac{n}{2}\\rfloor$. For example, we prove that\n$\\mathrm{VR}(T_{2n,2n};2n-1)\\cong S^{2n^2-1}$ for $n\\ge 2$, that\n$\\mathrm{VR}(T_{3n,3n};n)\\simeq\\vee^{6n^2-1}S^2$ for $n\\ge 2$, and that\n$\\mathrm{VR}(T_{3n-1,3n-1};n)\\simeq \\bigvee_{6n-3} S^2\\vee \\bigvee_{6n-2}S^3$\nfor $n\\geq 3$. Based on homology computations, we conjecture that\n$\\mathrm{VR}(T_{n,n};k)$ is homotopy equivalent to a $3$-sphere for a countable\nfamily of $(n,k)$ pairs, and we prove this for $(n,k)=(7,4)$."
      ]
    }
  },
  {
    "id":2411.05213,
    "research_type":"basic",
    "start_id":"b0",
    "start_title":"Effect of bioclogging in porous media on complex conductivity signatures",
    "start_abstract":"Flow through sand columns inoculated with Pseudomonas aeruginosa were used to investigate the effect of bioclogging on complex conductivity and flow transport properties. Complex (0.1\u20131000 Hz), bulk hydraulic (K), volumetric rate (Q), dispersivity (D), microbial cell concentrations monitored over time. Environmental scanning electron microscope images sands obtained at end experiment. Bioclogging resulting from increases in concentration production exopolymeric substances (EPS) had a large impact imaginary ( \u03c3 \u2033), K, Q, D, porosity (\u03a6). Changes electrical properties developed three stages: an initial stage 1 no significant changes all measured parameters (Days 1\u20138), which we attribute reversible irreversible attachment cells sand. In 2a 9\u201316), caused by growth biomass either as microcolonies filling pore throats and\/or uniform covering surfaces resulted maximum decrease Q but only moderate \u2033. 2b 17\u201324), EPS increase biofilm thickness higher \u2033 compared 2a. 3 25\u201332), reached quasi steady state insignificant are observed parameters. The results this study suggest that can provide complimentary information assessment porous media.",
    "start_categories":[
      "math.MP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b1",
        "b8"
      ],
      "title":[
        "Microreactors gain wider use as alternative to batch production",
        "How flocculation can explain coexistence in the chemostat"
      ],
      "abstract":[
        "The microreactors are gaining wide use among the pharmaceuticals and chemical companies as an alternative to batch production. They not only offers a flexible approach to continuous processing, but promises to save much of the time and effort consumed while expanding the chemistries at commercial scale. Most of the ten global pharma and chemical companies have acquired the Cytos Lab System,, a microreactor product developed by Cellular Process Chemistry Systems GmbH (CPC). Microreactors are comprised of plates with distinct channels in the submillimeter range, providing high surface-to-volume ratio, ultra fast mixing and high degree of control at all levels of production.",
        "We study a chemostat model in which two microbial species grow on single resource. show that coexistence is possible when the would normally win exclusive competition aggregates flocs. Our mathematical analysis exploits fact flocculation fast compared to biological growth, common hypothesis floc models. A numerical shows validity of this approach large parameter range. indicate how our yields mechanistic justification for so-called density-dependent growth."
      ],
      "categories":[
        "q-bio.OT"
      ]
    },
    "list":{
      "title":[
        "Reverse Markov Learning: Multi-Step Generative Models for Complex\n  Distributions",
        "CoNOAir: A Neural Operator for Forecasting Carbon Monoxide Evolution in\n  Cities",
        "Malware Detection based on API calls",
        "The Southern Photometrical Local Universe Survey (S-PLUS): searching for\n  metal-poor dwarf galaxies",
        "Integrating Spatiotemporal Vision Transformer into Digital Twins for\n  High-Resolution Heat Stress Forecasting in Campus Environments",
        "The Regular Ricci-Inverse Cosmology with Multiple Anticurvature Scalars",
        "Compliance while resisting: a shear-thickening fluid controller for\n  physical human-robot interaction",
        "Benchmarking Vision-Language Models on Optical Character Recognition in\n  Dynamic Video Environments",
        "Adaptive Negative Damping Control for User-Dependent Multi-Terrain\n  Walking Assistance with a Hip Exoskeleton",
        "MQG4AI Towards Responsible High-risk AI -- Illustrated for Transparency\n  Focusing on Explainability Techniques",
        "Typographic Attacks in a Multi-Image Setting",
        "DDRM-PR: Fourier Phase Retrieval using Denoising Diffusion Restoration\n  Models",
        "Doping dependence of the magnetic ground state in the frustrated magnets\n  Ba$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co)",
        "Adaptive Entanglement Routing with Deep Q-Networks in Quantum Networks",
        "Radio pulse search from Aql X-1",
        "Wideband Cognitive Radio for Joint Communication and Sensing:\n  Optimization of Subcarrier Allocation and beamforming",
        "Electron dynamics induced by quantum cat-state light",
        "Ultrasound Lung Aeration Map via Physics-Aware Neural Operators",
        "Curating Model Problems for Software Designing",
        "Mitigating Hallucinations in Diffusion Models through Adaptive Attention\n  Modulation",
        "Genetic algorithm enhanced Solovay-Kitaev algorithm for quantum\n  compiling",
        "MappedTrace: Tracing Pointer Remotely with Compiler-generated Maps",
        "Generative Modeling of Class Probability for Multi-Modal Representation\n  Learning",
        "Fast spin precession and strong perpendicular magnetic anisotropy in\n  ferrimagnetic Mn4N thin films improved by Pd buffer layer",
        "LuxNAS: A Coherent Photonic Neural Network Powered by Neural\n  Architecture Search",
        "Highly nondegenerate polarization-entangled photon pairs produced\n  through noncritical phasematching in single-domain KTiOPO$_4$",
        "V2Edit: Versatile Video Diffusion Editor for Videos and 3D Scenes",
        "Another one (BH+OB pair) bites the dust"
      ],
      "abstract":[
        "Learning complex distributions is a fundamental challenge in contemporary\napplications. Generative models, such as diffusion models, have demonstrated\nremarkable success in overcoming many limitations of traditional statistical\nmethods. Shen and Meinshausen (2024) introduced engression, a generative\napproach based on scoring rules that maps noise (and covariates, if available)\ndirectly to data. While effective, engression struggles with highly complex\ndistributions, such as those encountered in image data. In this work, we extend\nengression to improve its capability in learning complex distributions. We\npropose a framework that defines a general forward process transitioning from\nthe target distribution to a known distribution (e.g., Gaussian) and then\nlearns a reverse Markov process using multiple engression models. This reverse\nprocess reconstructs the target distribution step by step. Our approach\nsupports general forward processes, allows for dimension reduction, and\nnaturally discretizes the generative process. As a special case, when using a\ndiffusion-based forward process, our framework offers a method to discretize\nthe training and inference of diffusion models efficiently. Empirical\nevaluations on simulated and climate data validate our theoretical insights,\ndemonstrating the effectiveness of our approach in capturing complex\ndistributions.",
        "Carbon Monoxide (CO) is a dominant pollutant in urban areas due to the energy\ngeneration from fossil fuels for industry, automobile, and domestic\nrequirements. Forecasting the evolution of CO in real-time can enable the\ndeployment of effective early warning systems and intervention strategies.\nHowever, the computational cost associated with the physics and chemistry-based\nsimulation makes it prohibitive to implement such a model at the city and\ncountry scale. To address this challenge, here, we present a machine learning\nmodel based on neural operator, namely, Complex Neural Operator for Air Quality\n(CoNOAir), that can effectively forecast CO concentrations. We demonstrate this\nby developing a country-level model for short-term (hourly) and long-term\n(72-hour) forecasts of CO concentrations. Our model outperforms\nstate-of-the-art models such as Fourier neural operators (FNO) and provides\nreliable predictions for both short and long-term forecasts. We further analyse\nthe capability of the model to capture extreme events and generate forecasts in\nurban cities in India. Interestingly, we observe that the model predicts the\nnext hour CO concentrations with R2 values greater than 0.95 for all the cities\nconsidered. The deployment of such a model can greatly assist the governing\nbodies to provide early warning, plan intervention strategies, and develop\neffective strategies by considering several what-if scenarios. Altogether, the\npresent approach could provide a fillip to real-time predictions of CO\npollution in urban cities.",
        "Malware attacks pose a significant threat in today's interconnected digital\nlandscape, causing billions of dollars in damages. Detecting and identifying\nfamilies as early as possible provides an edge in protecting against such\nmalware. We explore a lightweight, order-invariant approach to detecting and\nmitigating malware threats: analyzing API calls without regard to their\nsequence. We publish a public dataset of over three hundred thousand samples\nand their function call parameters for this task, annotated with labels\nindicating benign or malicious activity. The complete dataset is above 550GB\nuncompressed in size. We leverage machine learning algorithms, such as random\nforests, and conduct behavioral analysis by examining patterns and anomalies in\nAPI call sequences. By investigating how the function calls occur regardless of\ntheir order, we can identify discriminating features that can help us identify\nmalware early on. The models we've developed are not only effective but also\nefficient. They are lightweight and can run on any machine with minimal\nperformance overhead, while still achieving an impressive F1-Score of over\n85\\%. We also empirically show that we only need a subset of the function call\nsequence, specifically calls to the ntdll.dll library, to identify malware. Our\nresearch demonstrates the efficacy of this approach through empirical\nevaluations, underscoring its accuracy and scalability. The code is open source\nand available at Github along with the dataset on Zenodo.",
        "The metal content of a galaxy's interstellar medium reflects the interplay\nbetween different evolutionary processes such as feedback from massive stars\nand the accretion of gas from the intergalactic medium. Despite the expected\nabundance of low-luminosity galaxies, the low-mass and low-metallicity regime\nremains relatively understudied. Since the properties of their interstellar\nmedium resemble those of early galaxies, identifying such objects in the Local\nUniverse is crucial to understand the early stages of galaxy evolution. We used\nthe DR3 catalog of the Southern Photometric Local Universe Survey (S-PLUS) to\nselect low-metallicity dwarf galaxy candidates based on color selection\ncriteria typical of metal-poor, star-forming, low-mass systems. The final\nsample contains approximately 50 candidates. Spectral energy distribution\nfitting of the 12 S-PLUS bands reveals that $\\sim$ 90\\% of the candidates are\nbest fit by models with very low stellar metallicities. We obtained long-slit\nobservations with the Gemini Multi-Object Spectrograph to follow-up a pilot\nsample and confirm whether these galaxies have low metallicities. We find\noxygen abundances in the range $7.35<$ 12 + log(O\/H) $< 7.93$ (5\\% to 17\\% of\nthe solar value), confirming their metal-poor nature. Most targets are outliers\nin the mass-metallicity relation, i.e. they display a low metal content\nrelative to their observed stellar masses. In some cases, perturbed optical\nmorphologies might give evidence of dwarf-dwarf interactions or mergers. These\nresults suggest that the low oxygen abundances may be associated with an\nexternal event causing the accretion of metal-poor gas, which dilutes the\noxygen abundance in these systems.",
        "Extreme heat events exacerbated by climate change pose significant challenges\nto urban resilience and planning. This study introduces a climate-responsive\ndigital twin framework integrating the Spatiotemporal Vision Transformer\n(ST-ViT) model to enhance heat stress forecasting and decision-making. Using a\nTexas campus as a testbed, we synthesized high-resolution physical model\nsimulations with spatial and meteorological data to develop fine-scale human\nthermal predictions. The ST-ViT-powered digital twin enables efficient,\ndata-driven insights for planners, policymakers, and campus stakeholders,\nsupporting targeted heat mitigation strategies and advancing climate-adaptive\nurban design.",
        "We investigate the modified gravity in which the Lagrangian of gravity is a\nfunction of the trace of the n-th matrix power of Ricci tensor in a\nFriedmann-Lemaitre-Robertson-Walker(FLRW) spacetime. When n is negative, the\ninverse of Ricci tensor, also called the anticurvature tensor, will be\nintroduced. We design a new class of Ricci-inverse theory containing two\nanticurvature scalars and resulting to be free from the singularity problem.",
        "Physical human-robot interaction (pHRI) is widely needed in many fields, such\nas industrial manipulation, home services, and medical rehabilitation, and puts\nhigher demands on the safety of robots. Due to the uncertainty of the working\nenvironment, the pHRI may receive unexpected impact interference, which affects\nthe safety and smoothness of the task execution. The commonly used linear\nadmittance control (L-AC) can cope well with high-frequency small-amplitude\nnoise, but for medium-frequency high-intensity impact, the effect is not as\ngood. Inspired by the solid-liquid phase change nature of shear-thickening\nfluid, we propose a Shear-thickening Fluid Control (SFC) that can achieve both\nan easy human-robot collaboration and resistance to impact interference. The\nSFC's stability, passivity, and phase trajectory are analyzed in detail, the\nfrequency and time domain properties are quantified, and parameter constraints\nin discrete control and coupled stability conditions are provided. We conducted\nsimulations to compare the frequency and time domain characteristics of L-AC,\nnonlinear admittance controller (N-AC), and SFC, and validated their dynamic\nproperties. In real-world experiments, we compared the performance of L-AC,\nN-AC, and SFC in both fixed and mobile manipulators. L-AC exhibits weak\nresistance to impact. N-AC can resist moderate impacts but not high-intensity\nones, and may exhibit self-excited oscillations. In contrast, SFC demonstrated\nsuperior impact resistance and maintained stable collaboration, enhancing\ncomfort in cooperative water delivery tasks. Additionally, a case study was\nconducted in a factory setting, further affirming the SFC's capability in\nfacilitating human-robot collaborative manipulation and underscoring its\npotential in industrial applications.",
        "This paper introduces an open-source benchmark for evaluating Vision-Language\nModels (VLMs) on Optical Character Recognition (OCR) tasks in dynamic video\nenvironments. We present a curated dataset containing 1,477 manually annotated\nframes spanning diverse domains, including code editors, news broadcasts,\nYouTube videos, and advertisements. Three state of the art VLMs - Claude-3,\nGemini-1.5, and GPT-4o are benchmarked against traditional OCR systems such as\nEasyOCR and RapidOCR. Evaluation metrics include Word Error Rate (WER),\nCharacter Error Rate (CER), and Accuracy. Our results highlight the strengths\nand limitations of VLMs in video-based OCR tasks, demonstrating their potential\nto outperform conventional OCR models in many scenarios. However, challenges\nsuch as hallucinations, content security policies, and sensitivity to occluded\nor stylized text remain. The dataset and benchmarking framework are publicly\navailable to foster further research.",
        "Hip exoskeletons are known for their versatility in assisting users across\nvaried scenarios. However, current assistive strategies often lack the\nflexibility to accommodate for individual walking patterns and adapt to diverse\nlocomotion environments. In this work, we present a novel control strategy that\nadapts the mechanical impedance of the human-exoskeleton system. We design the\nhip assistive torques as an adaptive virtual negative damping, which is able to\ninject energy into the system while allowing the users to remain in control and\ncontribute voluntarily to the movements. Experiments with five healthy subjects\ndemonstrate that our controller reduces the metabolic cost of walking compared\nto free walking (average reduction of 7.2%), and it preserves the lower-limbs\nkinematics. Additionally, our method achieves minimal power losses from the\nexoskeleton across the entire gait cycle (less than 2% negative mechanical\npower out of the total power), ensuring synchronized action with the users'\nmovements. Moreover, we use Bayesian Optimization to adapt the assistance\nstrength and allow for seamless adaptation and transitions across multi-terrain\nenvironments. Our strategy achieves efficient power transmission under all\nconditions. Our approach demonstrates an individualized, adaptable, and\nstraightforward controller for hip exoskeletons, advancing the development of\nviable, adaptive, and user-dependent control laws.",
        "As artificial intelligence (AI) systems become increasingly integrated into\ncritical domains, ensuring their responsible design and continuous development\nis imperative. Effective AI quality management (QM) requires tools and\nmethodologies that address the complexities of the AI lifecycle. In this paper,\nwe propose an approach for AI lifecycle planning that bridges the gap between\ngeneric guidelines and use case-specific requirements (MQG4AI). Our work aims\nto contribute to the development of practical tools for implementing\nResponsible AI (RAI) by aligning lifecycle planning with technical, ethical and\nregulatory demands. Central to our approach is the introduction of a flexible\nand customizable Methodology based on Quality Gates, whose building blocks\nincorporate RAI knowledge through information linking along the AI lifecycle in\na continuous manner, addressing AIs evolutionary character. For our present\ncontribution, we put a particular emphasis on the Explanation stage during\nmodel development, and illustrate how to align a guideline to evaluate the\nquality of explanations with MQG4AI, contributing to overall Transparency.",
        "Large Vision-Language Models (LVLMs) are susceptible to typographic attacks,\nwhich are misclassifications caused by an attack text that is added to an\nimage. In this paper, we introduce a multi-image setting for studying\ntypographic attacks, broadening the current emphasis of the literature on\nattacking individual images. Specifically, our focus is on attacking image sets\nwithout repeating the attack query. Such non-repeating attacks are stealthier,\nas they are more likely to evade a gatekeeper than attacks that repeat the same\nattack text. We introduce two attack strategies for the multi-image setting,\nleveraging the difficulty of the target image, the strength of the attack text,\nand text-image similarity. Our text-image similarity approach improves attack\nsuccess rates by 21% over random, non-specific methods on the CLIP model using\nImageNet while maintaining stealth in a multi-image scenario. An additional\nexperiment demonstrates transferability, i.e., text-image similarity calculated\nusing CLIP transfers when attacking InstructBLIP.",
        "Diffusion models have demonstrated their utility as learned priors for\nsolving various inverse problems. However, most existing approaches are limited\nto linear inverse problems. This paper exploits the efficient and unsupervised\nposterior sampling framework of Denoising Diffusion Restoration Models (DDRM)\nfor the solution of nonlinear phase retrieval problem, which requires\nreconstructing an image from its noisy intensity-only measurements such as\nFourier intensity. The approach combines the model-based alternating-projection\nmethods with the DDRM to utilize pretrained unconditional diffusion priors for\nphase retrieval. The performance is demonstrated through both simulations and\nexperimental data. Results demonstrate the potential of this approach for\nimproving the alternating-projection methods as well as its limitations.",
        "Theoretically, the relative change of the Heisenberg-type nearest-neighbor\ncoupling $J_1$ and next-nearest-neighbor coupling $J_2$ in the\nface-centered-cubic lattice can give rise to three main antiferromagnetic\norderings of type-I, type-II, and type-III. However, it is difficult to tune\nthe $J_2\/J_1$ ratio in real materials. Here, we report studies on the influence\nof Te$^{6+}$ and W$^{6+}$ ions replacement to the magnetic interactions and the\nmagnetic ground states in the double-perovskite compounds\nBa$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co). For\nBa$_2$MnTe$_{1-x}$W$_{x}$O$_6$, the W$^{6+}$ doping on Te$^{6+}$ site is\nsuccessful in $0.02 \\leq x \\leq 0.9$ with short-range orders of the type-I\n($0.02 \\leq x \\leq 0.08$) and type-II ($0.1 \\leq x \\leq 0.9$). In\nBa$_2$CoTe$_{1-x}$W${_x}$O$_6$, x-ray diffraction measurements reveal two\ncrystal structures, including the trigonal phase ($0 \\leq x \\leq 0.1$) and the\ncubic phase ($0.5 \\leq x \\leq 1$), between which is a miscibility gap. Two\nmagnetic transitions are identified in the trigonal phase due to two magnetic\nsubsystems, and the type-II magnetic order is observed in the cubic phase.\nMagnetic phase diagrams of Ba$_2M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co) are\nestablished. Our work shows that the magnetic interactions and ground states of\nBa$_2$$M$Te$_{1-x}$W$_x$O$_6$ can be tuned effectively by the replacement of\nTe$^{6+}$ by W$^{6+}$ ions.",
        "The quantum internet holds transformative potential for global communication\nby harnessing the principles of quantum information processing. Despite\nsignificant advancements in quantum communication technologies, the efficient\ndistribution of critical resources, such as qubits, remains a persistent and\nunresolved challenge. Conventional approaches often fall short of achieving\noptimal resource allocation, underscoring the necessity for more effective\nsolutions. This study proposes a novel reinforcement learning-based adaptive\nentanglement routing framework designed to enable resource allocation tailored\nto the specific demands of quantum applications. The introduced QuDQN model\nutilizes reinforcement learning to optimize the management of quantum networks,\nallocate resources efficiently, and enhance entanglement routing. The model\nintegrates key considerations, including fidelity requirements, network\ntopology, qubit capacity, and request demands.",
        "We present 12 observations of the accreting millisecond X-ray pulsar Aql X-1,\ntaken from August 2022 to October 2023 using the Five-hundred-meter Aperture\nSpherical Radio Telescope at 1250 MHz. These observations covered both the\nquiescence and X-ray outburst states, as determined by analyzing the X-ray data\nfrom the Neutron Star Interior Composition Explorer and the Monitor of All-sky\nX-ray Image. Periodicity and single-pulse searches were conducted for each\nobservation, but no pulsed signals were detected. The obtained upper limit flux\ndensities are in the range of 2.86-5.73 uJy, which provide the lowest limits to\ndate. We discuss several mechanisms that may prevent detection, suggesting that\nAql X-1 may be in the radio-ejection state during quiescence, where the radio\npulsed emissions are absorbed by the matter surrounding the system.",
        "As data traffic grows, wireless systems shift to higher frequency bands (6\nGHz and above), where radar systems also operate. This coexistence demands\neffective interference management and efficient wideband utilization. Cognitive\nRadio (CR) offers a solution but remains limited to single-node or narrowband\nsystems. This paper introduces a generalized wideband CR-enabled communication\nand sensing system with multiple users and targets. We propose a communication\nand sensing sub-carrier allocations framework, followed by transmit beamforming\nfor the primary communication BS and sensing signal design for the secondary\nradar BS. The goal is to maximize the communication sum rate while ensuring\nsensing requirements, minimizing interference, and adhering to power\nconstraints. To solve the resulting non-convex problem, we develop a manifold\noptimization algorithm for communication-only sub-carriers and an alternating\noptimization approach using the generalized Rayleigh quotient and semidefinite\nrelaxation for communication-sensing sub-carriers. Compared to a\nnon-cooperative benchmark, the proposed system achieves a \\qty{10}{\\percent}\ngain in communication sum rate and a \\qty{32}{\\percent} gain in sensing sum\nrate with \\num{12} BS antennas.",
        "We present an effective theory for describing electron dynamics driven by an\noptical external field in a Schr\\\"{o}dinger's cat state. We show that the\nelectron density matrix evolves as an average over trajectories\n$\\{\\rho_\\alpha\\}$ weighted by the Sudarshan--Glauber $P$ distribution\n$P(\\alpha)$ in the weak light--matter coupling regime. Each trajectory obeys an\nequation of motion, $\\mathrm{i} \\partial_t\\rho_\\alpha=\\mathcal{H}_{\\alpha}\n\\rho_\\alpha-\\rho_\\alpha\\mathcal{H}_{\\alpha}$, where an effective Hamiltonian\n$\\mathcal{H}_{\\alpha}$ becomes non-Hermitian due to quantum interference of\nlight. The optical quantum interference is transferred to electrons through the\nasymmetric action between the ket and bra state vectors in $\\rho_{\\alpha}$.\nThis non-Hermitian dynamics differs from the conventional one observed in open\nquantum systems, described by $\\mathrm{i} \\partial_t\\rho=\\mathcal{H}\\rho-\\rho\n\\mathcal{H}^\\dagger$, which has complex conjugation in the second term. We\nconfirm that the results of the effective theory agree with those of full\nelectron--photon system simulations for the few-electron Dicke model,\ndemonstrating experimental accessibility to exotic non-Hermitian dynamics.",
        "Lung ultrasound is a growing modality in clinics for diagnosing and\nmonitoring acute and chronic lung diseases due to its low cost and\naccessibility. Lung ultrasound works by emitting diagnostic pulses, receiving\npressure waves and converting them into radio frequency (RF) data, which are\nthen processed into B-mode images with beamformers for radiologists to\ninterpret. However, unlike conventional ultrasound for soft tissue anatomical\nimaging, lung ultrasound interpretation is complicated by complex\nreverberations from the pleural interface caused by the inability of ultrasound\nto penetrate air. The indirect B-mode images make interpretation highly\ndependent on reader expertise, requiring years of training, which limits its\nwidespread use despite its potential for high accuracy in skilled hands.\n  To address these challenges and democratize ultrasound lung imaging as a\nreliable diagnostic tool, we propose LUNA, an AI model that directly\nreconstructs lung aeration maps from RF data, bypassing the need for\ntraditional beamformers and indirect interpretation of B-mode images. LUNA uses\na Fourier neural operator, which processes RF data efficiently in Fourier\nspace, enabling accurate reconstruction of lung aeration maps. LUNA offers a\nquantitative, reader-independent alternative to traditional semi-quantitative\nlung ultrasound scoring methods. The development of LUNA involves synthetic and\nreal data: We simulate synthetic data with an experimentally validated approach\nand scan ex vivo swine lungs as real data. Trained on abundant simulated data\nand fine-tuned with a small amount of real-world data, LUNA achieves robust\nperformance, demonstrated by an aeration estimation error of 9% in ex-vivo lung\nscans. We demonstrate the potential of reconstructing lung aeration maps from\nRF data, providing a foundation for improving lung ultrasound reproducibility\nand diagnostic utility.",
        "Many disciplines use standard examples for education and to share and compare\nresearch results. The examples are rich enough to study from multiple points of\nview; they are often called model problems. Software design lacks such a\ncommunity resource. We propose an activity for Designing 2025 in which\nparticipants improve some existing model problem descriptions and initiate new\nones -- with a focus on use in software design education, plus potential\nutility in research.",
        "Diffusion models, while increasingly adept at generating realistic images,\nare notably hindered by hallucinations -- unrealistic or incorrect features\ninconsistent with the trained data distribution. In this work, we propose\nAdaptive Attention Modulation (AAM), a novel approach to mitigate\nhallucinations by analyzing and modulating the self-attention mechanism in\ndiffusion models. We hypothesize that self-attention during early denoising\nsteps may inadvertently amplify or suppress features, contributing to\nhallucinations. To counter this, AAM introduces a temperature scaling mechanism\nwithin the softmax operation of the self-attention layers, dynamically\nmodulating the attention distribution during inference. Additionally, AAM\nemploys a masked perturbation technique to disrupt early-stage noise that may\notherwise propagate into later stages as hallucinations. Extensive experiments\ndemonstrate that AAM effectively reduces hallucinatory artifacts, enhancing\nboth the fidelity and reliability of generated images. For instance, the\nproposed approach improves the FID score by 20.8% and reduces the percentage of\nhallucinated images by 12.9% (in absolute terms) on the Hands dataset.",
        "Quantum compiling trying to approximate the target qubit gate by finding an\noptimal sequence (braid word) of basic braid operations is a fundamental\nproblem in quantum computing. We develop a genetic algorithm (GA) enhanced\nSolovay-Kitaev algorithm (SKA) to approximate single qubit gates with four\nbasic braid matrices of Fibonacci anyons. The GA-enhanced SKA demonstrates that\nthe algorithm performs strongly and can easily find the ideal braid word from\nan exponentially large space. The resulting precision of the approximate\nsingle-qubit quantum gate is superior to that of the Monte Carlo (MC) enhanced\nSKA, as well as comparable to that of the deep reinforcement learning (RL) for\nthe length of braid word greater than 25. The 2(3)-order approximation of\nGA-enhanced SKA for basic braiding length l0=50(30) leads to an optimal braid\nword at a distance of 5.9*10-7, which is sufficient for most cases of quantum\ncomputing. Our work provides an alternative approach to solving and optimizing\nquantum compilation of non-Abelian anyon quantum gates and is useful for\nrealizing topological quantum computation in the future.",
        "Existing precise pointer tracing methods introduce substantial runtime\noverhead to the program being traced and are applicable only at specific\nprogram execution points. We propose MappedTrace that leverages\ncompiler-generated read-only maps to accurately identify all pointers in any\ngiven snapshot of a program's execution state. The maps record the locations\nand types of pointers, allowing the tracer to precisely identify pointers\nwithout requiring the traced program to maintain bookkeeping data structures or\npoll at safe points, thereby reducing runtime overhead. By running the tracer\nfrom a different address space or machine, MappedTrace presents new\nopportunities to improve memory management techniques like memory leak\ndetection and enables novel use cases such as infinite memory abstraction for\nresource-constrained environments.",
        "Multi-modal understanding plays a crucial role in artificial intelligence by\nenabling models to jointly interpret inputs from different modalities. However,\nconventional approaches such as contrastive learning often struggle with\nmodality discrepancies, leading to potential misalignments. In this paper, we\npropose a novel class anchor alignment approach that leverages class\nprobability distributions for multi-modal representation learning. Our method,\nClass-anchor-ALigned generative Modeling (CALM), encodes class anchors as\nprompts to generate and align class probability distributions for each\nmodality, enabling more effective alignment. Furthermore, we introduce a\ncross-modal probabilistic variational autoencoder to model uncertainty in the\nalignment, enhancing the ability to capture deeper relationships between\nmodalities and data variations. Extensive experiments on four benchmark\ndatasets demonstrate that our approach significantly outperforms\nstate-of-the-art methods, especially in out-of-domain evaluations. This\nhighlights its superior generalization capabilities in multi-modal\nrepresentation learning.",
        "Ferrimagnets take the advantages of both ferromagnets and antiferromagnets\nmaking them promise for spintronic applications. Here we prepared ferrimagnetic\nMn4N thin films with high Curie temperature and investigated the crystalline\nstructure and magnetic properties affected by the Pd buffer layer. We\ndemonstrated that both crystalline quality and perpendicular magnetic\nanisotropy (PMA) of Mn4N thin films are enhanced significantly due to the\nrelaxation of tensile stress induced by the Pd buffer layer. We also\ndemonstrated a fast spin precession at room temperature, almost 100 GHz, in\nMn4N thin films. With the characteristics of high thermal stability, enhanced\nPMA by buffer layer and fast spin precession, Mn4N thin film is a promising\nmaterial for spintronic applications.",
        "We demonstrate a novel coherent photonic neural network using tunable\nphase-change-material-based couplers and neural architecture search. Compared\nto the MZI-based Clements network, our results indicate 85% reduction in the\nnetwork footprint while maintaining the accuracy.",
        "Photon-pair sources are useful for entanglement distribution. The most mature\nof these are spontaneous parametric downconversion (SPDC) sources, most of\nwhich achieve phasematching via engineering the domains in poled crystals or\nthe angle between the optic axis and the pump beam. For multi-channel\nentanglement distribution of photon pairs, where one photon is transmitted\nthrough free-space and the other photon is transmitted through fiber, it is\nbeneficial to use highly nondegenerate photon-pair sources. The currently\naccepted approach in such sources is quasi-phasematching. In this paper, a\nsimpler, more stable alternative is presented for producing highly\nnondegenerate photon pairs. A source of polarization-entangled photon pairs\nwith low temperature sensitivity based on noncritical phasematched (NCPM) SPDC\nin single-domain potassium titanyl phosphate (KTP) was demonstrated. Over a\ncrystal temperature range of $75^\\circ$C, the center wavelength of the idler\nphotons was observed to change by $10.8$nm while the average entanglement\nvisibility was maintained above $98\\%$. With the signal photons detected\nlocally, the idler photons were transmitted through 62km and 93km of deployed\ntelecom fibers with average raw visibilities of $98.2(1)\\%$ and $95.6(3)\\%$\nrespectively.",
        "This paper introduces V$^2$Edit, a novel training-free framework for\ninstruction-guided video and 3D scene editing. Addressing the critical\nchallenge of balancing original content preservation with editing task\nfulfillment, our approach employs a progressive strategy that decomposes\ncomplex editing tasks into a sequence of simpler subtasks. Each subtask is\ncontrolled through three key synergistic mechanisms: the initial noise, noise\nadded at each denoising step, and cross-attention maps between text prompts and\nvideo content. This ensures robust preservation of original video elements\nwhile effectively applying the desired edits. Beyond its native video editing\ncapability, we extend V$^2$Edit to 3D scene editing via a\n\"render-edit-reconstruct\" process, enabling high-quality, 3D-consistent edits\neven for tasks involving substantial geometric changes such as object\ninsertion. Extensive experiments demonstrate that our V$^2$Edit achieves\nhigh-quality and successful edits across various challenging video editing\ntasks and complex 3D scene editing tasks, thereby establishing state-of-the-art\nperformance in both domains.",
        "Most (or possibly all) massive stars reside in multiple systems. From stellar\nevolution models, numerous systems with an OB star coupled to a black hole\nwould be expected to exist. There have been several claimed detections of such\npairs in recent years and this is notably the case of HD96670. Using\nhigh-quality photometry and spectroscopy in the optical range, we revisited the\nHD96670 system. We also examined complementary X-ray observations to provide a\nbroader view of the system properties. The TESS light curves of HD96670 clearly\nshow eclipses, ruling out the black hole companion scenario. This does not mean\nthat the system is not of interest. Indeed, the combined analysis of\nphotometric and spectroscopic data indicates that the system most likely\nconsists of a O8.5 giant star paired with a stripped-star companion with a mass\nof ~4.5Msol, a radius of ~1Rsol, and a surface temperature of ~50kK. While\nseveral B+sdOB systems have been reported in the literature, this would be the\nfirst case of a Galactic system composed of an O star and a faint stripped\nstar. In addition, the system appears brighter and harder than normal OB stars\nin the X-ray range, albeit less so than for X-ray binaries. The high-energy\nobservations provide hints of phase-locked variations, as typically seen in\ncolliding wind systems. As a post-interaction system, HD96670 actually\nrepresents a key case for probing binary evolution, even if it is not\nultimately found to host a black hole."
      ]
    }
  },
  {
    "id":2411.07031,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"ChatGPT Hallucinates Non-existent Citations: Evidence from Economics",
    "start_abstract":"In this study, we generate prompts derived from every topic within the Journal of Economic Literature to assess abilities both GPT-3.5 and GPT-4 versions ChatGPT large language model (LLM) write about economic concepts. demonstrates considerable competency in offering general summaries but also cites non-existent references. More than 30% citations provided by version do not exist rate is only slightly reduced for version. Additionally, our findings suggest that reliability decreases as become more specific. We provide quantitative evidence errors output demonstrate importance LLM verification. JEL Codes: B4; O33; I2",
    "start_categories":[
      "q-fin.ST"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Accuracy of Chatbots in Citing Journal Articles"
      ],
      "abstract":[
        "This cross-sectional study quantifies the journal article citation error rate of an artificial intelligence chatbot."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Second bounded cohomology of knot quandles",
        "PatentLMM: Large Multimodal Model for Generating Descriptions for Patent\n  Figures",
        "Conditioning on Local Statistics for Scalable Heterogeneous Federated\n  Learning",
        "Vertex-Minimal Triangulation of Complexes with Homology",
        "Epistemic Logic Programs: Non-Ground and Counting Complexity",
        "AudioSpa: Spatializing Sound Events with Text",
        "An upper bound on the size of a code with $s$ distances",
        "MEXA-CTP: Mode Experts Cross-Attention for Clinical Trial Outcome\n  Prediction",
        "Non-homogeneous problem for the fractional wave equation with irregular\n  coefficients and data",
        "QORT-Former: Query-optimized Real-time Transformer for Understanding Two\n  Hands Manipulating Objects",
        "DGNN: A Neural PDE Solver Induced by Discontinuous Galerkin Methods",
        "Blockchain with proof of quantum work",
        "Modularity of preferential attachment graphs",
        "Harmonic Structure of the Brunel spectra",
        "A maximum concurrence criterion to investigate absolutely maximally\n  entangled states",
        "Non-singular weakly symmetric nilmanifolds",
        "Semialgebraic Neural Networks: From roots to representations",
        "ALMA observations of CH3COCH3 and the related species CH3CHO, CH3OH, and\n  C2H5CN in line-rich molecular cores",
        "Breakdown of time-independent methods in non-Hermitian scattering\n  systems",
        "Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models\n  for Wireless Networks",
        "Duality of Wave Modulation and Nanotwinning in Ni-Mn-Ga Martensite via\n  Long-Period Commensurate States",
        "Pruning-aware Loss Functions for STOI-Optimized Pruned Recurrent\n  Autoencoders for the Compression of the Stimulation Patterns of Cochlear\n  Implants at Zero Delay",
        "Combating Interference for Over-the-Air Federated Learning: A\n  Statistical Approach via RIS",
        "Fourier analysis of equivariant quantum cohomology",
        "Higgs-portal vector dark matter at a low reheating temperature",
        "Neural Guided Diffusion Bridges",
        "Semi-rPPG: Semi-Supervised Remote Physiological Measurement with\n  Curriculum Pseudo-Labeling",
        "On The Concurrence of Layer-wise Preconditioning Methods and Provable\n  Feature Learning",
        "IoT Firmware Version Identification Using Transfer Learning with Twin\n  Neural Networks"
      ],
      "abstract":[
        "In this paper, we explore the bounded cohomology of quandles and its\napplications to knot theory. We establish two key results that provide\nsufficient conditions for the infinite dimensionality of the second bounded\ncohomology of quandles. The first condition involves a subspace of homogeneous\ngroup quasimorphisms on the inner automorphism group of the quandle, whereas\nthe second condition concerns the vanishing of the stable commutator length on\na subgroup of this inner automorphism group. As topological applications, we\nshow that the second bounded cohomology of the quandle of any non-split link\nwhose link group is non-solvable as well as the quandle of any split link, is\ninfinite dimensional. From these results, we conclude that the second bounded\ncohomology of the knot quandle detects the unknot. On the algebraic side, we\nprove that the second bounded cohomology of a free product of quandles is\ninfinite dimensional if the inner automorphism group of at least one of the\nfree factors is amenable. This leads to the result that the second bounded\ncohomology of free quandles of rank greater than one, as well as their\ncanonical quotients, is infinite dimensional.",
        "Writing comprehensive and accurate descriptions of technical drawings in\npatent documents is crucial to effective knowledge sharing and enabling the\nreplication and protection of intellectual property. However, automation of\nthis task has been largely overlooked by the research community. To this end,\nwe introduce PatentDesc-355K, a novel large-scale dataset containing ~355K\npatent figures along with their brief and detailed textual descriptions\nextracted from more than 60K US patent documents. In addition, we propose\nPatentLMM - a novel multimodal large language model specifically tailored to\ngenerate high-quality descriptions of patent figures. Our proposed PatentLMM\ncomprises two key components: (i) PatentMME, a specialized multimodal vision\nencoder that captures the unique structural elements of patent figures, and\n(ii) PatentLLaMA, a domain-adapted version of LLaMA fine-tuned on a large\ncollection of patents. Extensive experiments demonstrate that training a vision\nencoder specifically designed for patent figures significantly boosts the\nperformance, generating coherent descriptions compared to fine-tuning\nsimilar-sized off-the-shelf multimodal models. PatentDesc-355K and PatentLMM\npave the way for automating the understanding of patent figures, enabling\nefficient knowledge sharing and faster drafting of patent documents. We make\nthe code and data publicly available.",
        "Federated learning is a distributed machine learning approach where multiple\nclients collaboratively train a model without sharing their local data, which\ncontributes to preserving privacy. A challenge in federated learning is\nmanaging heterogeneous data distributions across clients, which can hinder\nmodel convergence and performance due to the need for the global model to\ngeneralize well across diverse local datasets. We propose to use local\ncharacteristic statistics, by which we mean some statistical properties\ncalculated independently by each client using only their local training\ndataset. These statistics, such as means, covariances, and higher moments, are\nused to capture the characteristics of the local data distribution. They are\nnot shared with other clients or a central node. During training, these local\nstatistics help the model learn how to condition on the local data\ndistribution, and during inference, they guide the client's predictions. Our\nexperiments show that this approach allows for efficient handling of\nheterogeneous data across the federation, has favorable scaling compared to\napproaches that directly try to identify peer nodes that share distribution\ncharacteristics, and maintains privacy as no additional information needs to be\ncommunicated.",
        "For a given pair of numbers $(d,k)$, we establish a lower bound on the number\nof vertices in pure $d$-dimensional simplicial complexes with non-trivial\nhomology in dimension $k$, and prove that this bound is tight. Furthermore, we\nsolve the problem under the additional constraint of strong connectivity with\nrespect to any intermediate dimension.",
        "Answer Set Programming (ASP) is a prominent problem-modeling and solving\nframework, whose solutions are called answer sets. Epistemic logic programs\n(ELP) extend ASP to reason about all or some answer sets. Solutions to an ELP\ncan be seen as consequences over multiple collections of answer sets, known as\nworld views. While the complexity of propositional programs is well studied,\nthe non-ground case remains open. This paper establishes the complexity of\nnon-ground ELPs. We provide a comprehensive picture for well-known program\nfragments, which turns out to be complete for the class NEXPTIME with access to\noracles up to \\Sigma^P_2. In the quantitative setting, we establish complexity\nresults for counting complexity beyond #EXP. To mitigate high complexity, we\nestablish results in case of bounded predicate arity, reaching up to the fourth\nlevel of the polynomial hierarchy. Finally, we provide ETH-tight runtime\nresults for the parameter treewidth, which has applications in quantitative\nreasoning, where we reason on (marginal) probabilities of epistemic literals.",
        "Text-to-audio (TTA) systems have recently demonstrated strong performance in\nsynthesizing monaural audio from text. However, the task of generating binaural\nspatial audio from text, which provides a more immersive auditory experience by\nincorporating the sense of spatiality, have not been explored yet. In this\nwork, we introduce text-guided binaural audio generation. As an early effort,\nwe focus on the scenario where a monaural reference audio is given\nadditionally. The core problem is to associate specific sound events with their\ndirections, thereby creating binaural spatial audio. The challenge lies in the\ncomplexity of textual descriptions and the limited availability of\nsingle-source sound event datasets. To address this, we propose AudioSpa, an\nend-to-end model that applies large language models to process both acoustic\nand textual information. We employ fusion multi-head attention (FMHA) to\nintegrate text tokens, which enhances the generation capability of the\nmultimodal learning. Additionally, we propose a binaural source localization\nmodel to assess the quality of the generated audio. Finally, we design a data\naugmentation strategy to generate diverse datasets, which enables the model to\nspatialize sound events across various spatial positions. Experimental results\ndemonstrate that our model is able to put sounds at the specified locations\naccurately. It achieves competitive performance in both localization accuracy\nand signal distortion. Our demonstrations are available at\nhttps:\/\/linfeng-feng.github.io\/AudioSpa-demo.",
        "Let $C$ be a binary code of length $n$ with distances $0<d_1<\\cdots<d_s\\le\nn$. In this note we prove a general upper bound on the size of $C$ without any\nrestriction on the distances $d_i$. The bound is asymptotically optimal.",
        "Clinical trials are the gold standard for assessing the effectiveness and\nsafety of drugs for treating diseases. Given the vast design space of drug\nmolecules, elevated financial cost, and multi-year timeline of these trials,\nresearch on clinical trial outcome prediction has gained immense traction.\nAccurate predictions must leverage data of diverse modes such as drug\nmolecules, target diseases, and eligibility criteria to infer successes and\nfailures. Previous Deep Learning approaches for this task, such as HINT, often\nrequire wet lab data from synthesized molecules and\/or rely on prior knowledge\nto encode interactions as part of the model architecture. To address these\nlimitations, we propose a light-weight attention-based model, MEXA-CTP, to\nintegrate readily-available multi-modal data and generate effective\nrepresentations via specialized modules dubbed \"mode experts\", while avoiding\nhuman biases in model design. We optimize MEXA-CTP with the Cauchy loss to\ncapture relevant interactions across modes. Our experiments on the Trial\nOutcome Prediction (TOP) benchmark demonstrate that MEXA-CTP improves upon\nexisting approaches by, respectively, up to 11.3% in F1 score, 12.2% in PR-AUC,\nand 2.5% in ROC-AUC, compared to HINT. Ablation studies are provided to\nquantify the effectiveness of each component in our proposed method.",
        "In this paper, we consider the Cauchy problem for a non-homogeneous wave\nequation generated by the fractional Laplacian and involving different kinds of\nlower order terms. We allow the equation coefficients and data to be of\ndistributional type or less regular, having in mind the Dirac delta function\nand its powers, and we prove that the problem is well-posed in the sense of the\nconcept of very weak solutions. Moreover, we prove the uniqueness in an\nappropriate sense and the coherence of the very weak solution concept with\nclassical theory.",
        "Significant advancements have been achieved in the realm of understanding\nposes and interactions of two hands manipulating an object. The emergence of\naugmented reality (AR) and virtual reality (VR) technologies has heightened the\ndemand for real-time performance in these applications. However, current\nstate-of-the-art models often exhibit promising results at the expense of\nsubstantial computational overhead. In this paper, we present a query-optimized\nreal-time Transformer (QORT-Former), the first Transformer-based real-time\nframework for 3D pose estimation of two hands and an object. We first limit the\nnumber of queries and decoders to meet the efficiency requirement. Given\nlimited number of queries and decoders, we propose to optimize queries which\nare taken as input to the Transformer decoder, to secure better accuracy: (1)\nwe propose to divide queries into three types (a left hand query, a right hand\nquery and an object query) and enhance query features (2) by using the contact\ninformation between hands and an object and (3) by using three-step update of\nenhanced image and query features with respect to one another. With proposed\nmethods, we achieved real-time pose estimation performance using just 108\nqueries and 1 decoder (53.5 FPS on an RTX 3090TI GPU). Surpassing\nstate-of-the-art results on the H2O dataset by 17.6% (left hand), 22.8% (right\nhand), and 27.2% (object), as well as on the FPHA dataset by 5.3% (right hand)\nand 10.4% (object), our method excels in accuracy. Additionally, it sets the\nstate-of-the-art in interaction recognition, maintaining real-time efficiency\nwith an off-the-shelf action recognition module.",
        "We propose a general framework for the Discontinuous Galerkin-induced Neural\nNetwork (DGNN), inspired by the Interior Penalty Discontinuous Galerkin Method\n(IPDGM). In this approach, the trial space consists of piecewise neural network\nspace defined over the computational domain, while the test function space is\ncomposed of piecewise polynomials. We demonstrate the advantages of DGNN in\nterms of accuracy and training efficiency across several numerical examples,\nincluding stationary and time-dependent problems. Specifically, DGNN easily\nhandles high perturbations, discontinuous solutions, and complex geometric\ndomains.",
        "We propose a blockchain architecture in which mining requires a quantum\ncomputer. The consensus mechanism is based on proof of quantum work, a\nquantum-enhanced alternative to traditional proof of work that leverages\nquantum supremacy to make mining intractable for classical computers. We have\nrefined the blockchain framework to incorporate the probabilistic nature of\nquantum mechanics, ensuring stability against sampling errors and hardware\ninaccuracies. To validate our approach, we implemented a prototype blockchain\non four D-Wave$^{\\rm TM}$ quantum annealing processors geographically\ndistributed within North America, demonstrating stable operation across\nhundreds of thousands of quantum hashing operations. Our experimental protocol\nfollows the same approach used in the recent demonstration of quantum supremacy\n[1], ensuring that classical computers cannot efficiently perform the same\ncomputation task. By replacing classical machines with quantum systems for\nmining, it is possible to significantly reduce the energy consumption and\nenvironmental impact traditionally associated with blockchain mining. Beyond\nserving as a proof of concept for a meaningful application of quantum\ncomputing, this work highlights the potential for other near-term quantum\ncomputing applications using existing technology.",
        "Modularity is a graph parameter measuring how clearly the set of graph\nvertices may be partitioned into subsets of high edge density. It indicates the\npresence of community structure in the graph. We study its value for a random\npreferential attachment model $G_n^h$ introduced by Barab\\'asi and Albert in\n1999. A graph $G_n^h$ is created from some finite starting graph by adding new\nvertices one by one. A new vertex always connects to $h\\geq1$ already existing\nvertices and those are chosen with probability proportional to their current\ndegrees. We prove that modularity of $G_n^h$ is with high probability upper\nbounded by a function tending to $0$ with $h$ tending to infinity. This\nresolves the conjecture of Prokhorenkova, Pralat and Raigorodskii from 2016. As\na byproduct we obtain novel concentration results for the volume and the edge\ndensity parameters of subsets of $G_n^h$.",
        "Electromagnetic emissions, known as Brunel radiations, are produced in\nplasmas through the coupling between the free electron density and ultrafast\nionizing laser pulses. The radiation spectrum generated in laser-gas\ninteractions is here investigated from a local current model for laser drivers\nwith two frequency components - or \"colors\" - being not necessarily integers of\none another. We provide a general description of this spectrum by deriving\nanalytically the convolution product of the Fourier transforms of the electron\ndensity and of the laser electric field. Our analysis reveals that the only\nknowledge of the optical field extrema in time domain is sufficient to\nreproduce faithfully the numerically-computed Brunel spectrum and justify the\nemergence of various resonance frequencies. The classical combination of two\nlaser harmonics, i.e., a fundamental and its second harmonic, is also\naddressed.",
        "We propose a straightforward method to determine the maximal entanglement of\npure states using the criterion of maximal I-concurrence, a measure of\nentanglement. The square of concurrence for a bipartition $X|X^\\prime$ of a\npure state is defined as $E^2_{X| X ^\\prime}=2[1-tr({\\rho_X}^2)]$. From this,\nwe can infer that the concurrence $E_{X| X ^\\prime}$ reaches its maximum when\n$tr({\\rho_X}^2)$ is minimized. Using this approach, we identify numerous\nAbsolutely Maximally Entangled (AME) pure states that exhibit maximal\nentanglement across all possible bipartitions. Conditions are derived for pure\nstates to achieve maximal mixedness in all bipartitions, revealing that any\npure state with an odd number of subsystem coefficients does not meet the AME\ncriterion. Furthermore, we obtain equal maximal multipartite entangled pure\nstates across all bipartitions using our maximal concurrence criterion.",
        "A Riemannian manifold $M$ is called weakly symmetric if any two points in $M$\ncan be interchanged by an isometry. The compact ones have been well understood,\nand the main remaining case is that of 2-step nilpotent Lie groups. We give a\ncomplete classification of simply connected non-singular weakly symmetric\nnilmanifolds. Besides previously known examples, there are new families with\n3-dimensional center, and a one-parameter family of dimensions 14. The\nclassification is based on the authors classification of non-singular 2-step\nnilpotent Lie groups for which every geodesic is the image of a one parameter\ngroup of isometries.",
        "Many numerical algorithms in scientific computing -- particularly in areas\nlike numerical linear algebra, PDE simulation, and inverse problems -- produce\noutputs that can be represented by semialgebraic functions; that is, the graph\nof the computed function can be described by finitely many polynomial\nequalities and inequalities. In this work, we introduce Semialgebraic Neural\nNetworks (SANNs), a neural network architecture capable of representing any\nbounded semialgebraic function, and computing such functions up to the accuracy\nof a numerical ODE solver chosen by the programmer. Conceptually, we encode the\ngraph of the learned function as the kernel of a piecewise polynomial selected\nfrom a class of functions whose roots can be evaluated using a particular\nhomotopy continuation method. We show by construction that the SANN\narchitecture is able to execute this continuation method, thus evaluating the\nlearned semialgebraic function. Furthermore, the architecture can exactly\nrepresent even discontinuous semialgebraic functions by executing a\ncontinuation method on each connected component of the target function. Lastly,\nwe provide example applications of these networks and show they can be trained\nwith traditional deep-learning techniques.",
        "Context. Acetone (CH3COCH3) is a carbonyl-bearing complex organic molecule,\nyet interstellar observations of acetone remain limited. Studying the formation\nand distribution of CH3COCH3 in the interstellar medium can provide valuable\ninsights into prebiotic chemistry and the evolution of interstellar molecules.\n  Aims. We explore the spatial distribution of CH3COCH3 and its correlation\nwith the O-bearing molecules acetaldehyde (CH3CHO) and methanol (CH3OH), as\nwell as the N-bearing molecule ethyl cyanide (C2H5CN), in massive protostellar\nclumps.\n  Methods. We observed 11 massive protostellar clumps using ALMA at 345 GHz,\nwith an angular resolution of 0.7''-1.0''. Spectral line transitions were\nidentified using the eXtended CASA Line Analysis Software Suite. We constructed\nintegrated intensity maps of CH3COCH3, CH3CHO, CH3OH, and C2H5CN and derived\ntheir rotation temperatures, column densities, and abundances under the\nassumption of local thermodynamic equilibrium.\n  Results. CH3COCH3 is detected in 16 line-rich cores from 9 massive\nprotostellar clumps: 12 high-mass cores, 3 intermediate-mass cores, and 1\nlow-mass core. CH3CHO and CH3OH are also detected in all 16 cores, while C2H5CN\nis detected in 15. The integrated intensity maps reveal similar spatial\ndistributions for CH3COCH3, CH3CHO, CH3OH, and C2H5CN. The line emission peaks\nof all four molecules coincide with the continuum emission peaks in regions\nwithout ultracompact HII regions. Significant correlations are observed in the\nabundances of these molecules, which also exhibit similar average temperatures.\n  Conclusions. Our observational results, supported by chemical models, suggest\nthat CH3COCH3, CH3CHO, and CH3OH originate from the same gas. The observed\ntemperatures and abundances of CH3COCH3 are consistent with model predictions\ninvolving grain surface chemistry.",
        "Time-independent methods, such as the transfer matrix method, are widely used\nto analyze the scattering properties of non-Hermitian systems. However, we\ndemonstrate that these methods become invalid when the scattering matrix\n(S-matrix) exhibits poles in the first quadrant of the complex wave-number\nplane, indicating the presence of time-growing bound states within the system.\nThe breakdown of time-independent approaches is attributed to their inherent\nomission of these bound states. We illustrate this using tight-binding models\nwhere non-Hermiticity is introduced through imaginary on-site potentials or\nasymmetric hopping terms. In all the models considered, parameter regimes exist\nwhere time-independent methods fail. Our findings highlight the critical\nimportance of examining the distribution of S-matrix poles when applying\ntime-independent methods to non-Hermitian scattering systems. Inappropriate\napplication of these methods can lead to unphysical results and erroneous\nconclusions.",
        "This paper proposes a novel framework for real-time adaptive-bitrate video\nstreaming by integrating latent diffusion models (LDMs) within the FFmpeg\ntechniques. This solution addresses the challenges of high bandwidth usage,\nstorage inefficiencies, and quality of experience (QoE) degradation associated\nwith traditional constant bitrate streaming (CBS) and adaptive bitrate\nstreaming (ABS). The proposed approach leverages LDMs to compress I-frames into\na latent space, offering significant storage and semantic transmission savings\nwithout sacrificing high visual quality. While it keeps B-frames and P-frames\nas adjustment metadata to ensure efficient video reconstruction at the user\nside, the proposed framework is complemented with the most state-of-the-art\ndenoising and video frame interpolation (VFI) techniques. These techniques\nmitigate semantic ambiguity and restore temporal coherence between frames, even\nin noisy wireless communication environments. Experimental results demonstrate\nthe proposed method achieves high-quality video streaming with optimized\nbandwidth usage, outperforming state-of-the-art solutions in terms of QoE and\nresource efficiency. This work opens new possibilities for scalable real-time\nvideo streaming in 5G and future post-5G networks.",
        "Understanding the crystal structure of magnetic shape memory alloys is\ncrucial for elucidating their martensite twin boundary supermobility and\nrelated functionalities. This study analyzes and discusses the structure of\nmartensitic single crystals of Ni50.0Mn27.7Ga22.3 and Ni50.0Mn28.1Ga21.9.\nNeutron and X-ray diffraction reveal an anharmonic, incommensurate five-layer\nmodulation that evolves with temperature. This evolution gives rise to two key\nmicrostructural features: i) periodic nanodomains, identified as emerging\na\/b-nanotwins, and ii) long-period commensurate structures, such as the 34O,\n24O, and 14O states, whose orthorhombic unit cells inherently realize\na\/b-nanotwins. Ab initio calculations show that these long-period structures\nare energetically favorable, leading to a lock-in transition. In the alloys\nstudied, the 24O state is the locked-in phase at low temperatures, whereas\nliterature data indicate that Ni50Mn25Ga25 (exact Ni2MnGa stoichiometry)\nevolves toward the 14O phase. Crucially, these results unify two seemingly\ncontrasting structural descriptions -- wave-like modulation and discrete\nnanotwinning -- thereby establishing a foundation for a deeper understanding of\nthe crystal-structure-functionality relationship in magnetic shape memory\nalloys.",
        "Cochlear implants (CIs) are surgically implanted hearing devices, which allow\nto restore a sense of hearing in people suffering from profound hearing loss.\nWireless streaming of audio from external devices to CI signal processors has\nbecome common place. Specialized compression based on the stimulation patterns\nof a CI by deep recurrent autoencoders can decrease the power consumption in\nsuch a wireless streaming application through bit-rate reduction at zero\nlatency.\n  While previous research achieved considerable bit-rate reductions, model\nsizes were ignored, which can be of crucial importance in hearing-aids due to\ntheir limited computational resources. This work investigates maximizing\nobjective speech intelligibility of the coded stimulation patterns of deep\nrecurrent autoencoders while minimizing model size. For this purpose, a\npruning-aware loss is proposed, which captures the impact of pruning during\ntraining. This training with a pruning-aware loss is compared to conventional\nmagnitude-informed pruning and is found to yield considerable improvements in\nobjective intelligibility, especially at higher pruning rates. After\nfine-tuning, little to no degradation of objective intelligibility is observed\nup to a pruning rate of about 55\\,\\%. The proposed pruning-aware loss yields\nsubstantial gains in objective speech intelligibility scores after pruning\ncompared to the magnitude-informed baseline for pruning rates above 45\\,\\%.",
        "Over-the-air computation (AirComp) integrates analog communication with\ntask-oriented computation, serving as a key enabling technique for\ncommunication-efficient federated learning (FL) over wireless networks.\nHowever, owing to its analog characteristics, AirComp-enabled FL (AirFL) is\nvulnerable to both unintentional and intentional interference. In this paper,\nwe aim to attain robustness in AirComp aggregation against interference via\nreconfigurable intelligent surface (RIS) technology to artificially reconstruct\nwireless environments. Concretely, we establish performance objectives tailored\nfor interference suppression in wireless FL systems, aiming to achieve unbiased\ngradient estimation and reduce its mean square error (MSE). Oriented at these\nobjectives, we introduce the concept of phase-manipulated favorable propagation\nand channel hardening for AirFL, which relies on the adjustment of RIS phase\nshifts to realize statistical interference elimination and reduce the error\nvariance of gradient estimation. Building upon this concept, we propose two\nrobust aggregation schemes of power control and RIS phase shifts design, both\nensuring unbiased gradient estimation in the presence of interference.\nTheoretical analysis of the MSE and FL convergence affirms the\nanti-interference capability of the proposed schemes. It is observed that\ncomputation and interference errors diminish by an order of\n$\\mathcal{O}\\left(\\frac{1}{N}\\right)$ where $N$ is the number of RIS elements,\nand the ideal convergence rate without interference can be asymptotically\nachieved by increasing $N$. Numerical results confirm the analytical results\nand validate the superior performance of the proposed schemes over existing\nbaselines.",
        "Equivariant quantum cohomology possesses the structure of a difference module\nby shift operators (Seidel representation) of equivariant parameters. Teleman's\nconjecture suggests that shift operators and equivariant parameters acting on\nQH_T(X) should be identified, respectively, with the Novikov variables and the\nquantum connection of the GIT quotient X\/\/T. This can be interpreted as a form\nof Fourier duality between equivariant quantum cohomology (D-module) of X and\nquantum cohomology (D-module) of the GIT quotient X\/\/T.\n  We introduce the notion of \"quantum volume,\" derived from Givental's path\nintegral over the Floer fundamental cycle, and present a conjectural Fourier\nduality relationship between the T-equivariant quantum volume of X and the\nquantum volume of X\/\/T. We also explore the \"reduction conjecture,\" developed\nin collaboration with Fumihiko Sanda, which expresses the I-function of X\/\/T as\na discrete Fourier transform of the equivariant J-function of X. Furthermore,\nwe demonstrate how to use Fourier analysis of equivariant quantum cohomology to\nobserve toric mirror symmetry and prove a decomposition of quantum cohomology\nD-modules of projective bundles or blowups.",
        "We study vector dark matter (DM) production with Higgs-portal type\ninteractions in the scenarios with a low reheating temperature which can be\nrealized by a prolonged decay of the inflaton after inflation. We take the\nreheating temperature to be large enough to match the observations in Standard\nCosmology such as Big Bang Nucleosynthesis but small enough below the DM mass\nfor the DM production. We analyze the impact of the model parameters including\nthe extra gauge coupling and the reheating temperature on the DM relic density,\ncollider bounds and DM direct and indirect detection experiments. Our results\nreveal a strong correlation between the DM mass ($M_{W_D}$) and the reheating\ntemperature ($T_R$) with ratio of around $T_R\/M_{W_D} \\sim 0.1$ to obtain\ncorrect DM density for detectable interaction strength. The decay processes are\ngenerally subdominant for the DM production but they can be important when\nkinematically allowed and the DM mass is close to half of the Higgses mass. The\nDM production with DM masses below 100 GeV is driven primarily by the\nscatterings of the SM fermions and Higgses decay whereas the case with higher\nDM masses is achieved mainly due to the Higgses scatterings. The enhanced\ncoupling for the strong freeze-in in our framework enables potential detection\nprospects in direct and indirect detections and collider experiments. The\nparameter space of the model has already been explored partly by the current\ndirect detection experiments and it can be explored further by future\nexperiments such as Darwin. On the other hand, the indirect detection\nexperiments in the current and near future are not sensitive enough to test our\nmodel.",
        "We propose a novel method for simulating conditioned diffusion processes\n(diffusion bridges) in Euclidean spaces. By training a neural network to\napproximate bridge dynamics, our approach eliminates the need for\ncomputationally intensive Markov Chain Monte Carlo (MCMC) methods or\nreverse-process modeling. Compared to existing methods, it offers greater\nrobustness across various diffusion specifications and conditioning scenarios.\nThis applies in particular to rare events and multimodal distributions, which\npose challenges for score-learning- and MCMC-based approaches. We propose a\nflexible variational family for approximating the diffusion bridge path measure\nwhich is partially specified by a neural network. Once trained, it enables\nefficient independent sampling at a cost comparable to sampling the\nunconditioned (forward) process.",
        "Remote Photoplethysmography (rPPG) is a promising technique to monitor\nphysiological signals such as heart rate from facial videos. However, the\nlabeled facial videos in this research are challenging to collect. Current rPPG\nresearch is mainly based on several small public datasets collected in simple\nenvironments, which limits the generalization and scale of the AI models.\nSemi-supervised methods that leverage a small amount of labeled data and\nabundant unlabeled data can fill this gap for rPPG learning. In this study, a\nnovel semi-supervised learning method named Semi-rPPG that combines curriculum\npseudo-labeling and consistency regularization is proposed to extract intrinsic\nphysiological features from unlabelled data without impairing the model from\nnoises. Specifically, a curriculum pseudo-labeling strategy with\nsignal-to-noise ratio (SNR) criteria is proposed to annotate the unlabelled\ndata while adaptively filtering out the low-quality unlabelled data. Besides, a\nnovel consistency regularization term for quasi-periodic signals is proposed\nthrough weak and strong augmented clips. To benefit the research on\nsemi-supervised rPPG measurement, we establish a novel semi-supervised\nbenchmark for rPPG learning through intra-dataset and cross-dataset evaluation\non four public datasets. The proposed Semi-rPPG method achieves the best\nresults compared with three classical semi-supervised methods under different\nprotocols. Ablation studies are conducted to prove the effectiveness of the\nproposed methods.",
        "Layer-wise preconditioning methods are a family of memory-efficient\noptimization algorithms that introduce preconditioners per axis of each layer's\nweight tensors. These methods have seen a recent resurgence, demonstrating\nimpressive performance relative to entry-wise (\"diagonal\") preconditioning\nmethods such as Adam(W) on a wide range of neural network optimization tasks.\nComplementary to their practical performance, we demonstrate that layer-wise\npreconditioning methods are provably necessary from a statistical perspective.\nTo showcase this, we consider two prototypical models, linear representation\nlearning and single-index learning, which are widely used to study how typical\nalgorithms efficiently learn useful features to enable generalization. In these\nproblems, we show SGD is a suboptimal feature learner when extending beyond\nideal isotropic inputs $\\mathbf{x} \\sim \\mathsf{N}(\\mathbf{0}, \\mathbf{I})$ and\nwell-conditioned settings typically assumed in prior work. We demonstrate\ntheoretically and numerically that this suboptimality is fundamental, and that\nlayer-wise preconditioning emerges naturally as the solution. We further show\nthat standard tools like Adam preconditioning and batch-norm only mildly\nmitigate these issues, supporting the unique benefits of layer-wise\npreconditioning.",
        "As the Internet of Things (IoT) becomes more embedded within our daily lives,\nthere is growing concern about the risk `smart' devices pose to network\nsecurity. To address this, one avenue of research has focused on automated IoT\ndevice identification. Research has however largely neglected the\nidentification of IoT device firmware versions. There is strong evidence that\nIoT security relies on devices being on the latest version patched for known\nvulnerabilities. Identifying when a device has updated (has changed version) or\nnot (is on a stable version) is therefore useful for IoT security. Version\nidentification involves challenges beyond those for identifying the model,\ntype, and manufacturer of IoT devices, and traditional machine learning\nalgorithms are ill-suited for effective version identification due to being\nlimited by the availability of data for training. In this paper, we introduce\nan effective technique for identifying IoT device versions based on transfer\nlearning. This technique relies on the idea that we can use a Twin Neural\nNetwork (TNN) - trained at distinguishing devices - to detect differences\nbetween a device on different versions. This facilitates real-world\nimplementation by requiring relatively little training data. We extract\nstatistical features from on-wire packet flows, convert these features into\ngreyscale images, pass these images into a TNN, and determine version changes\nbased on the Hedges' g effect size of the similarity scores. This allows us to\ndetect the subtle changes present in on-wire traffic when a device changes\nversion. To evaluate our technique, we set up a lab containing 12 IoT devices\nand recorded their on-wire packet captures for 11 days across multiple firmware\nversions. For testing data held out from training, our best performing model is\nshown to be 95.83% and 84.38% accurate at identifying stable versions and\nversion changes respectively."
      ]
    }
  },
  {
    "id":2411.07031,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Accuracy of Chatbots in Citing Journal Articles",
    "start_abstract":"This cross-sectional study quantifies the journal article citation error rate of an artificial intelligence chatbot.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "ChatGPT Hallucinates Non-existent Citations: Evidence from Economics"
      ],
      "abstract":[
        "In this study, we generate prompts derived from every topic within the Journal of Economic Literature to assess abilities both GPT-3.5 and GPT-4 versions ChatGPT large language model (LLM) write about economic concepts. demonstrates considerable competency in offering general summaries but also cites non-existent references. More than 30% citations provided by version do not exist rate is only slightly reduced for version. Additionally, our findings suggest that reliability decreases as become more specific. We provide quantitative evidence errors output demonstrate importance LLM verification. JEL Codes: B4; O33; I2"
      ],
      "categories":[
        "q-fin.ST"
      ]
    },
    "list":{
      "title":[
        "Existence of optimal controls for stochastic partial differential\n  equations with fully local monotone coefficients",
        "Spin-polarized STM measurement scheme for quantum geometric tensor",
        "Mechanism of tulip flame formation in highly reactive and low reactive\n  gas mixtures",
        "Poincar\\'{e} sphere engineering of dynamical ferroelectric topological\n  solitons",
        "An Optimal Transport approach to arbitrage correction: Application to\n  volatility Stress-Tests",
        "Comment on: \"2005 VL1 is not Venera-2\"",
        "Balancing Flexibility and Interpretability: A Conditional Linear Model\n  Estimation via Random Forest",
        "Numerical simulation of a fine-tunable F\\\"oppl-von K\\'arm\\'an model for\n  foldable and bilayer plates",
        "Bring the noise: exact inference from noisy simulations in collider\n  physics",
        "Quantum Hamiltonian Descent for Non-smooth Optimization",
        "On the spatial distribution of luminous blue variables in the M33 galaxy",
        "The existence of pyramidal Steiner triple systems over abelian groups",
        "Change Point Detection for Random Objects with Possibly Periodic\n  Behavior",
        "Rotating and non-linear magnetic-charged black hole with an anisotropic\n  matter field",
        "SUSY transformation as the coupler of non-interacting systems",
        "Multisymplectic structure of nonintegrable Henon-Heiles system",
        "Tunable Kernel-Nulling interferometry for direct exoplanet detection",
        "$\\Omega_{bbb}\\Omega_{bbb}\\Omega_{bbb}$ tribaryons",
        "ALMA observations of massive clouds in the central molecular zone: slim\n  filaments tracing parsec-scale shocks",
        "Reconstruction of proton relative stopping power with a granular\n  calorimeter detector model",
        "Correlations drive the attosecond response of strongly-correlated\n  insulators",
        "Nesting is not Contracting",
        "Interpreting the $X(2370)$ and $X(2600)$ as light tetraquark states",
        "Mean Field Games with Reflected Dynamics",
        "Two repeated quasi-periodic oscillations in the FSRQ S5 1044+71 observed\n  by TESS",
        "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
        "The three obdurate conjectures of differential geometry",
        "Photonic Negative Differential Thermal Conductance Enabled by NIS\n  Junctions",
        "Non-dispersive graded impedance acoustic lenses"
      ],
      "abstract":[
        "This paper deals with a stochastic optimal feedback control problem for the\ncontrolled stochastic partial differential equations. More precisely, we\nestablish the existence of stochastic optimal feedback control for the\ncontrolled stochastic partial differential equations with fully monotone\ncoefficients by a minimizing sequence for the control problem. Using the\nFaedo-Galerkin approximations, the uniform estimates and the tightness in some\nappropriate space for the Faedo-Galerkin approximating solution can be obtain\nto prove the well-posedness of the controlled stochastic partial differential\nequations with fully monotone coefficients. The results obtained in the present\npaper may be applied to various types of controlled stochastic partial\ndifferential equations, such as the controlled stochastic convection diffusion\nequation.",
        "Quantum geometric tensor (QGT) reflects the geometry of the eigenstates of a\nsystem's Hamiltonian. The full characterization of QGT is essential for various\nquantum systems. However, it is challenging to characterize the QGT of the\nsolid-state systems. Here we present a scheme by using spin-polarized STM to\nmeasure QGT of two-dimensional solid-state systems, in which the spin texture\nis extracted from geometric amplitudes of Friedel oscillations induced by the\nintentionally introduced magnetic impurity and then the QGT is derived from the\nmomentum differential of spin texture. The surface states of topological\ninsulator (TISS), as a model spin system, is promising to demonstrate the\nscheme. In a TI slab, the gapped TISS host finite quantum metric and Berry\ncurvature as the symmetric real part and the antisymmetric imaginary part of\nQGT, respectively. Thus, a detailed calculations guide the use of the developed\nscheme to measure the QGT of gapped TISS with or without an external in-plane\nmagnetic field. This study provides a feasible scheme for measuring QGT of\ntwo-dimensional solid-state systems, and hints at the great potential of the\ninformation extraction from the geometric amplitudes of STM and other\nmeasurement.",
        "The early stages of flame dynamics and the development and evolution of tulip\nflames in closed tubes of various aspect ratios and in a semi-open tube are\nstudied by solving the fully compressible reactive Navier-Stokes equations\nusing a high-order numerical method coupled to detailed chemical models in a\nstoichiometric hydrogen\/air and methane\/air mixtures. The use of adaptive mesh\nrefinement provides adequate resolution of the flame reaction zone, pressure\nwaves, and flame-pressure wave interactions. The purpose of this study is to\ngain a deeper insight into the influence of chemical kinetics on the combustion\nregimes leading to the formation of a tulip flame and its subsequent evolution.\nThe simulations highlight the effect of flame thickness, flame velocity, and\nreaction order on the intensity of the rarefaction wave generated by the flame\nduring the deceleration phase, which is the principal physical mechanism of\ntulip flame formation. The obtained results explain most of the experimentally\nobserved features of tulip flame formation, e.g. faster tulip flame formation\nwith deeper tulip shape for faster flames compared to slower flames.",
        "Geometric representation lays the basis for understanding and flexible tuning\nof topological transitions in many physical systems. An example is given by the\nPoincar\\'{e} sphere (PS) that provides an intuitive and continuous\nparameterization of the spin or orbital angular momentum (OAM) light states.\nHere, we apply this geometric construction to understand and continuously\nencode dynamical topologies of ferroelectric solitons driven by OAM-tunable\nlight. We show that: (1) PS engineering enables controlled creation of dynamic\npolar antiskyrmions that are rarely found in ferroelectrics; (2) We link such\ntopological transition to the tuning of the light beam as a ``knob'' from OAM\n(PS pole) to non-OAM (PS equator) modes; (3) Intermediate OAM-state structured\nlight results in new ferroelectric topologies of temporally hybrid\nskyrmion-antiskyrmion states. Our study offers new approaches of robust control\nand flexible tuning of topologies of matter using structured light.",
        "We present a method based on optimal transport to remove arbitrage\nopportunities within a finite set of option prices. The method is notably\nintended for regulatory stress-tests, which impose to apply important local\ndistortions to implied volatility surfaces. The resulting stressed option\nprices are naturally associated to a family of signed marginal measures: we\nformulate the process of removing arbitrage as a projection onto the subset of\nmartingale measures with respect to a Wasserstein metric in the space of signed\nmeasures. We show how this projection problem can be recast as an optimal\ntransport problem; in view of the numerical solution, we apply an entropic\nregularization technique. For the regularized problem, we derive a strong\nduality formula, show convergence results as the regularization parameter\napproaches zero, and formulate a multi-constrained Sinkhorn algorithm, where\neach iteration involves, at worse, finding the root of an explicit scalar\nfunction. The convergence of this algorithm is also established. We compare our\nmethod with the existing approach by [Cohen, Reisinger and Wang, Appl.\\ Math.\\\nFin.\\ 2020] across various scenarios and test cases.",
        "I show that the small differences between the orbital parameters of the dark\ncomet 2005 VL1 and the Venera 2 spacecraft (reported in arXiv:2503.07972) are\nof the magnitude expected from gravitational deflection by a close encounter of\nVenera 2 with Venus.",
        "Traditional parametric econometric models often rely on rigid functional\nforms, while nonparametric techniques, despite their flexibility, frequently\nlack interpretability. This paper proposes a parsimonious alternative by\nmodeling the outcome $Y$ as a linear function of a vector of variables of\ninterest $\\boldsymbol{X}$, conditional on additional covariates\n$\\boldsymbol{Z}$. Specifically, the conditional expectation is expressed as\n$\\mathbb{E}[Y|\\boldsymbol{X},\\boldsymbol{Z}]=\\boldsymbol{X}^{T}\\boldsymbol{\\beta}(\\boldsymbol{Z})$,\nwhere $\\boldsymbol{\\beta}(\\cdot)$ is an unknown Lipschitz-continuous function.\nWe introduce an adaptation of the Random Forest (RF) algorithm to estimate this\nmodel, balancing the flexibility of machine learning methods with the\ninterpretability of traditional linear models. This approach addresses a key\nchallenge in applied econometrics by accommodating heterogeneity in the\nrelationship between covariates and outcomes. Furthermore, the heterogeneous\npartial effects of $\\boldsymbol{X}$ on $Y$ are represented by\n$\\boldsymbol{\\beta}(\\cdot)$ and can be directly estimated using our proposed\nmethod. Our framework effectively unifies established parametric and\nnonparametric models, including varying-coefficient, switching regression, and\nadditive models. We provide theoretical guarantees, such as pointwise and\n$L^p$-norm rates of convergence for the estimator, and establish a pointwise\ncentral limit theorem through subsampling, aiding inference on the function\n$\\boldsymbol\\beta(\\cdot)$. We present Monte Carlo simulation results to assess\nthe finite-sample performance of the method.",
        "A numerical scheme is proposed to identify low energy configurations of a\nF\\\"oppl-von K\\'arm\\'an model for bilayer plates. The dependency of the\ncorresponding elastic energy on the in-plane displacement $u$ and the\nout-of-plane deflection $w$ leads to a practical minimization of the functional\nvia a decoupled gradient flow. In particular, the energies of the resulting\niterates are shown to be monotonically decreasing. The discretization of the\nmodel relies on $P1$ finite elements for the horizontal part $u$ and utilizes\nthe discrete Kirchhoff triangle for the vertical component $w$. The model\nallows for analysing various different problem settings via numerical\nsimulation: (i) stable low-energy configurations are detected dependent on a\nspecified prestrain described by elastic material properties, (ii) curvature\ninversions of spherical and cylindrical configurations are investigated, (iii)\nelastic responses of foldable cardboards for different spontaneous curvatures\nand crease geometries are compared.",
        "We rely on Monte Carlo (MC) simulations to interpret searches for new physics\nat the Large Hadron Collider (LHC) and elsewhere. These simulations result in\nnoisy and approximate estimators of selection efficiencies and likelihoods. In\nthis context we pioneer an exact-approximate computational method -\nexact-approximate Markov Chain Monte Carlo - that returns exact inferences\ndespite noisy simulations. To do so, we introduce an unbiased estimator for a\nPoisson likelihood. We demonstrate the new estimator and new techniques in\nexamples based on a search for neutralinos and charginos at the LHC using a\nsimplified model. We find attractive performance characteristics - exact\ninferences are obtained for a similar computational cost to approximate ones\nfrom existing methods and inferences are robust with respect to the number of\nevents generated per point.",
        "Non-smooth optimization models play a fundamental role in various\ndisciplines, including engineering, science, management, and finance. However,\nclassical algorithms for solving such models often struggle with convergence\nspeed, scalability, and parameter tuning, particularly in high-dimensional and\nnon-convex settings. In this paper, we explore how quantum mechanics can be\nleveraged to overcome these limitations. Specifically, we investigate the\ntheoretical properties of the Quantum Hamiltonian Descent (QHD) algorithm for\nnon-smooth optimization in both continuous and discrete time. First, we propose\ncontinuous-time variants of the general QHD algorithm and establish their\nglobal convergence and convergence rate for non-smooth convex and strongly\nconvex problems through a novel Lyapunov function design. Furthermore, we prove\nthe finite-time global convergence of continuous-time QHD for non-smooth\nnon-convex problems under mild conditions (i.e., locally Lipschitz). In\naddition, we propose discrete-time QHD, a fully digitized implementation of QHD\nvia operator splitting (i.e., product formula). We find that discrete-time QHD\nexhibits similar convergence properties even with large time steps. Finally,\nnumerical experiments validate our theoretical findings and demonstrate the\ncomputational advantages of QHD over classical non-smooth non-convex\noptimization algorithms.",
        "In the current paper, we present a study of the spatial distribution of\nluminous blue variables (LBVs) and various LBV candidates (cLBVs) with respect\nto OB associations in the M33 galaxy. The identification of blue star groups\nwas based on the LGGS data and was carried out by two clustering algorithms\nwith initial parameters determined during simulations of random stellar fields.\nWe have found that the distribution of distances to the nearest OB association\nobtained for the LBV\/cLBV sample is close to that for massive stars with\n$M_{\\rm init}>20\\,M_\\odot$ and Wolf-Rayet stars. This result is in good\nagreement with the standard assumption that luminous blue variables represent\nan intermediate stage in the evolution of the most massive stars. However, some\nobjects from the LBV\/cLBV sample, particularly Fe$\\,$II-emission stars,\ndemonstrated severe isolation compared to other massive stars, which, together\nwith certain features of their spectra, implicitly indicates that the nature of\nthese objects and other LBVs\/cLBVs may differ radically.",
        "A Steiner triple system STS$(v)$ is called $f$-pyramidal if it has an\nautomorphism group fixing $f$ points and acting sharply transitively on the\nremaining ones. In this paper, we focus on the STSs that are $f$-pyramidal over\nsome abelian group. Their existence has been settled only for the smallest\nadmissible values of $f$, that is, $f=0,1,3$.\n  In this paper, we complete this result and determine, for every $f>3$, the\nspectrum of values $(f,v)$ for which there is an $f$-pyramidal STS$(v)$ over an\nabelian group. This result is obtained by constructing difference families\nrelative to a suitable partial spread.",
        "Time-varying random objects have been increasingly encountered in modern data\nanalysis. Moreover, in a substantial number of these applications, periodic\nbehavior of the random objects has been observed. We introduce a new, powerful\nscan statistic and corresponding test for the precise identification and\nlocalization of abrupt changes in the distribution of non-Euclidean random\nobjects with possibly periodic behavior. Our approach is nonparametric and\neffectively captures the entire distribution of these random objects.\nRemarkably, it operates with minimal tuning parameters, requiring only the\nspecification of cut-off intervals near endpoints, where change points are\nassumed not to occur. Our theoretical contributions include deriving the\nasymptotic distribution of the test statistic under the null hypothesis of no\nchange points, establishing the consistency of the test in the presence of\nchange points under contiguous alternatives and providing rigorous guarantees\non the near-optimal consistency in estimating the number and locations of\nchange points, whether dealing with a single change point or multiple ones. We\ndemonstrate that the most competitive method currently in the literature for\nchange point detection in random objects is degraded by periodic behavior, as\nperiodicity leads to blurring of the changes that this procedure aims to\ndiscover. Through comprehensive simulation studies, we demonstrate the superior\npower and accuracy of our approach in both detecting change points and\npinpointing their locations, across scenarios involving both periodic and\nnonperiodic random objects. Our main application is to weighted networks,\nrepresented through graph Laplacians. The proposed method delivers highly\ninterpretable results, as evidenced by the identification of meaningful change\npoints in the New York City Citi Bike sharing system that align with\nsignificant historical events.",
        "We present the solution of a non-linear magnetic-charged black hole with an\nanisotropic matter field and further extend it to obtain the corresponding\nrotating black hole solution using the modified Newman-Janis algorithm. The\nevent horizon and ergosphere of the rotating black hole are studied in terms of\nthe perspective of geometric properties, revealing that the rotating black hole\ncan have up to three horizons. The first law of thermodynamics and the\nsquared-mass formula for the rotating black hole are derived from a\nthermodynamic perspective, based on which we obtain the thermodynamic\nquantities and study the thermodynamic stability of the rotating black hole.\nAdditionally, we calculate the Penrose process for the rotating black hole,\nindicating the influence of various black hole parameters on the maximal\nefficiency of the Penrose process.",
        "Quasi-one-dimensional chains of atoms can be effectively described by\none-dimensional Dirac-type equation. Crystal structure of the chain is\nreflected by pseudo-spin of the quasi-particles. In the article, we present a\nsimple framework where supersymmetric transformation is utilized to generate an\ninteraction between two, initially non-interacting systems described by\npseudo-spin-one Dirac-type equation. In the presented example, the\ntransformation converts two asymptotically non-interacting atomic chains into a\nsaw chain locally. The model possesses a flat band whose energy can be\nfine-tuned deliberately.",
        "Multi-symplectic integrators are typically regarded as a discretization of\nthe Hamiltonian partial differential equations. This is due to the fact that,\nfor generic finite-dimensional Hamiltonian systems, there exists only one\nindependent symplectic structure. In this note, the second invariant symplectic\nform is presented for the nonintegrable Henon-Heiles system, Kepler problem,\nintegrable and non-integrable Toda type systems. This approach facilitates the\nconstruction of a multi-symplectic integrator, which effectively preserves both\nsymplectic forms for these benchmark problems.",
        "Nulling interferometry is a promising technique for direct detection of\nexoplanets. However, the performance of current devices is limited by different\nperturbations sources and especially by its sensitivity to any phase\naberrations. The work presented here attempts to overcome those limitations by\nusing a four-telescopes nulling interferometer architecture, called\nKernel-Nuller, which includes a recombiner that positions the four signals in\nphase quadrature. This architecture is based on an integrated optical component\ncontaining 14 electronically controlled phase shifters, used to correct optical\npath differences that would be induced by manufacturing defects. The first part\nof the study consists in the development of an algorithm providing the delays\nto be injected into the component to optimize the performance of that device.\nThe next step of this study deals with the analysis of the intensity\ndistributions produced at the output of the Kernel-Nuller through a series of\nobservations. Then we apply statistical tests and data treatment techniques to\ndetect the signature of an exoplanets.",
        "We study the possible existence of bound states of three $\\Omega_{bbb}$\nbaryons. We consider only $S$ wave interactions and we start from recent\nlattice QCD results which give a strongly attractive potential between two\n$\\Omega_{bbb}$ baryons in the $^1S_0$ channel. We analyze different scenarios.\nAt baryonic level, the $\\Omega_{bbb}\\Omega_{bbb}$ interaction could be\nunderstood to be basically spin-independent, so that the two contributing\nchannels, $^1S_0$ and $^5S_2$, would have a very similar interaction. This\nbaryonic analysis leads to the existence of bound states in the three-body\nsystem. At the quark level, repulsive effects would appear in the $^5S_2$\nchannel, making it more repulsive than the $^1S_0$ channel. We study the effect\nof such repulsion in terms of its range.",
        "The central molecular zone (CMZ) of our Galaxy exhibits widespread emission\nfrom SiO and various complex organic molecules (COMs), yet the exact origin of\nsuch emission is uncertain. Here we report the discovery of a unique class of\nlong ($>$0.5 pc) and narrow ($<$0.03 pc) filaments in the emission of SiO 5$-$4\nand eight additional molecular lines, including several COMs, in our ALMA 1.3\nmm spectral line observations toward two massive molecular clouds in the CMZ,\nwhich we name as slim filaments. However, these filaments are not detected in\nthe 1.3 mm continuum at the 5$\\sigma$ level. Their line-of-sight velocities are\ncoherent and inconsistent with being outflows. The column densities and\nrelative abundances of the detected molecules are statistically similar to\nthose in protostellar outflows but different from those in dense cores within\nthe same clouds. Turbulent pressure in these filaments dominates over self\ngravity and leads to hydrostatic inequilibrium, indicating that they are a\ndifferent class of objects than the dense gas filaments in dynamical\nequilibrium ubiquitously found in nearby molecular clouds. We argue that these\nnewly detected slim filaments are associated with parsec-scale shocks, likely\narising from dynamic interactions between shock waves and molecular clouds. The\ndissipation of the slim filaments may replenish SiO and COMs in the\ninterstellar medium and lead to their widespread emission in the CMZ.",
        "Proton computed tomography (pCT) aims to facilitate precise dose planning for\nhadron therapy, a promising and effective method for cancer treatment. Hadron\ntherapy utilizes protons and heavy ions to deliver well focused doses of\nradiation, leveraging the Bragg peak phenomenon to target tumors while sparing\nhealthy tissues. The Bergen pCT Collaboration aims to develop a novel pCT\nscanner, and accompanying reconstruction algorithms to overcome current\nlimitations. This paper focuses on advancing the track- and image\nreconstruction algorithms, thereby enhancing the precision of the dose planning\nand reducing side effects of hadron therapy. A neural network aided track\nreconstruction method is presented.",
        "Attosecond spectroscopy of materials has provided invaluable insight into\nlight-driven coherent electron dynamics. However, attosecond spectroscopies\nhave so far been focused on weakly-correlated materials. As a result, the\nbehavior of strongly-correlated systems is largely unknown at sub- to\nfew-femtosecond timescales, even though it is typically the realm at which\nelectron-electron interactions operate. Here we conduct attosecond-resolved\nexperiments on the correlated insulator nickel oxide, and compare its response\nto a common band insulator, revealing fundamentally different behaviors. The\nresults, together with state-of-the art time-dependent $\\textit{ab initio}$\ncalculations, show that the correlated system response is governed by a\nlaser-driven quench of electron correlations. The evolution of the on-site\nelectronic interaction is measured here at its natural timescale, marking the\nfirst direct measurement of Hubbard $U$ renormalization in NiO. It is found to\ntake place within a few femtoseconds, after which structural changes slowly\nstart to take place. The resulting picture sheds light on the entire\nlight-induced response of a strongly-correlated system, from attosecond to\nlong-lived effects.",
        "The default way of proving holographic entropy inequalities is the\ncontraction method. It divides Ryu-Takayanagi (RT) surfaces on the `greater\nthan' side of the inequality into segments, then glues the segments into\ncandidate RT surfaces for terms on the `less than' side. Here we discuss how\nproofs by contraction are constrained and informed by entanglement wedge\nnesting (EWN) -- the property that enlarging a boundary region can only enlarge\nits entanglement wedge. We propose that: (i) all proofs by contraction\nnecessarily involve candidate RT surfaces, which violate EWN; (ii) violations\nof EWN in contraction proofs of maximally tight inequalities occur commonly and\n-- where this can be quantified -- with maximal density near boundary\nconditions; (iii) the non-uniqueness of proofs by contraction reflects\ninequivalent ways of violating EWN. As evidence and illustration, we study the\nrecently discovered infinite families of holographic entropy inequalities,\nwhich are associated with tessellations of the torus and the projective plane.\nWe explain the logic, which underlies their proofs by contraction. We find that\nall salient aspects of the requisite contraction maps are dictated by EWN while\nall their variable aspects set the scheme for how to violate EWN. We comment on\nwhether the tension between EWN and contraction maps might help in\ncharacterizing maximally tight holographic entropy inequalities.",
        "Inspired by the states $X(2370)$ and $X(2600)$ reported by the BESIII\nCollaboration, we systematically investigate the mass spectra of light compact\ntetraquark states with configurations $ud\\bar{u}\\bar{d}$, $us\\bar{u}\\bar{s}$,\nand $ss\\bar{s}\\bar{s}$ in the $J^{PC}=0^{-+}$ and $2^{-+}$ channels using the\nQCD sum rules approach. To effectively describe these tetraquark states, we\nconstruct appropriate interpolating tetraquark currents featuring three Lorentz\nindices while avoiding derivative operators. Through meticulous calculations of\ncorrelation functions up to dimension 10 condensates, we extract the mass\nspectra for both $0^{-+}$ and $2^{-+}$ states by employing the projection\noperator technique. Our results indicate that the masses of light tetraquarks\nspan $1.5-2.5~\\text{GeV}$ for $0^{-+}$ states and $2.4-2.7~\\text{GeV}$ for\n$2^{-+}$ states. Notably, our analysis suggests that the $X(2370)$ state could\nbe interpreted as a $0^{-+}$ $us\\bar{u}\\bar{s}$ or $ss\\bar{s}\\bar{s}$\ntetraquark state, while the $X(2600)$ state is likely to be a $2^{-+}$\n$us\\bar{u}\\bar{s}$ tetraquark. These intriguing findings warrant further\ndetailed investigation in future studies to better understand the nature of\nthese states.",
        "This paper establishes an equilibrium existence result for a class of Mean\nField Games involving Reflected Stochastic Differential Equations. The proof\nrelies on the framework of relaxed controls and martingale problems.",
        "In this work, we report for the first time two repeated quasi-periodic\noscillations (QPOs) in the light curve of the Flat Spectrum Radio Quasar (FSRQ)\nS5 1044+71. This source was observed by the Transiting Exoplanet Survey\nSatellite (TESS) in multiple sectors. We used the generalized Lomb-Scargle\nperiodogram method and weighted wavelet Z-transform method to search for\nsignificant periodic signals. The main results are as follows: We found QPOs of\n$\\sim$ 7.0 days (persisted for 4 cycles, with a significance of\n$\\sim3.5\\sigma$) and $\\sim$ 7.3 days (persisted for 5 cycles, with a\nsignificance of $\\sim3.8\\sigma$) in the light curves of Sector 47 and EP1,\nrespectively. Considering range of error, we consider them to be the same. We\ndiscussed two likely models of these rapid quasi-periodic variations: One comes\nfrom the jet and the other from the accretion disk. For the first one, we\nconsider kink instability of the jet as a plausible explanation. Second, the\nQPO is probable to come from the main hot spots in the accretion disk, which is\nlocated approximately within the innermost stable circular orbit allowed by\ngeneral relativity. Based on this model, we estimate the mass of the black hole\nin S5 1044+71 to be $3.49 \\times 10^9 M_{\\odot}$.",
        "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate.",
        "We explore the role of symmetry in three obdurate conjectures of differential\ngeometry: the Carath\\'eodory, the Willmore and the Lawson Conjectures.",
        "Owing to their sensitivity to temperature fluctuations, normal\nmetal-insulator-superconductor (NIS) junctions are leveraged in various thermal\ndevices. This study illustrates that two NISIN reservoirs can achieve a\nmeasurable negative differential thermal conductance (NDTC). This phenomenon is\nenabled by photon-mediated heat exchange, which is profoundly affected by the\ntemperature-dependent impedance matching between the reservoirs. Under\nappropriate configurations, the heat current is suppressed for increasingly\nlarge temperature gradients, leading to NDTC. We also propose experimental\nconfigurations where it is possible to discriminate this effect unambiguously.\nWe employ superconducting aluminum in conjunction with either silver or\nepitaxial InAs to facilitate the experimental observation of NDTC at low\ntemperatures over significant sub-Kelvin ranges. This advances the development\nof devices that exploit NDTC to enhance heat and temperature regulation in\ncryogenic environments, such as thermal switches, transistors, and amplifiers.",
        "Lenses are typically based on refractive index profiles derived from the\ngeometric approximation of high-frequency waves, yet the critical issue of\nimpedance mismatch is often neglected. Mismatched devices suffer from unwanted\nreflections and dispersion, which can significantly degrade performance in\npractical applications. In this work, we propose impedance profiles for lenses\nto achieve efficient wave transmission while maintaining the desired refractive\nindex and minimizing dispersion effects. A family of impedance profiles is\nderived from the acoustic wave equation such that the phase velocity is\npreserved. First, the 1D setting is considered to explain how dispersion occurs\ninside a lens and at its interfaces. Then, the method is applied to 2D\naxisymmetric configurations where the impedance mismatch is radially\nredistributed. These profiles are demonstrated in the acoustic setting of a\nLuneburg lens, but can be easily extended to more general scenarios such as\nimaging or cloaking in air and water, where matching the impedance of the\nbackground poses significant challenges."
      ]
    }
  },
  {
    "id":2411.06184,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Multi-Task Bayesian Optimization",
    "start_abstract":"Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and shown to yield state-of-the-art performance with impressive ease efficiency. In this paper, we explore whether it is possible transfer knowledge gained from previous optimizations new tasks in order find optimal hyperparameter settings more efficiently. Our approach based on extending multi-task Gaussian processes optimization. We show that method significantly speeds up process when compared standard single-task approach. further propose straightforward extension our algorithm jointly minimize average error across multiple demonstrate how can be used greatly speed k-fold cross-validation. Lastly, an adaptation developed acquisition function, entropy search, cost-sensitive, setting. utility function by leveraging small dataset hyper-parameter large dataset. dynamically chooses which query most information per unit cost.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "New Pathologic Classification of Lung Cancer: Relevance for Clinical Practice and Clinical Trials"
      ],
      "abstract":[
        "We summarize significant changes in pathologic classification of lung cancer resulting from the 2011 International Association for Study Lung Cancer\/American Thoracic Society\/European Respiratory Society (IASLC\/ATS\/ERS) adenocarcinoma classification. The was developed by an international core panel experts representing IASLC, ATS, and ERS with oncologists\/pulmonologists, pathologists, radiologists, molecular biologists, thoracic surgeons. Because 70% patients present advanced stages, a new approach to small biopsies cytology specific terminology criteria focused on need distinguishing squamous cell carcinoma testing EGFR mutations ALK rearrangement. Tumors previously classified as non-small-cell carcinoma, not otherwise specified, because lack clear or morphology should be further using limited immunohistochemical workup preserve tissue testing. terms \"bronchioloalveolar carcinoma\" \"mixed subtype adenocarcinoma\" have been discontinued. For resected adenocarcinomas, concepts situ minimally invasive define who, if they undergo complete resection, will 100% disease-free survival. Invasive adenocarcinomas are now predominant pattern after comprehensive histologic subtyping lepidic, acinar, papillary, solid patterns; micropapillary is added poor prognosis. Former mucinous bronchioloalveolar carcinomas called \"invasive adenocarcinoma.\" field rapidly evolving advances occurring frequent basis, particularly arena, this provides much needed standard diagnosis only patient care but also clinical trials TNM"
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Exact Learning of Permutations for Nonzero Binary Inputs with\n  Logarithmic Training Size and Quadratic Ensemble Complexity",
        "Entropy-Guided Attention for Private LLMs",
        "On the Uncertainty of a Simple Estimator for Remote Source Monitoring\n  over ALOHA Channels",
        "Unified Guidance for Geometry-Conditioned Molecular Generation",
        "In Situ Optimization of an Optoelectronic Reservoir Computer with\n  Digital Delayed Feedback",
        "Explaining 3D Computed Tomography Classifiers with Counterfactuals",
        "Detection of Rumors and Their Sources in Social Networks: A\n  Comprehensive Survey",
        "Anisotropic Exchange Spin Model to Investigate the Curie Temperature\n  Dispersion of Finite-Size L10-FePt Magnetic Nanoparticles",
        "PluralLLM: Pluralistic Alignment in LLMs via Federated Learning",
        "Fifteen Years of M31* X-ray Variability and Flares",
        "A Semantic-Loss Function Modeling Framework With Task-Oriented Machine\n  Learning Perspectives",
        "YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised\n  Re-ID",
        "Robust Egoistic Rigid Body Localization",
        "Diminishing Waters: The Great Salt Lake's Desiccation and Its Mental\n  Health Consequences",
        "Code-Verification Techniques for an Arbitrary-Depth Electromagnetic Slot\n  Model",
        "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems",
        "From Trust to Truth: Actionable policies for the use of AI in\n  fact-checking in Germany and Ukraine",
        "The assembly of supermassive black holes at $z<1$ in early-type galaxies\n  from scaling relations",
        "Partially connected contributions to baryon masses in QCD+QED",
        "Reading the unreadable: Creating a dataset of 19th century English\n  newspapers using image-to-text language models",
        "Finite Sample Analysis of System Poles for Ho-Kalman Algorithm",
        "Does Generation Require Memorization? Creative Diffusion Models using\n  Ambient Diffusion",
        "Incorporating Cyclic Group Equivariance into Deep Learning for Reliable\n  Reconstruction of Rotationally Symmetric Tomography Systems",
        "The Fertile Steppe: Computability Logic and the decidability of one of\n  its fragments",
        "HuGDiffusion: Generalizable Single-Image Human Rendering via 3D Gaussian\n  Diffusion",
        "Mol-LLM: Generalist Molecular LLM with Improved Graph Utilization",
        "Policy Learning with a Natural Language Action Space: A Causal Approach",
        "Parabolic dielectric reflector for extreme on-chip spot-size conversion\n  with broad bandwidth",
        "Maximum force conjecture in curved spacetimes of stable self-gravitating\n  matter configurations"
      ],
      "abstract":[
        "The ability of an architecture to realize permutations is quite fundamental.\nFor example, Large Language Models need to be able to correctly copy (and\nperhaps rearrange) parts of the input prompt into the output. Classical\nuniversal approximation theorems guarantee the existence of parameter\nconfigurations that solve this task but offer no insights into whether\ngradient-based algorithms can find them. In this paper, we address this gap by\nfocusing on two-layer fully connected feed-forward neural networks and the task\nof learning permutations on nonzero binary inputs. We show that in the infinite\nwidth Neural Tangent Kernel (NTK) regime, an ensemble of such networks\nindependently trained with gradient descent on only the $k$ standard basis\nvectors out of $2^k - 1$ possible inputs successfully learns any fixed\npermutation of length $k$ with arbitrarily high probability. By analyzing the\nexact training dynamics, we prove that the network's output converges to a\nGaussian process whose mean captures the ground truth permutation via\nsign-based features. We then demonstrate how averaging these runs (an\n\"ensemble\" method) and applying a simple rounding step yields an arbitrarily\naccurate prediction on any possible input unseen during training. Notably, the\nnumber of models needed to achieve exact learning with high probability (which\nwe refer to as ensemble complexity) exhibits a linearithmic dependence on the\ninput size $k$ for a single test input and a quadratic dependence when\nconsidering all test inputs simultaneously.",
        "The pervasiveness of proprietary language models has raised critical privacy\nconcerns, necessitating advancements in private inference (PI), where\ncomputations are performed directly on encrypted data without revealing users'\nsensitive information. While PI offers a promising solution, its practical\ndeployment is hindered by substantial communication and latency overheads,\nprimarily stemming from nonlinear operations. To address this, we introduce an\ninformation-theoretic framework to characterize the role of nonlinearities in\ndecoder-only language models, laying a principled foundation for optimizing\ntransformer-architectures tailored to the demands of PI.\n  By leveraging Shannon's entropy as a quantitative measure, we uncover the\npreviously unexplored dual significance of nonlinearities: beyond ensuring\ntraining stability, they are crucial for maintaining attention head diversity.\nSpecifically, we find that their removal triggers two critical failure modes:\n{\\em entropy collapse} in deeper layers that destabilizes training, and {\\em\nentropic overload} in earlier layers that leads to under-utilization of\nMulti-Head Attention's (MHA) representational capacity.\n  We propose an entropy-guided attention mechanism paired with a novel entropy\nregularization technique to mitigate entropic overload. Additionally, we\nexplore PI-friendly alternatives to layer normalization for preventing entropy\ncollapse and stabilizing the training of LLMs with reduced-nonlinearities. Our\nstudy bridges the gap between information theory and architectural design,\nestablishing entropy dynamics as a principled guide for developing efficient PI\narchitectures. The code and implementation are available at\nhttps:\/\/github.com\/Nandan91\/entropy-guided-attention-llm",
        "Efficient remote monitoring of distributed sources is essential for many\nInternet of Things (IoT) applications. This work studies the uncertainty at the\nreceiver when tracking two-state Markov sources over a slotted random access\nchannel without feedback, using the conditional entropy as a performance\nindicator, and considering the last received value as current state estimate.\nWe provide an analytical characterization of the metric, and evaluate three\naccess strategies: (i) maximizing throughput, (ii) transmitting only on state\nchanges, and (iii) minimizing uncertainty through optimized access\nprobabilities. Our results reveal that throughput optimization does not always\nreduce uncertainty. Moreover, while reactive policies are optimal for symmetric\nsources, asymmetric processes benefit from mixed strategies allowing\ntransmissions during state persistence.",
        "Effectively designing molecular geometries is essential to advancing\npharmaceutical innovations, a domain, which has experienced great attention\nthrough the success of generative models and, in particular, diffusion models.\nHowever, current molecular diffusion models are tailored towards a specific\ndownstream task and lack adaptability. We introduce UniGuide, a framework for\ncontrolled geometric guidance of unconditional diffusion models that allows\nflexible conditioning during inference without the requirement of extra\ntraining or networks. We show how applications such as structure-based,\nfragment-based, and ligand-based drug design are formulated in the UniGuide\nframework and demonstrate on-par or superior performance compared to\nspecialised models. Offering a more versatile approach, UniGuide has the\npotential to streamline the development of molecular generative models,\nallowing them to be readily used in diverse application scenarios.",
        "Reservoir computing (RC) is an innovative paradigm in neuromorphic computing\nthat leverages fixed, randomized, internal connections to address the challenge\nof overfitting. RC has shown remarkable effectiveness in signal processing and\npattern recognition tasks, making it well-suited for hardware implementations\nacross various physical substrates, which promise enhanced computation speeds\nand reduced energy consumption. However, achieving optimal performance in RC\nsystems requires effective parameter optimization. Traditionally, this\noptimization has relied on software modeling, limiting the practicality of\nphysical computing approaches. Here, we report an \\emph{in situ} optimization\nmethod for an optoelectronic delay-based RC system with digital delayed\nfeedback. By simultaneously optimizing five parameters, normalized mean squared\nerror (NMSE) of 0.028, 0.561, and 0.271 is achieved in three benchmark tasks:\nwaveform classification, time series prediction, and speech recognition\noutperforming simulation-based optimization (NMSE 0.054, 0.543, and 0.329,\nrespectively) in the two of the three tasks. This method marks a significant\nadvancement in physical computing, facilitating the optimization of RC and\nneuromorphic systems without the need for simulation, thus enhancing their\npractical applicability.",
        "Counterfactual explanations in medical imaging are critical for understanding\nthe predictions made by deep learning models. We extend the Latent Shift\ncounterfactual generation method from 2D applications to 3D computed tomography\n(CT) scans. We address the challenges associated with 3D data, such as limited\ntraining samples and high memory demands, by implementing a slice-based\napproach. This method leverages a 2D encoder trained on CT slices, which are\nsubsequently combined to maintain 3D context. We demonstrate this technique on\ntwo models for clinical phenotype prediction and lung segmentation. Our\napproach is both memory-efficient and effective for generating interpretable\ncounterfactuals in high-resolution 3D medical imaging.",
        "With the recent advancements in social network platform technology, an\noverwhelming amount of information is spreading rapidly. In this situation, it\ncan become increasingly difficult to discern what information is false or true.\nIf false information proliferates significantly, it can lead to undesirable\noutcomes. Hence, when we receive some information, we can pose the following\ntwo questions: $(i)$ Is the information true? $(ii)$ If not, who initially\nspread that information? % The first problem is the rumor detection issue,\nwhile the second is the rumor source detection problem. A rumor-detection\nproblem involves identifying and mitigating false or misleading information\nspread via various communication channels, particularly online platforms and\nsocial media. Rumors can range from harmless ones to deliberately misleading\ncontent aimed at deceiving or manipulating audiences. Detecting misinformation\nis crucial for maintaining the integrity of information ecosystems and\npreventing harmful effects such as the spread of false beliefs, polarization,\nand even societal harm. Therefore, it is very important to quickly distinguish\nsuch misinformation while simultaneously finding its source to block it from\nspreading on the network. However, most of the existing surveys have analyzed\nthese two issues separately. In this work, we first survey the existing\nresearch on the rumor-detection and rumor source detection problems with joint\ndetection approaches, simultaneously. % This survey deals with these two issues\ntogether so that their relationship can be observed and it provides how the two\nproblems are similar and different. The limitations arising from the rumor\ndetection, rumor source detection, and their combination problems are also\nexplained, and some challenges to be addressed in future works are presented.",
        "We developed an anisotropic spin model that accounts for magnetic anisotropy\nand evaluated the Curie temperature (Tc) dispersion due to finite size effects\nin L10-FePt nanoparticles. In heat-assisted magnetic recording (HAMR) media, a\nnext-generation magnetic recording technology, high-density recording is\nachieved by locally heating L10-FePt nanoparticles near their Tc and rapidly\ncooling them. However, variations in Tc caused by differences in particle size\nand shape can compromise recording stability and areal density capacity, making\nthe control of Tc dispersion critical. In this study, we constructed atomistic\nLLG models to explicitly incorporate the spin exchange anisotropy of L10-FePt,\nbased on parameters determined by first-principles calculations. Using this\nmodel, we evaluated the impact of particle size on Tc dispersion. As a result,\n(1) the Tc dispersion critical to the performance of HAMR can be reproduced,\nwhereas it was previously underestimated by isotropic models and (2)\napproximately 70% of the experimentally observed Tc dispersion can be\nattributed to particle size effects. This research highlights the role of\nexchange anisotropy in amplifying finite-size effects and underscores the\nimportance of size control in HAMR media.",
        "Ensuring Large Language Models (LLMs) align with diverse human preferences\nwhile preserving privacy and fairness remains a challenge. Existing methods,\nsuch as Reinforcement Learning from Human Feedback (RLHF), rely on centralized\ndata collection, making them computationally expensive and privacy-invasive. We\nintroduce PluralLLM a federated learning-based approach that enables multiple\nuser groups to collaboratively train a transformer-based preference predictor\nwithout sharing sensitive data, which can also serve as a reward model for\naligning LLMs. Our method leverages Federated Averaging (FedAvg) to aggregate\npreference updates efficiently, achieving 46% faster convergence, a 4%\nimprovement in alignment scores, and nearly the same group fairness measure as\nin centralized training. Evaluated on a Q\/A preference alignment task,\nPluralLLM demonstrates that federated preference learning offers a scalable and\nprivacy-preserving alternative for aligning LLMs with diverse human values.",
        "We append an additional fifteen years (2009-2024) to the Chandra X-ray light\ncurve of M31*, the supermassive black hole at the center of M31, the Andromeda\ngalaxy. Extending and expanding on the work in Li et al. 2011, we show that\nM31* has remained in an elevated X-ray state from 2006 through at least 2016\n(when regular Chandra monitoring ceased) and likely through 2024, with the most\nrecent observations still showing an elevated X-ray flux. We identify one\nmoderate flare in 2013 where the other nuclear X-ray sources are in low-flux\nstates, making that flare a valuable target for followup with multiwavelength\nand multimessenger archival data. We extract a mostly uncontaminated spectrum\nfor M31* from this observation, showing that its X-ray properties are similar\nto those observed at Sgr A* in its quiescent state by Baganoff et al. 2003.\nFurthermore, we find no substantial change in the source's hardness ratio in\nthe 2006 and 2013 flares compared to the post-2006 elevated state, suggesting\nthe these flares are increases in the regular X-ray emission mechanisms instead\nof entirely new emission modes. Our extended light curve for M31* provides\nvaluable context for multimessenger or multiwavelength observations of nearby\nsupermassive black holes.",
        "The integration of machine learning (ML) has significantly enhanced the\ncapabilities of Earth Observation (EO) systems by enabling the extraction of\nactionable insights from complex datasets. However, the performance of\ndata-driven EO applications is heavily influenced by the data collection and\ntransmission processes, where limited satellite bandwidth and latency\nconstraints can hinder the full transmission of original data to the receivers.\nTo address this issue, adopting the concepts of Semantic Communication (SC)\noffers a promising solution by prioritizing the transmission of essential data\nsemantics over raw information. Implementing SC for EO systems requires a\nthorough understanding of the impact of data processing and communication\nchannel conditions on semantic loss at the processing center. This work\nproposes a novel data-fitting framework to empirically model the semantic loss\nusing real-world EO datasets and domain-specific insights. The framework\nquantifies two primary types of semantic loss: (1) source coding loss, assessed\nvia a data quality indicator measuring the impact of processing on raw source\ndata, and (2) transmission loss, evaluated by comparing practical transmission\nperformance against the Shannon limit. Semantic losses are estimated by\nevaluating the accuracy of EO applications using four task-oriented ML models,\nEfficientViT, MobileViT, ResNet50-DINO, and ResNet8-KD, on lossy image datasets\nunder varying channel conditions and compression ratios. These results underpin\na framework for efficient semantic-loss modeling in bandwidth-constrained EO\nscenarios, enabling more reliable and effective operations.",
        "We introduce YOLO11-JDE, a fast and accurate multi-object tracking (MOT)\nsolution that combines real-time object detection with self-supervised\nRe-Identification (Re-ID). By incorporating a dedicated Re-ID branch into\nYOLO11s, our model performs Joint Detection and Embedding (JDE), generating\nappearance features for each detection. The Re-ID branch is trained in a fully\nself-supervised setting while simultaneously training for detection,\neliminating the need for costly identity-labeled datasets. The triplet loss,\nwith hard positive and semi-hard negative mining strategies, is used for\nlearning discriminative embeddings. Data association is enhanced with a custom\ntracking implementation that successfully integrates motion, appearance, and\nlocation cues. YOLO11-JDE achieves competitive results on MOT17 and MOT20\nbenchmarks, surpassing existing JDE methods in terms of FPS and using up to ten\ntimes fewer parameters. Thus, making our method a highly attractive solution\nfor real-world applications.",
        "We consider a robust and self-reliant (or \"egoistic\") variation of the rigid\nbody localization (RBL) problem, in which a primary rigid body seeks to\nestimate the pose (i.e., location and orientation) of another rigid body (or\n\"target\"), relative to its own, without the assistance of external\ninfrastructure, without prior knowledge of the shape of the target, and taking\ninto account the possibility that the available observations are incomplete.\nThree complementary contributions are then offered for such a scenario. The\nfirst is a method to estimate the translation vector between the center point\nof both rigid bodies, which unlike existing techniques does not require that\nboth objects have the same shape or even the same number of landmark points.\nThis technique is shown to significantly outperform the state-of-the-art (SotA)\nunder complete information, but to be sensitive to data erasures, even when\nenhanced by matrix completion methods. The second contribution, designed to\noffer improved performance in the presence of incomplete information, offers a\nrobust alternative to the latter, at the expense of a slight relative loss\nunder complete information. Finally, the third contribution is a scheme for the\nestimation of the rotation matrix describing the relative orientation of the\ntarget rigid body with respect to the primary. Comparisons of the proposed\nschemes and SotA techniques demonstrate the advantage of the contributed\nmethods in terms of root mean square error (RMSE) performance under fully\ncomplete information and incomplete conditions.",
        "This study examines how the desiccation of Utah Great Salt Lake GSL,\nexacerbated by anthropogenic changes, poses significant health risks,\nparticularly communities mental health. Reduced water inflow has exposed the\nlakebed, increasing airborne particulate matter PM2.5 and dust storms, which\nimpact air quality. By integrating diverse datasets spanning from 1980 to\npresent including insitu measurements, satellite imagery, and reanalysis\nproducts this study synthesizes hydrological, atmospheric, and epidemiological\nvariables to comprehensively track the extent of the GSL surface water, local\nair quality fluctuations, and their effects on community mental health. The\nfindings indicate a clear relationship between higher pollution days and more\nsevere depressive symptoms. Specifically, individuals exposed to 22 days with\nPM2.5 levels above the World Health Organizations 24 hour guideline of 15 ug\nper m3 were more likely to experience severe depressive symptoms. Our results\nalso suggest that people experiencing more severe depression not only face a\nhigher number of high pollution days but also encounter such days more\nfrequently. The study highlights the interconnectedness of poor air quality,\nenvironmental degradation and mental health emphasizing the need for more\nsustainable economic growth in the region.",
        "Electromagnetic penetration through openings in an otherwise closed\nelectromagnetic scatterer is an important topic in computational\nelectromagnetics. To efficiently model this phenomenon, aperture or slot models\nare often used in conjunction with surface integral equations to solve\nMaxwell's equations. To establish the credibility of these models, code\nverification is necessary to assess the correctness of the implementation of\nthe underlying numerical methods. However, many characteristics of surface\nintegral equations and slot models render traditional code-verification\napproaches ineffective. In this paper, we present approaches to separately\nmeasure the different sources of numerical error arising from the\nmethod-of-moments implementation of the electric-field integral equation with\nan arbitrary-depth slot model. We demonstrate the effectiveness of these\napproaches for several cases.",
        "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher.",
        "The rise of Artificial Intelligence (AI) presents unprecedented opportunities\nand challenges for journalism, fact-checking and media regulation. While AI\noffers tools to combat disinformation and enhance media practices, its\nunregulated use and associated risks necessitate clear policies and\ncollaborative efforts. This policy paper explores the implications of\nartificial intelligence (AI) for journalism and fact-checking, with a focus on\naddressing disinformation and fostering responsible AI integration. Using\nGermany and Ukraine as key case studies, it identifies the challenges posed by\ndisinformation, proposes regulatory and funding strategies, and outlines\ntechnical standards to enhance AI adoption in media. The paper offers\nactionable recommendations to ensure AI's responsible and effective integration\ninto media ecosystems. AI presents significant opportunities to combat\ndisinformation and enhance journalistic practices. However, its implementation\nlacks cohesive regulation, leading to risks such as bias, transparency issues,\nand over-reliance on automated systems. In Ukraine, establishing an independent\nmedia regulatory framework adapted to its governance is crucial, while Germany\ncan act as a leader in advancing EU-wide collaborations and standards.\nTogether, these efforts can shape a robust AI-driven media ecosystem that\npromotes accuracy and trust.",
        "The assembly of supermassive black hole (SMBH) mass ($M_{\\bullet}$) and\nstellar mass ($M_{*}$) in galaxies can be studied via the redshift evolution of\nthe $M_{\\bullet}-M_{*}$ relation, but the ways in which selection bias and\nphysical assembly channels affect this evolution are uncertain. To address\nthis, we compare the $M_{\\bullet}-M_{*}$ relation for local massive\n($M_{*}>10^{10.5}$M$_{\\odot}$) quiescent early-type galaxies (ETGs) to that for\nmassive ETGs hosting active galactic nuclei (AGN) at $z\\sim0.8$. The\nrestrictions on stellar mass and galaxy type limit the assembly channels that\nmay connect the two relations. For the local sample we find $\\log(M_{\\bullet})\n= 8.80 + 1.10(\\log{M_{*}-11})$, in line with prior work. For the $z\\sim0.8$\nsample we find a bias-corrected relation: $\\log(M_{\\bullet}) = 7.80 +\n1.25(\\log{M_{*}-11})$. We show, however, that this relation depends on the\nstellar and SMBH mass functions used to compute the selection bias, the virial\nrelation, the virial factor, and the active fraction, which together introduce\nuncertainty of up to $\\sim0.6$\\,dex in the $z\\sim0.8$ relation. Adopting\nreasonable choices of these parameters then our $z\\sim0.8$ relation lies above\nthat for $z\\sim0$ AGN by $\\sim0.5$\\,dex, but below our $z\\sim0$ ETG relation by\n$0.4-1$\\,dex in SMBH mass. We discuss possible sources of this offset,\nincluding further bias corrections, `downsizing\" in SMBH mass assembly, and\npreferential SMBH growth. Our results highlight the need to reduce\nuncertainties from selection and measurement bias in SMBH and stellar masses at\nall redshifts.",
        "Full QCD+QED simulations allow to evaluate isospin breaking corrections to\nhadron masses. With the openQxD code, we are able to perform these simulations\nemploying C-periodic boundary conditions, implemented through a doubling of the\nphysical lattice along one spatial direction. The use of these boundary\nconditions introduces non-zero Wick contractions between two quark or two\nantiquark fields, that, in the case of the computation of baryon masses, lead\nto partially connected additional contributions that we expect to vanish in the\ninfinite volume limit. These contributions are challenging because they involve\nan all-to-all propagator connecting one point in the physical lattice and one\nin the mirror lattice. We present a way to compute these corrections to the\n$\\Omega^-$ baryon mass using a combination of point and stochastic source\ninversions. This work is part of the program of the RC* collaboration.",
        "Oscar Wilde said, \"The difference between literature and journalism is that\njournalism is unreadable, and literature is not read.\" Unfortunately, The\ndigitally archived journalism of Oscar Wilde's 19th century often has no or\npoor quality Optical Character Recognition (OCR), reducing the accessibility of\nthese archives and making them unreadable both figuratively and literally. This\npaper helps address the issue by performing OCR on \"The Nineteenth Century\nSerials Edition\" (NCSE), an 84k-page collection of 19th-century English\nnewspapers and periodicals, using Pixtral 12B, a pre-trained image-to-text\nlanguage model. The OCR capability of Pixtral was compared to 4 other OCR\napproaches, achieving a median character error rate of 1%, 5x lower than the\nnext best model. The resulting NCSE v2.0 dataset features improved article\nidentification, high-quality OCR, and text classified into four types and\nseventeen topics. The dataset contains 1.4 million entries, and 321 million\nwords. Example use cases demonstrate analysis of topic similarity, readability,\nand event tracking. NCSE v2.0 is freely available to encourage historical and\nsociological research. As a result, 21st-century readers can now share Oscar\nWilde's disappointment with 19th-century journalistic standards, reading the\nunreadable from the comfort of their own computers.",
        "This paper investigates the error analysis of system pole estimation in\n$n$-dimensional discrete-time Linear Time-Invariant systems with $m$ outputs\nand $p$ inputs, using the classical Ho-Kalman algorithm based on finite\ninput-output sample data. Building upon prior work, we establish end-to-end\nestimation guarantees for system poles under both single-trajectory and\nmultiple-trajectory settings. Specifically, we prove that, with high\nprobability, the estimation error of system poles decreases at a rate of at\nleast $\\mathcal{O}\\{T^{-\\frac{1}{2n}}\\}$ in the single-trajectory case and\n$\\mathcal{O}\\{N^{-\\frac{1}{2n}}\\}$ in the multiple-trajectory case, where $T$\nis the length of a single trajectory, and $N$ is the number of trajectories.\nFurthermore, we reveal that in both settings, achieving a constant estimation\naccuracy for system poles requires the sample size to grow super-polynomially\nwith respect to the larger of the two ratios, $ \\max\\{n\/m, n\/p\\} $. Numerical\nexperiments are conducted to validate the non-asymptotic results of system pole\nestimation.",
        "There is strong empirical evidence that the state-of-the-art diffusion\nmodeling paradigm leads to models that memorize the training set, especially\nwhen the training set is small. Prior methods to mitigate the memorization\nproblem often lead to a decrease in image quality. Is it possible to obtain\nstrong and creative generative models, i.e., models that achieve high\ngeneration quality and low memorization? Despite the current pessimistic\nlandscape of results, we make significant progress in pushing the trade-off\nbetween fidelity and memorization. We first provide theoretical evidence that\nmemorization in diffusion models is only necessary for denoising problems at\nlow noise scales (usually used in generating high-frequency details). Using\nthis theoretical insight, we propose a simple, principled method to train the\ndiffusion models using noisy data at large noise scales. We show that our\nmethod significantly reduces memorization without decreasing the image quality,\nfor both text-conditional and unconditional models and for a variety of data\navailability settings.",
        "Rotational symmetry is a defining feature of many tomography systems,\nincluding computed tomography (CT) and emission computed tomography (ECT),\nwhere detectors are arranged in a circular or periodically rotating\nconfiguration. This study revisits the image reconstruction process from the\nperspective of hardware-induced rotational symmetry and introduces a cyclic\ngroup equivariance framework for deep learning-based reconstruction.\nSpecifically, we derive a mathematical correspondence that couples cyclic\nrotations in the projection domain to discrete rotations in the image domain,\nboth arising from the same cyclic group inherent in the hardware design. This\ninsight also reveals the uniformly distributed circular structure of the\nprojection space. Building on this principle, we provide a cyclic rotation\nequivariant convolution design method to preserve projection domain symmetry\nand a cyclic group equivariance regularization approach that enforces\nconsistent rotational transformations across the entire network. We further\nintegrate these modules into a domain transform reconstruction framework and\nvalidate them using digital brain phantoms, training on discrete models and\ntesting on more complex and realistic fuzzy variants. Results indicate markedly\nimproved generalization and stability, with fewer artifacts and better detail\npreservation, especially under data distribution deviation. These findings\nhighlight the potential of cyclic group equivariance as a unifying principle\nfor tomographic reconstruction in rotationally symmetric systems, offering a\nflexible and interpretable solution for scenarios with limited data.",
        "The present work is devoted to Computability Logic (CoL), the young and\nvolcanic research-project developed by Giorgi Japaridze. Our main goal is to\nprovide the reader with a clear panoramic view of this vast new land, starting\nfrom its core knots and making our way towards the outer threads, in a somewhat\nthree-dimensional, spacial gait. Furthermore, through the present work, we\nprovide a tentative proof for the decidability of one of CoL's numerous\naxiomatisations, namely CL15. Thus, our expedition initially takes off for an\naerial, perusal overview of this fertile steppe. The first chapter introduces\nCoL in a philosophical fashion, exposing and arguing its main key points. We\nthen move over to unfold its semantics and syntax profiles, allowing the reader\nto become increasingly more familiar with this new environment. Landing on to\nthe second chapter, we thoroughly introduce Cirquent Calculus, the new\ndeductive system Japaridze has developed in order to axiomatise Computability\nLogic. Indeed, this new proof-system can also be a useful tool for many other\nlogics. We then review each of the 17 axiomatisations found so far. The third\nchapter zooms-in on CL15, in order to come up with a possible solution to its\nopen problem. We outline its soundness and completeness proofs; then provide\nsome few deductive examples; and, finally, build a tentative proof of its\ndecidability. Lastly, the fourth chapter focuses on the potential and actual\napplications of Computability Logic, both in arithmetic (clarithmetic) and in\nArtificial Intelligence systems (meaning knowledgebase and planning-and-action\nones). We close our journey with some final remarks on the richness of this\nframework and, hence, the research-worthiness it entails.",
        "We present HuGDiffusion, a generalizable 3D Gaussian splatting (3DGS)\nlearning pipeline to achieve novel view synthesis (NVS) of human characters\nfrom single-view input images. Existing approaches typically require monocular\nvideos or calibrated multi-view images as inputs, whose applicability could be\nweakened in real-world scenarios with arbitrary and\/or unknown camera poses. In\nthis paper, we aim to generate the set of 3DGS attributes via a diffusion-based\nframework conditioned on human priors extracted from a single image.\nSpecifically, we begin with carefully integrated human-centric feature\nextraction procedures to deduce informative conditioning signals. Based on our\nempirical observations that jointly learning the whole 3DGS attributes is\nchallenging to optimize, we design a multi-stage generation strategy to obtain\ndifferent types of 3DGS attributes. To facilitate the training process, we\ninvestigate constructing proxy ground-truth 3D Gaussian attributes as\nhigh-quality attribute-level supervision signals. Through extensive\nexperiments, our HuGDiffusion shows significant performance improvements over\nthe state-of-the-art methods. Our code will be made publicly available.",
        "Recent advances in Large Language Models (LLMs) have motivated the\ndevelopment of general LLMs for molecular tasks. While several studies have\ndemonstrated that fine-tuned LLMs can achieve impressive benchmark\nperformances, they are far from genuine generalist molecular LLMs due to a lack\nof fundamental understanding of molecular structure. Specifically, when given\nmolecular task instructions, LLMs trained with naive next-token prediction\ntraining assign similar likelihood scores to both original and negatively\ncorrupted molecules, revealing their lack of molecular structure understanding\nthat is crucial for reliable and general molecular LLMs. To overcome this\nlimitation and obtain a true generalist molecular LLM, we introduce a novel\nmulti-modal training method based on a thorough multi-modal instruction tuning\nas well as a molecular structure preference optimization between chosen and\nrejected graphs. On various molecular benchmarks, the proposed generalist\nmolecular LLM, called Mol-LLM, achieves state-of-the-art performances among\ngeneralist LLMs on most tasks, at the same time, surpassing or comparable to\nstate-of-the-art specialist LLMs. Moreover, Mol-LLM also shows superior\ngeneralization performances in reaction prediction tasks, demonstrating the\neffect of the molecular structure understanding for generalization perspective.",
        "This paper introduces a novel causal framework for multi-stage\ndecision-making in natural language action spaces where outcomes are only\nobserved after a sequence of actions. While recent approaches like Proximal\nPolicy Optimization (PPO) can handle such delayed-reward settings in\nhigh-dimensional action spaces, they typically require multiple models (policy,\nvalue, and reward) and substantial training data. Our approach employs\nQ-learning to estimate Dynamic Treatment Regimes (DTR) through a single model,\nenabling data-efficient policy learning via gradient ascent on language\nembeddings. A key technical contribution of our approach is a decoding strategy\nthat translates optimized embeddings back into coherent natural language. We\nevaluate our approach on mental health intervention, hate speech countering,\nand sentiment transfer tasks, demonstrating significant improvements over\ncompetitive baselines across multiple metrics. Notably, our method achieves\nsuperior transfer strength while maintaining content preservation and fluency,\nas validated through human evaluation. Our work provides a practical foundation\nfor learning optimal policies in complex language tasks where training data is\nlimited.",
        "Spot-size converters are key for efficient coupling of light between\nwaveguides of different sizes. While adiabatic tapers are well suited for small\nsize differences, they become impractically long for expansion factors around\nx100 which are often required when coupling integrated waveguides and\nfree-space beams. Evanescent couplers and bragg deflectors can be used in this\nscenario, but their operation is inherently limited in bandwidth. Here we\npropose a solution based on a parabolic dielectric interface that couples light\nfrom a 0.5 um-wide waveguide to a 285 um-wide waveguide, i.e. an expansion\nfactor of x570. We experimentally demonstrate an unprecedented bandwidth of\nmore than 380 nm with insertion losses below 0.35 dB. We furthermore provide\nanalytical expressions for the design of such parabolic spot-size-converters\nfor arbitrary expansion factors.",
        "Gibbons and Schiller have raised the physically interesting conjecture that\nforces in general relativity are bounded from above by the mathematically\ncompact relation ${\\cal F}\\leq c^4\/4G$. In the present compact paper we\nexplicitly prove, using the non-linearly coupled Einstein-matter field\nequations, that the force function ${\\cal F}\\equiv 4\\pi r^2 p(r)$ in {\\it\nstable} self-gravitating horizonless matter configurations is characterized by\nthe upper bound ${\\cal F}\\leq c^4\/G$ [here $p(r)$ is the radial pressure inside\nthe self-gravitating matter configuration]."
      ]
    }
  }
]