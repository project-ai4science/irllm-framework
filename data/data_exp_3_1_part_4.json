[
  {
    "id":2412.00225,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"The Burgers equation",
    "start_abstract":"The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows.",
    "start_categories":[
      "math.AP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b17"
      ],
      "title":[
        "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"
      ],
      "abstract":[
        "We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Teach-to-Reason with Scoring: Self-Explainable Rationale-Driven\n  Multi-Trait Essay Scoring",
        "FlashSR: One-step Versatile Audio Super-resolution via Diffusion\n  Distillation",
        "Insights of Transitions to Thermoacoustic Instability in Inverse\n  Diffusion Flame using Multifractal Detrended Fluctuation Analysis",
        "Delta-WKV: A Novel Meta-in-Context Learner for MRI Super-Resolution",
        "AAS2RTO: Automated Alert Streams to Real-Time Observations: Preparing\n  for rapid follow-up of transient objects in the era of LSST",
        "\"Auntie, Please Don't Fall for Those Smooth Talkers\": How Chinese\n  Younger Family Members Safeguard Seniors from Online Fraud",
        "Complex Riemannian spacetime and singularity-free black holes and\n  cosmology",
        "Contrast-Free Myocardial Scar Segmentation in Cine MRI using Motion and\n  Texture Fusion",
        "Proof-Driven Clause Learning in Neural Network Verification",
        "The impact of baryons on the sparsity of simulated galaxy clusters from\n  The Three Hundred Project",
        "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching",
        "Efficient Language Modeling for Low-Resource Settings with Hybrid\n  RNN-Transformer Architectures",
        "Semi-supervised Anomaly Detection with Extremely Limited Labels in\n  Dynamic Graphs",
        "Quantifying the degree of hydrodynamic behaviour in heavy-ion collisions",
        "Funnelling super-resolution STED microscopy through multimode fibres",
        "QLIO: Quantized LiDAR-Inertial Odometry",
        "Geometric Iterative Approach for Efficient Inverse Kinematics and\n  Planning of Continuum Robots with a Floating Base Under Environment\n  Constraints",
        "Black Hole Evaporation in Loop Quantum Gravity",
        "FAIR: Facilitating Artificial Intelligence Resilience in Manufacturing\n  Industrial Internet",
        "Grasping in Uncertain Environments: A Case Study For Industrial Robotic\n  Recycling",
        "Implicit Communication in Human-Robot Collaborative Transport",
        "CENTS: Generating synthetic electricity consumption time series for rare\n  and unseen scenarios",
        "Thermal emission from bow shocks. III. Variable diffuse X-ray emission\n  from stellar-wind bow shocks driven by dynamical instabilities",
        "Computing the generalized plasma dispersion function for non-Maxwellian\n  plasmas, with applications to Thomson scattering",
        "Gaussian Random Fields as an Abstract Representation of Patient Metadata\n  for Multimodal Medical Image Segmentation",
        "Can You Move These Over There? An LLM-based VR Mover for Supporting\n  Object Manipulation",
        "Friction-Scaled Vibrotactile Feedback for Real-Time Slip Detection in\n  Manipulation using Robotic Sixth Finger",
        "Estimating Multi-chirp Parameters using Curvature-guided Langevin Monte\n  Carlo",
        "Data-Driven Distributionally Robust Mixed-Integer Control through Lifted\n  Control Policy"
      ],
      "abstract":[
        "Multi-trait automated essay scoring (AES) systems provide a fine-grained\nevaluation of an essay's diverse aspects. While they excel in scoring, prior\nsystems fail to explain why specific trait scores are assigned. This lack of\ntransparency leaves instructors and learners unconvinced of the AES outputs,\nhindering their practical use. To address this, we propose a self-explainable\nRationale-Driven Multi-trait automated Essay scoring (RaDME) framework. RaDME\nleverages the reasoning capabilities of large language models (LLMs) by\ndistilling them into a smaller yet effective scorer. This more manageable\nstudent model is optimized to sequentially generate a trait score followed by\nthe corresponding rationale, thereby inherently learning to select a more\njustifiable score by considering the subsequent rationale during training. Our\nfindings indicate that while LLMs underperform in direct AES tasks, they excel\nin rationale generation when provided with precise numerical scores. Thus,\nRaDME integrates the superior reasoning capacities of LLMs into the robust\nscoring accuracy of an optimized smaller model. Extensive experiments\ndemonstrate that RaDME achieves both accurate and adequate reasoning while\nsupporting high-quality multi-trait scoring, significantly enhancing the\ntransparency of AES.",
        "Versatile audio super-resolution (SR) is the challenging task of restoring\nhigh-frequency components from low-resolution audio with sampling rates between\n4kHz and 32kHz in various domains such as music, speech, and sound effects.\nPrevious diffusion-based SR methods suffer from slow inference due to the need\nfor a large number of sampling steps. In this paper, we introduce FlashSR, a\nsingle-step diffusion model for versatile audio super-resolution aimed at\nproducing 48kHz audio. FlashSR achieves fast inference by utilizing diffusion\ndistillation with three objectives: distillation loss, adversarial loss, and\ndistribution-matching distillation loss. We further enhance performance by\nproposing the SR Vocoder, which is specifically designed for SR models\noperating on mel-spectrograms. FlashSR demonstrates competitive performance\nwith the current state-of-the-art model in both objective and subjective\nevaluations while being approximately 22 times faster.",
        "The inverse diffusion flame (IDF) can experience thermoacoustic instability\ndue to variations in power input or flow conditions. However, the dynamical\ntransitions in IDF that lead to this instability when altering control\nparameters have not been thoroughly investigated. In this study, we explore the\ncontrol parameters through two different approaches and employ multifractal\ndetrended fluctuation analysis to characterize the transitions observed prior\nto the onset of thermoacoustic instability in the inverse diffusion flame. Our\nfindings reveal a loss of multifractality near the region associated with\nthermoacoustic instability, which suggests a more ordered behavior. We\ndetermine that the singularity exponent, the width of the multifractal\nspectrum, and the Hurst exponent are reliable indicators of thermoacoustic\ninstability and serve as effective classifiers of dynamical states in inverse\ndiffusion flames.",
        "Magnetic Resonance Imaging (MRI) Super-Resolution (SR) addresses the\nchallenges such as long scan times and expensive equipment by enhancing image\nresolution from low-quality inputs acquired in shorter scan times in clinical\nsettings. However, current SR techniques still have problems such as limited\nability to capture both local and global static patterns effectively and\nefficiently. To address these limitations, we propose Delta-WKV, a novel MRI\nsuper-resolution model that combines Meta-in-Context Learning (MiCL) with the\nDelta rule to better recognize both local and global patterns in MRI images.\nThis approach allows Delta-WKV to adjust weights dynamically during inference,\nimproving pattern recognition with fewer parameters and less computational\neffort, without using state-space modeling. Additionally, inspired by\nReceptance Weighted Key Value (RWKV), Delta-WKV uses a quad-directional\nscanning mechanism with time-mixing and channel-mixing structures to capture\nlong-range dependencies while maintaining high-frequency details. Tests on the\nIXI and fastMRI datasets show that Delta-WKV outperforms existing methods,\nimproving PSNR by 0.06 dB and SSIM by 0.001, while reducing training and\ninference times by over 15\\%. These results demonstrate its efficiency and\npotential for clinical use with large datasets and high-resolution imaging.",
        "The upcoming Vera C. Rubin Legacy Survey of Space and Time (LSST) will\ndiscover tens of thousands of astrophysical transients per night, far outpacing\navailable spectroscopic follow-up capabilities. Carefully prioritising\ncandidates for follow-up observations will maximise the scientific return from\nsmall telescopes with a single-object spectrograph. We introduce AAS2RTO, an\nastrophysical transient candidate prioritisation tool written in Python.\nAAS2RTO is flexible in that any number of criteria that consider observed\nproperties of transients can be implemented. The visibility of candidates from\na given observing site is also considered. The prioritised list of candidates\nprovided by AAS2RTO is continually updated when new transient data are made\navailable. Therefore, it can be applied to observing campaigns with a wide\nvariety of scientific motivations. AAS2RTO uses a greedy algorithm to\nprioritise candidates. Candidates are represented by a single numerical value,\nor `score'. Scores are computed by constructing simple numerical factors which\nindividually consider the competing facets of a candidate which make it\nsuitable for follow-up observation. AAS2RTO is currently configured to work\nprimarily with photometric data from the Zwicky Transient Facility (ZTF),\ndistributed by certified LSST community brokers. We provide an example of how\nAAS2RTO can be used by defining a set of criteria to prioritise observations of\ntype Ia supernovae (SNe Ia) close to peak brightness, in preparation for\nobservations with the spectrograph at the Danish-1.54m telescope. Using a\nsample of archival alerts from ZTF, we evaluate the criteria we have designed\nto estimate the number of SNe Ia that we will be able to observe with a 1.5m\ntelescope. Finally, we evaluate the performance of our criteria when applied to\nmock LSST observations of SNe Ia.",
        "Online fraud substantially harms individuals and seniors are\ndisproportionately targeted. While family is crucial for seniors, little\nresearch has empirically examined how they protect seniors against fraud. To\naddress this gap, we employed an inductive thematic analysis of 124 posts and\n16,872 comments on RedNote (Xiaohongshu), exploring the family support\necosystem for senior-targeted online fraud in China. We develop a taxonomy of\nsenior-targeted online fraud from a familial perspective, revealing younger\nmembers often spot frauds hard for seniors to detect, such as unusual charges.\nYounger family members fulfill multiple safeguarding roles, including\npreventative measures, fraud identification, fraud persuasion, loss recovery,\nand education. They also encounter numerous challenges, such as seniors'\nrefusal of help and considerable mental and financial stress. Drawing on these,\nwe develop a conceptual framework to characterize family support in\nsenior-targeted fraud, and outline implications for researchers and\npractitioners to consider the broader stakeholder ecosystem and cultural\naspects.",
        "An approach is presented to address singularities in general relativity using\na complex Riemannian spacetime extension. We demonstrate how this method can be\napplied to both black hole and cosmological singularities, specifically\nfocusing on the Schwarzschild and Kerr black holes and the\nFriedmann-Lema\\^itre-Robertson-Walker (FLRW) Big Bang cosmology. By extending\nthe relevant coordinates into the complex plane and carefully choosing\nintegration contours, we show that it is possible to regularize these\nsingularities, resulting in physically meaningful, singularity-free solutions\nwhen projected back onto real spacetime. The removal of the singularity at the\nBig Bang allows for a bounce cosmology. This approach offers a potential bridge\nbetween classical general relativity and quantum gravity effects, suggesting a\nway to resolve longstanding issues in gravitational physics without requiring a\nfull theory of quantum gravity.",
        "Late gadolinium enhancement MRI (LGE MRI) is the gold standard for the\ndetection of myocardial scars for post myocardial infarction (MI). LGE MRI\nrequires the injection of a contrast agent, which carries potential side\neffects and increases scanning time and patient discomfort. To address these\nissues, we propose a novel framework that combines cardiac motion observed in\ncine MRI with image texture information to segment the myocardium and scar\ntissue in the left ventricle. Cardiac motion tracking can be formulated as a\nfull cardiac image cycle registration problem, which can be solved via deep\nneural networks. Experimental results prove that the proposed method can\nachieve scar segmentation based on non-contrasted cine images with comparable\naccuracy to LGE MRI. This demonstrates its potential as an alternative to\ncontrast-enhanced techniques for scar detection.",
        "The widespread adoption of deep neural networks (DNNs) requires efficient\ntechniques for safety verification. Existing methods struggle to scale to\nreal-world DNNs, and tremendous efforts are being put into improving their\nscalability. In this work, we propose an approach for improving the scalability\nof DNN verifiers using Conflict-Driven Clause Learning (CDCL) -- an approach\nthat has proven highly successful in SAT and SMT solving. We present a novel\nalgorithm for deriving conflict clauses using UNSAT proofs, and propose several\noptimizations for expediting it. Our approach allows a modular integration of\nSAT solvers and DNN verifiers, and we implement it on top of an interface\ndesigned for this purpose. The evaluation of our implementation over several\nbenchmarks suggests a 2X--3X improvement over a similar approach, with specific\ncases outperforming the state of the art.",
        "Measurements of the sparsity of galaxy clusters can be used to probe the\ncosmological information encoded in the host dark matter halo profile, and\ninfer constraints on the cosmological model parameters. Key to the success of\nthese analyses is the control of potential sources of systematic uncertainty.\nAs an example, the presence of baryons can alter the cluster sparsity with\nrespect to predictions from N-body simulations. Similarly, a radial dependent\nmass bias, as in the case of masses inferred under the hydrostatic equilibrium\n(HE) hypothesis, can affect sparsity estimates. We examine the imprint of\nbaryonic processes on the sparsity statistics. Then, we investigate the\nrelation between cluster sparsities and gas mass fraction. Finally, we perform\na study of the impact of HE mass bias on sparsity measurements and the\nimplication on cosmological parameter inference analyses. We use catalogues of\nsimulated galaxy clusters from The Three Hundred project and run a comparative\nanalysis of the sparsity of clusters from N-body\/hydro simulations implementing\ndifferent feedback model scenarios. Sparsities which probe the mass profile\nacross a large radial range are affected by the presence of baryons in a way\nthat is particularly sensitive to astrophysical feedback, whereas those probing\nexclusively external cluster regions are less affected. In the former case, we\nfind the sparsities to be moderately correlated with measurements of the gas\nfraction in the inner cluster regions. We infer constraints on $S_8$ using\nsynthetic average sparsity measurements generated to evaluate the impact of\nbaryons, selection effects and HE bias. In the case of multiple sparsities\nthese lead to highly bias results. Hence, we calibrate linear bias models that\nenable us to correct for these effects and recover unbiased constraints that\nare significantly tighter than those inferred from single sparsity analyses.",
        "Recent advances in large language models have demonstrated remarkable\nreasoning capabilities through Chain of Thought (CoT) prompting, but often at\nthe cost of excessive verbosity in their intermediate outputs, which increases\ncomputational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting\nframework that combines cognitive-inspired reasoning paradigms with linguistic\nconstraints to minimize token usage while preserving reasoning accuracy. SoT is\ndesigned as a flexible framework that can incorporate any custom reasoning\nparadigms based on cognitive science, and we instantiate it with three such\nparadigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each\ntailored to different reasoning tasks and selected dynamically via a\nlightweight routing model. Through comprehensive evaluation across 15 reasoning\ndatasets with multiple languages and multimodal scenarios, we demonstrate that\nSoT achieves token reductions of 76% with negligible accuracy impact. In\ncertain domains like mathematical and multi-hop reasoning, it even improves\naccuracy while using significantly fewer tokens. Our code is publicly\navailable: https:\/\/www.github.com\/SimonAytes\/SoT.",
        "Transformer-based language models have recently been at the forefront of\nactive research in text generation. However, these models' advances come at the\nprice of prohibitive training costs, with parameter counts in the billions and\ncompute requirements measured in petaflop\/s-decades. In this paper, we\ninvestigate transformer-based architectures for improving model performance in\na low-data regime by selectively replacing attention layers with feed-forward\nand quasi-recurrent neural network layers. We test these architectures on the\nstandard Enwik8 and Wikitext-103 corpora. Our results show that our reduced\narchitectures outperform existing models with a comparable number of\nparameters, and obtain comparable performance to larger models while\nsignificantly reducing the number of parameters.",
        "Semi-supervised graph anomaly detection (GAD) has recently received\nincreasing attention, which aims to distinguish anomalous patterns from graphs\nunder the guidance of a moderate amount of labeled data and a large volume of\nunlabeled data. Although these proposed semi-supervised GAD methods have\nachieved great success, their superior performance will be seriously degraded\nwhen the provided labels are extremely limited due to some unpredictable\nfactors. Besides, the existing methods primarily focus on anomaly detection in\nstatic graphs, and little effort was paid to consider the continuous evolution\ncharacteristic of graphs over time (dynamic graphs). To address these\nchallenges, we propose a novel GAD framework (EL$^{2}$-DGAD) to tackle anomaly\ndetection problem in dynamic graphs with extremely limited labels.\nSpecifically, a transformer-based graph encoder model is designed to more\neffectively preserve evolving graph structures beyond the local neighborhood.\nThen, we incorporate an ego-context hypersphere classification loss to classify\ntemporal interactions according to their structure and temporal neighborhoods\nwhile ensuring the normal samples are mapped compactly against anomalous data.\nFinally, the above loss is further augmented with an ego-context contrasting\nmodule which utilizes unlabeled data to enhance model generalization. Extensive\nexperiments on four datasets and three label rates demonstrate the\neffectiveness of the proposed method in comparison to the existing GAD methods.",
        "Exploiting the first measurements of the same ion species in O+O collisons at\nRHIC and LHC, we propose an experimentally accessible observable to distinguish\nwhether collective behaviour builds up through a hydrodynamic expansion of a\nstrongly interacting QGP or through few rescatterings in a non-equilibrated\ndilute medium. Our procedure allows to disentangle the effects of the initial\nstate geometry and the dynamical response mechanism on the total resulting\nanisotropic flow. We validate the ability of our proposed observable to\ndiscriminate between systems with different interaction rates using results\nfrom event-by-event simulations in kinetic theory in the Relaxation Time\nApproximation (RTA). As a proof of concept, we extract the degree of\nhydrodynamization for Pb+Pb collisions at LHC from experimental data.",
        "Holographic multimode fibre endoscopes have recently shown their ability to\nunveil and monitor deep brain structures with sub-micrometre resolution,\nestablishing themselves as a minimally-invasive technology with promising\napplications in neurobiology. In this approach, holographic control of the\ninput light field entering the multimode fibres is achieved by means of\nwavefront shaping, usually treating the fibre as a complex medium. In contrast\nto other unpredictable and highly scattering complex media, multimode fibres\nfeature symmetries and strong correlations between their input and output\nfields. Both step-index and graded-index multimode fibres offer a specific set\nof such correlations which, when appropriately leveraged, enable generating\nhigh-quality focused pulses with minimal intermodal dispersion. With this, we\nfunnelled pulsed super-resolution STED microscopy with time-gated detection\nthrough a custom multimode fibre probe, combining the correlations of both\nmultimode fibre types. We demonstrate resolution improvements over 3-times\nbeyond the diffraction limit and showcase its applicability in bioimaging. This\nwork provides not only a solution for delivering short pulses through\nstep-index multimode fibre segments but also marks a step towards bringing\nadvanced super-resolution imaging techniques with virtually no depth\nlimitations.",
        "LiDAR-Inertial Odometry (LIO) is widely used for autonomous navigation, but\nits deployment on Size, Weight, and Power (SWaP)-constrained platforms remains\nchallenging due to the computational cost of processing dense point clouds.\nConventional LIO frameworks rely on a single onboard processor, leading to\ncomputational bottlenecks and high memory demands, making real-time execution\ndifficult on embedded systems. To address this, we propose QLIO, a\nmulti-processor distributed quantized LIO framework that reduces computational\nload and bandwidth consumption while maintaining localization accuracy. QLIO\nintroduces a quantized state estimation pipeline, where a co-processor\npre-processes LiDAR measurements, compressing point-to-plane residuals before\ntransmitting only essential features to the host processor. Additionally, an\nrQ-vector-based adaptive resampling strategy intelligently selects and\ncompresses key observations, further reducing computational redundancy.\nReal-world evaluations demonstrate that QLIO achieves a 14.1% reduction in\nper-observation residual data while preserving localization accuracy.\nFurthermore, we release an open-source implementation to facilitate further\nresearch and real-world deployment. These results establish QLIO as an\nefficient and scalable solution for real-time autonomous systems operating\nunder computational and bandwidth constraints.",
        "Continuum robots with floating bases demonstrate exceptional operational\ncapabilities in confined spaces, such as those encountered in medical surgeries\nand equipment maintenance. However, developing low-cost solutions for their\nmotion and planning problems remains a significant challenge in this field.\nThis paper investigates the application of geometric iterative strategy methods\nto continuum robots, and proposes the algorithm based on an improved two-layer\ngeometric iterative strategy for motion planning. First, we thoroughly study\nthe kinematics and effective workspace of a multi-segment tendon-driven\ncontinuum robot with a floating base. Then, generalized iterative algorithms\nfor solving arbitrary-segment continuum robots are proposed based on a series\nof problems such as initial arm shape dependence exhibited by similar methods\nwhen applied to continuum robots. Further, the task scenario is extended to a\nfollow-the-leader task considering environmental factors, and further extended\nalgorithm are proposed. Simulation comparison results with similar methods\ndemonstrate the effectiveness of the proposed method in eliminating the initial\narm shape dependence and improving the solution efficiency and accuracy. The\nexperimental results further demonstrate that the method based on improved\ntwo-layer geometric iteration can be used for motion planning task of a\ncontinuum robot with a floating base, under an average deviation of about 4 mm\nin the end position, an average orientation deviation of no more than 1 degree,\nand the reduction of average number of iterations and time cost is 127.4\niterations and 72.6 ms compared with similar methods, respectively.",
        "The conference \\emph{Black Holes Inside and Out} marked the 50th anniversary\nof Hawking's seminal paper on black hole radiance. It was clear already from\nHawking's analysis that a proper quantum gravity theory would be essential for\na more complete understanding of the evaporation process. This task was\nundertaken in Loop Quantum Gravity (LQG) two decades ago and by now the\nliterature on the subject is quite rich. The goal of this contribution is to\nsummarize a mainstream perspective that has emerged. The intended audience is\nthe broader gravitational physics community, rather than quantum gravity\nexperts. Therefore, the emphasis is on conceptual issues, especially on the key\nfeatures that distinguish the LQG approach, and on concrete results that\nunderlie the paradigm that has emerged. This is \\emph{not} meant to be an\nexhaustive review. Rather, it is a broad-brush stroke portrait of the present\nstatus. Further details can be found in the references listed.",
        "Artificial intelligence (AI) systems have been increasingly adopted in the\nManufacturing Industrial Internet (MII). Investigating and enabling the AI\nresilience is very important to alleviate profound impact of AI system failures\nin manufacturing and Industrial Internet of Things (IIoT) operations, leading\nto critical decision making. However, there is a wide knowledge gap in defining\nthe resilience of AI systems and analyzing potential root causes and\ncorresponding mitigation strategies. In this work, we propose a novel framework\nfor investigating the resilience of AI performance over time under hazard\nfactors in data quality, AI pipelines, and the cyber-physical layer. The\nproposed method can facilitate effective diagnosis and mitigation strategies to\nrecover AI performance based on a multimodal multi-head self latent attention\nmodel. The merits of the proposed method are elaborated using an MII testbed of\nconnected Aerosol Jet Printing (AJP) machines, fog nodes, and Cloud with\ninference tasks via AI pipelines.",
        "Autonomous robotic grasping of uncertain objects in uncertain environments is\nan impactful open challenge for the industries of the future. One such industry\nis the recycling of Waste Electrical and Electronic Equipment (WEEE) materials,\nin which electric devices are disassembled and readied for the recovery of raw\nmaterials. Since devices may contain hazardous materials and their disassembly\ninvolves heavy manual labor, robotic disassembly is a promising venue. However,\nsince devices may be damaged, dirty and unidentified, robotic disassembly is\nchallenging since object models are unavailable or cannot be relied upon. This\ncase study explores grasping strategies for industrial robotic disassembly of\nWEEE devices with uncertain vision data. We propose three grippers and\nappropriate tactile strategies for force-based manipulation that improves\ngrasping robustness. For each proposed gripper, we develop corresponding\nstrategies that can perform effectively in different grasping tasks and\nleverage the grippers design and unique strengths. Through experiments\nconducted in lab and factory settings for four different WEEE devices, we\ndemonstrate how object uncertainty may be overcome by tactile sensing and\ncompliant techniques, significantly increasing grasping success rates.",
        "We focus on human-robot collaborative transport, in which a robot and a user\ncollaboratively move an object to a goal pose. In the absence of explicit\ncommunication, this problem is challenging because it demands tight implicit\ncoordination between two heterogeneous agents, who have very different sensing,\nactuation, and reasoning capabilities. Our key insight is that the two agents\ncan coordinate fluently by encoding subtle, communicative signals into actions\nthat affect the state of the transported object. To this end, we design an\ninference mechanism that probabilistically maps observations of joint actions\nexecuted by the two agents to a set of joint strategies of workspace traversal.\nBased on this mechanism, we define a cost representing the human's uncertainty\nover the unfolding traversal strategy and introduce it into a model predictive\ncontroller that balances between uncertainty minimization and efficiency\nmaximization. We deploy our framework on a mobile manipulator (Hello Robot\nStretch) and evaluate it in a within-subjects lab study (N=24). We show that\nour framework enables greater team performance and empowers the robot to be\nperceived as a significantly more fluent and competent partner compared to\nbaselines lacking a communicative mechanism.",
        "Recent breakthroughs in large-scale generative modeling have demonstrated the\npotential of foundation models in domains such as natural language, computer\nvision, and protein structure prediction. However, their application in the\nenergy and smart grid sector remains limited due to the scarcity and\nheterogeneity of high-quality data. In this work, we propose a method for\ncreating high-fidelity electricity consumption time series data for rare and\nunseen context variables (e.g. location, building type, photovoltaics). Our\napproach, Context Encoding and Normalizing Time Series Generation, or CENTS,\nincludes three key innovations: (i) A context normalization approach that\nenables inverse transformation for time series context variables unseen during\ntraining, (ii) a novel context encoder to condition any state-of-the-art\ntime-series generator on arbitrary numbers and combinations of context\nvariables, (iii) a framework for training this context encoder jointly with a\ntime-series generator using an auxiliary context classification loss designed\nto increase expressivity of context embeddings and improve model performance.\nWe further provide a comprehensive overview of different evaluation metrics for\ngenerative time series models. Our results highlight the efficacy of the\nproposed method in generating realistic household-level electricity consumption\ndata, paving the way for training larger foundation models in the energy domain\non synthetic as well as real-world data.",
        "X-ray emission from wind-driven bow shocks is both difficult to measure and\npredict, but may give important insights into the energy budget of the hot\nphase of the ISM by quantifying mixing at the interface between hot and warm\ngas phases. We investigate the effect of magnetic fields and numerical\nresolution on predicted X-ray emission and other observable properties of bow\nshocks, to study convergence properties and assess robustness of predicted\nobservables from simulations. A suite of 2D and 3D HD and MHD simulations of\nbow shocks were run and analysed to generate synthetic emission maps and light\ncurves in X-ray and infrared emission. Resolving the Kelvin-Helmholtz (KH)\ninstability at the wind-ISM contact discontinuity is crucial for obtaining\nconverged results and for predicting X-ray emission and the properties of the\nhot shocked wind. When sufficient spatial resolution is used, we measure time\nvariation of X-ray emission of at least an order of magnitude on a timescale\ncomparable to the advection timescale of the wake downstream from the bow\nshock. Good correspondence is found between 2D and 3D simulations with\ncomparable resolution, and 3D simulations can achieve the required resolution\nwith reasonable computing resources. Development of the KH instability is\ninhibited for shear flows parallel to the ISM magnetic field, compared with\nwhat is seen in the perpendicular direction, resulting in synthetic IR emission\nmaps of bow shocks that are smooth when seen from one perspective but show\nstrong distortions from another. Measuring the X-ray morphology and luminosity\nin bow shocks may be useful for constraining mixing and energy-transfer rates\nbetween hot and warm gas phases of the ISM. Dynamical instabilities at the\nwind-ISM interface are a crucial ingredient in determining the properties of\nthe hot-gas phase in stellar bow-shocks, in particular to capture its time\ndependence.",
        "Kinetic plasma studies often require computing integrals of the velocity\ndistribution over a complex-valued pole. The standard method is to solve the\nintegral in the complex plane using the Plemelj theorem, resulting in the\nstandard plasma dispersion function for Maxwellian plasmas. For non-Maxwellian\nplasmas, the Plemelj theorem does not generalize to an analytic form, and\ncomputational methods must be used. In this paper, a new computational method\nis developed to accurately integrate a non-Maxwellian velocity distribution\nover an arbitrary set of complex valued poles. This method works by keeping the\nintegration contour on the real line, and applying a trapezoid rule-like\nintegration scheme over all discretized intervals. In intervals containing a\npole, the velocity distribution is linearly interpolated, and the analytic\nresult for the integral over a linear function is used. The integration scheme\nis validated by comparing its results to the analytic plasma dispersion\nfunction for Maxwellian distributions. We then show the utility of this method\nby computing the Thomson scattering spectra for several non-Maxwellian\ndistributions: the kappa, super Gaussian, and toroidal distributions. Thomson\nscattering is a valuable plasma diagnostic tool for both laboratory and space\nplasmas, but the technique relies on fitting measured wave spectra to a forward\nmodel, which typically assumes Maxwellian plasmas. Therefore, this integration\nmethod can expand the capabilities of Thomson scatter diagnostics to regimes\nwhere the plasma is non-Maxwellian, including high energy density plasmas,\nfrictionally heated plasmas in the ionosphere, and plasmas with a substantial\nsuprathermal electron tail.",
        "The growing rate of chronic wound occurrence, especially in patients with\ndiabetes, has become a concerning trend in recent years. Chronic wounds are\ndifficult and costly to treat, and have become a serious burden on health care\nsystems worldwide. Chronic wounds can have devastating consequences for the\npatient, with infection often leading to reduced quality of life and increased\nmortality risk. Innovative deep learning methods for the detection and\nmonitoring of such wounds have the potential to reduce the impact to both\npatient and clinician. We present a novel multimodal segmentation method which\nallows for the introduction of patient metadata into the training workflow\nwhereby the patient data are expressed as Gaussian random fields. Our results\nindicate that the proposed method improved performance when utilising multiple\nmodels, each trained on different metadata categories. Using the Diabetic Foot\nUlcer Challenge 2022 test set, when compared to the baseline results\n(intersection over union = 0.4670, Dice similarity coefficient = 0.5908) we\ndemonstrate improvements of +0.0220 and +0.0229 for intersection over union and\nDice similarity coefficient respectively. This paper presents the first study\nto focus on integrating patient data into a chronic wound segmentation\nworkflow. Our results show significant performance gains when training\nindividual models using specific metadata categories, followed by average\nmerging of prediction masks using distance transforms. All source code for this\nstudy is available at:\nhttps:\/\/github.com\/mmu-dermatology-research\/multimodal-grf",
        "In our daily lives, we can naturally convey instructions for the spatial\nmanipulation of objects using words and gestures. Transposing this form of\ninteraction into virtual reality (VR) object manipulation can be beneficial. We\npropose VR Mover, an LLM-empowered solution that can understand and interpret\nthe user's vocal instruction to support object manipulation. By simply pointing\nand speaking, the LLM can manipulate objects without structured input. Our user\nstudy demonstrates that VR Mover enhances user usability, overall experience\nand performance on multi-object manipulation, while also reducing workload and\narm fatigue. Users prefer the proposed natural interface for broad movements\nand may complementarily switch to gizmos or virtual hands for finer\nadjustments. These findings are believed to contribute to design implications\nfor future LLM-based object manipulation interfaces, highlighting the potential\nfor more intuitive and efficient user interactions in VR environments.",
        "The integration of extra-robotic limbs\/fingers to enhance and expand motor\nskills, particularly for grasping and manipulation, possesses significant\nchallenges. The grasping performance of existing limbs\/fingers is far inferior\nto that of human hands. Human hands can detect onset of slip through tactile\nfeedback originating from tactile receptors during the grasping process,\nenabling precise and automatic regulation of grip force. The frictional\ninformation is perceived by humans depending upon slip happening between finger\nand object. Enhancing this capability in extra-robotic limbs or fingers used by\nhumans is challenging. To address this challenge, this paper introduces novel\napproach to communicate frictional information to users through encoded\nvibrotactile cues. These cues are conveyed on onset of incipient slip thus\nallowing users to perceive friction and ultimately use this information to\nincrease force to avoid dropping of object. In a 2-alternative forced-choice\nprotocol, participants gripped and lifted a glass under three different\nfrictional conditions, applying a normal force of 3.5 N. After reaching this\nforce, glass was gradually released to induce slip. During this slipping phase,\nvibrations scaled according to static coefficient of friction were presented to\nusers, reflecting frictional conditions. The results suggested an accuracy of\n94.53 p\/m 3.05 (mean p\/mSD) in perceiving frictional information upon lifting\nobjects with varying friction. The results indicate effectiveness of using\nvibrotactile feedback for sensory feedback, allowing users of extra-robotic\nlimbs or fingers to perceive frictional information. This enables them to\nassess surface properties and adjust grip force according to frictional\nconditions, enhancing their ability to grasp, manipulate objects more\neffectively.",
        "This paper considers the problem of estimating chirp parameters from a noisy\nmixture of chirps. While a rich body of work exists in this area, challenges\nremain when extending these techniques to chirps of higher order polynomials.\nWe formulate this as a non-convex optimization problem and propose a modified\nLangevin Monte Carlo (LMC) sampler that exploits the average curvature of the\nobjective function to reliably find the minimizer. Results show that our\nCurvature-guided LMC (CG-LMC) algorithm is robust and succeeds even in low SNR\nregimes, making it viable for practical applications.",
        "This paper investigates the finite-horizon distributionally robust\nmixed-integer control (DRMIC) of uncertain linear systems. However, deriving an\noptimal causal feedback control policy to this DRMIC problem is computationally\nformidable for most ambiguity sets. To address the computational challenge, we\npropose a novel distributionally robust lifted control policy (DR-LCP) method\nto derive a high-quality approximate solution to this DRMIC problem for a rich\nclass of Wasserstein metric-based ambiguity sets, including the Wasserstein\nambiguity set and its variants. In theory, we analyze the asymptotic\nperformance and establish a tight non-asymptotic bound of the proposed method.\nIn numerical experiments, the proposed DR-LCP method empirically demonstrates\nsuperior performance compared with existing methods in the literature."
      ]
    }
  },
  {
    "id":2412.00225,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
    "start_abstract":"We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "The Burgers equation"
      ],
      "abstract":[
        "The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows."
      ],
      "categories":[
        "math.AP"
      ]
    },
    "list":{
      "title":[
        "On the test properties of the Frobenius endomorphism",
        "Twinning in ferromagnetic Heusler Rh2MnSb epitaxial thin films",
        "Cepheids in spectroscopic binary systems -- current status and recent\n  discoveries",
        "Efficient Diffusion Posterior Sampling for Noisy Inverse Problems",
        "Biglobal resolvent analysis of separated flow over a NACA0012 airfoil",
        "Efficient $d$-ary Cuckoo Hashing at High Load Factors by Bubbling Up",
        "Liquidity Competition Between Brokers and an Informed Trader",
        "A plastic damage model with mixed isotropic-kinematic hardening for\n  low-cycle fatigue in 7020 aluminum",
        "A Fast Decoding Algorithm for Generalized Reed-Solomon Codes and\n  Alternant Codes",
        "Dirac-type condition for Hamilton-generated graphs",
        "A free boundary approach to the quasistatic evolution of debonding\n  models",
        "The Lagrangian approach to the compressible primitive equations",
        "Chemistry in the Galactic Center",
        "On complex eigenvalues of a real nonsymmetric matrix",
        "Robust triple-q magnetic order with trainable spin vorticity in\n  Na$_2$Co$_2$TeO$_6$",
        "Optimal properties of tensor product of B-bases",
        "Validate Quantum State Preparation Programs",
        "Linear-Time User-Level DP-SCO via Robust Statistics",
        "Score-Preserving Targeted Maximum Likelihood Estimation",
        "Tomography of the Ophiuchus Molecular Cloud with Velocity Features in\n  C$_2$H $N=1-0$ spectra: A Pilot Study of Coherent Sub-structures",
        "Regularity of $3$-Path Ideals of Trees and Unicyclic Graphs",
        "Highly Uniform Magnetic and Electronic Environment in\n  Non-Centrosymmetric Superconductor LaRhGe$_3$",
        "A varifold-type estimation for data sampled on a rectifiable set",
        "Numerical Aspects of the Tensor Product Multilevel Method for\n  High-dimensional, Kernel-based Reconstruction on Sparse Grids",
        "The Computational Advantage of Depth: Learning High-Dimensional\n  Hierarchical Functions with Gradient Descent",
        "Dislocation correlations in GaN epitaxial films revealed by EBSD and XRD",
        "Flux homomorphism and bilinear form constructed from Shelukhin's\n  quasimorphism",
        "Ranking dynamics of urban mobility",
        "Why a Bose-Einstein condensate cannot exist in a system of interacting\n  bosons at ultrahigh temperatures"
      ],
      "abstract":[
        "In this paper, we prove two theorems concerning the test properties of the\nFrobenius endomorphism over commutative Noetherian local rings of prime\ncharacteristic $p$. Our first theorem generalizes a result of Funk-Marley on\nthe vanishing of Ext and Tor modules, while our second theorem generalizes one\nof our previous results on maximal Cohen-Macaulay tensor products. In these\nearlier results, we replace $^{e}R$ with a more general module $^{e}M$, where\n$R$ is a Cohen-Macaulay ring, $M$ is a Cohen-Macaulay $R$-module with full\nsupport, and $^{e}M$ is the module viewed as an $R$-module via the $e$-th\niteration of the Frobenius endomorphism. We also provide examples and present\napplications of our results, yielding new characterizations of the regularity\nof local rings.",
        "Epitaxially grown full Heusler alloy of Rh2MnSb thin films were prepared for\nthe first time using DC magnetron sputtering. The films were deposited on MgO\n[001] substrates with a deposition temperature of 600{\\deg}C, 700{\\deg}C, and\n800{\\deg}C. We report the structural, morphological, optical, magneto-optical,\nand magnetic properties of the films with a 200 nm nominal thickness. The\ngrown-at-600{\\deg}C film was close to stoichiometric and exhibited L21 ordering\ntypical for Heusler alloys. The single-phase Rh2MnSb film had a tetragonal\nstructure with lattice parameters close to the bulk material. X-ray\nphotoelectron spectroscopy revealed the metallic character of the film free\nfrom contamination. The tetragonal films exhibited discernible regular twinning\nwith the majority of twin domains with the c-axis perpendicular to the surface\ndue to a substrate constraint. The twin formation was studied by atomic force\nand transmission electron microscopy and by X-ray diffraction. Magnetic\nmeasurements showed TC of about 220-275 K and saturation magnetization of about\n55 emu\/g, close to the bulk material. Magneto-optical Kerr effect measurements\nof the film prepared at 600 {\\deg}C affirmed paramagnetic behavior at room\ntemperature and suggested the half-metallic behavior. The observed properties\nhighlight the potential for further investigations of Rh2MnSb's thin films,\nfocusing on compositional and structural control.",
        "We present a summary of the current knowledge about Cepheids in binary\nsystems. We focus on the most recent findings and discoveries, such as the\nhighly increasing number of confirmed and candidate spectroscopic binary\nCepheids and the progress in determining their physical parameters. This\nincludes new and newly analyzed binary Cepheids in the Milky Way and Magellanic\nClouds. We will provide an update on the project to increase the number of the\nmost valuable Cepheids in double-lined binary (SB2) systems from six to more\nthan 100. To date, we have confirmed 60 SB2 systems, including detecting a\nsignificant orbital motion for 37. We identified systems with orbital periods\nup to five times shorter than the shortest period reported before and systems\nwith mass ratios significantly different from unity (suggesting past binary\ninteractions, including merger events). Both features are essential to\nunderstanding how multiplicity affects the formation and destruction of Cepheid\nprogenitors and how this influences global Cepheid properties. We will also\npresent nine new systems composed of two Cepheids. Only one such double Cepheid\nsystem was known before.",
        "The pretrained diffusion model as a strong prior has been leveraged to\naddress inverse problems in a zero-shot manner without task-specific\nretraining. Different from the unconditional generation, the measurement-guided\ngeneration requires estimating the expectation of clean image given the current\nimage and the measurement. With the theoretical expectation expression, the\ncrucial task of solving inverse problems is to estimate the noisy likelihood\nfunction at the intermediate image sample. Using the Tweedie's formula and the\nknown noise model, the existing diffusion posterior sampling methods perform\ngradient descent step with backpropagation through the pretrained diffusion\nmodel. To alleviate the costly computation and intensive memory consumption of\nthe backpropagation, we propose an alternative maximum-a-posteriori (MAP)-based\nsurrogate estimator to the expectation. With this approach and further density\napproximation, the MAP estimator for linear inverse problem is the solution to\na traditional regularized optimization, of which the loss comprises of data\nfidelity term and the diffusion model related prior term. Integrating the MAP\nestimator into a general denoising diffusion implicit model (DDIM)-like\nsampler, we achieve the general solving framework for inverse problems. Our\napproach highly resembles the existing $\\Pi$GDM without the manifold projection\noperation of the gradient descent direction. The developed method is also\nextended to nonlinear JPEG decompression. The performance of the proposed\nposterior sampling is validated across a series of inverse problems, where both\nVP and VE SDE-based pretrained diffusion models are taken into consideration.",
        "The effects of Reynolds number across $Re=1000$, $2500$, $5000$, and $10000$\non separated flow over a two-dimensional NACA0012 airfoil at an angle of attack\nof $\\alpha=14^\\circ$ are investigated through the biglobal resolvent analysis.\nWe identify modal structures and energy amplifications over a range of\nfrequency, spanwise wavenumber, and discount parameter, providing insights\nacross various timescales. Using temporal discounting, we find that the shear\nlayer dynamics dominates over short time horizons, while the wake dynamics\nbecomes the primary amplification mechanism over long time horizons. Spanwise\neffects also appear over long time horizon, sustained by low frequencies. At a\nfixed timescale, we investigate the influence of Reynolds number on response\nand forcing mode structures, as well as the energy gain over different\nfrequencies. Across all Reynolds numbers, the response modes shift from\nwake-dominated structures at low frequencies to shear layer-dominated\nstructures at higher frequencies. The frequency at which the dominant mechanism\nchanges is independent of the Reynolds number. The response mode structures\nshow similarities across different Reynolds numbers, with local streamwise\nwavelengths only depending on frequency. Comparisons at a different angle of\nattack ($\\alpha=9^\\circ$) show that the transition from wake to shear layer\ndynamics with increasing frequency only occurs if the unsteady flow is\nthree-dimensional. We also study the dominant frequencies associated with wake\nand shear layer dynamics across the angles of attack and Reynolds numbers, and\npresent the characteristic scaling for each mechanism.",
        "A $d$-ary cuckoo hash table is an open-addressed hash table that stores each\nkey $x$ in one of $d$ random positions $h_1(x), h_2(x), \\ldots, h_d(x)$. In the\noffline setting, where all items are given and keys need only be matched to\nlocations, it is possible to support a load factor of $1 - \\epsilon$ while\nusing $d = \\lceil \\ln \\epsilon^{-1} + o(1) \\rceil$ hashes. The online setting,\nwhere keys are moved as new keys arrive sequentially, has the additional\nchallenge of the time to insert new keys, and it has not been known whether one\ncan use $d = O(\\ln \\epsilon^{-1})$ hashes to support $\\poly(\\epsilon^{-1})$\nexpected-time insertions.\n  In this paper, we introduce bubble-up cuckoo hashing, an implementation of\n$d$-ary cuckoo hashing that achieves all of the following properties\nsimultaneously:\n  (1) uses $d = \\lceil \\ln \\epsilon^{-1} + \\alpha \\rceil$ hash locations per\nitem for an arbitrarily small positive constant $\\alpha$.\n  (2) achieves expected insertion time $O(\\delta^{-1})$ for any insertion\ntaking place at load factor $1 - \\delta \\le 1 - \\epsilon$.\n  (3) achieves expected positive query time $O(1)$, independent of $d$ and\n$\\epsilon$.\n  The first two properties give an essentially optimal value of $d$ without\ncompromising insertion time. The third property is interesting even in the\noffline setting: it says that, even though \\emph{negative} queries must take\ntime $d$, positive queries can actually be implemented in $O(1)$ expected time,\neven when $d$ is large.",
        "We study a multi-agent setting in which brokers transact with an informed\ntrader. Through a sequential Stackelberg-type game, brokers manage trading\ncosts and adverse selection with an informed trader. In particular, supplying\nliquidity to the informed traders allows the brokers to speculate based on the\nflow information. They simultaneously attempt to minimize inventory risk and\ntrading costs with the lit market based on the informed order flow, also known\nas the internalization-externalization strategy. We solve in closed form for\nthe trading strategy that the informed trader uses with each broker and propose\na system of equations which classify the equilibrium strategies of the brokers.\nBy solving these equations numerically we may study the resulting strategies in\nequilibrium. Finally, we formulate a competitive game between brokers in order\nto determine the liquidity prices subject to precommitment supplied to the\ninformed trader and provide a numerical example in which the resulting\nequilibrium is not Pareto efficient.",
        "The paper at hand presents an in-depth investigation into the fatigue\nbehavior of the high-strength aluminum alloy EN AW-7020 T6 using both\nexperimental and numerical approaches. Two types of specimens are investigated:\na dog-bone specimen subjected to cyclic loading in a symmetric\nstrain-controlled regime, and a compact tension specimen subjected to repeated\nloading and unloading, which leads to damage growth from the notch tip.\nExperimental data from these tests are used to identify the different phases of\nfatigue. Subsequently, a plastic-damage model is developed, incorporating J2\nplasticity with Chaboche-type mixed isotropic-kinematic hardening. A detailed\ninvestigation reveals that the Chaboche model must be blended with a suitable\nisotropic hardening and combined with a proper damage growth model to\naccurately describe cyclic fatigue including large plastic strains up to\nfailure. Multiple back-stress components with independent properties are\nsuperimposed, and exponential isotropic hardening with saturation effects is\nintroduced to improve alignment with experimental results. For damage,\ndifferent stress splits are tested, with the deviatoric\/volumetric split\nproving successful in reproducing the desired degradation in peak stress and\nstiffness. A nonlinear activation function is introduced to ensure smooth\ntransitions between tension and compression. Two damage indices, one for the\ndeviatoric part and one for the volumetric part, are defined, each of which is\ngoverned by a distinct trilinear damage growth function. The governing\ndifferential equation of the problem is regularized by higher-order gradient\nterms to address the ill-posedness induced by softening. Finally, the\nplasticity model is calibrated using finite element simulations of the dog-bone\ntest and subsequently applied to the cyclic loading of the compact tension\nspecimen.",
        "In this paper, it is shown that the syndromes of generalized Reed-Solomon\n(GRS) codes and alternant codes can be characterized in terms of inverse fast\nFourier transform, regardless of code definitions. Then a fast decoding\nalgorithm is proposed, which has a computational complexity of $O(n\\log(n-k) +\n(n-k)\\log^2(n-k))$ for all $(n,k)$ GRS codes and $(n,k)$ alternant codes.\nParticularly, this provides a new decoding method for Goppa codes, which is an\nimportant subclass of alternant codes. When decoding the binary Goppa code with\nlength $8192$ and correction capability $128$, the new algorithm is nearly 10\ntimes faster than traditional methods. The decoding algorithm is suitable for\nthe McEliece cryptosystem, which is a candidate for post-quantum cryptography\ntechniques.",
        "The cycle space $\\mathcal{C}(G)$ of a graph $G$ is defined as the linear\nspace spanned by all cycles in $G$. For an integer $k\\ge 3$, let $\\mathcal{C}_k\n(G)$ denote the subspace of $\\mathcal{C}(G)$ generated by the cycles of length\nexactly $k$. A graph $G$ on $n$ vertices is called Hamilton-generated if\n$\\mathcal{C}_n (G) = \\mathcal{C}(G)$, meaning every cycle in $G$ is a symmetric\ndifference of some Hamilton cycles of $G$. %A necessary condition for this\nproperty is that $n$ must be odd. Heinig (European J. Combin., 2014) showed\nthat for any $\\sigma >0$ and sufficiently large odd $n$, every $n$-vertex graph\nwith minimum degree $(1+ \\sigma)n\/2$ is Hamilton-generated. He further posed\nthe question that whether the minimum degree requirement could be lowered to\nthe Dirac threshold $n\/2$. Recent progress by Christoph, Nenadov, and\nPetrova~(arXiv:2402.01447) reduced the minimum degree condition to $n\/2 + C$\nfor some large constant $C$. In this paper, we resolve Heinig's problem\ncompletely by proving that for sufficiently large odd $n$, every\nHamilton-connected graph $G$ on $n$ vertices with minimum degree at least\n$(n-1)\/2$ is Hamilton-generated. Moreover, this result is tight for the minimum\ndegree and the Hamilton-connected condition. The proof relies on the\nparity-switcher technique introduced by Christoph, et al in their recent work,\nas well as a classification lemma that strengthens a previous result by\nKrivelevich, Lee, and Sudakov~(Trans. Amer. Math. Soc., 2014).",
        "The mechanical process of progressively debonding an adhesive membrane from a\nsubstrate is described as a quasistatic variational evolution of sets and\nherein investigated. Existence of energetic solutions, based on global\nminimisers of a suitable functional together with an energy balance, is\nobtained within the natural class of open sets, improving and simplifying\nprevious results known in literature. The proposed approach relies on an\nequivalent reformulation of the model in terms of the celebrated one-phase\nBernoulli free boundary problem. This point of view allows performing the\nMinimizing Movements scheme in spaces of functions instead of the more\ncomplicated framework of sets. Nevertheless, in order to encompass\nirreversibility of the phenomenon, it remains crucial to keep track of the\ndebonded region at each discrete time-step, thus actually resulting in a\ncoupled algorithm.",
        "This article develops the hydrostatic Lagrangian approach to the compressible\nprimitive equations. A fundamental aspect in the analysis is the investigation\nof the compressible hydrostatic Lam\\'{e} and Stokes operators. Local strong\nwell-posedness for large data and global strong well-posedness for small data\nare established under various assumptions on the pressure law, both in the\npresence and absence of gravity.",
        "Gas and dust in the Galactic Center are subjected to energetic processing by\nintense UV radiation fields, widespread shocks, enhanced rates of cosmic-rays\nand X-rays, and strong magnetic fields. The Giant Molecular Clouds in the\nGalactic Center present a rich chemistry in a wide variety of chemical\ncompounds, some of which are prebiotic. We have conducted unbiased,\nultrasensitive and broadband spectral surveys toward the G+0.693-0.027\nmolecular cloud located in the Galactic Center, which have yielded the\ndiscovery of new complex organic molecules proposed as precursors of the\n\"building blocks\" of life. I will review our current understanding of the\nchemistry in Galactic Center molecular clouds, and summarize the recent\ndetections toward G+0.693-0.027 of key precursors of prebiotic chemistry. All\nthis suggests that the ISM is an important source of prebiotic material that\ncould have contributed to the process of the origin of life on Earth and\nelsewhere in the Universe.",
        "We consider real non-symmetric matrices and their factorisation as a product\nof real symmetric matrices. The number of complex eigenvalues of the original\nmatrix reveals restrictions on such factorisations as we shall prove.",
        "Recent studies suggest that the candidate Kitaev magnet Na$_2$Co$_2$TeO$_6$\npossesses novel triple-$\\mathbf{q}$ magnetic order instead of conventional\nsingle-$\\mathbf{q}$ zigzag order. Here we present dedicated experiments in\nsearch for distinct properties expected of the triple-$\\mathbf{q}$ order,\nnamely, insensitivity of the magnetic domains to weak $C_3$ symmetry-breaking\nfields and fictitious magnetic fields generated by the spin vorticity. In\nstructurally pristine single crystals, we show that $C_3$ symmetry-breaking\nin-plane uniaxial strains do not affect the order's magnetic neutron\ndiffraction signals. We further show that $\\mathbf{c}$-axis propagating light\nexhibits large Faraday rotations in the ordered state due to the spin\nvorticity, the sign of which can be trained via the system's ferrimagnetic\nmoment. These results are in favor of the triple-$\\mathbf{q}$ order in\nNa$_2$Co$_2$TeO$_6$ and reveal its unique emerging behavior.",
        "It is proved the optimal conditioning for the infinity norm of collocation\nmatrices of the tensor product of normalized B-bases among the tensor product\nof all normalized totally positive bases of the corresponding space of\nfunctions. Bounds for the minimal eigenvalue and singular value and\nillustrative numerical examples are also included.",
        "One of the key steps in quantum algorithms is to prepare an initial quantum\nsuperposition state with different kinds of features. These so-called state\npreparation algorithms are essential to the behavior of quantum algorithms, and\ncomplicated state preparation algorithms are difficult to develop correctly and\neffectively. This paper presents Pqasm: a high-assurance framework implemented\nwith the Coq proof assistant, allowing us to certify our Pqasm tool to\ncorrectly reflect quantum program behaviors. The key in the framework is to\nreduce the program correctness assurance of a program containing a quantum\nsuperposition state to the program correctness assurance for the program state\nwithout superposition. The reduction allows the development of an effective\ntesting framework for testing quantum state preparation algorithm\nimplementations on a classical computer - considered to be a hard problem with\nno clear solution until this point. We utilize the QuickChick property-based\ntesting framework to test state preparation programs. We evaluated the\neffectiveness of our approach over 5 case studies implemented using Pqasm; such\ncases are not even simulatable in the current quantum simulators.",
        "User-level differentially private stochastic convex optimization (DP-SCO) has\ngarnered significant attention due to the paramount importance of safeguarding\nuser privacy in modern large-scale machine learning applications. Current\nmethods, such as those based on differentially private stochastic gradient\ndescent (DP-SGD), often struggle with high noise accumulation and suboptimal\nutility due to the need to privatize every intermediate iterate. In this work,\nwe introduce a novel linear-time algorithm that leverages robust statistics,\nspecifically the median and trimmed mean, to overcome these challenges. Our\napproach uniquely bounds the sensitivity of all intermediate iterates of SGD\nwith gradient estimation based on robust statistics, thereby significantly\nreducing the gradient estimation noise for privacy purposes and enhancing the\nprivacy-utility trade-off. By sidestepping the repeated privatization required\nby previous methods, our algorithm not only achieves an improved theoretical\nprivacy-utility trade-off but also maintains computational efficiency. We\ncomplement our algorithm with an information-theoretic lower bound, showing\nthat our upper bound is optimal up to logarithmic factors and the dependence on\n$\\epsilon$. This work sets the stage for more robust and efficient\nprivacy-preserving techniques in machine learning, with implications for future\nresearch and application in the field.",
        "Targeted maximum likelihood estimators (TMLEs) are asymptotically optimal\namong regular, asymptotically linear estimators. In small samples, however, we\nmay be far from \"asymptopia\" and not reap the benefits of optimality. Here we\npropose a variant (score-preserving TMLE; SP-TMLE) that leverages an initial\nestimator defined as the solution of a large number of possibly data-dependent\nscore equations. Instead of targeting only the efficient influence function in\nthe TMLE update to knock out the plug-in bias, we also target the\nalready-solved scores. Solving additional scores reduces the remainder term in\nthe von-Mises expansion of our estimator because these scores may come close to\nspanning higher-order influence functions. The result is an estimator with\nbetter finite-sample performance. We demonstrate our approach in simulation\nstudies leveraging the (relaxed) highly adaptive lasso (HAL) as our initial\nestimator. These simulations show that in small samples SP-TMLE has reduced\nbias relative to plug-in HAL and reduced variance relative to vanilla TMLE,\nblending the advantages of the two approaches. We also observe improved\nestimation of standard errors in small samples.",
        "The C$_2$H $N=1-0$ transition was used to investigate the possible line of\nsight sub-structures from the dense and optically thick in $^{13}$CO $J=1-0$\nregions in the Ophiuchus star forming molecular cloud. With a 0.2 K or lower\nnoise, multi-peak spectra were obtained and then used for identifying\nsub-structures. There are clues, e.g., the core velocity dispersion remains\nunchanged with the increasing scale that this cloud has a mild thickness in the\nline of sight direction and a large amount of overlapping CO cores, as\nexpected, at least two coherent layers have been found. The integrated\nintensity maps of these two layers are different in shape and morphology.\nInferred from the point velocity dispersion, one sub-structure with a thickness\nof $\\sim 1$ pc was found, while other substructures were more likely to be\nfragments.",
        "Let $G$ be a simple graph and $I_3(G)$ be its $3$-path ideal in the\ncorresponding polynomial ring $R$. In this article, we prove that for an\narbitrary graph $G$, $reg(R\/I_3(G))$ is bounded below by $2\\nu_3(G)$, where\n$\\nu_3(G)$ denotes the $3$-path induced matching number of $G$. We give a class\nof graphs, namely, trees for which the lower bound is attained. Also, for a\nunicyclic graph $G$, we show that $reg(R\/I_3(G))\\leq 2\\nu_3(G)+2$ and provide\nan example that shows that the given upper bound is sharp.",
        "We report the results of $^{139}$La NMR measurements in the\nnon-centrosymmetric superconductor LaRhGe$_3$. This material crystallizes in a\ntetragonal structure without inversion symmetry and exhibits type-I\nsuperconductivity below 385 mK. We observed remarkably sharp NMR signals,\nindicating that the magnetic and electronic properties of the sample are\nextremely uniform in LaRhGe$_3$ despite the complex crystal structure. Our NMR\nresults indicate that LaRhGe$_3$ is a weakly correlated semimetal in the normal\nstate.",
        "We investigate the inference of varifold structures in a statistical\nframework: assuming that we have access to i.i.d. samples in $\\mathbb{R}^n$\nobtained from an underlying $d$--dimensional shape $S$ endowed with a possibly\nnon uniform density $\\theta$, we propose and analyse an estimator of the\nvarifold structure associated to $S$. The shape $S$ is assumed to be piecewise\n$C^{1,a}$ in a sense that allows for a singular set whose small enlargements\nare of small $d$--dimensional measure. The estimators are kernel--based both\nfor infering the density and the tangent spaces and the convergence result\nholds for the bounded Lipschitz distance between varifolds, in expectation and\nin a noiseless model. The mean convergence rate involves the dimension $d$ of\n$S$, its regularity through $a \\in (0, 1]$ and the regularity of the density\n$\\theta$.",
        "This paper investigates the approximation of functions with finite smoothness\ndefined on domains with a Cartesian product structure. The recently proposed\ntensor product multilevel method (TPML) combines Smolyak's sparse grid method\nwith a kernel-based residual correction technique. The contributions of this\npaper are twofold. First, we present two improvements on the TPML that reduce\nthe computational cost of point evaluations compared to a naive implementation.\nSecond, we provide numerical examples that demonstrate the effectiveness and\ninnovation of the TPML.",
        "Understanding the advantages of deep neural networks trained by gradient\ndescent (GD) compared to shallow models remains an open theoretical challenge.\nWhile the study of multi-index models with Gaussian data in high dimensions has\nprovided analytical insights into the benefits of GD-trained neural networks\nover kernels, the role of depth in improving sample complexity and\ngeneralization in GD-trained networks remains poorly understood. In this paper,\nwe introduce a class of target functions (single and multi-index Gaussian\nhierarchical targets) that incorporate a hierarchy of latent subspace\ndimensionalities. This framework enables us to analytically study the learning\ndynamics and generalization performance of deep networks compared to shallow\nones in the high-dimensional limit. Specifically, our main theorem shows that\nfeature learning with GD reduces the effective dimensionality, transforming a\nhigh-dimensional problem into a sequence of lower-dimensional ones. This\nenables learning the target function with drastically less samples than with\nshallow networks. While the results are proven in a controlled training\nsetting, we also discuss more common training procedures and argue that they\nlearn through the same mechanisms. These findings open the way to further\nquantitative studies of the crucial role of depth in learning hierarchical\nstructures with deep networks.",
        "Correlations between dislocations in crystals reduce the elastic energy via\nscreening of the strain by the surrounding dislocations. We study the\ncorrelations of threading dislocations in GaN epitaxial films with dislocation\ndensities of 5x10^8 cm^-2 and 1.8x10^10 cm^-2 by X-ray diffraction (XRD) in\nreciprocal space and by high-resolution electron backscatter diffraction (EBSD)\nin real space, where the strain is derived from a cross-correlation analysis of\nthe Kikuchi patterns. The measured XRD curves and EBSD strain and rotation maps\nare compared with Monte Carlo simulations within one and the same model for the\ndislocation distributions. The screening of the dislocation strains is provided\nby creating pairs of dislocations with opposite Burgers vectors, with the mean\ndistance between dislocations in a pair equal to the screening distance. The\npairs overlap and cannot be distinguished as separate dipoles. The\nEBSD-measured autocorrelation functions of the strain and rotation components\nfollow the expected logarithmic law for distances smaller than the screening\ndistances and become zero for larger distances, which is confirmed by the Monte\nCarlo simulations. Screening distances of 2 \\textmu m and 0.3 \\textmu m are\nobtained for the samples with low and high dislocation densities, respectively.\nThe dislocation strain is thus screened by only 4 neighboring dislocations.\nHigh-resolution EBSD allows for a more precise determination of the screening\ndistances than from fits of the XRD curves. In addition, an anisotropic\nresolution of the EBSD measurements is observed and quantified.",
        "Given a closed connected symplectic manifold $(M,\\omega)$, we construct an\nalternating $\\mathbb{R}$-bilinear form\n$\\mathfrak{b}=\\mathfrak{b}_{\\mu_{\\mathrm{Sh}}}$ on the real first cohomology of\n$M$ from Shelukhin's quasimorphism $\\mu_{\\mathrm{Sh}}$. Here\n$\\mu_{\\mathrm{Sh}}$ is defined on the universal cover of the group of\nHamiltonian diffeomorphisms on $(M,\\omega)$. This bilinear form is invariant\nunder the symplectic mapping class group action, and $\\mathfrak{b}$ yields a\nconstraint on the fluxes of commuting two elements in the group of\nsymplectomorphisms on $(M,\\omega)$. These results might be seen as an analog of\nRousseau's result for an open connected symplectic manifold, where he recovered\nthe symplectic pairing from the Calabi homomorphism. Furthermore,\n$\\mathfrak{b}$ controls the extendability of Shelukhin's quasimorphisms, as\nwell as the triviality of a characteristic class of Reznikov. To construct\n$\\mathfrak{b}$, we build general machinery for a group $G$ of producing a\nreal-valued $\\mathbb{Z}$-bilinear form $\\mathfrak{b}_{\\mu}$ from a\n$G$-invariant quasimorphism $\\mu$ on the commutator subgroup of $G$.",
        "Human mobility, a pivotal aspect of urban dynamics, displays a profound and\nmultifaceted relationship with urban sustainability. Despite considerable\nefforts analyzing mobility patterns over decades, the ranking dynamics of urban\nmobility has received limited attention. This study aims to contribute to the\nfield by investigating changes in rank and size of hourly inflows to various\nlocations across 60 Chinese cities throughout the day. We find that the\nrank-size distribution of hourly inflows over the course of the day is stable\nacross cities. To uncover the microdynamics beneath the stable aggregate\ndistribution amidst shifting location inflows, we analyzed consecutive-hour\ninflow size and ranking variations. Our findings reveal a dichotomy: locations\nwith higher daily average inflow display a clear monotonic trend, with more\npronounced increases or decreases in consecutive-hour inflow. In contrast,\nranking variations exhibit a non-monotonic pattern, distinguished by the\nstability of not only the top and bottom rankings but also those in\nmoderately-inflowed locations. Finally, we compare ranking dynamics across\ncities using a ranking metric, the rank turnover. The results advance our\nunderstanding of urban mobility dynamics, providing a basis for applications in\nurban planning and traffic engineering.",
        "It is well known that a Bose-Einstein (BE) condensate of atoms exists in a\nsystem of interacting Bose atoms at $T\\lesssim T^{(i)}_{c}$, where\n$T^{(i)}_{c}$ is the BE condensation temperature of an ideal gas. It is also\ngenerally accepted that BE condensation is impossible at ``ultrahigh''\ntemperatures $T\\gg T^{(i)}_{c}$. While the latter property has been\ntheoretically proven for an ideal gas, no such proof exists for an interacting\nsystem, to our knowledge. In this paper, we propose an approximate mathematical\nproof for a finite, nonrelativistic, periodic system of $N$ spinless\ninteracting bosons. The key point is that, at $T\\gg T^{(i)}_{c}$, the main\ncontribution to the occupation number\n$N_{0}=\\frac{1}{Z}\\sum_{\\wp}e^{-E_{\\wp}\/k_{B}T}\\langle\n\\Psi_{\\wp}|\\hat{a}^{+}_{\\mathbf{0}}\\hat{a}_{\\mathbf{0}}|\\Psi_{\\wp}\\rangle$,\ncorresponding to atoms with zero momentum, originates from the states\ncontaining $N$ elementary quasiparticles. These states do not contain the BE\ncondensate of zero-momentum atoms, implying that an ultrahigh temperature\nshould ``blur'' such a condensate."
      ]
    }
  },
  {
    "id":2411.05456,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"MRI segmentation of the human brain: challenges, methods, and applications",
    "start_abstract":"Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "U-net: Convolutional networks for biomedical image segmentation"
      ],
      "abstract":[
        "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Robust Evidence for Declining Disruptiveness: Assessing the Role of\n  Zero-Backward-Citation Works",
        "Coresets for Robust Clustering via Black-box Reductions to Vanilla Case",
        "Thermal Radiation Force and Torque on Moving Nanostructures with\n  Anisotropic Optical Response",
        "Efficient Transformed Gaussian Process State-Space Models for\n  Non-Stationary High-Dimensional Dynamical Systems",
        "Wheel-GINS: A GNSS\/INS Integrated Navigation System with a Wheel-mounted\n  IMU",
        "Electron-Chiral Phonon Coupling, Crystal Angular Momentum, and Phonon\n  Chirality",
        "Exploring the Hidden Reasoning Process of Large Language Models by\n  Misleading Them",
        "Characterizing the Burst Error Correction Ability of Quantum Cyclic\n  Codes",
        "Global Existence and Nonlinear Stability of Finite-Energy Solutions of\n  the Compressible Euler-Riesz Equations with Large Initial Data of Spherical\n  Symmetry",
        "Probing the hollowing transition of a shell-shaped BEC with collective\n  excitation",
        "Gender Dynamics in Software Engineering: Insights from Research on\n  Concurrency Bug Reproduction",
        "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time\n  Cognitive Task Solving and Reasoning in UAVs",
        "On connected subgraph arrangements",
        "Scale-wise Distillation of Diffusion Models",
        "Revealed Social Networks",
        "Derivation of the Gromeka Acceleration Vector for Dimensionless\n  Womersley Flow",
        "Anisotropic temperature-dependent lattice parameters and elastic\n  constants from first principles",
        "Tensor meson transition form factors in holographic QCD and the muon\n  $g-2$",
        "EXPRESS: An LLM-Generated Explainable Property Valuation System with\n  Neighbor Imputation",
        "Contrastive Similarity Learning for Market Forecasting: The ContraSim\n  Framework",
        "Pseudo-Hermitian physics from dynamically coupled macrospins",
        "Turan type inequalities for rational functions with pescribed poles and\n  restricted zeros",
        "Entanglement entropy evolution during gravitational collapse",
        "\"Who Has the Time?\": Understanding Receptivity to Health Chatbots among\n  Underserved Women in India",
        "Attention Distillation: A Unified Approach to Visual Characteristics\n  Transfer",
        "The Ultraviolet Problem in Supergravity",
        "Primordial Black Hole Hot Spots and Nucleosynthesis",
        "Separating the bulk and interface contribution of spin-orbit torque in\n  ferromagnet-Heavy metal bilayers tuned by variation of resistivity of heavy\n  metal",
        "Two-photon spectroscopy and a verification of the Kennard-Stepanov\n  relation in high-pressure two-species xenon-noble gas mixtures"
      ],
      "abstract":[
        "We respond to Holst et al.'s (HATWG) critique that the observed decline in\nscientific disruptiveness demonstrated in Park et al. (PLF) stems from\nincluding works with zero backward citations (0-bcites). Applying their own\nadvocated dataset, metric, and exclusion criteria, we demonstrate statistically\nand practically significant declines in disruptiveness that equal major\nbenchmark transformations in science. Notably, we show that HATWG's own\nregression model -- designed specifically to address their concerns about\n0-bcite works -- reveals highly significant declines for both papers (p<0.001)\nand patents (p<0.001), a finding they neither acknowledge nor interpret. Their\ncritique is undermined by methodological deficiencies, including reliance on\nvisual inspection without statistical assessment, and severe data quality\nissues in their SciSciNet dataset, which contains nearly three times more\n0-bcite papers than our original data. HATWG's departure from established\nscientometric practices -- notably their inclusion of document types and fields\nknown for poor metadata quality -- invalidates their conclusions. Monte Carlo\nsimulations and additional analyses using multiple disruptiveness measures\nacross datasets further validate the robustness of the declining trend. Our\nfindings collectively demonstrate that the observed decline in disruptiveness\nis not an artifact of 0-bcite works but represents a substantive change in\nscientific and technological innovation patterns.",
        "We devise $\\epsilon$-coresets for robust $(k,z)$-Clustering with $m$ outliers\nthrough black-box reductions to vanilla case. Given an $\\epsilon$-coreset\nconstruction for vanilla clustering with size $N$, we construct coresets of\nsize $N\\cdot \\mathrm{poly}\\log(km\\epsilon^{-1}) +\nO_z\\left(\\min\\{km\\epsilon^{-1}, m\\epsilon^{-2z}\\log^z(km\\epsilon^{-1})\n\\}\\right)$ for various metric spaces, where $O_z$ hides $2^{O(z\\log z)}$\nfactors. This increases the size of the vanilla coreset by a small\nmultiplicative factor of $\\mathrm{poly}\\log(km\\epsilon^{-1})$, and the additive\nterm is up to a $(\\epsilon^{-1}\\log (km))^{O(z)}$ factor to the size of the\noptimal robust coreset. Plugging in vanilla coreset results of [Cohen-Addad et\nal., STOC'21], we obtain the first coresets for $(k,z)$-Clustering with $m$\noutliers with size near-linear in $k$ while previous results have size at least\n$\\Omega(k^2)$ [Huang et al., ICLR'23; Huang et al., SODA'25].\n  Technically, we establish two conditions under which a vanilla coreset is as\nwell a robust coreset. The first condition requires the dataset to satisfy\nspecial structures - it can be broken into \"dense\" parts with bounded diameter.\nWe combine this with a new bounded-diameter decomposition that has only $O_z(km\n\\epsilon^{-1})$ non-dense points to obtain the $O_z(km \\epsilon^{-1})$ additive\nbound. Another condition requires the vanilla coreset to possess an extra\nsize-preserving property. We further give a black-box reduction that turns a\nvanilla coreset to the one satisfying the said size-preserving property,\nleading to the alternative $O_z(m\\epsilon^{-2z}\\log^{z}(km\\epsilon^{-1}))$\nadditive bound.\n  We also implement our reductions in the dynamic streaming setting and obtain\nthe first streaming algorithms for $k$-Median and $k$-Means with $m$ outliers,\nusing space $\\tilde{O}(k+m)\\cdot\\mathrm{poly}(d\\epsilon^{-1}\\log\\Delta)$ for\ninputs on the grid $[\\Delta]^d$.",
        "Nanoscale objects moving relative to a thermal radiation bath experience a\ndrag force due to the imbalance in their interaction with the blue- and\nredshifted components of the electromagnetic field. Here, we show that, in\naddition to this drag force, moving nanostructures with an anisotropic optical\nresponse experience a lateral force and a torque that substantially modify\ntheir trajectory. These phenomena emerge from the additional coupling between\nthe electromagnetic field components polarized parallel and perpendicular to\nthe trajectory, enabled by the anisotropic response of the nanostructure. This\nwork unveils the intricate dynamics of anisotropic nanostructures moving in a\nthermal radiation bath.",
        "Gaussian process state-space models (GPSSMs) have emerged as a powerful\nframework for modeling dynamical systems, offering interpretable uncertainty\nquantification and inherent regularization. However, existing GPSSMs face\nsignificant challenges in handling high-dimensional, non-stationary systems due\nto computational inefficiencies, limited scalability, and restrictive\nstationarity assumptions. In this paper, we propose an efficient transformed\nGaussian process state-space model (ETGPSSM) to address these limitations. Our\napproach leverages a single shared Gaussian process (GP) combined with\nnormalizing flows and Bayesian neural networks, enabling efficient modeling of\ncomplex, high-dimensional state transitions while preserving scalability. To\naddress the lack of closed-form expressions for the implicit process in the\ntransformed GP, we follow its generative process and introduce an efficient\nvariational inference algorithm, aided by the ensemble Kalman filter (EnKF), to\nenable computationally tractable learning and inference. Extensive empirical\nevaluations on synthetic and real-world datasets demonstrate the superior\nperformance of our ETGPSSM in system dynamics learning, high-dimensional state\nestimation, and time-series forecasting, outperforming existing GPSSMs and\nneural network-based methods in both accuracy and computational efficiency.",
        "A long-term accurate and robust localization system is essential for mobile\nrobots to operate efficiently outdoors. Recent studies have shown the\nsignificant advantages of the wheel-mounted inertial measurement unit\n(Wheel-IMU)-based dead reckoning system. However, it still drifts over extended\nperiods because of the absence of external correction signals. To achieve the\ngoal of long-term accurate localization, we propose Wheel-GINS, a Global\nNavigation Satellite System (GNSS)\/inertial navigation system (INS) integrated\nnavigation system using a Wheel-IMU. Wheel-GINS fuses the GNSS position\nmeasurement with the Wheel-IMU via an extended Kalman filter to limit the\nlong-term error drift and provide continuous state estimation when the GNSS\nsignal is blocked. Considering the specificities of the GNSS\/Wheel-IMU\nintegration, we conduct detailed modeling and online estimation of the\nWheel-IMU installation parameters, including the Wheel-IMU leverarm and\nmounting angle and the wheel radius error. Experimental results have shown that\nWheel-GINS outperforms the traditional GNSS\/Odometer\/INS integrated navigation\nsystem during GNSS outages. At the same time, Wheel-GINS can effectively\nestimate the Wheel-IMU installation parameters online and, consequently,\nimprove the localization accuracy and practicality of the system. The source\ncode of our implementation is publicly available\n(https:\/\/github.com\/i2Nav-WHU\/Wheel-GINS).",
        "We explicitly derive the wavefunctions of chiral phonons propagating along\nthe helical axis in chiral crystals and clarify the characteristics of\nelectron-phonon interactions in chiral helical crystals. In particular, we\nelucidate how the conservation of not only the crystal momentum (CM) but also\nthe crystal angular momentum (CAM) manifests in the interaction vertex. This\nformulation provides a microscopic framework for describing physical processes\ninvolving chiral phonons. Furthermore, we construct a phononic analogue of\nZilch, a known measure of chirality carried by light, and discuss its\nrelationship with phonon angular momentum.",
        "Large language models (LLMs) and Vision language models (VLMs) have been able\nto perform various forms of reasoning tasks in a wide range of scenarios, but\nare they truly engaging in task abstraction and rule-based reasoning beyond\nmere memorization and pattern matching? To answer this question, we propose a\nnovel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether\nLLMs\/VLMs perform abstract reasoning by altering their original understanding\nof fundamental rules. In particular, by constructing a dataset with math\nexpressions that contradict correct operation principles, we fine-tune the\nmodel to learn those contradictory rules and assess its generalization ability\non different test domains. Through a series of experiments, we find that\ncurrent LLMs\/VLMs are capable of effectively applying contradictory rules to\nsolve practical math word problems and math expressions represented by images,\nimplying the presence of an internal mechanism that abstracts before reasoning.",
        "Quantum burst error correction codes (QBECCs) are of great importance to deal\nwith the memory effect in quantum channels. As the most important family of\nQBECCs, quantum cyclic codes (QCCs) play a vital role in the correction of\nburst errors. In this work, we characterize the burst error correction ability\nof QCCs constructed from the Calderbank-Shor-Steane (CSS) and the Hermitian\nconstructions. We determine the burst error correction limit of QCCs and\nquantum Reed-Solomon codes with algorithms in polynomial-time complexities. As\na result, lots of QBECCs saturating the quantum Reiger bound are obtained. We\nshow that quantum Reed-Solomon codes have better burst error correction\nabilities than the previous results. At last, we give the quantum\nerror-trapping decoder (QETD) of QCCs for decoding burst errors. The decoder\nruns in linear time and can decode both degenerate and nondegenerate burst\nerrors. What's more, the numerical results show that QETD can decode much more\ndegenerate burst errors than the nondegenerate ones.",
        "The compressible Euler-Riesz equations are fundamental with wide applications\nin astrophysics, plasma physics, and mathematical biology. In this paper, we\nare concerned with the global existence and nonlinear stability of\nfinite-energy solutions of the multidimensional Euler-Riesz equations with\nlarge initial data of spherical symmetry. We consider both attractive and\nrepulsive interactions for a wide range of Riesz and logarithmic potentials for\ndimensions larger than or equal to two. This is achieved by the inviscid limit\nof the solutions of the corresponding Cauchy problem for the\nNavier-Stokes-Riesz equations. The strong convergence of the vanishing\nviscosity solutions is achieved through delicate uniform estimates in $L^p$. It\nis observed that, even if the attractive potential is super-Coulomb, no\nconcentration is formed near the origin in the inviscid limit. Moreover, we\nprove that the nonlinear stability of global finite-energy solutions for the\nEuler-Riesz equations is unconditional under a spherically symmetric\nperturbation around the steady solutions. Unlike the Coulomb case where the\npotential can be represented locally, the singularity and regularity of the\nnonlocal radial Riesz potential near the origin require careful analysis, which\nis a crucial step. Finally, unlike the Coulomb case, a Gr\\\"onwall type estimate\nis required to overcome the difficulty of the appearance of boundary terms in\nthe sub-Coulomb case and the singularity of the super-Coulomb potential.\nFurthermore, we prove the nonlinear stability of global finite-energy solutions\nfor the compressible Euler-Riesz equations around steady states by employing\nconcentration compactness arguments. Steady states properties are obtained by\nvariational arguments connecting to recent advances in aggregation-diffusion\nequations.",
        "We investigate the hollowing transition of a shell-shaped Bose-Einstein\ncondensate using collective excitations. The shell is created using an\nimmiscible dual-species BEC mixture, with its hollowness controlled by tuning\nthe repulsive interspecies interaction via a Feshbach resonance. Our results\nreveal two distinct monopole modes in which the two condensates oscillate\neither in-phase or out-of-phase. The spectrum of the out-of-phase mode exhibits\na non-monotonic dependence on the interspecies interaction, providing a clear\nsignature of the topology change from a filled to a hollow condensate.\nFurthermore, we find that the critical point of the hollowing transition\ndepends strongly on the number ratio of the two species. Our findings provide a\ndetailed understanding of the topology change in shell-shaped quantum gases and\npave the way for future study of quantum many-body phenomena in curved spaces.",
        "Reproducing concurrency bugs is a complex task due to their unpredictable\nbehavior. Researchers, regardless of gender, are contributing to automating\nthis complex task to aid software developers. While some studies have\ninvestigated gender roles in the broader software industry, limited research\nexists on gender representation specifically among researchers working in\nconcurrent bug reproduction. To address this gap, in this paper, we present a\nliterature review to assess the gender ratio in this field. We also explore\npotential variations in technique selection and bug-type focus across genders.\nOur findings indicate that female researchers are underrepresented compared to\ntheir male counterparts in this area, with a current male-to-female author\nratio of 29:6. Through this study, we emphasize the importance of fostering\ngender equity in software engineering research, ensuring a diversity of\nperspectives in the development of automated bug reproduction tools.",
        "This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA)\nmodel tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand\nadvanced cognitive abilities. Trained on a dataset comprising over 8,000\nsimulated flight trajectories across three key categories-Human Recognition,\nSymbol Understanding, and Reasoning-the model generates real-time 4D action\ncommands based on first-person visual inputs and textual instructions. To\nfurther enhance performance in intricate scenarios, we propose\nCognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM)\nreasoning module to simplify task directives prior to high-frequency control.\nExperimental evaluations using our open-source benchmark, CognitiveDroneBench,\nreveal that while a racing-oriented model (RaceVLA) achieves an overall success\nrate of 31.3%, the base CognitiveDrone model reaches 59.6%, and\nCognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate\nimprovements of up to 30% in critical cognitive tasks, underscoring the\neffectiveness of incorporating advanced reasoning capabilities into UAV control\nsystems. Our contributions include the development of a state-of-the-art VLA\nmodel for UAV control and the introduction of the first dedicated benchmark for\nassessing cognitive tasks in drone operations. The complete repository is\navailable at cognitivedrone.github.io",
        "Recently, Cuntz and K\\\"uhne introduced a particular class of hyperplane\narrangements stemming from a given graph $G$, so called connected subgraph\narrangements $A_G$. In this note we strengthen some of the result from their\nwork and prove new ones for members of this class. For instance, we show that\naspherical members withing this class stem from a rather restricted set of\ngraphs. Specifically, if $A_G$ is an aspherical connected subgraph arrangement,\nthen $A_G$ is free with the unique possible exception when the underlying graph\n$G$ is the complete graph on $4$ nodes.",
        "We present SwD, a scale-wise distillation framework for diffusion models\n(DMs), which effectively employs next-scale prediction ideas for\ndiffusion-based few-step generators. In more detail, SwD is inspired by the\nrecent insights relating diffusion processes to the implicit spectral\nautoregression. We suppose that DMs can initiate generation at lower data\nresolutions and gradually upscale the samples at each denoising step without\nloss in performance while significantly reducing computational costs. SwD\nnaturally integrates this idea into existing diffusion distillation methods\nbased on distribution matching. Also, we enrich the family of distribution\nmatching approaches by introducing a novel patch loss enforcing finer-grained\nsimilarity to the target distribution. When applied to state-of-the-art\ntext-to-image diffusion models, SwD approaches the inference times of two full\nresolution steps and significantly outperforms the counterparts under the same\ncomputation budget, as evidenced by automated metrics and human preference\nstudies.",
        "People are influenced by their peers when making decisions. In this paper, we\nstudy the linear-in-means model which is the standard empirical model of peer\neffects. As data on the underlying social network is often difficult to come\nby, we focus on data that only captures an agent's choices. Under exogenous\nagent participation variation, we study two questions. We first develop a\nrevealed preference style test for the linear-in-means model. We then study the\nidentification properties of the linear-in-means model. With sufficient\nparticipation variation, we show how an analyst is able to recover the\nunderlying network structure and social influence parameters from choice data.\nOur identification result holds when we allow the social network to vary across\ncontexts. To recover predictive power, we consider a refinement which allows us\nto extrapolate the underlying network structure across groups and provide a\ntest of this version of the model.",
        "This manuscript presents an analytical and theoretical investigation of the\nGromeka acceleration field in a dimensionless Womersley flow, derived through\nthe exact solution of the governing Navier-Stokes equations in phase space. By\ndecomposing the convective acceleration into rotational and nonrotational\ncomponents, the derivation highlights the dominant role of vorticity dynamics\nnear the wall, where steep velocity gradients interact with the oscillatory\naxial velocity to produce localized radial accelerations. The solution reveals\nthat the Gromeka acceleration mediates nonlinear interactions between\nharmonics, driving energy redistribution and boundary layer development under\nmulti-harmonic boundary conditions. Complementary analysis of the kinetic\nenergy gradient further delineates inertial effects, demonstrating their role\nin phase-dependent flow separation and reattachment. These findings provide a\ncomprehensive framework for understanding momentum transport and instability\ngeneration in pulsatile wall-bounded flows.",
        "The Quasi-harmonic Approximation (QHA) is a widely used method for\ncalculating the temperature dependence of lattice parameters and the thermal\nexpansion coefficients from first principles. However, applying QHA to\nanisotropic systems typically requires several dozens or even hundreds of\nphonon band structure calculations, leading to high computational costs. The\nZero Static Internal Stress Approximation (ZSISA) QHA method partly addresses\nsuch caveat, but the computational load of its implementation remains high, so\nthat its volumetric-only counterpart v-ZSISA-QHA is preferred. In this work, we\npresent an efficient implementation of the ZSISA-QHA, enabling its application\nacross a wide range of crystal structures under varying temperature (T) and\npressure (P) conditions. By incorporating second-order derivatives of the\nvibrational free energy with respect to lattice degrees of freedom, we\nsignificantly reduce the number of required phonon band structure calculations\nfor the determination of all lattice parameters and angles. For hexagonal,\ntrigonal, and tetragonal systems, only six phonon band structure calculations\nare needed, while 10, 15, and 28 calculations suffice for orthorhombic,\nmonoclinic, and triclinic systems, respectively. This method is tested for a\nvariety of non-cubic materials, from uniaxial ones like ZnO and CaCO3 to\nmonoclinic or triclinic materials such as ZrO2, HfO2, and Al2SiO5,\ndemonstrating a significant reduction in computational effort while maintaining\naccuracy in modeling anisotropic thermal expansion, unlike the v-ZSISA-QHA. The\nmethod is also applied to the first-principles calculation of\ntemperature-dependent elastic constants, with only up to six more phonon band\nstructure calculations, depending on the crystallographic system.",
        "Despite the prominence of tensor mesons in photon-photon collisions, until\nrecently their contribution to the hadronic light-by-light (HLBL) scattering\npart of the anomalous magnetic moment of the muon has been estimated at the\nlevel of only a few $10^{-12}$, with an almost negligible contribution to the\nerror budget of the Standard Model prediction. A recent reanalysis within the\ndispersive approach has found that after resolving the issue of kinematic\nsingularities in previous approaches, a larger result is obtained, a few\n$10^{-11}$, and with opposite sign as in previous results, when a simple quark\nmodel for the transition form factors is employed. In this paper, we present\nthe first complete evaluation of tensor meson contributions within a hard-wall\nmodel in holographic QCD, which reproduces surprisingly well mass, two-photon\nwidth, and the observed singly virtual transition form factors of the dominant\n$f_2(1270)$, requiring only that the energy-momentum tensor correlator is\nmatched to the leading OPE result of QCD. Due to a second structure function\nthat is absent in the quark model, the result for $a_\\mu$ turns out to be\npositive instead of negative, and also with a magnitude of a few $10^{-11}$. We\ndiscuss both pole and non-pole contributions arising from tensor meson\nexchanges in the holographic HLBL amplitude, finding that keeping all\ncontributions improves dramatically the convergence of a sum over excited\ntensor mesons and avoids unnaturally large contributions from the first few\nexcited modes at low energies. Moreover, we find that the infinite tower of\ntensor mesons permits to fill the gap in the symmetric longitudinal\nshort-distance constraint on the HLBL amplitude left by the contribution of\naxial vector mesons. Total $a_\\mu^\\mathrm{Tensor}$ contribution: $+12.4\\times\n10^{-11}$; with an $F_\\rho$ fit this is reduced slightly to $+11.1\\times\n10^{-11}$.",
        "The demand for property valuation has attracted significant attention from\nsellers, buyers, and customers applying for loans. Reviews of existing\napproaches have revealed shortcomings in terms of not being able to handle\nmissing value situations, as well as lacking interpretability, which means they\ncannot be used in real-world applications. To address these challenges, we\npropose an LLM-Generated EXplainable PRopErty valuation SyStem with neighbor\nimputation called EXPRESS, which provides the customizable missing value\nimputation technique, and addresses the opaqueness of prediction by providing\nthe feature-wise explanation generated by LLM. The dynamic nearest neighbor\nsearch finds similar properties depending on different application scenarios by\nproperty configuration set by users (e.g., house age as criteria for the house\nin rural areas, and locations for buildings in urban areas). Motivated by the\nhuman appraisal procedure, we generate feature-wise explanations to provide\nusers with a more intuitive understanding of the prediction results.",
        "We introduce the Contrastive Similarity Space Embedding Algorithm\n(ContraSim), a novel framework for uncovering the global semantic relationships\nbetween daily financial headlines and market movements. ContraSim operates in\ntwo key stages: (I) Weighted Headline Augmentation, which generates augmented\nfinancial headlines along with a semantic fine-grained similarity score, and\n(II) Weighted Self-Supervised Contrastive Learning (WSSCL), an extended version\nof classical self-supervised contrastive learning that uses the similarity\nmetric to create a refined weighted embedding space. This embedding space\nclusters semantically similar headlines together, facilitating deeper market\ninsights. Empirical results demonstrate that integrating ContraSim features\ninto financial forecasting tasks improves classification accuracy from WSJ\nheadlines by 7%. Moreover, leveraging an information density analysis, we find\nthat the similarity spaces constructed by ContraSim intrinsically cluster days\nwith homogeneous market movement directions, indicating that ContraSim captures\nmarket dynamics independent of ground truth labels. Additionally, ContraSim\nenables the identification of historical news days that closely resemble the\nheadlines of the current day, providing analysts with actionable insights to\npredict market trends by referencing analogous past events.",
        "We consider two classical macrospins with dynamical (frequency-dependent)\ncoupling, modeled by a generalized Landau-Lifshitz-Gilbert equation. We show\nthat, in the absence of local damping, the resulting dynamics are\npseudo-Hermitian. When two precessional modes hybridize near a crossing, the\nspectral behavior takes the form either of an anticrossing or level attraction,\nwith the latter formalized in terms of spontaneous $\\mathcal{PT}$-symmetry\nbreaking. Near equilibrium, mixing due to nondissipative interactions results\nin repulsion, while dissipative mixing results in attraction. In contrast, when\nthe fluctuating degrees of freedom form a free-energy saddle point, we find\nthat nondissipative interactions result in level attraction, while dissipative\ninteractions produce level repulsion. Accounting for the effects of local\nGilbert damping, we examine the cases in which approximate\n$\\mathcal{PT}$-symmetry breaking is still possible and determine the degree to\nwhich the qualitative spectral properties still persist.",
        "In this paper, we establish some inequalities for rational functions\n  with prescribed poles having s-fold zeros at origin and also show that\n  it implies some inequalities for polynomials and their polar derivatives.",
        "We investigate the dynamics of the ground state entanglement entropy for a\ndiscretized scalar field propagating within the Oppenheimer-Snyder collapse\nmetric. Starting from a well-controlled initial configuration, we follow the\nsystem as it evolves toward the formation of a horizon and, eventually, a\nsingularity. Our approach employs an Ermakov-like equation to determine the\ntime-dependent ground state of the field and calculates the resulting\nentanglement entropy by tracing out the degrees of freedom inside a spherical\nregion within the matter sphere. We find that the entanglement entropy exhibits\nnontrivial scaling and time dependence during collapse. Close to the horizon,\nthe entropy can deviate from the simple area law, reflecting the rapid changes\nin geometry and field configuration. Although the model is idealized, these\nresults provide insights into the generation and scaling of entanglement in the\npresence of realistic, dynamically evolving gravitational fields.",
        "Access to health information and services among women continues to be a major\nchallenge in many communities globally. In recent years, there has been a\ngrowing interest in the potential of chatbots to address this information and\naccess gap. We conducted interviews and focus group discussions with\nunderserved women in urban India to understand their receptivity towards the\nuse of chatbots for maternal and child health, as well as barriers to their\nadoption. Our findings uncover gaps in digital access and literacies, and\nperceived conflict with various responsibilities that women are burdened with,\nwhich shape their interactions with digital technology. Our paper offers\ninsights into the design of chatbots for community health that can meet the\nlived realities of women in underserved settings.",
        "Recent advances in generative diffusion models have shown a notable inherent\nunderstanding of image style and semantics. In this paper, we leverage the\nself-attention features from pretrained diffusion networks to transfer the\nvisual characteristics from a reference to generated images. Unlike previous\nwork that uses these features as plug-and-play attributes, we propose a novel\nattention distillation loss calculated between the ideal and current\nstylization results, based on which we optimize the synthesized image via\nbackpropagation in latent space. Next, we propose an improved Classifier\nGuidance that integrates attention distillation loss into the denoising\nsampling process, further accelerating the synthesis and enabling a broad range\nof image generation applications. Extensive experiments have demonstrated the\nextraordinary performance of our approach in transferring the examples' style,\nappearance, and texture to new images in synthesis. Code is available at\nhttps:\/\/github.com\/xugao97\/AttentionDistillation.",
        "We review the development of understanding for the problem of ultraviolet\ndivergences in supergravity. This history proceeds from initial constructions\nof counterterms invariant under the relevant degrees of local supersymmetry,\nthrough a deeper understanding of non-renormalisation theorems for the relevant\ndegrees of ``off-shell'' linearly realisable supersymmetry, and on to current\nunderstanding of limitations on counterterm eligibility based on the duality\nsymmetries of supergravity theories, as well as of the related structures\nemerging in superstring theory.",
        "Upon their evaporation via Hawking radiation, primordial black holes (PBHs)\nmay deposit energy in the ambient plasma on scales smaller than the typical\ndistance between two black holes, leading to the formation of hot spots around\nthem. We investigate how the corresponding rise of the local temperature during\nthe evaporation may act as a shield against the release of low-energy photons,\naffecting PBH's capacity to dissociate light nuclei after Big-Bang\nNucleosynthesis through photo-dissociation. We study the different ways PBH hot\nspots affect the flux of low-energy photons expected from PBH evaporation, and\nwe find that such effects can be particularly relevant to the physics of\nphoto-dissociation during Big-Bang Nucleosynthesis for PBHs with masses between\n$10^{11}$g and $3\\times 10^{12}$g. We emphasize that the magnitude of this\neffect is highly dependent on the specific shape of the temperature profile\naround PBHs and its time evolution. This underscores the necessity for a\ncomprehensive study of PBH hot spots and their dynamics in the future.",
        "Harmonic Hall measurements were conducted on a series of Ferromagnetic\nmetal\/Heavy metal (FM\/HM) bilayers with beta-Tungsten (W) as the HM and\nin-plane magnetized permalloy (Py) as the FM and the efficiencies of the two\northogonal components of the spin orbit-torque were extracted. Two sets of Hall\nbar-shaped devices were considered where the HM resistivity systematically\nvaried over a wide range (sim150-1000 muOmega-cm) while the FM layer remained\nthe same and each set having a different aspect ratio of voltage pickup line\nwidth and Hall bar width. Using numerical simulations of current distribution\nat the region between voltage pickup lines we have normalised the SOT\nefficiencies and examined their dependence. The current-induced spin-orbit\ntorque efficiency in ferromagnetic metal (FM)\/heavy metal (HM) bilayers is\nquantitatively investigated in this study.beta-W, known for its high spin-orbit\ncoupling, served as the HM layer, while Py, an FM with an in-plane magnetic\nanisotropy, comprised the other layer. We performed a thorough analysis of the\nsecond harmonic Hall resistance (R_{xy}^{2\\omega}) obtained from Py\/beta-W\nbilayer devices, systematically varying the resistivity (rho_W) of the beta-W\nlayer within the range of 200 to 1000 \\mu\\Omega-cm by employing a fixed current\ndensity (J_W\\sim0.8\\times10^{11} A\/m^2) through beta-W. Through this analysis,\nwe derived the Slonczewski-like efficiency (xi_{SL}) and field-like efficiency\n(\\xi_{FL}) as a function of rho_W. Notably, the device with a resistivity of\n980 muOmega-cm exhibited the highest xi_{SL}, yielding a value of -0.42 0.09.\nThese results highlight the promising potential of highly resistiv beta-W as a\nmaterial of interest in spintronics research.",
        "Between the absorption and the emission spectral lineshapes of dense atomic\nand molecular media, such as dye solutions and alkali-noble buffer gas mixtures\nat high pressure, in many cases there exists a universal scaling, the\nKennard-Stepanov relation, which is a manifestation of detailed balance. This\nrelation plays a crucial role in recent Bose-Einstein condensation experiments\nof visible-spectral-photons in e.g. dye-solution-filled optical microcavities.\nIt has recently been proposed to use high-pressure xenon-noble gas mixtures as\na thermalization medium for vacuum-ultraviolet regime photons, so as to extend\nthe achievable wavelength range of such Bose-Einstein-condensed optical sources\nfrom the visible to the vacuum-ultraviolet regime. In this work, we report\ntwo-photon excitation spectroscopy measurements of ground state ($5p^6$) xenon\natoms subject to up to 80bar of helium or krypton buffer gas pressure,\nrespectively, in the 220nm - 260nm wavelength range. The study of such\ntwo-photon spectra is of interest e.g. for the exploration of possible pumping\nschemes of a future vacuum-ultraviolet photon Bose-Einstein condensate. We have\nalso recorded absorption and emission spectra of the $5p^6 \\leftrightarrow\n5p^56s$ single-photon transition near 147nm wavelength of xenon atoms subject\nto 80bar of krypton buffer gas pressure. We find that the ratio of absorption\nand emission follows a Kennard-Stepanov scaling, which suggests that such gas\nmixtures are promising candidates as a thermalization medium for a\nBose-Einstein condensate of vacuum-ultraviolet photons."
      ]
    }
  },
  {
    "id":2411.05456,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"U-net: Convolutional networks for biomedical image segmentation",
    "start_abstract":"There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "MRI segmentation of the human brain: challenges, methods, and applications"
      ],
      "abstract":[
        "Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Operator Learning for Reconstructing Flow Fields from Sparse\n  Measurements: an Energy Transformer Approach",
        "Excitability and oscillations of active droplets",
        "Several classes of linear codes with few weights derived from Weil sums",
        "ByteQC: GPU-Accelerated Quantum Chemistry Package for Large-Scale\n  Systems",
        "What exactly has TabPFN learned to do?",
        "Amplification of turbulence through multiple planar shocks",
        "Diophantine approximation and the subspace theorem",
        "Optimal $L^p$-approximation of convex sets by convex subsets",
        "The Hierarchy of Saturating Matching Numbers",
        "Bayesianize Fuzziness in the Statistical Analysis of Fuzzy Data",
        "Escalation dynamics and the severity of wars",
        "Efficient and stable derivative-free Steffensen algorithm for root\n  finding",
        "Analytical Strategies and Winning Conditions for Elliptic-Orbit\n  Target-Attacker-Defender Game",
        "Parameter Estimation of State Space Models Using Particle Importance\n  Sampling",
        "A family of asymptotically bad wild towers of function fields",
        "Weak-Strong Uniqueness and the d'Alembert Paradox",
        "Tensor-structured PCG for finite difference solver of domain patterns in\n  ferroelectric material",
        "On the reproducibility of discrete-event simulation studies in health\n  research: an empirical study using open models",
        "System Architecture Optimization Strategies: Dealing with Expensive\n  Hierarchical Problems",
        "Tremblay-Turbiner-Winternitz (TTW) system at integer index $k$:\n  polynomial algebras of integrals",
        "VeloxQ: A Fast and Efficient QUBO Solver",
        "Zero Estimation Cost Strategy for Witsenhausen Counterexample with\n  Causal Encoder",
        "QE-CONVERSE: An open-source package for the Quantum ESPRESSO\n  distribution to compute non-perturbatively orbital magnetization from first\n  principles, including NMR chemical shifts and EPR parameters",
        "Optimal control over the full counting statistics in a non-adiabatic\n  pump",
        "X-ray cavities in TNG-Cluster: a direct comparison to observations",
        "Mass-manufactured Gradient Plasmonic Metasurfaces for Enhanced Mid-IR\n  Spectrochemical Analysis of Complex Biofluids",
        "Limiting behavior of mixed coherent systems with L\\'evy-frailty\n  Marshall-Olkin failure times",
        "An Equivalence Relation Classification of the Isomorphism Types of\n  Absolutely Indecomposable (resp. Absolutely Simple) Modules in Finite Group\n  Modular Representation Theory with a Green Theory Invariants \"Compatibility\"",
        "Convergence Analysis of EXTRA in Non-convex Distributed Optimization"
      ],
      "abstract":[
        "Machine learning methods have shown great success in various scientific\nareas, including fluid mechanics. However, reconstruction problems, where full\nvelocity fields must be recovered from partial observations, remain\nchallenging. In this paper, we propose a novel operator learning framework for\nsolving reconstruction problems by using the Energy Transformer (ET), an\narchitecture inspired by associative memory models. We formulate reconstruction\nas a mapping from incomplete observed data to full reconstructed fields. The\nmethod is validated on three fluid mechanics examples using diverse types of\ndata: (1) unsteady 2D vortex street in flow past a cylinder using simulation\ndata; (2) high-speed under-expanded impinging supersonic jets impingement using\nSchlieren imaging; and (3) 3D turbulent jet flow using particle tracking. The\nresults demonstrate the ability of ET to accurately reconstruct complex flow\nfields from highly incomplete data (90\\% missing), even for noisy experimental\nmeasurements, with fast training and inference on a single GPU. This work\nprovides a promising new direction for tackling reconstruction problems in\nfluid mechanics and other areas in mechanics, geophysics, weather prediction,\nand beyond.",
        "In living cells, cycles of formation and dissolution of liquid droplets can\nmediate biological functions such as DNA repair. However, the minimal\nphysicochemical prerequisite for such droplet oscillations remains elusive.\nHere, we present a simple model composed of only two independent chemical\ncomponents with their diffusive and chemical fluxes governed by non-equilibrium\nthermodynamics. There is turnover of fuel that maintains a chemical reaction\naway from equilibrium, leading to active droplets. We find that a single active\ndroplet undergoes a pitchfork-bifurcation in the droplet volume upon increasing\nthe fueling strength. Strikingly, the active droplet becomes excitable upon\nadding a further chemical reaction. For sufficient fueling, the system\nundergoes self-sustained oscillations cycling between droplet formation and\ndissolution. The minimal nature of our model suggests self-sustained active\ndroplets as functional modules for de novo life.",
        "Linear codes with few weights have applications in secret sharing,\nauthentication codes, association schemes and strongly regular graphs. In this\npaper, several classes of $t$-weight linear codes over ${\\mathbb F}_{q}$ are\npresented with the defining sets given by the intersection, difference and\nunion of two certain sets, where $t=3,4,5,6$ and $q$ is an odd prime power. By\nusing Weil sums and Gauss sums, the parameters and weight distributions of\nthese codes are determined completely. Moreover, three classes of optimal codes\nmeeting the Griesmer bound are obtained, and computer experiments show that\nmany (almost) optimal codes can be derived from our constructions.",
        "Applying quantum chemistry algorithms to large-scale systems requires\nsubstantial computational resources scaled with the system size and the desired\naccuracy. To address this, ByteQC, a fully-functional and efficient package for\nlarge-scale quantum chemistry simulations, has been open-sourced at\nhttps:\/\/github.com\/bytedance\/byteqc, leveraging recent advances in\ncomputational power and many-body algorithms.\n  Regarding computational power, several standard algorithms are efficiently\nimplemented on modern GPUs, ranging from mean-field calculations (Hartree-Fock\nand density functional theory) to post-Hartree-Fock methods such as\nM{\\o}ller-Plesset perturbation theory, random phase approximation, coupled\ncluster methods, and quantum Monte Carlo methods. For the algorithmic approach,\nwe also employ a quantum embedding method, which significantly expands the\ntractable system size while preserving high accuracy at the gold-standard\nlevel.\n  All these features have been systematically benchmarked. For standalone\nalgorithms, the benchmark results demonstrate up to a 60$\\times$ speedup when\ncompared to 100-core CPUs. Additionally, the tractable system sizes have been\nsignificantly expanded: 1,610 orbitals for coupled cluster with single and\ndouble excitations (1,380 orbitals with perturbative triple excitations),\n11,040 orbitals for M{\\o}ller-Plesset perturbation theory of second order,\n37,120 orbitals for mean-field calculations under open boundary conditions, and\nover 100,000 orbitals for periodic boundary conditions. For the advanced\nquantum embedding feature, two representative examples are demonstrated: the\nwater cluster problem (2,752 orbitals) and a water monomer adsorbed on a boron\nnitride surface (3,929 orbitals), achieving the gold-standard accuracy.",
        "TabPFN [Hollmann et al., 2023], a Transformer model pretrained to perform\nin-context learning on fresh tabular classification problems, was presented at\nthe last ICLR conference. To better understand its behavior, we treat it as a\nblack-box function approximator generator and observe its generated function\napproximations on a varied selection of training datasets. Exploring its\nlearned inductive biases in this manner, we observe behavior that is at turns\neither brilliant or baffling. We conclude this post with thoughts on how these\nresults might inform the development, evaluation, and application of prior-data\nfitted networks (PFNs) in the future.",
        "We study the amplification of isotropic, incompressible turbulence through\nmultiple planar, collisional shocks, using analytical linear theory. There are\ntwo limiting cases we explore. The first assumes shocks occur rapidly in time\nsuch that the turbulence does not evolve between shocks. Whereas the second\ncase allows enough time for turbulence to isotropize between each shock. For\nthe latter case, through a quasi-equation-of-state, we show that the weak\nmulti-shock limit is agnostic to the distinction between thermal and vortical\nturbulent pressures, like an isotropic volumetric compression. When turbulence\ndoes not return to isotropy between shocks, the generated anisotropy -- itself\na function of shock strength -- can feedback on amplification by further\nshocks, altering choices for maximal or minimal amplification. In addition for\nthis case, we find that amplification is sensitive to the shock ordering. We\nmap how choices of shock strength can impact these amplification differences\ndue to ordering, finding, for example, shock pairs which lead to identical mean\npost-shock fields (density, temperature, pressure) but maximally distinct\nturbulent amplification.",
        "Diophantine approximation explores how well irrational numbers can be\napproximated by rationals, with foundational results by Dirichlet, Hurwitz, and\nLiouville culminating in Roth's theorem. Schmidt's subspace theorem extends\nRoth's results to higher dimensions, with profound implications to Diophantine\nequations and transcendence theory. This article provides a self-contained and\naccessible exposition of Roth's theorem and Schlickewei's refinement of the\nsubspace theorem, with an emphasis on proofs. The arguments presented are\nclassical and approachable for readers with a background in algebraic number\ntheory, serving as a streamlined, yet condensed reference for these fundamental\nresults.",
        "Given a convex set $\\Omega$ of $\\mathbb{R}^n$, we consider the shape\noptimization problem of finding a convex subset $\\omega\\subset \\Omega$, of a\ngiven measure, minimizing the $p$-distance functional $$\\mathcal{J}_p(\\omega)\n:= \\left(\\int_{\\mathbb{S}^{n-1}} |h_\\Omega-h_\\omega|^p\nd\\mathcal{H}^{n-1}\\right)^{\\frac{1}{p}},$$ where $1 \\le p <\\infty$ and\n$h_\\omega$ and $h_\\Omega$ are the support functions of $\\omega$ and the fixed\ncontainer $\\Omega$, respectively.\n  We prove the existence of solutions and show that this minimization problem\n$\\Gamma$-converges, when $p$ tends to $+\\infty$, towards the problem of finding\na convex subset $\\omega\\subset \\Omega$, of a given measure, minimizing the\nHausdorff distance to the convex $\\Omega$.\n  In the planar case, we show that the free parts of the boundary of the\noptimal shapes, i.e., those that are in the interior of $\\Omega$, are given by\npolygonal lines.\n  Still in the $2-d$ setting, from a computational perspective, the classical\nmethod based on optimizing Fourier coefficients of support functions is not\nefficient, as it is unable to efficiently capture the presence of segments on\nthe boundary of optimal shapes. We subsequently propose a method combining\nFourier analysis and a recent numerical scheme, allowing to obtain accurate\nresults, as demonstrated through numerical experiments.",
        "In this paper, we study three matching problems all of which came up quite\nrecently in the field of machine teaching. The cost of a matching is defined in\nsuch a way that, for some formal model of teaching, it equals (or bounds) the\nnumber of labeled examples needed to solve a given teaching task. We show how\nthe cost parameters associated with these problems depend on each other and how\nthey are related to other well known combinatorial parameters (like, for\ninstance, the VC-dimension).",
        "Fuzzy data, prevalent in social sciences and other fields, capture\nuncertainties arising from subjective evaluations and measurement imprecision.\nDespite significant advancements in fuzzy statistics, a unified inferential\nregression-based framework remains undeveloped. Hence, we propose a novel\napproach for analyzing bounded fuzzy variables within a regression framework.\nBuilding on the premise that fuzzy data result from a process analogous to\nstatistical coarsening, we introduce a conditional probabilistic approach that\nlinks observed fuzzy statistics (e.g., mode, spread) to the underlying,\nunobserved statistical model, which depends on external covariates. The\ninferential problem is addressed using Approximate Bayesian methods, mainly\nthrough a Gibbs sampler incorporating a quadratic approximation of the\nposterior distribution. Simulation studies and applications involving external\nvalidations are employed to evaluate the effectiveness of the proposed approach\nfor fuzzy data analysis. By reintegrating fuzzy data analysis into a more\ntraditional statistical framework, this work provides a significant step toward\nenhancing the interpretability and applicability of fuzzy statistical methods\nin many applicative contexts.",
        "Although very large wars remain an enduring threat in global politics, we\nlack a clear understanding of how some wars become large and costly, while most\ndo not. There are three possibilities: large conflicts start with and maintain\nintense fighting, they persist over a long duration, or they escalate in\nintensity over time. Using detailed within-conflict data on civil and\ninterstate wars 1946--2008, we show that escalation dynamics -- variations in\nfighting intensity within an armed conflict -- play a fundamental role in\nproducing large conflicts and are a generic feature of both civil and\ninterstate wars. However, civil wars tend to deescalate when they become very\nlarge, limiting their overall severity, while interstate wars exhibit a\npersistent risk of continual escalation. A non-parametric model demonstrates\nthat this distinction in escalation dynamics can explain the differences in the\nhistorical sizes of civil vs. interstate wars, and explain Richardson's Law\ngoverning the frequency and severity of interstate conflicts over the past 200\nyears. Escalation dynamics also drive enormous uncertainty in forecasting the\neventual sizes of both hypothetical and ongoing civil wars, indicating a need\nto better understand the causes of escalation and deescalation within\nconflicts. The close relationship between the size, and hence the cost, of an\narmed conflict and its potential for escalation has broad implications for\ntheories of conflict onset or termination and for risk assessment in\ninternational relations.",
        "We explore a family of numerical methods, based on the Steffensen divided\ndifference iterative algorithm, that do not evaluate the derivative of the\nobjective functions. The family of methods achieves second-order convergence\nwith two function evaluations per iteration with marginal additional\ncomputational cost. An important side benefit of the method is the improvement\nin stability for different initial conditions compared to the vanilla\nSteffensen method. We present numerical results for scalar functions, fields,\nand scalar fields. This family of methods outperforms the Steffensen method\nwith respect to standard quantitative metrics in most cases.",
        "This paper proposes an analytical framework for the orbital\nTarget-Attacker-Defender game with a non-maneuvering target along elliptic\norbits. Focusing on the linear quadratic game, we derive an analytical solution\nto the matrix Riccati equation, which yields analytical Nash-equilibrium\nstrategies for all players. Based on the analytical strategies, we derive the\nanalytical form of the necessary and sufficient winning conditions for the\nattacker. The simulation results show good consistency between the analytical\nand numerical methods, exhibiting 0.004$\\%$ relative error in the cost\nfunction. The analytical method achieves over 99.9$\\%$ reduction in CPU time\ncompared to the conventional numerical method, strengthening the advantage of\ndeveloping the analytical strategies. Furthermore, we verify the proposed\nwinning conditions and investigate the effects of eccentricity on the game\noutcomes. Our analysis reveals that for games with hovering initial states, the\ninitial position of the defender should be constrained inside a mathematically\ndefinable set to ensure that the attacker wins the game. This constrained set\nfurthermore permits geometric interpretation through our proposed framework.\nThis work establishes the analytical framework for orbital\nTarget-Attacker-Defender games, providing fundamental insights into the\nsolution analysis of the game.",
        "State-space models have been used in many applications, including\neconometrics, engineering, medical research, etc. The maximum likelihood\nestimation (MLE) of the static parameter of general state-space models is not\nstraightforward because the likelihood function is intractable. It is popular\nto use the sequential Monte Carlo(SMC) method to perform gradient ascent\noptimisation in either offline or online fashion. One problem with existing\nonline SMC methods for MLE is that the score estimators are inconsistent, i.e.\nthe bias does not vanish with increasing particle size. In this paper, two SMC\nalgorithms are proposed based on an importance sampling weight function to use\neach set of generated particles more efficiently. The first one is an offline\nalgorithm that locally approximates the likelihood function using importance\nsampling, where the locality is adapted by the effective sample size (ESS). The\nsecond one is a semi-online algorithm that has a computational cost linear in\nthe particle size and uses score estimators that are consistent. We study its\nconsistency and asymptotic normality. Their computational superiority is\nillustrated in numerical studies for long time series.",
        "In a previous work general conditions were given to prove the infiniteness of\nthe genus of certain towers of function fields over a perfect field. It was\nshown that many examples where particular cases of those general results. In\nthis paper the genus of a family of wild towers of function fields will be\nconsidered together with a result with less restrictive sufficient conditions\nfor a wild tower to have infinite genus.",
        "We prove conditional weak-strong uniqueness of the potential Euler solution\nfor external flow around a smooth body in three space dimensions, within the\nclass of viscosity weak solutions with the same initial data. Our sufficient\ncondition is the vanishing of the streamwise component of the skin friction in\nthe inviscid limit, somewhat weaker than the condition of Bardos-Titi in\nbounded domains. Because global-in-time existence of the smooth potential\nsolution leads back to the d'Alembert paradox, we argue that weak-strong\nuniqueness is not a valid criterion for \"relevant\" notions of generalized Euler\nsolution and that our condition is likely to be violated in the inviscid limit.\nWe prove also that the Drivas-Nguyen condition on uniform continuity at the\nwall of the normal velocity component implies weak-strong uniqueness within the\ngeneral class of admissible weak Euler solutions in bounded domains.",
        "This paper presents a case study of application of the preconditioned method\nof conjugate gradients (CG) on a problem with operator resembling the structure\nof sum of Kronecker products. In particular, we are solving the Poisson's\nequation on a sample of homogeneous isotropic ferroelectric material of cuboid\nshape, where the Laplacian is discretized by finite difference. We present\nseveral preconditioners that fits the Kronecker structure and thus can be\nefficiently implemented and applied. Preconditioner based on the Moore--Penrose\npseudoinverse is extremely efficient for this particular problem, and also\napplicable (if we are able to store the dense right-hand side of our problem).\nWe briefly analyze the computational cost of the method and individual\npreconditioners, and illustrate effectiveness of the chosen one by numerical\nexperiments.\n  Although we describe our method as preconditioned CG with pseudoinverse-based\npreconditioner, it can also be seen as pseudoinverse-based direct solver with\niterative refinement by CG iteration.\n  This work is motivated by real application, the method was already\nimplemented in C\/C++ code Ferrodo2 and first results were published in Physical\nReview B 107(9) (2023), paper id 094102.",
        "Reproducibility of computational research is critical for ensuring\ntransparency, reliability and reusability. Challenges with computational\nreproducibility have been documented in several fields, but healthcare\ndiscrete-event simulation (DES) models have not been thoroughly examined in\nthis context. This study assessed the computational reproducibility of eight\npublished healthcare DES models (Python or R), selected to represent diverse\ncontexts, complexities, and years of publication. Repositories and articles\nwere also assessed against guidelines and reporting standards, offering\ninsights into their relationship with reproducibility success. Reproducing\nresults required up to 28 hours of troubleshooting per model, with 50% fully\nreproduced and 50% partially reproduced (12.5% to 94.1% of reported outcomes).\nKey barriers included the absence of open licences, discrepancies between\nreported and coded parameters, and missing code to produce model outputs, run\nscenarios, and generate tables and figures. Addressing these issues would often\nrequire relatively little effort from authors: adding an open licence and\nsharing all materials used to produce the article. Actionable recommendations\nare proposed to enhance reproducibility practices for simulation modellers and\nreviewers.",
        "Choosing the right system architecture for the problem at hand is challenging\ndue to the large design space and high uncertainty in the early stage of the\ndesign process. Formulating the architecting process as an optimization problem\nmay mitigate some of these challenges. This work investigates strategies for\nsolving System Architecture Optimization (SAO) problems: expensive, black-box,\nhierarchical, mixed-discrete, constrained, multi-objective problems that may be\nsubject to hidden constraints. Imputation ratio, correction ratio, correction\nfraction, and max rate diversity metrics are defined for characterizing hierar\nchical design spaces. This work considers two classes of optimization\nalgorithms for SAO: Multi-Objective Evolutionary Algorithms (MOEA) such as\nNSGA-II, and Bayesian Optimization (BO) algorithms. A new Gaussian process\nkernel is presented that enables modeling hierarchical categorical variables,\nextending previous work on modeling continuous and integer hierarchical\nvariables. Next, a hierarchical sampling algorithm that uses design space\nhierarchy to group design vectors by active design variables is developed.\nThen, it is demonstrated that integrating more hierarchy information in the\noptimization algorithms yields better optimization results for BO algorithms.\nSeveral realistic single-objective and multi-objective test problems are used\nfor investigations. Finally, the BO algorithm is applied to a jet engine\narchitecture optimization problem. This work shows that the developed BO\nalgorithm can effectively solve the problem with one order of magnitude less\nfunction evaluations than NSGA-II. The algorithms and problems used in this\nwork are implemented in the open-source Python library SBArchOpt.",
        "An infinite 3-parametric family of superintegrable and exactly-solvable\nquantum models on a plane, admitting separation of variables in polar\ncoordinates, marked by integer index $k$ was introduced in Journ Phys A 42\n(2009) 242001 and was called in literature the TTW system. In this paper it is\nconjectured that the Hamiltonian and both integrals of TTW system have hidden\nalgebra $g^{(k)}$ - it was checked for $k=1,2,3,4$ - having its\nfinite-dimensional representation spaces as the invariant subspaces. It is\nchecked that for $k=1,2,3,4$ that the Hamiltonian $H$, two integrals ${\\cal\nI}_{1,2}$ and their commutator ${\\cal I}_{12} = [{\\cal I}_1,{\\cal I}_2]$ are\nfour generating elements of the polynomial algebra of integrals of the order\n$(k+1)$: $[{\\cal I}_1,{\\cal I}_{12}] = P_{k+1}(H, {\\cal I}_{1,2},{\\cal\nI}_{12})$, $[{\\cal I}_2,{\\cal I}_{12}] = Q_{k+1}(H, {\\cal I}_{1,2},{\\cal\nI}_{12})$, where $P_{k+1},Q_{k+1}$ are polynomials of degree $(k+1)$ written in\nterms of ordered monomials of $H, {\\cal I}_{1,2},{\\cal I}_{12}$. This implies\nthat polynomial algebra of integrals is subalgebra of $g^{(k)}$. It is\nconjectured that all is true for any integer $k$.",
        "We introduce VeloxQ, a fast and efficient solver for Quadratic Unconstrained\nBinary Optimization (QUBO) problems, which are central to numerous real-world\noptimization tasks. Unlike other physics-inspired approaches to optimization\nproblems, such as quantum annealing and quantum computing, VeloxQ does not\nrequire substantial progress of technology to unlock its full potential. We\nbenchmark VeloxQ against the state-of-the-art QUBO solvers based on emerging\ntechnologies. Our comparison includes quantum annealers, specifically D-Wave's\nAdvantage, and Advantage2 prototype platforms, the digital-quantum algorithm\ndesigned to solve Higher-Order Unconstrained Binary Optimization (HUBO)\ndeveloped by Kipu Quantum, physics-inspired algorithms: Simulated Bifurcation\nand Parallel Annealing and an algorithm based on tropical tensor networks. We\nalso take into account modern developments of conventional algorithms: Branch\nand Bound algorithm, an optimal implementation of the brute-force algorithm and\nBEIT QUBO solver. Our results show that VeloxQ not only matches but often\nsurpasses the mentioned solvers in solution quality and runtime. Additionally,\nVeloxQ demonstrates excellent scalability being the only solver capable of\nsolving large-scale optimization problems, including up to $2\\times 10^{8}$\nsparsely connected variables, that are currently intractable for its\ncompetitors. These findings position VeloxQ as a powerful and practical tool\nfor tackling large-scale QUBO and HUBO problems, offering a compelling\nalternative to existing quantum and classical optimization methods.",
        "We propose a zero estimation cost (ZEC) scheme for causal-encoding\nnoncausal-decoding vector-valued Witsenhausen counterexample based on the\ncoordination coding result. In contrast to source coding, our goal is to\ncommunicate a controlled system state. The introduced ZEC scheme is a joint\ncontrol-communication approach that transforms the system state into a sequence\nthat can be efficiently communicated using block coding. Numerical results show\nthat our approach significantly reduces the power required for achieving\nzero-estimation-cost state reconstruction at the decoder. In the second part,\nwe introduce a more general non-zero estimation cost (Non-ZEC) scheme. We\nobserve numerically that the Non-ZEC scheme operates as a time-sharing\nmechanism between the two-point strategy and the ZEC scheme. Overall, by\nleveraging block-coding gain, our proposed methods substantially improve the\npower-estimation trade-off for Witsenhausen counterexample.",
        "Orbital magnetization, a key property arising from the orbital motion of\nelectrons, plays a crucial role in determining the magnetic behavior of\nmolecules and solids. Despite its straightforward calculation in finite\nsystems, the computation in periodic systems poses challenges due to the\nill-defined position operator and surface current contributions. The modern\ntheory of orbital magnetization, implemented in the Density Functional Theory\n(DFT) framework, offers an accurate solution via the \"converse approach.\" Here,\nwe introduce QE-CONVERSE, a refactored, modular implementation of this method,\nreplacing outdated routines from Quantum ESPRESSO (version 3.2). QE-CONVERSE\nintegrates modern computational libraries like scaLAPACK and ELPA, enhancing\nscalability, especially for large supercell calculations. This work focuses on\nproviding the community with a reliable, accurate orbital magnetization package\nfor properties such as Electron Paramagnetic Resonance (EPR) g-tensors and\nNuclear Magnetic Resonance (NMR) chemical shifts, particularly where\nperturbative methods fail. We demonstrate QE-CONVERSE's effectiveness with\nbenchmark cases, including the NMR shifts of ${}^{27}$Al in alumina and\n${}^{17}$O and ${}^{29}$Si in $\\alpha$-quartz, as well as the EPR g-tensor for\n$_{ }^{n}\\Sigma(n\\geq 2)$ radicals and nitrogen defects in silicon. Results\nshow excellent agreement with theoretical and experimental data, with improved\naccuracy in EPR calculations over linear response methods. QE-CONVERSE is fully\ncompatible with recent Quantum ESPRESSO versions, enabling new possibilities\nfor studying complex materials",
        "We introduce a systematic procedure based on optimal control theory to\naddress the full counting statistics of particle transport in a stochastic\nsystem. Our approach enhances the performance of a Thouless pump in the\nnon-adiabatic regime by simultaneously optimizing the average pumping rate\nwhile minimizing noise. We demonstrate our optimization procedure on a\nparadigmatic model for the electronic transport through a quantum dot, both in\nthe limit of vanishing Coulomb interaction and in the interacting regime. Our\nmethod enables independent control of the moments associated with charge and\nspin transfer, allowing for the enhancement of spin current with minimal charge\ncurrent or the independent tuning of spin and charge fluctuations. These\nresults underscore the versatility of our approach, which can be applied to a\nbroad class of stochastic systems.",
        "The TNG-Cluster magnetohydrodynamic cosmological simulations, produce a\ndiverse population of X-ray cavities in the intracluster medium (ICM) of\nsimulated galaxy clusters. These arise from episodic, high velocity, kinetic\nenergy injections from the central active supermassive black hole (AGN, SMBH).\nHere, we present the first comprehensive comparative analysis of X-ray cavities\nin TNG-Cluster with observational data. First, we select a volume-limited\nsample of 35 real clusters ($z \\leq 0.071$, M$_\\text{500c}$ = 10$^{14-14.8}$\nM$_\\odot$) observed with the Chandra X-ray Observatory, identify 3 analogs for\neach in TNG-Cluster (total of 105) and generate mock Chandra images using same\nexposure times as their observed counterparts. We identify X-ray cavities and\nmeasure their properties in both datasets using identical techniques, ensuring\na direct, apples-to-apples comparison. Our analysis reveals that both samples\nhave a similar fraction of X-ray cavities (35-43 per cent). They exhibit\ncomparable sizes and morphologies, although the sizes of simulated X-ray\ncavities still attached to the SMBH are somewhat larger in TNG-Cluster -- a\nscarcity at $< 10$ kpc. The area of TNG X-ray cavities increases as they rise\nin the ICM, consistent with the trend seen in the observational sample. The\ncavity powers, estimated using observational techniques, show good agreement\nbetween the two samples (10$^{42-45}$ erg.s$^{-1}$), suggesting that X-ray\ncavities in the simulation are an important heating mechanism in cluster cores.\nOverall, the rather simple AGN feedback model of TNG, with no model choices\nmade to reproduce X-ray morphological features, and without cosmic rays,\ncreates a quantitatively realistic population of X-ray cavities at cluster\nscales.",
        "Mid-infrared spectroscopy offers powerful label-free molecular analysis\ncapabilities but faces significant challenges when analyzing complex biological\nsamples. Here, we present a transformative surface-enhanced infrared absorption\nspectroscopy (SEIRAS) platform that overcomes fundamental limitations through\nkey innovations. First, we demonstrate high-throughput wafer-scale fabrication\nof mid-IR plasmonic micro-hole-array (MHA) metasurfaces on free-standing\nsilicon nitride membranes, yielding approximately 400 sensor chips per 6-inch\nwafer. Second, our gradient MHA metasurface design supports spectrally cascaded\nplasmonic modes, generating over 400 sharp resonance peaks across the 1200-2000\ncm-1 fingerprint region. This approach enables comprehensive molecular\nfingerprinting using simple imaging optics in transmission mode. Third, we\nvalidate our SEIRAS platform using a model polymer system and clinical\nperitoneal fluid samples from ovarian cancer patients, demonstrating its\ncapability to resolve complex molecular signatures in real biological\nspecimens. The platform's dense spectral coverage ensures optimal on-resonance\nenhancement across the broad fingerprint region, revealing previously obscured\nvibrational bands that conventional IR spectroscopy cannot distinguish. By\ncombining high-throughput fabrication with simplified optical readout and the\ncapability to analyze complex biological samples, our work establishes a\nfoundation for translating SEIRAS technology into practical biomedical\napplications.",
        "In this paper we show a limit result for the reliability function of a system\n-- that is, the probability that the whole system is still operational after a\ncertain given time -- when the number of components of the system grows to\ninfinity. More specifically, we consider a sequence of mixed coherent systems\nwhose components are homogeneous and non-repairable, with failure-times\ngoverned by a L\\'evy-frailty Marshall-Olkin (LFMO) distribution -- a\ndistribution that allows simultaneous component failures. We show that under\nintegrability conditions the reliability function converges to the probability\nof a first-passage time of a L\\'evy subordinator process. To the best of our\nknowledge, this is the first result to tackle the asymptotic behavior of the\nreliability function as the number of components of the system grows. To\nillustrate our approach, we give an example of a parametric family of\nreliability functions where the system failure time converges in distribution\nto an exponential random variable, and give computational experiments testing\nconvergence.",
        "Let $G$ be a finite group, let $F$ be a field of prime $p$ order and let\n$\\bar{F}$ be an algebraic closure of $F$. In this context, we develop an\nequivalence relation on the set of isomorphic types of indecomposable (resp.\nsimple) $\\bar{F}G$-modules. Note that here all indecomposable (resp. simple)\nmodules are absolutely indecomposable (resp. simple). Also we show that members\nof the same equivalence have related Green Theory invariants. This work\ncompletes and extends previous results of the author.",
        "Optimization problems involving the minimization of a finite sum of smooth,\npossibly non-convex functions arise in numerous applications. To achieve a\nconsensus solution over a network, distributed optimization algorithms, such as\n\\textbf{EXTRA} (decentralized exact first-order algorithm), have been proposed\nto address these challenges. In this paper, we analyze the convergence\nproperties of \\textbf{EXTRA} in the context of smooth, non-convex optimization.\nBy interpreting its updates as a nonlinear dynamical system, we show novel\ninsights into its convergence properties. Specifically, i) \\textbf{EXTRA}\nconverges to a consensual first-order stationary point of the global objective\nwith a sublinear rate; and ii) \\textbf{EXTRA} avoids convergence to consensual\nstrict saddle points, offering second-order guarantees that ensure robustness.\nThese findings provide a deeper understanding of \\textbf{EXTRA} in a non-convex\ncontext."
      ]
    }
  },
  {
    "id":2411.05213,
    "research_type":"basic",
    "start_id":"b1",
    "start_title":"Microreactors gain wider use as alternative to batch production",
    "start_abstract":"The microreactors are gaining wide use among the pharmaceuticals and chemical companies as an alternative to batch production. They not only offers a flexible approach to continuous processing, but promises to save much of the time and effort consumed while expanding the chemistries at commercial scale. Most of the ten global pharma and chemical companies have acquired the Cytos Lab System,, a microreactor product developed by Cellular Process Chemistry Systems GmbH (CPC). Microreactors are comprised of plates with distinct channels in the submillimeter range, providing high surface-to-volume ratio, ultra fast mixing and high degree of control at all levels of production.",
    "start_categories":[
      "q-bio.OT"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Effect of bioclogging in porous media on complex conductivity signatures"
      ],
      "abstract":[
        "Flow through sand columns inoculated with Pseudomonas aeruginosa were used to investigate the effect of bioclogging on complex conductivity and flow transport properties. Complex (0.1\u20131000 Hz), bulk hydraulic (K), volumetric rate (Q), dispersivity (D), microbial cell concentrations monitored over time. Environmental scanning electron microscope images sands obtained at end experiment. Bioclogging resulting from increases in concentration production exopolymeric substances (EPS) had a large impact imaginary ( \u03c3 \u2033), K, Q, D, porosity (\u03a6). Changes electrical properties developed three stages: an initial stage 1 no significant changes all measured parameters (Days 1\u20138), which we attribute reversible irreversible attachment cells sand. In 2a 9\u201316), caused by growth biomass either as microcolonies filling pore throats and\/or uniform covering surfaces resulted maximum decrease Q but only moderate \u2033. 2b 17\u201324), EPS increase biofilm thickness higher \u2033 compared 2a. 3 25\u201332), reached quasi steady state insignificant are observed parameters. The results this study suggest that can provide complimentary information assessment porous media."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation",
        "AI Load Dynamics--A Power Electronics Perspective",
        "In-sensor 24 classes HAR under 850 Bytes",
        "MapGS: Generalizable Pretraining and Data Augmentation for Online\n  Mapping via Novel View Synthesis",
        "MOVE: A Mixture-of-Vision-Encoders Approach for Domain-Focused\n  Vision-Language Processing",
        "Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation",
        "A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for\n  General Industrial Process Tasks Based on Large Language Model",
        "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models",
        "A Novel Control Strategy for Offset Points Tracking in the Context of\n  Agricultural Robotics",
        "EFT & Species Scale: Friends or foes?",
        "ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code\n  Generation",
        "Intuitionistic modal logics: new and simpler decidability proofs for FIK\n  and LIK",
        "PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via\n  Multimodal LLM Agents",
        "Enhancing Disinformation Detection with Explainable AI and Named Entity\n  Replacement",
        "Enhancing Highway Safety: Accident Detection on the A9 Test Stretch\n  Using Roadside Sensors",
        "Ray-Tracing Channel Modeling for LEO Satellite-to-Ground Communication\n  Systems",
        "An Amplitude-Encoding-Based Classical-Quantum Transfer Learning\n  framework: Outperforming Classical Methods in Image Recognition",
        "Monocular Person Localization under Camera Ego-motion",
        "Translational diffusion in supercooled water at and near the glass\n  transition temperature -- 136 K",
        "NICER, NuSTAR and Insight-HXMT views to the newly discovered black hole\n  X-ray binary Swift J1727.8--1613",
        "Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool\n  for Perceived Socio-Economic Bias in LLMs",
        "Analysis of the Effect of Bars on Environmental Dependence of Disc\n  Galaxies with MaNGA Survey Data",
        "The Human-AI Handshake Framework: A Bidirectional Approach to Human-AI\n  Collaboration",
        "Optimal Transport Barycenter via Nonconvex-Concave Minimax Optimization",
        "Towards understanding structure-function relationships in random fiber\n  networks",
        "GPU-accelerated Subcycling Time Integration with the Einstein Toolkit",
        "Sparsity learning via structured functional factor augmentation",
        "Crosscap states with tunable entanglement as exact eigenstates of local\n  spin chain Hamiltonians",
        "Vietoris-Rips complexes of torus grids"
      ],
      "abstract":[
        "Long-term, open-domain dialogue capabilities are essential for chatbots\naiming to recall past interactions and demonstrate emotional intelligence (EI).\nYet, most existing research relies on synthetic, LLM-generated data, leaving\nopen questions about real-world conversational patterns. To address this gap,\nwe introduce REALTALK, a 21-day corpus of authentic messaging app dialogues,\nproviding a direct benchmark against genuine human interactions.\n  We first conduct a dataset analysis, focusing on EI attributes and persona\nconsistency to understand the unique challenges posed by real-world dialogues.\nBy comparing with LLM-generated conversations, we highlight key differences,\nincluding diverse emotional expressions and variations in persona stability\nthat synthetic dialogues often fail to capture.\n  Building on these insights, we introduce two benchmark tasks: (1) persona\nsimulation where a model continues a conversation on behalf of a specific user\ngiven prior dialogue context; and (2) memory probing where a model answers\ntargeted questions requiring long-term memory of past interactions.\n  Our findings reveal that models struggle to simulate a user solely from\ndialogue history, while fine-tuning on specific user chats improves persona\nemulation. Additionally, existing models face significant challenges in\nrecalling and leveraging long-term context within real-world conversations.",
        "As AI-driven computing infrastructures rapidly scale, discussions around data\ncenter design often emphasize energy consumption, water and electricity usage,\nworkload scheduling, and thermal management. However, these perspectives often\noverlook the critical interplay between AI-specific load transients and power\nelectronics. This paper addresses that gap by examining how large-scale AI\nworkloads impose unique demands on power conversion chains and, in turn, how\nthe power electronics themselves shape the dynamic behavior of AI-based\ninfrastructure. We illustrate the fundamental constraints imposed by\nmulti-stage power conversion architectures and highlight the key role of\nfinal-stage modules in defining realistic power slew rates for GPU clusters.\nOur analysis shows that traditional designs, optimized for slower-varying or\nCPU-centric workloads, may not adequately accommodate the rapid load ramps and\ndrops characteristic of AI accelerators. To bridge this gap, we present\ninsights into advanced converter topologies, hierarchical control methods, and\nenergy buffering techniques that collectively enable robust and efficient power\ndelivery. By emphasizing the bidirectional influence between AI workloads and\npower electronics, we hope this work can set a good starting point and offer\npractical design considerations to ensure future exascale-capable data centers\ncan meet the stringent performance, reliability, and scalability requirements\nof next-generation AI deployments.",
        "The year 2023 was a key year for tinyML unleashing a new age of intelligent\nsensors pushing intelligence from the MCU into the source of the data at the\nsensor level, enabling them to perform sophisticated algorithms and machine\nlearning models in real-time. This study presents an innovative approach to\nHuman Activity Recognition (HAR) using Intelligent Sensor Processing Units\n(ISPUs), demonstrating the feasibility of deploying complex machine learning\nmodels directly on ultra-constrained sensor hardware. We developed a 24-class\nHAR model achieving 85\\% accuracy while operating within an 850-byte stack\nmemory limit. The model processes accelerometer and gyroscope data in real\ntime, reducing latency, enhancing data privacy, and consuming only 0.5 mA of\npower. To address memory constraints, we employed incremental class injection\nand feature optimization techniques, enabling scalability without compromising\nperformance. This work underscores the transformative potential of on-sensor\nprocessing for applications in healthcare, predictive maintenance, and smart\nenvironments, while introducing a publicly available, diverse HAR dataset for\nfurther research. Future efforts will explore advanced compression techniques\nand broader IoT integration to push the boundaries of TinyML on constrained\ndevices.",
        "Online mapping reduces the reliance of autonomous vehicles on high-definition\n(HD) maps, significantly enhancing scalability. However, recent advancements\noften overlook cross-sensor configuration generalization, leading to\nperformance degradation when models are deployed on vehicles with different\ncamera intrinsics and extrinsics. With the rapid evolution of novel view\nsynthesis methods, we investigate the extent to which these techniques can be\nleveraged to address the sensor configuration generalization challenge. We\npropose a novel framework leveraging Gaussian splatting to reconstruct scenes\nand render camera images in target sensor configurations. The target config\nsensor data, along with labels mapped to the target config, are used to train\nonline mapping models. Our proposed framework on the nuScenes and Argoverse 2\ndatasets demonstrates a performance improvement of 18% through effective\ndataset augmentation, achieves faster convergence and efficient training, and\nexceeds state-of-the-art performance when using only 25% of the original\ntraining data. This enables data reuse and reduces the need for laborious data\nlabeling. Project page at https:\/\/henryzhangzhy.github.io\/mapgs.",
        "Multimodal language models (MLMs) integrate visual and textual information by\ncoupling a vision encoder with a large language model through the specific\nadapter. While existing approaches commonly rely on a single pre-trained vision\nencoder, there is a great variability of specialized encoders that can boost\nmodel's performance in distinct domains. In this work, we propose MOVE (Mixture\nof Vision Encoders) a simple yet effective approach to leverage multiple\npre-trained encoders for specialized multimodal tasks. MOVE automatically\nroutes inputs to the most appropriate encoder among candidates such as Unichat,\nInternViT, and Texify, thereby enhancing performance across a diverse set of\nbenchmarks, including ChartQA, MMBench, and MMMU. Experimental results\ndemonstrate that MOVE achieves competitive accuracy without incurring the\ncomplexities of image slicing for high-resolution images.",
        "The Qwen 2.5 3B base model was fine-tuned to generate contextually rich and\nengaging movie dialogue, leveraging the Cornell Movie-Dialog Corpus, a curated\ndataset of movie conversations. Due to the limitations in GPU computing and\nVRAM, the training process began with the 0.5B model progressively scaling up\nto the 1.5B and 3B versions as efficiency improvements were implemented. The\nQwen 2.5 series, developed by Alibaba Group, stands at the forefront of small\nopen-source pre-trained models, particularly excelling in creative tasks\ncompared to alternatives like Meta's Llama 3.2 and Google's Gemma. Results\ndemonstrate the ability of small models to produce high-quality, realistic\ndialogue, offering a promising approach for real-time, context-sensitive\nconversation generation.",
        "Data-driven soft sensors (DDSS) have become mainstream methods for predicting\nkey performance indicators in process industries. However, DDSS development\nrequires complex and costly customized designs tailored to various tasks during\nthe modeling process. Moreover, DDSS are constrained to a single structured\ndata modality, limiting their ability to incorporate additional contextual\nknowledge. Furthermore, DDSSs' limited representation learning leads to weak\npredictive performance with scarce data. To address these challenges, we\npropose a general framework named LLM-TKESS (large language model for\ntext-based knowledge-embedded soft sensing), harnessing the powerful general\nproblem-solving capabilities, cross-modal knowledge transfer abilities, and\nfew-shot capabilities of LLM for enhanced soft sensing modeling. Specifically,\nan auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's\npotential for capturing temporal relationships within series and spatial\nsemantic relationships among auxiliary variables. Then, we propose a two-stage\nfine-tuning alignment strategy: in the first stage, employing\nparameter-efficient fine-tuning through autoregressive training adjusts LLM to\nrapidly accommodate process variable data, resulting in a soft sensing\nfoundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM\nto various downstream tasks without modifying its architecture. Then, we\npropose two text-based knowledge-embedded soft sensors, integrating new natural\nlanguage modalities to overcome the limitations of pure structured data models.\nFurthermore, benefiting from LLM's pre-existing world knowledge, our model\ndemonstrates outstanding predictive capabilities in small sample conditions.\nUsing the thermal deformation of air preheater rotor as a case study, we\nvalidate through extensive experiments that LLM-TKESS exhibits outstanding\nperformance.",
        "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. Experimental results on multiple datasets demonstrate\nthat AdEval effectively reduces the impact of data contamination on evaluation\noutcomes, enhancing both the fairness and reliability of the evaluation\nprocess.",
        "In this paper, we present a novel method to control a rigidly connected\nlocation on the vehicle, such as a point on the implement in case of\nagricultural tasks. Agricultural robots are transforming modern farming by\nenabling precise and efficient operations, replacing humans in arduous tasks\nwhile reducing the use of chemicals. Traditionnaly, path_following algorithms\nare designed to guide the vehicle's center along a predefined trajetory.\nHowever, since the actual agronomic task is performed by the implement, it is\nessential to control a specific point on the implement itself rather than\nvehicle's center. As such, we present in this paper two approaches for\nachieving the control of an offset point on the robot. The first approach\nadapts existing control laws, initially inteded for rear axle's midpoint, to\nmanage the desired lateral deviation. The second approach employs backstepping\ncontrol techniques to create a control law that directly targets the implement.\nWe conduct real-world experiments, highlighting the limitations of traditional\napproaches for offset points control, and demonstrating the strengths and\nweaknesses of the proposed methods.",
        "Recently the notion that quantum gravity effects could manifest at scales\nmuch lower than the Planck scale has seen an intense Swamplandish revival.\nDozens of works have explored how the so-called species scale -- at which an\neffective description of gravity must break down -- relates to String Theory\nand the Swampland conjectures. In particular, the interplay between this scale\nand the abundant towers of states becoming lighter in asymptotic regions of\nmoduli spaces has proved to be key in understanding the real scale of quantum\ngravity. Nevertheless concerns have been raised regarding the validity of using\ninfinite towers of states when estimating this scale within Effective Field\nTheory and, more precisely, the consistency of cutting the tower part way\nthrough in a framework that relies on a clear separation of scales. In this\nwork we take an EFT point-of-view and provide a detailed perturbative\nderivation of the species scale -- by computing the 1-loop graviton propagator\nin the presence of many fields -- thereby clarifying common sources of\nconfusion in the literature. Not only do we clarify the setup, assumptions and\nregimes of validity of the result, but more importantly apply the same methods\nto an infinite tower of states. We show how each state in the tower contributes\nto the species scale and how the procedure of counting only ''light fields''\ncan be compatible with not cutting the tower, thereby maintaining the harmony\nbetween infinite towers and EFTs even in the context of the species scale.",
        "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in chart understanding tasks. However, interpreting charts with\ntextual descriptions often leads to information loss, as it fails to fully\ncapture the dense information embedded in charts. In contrast, parsing charts\ninto code provides lossless representations that can effectively contain all\ncritical details. Although existing open-source MLLMs have achieved success in\nchart understanding tasks, they still face two major challenges when applied to\nchart-to-code tasks.: (1) Low executability and poor restoration of chart\ndetails in the generated code and (2) Lack of large-scale and diverse training\ndata. To address these challenges, we propose \\textbf{ChartCoder}, the first\ndedicated chart-to-code MLLM, which leverages Code LLMs as the language\nbackbone to enhance the executability of the generated code. Furthermore, we\nintroduce \\textbf{Chart2Code-160k}, the first large-scale and diverse dataset\nfor chart-to-code generation, and propose the \\textbf{Snippet-of-Thought (SoT)}\nmethod, which transforms direct chart-to-code generation data into step-by-step\ngeneration. Experiments demonstrate that ChartCoder, with only 7B parameters,\nsurpasses existing open-source MLLMs on chart-to-code benchmarks, achieving\nsuperior chart restoration and code excitability. Our code will be available at\nhttps:\/\/github.com\/thunlp\/ChartCoder.",
        "In this note, by integrating ideas concerning terminating tableaux-based\nprocedures in modal logics and finite frame property of intuitionistic modal\nlogic IK, we provide new and simpler decidability proofs for FIK and LIK.",
        "Chart visualizations, while essential for data interpretation and\ncommunication, are predominantly accessible only as images in PDFs, lacking\nsource data tables and stylistic information. To enable effective editing of\ncharts in PDFs or digital scans, we present PlotEdit, a novel multi-agent\nframework for natural language-driven end-to-end chart image editing via\nself-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1)\nChart2Table for data table extraction, (2) Chart2Vision for style attribute\nidentification, (3) Chart2Code for retrieving rendering code, (4) Instruction\nDecomposition Agent for parsing user requests into executable steps, and (5)\nMultimodal Editing Agent for implementing nuanced chart component modifications\n- all coordinated through multimodal feedback to maintain visual fidelity.\nPlotEdit outperforms existing baselines on the ChartCraft dataset across style,\nlayout, format, and data-centric edits, enhancing accessibility for visually\nchallenged users and improving novice productivity.",
        "The automatic detection of disinformation presents a significant challenge in\nthe field of natural language processing. This task addresses a multifaceted\nsocietal and communication issue, which needs approaches that extend beyond the\nidentification of general linguistic patterns through data-driven algorithms.\nIn this research work, we hypothesise that text classification methods are not\nable to capture the nuances of disinformation and they often ground their\ndecision in superfluous features. Hence, we apply a post-hoc explainability\nmethod (SHAP, SHapley Additive exPlanations) to identify spurious elements with\nhigh impact on the classification models. Our findings show that\nnon-informative elements (e.g., URLs and emoticons) should be removed and named\nentities (e.g., Rwanda) should be pseudo-anonymized before training to avoid\nmodels' bias and increase their generalization capabilities. We evaluate this\nmethodology with internal dataset and external dataset before and after\napplying extended data preprocessing and named entity replacement. The results\nshow that our proposal enhances on average the performance of a disinformation\nclassification method with external test data in 65.78% without a significant\ndecrease of the internal test performance.",
        "Road traffic injuries are the leading cause of death for people aged 5-29,\nresulting in about 1.19 million deaths each year. To reduce these fatalities,\nit is essential to address human errors like speeding, drunk driving, and\ndistractions. Additionally, faster accident detection and quicker medical\nresponse can help save lives. We propose an accident detection framework that\ncombines a rule-based approach with a learning-based one. We introduce a\ndataset of real-world highway accidents featuring high-speed crash sequences.\nIt includes 294,924 labeled 2D boxes, 93,012 labeled 3D boxes, and track IDs\nacross 48,144 frames captured at 10 Hz using four roadside cameras and LiDAR\nsensors. The dataset covers ten object classes and is released in the OpenLABEL\nformat. Our experiments and analysis demonstrate the reliability of our method.",
        "Based on the vision of global coverage for sixth-generation (6G) wireless\ncommunication systems, the low earth orbit (LEO) satellite-to-ground channel\nmodel for urban scenarios has emerged as highly important for the system\ndesign. In this paper, we propose an LEO satellite-to-ground channel model\nthrough shooting and bouncing rays (SBR) algorithm to analyze the channel\ncharacteristics. The orbit of LEO is modeled by the simplified general\nperturbations 4 (SGP4), and an accurate celestial model is applied to calculate\nthe Doppler shift of multipath in a transmission time window of LEO\nsatellite-to-ground communications. Channel characteristics of LEO\nsatellite-to-ground communications such as the root-mean-square (RMS) delay\nspread, the Doppler shift, and the received power at different times are\nobtained. The simulation results show that the received power is only\nsignificantly noticeable in the transmission time window when the satellite is\nclose to the receiver. Proposed model validates the effectiveness of\nray-tracing in actual LEO satellite-to-ground communication scenarios and\nextends the calculation of the Doppler shift.",
        "The classical-quantum transfer learning (CQTL) method is introduced to\naddress the challenge of training large-scale, high-resolution image data on a\nlimited number of qubits (ranging from tens to hundreds) in the current Noisy\nIntermediate-Scale quantum (NISQ) era. existing CQTL frameworks have been\ndemonstrate quantum advantages with a small number of parameters (around 50),\nbut the performance of quantum neural networks is sensitive to the number of\nparameters. Currently, there is a lack of exploration into larger-scale quantum\ncircuits with more parameters. This paper proposes an amplitude-encoding-based\nclassical-quantum transfer learning (AE-CQTL) framework, accompanied by an\neffective learning algorithm. The AE-CQTL framework multiplies the parameters\nof quantum circuits by using multi-layer ansatz. Based on the AE-CQTL\nframework, we designed and implemented two CQTL neural network models: Transfer\nlearning Quantum Neural Network (TLQNN) and Transfer Learning Quantum\nConvolutional Neural Network (TLQCNN). Both models significantly expand the\nparameter capacity of quantum circuits, elevating the parameter scale from a\nfew dozen to over one hundred parameters. In cross-experiments with three\nbenchmark datasets (MNIST, Fashion-MNIST and CIFAR10) and three source models\n(ResNet18, ResNet50 and DenseNet121), TLQNN and TLQCNN have exceeded the\nbenchmark classical classifier in multiple performance metrics, including\naccuracy, convergence, stability, and generalization capability. Our work\ncontributes to advancing the application of classical-quantum transfer learning\non larger-scale quantum devices in future.",
        "Localizing a person from a moving monocular camera is critical for\nHuman-Robot Interaction (HRI). To estimate the 3D human position from a 2D\nimage, existing methods either depend on the geometric assumption of a fixed\ncamera or use a position regression model trained on datasets containing little\ncamera ego-motion. These methods are vulnerable to fierce camera ego-motion,\nresulting in inaccurate person localization. We consider person localization as\na part of a pose estimation problem. By representing a human with a four-point\nmodel, our method jointly estimates the 2D camera attitude and the person's 3D\nlocation through optimization. Evaluations on both public datasets and real\nrobot experiments demonstrate our method outperforms baselines in person\nlocalization accuracy. Our method is further implemented into a\nperson-following system and deployed on an agile quadruped robot.",
        "The properties of amorphous solid water at and near the calorimetric glass\ntransition temperature, $T_{g}$, of 136 K have been debated for years. One\nhypothesis is that water turns into a \"true\" liquid at $T_{g}$ (i.e., it\nbecomes ergodic) and exhibits all the characteristics of an ergodic liquid,\nincluding translational diffusion. A competing hypothesis is that only\nrotational motion becomes active at $T_{g}$, while the \"real\" glass transition\nin water is at a considerably higher temperature. To address this dispute, we\nhave investigated the diffusive mixing in nanoscale water films, with\nthicknesses up to ~100 nm, using infrared (IR) spectroscopy. The experiments\nused films that were composed of at least 90% $H_{2}O$ with $D_{2}O$ making up\nthe balance and were conducted in conditions where H\/D exchange was essentially\neliminated. Because the IR spectra of multilayer $D_{2}O$ films (e.g.,\nthicknesses of ~3 - 6 nm) embedded within thick $H_{2}O$ films are distinct\nfrom the spectrum of isolated $D_{2}O$ molecules within $H_{2}O$, the diffusive\nmixing of (initially) isotopically layered water films could be followed as a\nfunction of annealing time and temperature. The results show that water films\nwith total thicknesses ranging from ~20 to 100 nm diffusively mixed prior to\ncrystallization for temperatures between 120 and 144 K. The translational\ndiffusion had an Arrhenius temperature dependence with an activation energy of\n40.8 kJ\/mol, which indicates that water at and near $T_{g}$ is a strong liquid.\nThe measured diffusion coefficient at 136 K is 6.25 x 10$^{-21} m^{2}\/s$.",
        "Swift J1727.8--1613 is a black hole X-ray binary newly discovered in 2023. We\nperform spectral analysis with simultaneous Insight-HXMT, NICER and NuSTAR\nobservations when the source was approaching to the hard intermediate state.\nSuch a joint view reveals an additional hard component apart from the normally\nobserved hard component with reflection in the spectrum, to be distinguished\nfrom the usual black hole X-ray binary systems. By including this extra\ncomponent in the spectrum, we have measured a high spin of\n$0.98^{+0.02}_{-0.07}$ and an inclination of around $40^{+1.2}_{-0.8}$ degrees,\nwhich is consistent with NICER results reported before. However, we find that\nthe additional spectral component can not be exclusively determined due to the\nmodel degeneracy. Accordingly, a possible jet\/corona configuration is adjusted\nto account for the spectral fitting with different model trials. The extra\ncomponent may originate either from a relativistic jet or a jet base\/corona\nunderneath a slow jet.",
        "We introduce an Item Response Theory (IRT)-based framework to detect and\nquantify socioeconomic bias in large language models (LLMs) without relying on\nsubjective human judgments. Unlike traditional methods, IRT accounts for item\ndifficulty, improving ideological bias estimation. We fine-tune two LLM\nfamilies (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct\nideological positions and introduce a two-stage approach: (1) modeling response\navoidance and (2) estimating perceived bias in answered responses. Our results\nshow that off-the-shelf LLMs often avoid ideological engagement rather than\nexhibit bias, challenging prior claims of partisanship. This empirically\nvalidated framework enhances AI alignment research and promotes fairer AI\ngovernance.",
        "Bars are fundamental structures in disc galaxies, although their role in\ngalaxy evolution is still not fully known. This study investigates the effect\nof the presence of bars on the environmental dependence of disc galaxies'\nproperties using the volume-limited sample from Mapping Nearby Galaxies at APO\n(MaNGA) survey. The disc galaxies with and without bars samples were obtained\nusing the Galaxy Zoo 2 project then assigned into field and group sub-samples.\nThese sub-samples were used to compare the stellar mass, star formation rate,\n$g-r$ colour, concentration index and gas phase metallicity, and their\nrelationships between field and group environments. Then these are used to\ninvestigate if there is an existence of any difference between galaxies with\nand without bars. A one-to-one correspondence between field and group galaxies'\nproperties were observed, and a strong dependence on the environment for\nproperties of unbarred galaxies was observed when compared to barred. The\nstellar mass against star formation rate, $g-r$ colour against concentration\nindex and stellar mass against gas phase metallicity of unbarred galaxies\nstrongly depend on environment while for barred these relations weakly depend\non environment. The study concludes that bars in disc galaxies decrease the\ndependence of analysed properties and its relations on the environment.",
        "Human-AI collaboration is evolving from a tool-based perspective to a\npartnership model where AI systems complement and enhance human capabilities.\nTraditional approaches often limit AI to a supportive role, missing the\npotential for reciprocal relationships where both human and AI inputs\ncontribute to shared goals. Although Human-Centered AI (HcAI) frameworks\nemphasize transparency, ethics, and user experience, they often lack mechanisms\nfor genuine, dynamic collaboration. The \"Human-AI Handshake Model\" addresses\nthis gap by introducing a bi-directional, adaptive framework with five key\nattributes: information exchange, mutual learning, validation, feedback, and\nmutual capability augmentation. These attributes foster balanced interaction,\nenabling AI to act as a responsive partner, evolving with users over time.\nHuman enablers like user experience and trust, alongside AI enablers such as\nexplainability and responsibility, facilitate this collaboration, while shared\nvalues of ethics and co-evolution ensure sustainable growth. Distinct from\nexisting frameworks, this model is reflected in tools like GitHub Copilot and\nChatGPT, which support bi-directional learning and transparency. Challenges\nremain, including maintaining ethical standards and ensuring effective user\noversight. Future research will explore these challenges, aiming to create a\ntruly collaborative human-AI partnership that leverages the strengths of both\nto achieve outcomes beyond what either could accomplish alone.",
        "The optimal transport barycenter (a.k.a. Wasserstein barycenter) is a\nfundamental notion of averaging that extends from the Euclidean space to the\nWasserstein space of probability distributions. Computation of the\nunregularized barycenter for discretized probability distributions on point\nclouds is a challenging task when the domain dimension $d > 1$. Most practical\nalgorithms for approximating the barycenter problem are based on entropic\nregularization. In this paper, we introduce a nearly linear time $O(m \\log{m})$\nand linear space complexity $O(m)$ primal-dual algorithm, the\nWasserstein-Descent $\\dot{\\mathbb{H}}^1$-Ascent (WDHA) algorithm, for computing\nthe exact barycenter when the input probability density functions are\ndiscretized on an $m$-point grid. The key success of the WDHA algorithm hinges\non alternating between two different yet closely related Wasserstein and\nSobolev optimization geometries for the primal barycenter and dual Kantorovich\npotential subproblems. Under reasonable assumptions, we establish the\nconvergence rate and iteration complexity of WDHA to its stationary point when\nthe step size is appropriately chosen. Superior computational efficacy,\nscalability, and accuracy over the existing Sinkhorn-type algorithms are\ndemonstrated on high-resolution (e.g., $1024 \\times 1024$ images) 2D synthetic\nand real data.",
        "Random fiber networks form the structural foundation of numerous biological\ntissues and engineered materials, from living tissue in the human body to\neveryday materials like fabric and paper. From a mechanics perspective,\nunderstanding the structure-function relationships of random fiber networks is\nparticularly interesting because when external force is applied to these\nnetworks, only a small subset of fibers will actually carry the majority of the\nload. Specifically, these load-bearing fibers propagate through the network to\nform load paths, also called force chains. However, the relationship between\nfiber network geometric structure, force chains, and the overall mechanical\nbehavior of random fiber network structures remains poorly understood. To this\nend, we implement a finite element model of random fiber networks with\ngeometrically exact beam elements, and use this model to explore random fiber\nnetwork mechanical behavior. Our focus is twofold. First, we explore the\nmechanical behavior of single fiber chains and random fiber networks. Second,\nwe propose and validate an interpretable analytical approach to predicting\nfiber network mechanics from structural information alone. Key findings include\ninsight into the critical strain-stiffening transition point for single fiber\nchains and fiber networks, and a connection between force chains and the\ndistance-weighted graph shortest paths that arise by treating fiber networks as\nspatial graph structures. This work marks an important step towards mapping the\nstructure-function relationships of random fiber networks undergoing large\ndeformations. Additionally, with our code distributed under open-source\nlicenses, we hope that future researchers can directly build on our work to\naddress related problems beyond the scope defined here.",
        "Adaptive Mesh Refinement (AMR) with subcycling in time enables different grid\nlevels to advance using their own time steps, ensuring finer grids employ\nsmaller steps for accuracy while coarser grids take larger steps to improve\ncomputational efficiency. We present the development, validation, and\nperformance analysis of a subcycling in time algorithm implemented within the\nCarpetX driver in the Einstein Toolkit framework. This new approach\nsignificantly improves upon the previous subcycling implementation in the\nCarpet driver by achieving higher-order convergence -- fourth order in time\ninstead of second order -- and enhanced scaling performance. The key innovation\nlies in optimizing the exchange of ghost points at refinement boundaries,\nlimiting it to the same number as those at inter-process boundaries using dense\noutput from coarser levels, thereby reducing computational and communication\noverhead compared to the implementation in Carpet, which required a larger\nnumber of buffer zones.\n  To validate the algorithm, we first demonstrate its fourth-order convergence\nusing a scalar wave test. We then apply the algorithm to binary black hole\n(BBH) simulations, confirming its robustness and accuracy in a realistic\nastrophysical scenario. The results show excellent agreement with the\nwell-established LazEv code. Scaling tests on CPU (Frontera) and GPU (Vista)\nclusters reveal significant performance gains, with the new implementation\nachieving improved speed and scalability compared to the Carpet-based version.",
        "As one of the most powerful tools for examining the association between\nfunctional covariates and a response, the functional regression model has been\nwidely adopted in various interdisciplinary studies. Usually, a limited number\nof functional covariates are assumed in a functional linear regression model.\nNevertheless, correlations may exist between functional covariates in\nhigh-dimensional functional linear regression models, which brings significant\nstatistical challenges to statistical inference and functional variable\nselection. In this article, a novel functional factor augmentation structure\n(fFAS) is proposed for multivariate functional series, and a multivariate\nfunctional factor augmentation selection model (fFASM) is further proposed to\ndeal with issues arising from variable selection of correlated functional\ncovariates. Theoretical justifications for the proposed fFAS are provided, and\nstatistical inference results of the proposed fFASM are established. Numerical\ninvestigations support the superb performance of the novel fFASM model in terms\nof estimation accuracy and selection consistency.",
        "It has been observed recently that various spin chain Hamiltonians admit\nspecial zero energy \"crosscap\" eigenstates. These states are made up of\nmaximally entangled Bell pairs prepared on antipodal sites of a periodic chain.\nWe generalize the states by allowing the antipodal pairs to have non-maximal,\ntunable entanglement. We give sufficient conditions for such states to be exact\nzero energy eigenstates of a local Hamiltonian. The conditions are naturally\nsatisfied in many models which have a global U(1) symmetry. These models\ninclude well known integrable models such as the XX model, the Bariev model,\nthe folded XXZ model, and also a variety of non-integrable models. Using the\nzero-energy crosscap states we also derive a family of exact zero modes with\nsub-volume law entanglement.",
        "We study the topology of Vietoris--Rips complexes of finite grids on the\ntorus. Let $T_{n,n}$ be the grid of $n\\times n$ points on the flat torus\n$S^1\\times S^1$, equipped with the $l^1$ metric. Let $\\mathrm{VR}(T_{n,n};k)$\nbe the Vietoris--Rips simplicial complex of this torus grid at scale $k\\ge 0$.\nFor $n\\ge 7$ and small scales $2\\le k\\le \\frac{n-1}{3}$, the complex\n$\\mathrm{VR}(T_{n,n};k)$ is homotopy equivalent to the torus. For large scales\n$k\\ge 2\\lfloor\\frac{n}{2}\\rfloor$, the complex $\\mathrm{VR}(T_{n,n};k)$ is a\nsimplex and hence contractible. Interesting topology arises over intermediate\nscales $\\frac{n-1}{3}<k<2\\lfloor\\frac{n}{2}\\rfloor$. For example, we prove that\n$\\mathrm{VR}(T_{2n,2n};2n-1)\\cong S^{2n^2-1}$ for $n\\ge 2$, that\n$\\mathrm{VR}(T_{3n,3n};n)\\simeq\\vee^{6n^2-1}S^2$ for $n\\ge 2$, and that\n$\\mathrm{VR}(T_{3n-1,3n-1};n)\\simeq \\bigvee_{6n-3} S^2\\vee \\bigvee_{6n-2}S^3$\nfor $n\\geq 3$. Based on homology computations, we conjecture that\n$\\mathrm{VR}(T_{n,n};k)$ is homotopy equivalent to a $3$-sphere for a countable\nfamily of $(n,k)$ pairs, and we prove this for $(n,k)=(7,4)$."
      ]
    }
  },
  {
    "id":2411.05213,
    "research_type":"basic",
    "start_id":"b8",
    "start_title":"How flocculation can explain coexistence in the chemostat",
    "start_abstract":"We study a chemostat model in which two microbial species grow on single resource. show that coexistence is possible when the would normally win exclusive competition aggregates flocs. Our mathematical analysis exploits fact flocculation fast compared to biological growth, common hypothesis floc models. A numerical shows validity of this approach large parameter range. indicate how our yields mechanistic justification for so-called density-dependent growth.",
    "start_categories":[
      "q-bio.OT"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Effect of bioclogging in porous media on complex conductivity signatures"
      ],
      "abstract":[
        "Flow through sand columns inoculated with Pseudomonas aeruginosa were used to investigate the effect of bioclogging on complex conductivity and flow transport properties. Complex (0.1\u20131000 Hz), bulk hydraulic (K), volumetric rate (Q), dispersivity (D), microbial cell concentrations monitored over time. Environmental scanning electron microscope images sands obtained at end experiment. Bioclogging resulting from increases in concentration production exopolymeric substances (EPS) had a large impact imaginary ( \u03c3 \u2033), K, Q, D, porosity (\u03a6). Changes electrical properties developed three stages: an initial stage 1 no significant changes all measured parameters (Days 1\u20138), which we attribute reversible irreversible attachment cells sand. In 2a 9\u201316), caused by growth biomass either as microcolonies filling pore throats and\/or uniform covering surfaces resulted maximum decrease Q but only moderate \u2033. 2b 17\u201324), EPS increase biofilm thickness higher \u2033 compared 2a. 3 25\u201332), reached quasi steady state insignificant are observed parameters. The results this study suggest that can provide complimentary information assessment porous media."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation",
        "AI Load Dynamics--A Power Electronics Perspective",
        "In-sensor 24 classes HAR under 850 Bytes",
        "MapGS: Generalizable Pretraining and Data Augmentation for Online\n  Mapping via Novel View Synthesis",
        "MOVE: A Mixture-of-Vision-Encoders Approach for Domain-Focused\n  Vision-Language Processing",
        "Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation",
        "A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for\n  General Industrial Process Tasks Based on Large Language Model",
        "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models",
        "A Novel Control Strategy for Offset Points Tracking in the Context of\n  Agricultural Robotics",
        "EFT & Species Scale: Friends or foes?",
        "ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code\n  Generation",
        "Intuitionistic modal logics: new and simpler decidability proofs for FIK\n  and LIK",
        "PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via\n  Multimodal LLM Agents",
        "Enhancing Disinformation Detection with Explainable AI and Named Entity\n  Replacement",
        "Enhancing Highway Safety: Accident Detection on the A9 Test Stretch\n  Using Roadside Sensors",
        "Ray-Tracing Channel Modeling for LEO Satellite-to-Ground Communication\n  Systems",
        "An Amplitude-Encoding-Based Classical-Quantum Transfer Learning\n  framework: Outperforming Classical Methods in Image Recognition",
        "Monocular Person Localization under Camera Ego-motion",
        "Translational diffusion in supercooled water at and near the glass\n  transition temperature -- 136 K",
        "NICER, NuSTAR and Insight-HXMT views to the newly discovered black hole\n  X-ray binary Swift J1727.8--1613",
        "Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool\n  for Perceived Socio-Economic Bias in LLMs",
        "Analysis of the Effect of Bars on Environmental Dependence of Disc\n  Galaxies with MaNGA Survey Data",
        "The Human-AI Handshake Framework: A Bidirectional Approach to Human-AI\n  Collaboration",
        "Optimal Transport Barycenter via Nonconvex-Concave Minimax Optimization",
        "Towards understanding structure-function relationships in random fiber\n  networks",
        "GPU-accelerated Subcycling Time Integration with the Einstein Toolkit",
        "Sparsity learning via structured functional factor augmentation",
        "Crosscap states with tunable entanglement as exact eigenstates of local\n  spin chain Hamiltonians",
        "Vietoris-Rips complexes of torus grids"
      ],
      "abstract":[
        "Long-term, open-domain dialogue capabilities are essential for chatbots\naiming to recall past interactions and demonstrate emotional intelligence (EI).\nYet, most existing research relies on synthetic, LLM-generated data, leaving\nopen questions about real-world conversational patterns. To address this gap,\nwe introduce REALTALK, a 21-day corpus of authentic messaging app dialogues,\nproviding a direct benchmark against genuine human interactions.\n  We first conduct a dataset analysis, focusing on EI attributes and persona\nconsistency to understand the unique challenges posed by real-world dialogues.\nBy comparing with LLM-generated conversations, we highlight key differences,\nincluding diverse emotional expressions and variations in persona stability\nthat synthetic dialogues often fail to capture.\n  Building on these insights, we introduce two benchmark tasks: (1) persona\nsimulation where a model continues a conversation on behalf of a specific user\ngiven prior dialogue context; and (2) memory probing where a model answers\ntargeted questions requiring long-term memory of past interactions.\n  Our findings reveal that models struggle to simulate a user solely from\ndialogue history, while fine-tuning on specific user chats improves persona\nemulation. Additionally, existing models face significant challenges in\nrecalling and leveraging long-term context within real-world conversations.",
        "As AI-driven computing infrastructures rapidly scale, discussions around data\ncenter design often emphasize energy consumption, water and electricity usage,\nworkload scheduling, and thermal management. However, these perspectives often\noverlook the critical interplay between AI-specific load transients and power\nelectronics. This paper addresses that gap by examining how large-scale AI\nworkloads impose unique demands on power conversion chains and, in turn, how\nthe power electronics themselves shape the dynamic behavior of AI-based\ninfrastructure. We illustrate the fundamental constraints imposed by\nmulti-stage power conversion architectures and highlight the key role of\nfinal-stage modules in defining realistic power slew rates for GPU clusters.\nOur analysis shows that traditional designs, optimized for slower-varying or\nCPU-centric workloads, may not adequately accommodate the rapid load ramps and\ndrops characteristic of AI accelerators. To bridge this gap, we present\ninsights into advanced converter topologies, hierarchical control methods, and\nenergy buffering techniques that collectively enable robust and efficient power\ndelivery. By emphasizing the bidirectional influence between AI workloads and\npower electronics, we hope this work can set a good starting point and offer\npractical design considerations to ensure future exascale-capable data centers\ncan meet the stringent performance, reliability, and scalability requirements\nof next-generation AI deployments.",
        "The year 2023 was a key year for tinyML unleashing a new age of intelligent\nsensors pushing intelligence from the MCU into the source of the data at the\nsensor level, enabling them to perform sophisticated algorithms and machine\nlearning models in real-time. This study presents an innovative approach to\nHuman Activity Recognition (HAR) using Intelligent Sensor Processing Units\n(ISPUs), demonstrating the feasibility of deploying complex machine learning\nmodels directly on ultra-constrained sensor hardware. We developed a 24-class\nHAR model achieving 85\\% accuracy while operating within an 850-byte stack\nmemory limit. The model processes accelerometer and gyroscope data in real\ntime, reducing latency, enhancing data privacy, and consuming only 0.5 mA of\npower. To address memory constraints, we employed incremental class injection\nand feature optimization techniques, enabling scalability without compromising\nperformance. This work underscores the transformative potential of on-sensor\nprocessing for applications in healthcare, predictive maintenance, and smart\nenvironments, while introducing a publicly available, diverse HAR dataset for\nfurther research. Future efforts will explore advanced compression techniques\nand broader IoT integration to push the boundaries of TinyML on constrained\ndevices.",
        "Online mapping reduces the reliance of autonomous vehicles on high-definition\n(HD) maps, significantly enhancing scalability. However, recent advancements\noften overlook cross-sensor configuration generalization, leading to\nperformance degradation when models are deployed on vehicles with different\ncamera intrinsics and extrinsics. With the rapid evolution of novel view\nsynthesis methods, we investigate the extent to which these techniques can be\nleveraged to address the sensor configuration generalization challenge. We\npropose a novel framework leveraging Gaussian splatting to reconstruct scenes\nand render camera images in target sensor configurations. The target config\nsensor data, along with labels mapped to the target config, are used to train\nonline mapping models. Our proposed framework on the nuScenes and Argoverse 2\ndatasets demonstrates a performance improvement of 18% through effective\ndataset augmentation, achieves faster convergence and efficient training, and\nexceeds state-of-the-art performance when using only 25% of the original\ntraining data. This enables data reuse and reduces the need for laborious data\nlabeling. Project page at https:\/\/henryzhangzhy.github.io\/mapgs.",
        "Multimodal language models (MLMs) integrate visual and textual information by\ncoupling a vision encoder with a large language model through the specific\nadapter. While existing approaches commonly rely on a single pre-trained vision\nencoder, there is a great variability of specialized encoders that can boost\nmodel's performance in distinct domains. In this work, we propose MOVE (Mixture\nof Vision Encoders) a simple yet effective approach to leverage multiple\npre-trained encoders for specialized multimodal tasks. MOVE automatically\nroutes inputs to the most appropriate encoder among candidates such as Unichat,\nInternViT, and Texify, thereby enhancing performance across a diverse set of\nbenchmarks, including ChartQA, MMBench, and MMMU. Experimental results\ndemonstrate that MOVE achieves competitive accuracy without incurring the\ncomplexities of image slicing for high-resolution images.",
        "The Qwen 2.5 3B base model was fine-tuned to generate contextually rich and\nengaging movie dialogue, leveraging the Cornell Movie-Dialog Corpus, a curated\ndataset of movie conversations. Due to the limitations in GPU computing and\nVRAM, the training process began with the 0.5B model progressively scaling up\nto the 1.5B and 3B versions as efficiency improvements were implemented. The\nQwen 2.5 series, developed by Alibaba Group, stands at the forefront of small\nopen-source pre-trained models, particularly excelling in creative tasks\ncompared to alternatives like Meta's Llama 3.2 and Google's Gemma. Results\ndemonstrate the ability of small models to produce high-quality, realistic\ndialogue, offering a promising approach for real-time, context-sensitive\nconversation generation.",
        "Data-driven soft sensors (DDSS) have become mainstream methods for predicting\nkey performance indicators in process industries. However, DDSS development\nrequires complex and costly customized designs tailored to various tasks during\nthe modeling process. Moreover, DDSS are constrained to a single structured\ndata modality, limiting their ability to incorporate additional contextual\nknowledge. Furthermore, DDSSs' limited representation learning leads to weak\npredictive performance with scarce data. To address these challenges, we\npropose a general framework named LLM-TKESS (large language model for\ntext-based knowledge-embedded soft sensing), harnessing the powerful general\nproblem-solving capabilities, cross-modal knowledge transfer abilities, and\nfew-shot capabilities of LLM for enhanced soft sensing modeling. Specifically,\nan auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's\npotential for capturing temporal relationships within series and spatial\nsemantic relationships among auxiliary variables. Then, we propose a two-stage\nfine-tuning alignment strategy: in the first stage, employing\nparameter-efficient fine-tuning through autoregressive training adjusts LLM to\nrapidly accommodate process variable data, resulting in a soft sensing\nfoundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM\nto various downstream tasks without modifying its architecture. Then, we\npropose two text-based knowledge-embedded soft sensors, integrating new natural\nlanguage modalities to overcome the limitations of pure structured data models.\nFurthermore, benefiting from LLM's pre-existing world knowledge, our model\ndemonstrates outstanding predictive capabilities in small sample conditions.\nUsing the thermal deformation of air preheater rotor as a case study, we\nvalidate through extensive experiments that LLM-TKESS exhibits outstanding\nperformance.",
        "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. Experimental results on multiple datasets demonstrate\nthat AdEval effectively reduces the impact of data contamination on evaluation\noutcomes, enhancing both the fairness and reliability of the evaluation\nprocess.",
        "In this paper, we present a novel method to control a rigidly connected\nlocation on the vehicle, such as a point on the implement in case of\nagricultural tasks. Agricultural robots are transforming modern farming by\nenabling precise and efficient operations, replacing humans in arduous tasks\nwhile reducing the use of chemicals. Traditionnaly, path_following algorithms\nare designed to guide the vehicle's center along a predefined trajetory.\nHowever, since the actual agronomic task is performed by the implement, it is\nessential to control a specific point on the implement itself rather than\nvehicle's center. As such, we present in this paper two approaches for\nachieving the control of an offset point on the robot. The first approach\nadapts existing control laws, initially inteded for rear axle's midpoint, to\nmanage the desired lateral deviation. The second approach employs backstepping\ncontrol techniques to create a control law that directly targets the implement.\nWe conduct real-world experiments, highlighting the limitations of traditional\napproaches for offset points control, and demonstrating the strengths and\nweaknesses of the proposed methods.",
        "Recently the notion that quantum gravity effects could manifest at scales\nmuch lower than the Planck scale has seen an intense Swamplandish revival.\nDozens of works have explored how the so-called species scale -- at which an\neffective description of gravity must break down -- relates to String Theory\nand the Swampland conjectures. In particular, the interplay between this scale\nand the abundant towers of states becoming lighter in asymptotic regions of\nmoduli spaces has proved to be key in understanding the real scale of quantum\ngravity. Nevertheless concerns have been raised regarding the validity of using\ninfinite towers of states when estimating this scale within Effective Field\nTheory and, more precisely, the consistency of cutting the tower part way\nthrough in a framework that relies on a clear separation of scales. In this\nwork we take an EFT point-of-view and provide a detailed perturbative\nderivation of the species scale -- by computing the 1-loop graviton propagator\nin the presence of many fields -- thereby clarifying common sources of\nconfusion in the literature. Not only do we clarify the setup, assumptions and\nregimes of validity of the result, but more importantly apply the same methods\nto an infinite tower of states. We show how each state in the tower contributes\nto the species scale and how the procedure of counting only ''light fields''\ncan be compatible with not cutting the tower, thereby maintaining the harmony\nbetween infinite towers and EFTs even in the context of the species scale.",
        "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in chart understanding tasks. However, interpreting charts with\ntextual descriptions often leads to information loss, as it fails to fully\ncapture the dense information embedded in charts. In contrast, parsing charts\ninto code provides lossless representations that can effectively contain all\ncritical details. Although existing open-source MLLMs have achieved success in\nchart understanding tasks, they still face two major challenges when applied to\nchart-to-code tasks.: (1) Low executability and poor restoration of chart\ndetails in the generated code and (2) Lack of large-scale and diverse training\ndata. To address these challenges, we propose \\textbf{ChartCoder}, the first\ndedicated chart-to-code MLLM, which leverages Code LLMs as the language\nbackbone to enhance the executability of the generated code. Furthermore, we\nintroduce \\textbf{Chart2Code-160k}, the first large-scale and diverse dataset\nfor chart-to-code generation, and propose the \\textbf{Snippet-of-Thought (SoT)}\nmethod, which transforms direct chart-to-code generation data into step-by-step\ngeneration. Experiments demonstrate that ChartCoder, with only 7B parameters,\nsurpasses existing open-source MLLMs on chart-to-code benchmarks, achieving\nsuperior chart restoration and code excitability. Our code will be available at\nhttps:\/\/github.com\/thunlp\/ChartCoder.",
        "In this note, by integrating ideas concerning terminating tableaux-based\nprocedures in modal logics and finite frame property of intuitionistic modal\nlogic IK, we provide new and simpler decidability proofs for FIK and LIK.",
        "Chart visualizations, while essential for data interpretation and\ncommunication, are predominantly accessible only as images in PDFs, lacking\nsource data tables and stylistic information. To enable effective editing of\ncharts in PDFs or digital scans, we present PlotEdit, a novel multi-agent\nframework for natural language-driven end-to-end chart image editing via\nself-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1)\nChart2Table for data table extraction, (2) Chart2Vision for style attribute\nidentification, (3) Chart2Code for retrieving rendering code, (4) Instruction\nDecomposition Agent for parsing user requests into executable steps, and (5)\nMultimodal Editing Agent for implementing nuanced chart component modifications\n- all coordinated through multimodal feedback to maintain visual fidelity.\nPlotEdit outperforms existing baselines on the ChartCraft dataset across style,\nlayout, format, and data-centric edits, enhancing accessibility for visually\nchallenged users and improving novice productivity.",
        "The automatic detection of disinformation presents a significant challenge in\nthe field of natural language processing. This task addresses a multifaceted\nsocietal and communication issue, which needs approaches that extend beyond the\nidentification of general linguistic patterns through data-driven algorithms.\nIn this research work, we hypothesise that text classification methods are not\nable to capture the nuances of disinformation and they often ground their\ndecision in superfluous features. Hence, we apply a post-hoc explainability\nmethod (SHAP, SHapley Additive exPlanations) to identify spurious elements with\nhigh impact on the classification models. Our findings show that\nnon-informative elements (e.g., URLs and emoticons) should be removed and named\nentities (e.g., Rwanda) should be pseudo-anonymized before training to avoid\nmodels' bias and increase their generalization capabilities. We evaluate this\nmethodology with internal dataset and external dataset before and after\napplying extended data preprocessing and named entity replacement. The results\nshow that our proposal enhances on average the performance of a disinformation\nclassification method with external test data in 65.78% without a significant\ndecrease of the internal test performance.",
        "Road traffic injuries are the leading cause of death for people aged 5-29,\nresulting in about 1.19 million deaths each year. To reduce these fatalities,\nit is essential to address human errors like speeding, drunk driving, and\ndistractions. Additionally, faster accident detection and quicker medical\nresponse can help save lives. We propose an accident detection framework that\ncombines a rule-based approach with a learning-based one. We introduce a\ndataset of real-world highway accidents featuring high-speed crash sequences.\nIt includes 294,924 labeled 2D boxes, 93,012 labeled 3D boxes, and track IDs\nacross 48,144 frames captured at 10 Hz using four roadside cameras and LiDAR\nsensors. The dataset covers ten object classes and is released in the OpenLABEL\nformat. Our experiments and analysis demonstrate the reliability of our method.",
        "Based on the vision of global coverage for sixth-generation (6G) wireless\ncommunication systems, the low earth orbit (LEO) satellite-to-ground channel\nmodel for urban scenarios has emerged as highly important for the system\ndesign. In this paper, we propose an LEO satellite-to-ground channel model\nthrough shooting and bouncing rays (SBR) algorithm to analyze the channel\ncharacteristics. The orbit of LEO is modeled by the simplified general\nperturbations 4 (SGP4), and an accurate celestial model is applied to calculate\nthe Doppler shift of multipath in a transmission time window of LEO\nsatellite-to-ground communications. Channel characteristics of LEO\nsatellite-to-ground communications such as the root-mean-square (RMS) delay\nspread, the Doppler shift, and the received power at different times are\nobtained. The simulation results show that the received power is only\nsignificantly noticeable in the transmission time window when the satellite is\nclose to the receiver. Proposed model validates the effectiveness of\nray-tracing in actual LEO satellite-to-ground communication scenarios and\nextends the calculation of the Doppler shift.",
        "The classical-quantum transfer learning (CQTL) method is introduced to\naddress the challenge of training large-scale, high-resolution image data on a\nlimited number of qubits (ranging from tens to hundreds) in the current Noisy\nIntermediate-Scale quantum (NISQ) era. existing CQTL frameworks have been\ndemonstrate quantum advantages with a small number of parameters (around 50),\nbut the performance of quantum neural networks is sensitive to the number of\nparameters. Currently, there is a lack of exploration into larger-scale quantum\ncircuits with more parameters. This paper proposes an amplitude-encoding-based\nclassical-quantum transfer learning (AE-CQTL) framework, accompanied by an\neffective learning algorithm. The AE-CQTL framework multiplies the parameters\nof quantum circuits by using multi-layer ansatz. Based on the AE-CQTL\nframework, we designed and implemented two CQTL neural network models: Transfer\nlearning Quantum Neural Network (TLQNN) and Transfer Learning Quantum\nConvolutional Neural Network (TLQCNN). Both models significantly expand the\nparameter capacity of quantum circuits, elevating the parameter scale from a\nfew dozen to over one hundred parameters. In cross-experiments with three\nbenchmark datasets (MNIST, Fashion-MNIST and CIFAR10) and three source models\n(ResNet18, ResNet50 and DenseNet121), TLQNN and TLQCNN have exceeded the\nbenchmark classical classifier in multiple performance metrics, including\naccuracy, convergence, stability, and generalization capability. Our work\ncontributes to advancing the application of classical-quantum transfer learning\non larger-scale quantum devices in future.",
        "Localizing a person from a moving monocular camera is critical for\nHuman-Robot Interaction (HRI). To estimate the 3D human position from a 2D\nimage, existing methods either depend on the geometric assumption of a fixed\ncamera or use a position regression model trained on datasets containing little\ncamera ego-motion. These methods are vulnerable to fierce camera ego-motion,\nresulting in inaccurate person localization. We consider person localization as\na part of a pose estimation problem. By representing a human with a four-point\nmodel, our method jointly estimates the 2D camera attitude and the person's 3D\nlocation through optimization. Evaluations on both public datasets and real\nrobot experiments demonstrate our method outperforms baselines in person\nlocalization accuracy. Our method is further implemented into a\nperson-following system and deployed on an agile quadruped robot.",
        "The properties of amorphous solid water at and near the calorimetric glass\ntransition temperature, $T_{g}$, of 136 K have been debated for years. One\nhypothesis is that water turns into a \"true\" liquid at $T_{g}$ (i.e., it\nbecomes ergodic) and exhibits all the characteristics of an ergodic liquid,\nincluding translational diffusion. A competing hypothesis is that only\nrotational motion becomes active at $T_{g}$, while the \"real\" glass transition\nin water is at a considerably higher temperature. To address this dispute, we\nhave investigated the diffusive mixing in nanoscale water films, with\nthicknesses up to ~100 nm, using infrared (IR) spectroscopy. The experiments\nused films that were composed of at least 90% $H_{2}O$ with $D_{2}O$ making up\nthe balance and were conducted in conditions where H\/D exchange was essentially\neliminated. Because the IR spectra of multilayer $D_{2}O$ films (e.g.,\nthicknesses of ~3 - 6 nm) embedded within thick $H_{2}O$ films are distinct\nfrom the spectrum of isolated $D_{2}O$ molecules within $H_{2}O$, the diffusive\nmixing of (initially) isotopically layered water films could be followed as a\nfunction of annealing time and temperature. The results show that water films\nwith total thicknesses ranging from ~20 to 100 nm diffusively mixed prior to\ncrystallization for temperatures between 120 and 144 K. The translational\ndiffusion had an Arrhenius temperature dependence with an activation energy of\n40.8 kJ\/mol, which indicates that water at and near $T_{g}$ is a strong liquid.\nThe measured diffusion coefficient at 136 K is 6.25 x 10$^{-21} m^{2}\/s$.",
        "Swift J1727.8--1613 is a black hole X-ray binary newly discovered in 2023. We\nperform spectral analysis with simultaneous Insight-HXMT, NICER and NuSTAR\nobservations when the source was approaching to the hard intermediate state.\nSuch a joint view reveals an additional hard component apart from the normally\nobserved hard component with reflection in the spectrum, to be distinguished\nfrom the usual black hole X-ray binary systems. By including this extra\ncomponent in the spectrum, we have measured a high spin of\n$0.98^{+0.02}_{-0.07}$ and an inclination of around $40^{+1.2}_{-0.8}$ degrees,\nwhich is consistent with NICER results reported before. However, we find that\nthe additional spectral component can not be exclusively determined due to the\nmodel degeneracy. Accordingly, a possible jet\/corona configuration is adjusted\nto account for the spectral fitting with different model trials. The extra\ncomponent may originate either from a relativistic jet or a jet base\/corona\nunderneath a slow jet.",
        "We introduce an Item Response Theory (IRT)-based framework to detect and\nquantify socioeconomic bias in large language models (LLMs) without relying on\nsubjective human judgments. Unlike traditional methods, IRT accounts for item\ndifficulty, improving ideological bias estimation. We fine-tune two LLM\nfamilies (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct\nideological positions and introduce a two-stage approach: (1) modeling response\navoidance and (2) estimating perceived bias in answered responses. Our results\nshow that off-the-shelf LLMs often avoid ideological engagement rather than\nexhibit bias, challenging prior claims of partisanship. This empirically\nvalidated framework enhances AI alignment research and promotes fairer AI\ngovernance.",
        "Bars are fundamental structures in disc galaxies, although their role in\ngalaxy evolution is still not fully known. This study investigates the effect\nof the presence of bars on the environmental dependence of disc galaxies'\nproperties using the volume-limited sample from Mapping Nearby Galaxies at APO\n(MaNGA) survey. The disc galaxies with and without bars samples were obtained\nusing the Galaxy Zoo 2 project then assigned into field and group sub-samples.\nThese sub-samples were used to compare the stellar mass, star formation rate,\n$g-r$ colour, concentration index and gas phase metallicity, and their\nrelationships between field and group environments. Then these are used to\ninvestigate if there is an existence of any difference between galaxies with\nand without bars. A one-to-one correspondence between field and group galaxies'\nproperties were observed, and a strong dependence on the environment for\nproperties of unbarred galaxies was observed when compared to barred. The\nstellar mass against star formation rate, $g-r$ colour against concentration\nindex and stellar mass against gas phase metallicity of unbarred galaxies\nstrongly depend on environment while for barred these relations weakly depend\non environment. The study concludes that bars in disc galaxies decrease the\ndependence of analysed properties and its relations on the environment.",
        "Human-AI collaboration is evolving from a tool-based perspective to a\npartnership model where AI systems complement and enhance human capabilities.\nTraditional approaches often limit AI to a supportive role, missing the\npotential for reciprocal relationships where both human and AI inputs\ncontribute to shared goals. Although Human-Centered AI (HcAI) frameworks\nemphasize transparency, ethics, and user experience, they often lack mechanisms\nfor genuine, dynamic collaboration. The \"Human-AI Handshake Model\" addresses\nthis gap by introducing a bi-directional, adaptive framework with five key\nattributes: information exchange, mutual learning, validation, feedback, and\nmutual capability augmentation. These attributes foster balanced interaction,\nenabling AI to act as a responsive partner, evolving with users over time.\nHuman enablers like user experience and trust, alongside AI enablers such as\nexplainability and responsibility, facilitate this collaboration, while shared\nvalues of ethics and co-evolution ensure sustainable growth. Distinct from\nexisting frameworks, this model is reflected in tools like GitHub Copilot and\nChatGPT, which support bi-directional learning and transparency. Challenges\nremain, including maintaining ethical standards and ensuring effective user\noversight. Future research will explore these challenges, aiming to create a\ntruly collaborative human-AI partnership that leverages the strengths of both\nto achieve outcomes beyond what either could accomplish alone.",
        "The optimal transport barycenter (a.k.a. Wasserstein barycenter) is a\nfundamental notion of averaging that extends from the Euclidean space to the\nWasserstein space of probability distributions. Computation of the\nunregularized barycenter for discretized probability distributions on point\nclouds is a challenging task when the domain dimension $d > 1$. Most practical\nalgorithms for approximating the barycenter problem are based on entropic\nregularization. In this paper, we introduce a nearly linear time $O(m \\log{m})$\nand linear space complexity $O(m)$ primal-dual algorithm, the\nWasserstein-Descent $\\dot{\\mathbb{H}}^1$-Ascent (WDHA) algorithm, for computing\nthe exact barycenter when the input probability density functions are\ndiscretized on an $m$-point grid. The key success of the WDHA algorithm hinges\non alternating between two different yet closely related Wasserstein and\nSobolev optimization geometries for the primal barycenter and dual Kantorovich\npotential subproblems. Under reasonable assumptions, we establish the\nconvergence rate and iteration complexity of WDHA to its stationary point when\nthe step size is appropriately chosen. Superior computational efficacy,\nscalability, and accuracy over the existing Sinkhorn-type algorithms are\ndemonstrated on high-resolution (e.g., $1024 \\times 1024$ images) 2D synthetic\nand real data.",
        "Random fiber networks form the structural foundation of numerous biological\ntissues and engineered materials, from living tissue in the human body to\neveryday materials like fabric and paper. From a mechanics perspective,\nunderstanding the structure-function relationships of random fiber networks is\nparticularly interesting because when external force is applied to these\nnetworks, only a small subset of fibers will actually carry the majority of the\nload. Specifically, these load-bearing fibers propagate through the network to\nform load paths, also called force chains. However, the relationship between\nfiber network geometric structure, force chains, and the overall mechanical\nbehavior of random fiber network structures remains poorly understood. To this\nend, we implement a finite element model of random fiber networks with\ngeometrically exact beam elements, and use this model to explore random fiber\nnetwork mechanical behavior. Our focus is twofold. First, we explore the\nmechanical behavior of single fiber chains and random fiber networks. Second,\nwe propose and validate an interpretable analytical approach to predicting\nfiber network mechanics from structural information alone. Key findings include\ninsight into the critical strain-stiffening transition point for single fiber\nchains and fiber networks, and a connection between force chains and the\ndistance-weighted graph shortest paths that arise by treating fiber networks as\nspatial graph structures. This work marks an important step towards mapping the\nstructure-function relationships of random fiber networks undergoing large\ndeformations. Additionally, with our code distributed under open-source\nlicenses, we hope that future researchers can directly build on our work to\naddress related problems beyond the scope defined here.",
        "Adaptive Mesh Refinement (AMR) with subcycling in time enables different grid\nlevels to advance using their own time steps, ensuring finer grids employ\nsmaller steps for accuracy while coarser grids take larger steps to improve\ncomputational efficiency. We present the development, validation, and\nperformance analysis of a subcycling in time algorithm implemented within the\nCarpetX driver in the Einstein Toolkit framework. This new approach\nsignificantly improves upon the previous subcycling implementation in the\nCarpet driver by achieving higher-order convergence -- fourth order in time\ninstead of second order -- and enhanced scaling performance. The key innovation\nlies in optimizing the exchange of ghost points at refinement boundaries,\nlimiting it to the same number as those at inter-process boundaries using dense\noutput from coarser levels, thereby reducing computational and communication\noverhead compared to the implementation in Carpet, which required a larger\nnumber of buffer zones.\n  To validate the algorithm, we first demonstrate its fourth-order convergence\nusing a scalar wave test. We then apply the algorithm to binary black hole\n(BBH) simulations, confirming its robustness and accuracy in a realistic\nastrophysical scenario. The results show excellent agreement with the\nwell-established LazEv code. Scaling tests on CPU (Frontera) and GPU (Vista)\nclusters reveal significant performance gains, with the new implementation\nachieving improved speed and scalability compared to the Carpet-based version.",
        "As one of the most powerful tools for examining the association between\nfunctional covariates and a response, the functional regression model has been\nwidely adopted in various interdisciplinary studies. Usually, a limited number\nof functional covariates are assumed in a functional linear regression model.\nNevertheless, correlations may exist between functional covariates in\nhigh-dimensional functional linear regression models, which brings significant\nstatistical challenges to statistical inference and functional variable\nselection. In this article, a novel functional factor augmentation structure\n(fFAS) is proposed for multivariate functional series, and a multivariate\nfunctional factor augmentation selection model (fFASM) is further proposed to\ndeal with issues arising from variable selection of correlated functional\ncovariates. Theoretical justifications for the proposed fFAS are provided, and\nstatistical inference results of the proposed fFASM are established. Numerical\ninvestigations support the superb performance of the novel fFASM model in terms\nof estimation accuracy and selection consistency.",
        "It has been observed recently that various spin chain Hamiltonians admit\nspecial zero energy \"crosscap\" eigenstates. These states are made up of\nmaximally entangled Bell pairs prepared on antipodal sites of a periodic chain.\nWe generalize the states by allowing the antipodal pairs to have non-maximal,\ntunable entanglement. We give sufficient conditions for such states to be exact\nzero energy eigenstates of a local Hamiltonian. The conditions are naturally\nsatisfied in many models which have a global U(1) symmetry. These models\ninclude well known integrable models such as the XX model, the Bariev model,\nthe folded XXZ model, and also a variety of non-integrable models. Using the\nzero-energy crosscap states we also derive a family of exact zero modes with\nsub-volume law entanglement.",
        "We study the topology of Vietoris--Rips complexes of finite grids on the\ntorus. Let $T_{n,n}$ be the grid of $n\\times n$ points on the flat torus\n$S^1\\times S^1$, equipped with the $l^1$ metric. Let $\\mathrm{VR}(T_{n,n};k)$\nbe the Vietoris--Rips simplicial complex of this torus grid at scale $k\\ge 0$.\nFor $n\\ge 7$ and small scales $2\\le k\\le \\frac{n-1}{3}$, the complex\n$\\mathrm{VR}(T_{n,n};k)$ is homotopy equivalent to the torus. For large scales\n$k\\ge 2\\lfloor\\frac{n}{2}\\rfloor$, the complex $\\mathrm{VR}(T_{n,n};k)$ is a\nsimplex and hence contractible. Interesting topology arises over intermediate\nscales $\\frac{n-1}{3}<k<2\\lfloor\\frac{n}{2}\\rfloor$. For example, we prove that\n$\\mathrm{VR}(T_{2n,2n};2n-1)\\cong S^{2n^2-1}$ for $n\\ge 2$, that\n$\\mathrm{VR}(T_{3n,3n};n)\\simeq\\vee^{6n^2-1}S^2$ for $n\\ge 2$, and that\n$\\mathrm{VR}(T_{3n-1,3n-1};n)\\simeq \\bigvee_{6n-3} S^2\\vee \\bigvee_{6n-2}S^3$\nfor $n\\geq 3$. Based on homology computations, we conjecture that\n$\\mathrm{VR}(T_{n,n};k)$ is homotopy equivalent to a $3$-sphere for a countable\nfamily of $(n,k)$ pairs, and we prove this for $(n,k)=(7,4)$."
      ]
    }
  },
  {
    "id":2411.05213,
    "research_type":"basic",
    "start_id":"b0",
    "start_title":"Effect of bioclogging in porous media on complex conductivity signatures",
    "start_abstract":"Flow through sand columns inoculated with Pseudomonas aeruginosa were used to investigate the effect of bioclogging on complex conductivity and flow transport properties. Complex (0.1\u20131000 Hz), bulk hydraulic (K), volumetric rate (Q), dispersivity (D), microbial cell concentrations monitored over time. Environmental scanning electron microscope images sands obtained at end experiment. Bioclogging resulting from increases in concentration production exopolymeric substances (EPS) had a large impact imaginary ( \u03c3 \u2033), K, Q, D, porosity (\u03a6). Changes electrical properties developed three stages: an initial stage 1 no significant changes all measured parameters (Days 1\u20138), which we attribute reversible irreversible attachment cells sand. In 2a 9\u201316), caused by growth biomass either as microcolonies filling pore throats and\/or uniform covering surfaces resulted maximum decrease Q but only moderate \u2033. 2b 17\u201324), EPS increase biofilm thickness higher \u2033 compared 2a. 3 25\u201332), reached quasi steady state insignificant are observed parameters. The results this study suggest that can provide complimentary information assessment porous media.",
    "start_categories":[
      "math.MP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b1",
        "b8"
      ],
      "title":[
        "Microreactors gain wider use as alternative to batch production",
        "How flocculation can explain coexistence in the chemostat"
      ],
      "abstract":[
        "The microreactors are gaining wide use among the pharmaceuticals and chemical companies as an alternative to batch production. They not only offers a flexible approach to continuous processing, but promises to save much of the time and effort consumed while expanding the chemistries at commercial scale. Most of the ten global pharma and chemical companies have acquired the Cytos Lab System,, a microreactor product developed by Cellular Process Chemistry Systems GmbH (CPC). Microreactors are comprised of plates with distinct channels in the submillimeter range, providing high surface-to-volume ratio, ultra fast mixing and high degree of control at all levels of production.",
        "We study a chemostat model in which two microbial species grow on single resource. show that coexistence is possible when the would normally win exclusive competition aggregates flocs. Our mathematical analysis exploits fact flocculation fast compared to biological growth, common hypothesis floc models. A numerical shows validity of this approach large parameter range. indicate how our yields mechanistic justification for so-called density-dependent growth."
      ],
      "categories":[
        "q-bio.OT"
      ]
    },
    "list":{
      "title":[
        "Reverse Markov Learning: Multi-Step Generative Models for Complex\n  Distributions",
        "CoNOAir: A Neural Operator for Forecasting Carbon Monoxide Evolution in\n  Cities",
        "Malware Detection based on API calls",
        "The Southern Photometrical Local Universe Survey (S-PLUS): searching for\n  metal-poor dwarf galaxies",
        "Integrating Spatiotemporal Vision Transformer into Digital Twins for\n  High-Resolution Heat Stress Forecasting in Campus Environments",
        "The Regular Ricci-Inverse Cosmology with Multiple Anticurvature Scalars",
        "Compliance while resisting: a shear-thickening fluid controller for\n  physical human-robot interaction",
        "Benchmarking Vision-Language Models on Optical Character Recognition in\n  Dynamic Video Environments",
        "Adaptive Negative Damping Control for User-Dependent Multi-Terrain\n  Walking Assistance with a Hip Exoskeleton",
        "MQG4AI Towards Responsible High-risk AI -- Illustrated for Transparency\n  Focusing on Explainability Techniques",
        "Typographic Attacks in a Multi-Image Setting",
        "DDRM-PR: Fourier Phase Retrieval using Denoising Diffusion Restoration\n  Models",
        "Doping dependence of the magnetic ground state in the frustrated magnets\n  Ba$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co)",
        "Adaptive Entanglement Routing with Deep Q-Networks in Quantum Networks",
        "Radio pulse search from Aql X-1",
        "Wideband Cognitive Radio for Joint Communication and Sensing:\n  Optimization of Subcarrier Allocation and beamforming",
        "Electron dynamics induced by quantum cat-state light",
        "Ultrasound Lung Aeration Map via Physics-Aware Neural Operators",
        "Curating Model Problems for Software Designing",
        "Mitigating Hallucinations in Diffusion Models through Adaptive Attention\n  Modulation",
        "Genetic algorithm enhanced Solovay-Kitaev algorithm for quantum\n  compiling",
        "MappedTrace: Tracing Pointer Remotely with Compiler-generated Maps",
        "Generative Modeling of Class Probability for Multi-Modal Representation\n  Learning",
        "Fast spin precession and strong perpendicular magnetic anisotropy in\n  ferrimagnetic Mn4N thin films improved by Pd buffer layer",
        "LuxNAS: A Coherent Photonic Neural Network Powered by Neural\n  Architecture Search",
        "Highly nondegenerate polarization-entangled photon pairs produced\n  through noncritical phasematching in single-domain KTiOPO$_4$",
        "V2Edit: Versatile Video Diffusion Editor for Videos and 3D Scenes",
        "Another one (BH+OB pair) bites the dust"
      ],
      "abstract":[
        "Learning complex distributions is a fundamental challenge in contemporary\napplications. Generative models, such as diffusion models, have demonstrated\nremarkable success in overcoming many limitations of traditional statistical\nmethods. Shen and Meinshausen (2024) introduced engression, a generative\napproach based on scoring rules that maps noise (and covariates, if available)\ndirectly to data. While effective, engression struggles with highly complex\ndistributions, such as those encountered in image data. In this work, we extend\nengression to improve its capability in learning complex distributions. We\npropose a framework that defines a general forward process transitioning from\nthe target distribution to a known distribution (e.g., Gaussian) and then\nlearns a reverse Markov process using multiple engression models. This reverse\nprocess reconstructs the target distribution step by step. Our approach\nsupports general forward processes, allows for dimension reduction, and\nnaturally discretizes the generative process. As a special case, when using a\ndiffusion-based forward process, our framework offers a method to discretize\nthe training and inference of diffusion models efficiently. Empirical\nevaluations on simulated and climate data validate our theoretical insights,\ndemonstrating the effectiveness of our approach in capturing complex\ndistributions.",
        "Carbon Monoxide (CO) is a dominant pollutant in urban areas due to the energy\ngeneration from fossil fuels for industry, automobile, and domestic\nrequirements. Forecasting the evolution of CO in real-time can enable the\ndeployment of effective early warning systems and intervention strategies.\nHowever, the computational cost associated with the physics and chemistry-based\nsimulation makes it prohibitive to implement such a model at the city and\ncountry scale. To address this challenge, here, we present a machine learning\nmodel based on neural operator, namely, Complex Neural Operator for Air Quality\n(CoNOAir), that can effectively forecast CO concentrations. We demonstrate this\nby developing a country-level model for short-term (hourly) and long-term\n(72-hour) forecasts of CO concentrations. Our model outperforms\nstate-of-the-art models such as Fourier neural operators (FNO) and provides\nreliable predictions for both short and long-term forecasts. We further analyse\nthe capability of the model to capture extreme events and generate forecasts in\nurban cities in India. Interestingly, we observe that the model predicts the\nnext hour CO concentrations with R2 values greater than 0.95 for all the cities\nconsidered. The deployment of such a model can greatly assist the governing\nbodies to provide early warning, plan intervention strategies, and develop\neffective strategies by considering several what-if scenarios. Altogether, the\npresent approach could provide a fillip to real-time predictions of CO\npollution in urban cities.",
        "Malware attacks pose a significant threat in today's interconnected digital\nlandscape, causing billions of dollars in damages. Detecting and identifying\nfamilies as early as possible provides an edge in protecting against such\nmalware. We explore a lightweight, order-invariant approach to detecting and\nmitigating malware threats: analyzing API calls without regard to their\nsequence. We publish a public dataset of over three hundred thousand samples\nand their function call parameters for this task, annotated with labels\nindicating benign or malicious activity. The complete dataset is above 550GB\nuncompressed in size. We leverage machine learning algorithms, such as random\nforests, and conduct behavioral analysis by examining patterns and anomalies in\nAPI call sequences. By investigating how the function calls occur regardless of\ntheir order, we can identify discriminating features that can help us identify\nmalware early on. The models we've developed are not only effective but also\nefficient. They are lightweight and can run on any machine with minimal\nperformance overhead, while still achieving an impressive F1-Score of over\n85\\%. We also empirically show that we only need a subset of the function call\nsequence, specifically calls to the ntdll.dll library, to identify malware. Our\nresearch demonstrates the efficacy of this approach through empirical\nevaluations, underscoring its accuracy and scalability. The code is open source\nand available at Github along with the dataset on Zenodo.",
        "The metal content of a galaxy's interstellar medium reflects the interplay\nbetween different evolutionary processes such as feedback from massive stars\nand the accretion of gas from the intergalactic medium. Despite the expected\nabundance of low-luminosity galaxies, the low-mass and low-metallicity regime\nremains relatively understudied. Since the properties of their interstellar\nmedium resemble those of early galaxies, identifying such objects in the Local\nUniverse is crucial to understand the early stages of galaxy evolution. We used\nthe DR3 catalog of the Southern Photometric Local Universe Survey (S-PLUS) to\nselect low-metallicity dwarf galaxy candidates based on color selection\ncriteria typical of metal-poor, star-forming, low-mass systems. The final\nsample contains approximately 50 candidates. Spectral energy distribution\nfitting of the 12 S-PLUS bands reveals that $\\sim$ 90\\% of the candidates are\nbest fit by models with very low stellar metallicities. We obtained long-slit\nobservations with the Gemini Multi-Object Spectrograph to follow-up a pilot\nsample and confirm whether these galaxies have low metallicities. We find\noxygen abundances in the range $7.35<$ 12 + log(O\/H) $< 7.93$ (5\\% to 17\\% of\nthe solar value), confirming their metal-poor nature. Most targets are outliers\nin the mass-metallicity relation, i.e. they display a low metal content\nrelative to their observed stellar masses. In some cases, perturbed optical\nmorphologies might give evidence of dwarf-dwarf interactions or mergers. These\nresults suggest that the low oxygen abundances may be associated with an\nexternal event causing the accretion of metal-poor gas, which dilutes the\noxygen abundance in these systems.",
        "Extreme heat events exacerbated by climate change pose significant challenges\nto urban resilience and planning. This study introduces a climate-responsive\ndigital twin framework integrating the Spatiotemporal Vision Transformer\n(ST-ViT) model to enhance heat stress forecasting and decision-making. Using a\nTexas campus as a testbed, we synthesized high-resolution physical model\nsimulations with spatial and meteorological data to develop fine-scale human\nthermal predictions. The ST-ViT-powered digital twin enables efficient,\ndata-driven insights for planners, policymakers, and campus stakeholders,\nsupporting targeted heat mitigation strategies and advancing climate-adaptive\nurban design.",
        "We investigate the modified gravity in which the Lagrangian of gravity is a\nfunction of the trace of the n-th matrix power of Ricci tensor in a\nFriedmann-Lemaitre-Robertson-Walker(FLRW) spacetime. When n is negative, the\ninverse of Ricci tensor, also called the anticurvature tensor, will be\nintroduced. We design a new class of Ricci-inverse theory containing two\nanticurvature scalars and resulting to be free from the singularity problem.",
        "Physical human-robot interaction (pHRI) is widely needed in many fields, such\nas industrial manipulation, home services, and medical rehabilitation, and puts\nhigher demands on the safety of robots. Due to the uncertainty of the working\nenvironment, the pHRI may receive unexpected impact interference, which affects\nthe safety and smoothness of the task execution. The commonly used linear\nadmittance control (L-AC) can cope well with high-frequency small-amplitude\nnoise, but for medium-frequency high-intensity impact, the effect is not as\ngood. Inspired by the solid-liquid phase change nature of shear-thickening\nfluid, we propose a Shear-thickening Fluid Control (SFC) that can achieve both\nan easy human-robot collaboration and resistance to impact interference. The\nSFC's stability, passivity, and phase trajectory are analyzed in detail, the\nfrequency and time domain properties are quantified, and parameter constraints\nin discrete control and coupled stability conditions are provided. We conducted\nsimulations to compare the frequency and time domain characteristics of L-AC,\nnonlinear admittance controller (N-AC), and SFC, and validated their dynamic\nproperties. In real-world experiments, we compared the performance of L-AC,\nN-AC, and SFC in both fixed and mobile manipulators. L-AC exhibits weak\nresistance to impact. N-AC can resist moderate impacts but not high-intensity\nones, and may exhibit self-excited oscillations. In contrast, SFC demonstrated\nsuperior impact resistance and maintained stable collaboration, enhancing\ncomfort in cooperative water delivery tasks. Additionally, a case study was\nconducted in a factory setting, further affirming the SFC's capability in\nfacilitating human-robot collaborative manipulation and underscoring its\npotential in industrial applications.",
        "This paper introduces an open-source benchmark for evaluating Vision-Language\nModels (VLMs) on Optical Character Recognition (OCR) tasks in dynamic video\nenvironments. We present a curated dataset containing 1,477 manually annotated\nframes spanning diverse domains, including code editors, news broadcasts,\nYouTube videos, and advertisements. Three state of the art VLMs - Claude-3,\nGemini-1.5, and GPT-4o are benchmarked against traditional OCR systems such as\nEasyOCR and RapidOCR. Evaluation metrics include Word Error Rate (WER),\nCharacter Error Rate (CER), and Accuracy. Our results highlight the strengths\nand limitations of VLMs in video-based OCR tasks, demonstrating their potential\nto outperform conventional OCR models in many scenarios. However, challenges\nsuch as hallucinations, content security policies, and sensitivity to occluded\nor stylized text remain. The dataset and benchmarking framework are publicly\navailable to foster further research.",
        "Hip exoskeletons are known for their versatility in assisting users across\nvaried scenarios. However, current assistive strategies often lack the\nflexibility to accommodate for individual walking patterns and adapt to diverse\nlocomotion environments. In this work, we present a novel control strategy that\nadapts the mechanical impedance of the human-exoskeleton system. We design the\nhip assistive torques as an adaptive virtual negative damping, which is able to\ninject energy into the system while allowing the users to remain in control and\ncontribute voluntarily to the movements. Experiments with five healthy subjects\ndemonstrate that our controller reduces the metabolic cost of walking compared\nto free walking (average reduction of 7.2%), and it preserves the lower-limbs\nkinematics. Additionally, our method achieves minimal power losses from the\nexoskeleton across the entire gait cycle (less than 2% negative mechanical\npower out of the total power), ensuring synchronized action with the users'\nmovements. Moreover, we use Bayesian Optimization to adapt the assistance\nstrength and allow for seamless adaptation and transitions across multi-terrain\nenvironments. Our strategy achieves efficient power transmission under all\nconditions. Our approach demonstrates an individualized, adaptable, and\nstraightforward controller for hip exoskeletons, advancing the development of\nviable, adaptive, and user-dependent control laws.",
        "As artificial intelligence (AI) systems become increasingly integrated into\ncritical domains, ensuring their responsible design and continuous development\nis imperative. Effective AI quality management (QM) requires tools and\nmethodologies that address the complexities of the AI lifecycle. In this paper,\nwe propose an approach for AI lifecycle planning that bridges the gap between\ngeneric guidelines and use case-specific requirements (MQG4AI). Our work aims\nto contribute to the development of practical tools for implementing\nResponsible AI (RAI) by aligning lifecycle planning with technical, ethical and\nregulatory demands. Central to our approach is the introduction of a flexible\nand customizable Methodology based on Quality Gates, whose building blocks\nincorporate RAI knowledge through information linking along the AI lifecycle in\na continuous manner, addressing AIs evolutionary character. For our present\ncontribution, we put a particular emphasis on the Explanation stage during\nmodel development, and illustrate how to align a guideline to evaluate the\nquality of explanations with MQG4AI, contributing to overall Transparency.",
        "Large Vision-Language Models (LVLMs) are susceptible to typographic attacks,\nwhich are misclassifications caused by an attack text that is added to an\nimage. In this paper, we introduce a multi-image setting for studying\ntypographic attacks, broadening the current emphasis of the literature on\nattacking individual images. Specifically, our focus is on attacking image sets\nwithout repeating the attack query. Such non-repeating attacks are stealthier,\nas they are more likely to evade a gatekeeper than attacks that repeat the same\nattack text. We introduce two attack strategies for the multi-image setting,\nleveraging the difficulty of the target image, the strength of the attack text,\nand text-image similarity. Our text-image similarity approach improves attack\nsuccess rates by 21% over random, non-specific methods on the CLIP model using\nImageNet while maintaining stealth in a multi-image scenario. An additional\nexperiment demonstrates transferability, i.e., text-image similarity calculated\nusing CLIP transfers when attacking InstructBLIP.",
        "Diffusion models have demonstrated their utility as learned priors for\nsolving various inverse problems. However, most existing approaches are limited\nto linear inverse problems. This paper exploits the efficient and unsupervised\nposterior sampling framework of Denoising Diffusion Restoration Models (DDRM)\nfor the solution of nonlinear phase retrieval problem, which requires\nreconstructing an image from its noisy intensity-only measurements such as\nFourier intensity. The approach combines the model-based alternating-projection\nmethods with the DDRM to utilize pretrained unconditional diffusion priors for\nphase retrieval. The performance is demonstrated through both simulations and\nexperimental data. Results demonstrate the potential of this approach for\nimproving the alternating-projection methods as well as its limitations.",
        "Theoretically, the relative change of the Heisenberg-type nearest-neighbor\ncoupling $J_1$ and next-nearest-neighbor coupling $J_2$ in the\nface-centered-cubic lattice can give rise to three main antiferromagnetic\norderings of type-I, type-II, and type-III. However, it is difficult to tune\nthe $J_2\/J_1$ ratio in real materials. Here, we report studies on the influence\nof Te$^{6+}$ and W$^{6+}$ ions replacement to the magnetic interactions and the\nmagnetic ground states in the double-perovskite compounds\nBa$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co). For\nBa$_2$MnTe$_{1-x}$W$_{x}$O$_6$, the W$^{6+}$ doping on Te$^{6+}$ site is\nsuccessful in $0.02 \\leq x \\leq 0.9$ with short-range orders of the type-I\n($0.02 \\leq x \\leq 0.08$) and type-II ($0.1 \\leq x \\leq 0.9$). In\nBa$_2$CoTe$_{1-x}$W${_x}$O$_6$, x-ray diffraction measurements reveal two\ncrystal structures, including the trigonal phase ($0 \\leq x \\leq 0.1$) and the\ncubic phase ($0.5 \\leq x \\leq 1$), between which is a miscibility gap. Two\nmagnetic transitions are identified in the trigonal phase due to two magnetic\nsubsystems, and the type-II magnetic order is observed in the cubic phase.\nMagnetic phase diagrams of Ba$_2M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co) are\nestablished. Our work shows that the magnetic interactions and ground states of\nBa$_2$$M$Te$_{1-x}$W$_x$O$_6$ can be tuned effectively by the replacement of\nTe$^{6+}$ by W$^{6+}$ ions.",
        "The quantum internet holds transformative potential for global communication\nby harnessing the principles of quantum information processing. Despite\nsignificant advancements in quantum communication technologies, the efficient\ndistribution of critical resources, such as qubits, remains a persistent and\nunresolved challenge. Conventional approaches often fall short of achieving\noptimal resource allocation, underscoring the necessity for more effective\nsolutions. This study proposes a novel reinforcement learning-based adaptive\nentanglement routing framework designed to enable resource allocation tailored\nto the specific demands of quantum applications. The introduced QuDQN model\nutilizes reinforcement learning to optimize the management of quantum networks,\nallocate resources efficiently, and enhance entanglement routing. The model\nintegrates key considerations, including fidelity requirements, network\ntopology, qubit capacity, and request demands.",
        "We present 12 observations of the accreting millisecond X-ray pulsar Aql X-1,\ntaken from August 2022 to October 2023 using the Five-hundred-meter Aperture\nSpherical Radio Telescope at 1250 MHz. These observations covered both the\nquiescence and X-ray outburst states, as determined by analyzing the X-ray data\nfrom the Neutron Star Interior Composition Explorer and the Monitor of All-sky\nX-ray Image. Periodicity and single-pulse searches were conducted for each\nobservation, but no pulsed signals were detected. The obtained upper limit flux\ndensities are in the range of 2.86-5.73 uJy, which provide the lowest limits to\ndate. We discuss several mechanisms that may prevent detection, suggesting that\nAql X-1 may be in the radio-ejection state during quiescence, where the radio\npulsed emissions are absorbed by the matter surrounding the system.",
        "As data traffic grows, wireless systems shift to higher frequency bands (6\nGHz and above), where radar systems also operate. This coexistence demands\neffective interference management and efficient wideband utilization. Cognitive\nRadio (CR) offers a solution but remains limited to single-node or narrowband\nsystems. This paper introduces a generalized wideband CR-enabled communication\nand sensing system with multiple users and targets. We propose a communication\nand sensing sub-carrier allocations framework, followed by transmit beamforming\nfor the primary communication BS and sensing signal design for the secondary\nradar BS. The goal is to maximize the communication sum rate while ensuring\nsensing requirements, minimizing interference, and adhering to power\nconstraints. To solve the resulting non-convex problem, we develop a manifold\noptimization algorithm for communication-only sub-carriers and an alternating\noptimization approach using the generalized Rayleigh quotient and semidefinite\nrelaxation for communication-sensing sub-carriers. Compared to a\nnon-cooperative benchmark, the proposed system achieves a \\qty{10}{\\percent}\ngain in communication sum rate and a \\qty{32}{\\percent} gain in sensing sum\nrate with \\num{12} BS antennas.",
        "We present an effective theory for describing electron dynamics driven by an\noptical external field in a Schr\\\"{o}dinger's cat state. We show that the\nelectron density matrix evolves as an average over trajectories\n$\\{\\rho_\\alpha\\}$ weighted by the Sudarshan--Glauber $P$ distribution\n$P(\\alpha)$ in the weak light--matter coupling regime. Each trajectory obeys an\nequation of motion, $\\mathrm{i} \\partial_t\\rho_\\alpha=\\mathcal{H}_{\\alpha}\n\\rho_\\alpha-\\rho_\\alpha\\mathcal{H}_{\\alpha}$, where an effective Hamiltonian\n$\\mathcal{H}_{\\alpha}$ becomes non-Hermitian due to quantum interference of\nlight. The optical quantum interference is transferred to electrons through the\nasymmetric action between the ket and bra state vectors in $\\rho_{\\alpha}$.\nThis non-Hermitian dynamics differs from the conventional one observed in open\nquantum systems, described by $\\mathrm{i} \\partial_t\\rho=\\mathcal{H}\\rho-\\rho\n\\mathcal{H}^\\dagger$, which has complex conjugation in the second term. We\nconfirm that the results of the effective theory agree with those of full\nelectron--photon system simulations for the few-electron Dicke model,\ndemonstrating experimental accessibility to exotic non-Hermitian dynamics.",
        "Lung ultrasound is a growing modality in clinics for diagnosing and\nmonitoring acute and chronic lung diseases due to its low cost and\naccessibility. Lung ultrasound works by emitting diagnostic pulses, receiving\npressure waves and converting them into radio frequency (RF) data, which are\nthen processed into B-mode images with beamformers for radiologists to\ninterpret. However, unlike conventional ultrasound for soft tissue anatomical\nimaging, lung ultrasound interpretation is complicated by complex\nreverberations from the pleural interface caused by the inability of ultrasound\nto penetrate air. The indirect B-mode images make interpretation highly\ndependent on reader expertise, requiring years of training, which limits its\nwidespread use despite its potential for high accuracy in skilled hands.\n  To address these challenges and democratize ultrasound lung imaging as a\nreliable diagnostic tool, we propose LUNA, an AI model that directly\nreconstructs lung aeration maps from RF data, bypassing the need for\ntraditional beamformers and indirect interpretation of B-mode images. LUNA uses\na Fourier neural operator, which processes RF data efficiently in Fourier\nspace, enabling accurate reconstruction of lung aeration maps. LUNA offers a\nquantitative, reader-independent alternative to traditional semi-quantitative\nlung ultrasound scoring methods. The development of LUNA involves synthetic and\nreal data: We simulate synthetic data with an experimentally validated approach\nand scan ex vivo swine lungs as real data. Trained on abundant simulated data\nand fine-tuned with a small amount of real-world data, LUNA achieves robust\nperformance, demonstrated by an aeration estimation error of 9% in ex-vivo lung\nscans. We demonstrate the potential of reconstructing lung aeration maps from\nRF data, providing a foundation for improving lung ultrasound reproducibility\nand diagnostic utility.",
        "Many disciplines use standard examples for education and to share and compare\nresearch results. The examples are rich enough to study from multiple points of\nview; they are often called model problems. Software design lacks such a\ncommunity resource. We propose an activity for Designing 2025 in which\nparticipants improve some existing model problem descriptions and initiate new\nones -- with a focus on use in software design education, plus potential\nutility in research.",
        "Diffusion models, while increasingly adept at generating realistic images,\nare notably hindered by hallucinations -- unrealistic or incorrect features\ninconsistent with the trained data distribution. In this work, we propose\nAdaptive Attention Modulation (AAM), a novel approach to mitigate\nhallucinations by analyzing and modulating the self-attention mechanism in\ndiffusion models. We hypothesize that self-attention during early denoising\nsteps may inadvertently amplify or suppress features, contributing to\nhallucinations. To counter this, AAM introduces a temperature scaling mechanism\nwithin the softmax operation of the self-attention layers, dynamically\nmodulating the attention distribution during inference. Additionally, AAM\nemploys a masked perturbation technique to disrupt early-stage noise that may\notherwise propagate into later stages as hallucinations. Extensive experiments\ndemonstrate that AAM effectively reduces hallucinatory artifacts, enhancing\nboth the fidelity and reliability of generated images. For instance, the\nproposed approach improves the FID score by 20.8% and reduces the percentage of\nhallucinated images by 12.9% (in absolute terms) on the Hands dataset.",
        "Quantum compiling trying to approximate the target qubit gate by finding an\noptimal sequence (braid word) of basic braid operations is a fundamental\nproblem in quantum computing. We develop a genetic algorithm (GA) enhanced\nSolovay-Kitaev algorithm (SKA) to approximate single qubit gates with four\nbasic braid matrices of Fibonacci anyons. The GA-enhanced SKA demonstrates that\nthe algorithm performs strongly and can easily find the ideal braid word from\nan exponentially large space. The resulting precision of the approximate\nsingle-qubit quantum gate is superior to that of the Monte Carlo (MC) enhanced\nSKA, as well as comparable to that of the deep reinforcement learning (RL) for\nthe length of braid word greater than 25. The 2(3)-order approximation of\nGA-enhanced SKA for basic braiding length l0=50(30) leads to an optimal braid\nword at a distance of 5.9*10-7, which is sufficient for most cases of quantum\ncomputing. Our work provides an alternative approach to solving and optimizing\nquantum compilation of non-Abelian anyon quantum gates and is useful for\nrealizing topological quantum computation in the future.",
        "Existing precise pointer tracing methods introduce substantial runtime\noverhead to the program being traced and are applicable only at specific\nprogram execution points. We propose MappedTrace that leverages\ncompiler-generated read-only maps to accurately identify all pointers in any\ngiven snapshot of a program's execution state. The maps record the locations\nand types of pointers, allowing the tracer to precisely identify pointers\nwithout requiring the traced program to maintain bookkeeping data structures or\npoll at safe points, thereby reducing runtime overhead. By running the tracer\nfrom a different address space or machine, MappedTrace presents new\nopportunities to improve memory management techniques like memory leak\ndetection and enables novel use cases such as infinite memory abstraction for\nresource-constrained environments.",
        "Multi-modal understanding plays a crucial role in artificial intelligence by\nenabling models to jointly interpret inputs from different modalities. However,\nconventional approaches such as contrastive learning often struggle with\nmodality discrepancies, leading to potential misalignments. In this paper, we\npropose a novel class anchor alignment approach that leverages class\nprobability distributions for multi-modal representation learning. Our method,\nClass-anchor-ALigned generative Modeling (CALM), encodes class anchors as\nprompts to generate and align class probability distributions for each\nmodality, enabling more effective alignment. Furthermore, we introduce a\ncross-modal probabilistic variational autoencoder to model uncertainty in the\nalignment, enhancing the ability to capture deeper relationships between\nmodalities and data variations. Extensive experiments on four benchmark\ndatasets demonstrate that our approach significantly outperforms\nstate-of-the-art methods, especially in out-of-domain evaluations. This\nhighlights its superior generalization capabilities in multi-modal\nrepresentation learning.",
        "Ferrimagnets take the advantages of both ferromagnets and antiferromagnets\nmaking them promise for spintronic applications. Here we prepared ferrimagnetic\nMn4N thin films with high Curie temperature and investigated the crystalline\nstructure and magnetic properties affected by the Pd buffer layer. We\ndemonstrated that both crystalline quality and perpendicular magnetic\nanisotropy (PMA) of Mn4N thin films are enhanced significantly due to the\nrelaxation of tensile stress induced by the Pd buffer layer. We also\ndemonstrated a fast spin precession at room temperature, almost 100 GHz, in\nMn4N thin films. With the characteristics of high thermal stability, enhanced\nPMA by buffer layer and fast spin precession, Mn4N thin film is a promising\nmaterial for spintronic applications.",
        "We demonstrate a novel coherent photonic neural network using tunable\nphase-change-material-based couplers and neural architecture search. Compared\nto the MZI-based Clements network, our results indicate 85% reduction in the\nnetwork footprint while maintaining the accuracy.",
        "Photon-pair sources are useful for entanglement distribution. The most mature\nof these are spontaneous parametric downconversion (SPDC) sources, most of\nwhich achieve phasematching via engineering the domains in poled crystals or\nthe angle between the optic axis and the pump beam. For multi-channel\nentanglement distribution of photon pairs, where one photon is transmitted\nthrough free-space and the other photon is transmitted through fiber, it is\nbeneficial to use highly nondegenerate photon-pair sources. The currently\naccepted approach in such sources is quasi-phasematching. In this paper, a\nsimpler, more stable alternative is presented for producing highly\nnondegenerate photon pairs. A source of polarization-entangled photon pairs\nwith low temperature sensitivity based on noncritical phasematched (NCPM) SPDC\nin single-domain potassium titanyl phosphate (KTP) was demonstrated. Over a\ncrystal temperature range of $75^\\circ$C, the center wavelength of the idler\nphotons was observed to change by $10.8$nm while the average entanglement\nvisibility was maintained above $98\\%$. With the signal photons detected\nlocally, the idler photons were transmitted through 62km and 93km of deployed\ntelecom fibers with average raw visibilities of $98.2(1)\\%$ and $95.6(3)\\%$\nrespectively.",
        "This paper introduces V$^2$Edit, a novel training-free framework for\ninstruction-guided video and 3D scene editing. Addressing the critical\nchallenge of balancing original content preservation with editing task\nfulfillment, our approach employs a progressive strategy that decomposes\ncomplex editing tasks into a sequence of simpler subtasks. Each subtask is\ncontrolled through three key synergistic mechanisms: the initial noise, noise\nadded at each denoising step, and cross-attention maps between text prompts and\nvideo content. This ensures robust preservation of original video elements\nwhile effectively applying the desired edits. Beyond its native video editing\ncapability, we extend V$^2$Edit to 3D scene editing via a\n\"render-edit-reconstruct\" process, enabling high-quality, 3D-consistent edits\neven for tasks involving substantial geometric changes such as object\ninsertion. Extensive experiments demonstrate that our V$^2$Edit achieves\nhigh-quality and successful edits across various challenging video editing\ntasks and complex 3D scene editing tasks, thereby establishing state-of-the-art\nperformance in both domains.",
        "Most (or possibly all) massive stars reside in multiple systems. From stellar\nevolution models, numerous systems with an OB star coupled to a black hole\nwould be expected to exist. There have been several claimed detections of such\npairs in recent years and this is notably the case of HD96670. Using\nhigh-quality photometry and spectroscopy in the optical range, we revisited the\nHD96670 system. We also examined complementary X-ray observations to provide a\nbroader view of the system properties. The TESS light curves of HD96670 clearly\nshow eclipses, ruling out the black hole companion scenario. This does not mean\nthat the system is not of interest. Indeed, the combined analysis of\nphotometric and spectroscopic data indicates that the system most likely\nconsists of a O8.5 giant star paired with a stripped-star companion with a mass\nof ~4.5Msol, a radius of ~1Rsol, and a surface temperature of ~50kK. While\nseveral B+sdOB systems have been reported in the literature, this would be the\nfirst case of a Galactic system composed of an O star and a faint stripped\nstar. In addition, the system appears brighter and harder than normal OB stars\nin the X-ray range, albeit less so than for X-ray binaries. The high-energy\nobservations provide hints of phase-locked variations, as typically seen in\ncolliding wind systems. As a post-interaction system, HD96670 actually\nrepresents a key case for probing binary evolution, even if it is not\nultimately found to host a black hole."
      ]
    }
  },
  {
    "id":2411.07031,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"ChatGPT Hallucinates Non-existent Citations: Evidence from Economics",
    "start_abstract":"In this study, we generate prompts derived from every topic within the Journal of Economic Literature to assess abilities both GPT-3.5 and GPT-4 versions ChatGPT large language model (LLM) write about economic concepts. demonstrates considerable competency in offering general summaries but also cites non-existent references. More than 30% citations provided by version do not exist rate is only slightly reduced for version. Additionally, our findings suggest that reliability decreases as become more specific. We provide quantitative evidence errors output demonstrate importance LLM verification. JEL Codes: B4; O33; I2",
    "start_categories":[
      "q-fin.ST"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Accuracy of Chatbots in Citing Journal Articles"
      ],
      "abstract":[
        "This cross-sectional study quantifies the journal article citation error rate of an artificial intelligence chatbot."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Second bounded cohomology of knot quandles",
        "PatentLMM: Large Multimodal Model for Generating Descriptions for Patent\n  Figures",
        "Conditioning on Local Statistics for Scalable Heterogeneous Federated\n  Learning",
        "Vertex-Minimal Triangulation of Complexes with Homology",
        "Epistemic Logic Programs: Non-Ground and Counting Complexity",
        "AudioSpa: Spatializing Sound Events with Text",
        "An upper bound on the size of a code with $s$ distances",
        "MEXA-CTP: Mode Experts Cross-Attention for Clinical Trial Outcome\n  Prediction",
        "Non-homogeneous problem for the fractional wave equation with irregular\n  coefficients and data",
        "QORT-Former: Query-optimized Real-time Transformer for Understanding Two\n  Hands Manipulating Objects",
        "DGNN: A Neural PDE Solver Induced by Discontinuous Galerkin Methods",
        "Blockchain with proof of quantum work",
        "Modularity of preferential attachment graphs",
        "Harmonic Structure of the Brunel spectra",
        "A maximum concurrence criterion to investigate absolutely maximally\n  entangled states",
        "Non-singular weakly symmetric nilmanifolds",
        "Semialgebraic Neural Networks: From roots to representations",
        "ALMA observations of CH3COCH3 and the related species CH3CHO, CH3OH, and\n  C2H5CN in line-rich molecular cores",
        "Breakdown of time-independent methods in non-Hermitian scattering\n  systems",
        "Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models\n  for Wireless Networks",
        "Duality of Wave Modulation and Nanotwinning in Ni-Mn-Ga Martensite via\n  Long-Period Commensurate States",
        "Pruning-aware Loss Functions for STOI-Optimized Pruned Recurrent\n  Autoencoders for the Compression of the Stimulation Patterns of Cochlear\n  Implants at Zero Delay",
        "Combating Interference for Over-the-Air Federated Learning: A\n  Statistical Approach via RIS",
        "Fourier analysis of equivariant quantum cohomology",
        "Higgs-portal vector dark matter at a low reheating temperature",
        "Neural Guided Diffusion Bridges",
        "Semi-rPPG: Semi-Supervised Remote Physiological Measurement with\n  Curriculum Pseudo-Labeling",
        "On The Concurrence of Layer-wise Preconditioning Methods and Provable\n  Feature Learning",
        "IoT Firmware Version Identification Using Transfer Learning with Twin\n  Neural Networks"
      ],
      "abstract":[
        "In this paper, we explore the bounded cohomology of quandles and its\napplications to knot theory. We establish two key results that provide\nsufficient conditions for the infinite dimensionality of the second bounded\ncohomology of quandles. The first condition involves a subspace of homogeneous\ngroup quasimorphisms on the inner automorphism group of the quandle, whereas\nthe second condition concerns the vanishing of the stable commutator length on\na subgroup of this inner automorphism group. As topological applications, we\nshow that the second bounded cohomology of the quandle of any non-split link\nwhose link group is non-solvable as well as the quandle of any split link, is\ninfinite dimensional. From these results, we conclude that the second bounded\ncohomology of the knot quandle detects the unknot. On the algebraic side, we\nprove that the second bounded cohomology of a free product of quandles is\ninfinite dimensional if the inner automorphism group of at least one of the\nfree factors is amenable. This leads to the result that the second bounded\ncohomology of free quandles of rank greater than one, as well as their\ncanonical quotients, is infinite dimensional.",
        "Writing comprehensive and accurate descriptions of technical drawings in\npatent documents is crucial to effective knowledge sharing and enabling the\nreplication and protection of intellectual property. However, automation of\nthis task has been largely overlooked by the research community. To this end,\nwe introduce PatentDesc-355K, a novel large-scale dataset containing ~355K\npatent figures along with their brief and detailed textual descriptions\nextracted from more than 60K US patent documents. In addition, we propose\nPatentLMM - a novel multimodal large language model specifically tailored to\ngenerate high-quality descriptions of patent figures. Our proposed PatentLMM\ncomprises two key components: (i) PatentMME, a specialized multimodal vision\nencoder that captures the unique structural elements of patent figures, and\n(ii) PatentLLaMA, a domain-adapted version of LLaMA fine-tuned on a large\ncollection of patents. Extensive experiments demonstrate that training a vision\nencoder specifically designed for patent figures significantly boosts the\nperformance, generating coherent descriptions compared to fine-tuning\nsimilar-sized off-the-shelf multimodal models. PatentDesc-355K and PatentLMM\npave the way for automating the understanding of patent figures, enabling\nefficient knowledge sharing and faster drafting of patent documents. We make\nthe code and data publicly available.",
        "Federated learning is a distributed machine learning approach where multiple\nclients collaboratively train a model without sharing their local data, which\ncontributes to preserving privacy. A challenge in federated learning is\nmanaging heterogeneous data distributions across clients, which can hinder\nmodel convergence and performance due to the need for the global model to\ngeneralize well across diverse local datasets. We propose to use local\ncharacteristic statistics, by which we mean some statistical properties\ncalculated independently by each client using only their local training\ndataset. These statistics, such as means, covariances, and higher moments, are\nused to capture the characteristics of the local data distribution. They are\nnot shared with other clients or a central node. During training, these local\nstatistics help the model learn how to condition on the local data\ndistribution, and during inference, they guide the client's predictions. Our\nexperiments show that this approach allows for efficient handling of\nheterogeneous data across the federation, has favorable scaling compared to\napproaches that directly try to identify peer nodes that share distribution\ncharacteristics, and maintains privacy as no additional information needs to be\ncommunicated.",
        "For a given pair of numbers $(d,k)$, we establish a lower bound on the number\nof vertices in pure $d$-dimensional simplicial complexes with non-trivial\nhomology in dimension $k$, and prove that this bound is tight. Furthermore, we\nsolve the problem under the additional constraint of strong connectivity with\nrespect to any intermediate dimension.",
        "Answer Set Programming (ASP) is a prominent problem-modeling and solving\nframework, whose solutions are called answer sets. Epistemic logic programs\n(ELP) extend ASP to reason about all or some answer sets. Solutions to an ELP\ncan be seen as consequences over multiple collections of answer sets, known as\nworld views. While the complexity of propositional programs is well studied,\nthe non-ground case remains open. This paper establishes the complexity of\nnon-ground ELPs. We provide a comprehensive picture for well-known program\nfragments, which turns out to be complete for the class NEXPTIME with access to\noracles up to \\Sigma^P_2. In the quantitative setting, we establish complexity\nresults for counting complexity beyond #EXP. To mitigate high complexity, we\nestablish results in case of bounded predicate arity, reaching up to the fourth\nlevel of the polynomial hierarchy. Finally, we provide ETH-tight runtime\nresults for the parameter treewidth, which has applications in quantitative\nreasoning, where we reason on (marginal) probabilities of epistemic literals.",
        "Text-to-audio (TTA) systems have recently demonstrated strong performance in\nsynthesizing monaural audio from text. However, the task of generating binaural\nspatial audio from text, which provides a more immersive auditory experience by\nincorporating the sense of spatiality, have not been explored yet. In this\nwork, we introduce text-guided binaural audio generation. As an early effort,\nwe focus on the scenario where a monaural reference audio is given\nadditionally. The core problem is to associate specific sound events with their\ndirections, thereby creating binaural spatial audio. The challenge lies in the\ncomplexity of textual descriptions and the limited availability of\nsingle-source sound event datasets. To address this, we propose AudioSpa, an\nend-to-end model that applies large language models to process both acoustic\nand textual information. We employ fusion multi-head attention (FMHA) to\nintegrate text tokens, which enhances the generation capability of the\nmultimodal learning. Additionally, we propose a binaural source localization\nmodel to assess the quality of the generated audio. Finally, we design a data\naugmentation strategy to generate diverse datasets, which enables the model to\nspatialize sound events across various spatial positions. Experimental results\ndemonstrate that our model is able to put sounds at the specified locations\naccurately. It achieves competitive performance in both localization accuracy\nand signal distortion. Our demonstrations are available at\nhttps:\/\/linfeng-feng.github.io\/AudioSpa-demo.",
        "Let $C$ be a binary code of length $n$ with distances $0<d_1<\\cdots<d_s\\le\nn$. In this note we prove a general upper bound on the size of $C$ without any\nrestriction on the distances $d_i$. The bound is asymptotically optimal.",
        "Clinical trials are the gold standard for assessing the effectiveness and\nsafety of drugs for treating diseases. Given the vast design space of drug\nmolecules, elevated financial cost, and multi-year timeline of these trials,\nresearch on clinical trial outcome prediction has gained immense traction.\nAccurate predictions must leverage data of diverse modes such as drug\nmolecules, target diseases, and eligibility criteria to infer successes and\nfailures. Previous Deep Learning approaches for this task, such as HINT, often\nrequire wet lab data from synthesized molecules and\/or rely on prior knowledge\nto encode interactions as part of the model architecture. To address these\nlimitations, we propose a light-weight attention-based model, MEXA-CTP, to\nintegrate readily-available multi-modal data and generate effective\nrepresentations via specialized modules dubbed \"mode experts\", while avoiding\nhuman biases in model design. We optimize MEXA-CTP with the Cauchy loss to\ncapture relevant interactions across modes. Our experiments on the Trial\nOutcome Prediction (TOP) benchmark demonstrate that MEXA-CTP improves upon\nexisting approaches by, respectively, up to 11.3% in F1 score, 12.2% in PR-AUC,\nand 2.5% in ROC-AUC, compared to HINT. Ablation studies are provided to\nquantify the effectiveness of each component in our proposed method.",
        "In this paper, we consider the Cauchy problem for a non-homogeneous wave\nequation generated by the fractional Laplacian and involving different kinds of\nlower order terms. We allow the equation coefficients and data to be of\ndistributional type or less regular, having in mind the Dirac delta function\nand its powers, and we prove that the problem is well-posed in the sense of the\nconcept of very weak solutions. Moreover, we prove the uniqueness in an\nappropriate sense and the coherence of the very weak solution concept with\nclassical theory.",
        "Significant advancements have been achieved in the realm of understanding\nposes and interactions of two hands manipulating an object. The emergence of\naugmented reality (AR) and virtual reality (VR) technologies has heightened the\ndemand for real-time performance in these applications. However, current\nstate-of-the-art models often exhibit promising results at the expense of\nsubstantial computational overhead. In this paper, we present a query-optimized\nreal-time Transformer (QORT-Former), the first Transformer-based real-time\nframework for 3D pose estimation of two hands and an object. We first limit the\nnumber of queries and decoders to meet the efficiency requirement. Given\nlimited number of queries and decoders, we propose to optimize queries which\nare taken as input to the Transformer decoder, to secure better accuracy: (1)\nwe propose to divide queries into three types (a left hand query, a right hand\nquery and an object query) and enhance query features (2) by using the contact\ninformation between hands and an object and (3) by using three-step update of\nenhanced image and query features with respect to one another. With proposed\nmethods, we achieved real-time pose estimation performance using just 108\nqueries and 1 decoder (53.5 FPS on an RTX 3090TI GPU). Surpassing\nstate-of-the-art results on the H2O dataset by 17.6% (left hand), 22.8% (right\nhand), and 27.2% (object), as well as on the FPHA dataset by 5.3% (right hand)\nand 10.4% (object), our method excels in accuracy. Additionally, it sets the\nstate-of-the-art in interaction recognition, maintaining real-time efficiency\nwith an off-the-shelf action recognition module.",
        "We propose a general framework for the Discontinuous Galerkin-induced Neural\nNetwork (DGNN), inspired by the Interior Penalty Discontinuous Galerkin Method\n(IPDGM). In this approach, the trial space consists of piecewise neural network\nspace defined over the computational domain, while the test function space is\ncomposed of piecewise polynomials. We demonstrate the advantages of DGNN in\nterms of accuracy and training efficiency across several numerical examples,\nincluding stationary and time-dependent problems. Specifically, DGNN easily\nhandles high perturbations, discontinuous solutions, and complex geometric\ndomains.",
        "We propose a blockchain architecture in which mining requires a quantum\ncomputer. The consensus mechanism is based on proof of quantum work, a\nquantum-enhanced alternative to traditional proof of work that leverages\nquantum supremacy to make mining intractable for classical computers. We have\nrefined the blockchain framework to incorporate the probabilistic nature of\nquantum mechanics, ensuring stability against sampling errors and hardware\ninaccuracies. To validate our approach, we implemented a prototype blockchain\non four D-Wave$^{\\rm TM}$ quantum annealing processors geographically\ndistributed within North America, demonstrating stable operation across\nhundreds of thousands of quantum hashing operations. Our experimental protocol\nfollows the same approach used in the recent demonstration of quantum supremacy\n[1], ensuring that classical computers cannot efficiently perform the same\ncomputation task. By replacing classical machines with quantum systems for\nmining, it is possible to significantly reduce the energy consumption and\nenvironmental impact traditionally associated with blockchain mining. Beyond\nserving as a proof of concept for a meaningful application of quantum\ncomputing, this work highlights the potential for other near-term quantum\ncomputing applications using existing technology.",
        "Modularity is a graph parameter measuring how clearly the set of graph\nvertices may be partitioned into subsets of high edge density. It indicates the\npresence of community structure in the graph. We study its value for a random\npreferential attachment model $G_n^h$ introduced by Barab\\'asi and Albert in\n1999. A graph $G_n^h$ is created from some finite starting graph by adding new\nvertices one by one. A new vertex always connects to $h\\geq1$ already existing\nvertices and those are chosen with probability proportional to their current\ndegrees. We prove that modularity of $G_n^h$ is with high probability upper\nbounded by a function tending to $0$ with $h$ tending to infinity. This\nresolves the conjecture of Prokhorenkova, Pralat and Raigorodskii from 2016. As\na byproduct we obtain novel concentration results for the volume and the edge\ndensity parameters of subsets of $G_n^h$.",
        "Electromagnetic emissions, known as Brunel radiations, are produced in\nplasmas through the coupling between the free electron density and ultrafast\nionizing laser pulses. The radiation spectrum generated in laser-gas\ninteractions is here investigated from a local current model for laser drivers\nwith two frequency components - or \"colors\" - being not necessarily integers of\none another. We provide a general description of this spectrum by deriving\nanalytically the convolution product of the Fourier transforms of the electron\ndensity and of the laser electric field. Our analysis reveals that the only\nknowledge of the optical field extrema in time domain is sufficient to\nreproduce faithfully the numerically-computed Brunel spectrum and justify the\nemergence of various resonance frequencies. The classical combination of two\nlaser harmonics, i.e., a fundamental and its second harmonic, is also\naddressed.",
        "We propose a straightforward method to determine the maximal entanglement of\npure states using the criterion of maximal I-concurrence, a measure of\nentanglement. The square of concurrence for a bipartition $X|X^\\prime$ of a\npure state is defined as $E^2_{X| X ^\\prime}=2[1-tr({\\rho_X}^2)]$. From this,\nwe can infer that the concurrence $E_{X| X ^\\prime}$ reaches its maximum when\n$tr({\\rho_X}^2)$ is minimized. Using this approach, we identify numerous\nAbsolutely Maximally Entangled (AME) pure states that exhibit maximal\nentanglement across all possible bipartitions. Conditions are derived for pure\nstates to achieve maximal mixedness in all bipartitions, revealing that any\npure state with an odd number of subsystem coefficients does not meet the AME\ncriterion. Furthermore, we obtain equal maximal multipartite entangled pure\nstates across all bipartitions using our maximal concurrence criterion.",
        "A Riemannian manifold $M$ is called weakly symmetric if any two points in $M$\ncan be interchanged by an isometry. The compact ones have been well understood,\nand the main remaining case is that of 2-step nilpotent Lie groups. We give a\ncomplete classification of simply connected non-singular weakly symmetric\nnilmanifolds. Besides previously known examples, there are new families with\n3-dimensional center, and a one-parameter family of dimensions 14. The\nclassification is based on the authors classification of non-singular 2-step\nnilpotent Lie groups for which every geodesic is the image of a one parameter\ngroup of isometries.",
        "Many numerical algorithms in scientific computing -- particularly in areas\nlike numerical linear algebra, PDE simulation, and inverse problems -- produce\noutputs that can be represented by semialgebraic functions; that is, the graph\nof the computed function can be described by finitely many polynomial\nequalities and inequalities. In this work, we introduce Semialgebraic Neural\nNetworks (SANNs), a neural network architecture capable of representing any\nbounded semialgebraic function, and computing such functions up to the accuracy\nof a numerical ODE solver chosen by the programmer. Conceptually, we encode the\ngraph of the learned function as the kernel of a piecewise polynomial selected\nfrom a class of functions whose roots can be evaluated using a particular\nhomotopy continuation method. We show by construction that the SANN\narchitecture is able to execute this continuation method, thus evaluating the\nlearned semialgebraic function. Furthermore, the architecture can exactly\nrepresent even discontinuous semialgebraic functions by executing a\ncontinuation method on each connected component of the target function. Lastly,\nwe provide example applications of these networks and show they can be trained\nwith traditional deep-learning techniques.",
        "Context. Acetone (CH3COCH3) is a carbonyl-bearing complex organic molecule,\nyet interstellar observations of acetone remain limited. Studying the formation\nand distribution of CH3COCH3 in the interstellar medium can provide valuable\ninsights into prebiotic chemistry and the evolution of interstellar molecules.\n  Aims. We explore the spatial distribution of CH3COCH3 and its correlation\nwith the O-bearing molecules acetaldehyde (CH3CHO) and methanol (CH3OH), as\nwell as the N-bearing molecule ethyl cyanide (C2H5CN), in massive protostellar\nclumps.\n  Methods. We observed 11 massive protostellar clumps using ALMA at 345 GHz,\nwith an angular resolution of 0.7''-1.0''. Spectral line transitions were\nidentified using the eXtended CASA Line Analysis Software Suite. We constructed\nintegrated intensity maps of CH3COCH3, CH3CHO, CH3OH, and C2H5CN and derived\ntheir rotation temperatures, column densities, and abundances under the\nassumption of local thermodynamic equilibrium.\n  Results. CH3COCH3 is detected in 16 line-rich cores from 9 massive\nprotostellar clumps: 12 high-mass cores, 3 intermediate-mass cores, and 1\nlow-mass core. CH3CHO and CH3OH are also detected in all 16 cores, while C2H5CN\nis detected in 15. The integrated intensity maps reveal similar spatial\ndistributions for CH3COCH3, CH3CHO, CH3OH, and C2H5CN. The line emission peaks\nof all four molecules coincide with the continuum emission peaks in regions\nwithout ultracompact HII regions. Significant correlations are observed in the\nabundances of these molecules, which also exhibit similar average temperatures.\n  Conclusions. Our observational results, supported by chemical models, suggest\nthat CH3COCH3, CH3CHO, and CH3OH originate from the same gas. The observed\ntemperatures and abundances of CH3COCH3 are consistent with model predictions\ninvolving grain surface chemistry.",
        "Time-independent methods, such as the transfer matrix method, are widely used\nto analyze the scattering properties of non-Hermitian systems. However, we\ndemonstrate that these methods become invalid when the scattering matrix\n(S-matrix) exhibits poles in the first quadrant of the complex wave-number\nplane, indicating the presence of time-growing bound states within the system.\nThe breakdown of time-independent approaches is attributed to their inherent\nomission of these bound states. We illustrate this using tight-binding models\nwhere non-Hermiticity is introduced through imaginary on-site potentials or\nasymmetric hopping terms. In all the models considered, parameter regimes exist\nwhere time-independent methods fail. Our findings highlight the critical\nimportance of examining the distribution of S-matrix poles when applying\ntime-independent methods to non-Hermitian scattering systems. Inappropriate\napplication of these methods can lead to unphysical results and erroneous\nconclusions.",
        "This paper proposes a novel framework for real-time adaptive-bitrate video\nstreaming by integrating latent diffusion models (LDMs) within the FFmpeg\ntechniques. This solution addresses the challenges of high bandwidth usage,\nstorage inefficiencies, and quality of experience (QoE) degradation associated\nwith traditional constant bitrate streaming (CBS) and adaptive bitrate\nstreaming (ABS). The proposed approach leverages LDMs to compress I-frames into\na latent space, offering significant storage and semantic transmission savings\nwithout sacrificing high visual quality. While it keeps B-frames and P-frames\nas adjustment metadata to ensure efficient video reconstruction at the user\nside, the proposed framework is complemented with the most state-of-the-art\ndenoising and video frame interpolation (VFI) techniques. These techniques\nmitigate semantic ambiguity and restore temporal coherence between frames, even\nin noisy wireless communication environments. Experimental results demonstrate\nthe proposed method achieves high-quality video streaming with optimized\nbandwidth usage, outperforming state-of-the-art solutions in terms of QoE and\nresource efficiency. This work opens new possibilities for scalable real-time\nvideo streaming in 5G and future post-5G networks.",
        "Understanding the crystal structure of magnetic shape memory alloys is\ncrucial for elucidating their martensite twin boundary supermobility and\nrelated functionalities. This study analyzes and discusses the structure of\nmartensitic single crystals of Ni50.0Mn27.7Ga22.3 and Ni50.0Mn28.1Ga21.9.\nNeutron and X-ray diffraction reveal an anharmonic, incommensurate five-layer\nmodulation that evolves with temperature. This evolution gives rise to two key\nmicrostructural features: i) periodic nanodomains, identified as emerging\na\/b-nanotwins, and ii) long-period commensurate structures, such as the 34O,\n24O, and 14O states, whose orthorhombic unit cells inherently realize\na\/b-nanotwins. Ab initio calculations show that these long-period structures\nare energetically favorable, leading to a lock-in transition. In the alloys\nstudied, the 24O state is the locked-in phase at low temperatures, whereas\nliterature data indicate that Ni50Mn25Ga25 (exact Ni2MnGa stoichiometry)\nevolves toward the 14O phase. Crucially, these results unify two seemingly\ncontrasting structural descriptions -- wave-like modulation and discrete\nnanotwinning -- thereby establishing a foundation for a deeper understanding of\nthe crystal-structure-functionality relationship in magnetic shape memory\nalloys.",
        "Cochlear implants (CIs) are surgically implanted hearing devices, which allow\nto restore a sense of hearing in people suffering from profound hearing loss.\nWireless streaming of audio from external devices to CI signal processors has\nbecome common place. Specialized compression based on the stimulation patterns\nof a CI by deep recurrent autoencoders can decrease the power consumption in\nsuch a wireless streaming application through bit-rate reduction at zero\nlatency.\n  While previous research achieved considerable bit-rate reductions, model\nsizes were ignored, which can be of crucial importance in hearing-aids due to\ntheir limited computational resources. This work investigates maximizing\nobjective speech intelligibility of the coded stimulation patterns of deep\nrecurrent autoencoders while minimizing model size. For this purpose, a\npruning-aware loss is proposed, which captures the impact of pruning during\ntraining. This training with a pruning-aware loss is compared to conventional\nmagnitude-informed pruning and is found to yield considerable improvements in\nobjective intelligibility, especially at higher pruning rates. After\nfine-tuning, little to no degradation of objective intelligibility is observed\nup to a pruning rate of about 55\\,\\%. The proposed pruning-aware loss yields\nsubstantial gains in objective speech intelligibility scores after pruning\ncompared to the magnitude-informed baseline for pruning rates above 45\\,\\%.",
        "Over-the-air computation (AirComp) integrates analog communication with\ntask-oriented computation, serving as a key enabling technique for\ncommunication-efficient federated learning (FL) over wireless networks.\nHowever, owing to its analog characteristics, AirComp-enabled FL (AirFL) is\nvulnerable to both unintentional and intentional interference. In this paper,\nwe aim to attain robustness in AirComp aggregation against interference via\nreconfigurable intelligent surface (RIS) technology to artificially reconstruct\nwireless environments. Concretely, we establish performance objectives tailored\nfor interference suppression in wireless FL systems, aiming to achieve unbiased\ngradient estimation and reduce its mean square error (MSE). Oriented at these\nobjectives, we introduce the concept of phase-manipulated favorable propagation\nand channel hardening for AirFL, which relies on the adjustment of RIS phase\nshifts to realize statistical interference elimination and reduce the error\nvariance of gradient estimation. Building upon this concept, we propose two\nrobust aggregation schemes of power control and RIS phase shifts design, both\nensuring unbiased gradient estimation in the presence of interference.\nTheoretical analysis of the MSE and FL convergence affirms the\nanti-interference capability of the proposed schemes. It is observed that\ncomputation and interference errors diminish by an order of\n$\\mathcal{O}\\left(\\frac{1}{N}\\right)$ where $N$ is the number of RIS elements,\nand the ideal convergence rate without interference can be asymptotically\nachieved by increasing $N$. Numerical results confirm the analytical results\nand validate the superior performance of the proposed schemes over existing\nbaselines.",
        "Equivariant quantum cohomology possesses the structure of a difference module\nby shift operators (Seidel representation) of equivariant parameters. Teleman's\nconjecture suggests that shift operators and equivariant parameters acting on\nQH_T(X) should be identified, respectively, with the Novikov variables and the\nquantum connection of the GIT quotient X\/\/T. This can be interpreted as a form\nof Fourier duality between equivariant quantum cohomology (D-module) of X and\nquantum cohomology (D-module) of the GIT quotient X\/\/T.\n  We introduce the notion of \"quantum volume,\" derived from Givental's path\nintegral over the Floer fundamental cycle, and present a conjectural Fourier\nduality relationship between the T-equivariant quantum volume of X and the\nquantum volume of X\/\/T. We also explore the \"reduction conjecture,\" developed\nin collaboration with Fumihiko Sanda, which expresses the I-function of X\/\/T as\na discrete Fourier transform of the equivariant J-function of X. Furthermore,\nwe demonstrate how to use Fourier analysis of equivariant quantum cohomology to\nobserve toric mirror symmetry and prove a decomposition of quantum cohomology\nD-modules of projective bundles or blowups.",
        "We study vector dark matter (DM) production with Higgs-portal type\ninteractions in the scenarios with a low reheating temperature which can be\nrealized by a prolonged decay of the inflaton after inflation. We take the\nreheating temperature to be large enough to match the observations in Standard\nCosmology such as Big Bang Nucleosynthesis but small enough below the DM mass\nfor the DM production. We analyze the impact of the model parameters including\nthe extra gauge coupling and the reheating temperature on the DM relic density,\ncollider bounds and DM direct and indirect detection experiments. Our results\nreveal a strong correlation between the DM mass ($M_{W_D}$) and the reheating\ntemperature ($T_R$) with ratio of around $T_R\/M_{W_D} \\sim 0.1$ to obtain\ncorrect DM density for detectable interaction strength. The decay processes are\ngenerally subdominant for the DM production but they can be important when\nkinematically allowed and the DM mass is close to half of the Higgses mass. The\nDM production with DM masses below 100 GeV is driven primarily by the\nscatterings of the SM fermions and Higgses decay whereas the case with higher\nDM masses is achieved mainly due to the Higgses scatterings. The enhanced\ncoupling for the strong freeze-in in our framework enables potential detection\nprospects in direct and indirect detections and collider experiments. The\nparameter space of the model has already been explored partly by the current\ndirect detection experiments and it can be explored further by future\nexperiments such as Darwin. On the other hand, the indirect detection\nexperiments in the current and near future are not sensitive enough to test our\nmodel.",
        "We propose a novel method for simulating conditioned diffusion processes\n(diffusion bridges) in Euclidean spaces. By training a neural network to\napproximate bridge dynamics, our approach eliminates the need for\ncomputationally intensive Markov Chain Monte Carlo (MCMC) methods or\nreverse-process modeling. Compared to existing methods, it offers greater\nrobustness across various diffusion specifications and conditioning scenarios.\nThis applies in particular to rare events and multimodal distributions, which\npose challenges for score-learning- and MCMC-based approaches. We propose a\nflexible variational family for approximating the diffusion bridge path measure\nwhich is partially specified by a neural network. Once trained, it enables\nefficient independent sampling at a cost comparable to sampling the\nunconditioned (forward) process.",
        "Remote Photoplethysmography (rPPG) is a promising technique to monitor\nphysiological signals such as heart rate from facial videos. However, the\nlabeled facial videos in this research are challenging to collect. Current rPPG\nresearch is mainly based on several small public datasets collected in simple\nenvironments, which limits the generalization and scale of the AI models.\nSemi-supervised methods that leverage a small amount of labeled data and\nabundant unlabeled data can fill this gap for rPPG learning. In this study, a\nnovel semi-supervised learning method named Semi-rPPG that combines curriculum\npseudo-labeling and consistency regularization is proposed to extract intrinsic\nphysiological features from unlabelled data without impairing the model from\nnoises. Specifically, a curriculum pseudo-labeling strategy with\nsignal-to-noise ratio (SNR) criteria is proposed to annotate the unlabelled\ndata while adaptively filtering out the low-quality unlabelled data. Besides, a\nnovel consistency regularization term for quasi-periodic signals is proposed\nthrough weak and strong augmented clips. To benefit the research on\nsemi-supervised rPPG measurement, we establish a novel semi-supervised\nbenchmark for rPPG learning through intra-dataset and cross-dataset evaluation\non four public datasets. The proposed Semi-rPPG method achieves the best\nresults compared with three classical semi-supervised methods under different\nprotocols. Ablation studies are conducted to prove the effectiveness of the\nproposed methods.",
        "Layer-wise preconditioning methods are a family of memory-efficient\noptimization algorithms that introduce preconditioners per axis of each layer's\nweight tensors. These methods have seen a recent resurgence, demonstrating\nimpressive performance relative to entry-wise (\"diagonal\") preconditioning\nmethods such as Adam(W) on a wide range of neural network optimization tasks.\nComplementary to their practical performance, we demonstrate that layer-wise\npreconditioning methods are provably necessary from a statistical perspective.\nTo showcase this, we consider two prototypical models, linear representation\nlearning and single-index learning, which are widely used to study how typical\nalgorithms efficiently learn useful features to enable generalization. In these\nproblems, we show SGD is a suboptimal feature learner when extending beyond\nideal isotropic inputs $\\mathbf{x} \\sim \\mathsf{N}(\\mathbf{0}, \\mathbf{I})$ and\nwell-conditioned settings typically assumed in prior work. We demonstrate\ntheoretically and numerically that this suboptimality is fundamental, and that\nlayer-wise preconditioning emerges naturally as the solution. We further show\nthat standard tools like Adam preconditioning and batch-norm only mildly\nmitigate these issues, supporting the unique benefits of layer-wise\npreconditioning.",
        "As the Internet of Things (IoT) becomes more embedded within our daily lives,\nthere is growing concern about the risk `smart' devices pose to network\nsecurity. To address this, one avenue of research has focused on automated IoT\ndevice identification. Research has however largely neglected the\nidentification of IoT device firmware versions. There is strong evidence that\nIoT security relies on devices being on the latest version patched for known\nvulnerabilities. Identifying when a device has updated (has changed version) or\nnot (is on a stable version) is therefore useful for IoT security. Version\nidentification involves challenges beyond those for identifying the model,\ntype, and manufacturer of IoT devices, and traditional machine learning\nalgorithms are ill-suited for effective version identification due to being\nlimited by the availability of data for training. In this paper, we introduce\nan effective technique for identifying IoT device versions based on transfer\nlearning. This technique relies on the idea that we can use a Twin Neural\nNetwork (TNN) - trained at distinguishing devices - to detect differences\nbetween a device on different versions. This facilitates real-world\nimplementation by requiring relatively little training data. We extract\nstatistical features from on-wire packet flows, convert these features into\ngreyscale images, pass these images into a TNN, and determine version changes\nbased on the Hedges' g effect size of the similarity scores. This allows us to\ndetect the subtle changes present in on-wire traffic when a device changes\nversion. To evaluate our technique, we set up a lab containing 12 IoT devices\nand recorded their on-wire packet captures for 11 days across multiple firmware\nversions. For testing data held out from training, our best performing model is\nshown to be 95.83% and 84.38% accurate at identifying stable versions and\nversion changes respectively."
      ]
    }
  },
  {
    "id":2411.07031,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Accuracy of Chatbots in Citing Journal Articles",
    "start_abstract":"This cross-sectional study quantifies the journal article citation error rate of an artificial intelligence chatbot.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "ChatGPT Hallucinates Non-existent Citations: Evidence from Economics"
      ],
      "abstract":[
        "In this study, we generate prompts derived from every topic within the Journal of Economic Literature to assess abilities both GPT-3.5 and GPT-4 versions ChatGPT large language model (LLM) write about economic concepts. demonstrates considerable competency in offering general summaries but also cites non-existent references. More than 30% citations provided by version do not exist rate is only slightly reduced for version. Additionally, our findings suggest that reliability decreases as become more specific. We provide quantitative evidence errors output demonstrate importance LLM verification. JEL Codes: B4; O33; I2"
      ],
      "categories":[
        "q-fin.ST"
      ]
    },
    "list":{
      "title":[
        "Existence of optimal controls for stochastic partial differential\n  equations with fully local monotone coefficients",
        "Spin-polarized STM measurement scheme for quantum geometric tensor",
        "Mechanism of tulip flame formation in highly reactive and low reactive\n  gas mixtures",
        "Poincar\\'{e} sphere engineering of dynamical ferroelectric topological\n  solitons",
        "An Optimal Transport approach to arbitrage correction: Application to\n  volatility Stress-Tests",
        "Comment on: \"2005 VL1 is not Venera-2\"",
        "Balancing Flexibility and Interpretability: A Conditional Linear Model\n  Estimation via Random Forest",
        "Numerical simulation of a fine-tunable F\\\"oppl-von K\\'arm\\'an model for\n  foldable and bilayer plates",
        "Bring the noise: exact inference from noisy simulations in collider\n  physics",
        "Quantum Hamiltonian Descent for Non-smooth Optimization",
        "On the spatial distribution of luminous blue variables in the M33 galaxy",
        "The existence of pyramidal Steiner triple systems over abelian groups",
        "Change Point Detection for Random Objects with Possibly Periodic\n  Behavior",
        "Rotating and non-linear magnetic-charged black hole with an anisotropic\n  matter field",
        "SUSY transformation as the coupler of non-interacting systems",
        "Multisymplectic structure of nonintegrable Henon-Heiles system",
        "Tunable Kernel-Nulling interferometry for direct exoplanet detection",
        "$\\Omega_{bbb}\\Omega_{bbb}\\Omega_{bbb}$ tribaryons",
        "ALMA observations of massive clouds in the central molecular zone: slim\n  filaments tracing parsec-scale shocks",
        "Reconstruction of proton relative stopping power with a granular\n  calorimeter detector model",
        "Correlations drive the attosecond response of strongly-correlated\n  insulators",
        "Nesting is not Contracting",
        "Interpreting the $X(2370)$ and $X(2600)$ as light tetraquark states",
        "Mean Field Games with Reflected Dynamics",
        "Two repeated quasi-periodic oscillations in the FSRQ S5 1044+71 observed\n  by TESS",
        "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
        "The three obdurate conjectures of differential geometry",
        "Photonic Negative Differential Thermal Conductance Enabled by NIS\n  Junctions",
        "Non-dispersive graded impedance acoustic lenses"
      ],
      "abstract":[
        "This paper deals with a stochastic optimal feedback control problem for the\ncontrolled stochastic partial differential equations. More precisely, we\nestablish the existence of stochastic optimal feedback control for the\ncontrolled stochastic partial differential equations with fully monotone\ncoefficients by a minimizing sequence for the control problem. Using the\nFaedo-Galerkin approximations, the uniform estimates and the tightness in some\nappropriate space for the Faedo-Galerkin approximating solution can be obtain\nto prove the well-posedness of the controlled stochastic partial differential\nequations with fully monotone coefficients. The results obtained in the present\npaper may be applied to various types of controlled stochastic partial\ndifferential equations, such as the controlled stochastic convection diffusion\nequation.",
        "Quantum geometric tensor (QGT) reflects the geometry of the eigenstates of a\nsystem's Hamiltonian. The full characterization of QGT is essential for various\nquantum systems. However, it is challenging to characterize the QGT of the\nsolid-state systems. Here we present a scheme by using spin-polarized STM to\nmeasure QGT of two-dimensional solid-state systems, in which the spin texture\nis extracted from geometric amplitudes of Friedel oscillations induced by the\nintentionally introduced magnetic impurity and then the QGT is derived from the\nmomentum differential of spin texture. The surface states of topological\ninsulator (TISS), as a model spin system, is promising to demonstrate the\nscheme. In a TI slab, the gapped TISS host finite quantum metric and Berry\ncurvature as the symmetric real part and the antisymmetric imaginary part of\nQGT, respectively. Thus, a detailed calculations guide the use of the developed\nscheme to measure the QGT of gapped TISS with or without an external in-plane\nmagnetic field. This study provides a feasible scheme for measuring QGT of\ntwo-dimensional solid-state systems, and hints at the great potential of the\ninformation extraction from the geometric amplitudes of STM and other\nmeasurement.",
        "The early stages of flame dynamics and the development and evolution of tulip\nflames in closed tubes of various aspect ratios and in a semi-open tube are\nstudied by solving the fully compressible reactive Navier-Stokes equations\nusing a high-order numerical method coupled to detailed chemical models in a\nstoichiometric hydrogen\/air and methane\/air mixtures. The use of adaptive mesh\nrefinement provides adequate resolution of the flame reaction zone, pressure\nwaves, and flame-pressure wave interactions. The purpose of this study is to\ngain a deeper insight into the influence of chemical kinetics on the combustion\nregimes leading to the formation of a tulip flame and its subsequent evolution.\nThe simulations highlight the effect of flame thickness, flame velocity, and\nreaction order on the intensity of the rarefaction wave generated by the flame\nduring the deceleration phase, which is the principal physical mechanism of\ntulip flame formation. The obtained results explain most of the experimentally\nobserved features of tulip flame formation, e.g. faster tulip flame formation\nwith deeper tulip shape for faster flames compared to slower flames.",
        "Geometric representation lays the basis for understanding and flexible tuning\nof topological transitions in many physical systems. An example is given by the\nPoincar\\'{e} sphere (PS) that provides an intuitive and continuous\nparameterization of the spin or orbital angular momentum (OAM) light states.\nHere, we apply this geometric construction to understand and continuously\nencode dynamical topologies of ferroelectric solitons driven by OAM-tunable\nlight. We show that: (1) PS engineering enables controlled creation of dynamic\npolar antiskyrmions that are rarely found in ferroelectrics; (2) We link such\ntopological transition to the tuning of the light beam as a ``knob'' from OAM\n(PS pole) to non-OAM (PS equator) modes; (3) Intermediate OAM-state structured\nlight results in new ferroelectric topologies of temporally hybrid\nskyrmion-antiskyrmion states. Our study offers new approaches of robust control\nand flexible tuning of topologies of matter using structured light.",
        "We present a method based on optimal transport to remove arbitrage\nopportunities within a finite set of option prices. The method is notably\nintended for regulatory stress-tests, which impose to apply important local\ndistortions to implied volatility surfaces. The resulting stressed option\nprices are naturally associated to a family of signed marginal measures: we\nformulate the process of removing arbitrage as a projection onto the subset of\nmartingale measures with respect to a Wasserstein metric in the space of signed\nmeasures. We show how this projection problem can be recast as an optimal\ntransport problem; in view of the numerical solution, we apply an entropic\nregularization technique. For the regularized problem, we derive a strong\nduality formula, show convergence results as the regularization parameter\napproaches zero, and formulate a multi-constrained Sinkhorn algorithm, where\neach iteration involves, at worse, finding the root of an explicit scalar\nfunction. The convergence of this algorithm is also established. We compare our\nmethod with the existing approach by [Cohen, Reisinger and Wang, Appl.\\ Math.\\\nFin.\\ 2020] across various scenarios and test cases.",
        "I show that the small differences between the orbital parameters of the dark\ncomet 2005 VL1 and the Venera 2 spacecraft (reported in arXiv:2503.07972) are\nof the magnitude expected from gravitational deflection by a close encounter of\nVenera 2 with Venus.",
        "Traditional parametric econometric models often rely on rigid functional\nforms, while nonparametric techniques, despite their flexibility, frequently\nlack interpretability. This paper proposes a parsimonious alternative by\nmodeling the outcome $Y$ as a linear function of a vector of variables of\ninterest $\\boldsymbol{X}$, conditional on additional covariates\n$\\boldsymbol{Z}$. Specifically, the conditional expectation is expressed as\n$\\mathbb{E}[Y|\\boldsymbol{X},\\boldsymbol{Z}]=\\boldsymbol{X}^{T}\\boldsymbol{\\beta}(\\boldsymbol{Z})$,\nwhere $\\boldsymbol{\\beta}(\\cdot)$ is an unknown Lipschitz-continuous function.\nWe introduce an adaptation of the Random Forest (RF) algorithm to estimate this\nmodel, balancing the flexibility of machine learning methods with the\ninterpretability of traditional linear models. This approach addresses a key\nchallenge in applied econometrics by accommodating heterogeneity in the\nrelationship between covariates and outcomes. Furthermore, the heterogeneous\npartial effects of $\\boldsymbol{X}$ on $Y$ are represented by\n$\\boldsymbol{\\beta}(\\cdot)$ and can be directly estimated using our proposed\nmethod. Our framework effectively unifies established parametric and\nnonparametric models, including varying-coefficient, switching regression, and\nadditive models. We provide theoretical guarantees, such as pointwise and\n$L^p$-norm rates of convergence for the estimator, and establish a pointwise\ncentral limit theorem through subsampling, aiding inference on the function\n$\\boldsymbol\\beta(\\cdot)$. We present Monte Carlo simulation results to assess\nthe finite-sample performance of the method.",
        "A numerical scheme is proposed to identify low energy configurations of a\nF\\\"oppl-von K\\'arm\\'an model for bilayer plates. The dependency of the\ncorresponding elastic energy on the in-plane displacement $u$ and the\nout-of-plane deflection $w$ leads to a practical minimization of the functional\nvia a decoupled gradient flow. In particular, the energies of the resulting\niterates are shown to be monotonically decreasing. The discretization of the\nmodel relies on $P1$ finite elements for the horizontal part $u$ and utilizes\nthe discrete Kirchhoff triangle for the vertical component $w$. The model\nallows for analysing various different problem settings via numerical\nsimulation: (i) stable low-energy configurations are detected dependent on a\nspecified prestrain described by elastic material properties, (ii) curvature\ninversions of spherical and cylindrical configurations are investigated, (iii)\nelastic responses of foldable cardboards for different spontaneous curvatures\nand crease geometries are compared.",
        "We rely on Monte Carlo (MC) simulations to interpret searches for new physics\nat the Large Hadron Collider (LHC) and elsewhere. These simulations result in\nnoisy and approximate estimators of selection efficiencies and likelihoods. In\nthis context we pioneer an exact-approximate computational method -\nexact-approximate Markov Chain Monte Carlo - that returns exact inferences\ndespite noisy simulations. To do so, we introduce an unbiased estimator for a\nPoisson likelihood. We demonstrate the new estimator and new techniques in\nexamples based on a search for neutralinos and charginos at the LHC using a\nsimplified model. We find attractive performance characteristics - exact\ninferences are obtained for a similar computational cost to approximate ones\nfrom existing methods and inferences are robust with respect to the number of\nevents generated per point.",
        "Non-smooth optimization models play a fundamental role in various\ndisciplines, including engineering, science, management, and finance. However,\nclassical algorithms for solving such models often struggle with convergence\nspeed, scalability, and parameter tuning, particularly in high-dimensional and\nnon-convex settings. In this paper, we explore how quantum mechanics can be\nleveraged to overcome these limitations. Specifically, we investigate the\ntheoretical properties of the Quantum Hamiltonian Descent (QHD) algorithm for\nnon-smooth optimization in both continuous and discrete time. First, we propose\ncontinuous-time variants of the general QHD algorithm and establish their\nglobal convergence and convergence rate for non-smooth convex and strongly\nconvex problems through a novel Lyapunov function design. Furthermore, we prove\nthe finite-time global convergence of continuous-time QHD for non-smooth\nnon-convex problems under mild conditions (i.e., locally Lipschitz). In\naddition, we propose discrete-time QHD, a fully digitized implementation of QHD\nvia operator splitting (i.e., product formula). We find that discrete-time QHD\nexhibits similar convergence properties even with large time steps. Finally,\nnumerical experiments validate our theoretical findings and demonstrate the\ncomputational advantages of QHD over classical non-smooth non-convex\noptimization algorithms.",
        "In the current paper, we present a study of the spatial distribution of\nluminous blue variables (LBVs) and various LBV candidates (cLBVs) with respect\nto OB associations in the M33 galaxy. The identification of blue star groups\nwas based on the LGGS data and was carried out by two clustering algorithms\nwith initial parameters determined during simulations of random stellar fields.\nWe have found that the distribution of distances to the nearest OB association\nobtained for the LBV\/cLBV sample is close to that for massive stars with\n$M_{\\rm init}>20\\,M_\\odot$ and Wolf-Rayet stars. This result is in good\nagreement with the standard assumption that luminous blue variables represent\nan intermediate stage in the evolution of the most massive stars. However, some\nobjects from the LBV\/cLBV sample, particularly Fe$\\,$II-emission stars,\ndemonstrated severe isolation compared to other massive stars, which, together\nwith certain features of their spectra, implicitly indicates that the nature of\nthese objects and other LBVs\/cLBVs may differ radically.",
        "A Steiner triple system STS$(v)$ is called $f$-pyramidal if it has an\nautomorphism group fixing $f$ points and acting sharply transitively on the\nremaining ones. In this paper, we focus on the STSs that are $f$-pyramidal over\nsome abelian group. Their existence has been settled only for the smallest\nadmissible values of $f$, that is, $f=0,1,3$.\n  In this paper, we complete this result and determine, for every $f>3$, the\nspectrum of values $(f,v)$ for which there is an $f$-pyramidal STS$(v)$ over an\nabelian group. This result is obtained by constructing difference families\nrelative to a suitable partial spread.",
        "Time-varying random objects have been increasingly encountered in modern data\nanalysis. Moreover, in a substantial number of these applications, periodic\nbehavior of the random objects has been observed. We introduce a new, powerful\nscan statistic and corresponding test for the precise identification and\nlocalization of abrupt changes in the distribution of non-Euclidean random\nobjects with possibly periodic behavior. Our approach is nonparametric and\neffectively captures the entire distribution of these random objects.\nRemarkably, it operates with minimal tuning parameters, requiring only the\nspecification of cut-off intervals near endpoints, where change points are\nassumed not to occur. Our theoretical contributions include deriving the\nasymptotic distribution of the test statistic under the null hypothesis of no\nchange points, establishing the consistency of the test in the presence of\nchange points under contiguous alternatives and providing rigorous guarantees\non the near-optimal consistency in estimating the number and locations of\nchange points, whether dealing with a single change point or multiple ones. We\ndemonstrate that the most competitive method currently in the literature for\nchange point detection in random objects is degraded by periodic behavior, as\nperiodicity leads to blurring of the changes that this procedure aims to\ndiscover. Through comprehensive simulation studies, we demonstrate the superior\npower and accuracy of our approach in both detecting change points and\npinpointing their locations, across scenarios involving both periodic and\nnonperiodic random objects. Our main application is to weighted networks,\nrepresented through graph Laplacians. The proposed method delivers highly\ninterpretable results, as evidenced by the identification of meaningful change\npoints in the New York City Citi Bike sharing system that align with\nsignificant historical events.",
        "We present the solution of a non-linear magnetic-charged black hole with an\nanisotropic matter field and further extend it to obtain the corresponding\nrotating black hole solution using the modified Newman-Janis algorithm. The\nevent horizon and ergosphere of the rotating black hole are studied in terms of\nthe perspective of geometric properties, revealing that the rotating black hole\ncan have up to three horizons. The first law of thermodynamics and the\nsquared-mass formula for the rotating black hole are derived from a\nthermodynamic perspective, based on which we obtain the thermodynamic\nquantities and study the thermodynamic stability of the rotating black hole.\nAdditionally, we calculate the Penrose process for the rotating black hole,\nindicating the influence of various black hole parameters on the maximal\nefficiency of the Penrose process.",
        "Quasi-one-dimensional chains of atoms can be effectively described by\none-dimensional Dirac-type equation. Crystal structure of the chain is\nreflected by pseudo-spin of the quasi-particles. In the article, we present a\nsimple framework where supersymmetric transformation is utilized to generate an\ninteraction between two, initially non-interacting systems described by\npseudo-spin-one Dirac-type equation. In the presented example, the\ntransformation converts two asymptotically non-interacting atomic chains into a\nsaw chain locally. The model possesses a flat band whose energy can be\nfine-tuned deliberately.",
        "Multi-symplectic integrators are typically regarded as a discretization of\nthe Hamiltonian partial differential equations. This is due to the fact that,\nfor generic finite-dimensional Hamiltonian systems, there exists only one\nindependent symplectic structure. In this note, the second invariant symplectic\nform is presented for the nonintegrable Henon-Heiles system, Kepler problem,\nintegrable and non-integrable Toda type systems. This approach facilitates the\nconstruction of a multi-symplectic integrator, which effectively preserves both\nsymplectic forms for these benchmark problems.",
        "Nulling interferometry is a promising technique for direct detection of\nexoplanets. However, the performance of current devices is limited by different\nperturbations sources and especially by its sensitivity to any phase\naberrations. The work presented here attempts to overcome those limitations by\nusing a four-telescopes nulling interferometer architecture, called\nKernel-Nuller, which includes a recombiner that positions the four signals in\nphase quadrature. This architecture is based on an integrated optical component\ncontaining 14 electronically controlled phase shifters, used to correct optical\npath differences that would be induced by manufacturing defects. The first part\nof the study consists in the development of an algorithm providing the delays\nto be injected into the component to optimize the performance of that device.\nThe next step of this study deals with the analysis of the intensity\ndistributions produced at the output of the Kernel-Nuller through a series of\nobservations. Then we apply statistical tests and data treatment techniques to\ndetect the signature of an exoplanets.",
        "We study the possible existence of bound states of three $\\Omega_{bbb}$\nbaryons. We consider only $S$ wave interactions and we start from recent\nlattice QCD results which give a strongly attractive potential between two\n$\\Omega_{bbb}$ baryons in the $^1S_0$ channel. We analyze different scenarios.\nAt baryonic level, the $\\Omega_{bbb}\\Omega_{bbb}$ interaction could be\nunderstood to be basically spin-independent, so that the two contributing\nchannels, $^1S_0$ and $^5S_2$, would have a very similar interaction. This\nbaryonic analysis leads to the existence of bound states in the three-body\nsystem. At the quark level, repulsive effects would appear in the $^5S_2$\nchannel, making it more repulsive than the $^1S_0$ channel. We study the effect\nof such repulsion in terms of its range.",
        "The central molecular zone (CMZ) of our Galaxy exhibits widespread emission\nfrom SiO and various complex organic molecules (COMs), yet the exact origin of\nsuch emission is uncertain. Here we report the discovery of a unique class of\nlong ($>$0.5 pc) and narrow ($<$0.03 pc) filaments in the emission of SiO 5$-$4\nand eight additional molecular lines, including several COMs, in our ALMA 1.3\nmm spectral line observations toward two massive molecular clouds in the CMZ,\nwhich we name as slim filaments. However, these filaments are not detected in\nthe 1.3 mm continuum at the 5$\\sigma$ level. Their line-of-sight velocities are\ncoherent and inconsistent with being outflows. The column densities and\nrelative abundances of the detected molecules are statistically similar to\nthose in protostellar outflows but different from those in dense cores within\nthe same clouds. Turbulent pressure in these filaments dominates over self\ngravity and leads to hydrostatic inequilibrium, indicating that they are a\ndifferent class of objects than the dense gas filaments in dynamical\nequilibrium ubiquitously found in nearby molecular clouds. We argue that these\nnewly detected slim filaments are associated with parsec-scale shocks, likely\narising from dynamic interactions between shock waves and molecular clouds. The\ndissipation of the slim filaments may replenish SiO and COMs in the\ninterstellar medium and lead to their widespread emission in the CMZ.",
        "Proton computed tomography (pCT) aims to facilitate precise dose planning for\nhadron therapy, a promising and effective method for cancer treatment. Hadron\ntherapy utilizes protons and heavy ions to deliver well focused doses of\nradiation, leveraging the Bragg peak phenomenon to target tumors while sparing\nhealthy tissues. The Bergen pCT Collaboration aims to develop a novel pCT\nscanner, and accompanying reconstruction algorithms to overcome current\nlimitations. This paper focuses on advancing the track- and image\nreconstruction algorithms, thereby enhancing the precision of the dose planning\nand reducing side effects of hadron therapy. A neural network aided track\nreconstruction method is presented.",
        "Attosecond spectroscopy of materials has provided invaluable insight into\nlight-driven coherent electron dynamics. However, attosecond spectroscopies\nhave so far been focused on weakly-correlated materials. As a result, the\nbehavior of strongly-correlated systems is largely unknown at sub- to\nfew-femtosecond timescales, even though it is typically the realm at which\nelectron-electron interactions operate. Here we conduct attosecond-resolved\nexperiments on the correlated insulator nickel oxide, and compare its response\nto a common band insulator, revealing fundamentally different behaviors. The\nresults, together with state-of-the art time-dependent $\\textit{ab initio}$\ncalculations, show that the correlated system response is governed by a\nlaser-driven quench of electron correlations. The evolution of the on-site\nelectronic interaction is measured here at its natural timescale, marking the\nfirst direct measurement of Hubbard $U$ renormalization in NiO. It is found to\ntake place within a few femtoseconds, after which structural changes slowly\nstart to take place. The resulting picture sheds light on the entire\nlight-induced response of a strongly-correlated system, from attosecond to\nlong-lived effects.",
        "The default way of proving holographic entropy inequalities is the\ncontraction method. It divides Ryu-Takayanagi (RT) surfaces on the `greater\nthan' side of the inequality into segments, then glues the segments into\ncandidate RT surfaces for terms on the `less than' side. Here we discuss how\nproofs by contraction are constrained and informed by entanglement wedge\nnesting (EWN) -- the property that enlarging a boundary region can only enlarge\nits entanglement wedge. We propose that: (i) all proofs by contraction\nnecessarily involve candidate RT surfaces, which violate EWN; (ii) violations\nof EWN in contraction proofs of maximally tight inequalities occur commonly and\n-- where this can be quantified -- with maximal density near boundary\nconditions; (iii) the non-uniqueness of proofs by contraction reflects\ninequivalent ways of violating EWN. As evidence and illustration, we study the\nrecently discovered infinite families of holographic entropy inequalities,\nwhich are associated with tessellations of the torus and the projective plane.\nWe explain the logic, which underlies their proofs by contraction. We find that\nall salient aspects of the requisite contraction maps are dictated by EWN while\nall their variable aspects set the scheme for how to violate EWN. We comment on\nwhether the tension between EWN and contraction maps might help in\ncharacterizing maximally tight holographic entropy inequalities.",
        "Inspired by the states $X(2370)$ and $X(2600)$ reported by the BESIII\nCollaboration, we systematically investigate the mass spectra of light compact\ntetraquark states with configurations $ud\\bar{u}\\bar{d}$, $us\\bar{u}\\bar{s}$,\nand $ss\\bar{s}\\bar{s}$ in the $J^{PC}=0^{-+}$ and $2^{-+}$ channels using the\nQCD sum rules approach. To effectively describe these tetraquark states, we\nconstruct appropriate interpolating tetraquark currents featuring three Lorentz\nindices while avoiding derivative operators. Through meticulous calculations of\ncorrelation functions up to dimension 10 condensates, we extract the mass\nspectra for both $0^{-+}$ and $2^{-+}$ states by employing the projection\noperator technique. Our results indicate that the masses of light tetraquarks\nspan $1.5-2.5~\\text{GeV}$ for $0^{-+}$ states and $2.4-2.7~\\text{GeV}$ for\n$2^{-+}$ states. Notably, our analysis suggests that the $X(2370)$ state could\nbe interpreted as a $0^{-+}$ $us\\bar{u}\\bar{s}$ or $ss\\bar{s}\\bar{s}$\ntetraquark state, while the $X(2600)$ state is likely to be a $2^{-+}$\n$us\\bar{u}\\bar{s}$ tetraquark. These intriguing findings warrant further\ndetailed investigation in future studies to better understand the nature of\nthese states.",
        "This paper establishes an equilibrium existence result for a class of Mean\nField Games involving Reflected Stochastic Differential Equations. The proof\nrelies on the framework of relaxed controls and martingale problems.",
        "In this work, we report for the first time two repeated quasi-periodic\noscillations (QPOs) in the light curve of the Flat Spectrum Radio Quasar (FSRQ)\nS5 1044+71. This source was observed by the Transiting Exoplanet Survey\nSatellite (TESS) in multiple sectors. We used the generalized Lomb-Scargle\nperiodogram method and weighted wavelet Z-transform method to search for\nsignificant periodic signals. The main results are as follows: We found QPOs of\n$\\sim$ 7.0 days (persisted for 4 cycles, with a significance of\n$\\sim3.5\\sigma$) and $\\sim$ 7.3 days (persisted for 5 cycles, with a\nsignificance of $\\sim3.8\\sigma$) in the light curves of Sector 47 and EP1,\nrespectively. Considering range of error, we consider them to be the same. We\ndiscussed two likely models of these rapid quasi-periodic variations: One comes\nfrom the jet and the other from the accretion disk. For the first one, we\nconsider kink instability of the jet as a plausible explanation. Second, the\nQPO is probable to come from the main hot spots in the accretion disk, which is\nlocated approximately within the innermost stable circular orbit allowed by\ngeneral relativity. Based on this model, we estimate the mass of the black hole\nin S5 1044+71 to be $3.49 \\times 10^9 M_{\\odot}$.",
        "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate.",
        "We explore the role of symmetry in three obdurate conjectures of differential\ngeometry: the Carath\\'eodory, the Willmore and the Lawson Conjectures.",
        "Owing to their sensitivity to temperature fluctuations, normal\nmetal-insulator-superconductor (NIS) junctions are leveraged in various thermal\ndevices. This study illustrates that two NISIN reservoirs can achieve a\nmeasurable negative differential thermal conductance (NDTC). This phenomenon is\nenabled by photon-mediated heat exchange, which is profoundly affected by the\ntemperature-dependent impedance matching between the reservoirs. Under\nappropriate configurations, the heat current is suppressed for increasingly\nlarge temperature gradients, leading to NDTC. We also propose experimental\nconfigurations where it is possible to discriminate this effect unambiguously.\nWe employ superconducting aluminum in conjunction with either silver or\nepitaxial InAs to facilitate the experimental observation of NDTC at low\ntemperatures over significant sub-Kelvin ranges. This advances the development\nof devices that exploit NDTC to enhance heat and temperature regulation in\ncryogenic environments, such as thermal switches, transistors, and amplifiers.",
        "Lenses are typically based on refractive index profiles derived from the\ngeometric approximation of high-frequency waves, yet the critical issue of\nimpedance mismatch is often neglected. Mismatched devices suffer from unwanted\nreflections and dispersion, which can significantly degrade performance in\npractical applications. In this work, we propose impedance profiles for lenses\nto achieve efficient wave transmission while maintaining the desired refractive\nindex and minimizing dispersion effects. A family of impedance profiles is\nderived from the acoustic wave equation such that the phase velocity is\npreserved. First, the 1D setting is considered to explain how dispersion occurs\ninside a lens and at its interfaces. Then, the method is applied to 2D\naxisymmetric configurations where the impedance mismatch is radially\nredistributed. These profiles are demonstrated in the acoustic setting of a\nLuneburg lens, but can be easily extended to more general scenarios such as\nimaging or cloaking in air and water, where matching the impedance of the\nbackground poses significant challenges."
      ]
    }
  },
  {
    "id":2411.06184,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Multi-Task Bayesian Optimization",
    "start_abstract":"Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and shown to yield state-of-the-art performance with impressive ease efficiency. In this paper, we explore whether it is possible transfer knowledge gained from previous optimizations new tasks in order find optimal hyperparameter settings more efficiently. Our approach based on extending multi-task Gaussian processes optimization. We show that method significantly speeds up process when compared standard single-task approach. further propose straightforward extension our algorithm jointly minimize average error across multiple demonstrate how can be used greatly speed k-fold cross-validation. Lastly, an adaptation developed acquisition function, entropy search, cost-sensitive, setting. utility function by leveraging small dataset hyper-parameter large dataset. dynamically chooses which query most information per unit cost.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "New Pathologic Classification of Lung Cancer: Relevance for Clinical Practice and Clinical Trials"
      ],
      "abstract":[
        "We summarize significant changes in pathologic classification of lung cancer resulting from the 2011 International Association for Study Lung Cancer\/American Thoracic Society\/European Respiratory Society (IASLC\/ATS\/ERS) adenocarcinoma classification. The was developed by an international core panel experts representing IASLC, ATS, and ERS with oncologists\/pulmonologists, pathologists, radiologists, molecular biologists, thoracic surgeons. Because 70% patients present advanced stages, a new approach to small biopsies cytology specific terminology criteria focused on need distinguishing squamous cell carcinoma testing EGFR mutations ALK rearrangement. Tumors previously classified as non-small-cell carcinoma, not otherwise specified, because lack clear or morphology should be further using limited immunohistochemical workup preserve tissue testing. terms \"bronchioloalveolar carcinoma\" \"mixed subtype adenocarcinoma\" have been discontinued. For resected adenocarcinomas, concepts situ minimally invasive define who, if they undergo complete resection, will 100% disease-free survival. Invasive adenocarcinomas are now predominant pattern after comprehensive histologic subtyping lepidic, acinar, papillary, solid patterns; micropapillary is added poor prognosis. Former mucinous bronchioloalveolar carcinomas called \"invasive adenocarcinoma.\" field rapidly evolving advances occurring frequent basis, particularly arena, this provides much needed standard diagnosis only patient care but also clinical trials TNM"
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Exact Learning of Permutations for Nonzero Binary Inputs with\n  Logarithmic Training Size and Quadratic Ensemble Complexity",
        "Entropy-Guided Attention for Private LLMs",
        "On the Uncertainty of a Simple Estimator for Remote Source Monitoring\n  over ALOHA Channels",
        "Unified Guidance for Geometry-Conditioned Molecular Generation",
        "In Situ Optimization of an Optoelectronic Reservoir Computer with\n  Digital Delayed Feedback",
        "Explaining 3D Computed Tomography Classifiers with Counterfactuals",
        "Detection of Rumors and Their Sources in Social Networks: A\n  Comprehensive Survey",
        "Anisotropic Exchange Spin Model to Investigate the Curie Temperature\n  Dispersion of Finite-Size L10-FePt Magnetic Nanoparticles",
        "PluralLLM: Pluralistic Alignment in LLMs via Federated Learning",
        "Fifteen Years of M31* X-ray Variability and Flares",
        "A Semantic-Loss Function Modeling Framework With Task-Oriented Machine\n  Learning Perspectives",
        "YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised\n  Re-ID",
        "Robust Egoistic Rigid Body Localization",
        "Diminishing Waters: The Great Salt Lake's Desiccation and Its Mental\n  Health Consequences",
        "Code-Verification Techniques for an Arbitrary-Depth Electromagnetic Slot\n  Model",
        "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems",
        "From Trust to Truth: Actionable policies for the use of AI in\n  fact-checking in Germany and Ukraine",
        "The assembly of supermassive black holes at $z<1$ in early-type galaxies\n  from scaling relations",
        "Partially connected contributions to baryon masses in QCD+QED",
        "Reading the unreadable: Creating a dataset of 19th century English\n  newspapers using image-to-text language models",
        "Finite Sample Analysis of System Poles for Ho-Kalman Algorithm",
        "Does Generation Require Memorization? Creative Diffusion Models using\n  Ambient Diffusion",
        "Incorporating Cyclic Group Equivariance into Deep Learning for Reliable\n  Reconstruction of Rotationally Symmetric Tomography Systems",
        "The Fertile Steppe: Computability Logic and the decidability of one of\n  its fragments",
        "HuGDiffusion: Generalizable Single-Image Human Rendering via 3D Gaussian\n  Diffusion",
        "Mol-LLM: Generalist Molecular LLM with Improved Graph Utilization",
        "Policy Learning with a Natural Language Action Space: A Causal Approach",
        "Parabolic dielectric reflector for extreme on-chip spot-size conversion\n  with broad bandwidth",
        "Maximum force conjecture in curved spacetimes of stable self-gravitating\n  matter configurations"
      ],
      "abstract":[
        "The ability of an architecture to realize permutations is quite fundamental.\nFor example, Large Language Models need to be able to correctly copy (and\nperhaps rearrange) parts of the input prompt into the output. Classical\nuniversal approximation theorems guarantee the existence of parameter\nconfigurations that solve this task but offer no insights into whether\ngradient-based algorithms can find them. In this paper, we address this gap by\nfocusing on two-layer fully connected feed-forward neural networks and the task\nof learning permutations on nonzero binary inputs. We show that in the infinite\nwidth Neural Tangent Kernel (NTK) regime, an ensemble of such networks\nindependently trained with gradient descent on only the $k$ standard basis\nvectors out of $2^k - 1$ possible inputs successfully learns any fixed\npermutation of length $k$ with arbitrarily high probability. By analyzing the\nexact training dynamics, we prove that the network's output converges to a\nGaussian process whose mean captures the ground truth permutation via\nsign-based features. We then demonstrate how averaging these runs (an\n\"ensemble\" method) and applying a simple rounding step yields an arbitrarily\naccurate prediction on any possible input unseen during training. Notably, the\nnumber of models needed to achieve exact learning with high probability (which\nwe refer to as ensemble complexity) exhibits a linearithmic dependence on the\ninput size $k$ for a single test input and a quadratic dependence when\nconsidering all test inputs simultaneously.",
        "The pervasiveness of proprietary language models has raised critical privacy\nconcerns, necessitating advancements in private inference (PI), where\ncomputations are performed directly on encrypted data without revealing users'\nsensitive information. While PI offers a promising solution, its practical\ndeployment is hindered by substantial communication and latency overheads,\nprimarily stemming from nonlinear operations. To address this, we introduce an\ninformation-theoretic framework to characterize the role of nonlinearities in\ndecoder-only language models, laying a principled foundation for optimizing\ntransformer-architectures tailored to the demands of PI.\n  By leveraging Shannon's entropy as a quantitative measure, we uncover the\npreviously unexplored dual significance of nonlinearities: beyond ensuring\ntraining stability, they are crucial for maintaining attention head diversity.\nSpecifically, we find that their removal triggers two critical failure modes:\n{\\em entropy collapse} in deeper layers that destabilizes training, and {\\em\nentropic overload} in earlier layers that leads to under-utilization of\nMulti-Head Attention's (MHA) representational capacity.\n  We propose an entropy-guided attention mechanism paired with a novel entropy\nregularization technique to mitigate entropic overload. Additionally, we\nexplore PI-friendly alternatives to layer normalization for preventing entropy\ncollapse and stabilizing the training of LLMs with reduced-nonlinearities. Our\nstudy bridges the gap between information theory and architectural design,\nestablishing entropy dynamics as a principled guide for developing efficient PI\narchitectures. The code and implementation are available at\nhttps:\/\/github.com\/Nandan91\/entropy-guided-attention-llm",
        "Efficient remote monitoring of distributed sources is essential for many\nInternet of Things (IoT) applications. This work studies the uncertainty at the\nreceiver when tracking two-state Markov sources over a slotted random access\nchannel without feedback, using the conditional entropy as a performance\nindicator, and considering the last received value as current state estimate.\nWe provide an analytical characterization of the metric, and evaluate three\naccess strategies: (i) maximizing throughput, (ii) transmitting only on state\nchanges, and (iii) minimizing uncertainty through optimized access\nprobabilities. Our results reveal that throughput optimization does not always\nreduce uncertainty. Moreover, while reactive policies are optimal for symmetric\nsources, asymmetric processes benefit from mixed strategies allowing\ntransmissions during state persistence.",
        "Effectively designing molecular geometries is essential to advancing\npharmaceutical innovations, a domain, which has experienced great attention\nthrough the success of generative models and, in particular, diffusion models.\nHowever, current molecular diffusion models are tailored towards a specific\ndownstream task and lack adaptability. We introduce UniGuide, a framework for\ncontrolled geometric guidance of unconditional diffusion models that allows\nflexible conditioning during inference without the requirement of extra\ntraining or networks. We show how applications such as structure-based,\nfragment-based, and ligand-based drug design are formulated in the UniGuide\nframework and demonstrate on-par or superior performance compared to\nspecialised models. Offering a more versatile approach, UniGuide has the\npotential to streamline the development of molecular generative models,\nallowing them to be readily used in diverse application scenarios.",
        "Reservoir computing (RC) is an innovative paradigm in neuromorphic computing\nthat leverages fixed, randomized, internal connections to address the challenge\nof overfitting. RC has shown remarkable effectiveness in signal processing and\npattern recognition tasks, making it well-suited for hardware implementations\nacross various physical substrates, which promise enhanced computation speeds\nand reduced energy consumption. However, achieving optimal performance in RC\nsystems requires effective parameter optimization. Traditionally, this\noptimization has relied on software modeling, limiting the practicality of\nphysical computing approaches. Here, we report an \\emph{in situ} optimization\nmethod for an optoelectronic delay-based RC system with digital delayed\nfeedback. By simultaneously optimizing five parameters, normalized mean squared\nerror (NMSE) of 0.028, 0.561, and 0.271 is achieved in three benchmark tasks:\nwaveform classification, time series prediction, and speech recognition\noutperforming simulation-based optimization (NMSE 0.054, 0.543, and 0.329,\nrespectively) in the two of the three tasks. This method marks a significant\nadvancement in physical computing, facilitating the optimization of RC and\nneuromorphic systems without the need for simulation, thus enhancing their\npractical applicability.",
        "Counterfactual explanations in medical imaging are critical for understanding\nthe predictions made by deep learning models. We extend the Latent Shift\ncounterfactual generation method from 2D applications to 3D computed tomography\n(CT) scans. We address the challenges associated with 3D data, such as limited\ntraining samples and high memory demands, by implementing a slice-based\napproach. This method leverages a 2D encoder trained on CT slices, which are\nsubsequently combined to maintain 3D context. We demonstrate this technique on\ntwo models for clinical phenotype prediction and lung segmentation. Our\napproach is both memory-efficient and effective for generating interpretable\ncounterfactuals in high-resolution 3D medical imaging.",
        "With the recent advancements in social network platform technology, an\noverwhelming amount of information is spreading rapidly. In this situation, it\ncan become increasingly difficult to discern what information is false or true.\nIf false information proliferates significantly, it can lead to undesirable\noutcomes. Hence, when we receive some information, we can pose the following\ntwo questions: $(i)$ Is the information true? $(ii)$ If not, who initially\nspread that information? % The first problem is the rumor detection issue,\nwhile the second is the rumor source detection problem. A rumor-detection\nproblem involves identifying and mitigating false or misleading information\nspread via various communication channels, particularly online platforms and\nsocial media. Rumors can range from harmless ones to deliberately misleading\ncontent aimed at deceiving or manipulating audiences. Detecting misinformation\nis crucial for maintaining the integrity of information ecosystems and\npreventing harmful effects such as the spread of false beliefs, polarization,\nand even societal harm. Therefore, it is very important to quickly distinguish\nsuch misinformation while simultaneously finding its source to block it from\nspreading on the network. However, most of the existing surveys have analyzed\nthese two issues separately. In this work, we first survey the existing\nresearch on the rumor-detection and rumor source detection problems with joint\ndetection approaches, simultaneously. % This survey deals with these two issues\ntogether so that their relationship can be observed and it provides how the two\nproblems are similar and different. The limitations arising from the rumor\ndetection, rumor source detection, and their combination problems are also\nexplained, and some challenges to be addressed in future works are presented.",
        "We developed an anisotropic spin model that accounts for magnetic anisotropy\nand evaluated the Curie temperature (Tc) dispersion due to finite size effects\nin L10-FePt nanoparticles. In heat-assisted magnetic recording (HAMR) media, a\nnext-generation magnetic recording technology, high-density recording is\nachieved by locally heating L10-FePt nanoparticles near their Tc and rapidly\ncooling them. However, variations in Tc caused by differences in particle size\nand shape can compromise recording stability and areal density capacity, making\nthe control of Tc dispersion critical. In this study, we constructed atomistic\nLLG models to explicitly incorporate the spin exchange anisotropy of L10-FePt,\nbased on parameters determined by first-principles calculations. Using this\nmodel, we evaluated the impact of particle size on Tc dispersion. As a result,\n(1) the Tc dispersion critical to the performance of HAMR can be reproduced,\nwhereas it was previously underestimated by isotropic models and (2)\napproximately 70% of the experimentally observed Tc dispersion can be\nattributed to particle size effects. This research highlights the role of\nexchange anisotropy in amplifying finite-size effects and underscores the\nimportance of size control in HAMR media.",
        "Ensuring Large Language Models (LLMs) align with diverse human preferences\nwhile preserving privacy and fairness remains a challenge. Existing methods,\nsuch as Reinforcement Learning from Human Feedback (RLHF), rely on centralized\ndata collection, making them computationally expensive and privacy-invasive. We\nintroduce PluralLLM a federated learning-based approach that enables multiple\nuser groups to collaboratively train a transformer-based preference predictor\nwithout sharing sensitive data, which can also serve as a reward model for\naligning LLMs. Our method leverages Federated Averaging (FedAvg) to aggregate\npreference updates efficiently, achieving 46% faster convergence, a 4%\nimprovement in alignment scores, and nearly the same group fairness measure as\nin centralized training. Evaluated on a Q\/A preference alignment task,\nPluralLLM demonstrates that federated preference learning offers a scalable and\nprivacy-preserving alternative for aligning LLMs with diverse human values.",
        "We append an additional fifteen years (2009-2024) to the Chandra X-ray light\ncurve of M31*, the supermassive black hole at the center of M31, the Andromeda\ngalaxy. Extending and expanding on the work in Li et al. 2011, we show that\nM31* has remained in an elevated X-ray state from 2006 through at least 2016\n(when regular Chandra monitoring ceased) and likely through 2024, with the most\nrecent observations still showing an elevated X-ray flux. We identify one\nmoderate flare in 2013 where the other nuclear X-ray sources are in low-flux\nstates, making that flare a valuable target for followup with multiwavelength\nand multimessenger archival data. We extract a mostly uncontaminated spectrum\nfor M31* from this observation, showing that its X-ray properties are similar\nto those observed at Sgr A* in its quiescent state by Baganoff et al. 2003.\nFurthermore, we find no substantial change in the source's hardness ratio in\nthe 2006 and 2013 flares compared to the post-2006 elevated state, suggesting\nthe these flares are increases in the regular X-ray emission mechanisms instead\nof entirely new emission modes. Our extended light curve for M31* provides\nvaluable context for multimessenger or multiwavelength observations of nearby\nsupermassive black holes.",
        "The integration of machine learning (ML) has significantly enhanced the\ncapabilities of Earth Observation (EO) systems by enabling the extraction of\nactionable insights from complex datasets. However, the performance of\ndata-driven EO applications is heavily influenced by the data collection and\ntransmission processes, where limited satellite bandwidth and latency\nconstraints can hinder the full transmission of original data to the receivers.\nTo address this issue, adopting the concepts of Semantic Communication (SC)\noffers a promising solution by prioritizing the transmission of essential data\nsemantics over raw information. Implementing SC for EO systems requires a\nthorough understanding of the impact of data processing and communication\nchannel conditions on semantic loss at the processing center. This work\nproposes a novel data-fitting framework to empirically model the semantic loss\nusing real-world EO datasets and domain-specific insights. The framework\nquantifies two primary types of semantic loss: (1) source coding loss, assessed\nvia a data quality indicator measuring the impact of processing on raw source\ndata, and (2) transmission loss, evaluated by comparing practical transmission\nperformance against the Shannon limit. Semantic losses are estimated by\nevaluating the accuracy of EO applications using four task-oriented ML models,\nEfficientViT, MobileViT, ResNet50-DINO, and ResNet8-KD, on lossy image datasets\nunder varying channel conditions and compression ratios. These results underpin\na framework for efficient semantic-loss modeling in bandwidth-constrained EO\nscenarios, enabling more reliable and effective operations.",
        "We introduce YOLO11-JDE, a fast and accurate multi-object tracking (MOT)\nsolution that combines real-time object detection with self-supervised\nRe-Identification (Re-ID). By incorporating a dedicated Re-ID branch into\nYOLO11s, our model performs Joint Detection and Embedding (JDE), generating\nappearance features for each detection. The Re-ID branch is trained in a fully\nself-supervised setting while simultaneously training for detection,\neliminating the need for costly identity-labeled datasets. The triplet loss,\nwith hard positive and semi-hard negative mining strategies, is used for\nlearning discriminative embeddings. Data association is enhanced with a custom\ntracking implementation that successfully integrates motion, appearance, and\nlocation cues. YOLO11-JDE achieves competitive results on MOT17 and MOT20\nbenchmarks, surpassing existing JDE methods in terms of FPS and using up to ten\ntimes fewer parameters. Thus, making our method a highly attractive solution\nfor real-world applications.",
        "We consider a robust and self-reliant (or \"egoistic\") variation of the rigid\nbody localization (RBL) problem, in which a primary rigid body seeks to\nestimate the pose (i.e., location and orientation) of another rigid body (or\n\"target\"), relative to its own, without the assistance of external\ninfrastructure, without prior knowledge of the shape of the target, and taking\ninto account the possibility that the available observations are incomplete.\nThree complementary contributions are then offered for such a scenario. The\nfirst is a method to estimate the translation vector between the center point\nof both rigid bodies, which unlike existing techniques does not require that\nboth objects have the same shape or even the same number of landmark points.\nThis technique is shown to significantly outperform the state-of-the-art (SotA)\nunder complete information, but to be sensitive to data erasures, even when\nenhanced by matrix completion methods. The second contribution, designed to\noffer improved performance in the presence of incomplete information, offers a\nrobust alternative to the latter, at the expense of a slight relative loss\nunder complete information. Finally, the third contribution is a scheme for the\nestimation of the rotation matrix describing the relative orientation of the\ntarget rigid body with respect to the primary. Comparisons of the proposed\nschemes and SotA techniques demonstrate the advantage of the contributed\nmethods in terms of root mean square error (RMSE) performance under fully\ncomplete information and incomplete conditions.",
        "This study examines how the desiccation of Utah Great Salt Lake GSL,\nexacerbated by anthropogenic changes, poses significant health risks,\nparticularly communities mental health. Reduced water inflow has exposed the\nlakebed, increasing airborne particulate matter PM2.5 and dust storms, which\nimpact air quality. By integrating diverse datasets spanning from 1980 to\npresent including insitu measurements, satellite imagery, and reanalysis\nproducts this study synthesizes hydrological, atmospheric, and epidemiological\nvariables to comprehensively track the extent of the GSL surface water, local\nair quality fluctuations, and their effects on community mental health. The\nfindings indicate a clear relationship between higher pollution days and more\nsevere depressive symptoms. Specifically, individuals exposed to 22 days with\nPM2.5 levels above the World Health Organizations 24 hour guideline of 15 ug\nper m3 were more likely to experience severe depressive symptoms. Our results\nalso suggest that people experiencing more severe depression not only face a\nhigher number of high pollution days but also encounter such days more\nfrequently. The study highlights the interconnectedness of poor air quality,\nenvironmental degradation and mental health emphasizing the need for more\nsustainable economic growth in the region.",
        "Electromagnetic penetration through openings in an otherwise closed\nelectromagnetic scatterer is an important topic in computational\nelectromagnetics. To efficiently model this phenomenon, aperture or slot models\nare often used in conjunction with surface integral equations to solve\nMaxwell's equations. To establish the credibility of these models, code\nverification is necessary to assess the correctness of the implementation of\nthe underlying numerical methods. However, many characteristics of surface\nintegral equations and slot models render traditional code-verification\napproaches ineffective. In this paper, we present approaches to separately\nmeasure the different sources of numerical error arising from the\nmethod-of-moments implementation of the electric-field integral equation with\nan arbitrary-depth slot model. We demonstrate the effectiveness of these\napproaches for several cases.",
        "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher.",
        "The rise of Artificial Intelligence (AI) presents unprecedented opportunities\nand challenges for journalism, fact-checking and media regulation. While AI\noffers tools to combat disinformation and enhance media practices, its\nunregulated use and associated risks necessitate clear policies and\ncollaborative efforts. This policy paper explores the implications of\nartificial intelligence (AI) for journalism and fact-checking, with a focus on\naddressing disinformation and fostering responsible AI integration. Using\nGermany and Ukraine as key case studies, it identifies the challenges posed by\ndisinformation, proposes regulatory and funding strategies, and outlines\ntechnical standards to enhance AI adoption in media. The paper offers\nactionable recommendations to ensure AI's responsible and effective integration\ninto media ecosystems. AI presents significant opportunities to combat\ndisinformation and enhance journalistic practices. However, its implementation\nlacks cohesive regulation, leading to risks such as bias, transparency issues,\nand over-reliance on automated systems. In Ukraine, establishing an independent\nmedia regulatory framework adapted to its governance is crucial, while Germany\ncan act as a leader in advancing EU-wide collaborations and standards.\nTogether, these efforts can shape a robust AI-driven media ecosystem that\npromotes accuracy and trust.",
        "The assembly of supermassive black hole (SMBH) mass ($M_{\\bullet}$) and\nstellar mass ($M_{*}$) in galaxies can be studied via the redshift evolution of\nthe $M_{\\bullet}-M_{*}$ relation, but the ways in which selection bias and\nphysical assembly channels affect this evolution are uncertain. To address\nthis, we compare the $M_{\\bullet}-M_{*}$ relation for local massive\n($M_{*}>10^{10.5}$M$_{\\odot}$) quiescent early-type galaxies (ETGs) to that for\nmassive ETGs hosting active galactic nuclei (AGN) at $z\\sim0.8$. The\nrestrictions on stellar mass and galaxy type limit the assembly channels that\nmay connect the two relations. For the local sample we find $\\log(M_{\\bullet})\n= 8.80 + 1.10(\\log{M_{*}-11})$, in line with prior work. For the $z\\sim0.8$\nsample we find a bias-corrected relation: $\\log(M_{\\bullet}) = 7.80 +\n1.25(\\log{M_{*}-11})$. We show, however, that this relation depends on the\nstellar and SMBH mass functions used to compute the selection bias, the virial\nrelation, the virial factor, and the active fraction, which together introduce\nuncertainty of up to $\\sim0.6$\\,dex in the $z\\sim0.8$ relation. Adopting\nreasonable choices of these parameters then our $z\\sim0.8$ relation lies above\nthat for $z\\sim0$ AGN by $\\sim0.5$\\,dex, but below our $z\\sim0$ ETG relation by\n$0.4-1$\\,dex in SMBH mass. We discuss possible sources of this offset,\nincluding further bias corrections, `downsizing\" in SMBH mass assembly, and\npreferential SMBH growth. Our results highlight the need to reduce\nuncertainties from selection and measurement bias in SMBH and stellar masses at\nall redshifts.",
        "Full QCD+QED simulations allow to evaluate isospin breaking corrections to\nhadron masses. With the openQxD code, we are able to perform these simulations\nemploying C-periodic boundary conditions, implemented through a doubling of the\nphysical lattice along one spatial direction. The use of these boundary\nconditions introduces non-zero Wick contractions between two quark or two\nantiquark fields, that, in the case of the computation of baryon masses, lead\nto partially connected additional contributions that we expect to vanish in the\ninfinite volume limit. These contributions are challenging because they involve\nan all-to-all propagator connecting one point in the physical lattice and one\nin the mirror lattice. We present a way to compute these corrections to the\n$\\Omega^-$ baryon mass using a combination of point and stochastic source\ninversions. This work is part of the program of the RC* collaboration.",
        "Oscar Wilde said, \"The difference between literature and journalism is that\njournalism is unreadable, and literature is not read.\" Unfortunately, The\ndigitally archived journalism of Oscar Wilde's 19th century often has no or\npoor quality Optical Character Recognition (OCR), reducing the accessibility of\nthese archives and making them unreadable both figuratively and literally. This\npaper helps address the issue by performing OCR on \"The Nineteenth Century\nSerials Edition\" (NCSE), an 84k-page collection of 19th-century English\nnewspapers and periodicals, using Pixtral 12B, a pre-trained image-to-text\nlanguage model. The OCR capability of Pixtral was compared to 4 other OCR\napproaches, achieving a median character error rate of 1%, 5x lower than the\nnext best model. The resulting NCSE v2.0 dataset features improved article\nidentification, high-quality OCR, and text classified into four types and\nseventeen topics. The dataset contains 1.4 million entries, and 321 million\nwords. Example use cases demonstrate analysis of topic similarity, readability,\nand event tracking. NCSE v2.0 is freely available to encourage historical and\nsociological research. As a result, 21st-century readers can now share Oscar\nWilde's disappointment with 19th-century journalistic standards, reading the\nunreadable from the comfort of their own computers.",
        "This paper investigates the error analysis of system pole estimation in\n$n$-dimensional discrete-time Linear Time-Invariant systems with $m$ outputs\nand $p$ inputs, using the classical Ho-Kalman algorithm based on finite\ninput-output sample data. Building upon prior work, we establish end-to-end\nestimation guarantees for system poles under both single-trajectory and\nmultiple-trajectory settings. Specifically, we prove that, with high\nprobability, the estimation error of system poles decreases at a rate of at\nleast $\\mathcal{O}\\{T^{-\\frac{1}{2n}}\\}$ in the single-trajectory case and\n$\\mathcal{O}\\{N^{-\\frac{1}{2n}}\\}$ in the multiple-trajectory case, where $T$\nis the length of a single trajectory, and $N$ is the number of trajectories.\nFurthermore, we reveal that in both settings, achieving a constant estimation\naccuracy for system poles requires the sample size to grow super-polynomially\nwith respect to the larger of the two ratios, $ \\max\\{n\/m, n\/p\\} $. Numerical\nexperiments are conducted to validate the non-asymptotic results of system pole\nestimation.",
        "There is strong empirical evidence that the state-of-the-art diffusion\nmodeling paradigm leads to models that memorize the training set, especially\nwhen the training set is small. Prior methods to mitigate the memorization\nproblem often lead to a decrease in image quality. Is it possible to obtain\nstrong and creative generative models, i.e., models that achieve high\ngeneration quality and low memorization? Despite the current pessimistic\nlandscape of results, we make significant progress in pushing the trade-off\nbetween fidelity and memorization. We first provide theoretical evidence that\nmemorization in diffusion models is only necessary for denoising problems at\nlow noise scales (usually used in generating high-frequency details). Using\nthis theoretical insight, we propose a simple, principled method to train the\ndiffusion models using noisy data at large noise scales. We show that our\nmethod significantly reduces memorization without decreasing the image quality,\nfor both text-conditional and unconditional models and for a variety of data\navailability settings.",
        "Rotational symmetry is a defining feature of many tomography systems,\nincluding computed tomography (CT) and emission computed tomography (ECT),\nwhere detectors are arranged in a circular or periodically rotating\nconfiguration. This study revisits the image reconstruction process from the\nperspective of hardware-induced rotational symmetry and introduces a cyclic\ngroup equivariance framework for deep learning-based reconstruction.\nSpecifically, we derive a mathematical correspondence that couples cyclic\nrotations in the projection domain to discrete rotations in the image domain,\nboth arising from the same cyclic group inherent in the hardware design. This\ninsight also reveals the uniformly distributed circular structure of the\nprojection space. Building on this principle, we provide a cyclic rotation\nequivariant convolution design method to preserve projection domain symmetry\nand a cyclic group equivariance regularization approach that enforces\nconsistent rotational transformations across the entire network. We further\nintegrate these modules into a domain transform reconstruction framework and\nvalidate them using digital brain phantoms, training on discrete models and\ntesting on more complex and realistic fuzzy variants. Results indicate markedly\nimproved generalization and stability, with fewer artifacts and better detail\npreservation, especially under data distribution deviation. These findings\nhighlight the potential of cyclic group equivariance as a unifying principle\nfor tomographic reconstruction in rotationally symmetric systems, offering a\nflexible and interpretable solution for scenarios with limited data.",
        "The present work is devoted to Computability Logic (CoL), the young and\nvolcanic research-project developed by Giorgi Japaridze. Our main goal is to\nprovide the reader with a clear panoramic view of this vast new land, starting\nfrom its core knots and making our way towards the outer threads, in a somewhat\nthree-dimensional, spacial gait. Furthermore, through the present work, we\nprovide a tentative proof for the decidability of one of CoL's numerous\naxiomatisations, namely CL15. Thus, our expedition initially takes off for an\naerial, perusal overview of this fertile steppe. The first chapter introduces\nCoL in a philosophical fashion, exposing and arguing its main key points. We\nthen move over to unfold its semantics and syntax profiles, allowing the reader\nto become increasingly more familiar with this new environment. Landing on to\nthe second chapter, we thoroughly introduce Cirquent Calculus, the new\ndeductive system Japaridze has developed in order to axiomatise Computability\nLogic. Indeed, this new proof-system can also be a useful tool for many other\nlogics. We then review each of the 17 axiomatisations found so far. The third\nchapter zooms-in on CL15, in order to come up with a possible solution to its\nopen problem. We outline its soundness and completeness proofs; then provide\nsome few deductive examples; and, finally, build a tentative proof of its\ndecidability. Lastly, the fourth chapter focuses on the potential and actual\napplications of Computability Logic, both in arithmetic (clarithmetic) and in\nArtificial Intelligence systems (meaning knowledgebase and planning-and-action\nones). We close our journey with some final remarks on the richness of this\nframework and, hence, the research-worthiness it entails.",
        "We present HuGDiffusion, a generalizable 3D Gaussian splatting (3DGS)\nlearning pipeline to achieve novel view synthesis (NVS) of human characters\nfrom single-view input images. Existing approaches typically require monocular\nvideos or calibrated multi-view images as inputs, whose applicability could be\nweakened in real-world scenarios with arbitrary and\/or unknown camera poses. In\nthis paper, we aim to generate the set of 3DGS attributes via a diffusion-based\nframework conditioned on human priors extracted from a single image.\nSpecifically, we begin with carefully integrated human-centric feature\nextraction procedures to deduce informative conditioning signals. Based on our\nempirical observations that jointly learning the whole 3DGS attributes is\nchallenging to optimize, we design a multi-stage generation strategy to obtain\ndifferent types of 3DGS attributes. To facilitate the training process, we\ninvestigate constructing proxy ground-truth 3D Gaussian attributes as\nhigh-quality attribute-level supervision signals. Through extensive\nexperiments, our HuGDiffusion shows significant performance improvements over\nthe state-of-the-art methods. Our code will be made publicly available.",
        "Recent advances in Large Language Models (LLMs) have motivated the\ndevelopment of general LLMs for molecular tasks. While several studies have\ndemonstrated that fine-tuned LLMs can achieve impressive benchmark\nperformances, they are far from genuine generalist molecular LLMs due to a lack\nof fundamental understanding of molecular structure. Specifically, when given\nmolecular task instructions, LLMs trained with naive next-token prediction\ntraining assign similar likelihood scores to both original and negatively\ncorrupted molecules, revealing their lack of molecular structure understanding\nthat is crucial for reliable and general molecular LLMs. To overcome this\nlimitation and obtain a true generalist molecular LLM, we introduce a novel\nmulti-modal training method based on a thorough multi-modal instruction tuning\nas well as a molecular structure preference optimization between chosen and\nrejected graphs. On various molecular benchmarks, the proposed generalist\nmolecular LLM, called Mol-LLM, achieves state-of-the-art performances among\ngeneralist LLMs on most tasks, at the same time, surpassing or comparable to\nstate-of-the-art specialist LLMs. Moreover, Mol-LLM also shows superior\ngeneralization performances in reaction prediction tasks, demonstrating the\neffect of the molecular structure understanding for generalization perspective.",
        "This paper introduces a novel causal framework for multi-stage\ndecision-making in natural language action spaces where outcomes are only\nobserved after a sequence of actions. While recent approaches like Proximal\nPolicy Optimization (PPO) can handle such delayed-reward settings in\nhigh-dimensional action spaces, they typically require multiple models (policy,\nvalue, and reward) and substantial training data. Our approach employs\nQ-learning to estimate Dynamic Treatment Regimes (DTR) through a single model,\nenabling data-efficient policy learning via gradient ascent on language\nembeddings. A key technical contribution of our approach is a decoding strategy\nthat translates optimized embeddings back into coherent natural language. We\nevaluate our approach on mental health intervention, hate speech countering,\nand sentiment transfer tasks, demonstrating significant improvements over\ncompetitive baselines across multiple metrics. Notably, our method achieves\nsuperior transfer strength while maintaining content preservation and fluency,\nas validated through human evaluation. Our work provides a practical foundation\nfor learning optimal policies in complex language tasks where training data is\nlimited.",
        "Spot-size converters are key for efficient coupling of light between\nwaveguides of different sizes. While adiabatic tapers are well suited for small\nsize differences, they become impractically long for expansion factors around\nx100 which are often required when coupling integrated waveguides and\nfree-space beams. Evanescent couplers and bragg deflectors can be used in this\nscenario, but their operation is inherently limited in bandwidth. Here we\npropose a solution based on a parabolic dielectric interface that couples light\nfrom a 0.5 um-wide waveguide to a 285 um-wide waveguide, i.e. an expansion\nfactor of x570. We experimentally demonstrate an unprecedented bandwidth of\nmore than 380 nm with insertion losses below 0.35 dB. We furthermore provide\nanalytical expressions for the design of such parabolic spot-size-converters\nfor arbitrary expansion factors.",
        "Gibbons and Schiller have raised the physically interesting conjecture that\nforces in general relativity are bounded from above by the mathematically\ncompact relation ${\\cal F}\\leq c^4\/4G$. In the present compact paper we\nexplicitly prove, using the non-linearly coupled Einstein-matter field\nequations, that the force function ${\\cal F}\\equiv 4\\pi r^2 p(r)$ in {\\it\nstable} self-gravitating horizonless matter configurations is characterized by\nthe upper bound ${\\cal F}\\leq c^4\/G$ [here $p(r)$ is the radial pressure inside\nthe self-gravitating matter configuration]."
      ]
    }
  },
  {
    "id":2411.06184,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Taking the Human Out of the Loop: A Review of Bayesian Optimization",
    "start_abstract":"Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing storage architectures. The construction such involves many distributed design choices. end products (e.g., recommendation medical analysis tools, real-time game engines, speech recognizers) thus involve tunable configuration parameters. These parameters often specified hard-coded into the by various developers or teams. If optimized jointly, these can result in significant improvements. Bayesian optimization is a powerful tool for joint choices that gaining great popularity recent years. It promises greater automation so as to increase both product quality human productivity. This review paper introduces optimization, highlights some its methodological aspects, showcases wide range applications.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "New Pathologic Classification of Lung Cancer: Relevance for Clinical Practice and Clinical Trials"
      ],
      "abstract":[
        "We summarize significant changes in pathologic classification of lung cancer resulting from the 2011 International Association for Study Lung Cancer\/American Thoracic Society\/European Respiratory Society (IASLC\/ATS\/ERS) adenocarcinoma classification. The was developed by an international core panel experts representing IASLC, ATS, and ERS with oncologists\/pulmonologists, pathologists, radiologists, molecular biologists, thoracic surgeons. Because 70% patients present advanced stages, a new approach to small biopsies cytology specific terminology criteria focused on need distinguishing squamous cell carcinoma testing EGFR mutations ALK rearrangement. Tumors previously classified as non-small-cell carcinoma, not otherwise specified, because lack clear or morphology should be further using limited immunohistochemical workup preserve tissue testing. terms \"bronchioloalveolar carcinoma\" \"mixed subtype adenocarcinoma\" have been discontinued. For resected adenocarcinomas, concepts situ minimally invasive define who, if they undergo complete resection, will 100% disease-free survival. Invasive adenocarcinomas are now predominant pattern after comprehensive histologic subtyping lepidic, acinar, papillary, solid patterns; micropapillary is added poor prognosis. Former mucinous bronchioloalveolar carcinomas called \"invasive adenocarcinoma.\" field rapidly evolving advances occurring frequent basis, particularly arena, this provides much needed standard diagnosis only patient care but also clinical trials TNM"
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Exact Learning of Permutations for Nonzero Binary Inputs with\n  Logarithmic Training Size and Quadratic Ensemble Complexity",
        "Entropy-Guided Attention for Private LLMs",
        "On the Uncertainty of a Simple Estimator for Remote Source Monitoring\n  over ALOHA Channels",
        "Unified Guidance for Geometry-Conditioned Molecular Generation",
        "In Situ Optimization of an Optoelectronic Reservoir Computer with\n  Digital Delayed Feedback",
        "Explaining 3D Computed Tomography Classifiers with Counterfactuals",
        "Detection of Rumors and Their Sources in Social Networks: A\n  Comprehensive Survey",
        "Anisotropic Exchange Spin Model to Investigate the Curie Temperature\n  Dispersion of Finite-Size L10-FePt Magnetic Nanoparticles",
        "PluralLLM: Pluralistic Alignment in LLMs via Federated Learning",
        "Fifteen Years of M31* X-ray Variability and Flares",
        "A Semantic-Loss Function Modeling Framework With Task-Oriented Machine\n  Learning Perspectives",
        "YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised\n  Re-ID",
        "Robust Egoistic Rigid Body Localization",
        "Diminishing Waters: The Great Salt Lake's Desiccation and Its Mental\n  Health Consequences",
        "Code-Verification Techniques for an Arbitrary-Depth Electromagnetic Slot\n  Model",
        "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems",
        "From Trust to Truth: Actionable policies for the use of AI in\n  fact-checking in Germany and Ukraine",
        "The assembly of supermassive black holes at $z<1$ in early-type galaxies\n  from scaling relations",
        "Partially connected contributions to baryon masses in QCD+QED",
        "Reading the unreadable: Creating a dataset of 19th century English\n  newspapers using image-to-text language models",
        "Finite Sample Analysis of System Poles for Ho-Kalman Algorithm",
        "Does Generation Require Memorization? Creative Diffusion Models using\n  Ambient Diffusion",
        "Incorporating Cyclic Group Equivariance into Deep Learning for Reliable\n  Reconstruction of Rotationally Symmetric Tomography Systems",
        "The Fertile Steppe: Computability Logic and the decidability of one of\n  its fragments",
        "HuGDiffusion: Generalizable Single-Image Human Rendering via 3D Gaussian\n  Diffusion",
        "Mol-LLM: Generalist Molecular LLM with Improved Graph Utilization",
        "Policy Learning with a Natural Language Action Space: A Causal Approach",
        "Parabolic dielectric reflector for extreme on-chip spot-size conversion\n  with broad bandwidth",
        "Maximum force conjecture in curved spacetimes of stable self-gravitating\n  matter configurations"
      ],
      "abstract":[
        "The ability of an architecture to realize permutations is quite fundamental.\nFor example, Large Language Models need to be able to correctly copy (and\nperhaps rearrange) parts of the input prompt into the output. Classical\nuniversal approximation theorems guarantee the existence of parameter\nconfigurations that solve this task but offer no insights into whether\ngradient-based algorithms can find them. In this paper, we address this gap by\nfocusing on two-layer fully connected feed-forward neural networks and the task\nof learning permutations on nonzero binary inputs. We show that in the infinite\nwidth Neural Tangent Kernel (NTK) regime, an ensemble of such networks\nindependently trained with gradient descent on only the $k$ standard basis\nvectors out of $2^k - 1$ possible inputs successfully learns any fixed\npermutation of length $k$ with arbitrarily high probability. By analyzing the\nexact training dynamics, we prove that the network's output converges to a\nGaussian process whose mean captures the ground truth permutation via\nsign-based features. We then demonstrate how averaging these runs (an\n\"ensemble\" method) and applying a simple rounding step yields an arbitrarily\naccurate prediction on any possible input unseen during training. Notably, the\nnumber of models needed to achieve exact learning with high probability (which\nwe refer to as ensemble complexity) exhibits a linearithmic dependence on the\ninput size $k$ for a single test input and a quadratic dependence when\nconsidering all test inputs simultaneously.",
        "The pervasiveness of proprietary language models has raised critical privacy\nconcerns, necessitating advancements in private inference (PI), where\ncomputations are performed directly on encrypted data without revealing users'\nsensitive information. While PI offers a promising solution, its practical\ndeployment is hindered by substantial communication and latency overheads,\nprimarily stemming from nonlinear operations. To address this, we introduce an\ninformation-theoretic framework to characterize the role of nonlinearities in\ndecoder-only language models, laying a principled foundation for optimizing\ntransformer-architectures tailored to the demands of PI.\n  By leveraging Shannon's entropy as a quantitative measure, we uncover the\npreviously unexplored dual significance of nonlinearities: beyond ensuring\ntraining stability, they are crucial for maintaining attention head diversity.\nSpecifically, we find that their removal triggers two critical failure modes:\n{\\em entropy collapse} in deeper layers that destabilizes training, and {\\em\nentropic overload} in earlier layers that leads to under-utilization of\nMulti-Head Attention's (MHA) representational capacity.\n  We propose an entropy-guided attention mechanism paired with a novel entropy\nregularization technique to mitigate entropic overload. Additionally, we\nexplore PI-friendly alternatives to layer normalization for preventing entropy\ncollapse and stabilizing the training of LLMs with reduced-nonlinearities. Our\nstudy bridges the gap between information theory and architectural design,\nestablishing entropy dynamics as a principled guide for developing efficient PI\narchitectures. The code and implementation are available at\nhttps:\/\/github.com\/Nandan91\/entropy-guided-attention-llm",
        "Efficient remote monitoring of distributed sources is essential for many\nInternet of Things (IoT) applications. This work studies the uncertainty at the\nreceiver when tracking two-state Markov sources over a slotted random access\nchannel without feedback, using the conditional entropy as a performance\nindicator, and considering the last received value as current state estimate.\nWe provide an analytical characterization of the metric, and evaluate three\naccess strategies: (i) maximizing throughput, (ii) transmitting only on state\nchanges, and (iii) minimizing uncertainty through optimized access\nprobabilities. Our results reveal that throughput optimization does not always\nreduce uncertainty. Moreover, while reactive policies are optimal for symmetric\nsources, asymmetric processes benefit from mixed strategies allowing\ntransmissions during state persistence.",
        "Effectively designing molecular geometries is essential to advancing\npharmaceutical innovations, a domain, which has experienced great attention\nthrough the success of generative models and, in particular, diffusion models.\nHowever, current molecular diffusion models are tailored towards a specific\ndownstream task and lack adaptability. We introduce UniGuide, a framework for\ncontrolled geometric guidance of unconditional diffusion models that allows\nflexible conditioning during inference without the requirement of extra\ntraining or networks. We show how applications such as structure-based,\nfragment-based, and ligand-based drug design are formulated in the UniGuide\nframework and demonstrate on-par or superior performance compared to\nspecialised models. Offering a more versatile approach, UniGuide has the\npotential to streamline the development of molecular generative models,\nallowing them to be readily used in diverse application scenarios.",
        "Reservoir computing (RC) is an innovative paradigm in neuromorphic computing\nthat leverages fixed, randomized, internal connections to address the challenge\nof overfitting. RC has shown remarkable effectiveness in signal processing and\npattern recognition tasks, making it well-suited for hardware implementations\nacross various physical substrates, which promise enhanced computation speeds\nand reduced energy consumption. However, achieving optimal performance in RC\nsystems requires effective parameter optimization. Traditionally, this\noptimization has relied on software modeling, limiting the practicality of\nphysical computing approaches. Here, we report an \\emph{in situ} optimization\nmethod for an optoelectronic delay-based RC system with digital delayed\nfeedback. By simultaneously optimizing five parameters, normalized mean squared\nerror (NMSE) of 0.028, 0.561, and 0.271 is achieved in three benchmark tasks:\nwaveform classification, time series prediction, and speech recognition\noutperforming simulation-based optimization (NMSE 0.054, 0.543, and 0.329,\nrespectively) in the two of the three tasks. This method marks a significant\nadvancement in physical computing, facilitating the optimization of RC and\nneuromorphic systems without the need for simulation, thus enhancing their\npractical applicability.",
        "Counterfactual explanations in medical imaging are critical for understanding\nthe predictions made by deep learning models. We extend the Latent Shift\ncounterfactual generation method from 2D applications to 3D computed tomography\n(CT) scans. We address the challenges associated with 3D data, such as limited\ntraining samples and high memory demands, by implementing a slice-based\napproach. This method leverages a 2D encoder trained on CT slices, which are\nsubsequently combined to maintain 3D context. We demonstrate this technique on\ntwo models for clinical phenotype prediction and lung segmentation. Our\napproach is both memory-efficient and effective for generating interpretable\ncounterfactuals in high-resolution 3D medical imaging.",
        "With the recent advancements in social network platform technology, an\noverwhelming amount of information is spreading rapidly. In this situation, it\ncan become increasingly difficult to discern what information is false or true.\nIf false information proliferates significantly, it can lead to undesirable\noutcomes. Hence, when we receive some information, we can pose the following\ntwo questions: $(i)$ Is the information true? $(ii)$ If not, who initially\nspread that information? % The first problem is the rumor detection issue,\nwhile the second is the rumor source detection problem. A rumor-detection\nproblem involves identifying and mitigating false or misleading information\nspread via various communication channels, particularly online platforms and\nsocial media. Rumors can range from harmless ones to deliberately misleading\ncontent aimed at deceiving or manipulating audiences. Detecting misinformation\nis crucial for maintaining the integrity of information ecosystems and\npreventing harmful effects such as the spread of false beliefs, polarization,\nand even societal harm. Therefore, it is very important to quickly distinguish\nsuch misinformation while simultaneously finding its source to block it from\nspreading on the network. However, most of the existing surveys have analyzed\nthese two issues separately. In this work, we first survey the existing\nresearch on the rumor-detection and rumor source detection problems with joint\ndetection approaches, simultaneously. % This survey deals with these two issues\ntogether so that their relationship can be observed and it provides how the two\nproblems are similar and different. The limitations arising from the rumor\ndetection, rumor source detection, and their combination problems are also\nexplained, and some challenges to be addressed in future works are presented.",
        "We developed an anisotropic spin model that accounts for magnetic anisotropy\nand evaluated the Curie temperature (Tc) dispersion due to finite size effects\nin L10-FePt nanoparticles. In heat-assisted magnetic recording (HAMR) media, a\nnext-generation magnetic recording technology, high-density recording is\nachieved by locally heating L10-FePt nanoparticles near their Tc and rapidly\ncooling them. However, variations in Tc caused by differences in particle size\nand shape can compromise recording stability and areal density capacity, making\nthe control of Tc dispersion critical. In this study, we constructed atomistic\nLLG models to explicitly incorporate the spin exchange anisotropy of L10-FePt,\nbased on parameters determined by first-principles calculations. Using this\nmodel, we evaluated the impact of particle size on Tc dispersion. As a result,\n(1) the Tc dispersion critical to the performance of HAMR can be reproduced,\nwhereas it was previously underestimated by isotropic models and (2)\napproximately 70% of the experimentally observed Tc dispersion can be\nattributed to particle size effects. This research highlights the role of\nexchange anisotropy in amplifying finite-size effects and underscores the\nimportance of size control in HAMR media.",
        "Ensuring Large Language Models (LLMs) align with diverse human preferences\nwhile preserving privacy and fairness remains a challenge. Existing methods,\nsuch as Reinforcement Learning from Human Feedback (RLHF), rely on centralized\ndata collection, making them computationally expensive and privacy-invasive. We\nintroduce PluralLLM a federated learning-based approach that enables multiple\nuser groups to collaboratively train a transformer-based preference predictor\nwithout sharing sensitive data, which can also serve as a reward model for\naligning LLMs. Our method leverages Federated Averaging (FedAvg) to aggregate\npreference updates efficiently, achieving 46% faster convergence, a 4%\nimprovement in alignment scores, and nearly the same group fairness measure as\nin centralized training. Evaluated on a Q\/A preference alignment task,\nPluralLLM demonstrates that federated preference learning offers a scalable and\nprivacy-preserving alternative for aligning LLMs with diverse human values.",
        "We append an additional fifteen years (2009-2024) to the Chandra X-ray light\ncurve of M31*, the supermassive black hole at the center of M31, the Andromeda\ngalaxy. Extending and expanding on the work in Li et al. 2011, we show that\nM31* has remained in an elevated X-ray state from 2006 through at least 2016\n(when regular Chandra monitoring ceased) and likely through 2024, with the most\nrecent observations still showing an elevated X-ray flux. We identify one\nmoderate flare in 2013 where the other nuclear X-ray sources are in low-flux\nstates, making that flare a valuable target for followup with multiwavelength\nand multimessenger archival data. We extract a mostly uncontaminated spectrum\nfor M31* from this observation, showing that its X-ray properties are similar\nto those observed at Sgr A* in its quiescent state by Baganoff et al. 2003.\nFurthermore, we find no substantial change in the source's hardness ratio in\nthe 2006 and 2013 flares compared to the post-2006 elevated state, suggesting\nthe these flares are increases in the regular X-ray emission mechanisms instead\nof entirely new emission modes. Our extended light curve for M31* provides\nvaluable context for multimessenger or multiwavelength observations of nearby\nsupermassive black holes.",
        "The integration of machine learning (ML) has significantly enhanced the\ncapabilities of Earth Observation (EO) systems by enabling the extraction of\nactionable insights from complex datasets. However, the performance of\ndata-driven EO applications is heavily influenced by the data collection and\ntransmission processes, where limited satellite bandwidth and latency\nconstraints can hinder the full transmission of original data to the receivers.\nTo address this issue, adopting the concepts of Semantic Communication (SC)\noffers a promising solution by prioritizing the transmission of essential data\nsemantics over raw information. Implementing SC for EO systems requires a\nthorough understanding of the impact of data processing and communication\nchannel conditions on semantic loss at the processing center. This work\nproposes a novel data-fitting framework to empirically model the semantic loss\nusing real-world EO datasets and domain-specific insights. The framework\nquantifies two primary types of semantic loss: (1) source coding loss, assessed\nvia a data quality indicator measuring the impact of processing on raw source\ndata, and (2) transmission loss, evaluated by comparing practical transmission\nperformance against the Shannon limit. Semantic losses are estimated by\nevaluating the accuracy of EO applications using four task-oriented ML models,\nEfficientViT, MobileViT, ResNet50-DINO, and ResNet8-KD, on lossy image datasets\nunder varying channel conditions and compression ratios. These results underpin\na framework for efficient semantic-loss modeling in bandwidth-constrained EO\nscenarios, enabling more reliable and effective operations.",
        "We introduce YOLO11-JDE, a fast and accurate multi-object tracking (MOT)\nsolution that combines real-time object detection with self-supervised\nRe-Identification (Re-ID). By incorporating a dedicated Re-ID branch into\nYOLO11s, our model performs Joint Detection and Embedding (JDE), generating\nappearance features for each detection. The Re-ID branch is trained in a fully\nself-supervised setting while simultaneously training for detection,\neliminating the need for costly identity-labeled datasets. The triplet loss,\nwith hard positive and semi-hard negative mining strategies, is used for\nlearning discriminative embeddings. Data association is enhanced with a custom\ntracking implementation that successfully integrates motion, appearance, and\nlocation cues. YOLO11-JDE achieves competitive results on MOT17 and MOT20\nbenchmarks, surpassing existing JDE methods in terms of FPS and using up to ten\ntimes fewer parameters. Thus, making our method a highly attractive solution\nfor real-world applications.",
        "We consider a robust and self-reliant (or \"egoistic\") variation of the rigid\nbody localization (RBL) problem, in which a primary rigid body seeks to\nestimate the pose (i.e., location and orientation) of another rigid body (or\n\"target\"), relative to its own, without the assistance of external\ninfrastructure, without prior knowledge of the shape of the target, and taking\ninto account the possibility that the available observations are incomplete.\nThree complementary contributions are then offered for such a scenario. The\nfirst is a method to estimate the translation vector between the center point\nof both rigid bodies, which unlike existing techniques does not require that\nboth objects have the same shape or even the same number of landmark points.\nThis technique is shown to significantly outperform the state-of-the-art (SotA)\nunder complete information, but to be sensitive to data erasures, even when\nenhanced by matrix completion methods. The second contribution, designed to\noffer improved performance in the presence of incomplete information, offers a\nrobust alternative to the latter, at the expense of a slight relative loss\nunder complete information. Finally, the third contribution is a scheme for the\nestimation of the rotation matrix describing the relative orientation of the\ntarget rigid body with respect to the primary. Comparisons of the proposed\nschemes and SotA techniques demonstrate the advantage of the contributed\nmethods in terms of root mean square error (RMSE) performance under fully\ncomplete information and incomplete conditions.",
        "This study examines how the desiccation of Utah Great Salt Lake GSL,\nexacerbated by anthropogenic changes, poses significant health risks,\nparticularly communities mental health. Reduced water inflow has exposed the\nlakebed, increasing airborne particulate matter PM2.5 and dust storms, which\nimpact air quality. By integrating diverse datasets spanning from 1980 to\npresent including insitu measurements, satellite imagery, and reanalysis\nproducts this study synthesizes hydrological, atmospheric, and epidemiological\nvariables to comprehensively track the extent of the GSL surface water, local\nair quality fluctuations, and their effects on community mental health. The\nfindings indicate a clear relationship between higher pollution days and more\nsevere depressive symptoms. Specifically, individuals exposed to 22 days with\nPM2.5 levels above the World Health Organizations 24 hour guideline of 15 ug\nper m3 were more likely to experience severe depressive symptoms. Our results\nalso suggest that people experiencing more severe depression not only face a\nhigher number of high pollution days but also encounter such days more\nfrequently. The study highlights the interconnectedness of poor air quality,\nenvironmental degradation and mental health emphasizing the need for more\nsustainable economic growth in the region.",
        "Electromagnetic penetration through openings in an otherwise closed\nelectromagnetic scatterer is an important topic in computational\nelectromagnetics. To efficiently model this phenomenon, aperture or slot models\nare often used in conjunction with surface integral equations to solve\nMaxwell's equations. To establish the credibility of these models, code\nverification is necessary to assess the correctness of the implementation of\nthe underlying numerical methods. However, many characteristics of surface\nintegral equations and slot models render traditional code-verification\napproaches ineffective. In this paper, we present approaches to separately\nmeasure the different sources of numerical error arising from the\nmethod-of-moments implementation of the electric-field integral equation with\nan arbitrary-depth slot model. We demonstrate the effectiveness of these\napproaches for several cases.",
        "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher.",
        "The rise of Artificial Intelligence (AI) presents unprecedented opportunities\nand challenges for journalism, fact-checking and media regulation. While AI\noffers tools to combat disinformation and enhance media practices, its\nunregulated use and associated risks necessitate clear policies and\ncollaborative efforts. This policy paper explores the implications of\nartificial intelligence (AI) for journalism and fact-checking, with a focus on\naddressing disinformation and fostering responsible AI integration. Using\nGermany and Ukraine as key case studies, it identifies the challenges posed by\ndisinformation, proposes regulatory and funding strategies, and outlines\ntechnical standards to enhance AI adoption in media. The paper offers\nactionable recommendations to ensure AI's responsible and effective integration\ninto media ecosystems. AI presents significant opportunities to combat\ndisinformation and enhance journalistic practices. However, its implementation\nlacks cohesive regulation, leading to risks such as bias, transparency issues,\nand over-reliance on automated systems. In Ukraine, establishing an independent\nmedia regulatory framework adapted to its governance is crucial, while Germany\ncan act as a leader in advancing EU-wide collaborations and standards.\nTogether, these efforts can shape a robust AI-driven media ecosystem that\npromotes accuracy and trust.",
        "The assembly of supermassive black hole (SMBH) mass ($M_{\\bullet}$) and\nstellar mass ($M_{*}$) in galaxies can be studied via the redshift evolution of\nthe $M_{\\bullet}-M_{*}$ relation, but the ways in which selection bias and\nphysical assembly channels affect this evolution are uncertain. To address\nthis, we compare the $M_{\\bullet}-M_{*}$ relation for local massive\n($M_{*}>10^{10.5}$M$_{\\odot}$) quiescent early-type galaxies (ETGs) to that for\nmassive ETGs hosting active galactic nuclei (AGN) at $z\\sim0.8$. The\nrestrictions on stellar mass and galaxy type limit the assembly channels that\nmay connect the two relations. For the local sample we find $\\log(M_{\\bullet})\n= 8.80 + 1.10(\\log{M_{*}-11})$, in line with prior work. For the $z\\sim0.8$\nsample we find a bias-corrected relation: $\\log(M_{\\bullet}) = 7.80 +\n1.25(\\log{M_{*}-11})$. We show, however, that this relation depends on the\nstellar and SMBH mass functions used to compute the selection bias, the virial\nrelation, the virial factor, and the active fraction, which together introduce\nuncertainty of up to $\\sim0.6$\\,dex in the $z\\sim0.8$ relation. Adopting\nreasonable choices of these parameters then our $z\\sim0.8$ relation lies above\nthat for $z\\sim0$ AGN by $\\sim0.5$\\,dex, but below our $z\\sim0$ ETG relation by\n$0.4-1$\\,dex in SMBH mass. We discuss possible sources of this offset,\nincluding further bias corrections, `downsizing\" in SMBH mass assembly, and\npreferential SMBH growth. Our results highlight the need to reduce\nuncertainties from selection and measurement bias in SMBH and stellar masses at\nall redshifts.",
        "Full QCD+QED simulations allow to evaluate isospin breaking corrections to\nhadron masses. With the openQxD code, we are able to perform these simulations\nemploying C-periodic boundary conditions, implemented through a doubling of the\nphysical lattice along one spatial direction. The use of these boundary\nconditions introduces non-zero Wick contractions between two quark or two\nantiquark fields, that, in the case of the computation of baryon masses, lead\nto partially connected additional contributions that we expect to vanish in the\ninfinite volume limit. These contributions are challenging because they involve\nan all-to-all propagator connecting one point in the physical lattice and one\nin the mirror lattice. We present a way to compute these corrections to the\n$\\Omega^-$ baryon mass using a combination of point and stochastic source\ninversions. This work is part of the program of the RC* collaboration.",
        "Oscar Wilde said, \"The difference between literature and journalism is that\njournalism is unreadable, and literature is not read.\" Unfortunately, The\ndigitally archived journalism of Oscar Wilde's 19th century often has no or\npoor quality Optical Character Recognition (OCR), reducing the accessibility of\nthese archives and making them unreadable both figuratively and literally. This\npaper helps address the issue by performing OCR on \"The Nineteenth Century\nSerials Edition\" (NCSE), an 84k-page collection of 19th-century English\nnewspapers and periodicals, using Pixtral 12B, a pre-trained image-to-text\nlanguage model. The OCR capability of Pixtral was compared to 4 other OCR\napproaches, achieving a median character error rate of 1%, 5x lower than the\nnext best model. The resulting NCSE v2.0 dataset features improved article\nidentification, high-quality OCR, and text classified into four types and\nseventeen topics. The dataset contains 1.4 million entries, and 321 million\nwords. Example use cases demonstrate analysis of topic similarity, readability,\nand event tracking. NCSE v2.0 is freely available to encourage historical and\nsociological research. As a result, 21st-century readers can now share Oscar\nWilde's disappointment with 19th-century journalistic standards, reading the\nunreadable from the comfort of their own computers.",
        "This paper investigates the error analysis of system pole estimation in\n$n$-dimensional discrete-time Linear Time-Invariant systems with $m$ outputs\nand $p$ inputs, using the classical Ho-Kalman algorithm based on finite\ninput-output sample data. Building upon prior work, we establish end-to-end\nestimation guarantees for system poles under both single-trajectory and\nmultiple-trajectory settings. Specifically, we prove that, with high\nprobability, the estimation error of system poles decreases at a rate of at\nleast $\\mathcal{O}\\{T^{-\\frac{1}{2n}}\\}$ in the single-trajectory case and\n$\\mathcal{O}\\{N^{-\\frac{1}{2n}}\\}$ in the multiple-trajectory case, where $T$\nis the length of a single trajectory, and $N$ is the number of trajectories.\nFurthermore, we reveal that in both settings, achieving a constant estimation\naccuracy for system poles requires the sample size to grow super-polynomially\nwith respect to the larger of the two ratios, $ \\max\\{n\/m, n\/p\\} $. Numerical\nexperiments are conducted to validate the non-asymptotic results of system pole\nestimation.",
        "There is strong empirical evidence that the state-of-the-art diffusion\nmodeling paradigm leads to models that memorize the training set, especially\nwhen the training set is small. Prior methods to mitigate the memorization\nproblem often lead to a decrease in image quality. Is it possible to obtain\nstrong and creative generative models, i.e., models that achieve high\ngeneration quality and low memorization? Despite the current pessimistic\nlandscape of results, we make significant progress in pushing the trade-off\nbetween fidelity and memorization. We first provide theoretical evidence that\nmemorization in diffusion models is only necessary for denoising problems at\nlow noise scales (usually used in generating high-frequency details). Using\nthis theoretical insight, we propose a simple, principled method to train the\ndiffusion models using noisy data at large noise scales. We show that our\nmethod significantly reduces memorization without decreasing the image quality,\nfor both text-conditional and unconditional models and for a variety of data\navailability settings.",
        "Rotational symmetry is a defining feature of many tomography systems,\nincluding computed tomography (CT) and emission computed tomography (ECT),\nwhere detectors are arranged in a circular or periodically rotating\nconfiguration. This study revisits the image reconstruction process from the\nperspective of hardware-induced rotational symmetry and introduces a cyclic\ngroup equivariance framework for deep learning-based reconstruction.\nSpecifically, we derive a mathematical correspondence that couples cyclic\nrotations in the projection domain to discrete rotations in the image domain,\nboth arising from the same cyclic group inherent in the hardware design. This\ninsight also reveals the uniformly distributed circular structure of the\nprojection space. Building on this principle, we provide a cyclic rotation\nequivariant convolution design method to preserve projection domain symmetry\nand a cyclic group equivariance regularization approach that enforces\nconsistent rotational transformations across the entire network. We further\nintegrate these modules into a domain transform reconstruction framework and\nvalidate them using digital brain phantoms, training on discrete models and\ntesting on more complex and realistic fuzzy variants. Results indicate markedly\nimproved generalization and stability, with fewer artifacts and better detail\npreservation, especially under data distribution deviation. These findings\nhighlight the potential of cyclic group equivariance as a unifying principle\nfor tomographic reconstruction in rotationally symmetric systems, offering a\nflexible and interpretable solution for scenarios with limited data.",
        "The present work is devoted to Computability Logic (CoL), the young and\nvolcanic research-project developed by Giorgi Japaridze. Our main goal is to\nprovide the reader with a clear panoramic view of this vast new land, starting\nfrom its core knots and making our way towards the outer threads, in a somewhat\nthree-dimensional, spacial gait. Furthermore, through the present work, we\nprovide a tentative proof for the decidability of one of CoL's numerous\naxiomatisations, namely CL15. Thus, our expedition initially takes off for an\naerial, perusal overview of this fertile steppe. The first chapter introduces\nCoL in a philosophical fashion, exposing and arguing its main key points. We\nthen move over to unfold its semantics and syntax profiles, allowing the reader\nto become increasingly more familiar with this new environment. Landing on to\nthe second chapter, we thoroughly introduce Cirquent Calculus, the new\ndeductive system Japaridze has developed in order to axiomatise Computability\nLogic. Indeed, this new proof-system can also be a useful tool for many other\nlogics. We then review each of the 17 axiomatisations found so far. The third\nchapter zooms-in on CL15, in order to come up with a possible solution to its\nopen problem. We outline its soundness and completeness proofs; then provide\nsome few deductive examples; and, finally, build a tentative proof of its\ndecidability. Lastly, the fourth chapter focuses on the potential and actual\napplications of Computability Logic, both in arithmetic (clarithmetic) and in\nArtificial Intelligence systems (meaning knowledgebase and planning-and-action\nones). We close our journey with some final remarks on the richness of this\nframework and, hence, the research-worthiness it entails.",
        "We present HuGDiffusion, a generalizable 3D Gaussian splatting (3DGS)\nlearning pipeline to achieve novel view synthesis (NVS) of human characters\nfrom single-view input images. Existing approaches typically require monocular\nvideos or calibrated multi-view images as inputs, whose applicability could be\nweakened in real-world scenarios with arbitrary and\/or unknown camera poses. In\nthis paper, we aim to generate the set of 3DGS attributes via a diffusion-based\nframework conditioned on human priors extracted from a single image.\nSpecifically, we begin with carefully integrated human-centric feature\nextraction procedures to deduce informative conditioning signals. Based on our\nempirical observations that jointly learning the whole 3DGS attributes is\nchallenging to optimize, we design a multi-stage generation strategy to obtain\ndifferent types of 3DGS attributes. To facilitate the training process, we\ninvestigate constructing proxy ground-truth 3D Gaussian attributes as\nhigh-quality attribute-level supervision signals. Through extensive\nexperiments, our HuGDiffusion shows significant performance improvements over\nthe state-of-the-art methods. Our code will be made publicly available.",
        "Recent advances in Large Language Models (LLMs) have motivated the\ndevelopment of general LLMs for molecular tasks. While several studies have\ndemonstrated that fine-tuned LLMs can achieve impressive benchmark\nperformances, they are far from genuine generalist molecular LLMs due to a lack\nof fundamental understanding of molecular structure. Specifically, when given\nmolecular task instructions, LLMs trained with naive next-token prediction\ntraining assign similar likelihood scores to both original and negatively\ncorrupted molecules, revealing their lack of molecular structure understanding\nthat is crucial for reliable and general molecular LLMs. To overcome this\nlimitation and obtain a true generalist molecular LLM, we introduce a novel\nmulti-modal training method based on a thorough multi-modal instruction tuning\nas well as a molecular structure preference optimization between chosen and\nrejected graphs. On various molecular benchmarks, the proposed generalist\nmolecular LLM, called Mol-LLM, achieves state-of-the-art performances among\ngeneralist LLMs on most tasks, at the same time, surpassing or comparable to\nstate-of-the-art specialist LLMs. Moreover, Mol-LLM also shows superior\ngeneralization performances in reaction prediction tasks, demonstrating the\neffect of the molecular structure understanding for generalization perspective.",
        "This paper introduces a novel causal framework for multi-stage\ndecision-making in natural language action spaces where outcomes are only\nobserved after a sequence of actions. While recent approaches like Proximal\nPolicy Optimization (PPO) can handle such delayed-reward settings in\nhigh-dimensional action spaces, they typically require multiple models (policy,\nvalue, and reward) and substantial training data. Our approach employs\nQ-learning to estimate Dynamic Treatment Regimes (DTR) through a single model,\nenabling data-efficient policy learning via gradient ascent on language\nembeddings. A key technical contribution of our approach is a decoding strategy\nthat translates optimized embeddings back into coherent natural language. We\nevaluate our approach on mental health intervention, hate speech countering,\nand sentiment transfer tasks, demonstrating significant improvements over\ncompetitive baselines across multiple metrics. Notably, our method achieves\nsuperior transfer strength while maintaining content preservation and fluency,\nas validated through human evaluation. Our work provides a practical foundation\nfor learning optimal policies in complex language tasks where training data is\nlimited.",
        "Spot-size converters are key for efficient coupling of light between\nwaveguides of different sizes. While adiabatic tapers are well suited for small\nsize differences, they become impractically long for expansion factors around\nx100 which are often required when coupling integrated waveguides and\nfree-space beams. Evanescent couplers and bragg deflectors can be used in this\nscenario, but their operation is inherently limited in bandwidth. Here we\npropose a solution based on a parabolic dielectric interface that couples light\nfrom a 0.5 um-wide waveguide to a 285 um-wide waveguide, i.e. an expansion\nfactor of x570. We experimentally demonstrate an unprecedented bandwidth of\nmore than 380 nm with insertion losses below 0.35 dB. We furthermore provide\nanalytical expressions for the design of such parabolic spot-size-converters\nfor arbitrary expansion factors.",
        "Gibbons and Schiller have raised the physically interesting conjecture that\nforces in general relativity are bounded from above by the mathematically\ncompact relation ${\\cal F}\\leq c^4\/4G$. In the present compact paper we\nexplicitly prove, using the non-linearly coupled Einstein-matter field\nequations, that the force function ${\\cal F}\\equiv 4\\pi r^2 p(r)$ in {\\it\nstable} self-gravitating horizonless matter configurations is characterized by\nthe upper bound ${\\cal F}\\leq c^4\/G$ [here $p(r)$ is the radial pressure inside\nthe self-gravitating matter configuration]."
      ]
    }
  },
  {
    "id":2411.06184,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"New Pathologic Classification of Lung Cancer: Relevance for Clinical Practice and Clinical Trials",
    "start_abstract":"We summarize significant changes in pathologic classification of lung cancer resulting from the 2011 International Association for Study Lung Cancer\/American Thoracic Society\/European Respiratory Society (IASLC\/ATS\/ERS) adenocarcinoma classification. The was developed by an international core panel experts representing IASLC, ATS, and ERS with oncologists\/pulmonologists, pathologists, radiologists, molecular biologists, thoracic surgeons. Because 70% patients present advanced stages, a new approach to small biopsies cytology specific terminology criteria focused on need distinguishing squamous cell carcinoma testing EGFR mutations ALK rearrangement. Tumors previously classified as non-small-cell carcinoma, not otherwise specified, because lack clear or morphology should be further using limited immunohistochemical workup preserve tissue testing. terms \"bronchioloalveolar carcinoma\" \"mixed subtype adenocarcinoma\" have been discontinued. For resected adenocarcinomas, concepts situ minimally invasive define who, if they undergo complete resection, will 100% disease-free survival. Invasive adenocarcinomas are now predominant pattern after comprehensive histologic subtyping lepidic, acinar, papillary, solid patterns; micropapillary is added poor prognosis. Former mucinous bronchioloalveolar carcinomas called \"invasive adenocarcinoma.\" field rapidly evolving advances occurring frequent basis, particularly arena, this provides much needed standard diagnosis only patient care but also clinical trials TNM",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b6",
        "b7"
      ],
      "title":[
        "Multi-Task Bayesian Optimization",
        "Taking the Human Out of the Loop: A Review of Bayesian Optimization"
      ],
      "abstract":[
        "Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and shown to yield state-of-the-art performance with impressive ease efficiency. In this paper, we explore whether it is possible transfer knowledge gained from previous optimizations new tasks in order find optimal hyperparameter settings more efficiently. Our approach based on extending multi-task Gaussian processes optimization. We show that method significantly speeds up process when compared standard single-task approach. further propose straightforward extension our algorithm jointly minimize average error across multiple demonstrate how can be used greatly speed k-fold cross-validation. Lastly, an adaptation developed acquisition function, entropy search, cost-sensitive, setting. utility function by leveraging small dataset hyper-parameter large dataset. dynamically chooses which query most information per unit cost.",
        "Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing storage architectures. The construction such involves many distributed design choices. end products (e.g., recommendation medical analysis tools, real-time game engines, speech recognizers) thus involve tunable configuration parameters. These parameters often specified hard-coded into the by various developers or teams. If optimized jointly, these can result in significant improvements. Bayesian optimization is a powerful tool for joint choices that gaining great popularity recent years. It promises greater automation so as to increase both product quality human productivity. This review paper introduces optimization, highlights some its methodological aspects, showcases wide range applications."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Human-AI Collaboration in Cloud Security: Cognitive Hierarchy-Driven\n  Deep Reinforcement Learning",
        "Fluctuations of non-local branching Markov processes",
        "Understanding In-Context Machine Translation for Low-Resource Languages:\n  A Case Study on Manchu",
        "Differential virial analysis: a new technique to determine the dynamical\n  state of molecular clouds",
        "Quantum geometry of non-Hermitian systems",
        "Gravity-induced collisions of uncharged cloud droplets in an electric\n  field",
        "Breaking the Clusters: Uniformity-Optimization for Text-Based Sequential\n  Recommendation",
        "Imprints of an early matter-dominated era arising from dark matter\n  dilution mechanism on cosmic string dynamics and gravitational wave\n  signatures",
        "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on\n  Graphs",
        "A Simple yet Effective DDG Predictor is An Unsupervised Antibody\n  Optimizer and Explainer",
        "Ways of Seeing, and Selling, AI Art",
        "Project portfolio planning in the pharmaceutical industry -- strategic\n  objectives and quantitative optimization",
        "Transforming Science with Large Language Models: A Survey on AI-assisted\n  Scientific Discovery, Experimentation, Content Generation, and Evaluation",
        "A Possible Four-Month Periodicity in the Activity of FRB 20240209A",
        "Hybrid Near\/Far-Field Frequency-Dependent Beamforming via Joint\n  Phase-Time Arrays",
        "Exploring the Collaborative Co-Creation Process with AI: A Case Study in\n  Novice Music Production",
        "Besov and Triebel-Lizorkin spaces on homogeneous groups",
        "Evaluating open-source Large Language Models for automated fact-checking",
        "A general form of Newton-Maclaurin type inequalities",
        "Recent Progress in Studies of Cobalt-based Quasi-1-dimensional Quantum\n  Magnets",
        "A Natural Transformation between the Completeness and Compactness\n  Theorems in Classical Logic",
        "Quasi-Fuchsian flows and the coupled vortex equations",
        "Billiard trajectories inside Cones",
        "Harvesting primordial black holes from stochastic trees with\n  $\\texttt{FOREST}$",
        "First direct observation of a wakefield generated with structured light",
        "Beyond Human Intervention: Algorithmic Collusion through Multi-Agent\n  Learning Strategies",
        "Walkthrough of Anthropomorphic Features in AI Assistant Tools",
        "Flat degenerate metrics and Riemannian foliations"
      ],
      "abstract":[
        "Given the complexity of multi-tenant cloud environments and the need for\nreal-time threat mitigation, Security Operations Centers (SOCs) must integrate\nAI-driven adaptive defenses against Advanced Persistent Threats (APTs).\nHowever, SOC analysts struggle with countering adaptive adversarial tactics,\nnecessitating intelligent decision-support frameworks. To enhance human-AI\ncollaboration in SOCs, we propose a Cognitive Hierarchy Theory-driven Deep\nQ-Network (CHT-DQN) framework that models SOC analysts' decision-making against\nAI-driven APT bots. The SOC analyst (defender) operates at cognitive level-1,\nanticipating attacker strategies, while the APT bot (attacker) follows a\nlevel-0 exploitative policy. By incorporating CHT into DQN, our framework\nenhances SOC defense strategies via Attack Graph (AG)-based reinforcement\nlearning. Simulation experiments across varying AG complexities show that\nCHT-DQN achieves higher data protection and lower action discrepancies compared\nto standard DQN. A theoretical lower bound analysis further validates its\nsuperior Q-value performance. A human-in-the-loop (HITL) evaluation on Amazon\nMechanical Turk (MTurk) reveals that SOC analysts using CHT-DQN-driven\ntransition probabilities align better with adaptive attackers, improving data\nprotection. Additionally, human decision patterns exhibit risk aversion after\nfailure and risk-seeking behavior after success, aligning with Prospect Theory.\nThese findings underscore the potential of integrating cognitive modeling into\ndeep reinforcement learning to enhance SOC operations and develop real-time\nadaptive cloud security mechanisms.",
        "The aim of this paper is to study the fluctuations of a general class of\nsupercritical branching Markov processes with non-local branching mechanisms.\nWe show the existence of three regimes according to the size of the spectral\ngap associated with the expectation semigroup of the branching process and\nestablish functional central limit theorems within each regime.",
        "In-context machine translation (MT) with large language models (LLMs) is a\npromising approach for low-resource MT, as it can readily take advantage of\nlinguistic resources such as grammar books and dictionaries. Such resources are\nusually selectively integrated into the prompt so that LLMs can directly\nperform translation without any specific training, via their in-context\nlearning capability (ICL). However, the relative importance of each type of\nresource e.g., dictionary, grammar book, and retrieved parallel examples, is\nnot entirely clear. To address this gap, this study systematically investigates\nhow each resource and its quality affects the translation performance, with the\nManchu language as our case study. To remove any prior knowledge of Manchu\nencoded in the LLM parameters and single out the effect of ICL, we also\nexperiment with an encrypted version of Manchu texts. Our results indicate that\nhigh-quality dictionaries and good parallel examples are very helpful, while\ngrammars hardly help. In a follow-up study, we showcase a promising application\nof in-context MT: parallel data augmentation as a way to bootstrap the\nconventional MT model. When monolingual data abound, generating synthetic\nparallel data through in-context MT offers a pathway to mitigate data scarcity\nand build effective and efficient low-resource neural MT systems.",
        "Since molecular clouds form stars, at least some parts of them must be in a\nstate of collapse. However, there is a long-standing debate as to whether that\ncollapse is local, involving only a small fraction of the cloud mass, or\nglobal, with most mass in a state of collapse up to the moment when it is\ndispersed by stellar feedback. In principle it is possible to distinguish these\npossibilities from clouds' virial ratios, which should be a factor of two\nlarger for collapse than for equilibrium, but systematic uncertainties have\nthus far prevented such measurements. Here we propose a new analysis method to\novercome this limitation: while the absolute value of a cloud's virial ratio is\ntoo uncertain to distinguish global from local collapse, the differential\nchange in virial ratio as a function of surface density is also diagnostic of\nclouds' dynamical state, and can be measured with far fewer systematic\nuncertainties. We demonstrate the basic principles of the method using simple\nanalytic models of supported and collapsing clouds, validate it from full 3D\nsimulations, and discuss possible challenges in applying the method to real\ndata. We then provide a preliminary application of the technique to recent\nobservations of the molecular clouds in Andromeda, showing that most of them\nare inconsistent with being in a state of global collapse.",
        "The Berry curvature characterizes one aspect of the geometry of quantum\nstates. It materializes, among other consequences, as an anomalous velocity of\nwave packets. In non-Hermitian systems, wave packet dynamics is enriched by\nadditional terms that can be expressed as generalizations of the Berry\nconnection to non-orthogonal eigenstates. Here, we contextualize these\nanomalous non-Hermitian contributions by showing that they directly arise from\nthe geometry of the underlying quantum states as corrections to the distance\nbetween left and perturbed right eigenstates. By calculating the electric\nsusceptibility for a single-band wave packet and comparing it with the wave\npacket's localization, we demonstrate that these terms can, in some\ncircumstances, lead to a violation of fluctuation-dissipation relations in\nnon-Hermitian systems. We discuss experimental signatures in terms of response\nfunctions and transport signatures.",
        "We investigate the collisions of uncharged, conducting droplets settling\nunder gravity in the presence of an external electric field. Previous studies\nhave derived a near-field asymptotic expression for the electric-field-induced\nattraction, suggesting that this force can overcome lubrication resistance and\ndrive surface-to-surface contact between two spherical conductors within a\nfinite time. However, for droplets moving in air, traditional lubrication\ntheory breaks down when the inter-droplet gap approaches the mean free path of\nair molecules. To account for this, we incorporate non-continuum hydrodynamic\neffects to estimate the gravity-driven collision efficiency under\nelectric-field-induced forces. This study examines how an external electric\nfield influences the trajectories of settling droplet pairs of unequal sizes.\nBy analyzing their motion, we compute collision efficiencies and explore their\ndependence on droplet size ratio, electric field strength, the angle between\nthe field and gravity, and key dimensionless parameters governing\nelectric-field-induced and van der Waals forces. Our findings reveal that\nelectric-field-induced forces significantly enhance collision efficiency,\nhighlighting their critical role in droplet coalescence dynamics.",
        "Traditional sequential recommendation (SR) methods heavily rely on explicit\nitem IDs to capture user preferences over time. This reliance introduces\ncritical limitations in cold-start scenarios and domain transfer tasks, where\nunseen items and new contexts often lack established ID mappings. To overcome\nthese limitations, recent studies have shifted towards leveraging text-only\ninformation for recommendation, thereby improving model generalization and\nadaptability across domains. Although promising, text-based SR faces unique\ndifficulties: items' text descriptions often share semantic similarities that\nlead to clustered item representations, compromising their uniformity, a\nproperty essential for promoting diversity and enhancing generalization in\nrecommendation systems. In this paper, we explore a novel framework to improve\nthe uniformity of item representations in text-based SR. Our analysis reveals\nthat items within a sequence exhibit marked semantic similarity, meaning they\nare closer in representation than items overall, and that this effect is more\npronounced for less popular items, which form tighter clusters compared to\ntheir more popular counterparts. Based on these findings, we propose UniT, a\nframework that employs three pairwise item sampling strategies: Unified General\nSampling Strategy, Sequence-Driven Sampling Strategy, and Popularity-Driven\nSampling Strategy. Each strategy applies varying degrees of repulsion to\nselectively adjust the distances between item pairs, thereby refining\nrepresentation uniformity while considering both sequence context and item\npopularity. Extensive experiments on multiple real-world datasets demonstrate\nthat our proposed approach outperforms state-of-the-art models, validating the\neffectiveness of UniT in enhancing both representation uniformity and\nrecommendation accuracy.The source code is available at\nhttps:\/\/github.com\/ccwwhhh\/Model-Rec.",
        "We investigate the influence of an early matter-dominated era in cosmic\nhistory on the dynamics of cosmic strings and the resulting stochastic\ngravitational waves. Specifically, we examine the case where this era\noriginates from the dark matter dilution mechanism within the framework of the\nminimal left-right symmetric model. By numerically solving the Boltzmann\nequations governing the energy densities of the relevant components, we\nmeticulously analyze the modifications to the cosmological scale factor, the\nnumber density of cosmic string loops, and the gravitational wave spectrum. Our\nresults reveal that the early matter-dominated era causes a characteristic\nsuppression in the high-frequency regime of the gravitational wave spectrum,\nproviding distinct and testable signatures for future ground-based\ninterferometer experiments.",
        "The rapid development of Multimodal Large Language Models (MLLMs) has enabled\nthe integration of multiple modalities, including texts and images, within the\nlarge language model (LLM) framework. However, texts and images are usually\ninterconnected, forming a multimodal attributed graph (MMAG). It is\nunderexplored how MLLMs can incorporate the relational information\n(\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts\nand images) on such graphs for multimodal comprehension and generation. In this\npaper, we propose GraphGPT-o, which supports omni-multimodal understanding and\ncreation on MMAGs. We first comprehensively study linearization variants to\ntransform semantic and structural information as input for MLLMs. Then, we\npropose a hierarchical aligner that enables deep graph encoding, bridging the\ngap between MMAGs and MLLMs. Finally, we explore the inference choices,\nadapting MLLM to interleaved text and image generation in graph scenarios.\nExtensive experiments on three datasets from different domains demonstrate the\neffectiveness of our proposed method. Datasets and codes will be open-sourced\nupon acceptance.",
        "The proteins that exist today have been optimized over billions of years of\nnatural evolution, during which nature creates random mutations and selects\nthem. The discovery of functionally promising mutations is challenged by the\nlimited evolutionary accessible regions, i.e., only a small region on the\nfitness landscape is beneficial. There have been numerous priors used to\nconstrain protein evolution to regions of landscapes with high-fitness\nvariants, among which the change in binding free energy (DDG) of protein\ncomplexes upon mutations is one of the most commonly used priors. However, the\nhuge mutation space poses two challenges: (1) how to improve the efficiency of\nDDG prediction for fast mutation screening; and (2) how to explain mutation\npreferences and efficiently explore accessible evolutionary regions. To address\nthese challenges, we propose a lightweight DDG predictor (Light-DDG), which\nadopts a structure-aware Transformer as the backbone and enhances it by\nknowledge distilled from existing powerful but computationally heavy DDG\npredictors. Additionally, we augmented, annotated, and released a large-scale\ndataset containing millions of mutation data for pre-training Light-DDG. We\nfind that such a simple yet effective Light-DDG can serve as a good\nunsupervised antibody optimizer and explainer. For the target antibody, we\npropose a novel Mutation Explainer to learn mutation preferences, which\naccounts for the marginal benefit of each mutation per residue. To further\nexplore accessible evolutionary regions, we conduct preference-guided antibody\noptimization and evaluate antibody candidates quickly using Light-DDG to\nidentify desirable mutations.",
        "In early 2025, Augmented Intelligence - Christie's first AI art auction -\ndrew criticism for showcasing a controversial genre. Amid wider legal\nuncertainty, artists voiced concerns over data mining practices, notably with\nrespect to copyright. The backlash could be viewed as a microcosm of AI's\ncontested position in the creative economy. Touching on the auction's\npresentation, reception, and results, this paper explores how, among social\ndissonance, machine learning finds its place in the artworld. Foregrounding\nresponsible innovation, the paper provides a balanced perspective that\nchampions creators' rights and brings nuance to this polarised debate. With a\nfocus on exhibition design, it centres framing, which refers to the way a piece\nis presented to influence consumer perception. Context plays a central role in\nshaping our understanding of how good, valuable, and even ethical an artwork\nis. In this regard, Augmented Intelligence situates AI art within a\nsurprisingly traditional framework, leveraging hallmarks of \"high art\" to\nestablish the genre's cultural credibility. Generative AI has a clear economic\ndimension, converging questions of artistic merit with those of monetary worth.\nScholarship on ways of seeing, or framing, could substantively inform the\ninterpretation and evaluation of creative outputs, including assessments of\ntheir aesthetic and commercial value.",
        "Many pharmaceutical companies face concerns with the maintenance of desired\nrevenue levels. Sales forecasts for the current portfolio of products and\nprojects may indicate a decline in revenue as the marketed products approach\npatent expiry. To counteract the potential downturn in revenue, and to\nestablish revenue growth, an in-flow of new projects into the development\nphases is required. In this article, we devise an approach with which the\nin-flow of new projects could be optimized, while adhering to the objectives\nand constraints set on revenue targets, budget limitations and strategic\nconsiderations on the composition of the company's portfolio.",
        "With the advent of large multimodal language models, science is now at a\nthreshold of an AI-based technological transformation. Recently, a plethora of\nnew AI models and tools has been proposed, promising to empower researchers and\nacademics worldwide to conduct their research more effectively and efficiently.\nThis includes all aspects of the research cycle, especially (1) searching for\nrelevant literature; (2) generating research ideas and conducting\nexperimentation; generating (3) text-based and (4) multimodal content (e.g.,\nscientific figures and diagrams); and (5) AI-based automatic peer review. In\nthis survey, we provide an in-depth overview over these exciting recent\ndevelopments, which promise to fundamentally alter the scientific research\nprocess for good. Our survey covers the five aspects outlined above, indicating\nrelevant datasets, methods and results (including evaluation) as well as\nlimitations and scope for future research. Ethical concerns regarding\nshortcomings of these tools and potential for misuse (fake science, plagiarism,\nharms to research integrity) take a particularly prominent place in our\ndiscussion. We hope that our survey will not only become a reference guide for\nnewcomers to the field but also a catalyst for new AI-based initiatives in the\narea of \"AI4Science\".",
        "Fast Radio Bursts (FRBs) are millisecond-duration radio transients from\ndistant galaxies. While most FRBs are singular events, repeaters emit multiple\nbursts, with only two-FRB 121102 and FRB 180916B-showing periodic activity (160\nand 16 days, respectively). FRB 20240209A, discovered by CHIME-FRB, is\nlocalized to the outskirts of a quiescent elliptical galaxy (z = 0.1384). We\ndiscovered a periodicity of ~ 126 days in the activity of the FRB 20240209A,\npotentially adding to the list of extremely rare periodic repeating FRBs. We\nused auto-correlation and Lomb-Scargle periodogram analyses, validated with\nrandomized control samples, to confirm the periodicity. The FRB's location in\nan old stellar population disfavors young progenitor models, instead pointing\nto scenarios involving globular clusters, late-stage magnetars, or low-mass\nX-ray binaries (LMXBs). Though deep X-ray or polarimetric observations are not\navailable, the localization of the FRB and a possible periodicity points to\nprogenitors likely to be a binary involving a compact object and a stellar\ncompanion or a precessing or rotating old neutron star.",
        "Joint phase-time arrays (JPTA) emerge as a cost-effective and\nenergy-efficient architecture for frequency-dependent beamforming in wideband\ncommunications by utilizing both true-time delay units and phase shifters. This\npaper exploits the potential of JPTA to simultaneously serve multiple users in\nboth near- and far-field regions with a single radio frequency chain. The goal\nis to jointly optimize JPTA-based beamforming and subband allocation to\nmaximize overall system performance. To this end, we formulate a system utility\nmaximization problem, including sum-rate maximization and proportional fairness\nas special cases. We develop a 3-step alternating optimization (AO) algorithm\nand an efficient deep learning (DL) method for this problem. The DL approach\nincludes a 2-layer convolutional neural network, a 3-layer graph attention\nnetwork (GAT), and a normalization module for resource and beamforming\noptimization. The GAT efficiently captures the interactions between resource\nallocation and analog beamformers. Simulation results confirm that JPTA\noutperforms conventional phased arrays (PA) in enhancing user rate and strikes\na good balance between PA and fully-digital approach in energy efficiency.\nEmploying a logarithmic utility function for user rates ensures greater\nfairness than maximizing sum-rates. Furthermore, the DL network achieves\ncomparable performance to the AO approach, while having orders of magnitude\nlower computational complexity.",
        "Artificial intelligence is reshaping creative domains, yet its co-creative\nprocesses, especially in group settings with novice users, remain under\nexplored. To bridge this gap, we conducted a case study in a college-level\ncourse where nine undergraduate students were tasked with creating three\noriginal music tracks using AI tools over 10 weeks. The study spanned the\nentire creative journey from ideation to releasing these songs on Spotify.\nParticipants leveraged AI for music and lyric production, cover art, and\ndistribution. Our findings highlight how AI transforms creative workflows:\naccelerating ideation but compressing the traditional preparation stage, and\nrequiring novices to navigate a challenging idea selection and validation\nphase. We also identified a new \"collaging and refinement\" stage, where\nparticipants creatively combined diverse AI-generated outputs into cohesive\nworks. Furthermore, AI influenced group social dynamics and role division among\nhuman creators. Based on these insights, we propose the Human-AI Co-Creation\nStage Model and the Human-AI Agency Model, offering new perspectives on\ncollaborative co-creation with AI.",
        "This paper develops a theory of Besov spaces $\\dot{\\mathbf{B}}^{\\sigma}_{p,q}\n(N)$ and Triebel-Lizorkin spaces $\\dot{\\mathbf{F}}^{\\sigma}_{p,q} (N)$ on an\narbitrary homogeneous group $N$ for the full range of parameters $p, q \\in (0,\n\\infty]$ and $\\sigma \\in \\mathbb{R}$. Among others, it is shown that these\nspaces are independent of the choice of the Littlewood-Paley decomposition and\nthat they admit characterizations in terms of continuous maximal functions and\nmolecular frame decompositions. The defined spaces include as special cases\nvarious classical function spaces, such as Hardy spaces on homogeneous groups\nand homogeneous Sobolev spaces and Lipschitz spaces associated to\nsub-Laplacians on stratified groups.",
        "The increasing prevalence of online misinformation has heightened the demand\nfor automated fact-checking solutions. Large Language Models (LLMs) have\nemerged as potential tools for assisting in this task, but their effectiveness\nremains uncertain. This study evaluates the fact-checking capabilities of\nvarious open-source LLMs, focusing on their ability to assess claims with\ndifferent levels of contextual information. We conduct three key experiments:\n(1) evaluating whether LLMs can identify the semantic relationship between a\nclaim and a fact-checking article, (2) assessing models' accuracy in verifying\nclaims when given a related fact-checking article, and (3) testing LLMs'\nfact-checking abilities when leveraging data from external knowledge sources\nsuch as Google and Wikipedia. Our results indicate that LLMs perform well in\nidentifying claim-article connections and verifying fact-checked stories but\nstruggle with confirming factual news, where they are outperformed by\ntraditional fine-tuned models such as RoBERTa. Additionally, the introduction\nof external knowledge does not significantly enhance LLMs' performance, calling\nfor more tailored approaches. Our findings highlight both the potential and\nlimitations of LLMs in automated fact-checking, emphasizing the need for\nfurther refinements before they can reliably replace human fact-checkers.",
        "In this paper, we extend the classical Newton-Maclaurin inequalities to\nfunctions $S_{k;s}(x)=E_k(x)+\\dsum_{i=1}^s \\al_i E_{k-i}(x)$, which are formed\nby linear combinations of multiple basic symmetric mean. We proved that when\nthe coefficients $\\al_1,\\al_2,\\cdots,\\al_s$ satisfy the condition that the\npolynomial $$t^s+\\al_1 t^{s-1}+\\al_2 t^{s-2}+\\cdots+\\al_s $$ has only real\nroots, the Newton-Maclaurin type inequalities hold for $S_{k;s}(x)$.",
        "The interplay of crystal electric field, temperature, and spin-orbit coupling\ncan yield a Kramers ion and thus an effective S = 1\/2 ground state for Co2+\nions (3d7), which is often the case for low dimensional materials. This is\nbecause a highly anisotropic structural motif can force the spins to point\neither up or down, hence becoming a system where spins communicate via Ising\ninteractions. Cobalt-based quasi-1-dimensional materials have been studied in\nthis context since the latter half of the 20th century, but due to the\ndevelopment of modern characterization techniques and advances in sample\npreparation, the exotic physical phenomena that have generated the most\ninterest have only emerged in the most recent three to four decades. This\ntopical review mainly summarizes progress in cobalt-based quasi-1-dimensional\nquantum magnets, and comments on a few research directions of potential future\ninterest.",
        "In this paper, we present a categorical framework that reveals the deep\nrelationship between the completeness and compactness theorems in classical\nlogic. Although these theorems have traditionally been approached by distinct\nmethods, our study shows that they are naturally equivalent through the lens of\ncategory theory. By introducing the basic concepts of categories, functors, and\nnatural transformations, we establish that the model constructions derived from\neach theorem can be viewed as functors from the category of logical theories to\nthe category of their models. In particular, one functor is defined using the\nHenkin construction from the completeness theorem, while another is defined\nbased on the finite satisfiability condition of the compactness theorem. We\nthen prove the existence of a natural transformation between these two\nfunctors, which demonstrates that the models produced by each method are\nisomorphic in a canonical way. This result not only bridges the gap between the\nproof-theoretic and model-theoretic perspectives but also provides a unified,\nconceptual framework that can be extended to other areas of mathematical logic.\nOur exposition is designed to be accessible, offering insight into both the\ncategorical formalism and its implications for understanding fundamental\nlogical theorems.",
        "We provide an alternative construction of the quasi-Fuchsian flows introduced\nby Ghys in \\cite{Ghys-92}. Our approach is based on the coupled vortex\nequations that allows to see these flows as thermostats on the unit tangent\nbundle of the Blaschke metric uniquely determined by a conformal class and a\nholomorphic quadratic differential. We also give formulas for the marked length\nspectrum of a quasi-Fuchsian flow in the thermostat parametrization.",
        "Recently it was proved that every billiard trajectory inside a $C^3$ convex\ncone has a finite number of reflections.\n  Here, by a $C^3$ convex cone, we mean a cone whose section with some\nhyperplane is a strictly convex closed $C^3$ submanifold of the hyperplane with\nnondegenerate second fundamental form.\n  In this paper,\n  we prove the existence of $C^2$ convex cones admitting billiard trajectories\nwith infinitely many reflections in finite time.\n  We also estimate the number of reflections of billiard trajectories in\nelliptic cones in $\\mathbb{R}^3$\n  using two first integrals.",
        "We introduce a novel framework to implement stochastic inflation on\nstochastic trees, modelling the inflationary expansion as a branching process.\nCombined with the $\\delta N$ formalism, this allows us to generate real-space\nmaps of the curvature perturbation that fully capture quantum diffusion and its\nnon-perturbative backreaction during inflation. Unlike lattice methods, trees\ndo not proceed on a fixed background since new spacetime units emerge\ndynamically as trees unfold, naturally incorporating metric fluctuations. The\nrecursive structure of stochastic trees also offers remarkable numerical\nefficiency, and we develop the FOrtran Recursive Exploration of Stochastic\nTrees ($\\texttt{FOREST}$) tool and demonstrate its performance. We show how\nprimordial black holes blossom at unbalanced nodes of the trees, and how their\nmass distribution can be obtained while automatically accounting for the\n\"cloud-in-cloud\" effect. In the \"quantum-well\" toy model, we find broad mass\ndistributions, with mild power laws terminated by exponential tails. We finally\ncompare our results with existing approximations in the literature and discuss\nseveral prospects.",
        "The use of structured light to control the phase velocity of the wake in\nlaser-wakefield accelerators has generated significant interest for its ability\nto mitigate electron dephasing. Combining the diffraction-free properties of\nBessel beams with spatio-temporal shaping of the pulse promises to enable\nacceleration with an unprecedented combination of long acceleration lengths and\nhigh gradients. This would facilitate the acceleration of electrons to energies\nabove 100 GeV in existing laser facilities. In-depth understanding of the\nphysical mechanisms involved is critical to achieving dephasing-free electron\nacceleration. Here we present the first experimental observation of wakefields\ngenerated by beams that were spatio-temporally sculpted and then focused with a\nlong-focal-depth mirror, known as an axiparabola, which generates a\nquasi-Bessel beam. The resulting wakefield was imaged using femtosecond\nrelativistic electron microscopy. Novel insights into this minimally explored\nregime include mapping the wakefield development over the focal depth and\nstudying the effects of spatio-temporal manipulations of the beam on the\nstructure and phase velocity of the wakefield. Such insights pave the way\ntowards realizing the potential of structured-light based solutions to\ndephasing in laser-wakefield acceleration.",
        "Collusion in market pricing is a concept associated with human actions to\nraise market prices through artificially limited supply. Recently, the idea of\nalgorithmic collusion was put forward, where the human action in the pricing\nprocess is replaced by automated agents. Although experiments have shown that\ncollusive market equilibria can be reached through such techniques, without the\nneed for human intervention, many of the techniques developed remain\nsusceptible to exploitation by other players, making them difficult to\nimplement in practice. In this article, we explore a situation where an agent\nhas a multi-objective strategy, and not only learns to unilaterally exploit\nmarket dynamics originating from other algorithmic agents, but also learns to\nmodel the behaviour of other agents directly. Our results show how common\ncritiques about the viability of algorithmic collusion in real-life settings\ncan be overcome through the usage of slightly more complex algorithms.",
        "In this paper, we attempt to understand the anthropomorphic features of\nchatbot outputs and how these features provide a discursive frame for human-AI\ninteractions. To do so, we explore the use of a prompt-based walkthrough method\nwith two phases: (1) interview-style prompting to reveal the chatbots' context\nof expected use and (2) roleplaying-type prompting to evoke everyday use\nscenarios and typical chatbot outputs. We applied this method to catalogue\nanthropomorphic features across four different LLM chatbots, finding that\nanthropomorphism was exhibited as both subjective language and a sympathetic\nconversational tone. We also found that socio-emotional cues in prompts\nincrease the incidence of anthropomorphic expressions in outputs. We argue that\nthe prompt-based walkthrough method was successful in stimulating social role\nperformance in LLM chatbots and in eliciting a variety of anthropomorphic\nfeatures, making it useful in the study of interaction-based algorithmic harms\nwhere users project inappropriate social roles onto LLM-based tools.",
        "Bandyopadhyay, Dacorogna, Matveev and Troyanov conjectured that a closed\nmanifold admitting a flat, non-negative definite metric of constant rank $m$\nshould be finitely covered by a fiber bundle over the $m$-torus. We give a\ncounter-example to this statement and we discuss the link between this problem\nand the study of transversely flat Riemannian foliations."
      ]
    }
  },
  {
    "id":2412.20675,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Discriminative feature learning using a multiscale convolutional capsule network from attitude data for fault diagnosis of industrial robots",
    "start_abstract":"Effective fault diagnosis is important to ensure the reliability, safety, and efficiency of industrial robots. This article proposes a simple yet effective data acquisition strategy based on transmission mechanism analysis, using only one attitude sensor mounted on an end effector or an output component to monitor the attitude of all transmission components. Unlike widely used vibration-monitoring signals, attitude signals can provide fault features reflecting spatial relationships. Using one attitude sensor facilitates the data collection, but weakens fault features and introduces strong background noise in attitude signals. To learn discriminative features from the attitude data collected by the attitude sensor, a multiscale convolutional capsule network (MCCN) is proposed. In MCCN, integrating low-level and high-level features in a convolutional neural network (CNN) as multiscale features is conductive to noise reduction and robust feature extraction, and a capsule network (CapsNet) is used to recognize the spatial relationships in attitude data. The extracted multiscale features in CNN and the spatial-relational features in CapsNet are fused for effective fault diagnosis of industrial robots. The performance of MCCN is evaluated by attaching a softmax-based classifier and integrating it into different transfer learning frameworks to diagnose faults in industrial robots under single and variable working conditions, respectively. Fault diagnosis experiments were conducted on a 6-axis series industrial robot and a parallel robot-driven 3D printer. The superiority of the proposed MCCN was demonstrated by comparing its performance with the other feature learning methods.",
    "start_categories":[
      "eess.SP"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b21"
      ],
      "title":[
        "LSTM Based Bearing Fault Diagnosis of Electrical Machines using Motor Current Signal"
      ],
      "abstract":[
        "Rolling element bearings are one of the most critical components rotating machinery, with bearing faults amounting up to 50% in electrical machines. Therefore, fault diagnosis has attracted attention many researchers. Typically, is performed using vibration signals from machine. In addition, by deep learning algorithms on signals, detection accuracy close 100% can be achieved. However, measurement requires an additional sensor, which not present majority Nevertheless, alternative approach, stator current used for diagnosis. paper emphasizes current. The signal processing signature extraction that buried underneath noise signal. uses Paderborn University damaged dataset, contains data healthy, real inner raceway and outer different severity. For redundant frequencies filtered, then filtered eight features extracted time time-frequency domain wavelet packet decomposition (WPD). Then, these well known algorithm Long Short-Term Memory (LSTM), classification made. LSTM mostly speech recognition due its coherence, but this paper, ability also demonstrated 96%, outperforms perform method developed independent speed loading conditions."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Love numbers beyond GR from the modified Teukolsky equation",
        "Formation of Be star decretion discs through boundary layer effects",
        "Action representability in categories of unitary algebras",
        "Impact of the Pandemic on Currency Circulation in Brazil: Projections\n  using the SARIMA Model",
        "A precise asymptotic analysis of learning diffusion models: theory and\n  insights",
        "PINK: physical-informed machine learning for lattice thermal\n  conductivity",
        "Search for doubly charmed dibaryons in baryon-baryon scattering",
        "Nonlinear Nanophotonics for High-Dimensional Quantum States",
        "From discrete-time policies to continuous-time diffusion samplers:\n  Asymptotic equivalences and faster training",
        "Linear-Quadratic Optimal Control for Mean-Field Stochastic Differential\n  Equations in Infinite-Horizon with Regime Switching",
        "Pair correlations of one-dimensional model sets and monstrous\n  covariograms of Rauzy fractals",
        "A population synthesis study of the Gaia 100 pc unresolved white\n  dwarf-main sequence binary population",
        "Max-Min Fairness for IRS-Assisted Secure Two-Way Communications",
        "On Coordinated Drone-Courier Logistics for Intra-city Express Services",
        "Extinction and Extirpation Conditions in Coalescent and Ecotonal\n  Metacommunities",
        "The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model",
        "Robust design of bicycle infrastructure networks",
        "Single-Source Localization as an Eigenvalue Problem",
        "Simulating Raman Scattering Impairments with Depolarization Noise in\n  Quantum-Classical Links",
        "Quantized Analog Beamforming Enabled Multi-task Federated Learning\n  Over-the-air",
        "Incentive-Compatible Recovery from Manipulated Signals, with\n  Applications to Decentralized Physical Infrastructure",
        "MDCrow: Automating Molecular Dynamics Workflows with Large Language\n  Models",
        "Turbulent-like flows in quasi two-dimensional dense suspensions of\n  motile colloids",
        "Vanishing theorems for Hodge numbers and the Calabi curvature operator",
        "GPU-accelerated LISA parameter estimation with full time domain response",
        "On the number of cofinalities of cuts in ultraproducts of linear orders",
        "Towards an AI co-scientist",
        "Well-posedness and blowup of 1D electron magnetohydrodynamics",
        "Bayesian estimation of Unit-Weibull distribution based on dual\n  generalized order statistics with application to the Cotton Production Data"
      ],
      "abstract":[
        "We obtain the full set of tidal Love numbers of non-rotating black holes in\nan effective field theory extension of general relativity. We achieve our\nresults using a recently introduced modified Teukolsky equation that describes\nthe perturbations of black holes in this theory. We show how to identify the\nLove numbers and their beta functions in a systematic and gauge invariant way,\napplying analytic continuation on the angular number $\\ell$ when necessary. We\nobserve that there are three types of Love numbers: electric, magnetic, and a\n``mixing'' type, associated to parity-breaking theories, that we identify here\nfor the first time. The modified Teukolsky equation proves to be very useful as\nit allows us to obtain all the different Love numbers in a unified framework.\nWe compare our results with previous literature that utilized the\nRegge-Wheeler-Zerilli equations to compute Love numbers, finding perfect\nagreement. The method introduced here paves the way towards the computation of\nLove numbers of rotating black holes beyond general relativity.",
        "Be stars are rapidly rotating, with angular frequency around $0.7-0.8$ of\ntheir Keplerian break up frequency, as a result of significant accretion during\nthe earlier stellar evolution of a companion star. Material from the equator of\nthe Be star is ejected and forms a decretion disc, although the mechanism for\nthe disc formation has remained elusive. We find one-dimensional steady state\ndecretion disc solutions that smoothly transition from a rapidly rotating star\nthat is in hydrostatic balance. Boundary layer effects in a geometrically thick\ndisc which connects to a rotationally flattened star enable the formation of a\ndecretion disc at stellar spin rates below the break up rate. For a disc with\nan aspect ratio $H\/R\\approx 0.1$ at the inner edge, the torque from the disc on\nthe star slows the stellar spin to the observed range and mass ejection\ncontinues at a rate consistent with observed decretion rates. The critical\nrotation rate, to which the star slows down to, decreases as the disc aspect\nratio increases. More generally, steady state accretion and decretion disc\nsolutions can be found for all stellar spin rates. The outcome for a particular\nsystem depends upon the balance between the decretion rate and any external\ninfall accretion rate.",
        "In a recent article [13], G. Janelidze introduced the concept of ideally\nexact categories as a generalization of semi-abelian categories, aiming to\nincorporate relevant examples of non-pointed categories, such as the categories\n$\\textbf{Ring}$ and $\\textbf{CRing}$ of unitary (commutative) rings. He also\nextended the notion of action representability to this broader framework,\nproving that both $\\textbf{Ring}$ and $\\textbf{CRing}$ are action\nrepresentable.\n  This article investigates the representability of actions of unitary\nnon-associative algebras. After providing a detailed description of the monadic\nadjunction associated with any category of unitary algebra, we use the\nconstruction of the external weak actor [4] in order to prove that the\ncategories of unitary (commutative) associative algebras and that of unitary\nalternative algebras are action representable. The result is then extended for\nunitary (commutative) Poisson algebras, where the explicit construction of the\nuniversal strict general actor is employed.",
        "This study analyzes the impact of the COVID-19 pandemic on currency\ncirculation in Brazil by comparing actual data from 2000 to 2023 with\ncounterfactual projections using the\n\\textbf{SARIMA(3,1,1)(3,1,4)\\textsubscript{12}} model. The model was selected\nbased on an extensive parameter search, balancing accuracy and simplicity, and\nvalidated through the metrics MAPE, RMSE, and AIC. The results indicate a\nsignificant deviation between projected and observed values, with an average\ndifference of R\\$ 47.57 billion (13.95\\%). This suggests that the pandemic,\nemergency policies, and the introduction of \\textit{Pix} had a substantial\nimpact on currency circulation. The robustness of the SARIMA model was\nconfirmed, effectively capturing historical trends and seasonality, though\nfindings emphasize the importance of considering exogenous variables, such as\ninterest rates and macroeconomic policies, in future analyses. Future research\nshould explore multivariate models incorporating economic indicators, long-term\nanalysis of post-pandemic currency circulation trends, and studies on public\ncash-holding behavior. The results reinforce the need for continuous monitoring\nand econometric modeling to support decision-making in uncertain economic\ncontexts.",
        "In this manuscript, we consider the problem of learning a flow or\ndiffusion-based generative model parametrized by a two-layer auto-encoder,\ntrained with online stochastic gradient descent, on a high-dimensional target\ndensity with an underlying low-dimensional manifold structure. We derive a\ntight asymptotic characterization of low-dimensional projections of the\ndistribution of samples generated by the learned model, ascertaining in\nparticular its dependence on the number of training samples. Building on this\nanalysis, we discuss how mode collapse can arise, and lead to model collapse\nwhen the generative model is re-trained on generated synthetic data.",
        "Lattice thermal conductivity ($\\kappa_L$) is crucial for efficient thermal\nmanagement in electronics and energy conversion technologies. Traditional\nmethods for predicting \\k{appa}L are often computationally expensive, limiting\ntheir scalability for large-scale material screening. Empirical models, such as\nthe Slack model, offer faster alternatives but require time-consuming\ncalculations for key parameters such as sound velocity and the Gruneisen\nparameter. This work presents a high-throughput framework, physical-informed\nkappa (PINK), which combines the predictive power of crystal graph\nconvolutional neural networks (CGCNNs) with the physical interpretability of\nthe Slack model to predict \\k{appa}L directly from crystallographic information\nfiles (CIFs). Unlike previous approaches, PINK enables rapid, batch predictions\nby extracting material properties such as bulk and shear modulus from CIFs\nusing a well-trained CGCNN model. These properties are then used to compute the\nnecessary parameters for $\\kappa_L$ calculation through a simplified physical\nformula. PINK was applied to a dataset of 377,221 stable materials, enabling\nthe efficient identification of promising candidates with ultralow $\\kappa_L$\nvalues, such as Ag$_3$Te$_4$W and Ag$_3$Te$_4$Ta. The platform, accessible via\na user-friendly interface, offers an unprecedented combination of speed,\naccuracy, and scalability, significantly accelerating material discovery for\nthermal management and energy conversion applications.",
        "We perform a systematical investigation of the doubly charmed dibaryon system\nwith quantum numbers $IJ=01$, and strangeness numbers $S=0$, $-2$ and $-4$ in\nthe framework of the chiral quark model. Two resonance states with strangeness\nnumbers $S=-2$ is obtained in the $\\Lambda\\Omega_{cc}$ scattering channel,\nwhich are $\\Xi_{cc}^{\\ast}\\Xi$ with resonance mass 5081 MeV and decay width 0.3\nMeV, and the $\\Xi_{c}\\Xi_{c}^{\\ast}$ state with the mass 5213 MeV and decay\nwidth 19.8 MeV, respectively. These two predicted charmed dibaryon candidates\nare worth searching for experimentally. Besides, we would like to emphasize\nthat the multi-channel coupling calculation is important to confirm the\nexistence of multiquark states. The coupling can shift the energy of the\nresonance, give the width to the resonance and even destroy the resonance.\nTherefore, to provide the necessary information for experiments to search for\nexotic hadron states, the coupling calculation between the bound channels and\nopen channels is indispensable.",
        "Quantum nanophotonics merges the precision of nanoscale light manipulation\nwith the capabilities of quantum technologies, offering a pathway for enhanced\nlight-matter interaction and compact realization of quantum devices. Here, we\nshow how a recently-demonstrated nonlinear nanophotonic process can be employed\nto selectively create photonic high-dimensional quantum states (qudits). We\nutilize the nonlinearity on the surface of the nanophotonic device to dress,\nthrough the polarization of the pump field, the near-field modes carrying\nangular momentum and their superpositions. We then use this approach for the\nrealization of a multilevel quantum key distribution protocol, which doubles\nthe key rate compared to standard schemes. This idea is an important step\ntowards experimental realizations of quantum state generation and manipulation\nthrough nonlinearity within nanophotonic platforms, and enables new\ncapabilities for on-chip quantum devices.",
        "We study the problem of training neural stochastic differential equations, or\ndiffusion models, to sample from a Boltzmann distribution without access to\ntarget samples. Existing methods for training such models enforce time-reversal\nof the generative and noising processes, using either differentiable simulation\nor off-policy reinforcement learning (RL). We prove equivalences between\nfamilies of objectives in the limit of infinitesimal discretization steps,\nlinking entropic RL methods (GFlowNets) with continuous-time objects (partial\ndifferential equations and path space measures). We further show that an\nappropriate choice of coarse time discretization during training allows greatly\nimproved sample efficiency and the use of time-local objectives, achieving\ncompetitive performance on standard sampling benchmarks with reduced\ncomputational cost.",
        "This paper is concerned with stochastic linear quadratic (LQ, for short)\noptimal control problems in an infinite horizon with conditional mean-field\nterm in a switching regime environment. The orthogonal decomposition introduced\nin [21] has been adopted. Desired algebraic Riccati equations (AREs, for short)\nand a system of backward stochastic differential equations (BSDEs, for short)\nin infinite time horizon with the coefficients depending on the Markov chain\nhave been derived. The determination of closed-loop optimal strategy follows\nfrom the solvability of ARE and BSDE. Moreover, the solvability of BSDEs leads\nto a characterization of open-loop solvability of the optimal control problem.",
        "The averaged distance structure of one-dimensional regular model sets is\ndetermined via their pair correlation functions. The latter lead to\ncovariograms and cross covariograms of the windows, which give continuous\nfunctions in internal space. While they are simple tent-shaped, piecewise\nlinear functions for intervals, the typical case for inflation systems leads to\nconvolutions of Rauzy fractals, which are difficult to compute. In the presence\nof an inflation structure, an alternative path is possible via the exact\nrenormalisation structures of the pair correlation functions. We introduce this\napproach and derive two concrete examples, which display an unexpectedly\ncomplex and wild behaviour.",
        "Binary stars consisting of a white dwarf and a main sequence star (WDMS) are\nvaluable for studying key astrophysical questions. However, observational\nbiases strongly affect the known population, particularly unresolved systems\nwhere the main sequence star outshines the white dwarf. This work aims to\ncomprehensively simulate the population of unresolved WDMS binaries within 100\npc of the Sun and to compare the outcome with the currently most complete\nvolume-limited sample available from Gaia data. We employ a population\nsynthesis code, MRBIN, extensively developed by our group and based on Monte\nCarlo techniques, which uses a standard binary stellar evolutionary code\nadapted to cover a wide range of stars across all ages, masses, and\nmetallicities. Selection criteria matching those of Gaia observations are\napplied to generate synthetic populations comparable to the observed WDMS\nsample. The synthetic data accurately populate the expected regions in the Gaia\ncolor-magnitude diagram. However, simulations predict a lower number of\nextremely low-mass white dwarfs, suggesting potential issues in observed mass\nderivations. Additionally, our analysis constrains the common envelope\nefficiency to 0.1-0.4, consistent with previous findings, and estimates a total\ncompleteness of about 25% for the observed sample, confirming the strong\nobservational limitations for unresolved WDMS.",
        "This paper investigates an intelligent reflective surface (IRS) assisted\nsecure multi-user two-way communication system. The aim of this paper is to\nenhance the physical layer security by optimizing the minimum secrecy-rate\namong all user-pairs in the presence of a malicious user. The optimization\nproblem is converted into an alternating optimization problem consisting of two\nsub-problems. Transmit power optimization is handled using a fractional\nprogramming method, whereas IRS phase shift optimization is handled with\nsemi-definite programming. The convergence of the proposed algorithm is\ninvestigated numerically. The performance gain in minimum secrecy-rate is\nquantified for four different user configurations in comparison to the baseline\nscheme. Results indicate a 3.6-fold gain in minimum secrecy rate over the\nbaseline scheme when the IRS is positioned near a legitimate user, even when\nthe malicious user is located close to the same legitimate user.",
        "Problem definition: Drones, despite being acknowledged as a transformative\nforce in the city logistics sector, are unable to execute the\n\\textit{last-meter delivery} (unloading goods directly to customers' doorsteps)\ndue to airspace restrictions and safety concerns. To leverage advancements and\novercome the limitations of drones in providing intra-city express services, we\nintroduce a coordinated drone-courier logistics system where drones operate\nwithin a closed network among vertiports, while couriers connect customers to\nthe drone delivery system. This paper aims to shed light on this coordinated\nsystem in terms of system feasibility, network interactivity, and long-term\nsustainability. Methodology\/Results: We develop an integrated optimization\nmodel to optimize the network planning of the coordinated logistics system. The\ninterplay between network planning and tactical operations is mirrored by a\nqueueing network model, resulting in the nonlinear and nonconvex (partially\nconvex and partially concave) feasible region of the optimization model. An\niterative exact algorithm that tightens lower and upper bounds by adaptively\nrefining the linear approximations of nonlinear constraints is developed to\nprovide optimality-guaranteed solutions with finite convergence. The\ncomputational experiments demonstrate the scalability and robustness of our\nalgorithm across various network configurations and scenarios.Managerial\nimplications: The case study, based on a real-world dataset from SF Express, a\nlogistics giant in China, validates that the coordinated logistics system\nefficiently attains cost and time savings by leveraging the effective turnover\nof drones and the coordination between drones and couriers. The optimal network\ndesign features a concentrated structure, streamlining demand consolidation and\nreducing deadhead repositioning.",
        "Here we present extinction, extirpation and coexistence conditions where \/\nwhen two communities combine. We consider one specific model where two\ncommunities coalesce, and another model where the communities coexist side by\nside, blending in a transitionary zone called the ecotone. Specifically, (1) we\nanalytically calculate the shifts in abundances as a function of mixing\nstrength. (2) Obtain a critical value for the mixing strength leading to\nextinction. (3) Derive an inequality condition for full coexistent mixing. (4)\nfind how the individual communities penetrate into one other as a function of\nmixing strength. (5) derive the conditions for one species to cross the ecotone\nand invade an neighboring community and (6) conditions for a native species to\nget extirpated. Lastly, (7) we spatially investigate the species richness\nwithin the ecotone and derive a condition that determines whether the ecotone\nwill have higher or lower richness compared to its surrounding habitats.",
        "Self-distillation (SD), a technique where a model refines itself from its own\npredictions, has garnered attention as a simple yet powerful approach in\nmachine learning. Despite its widespread use, the mechanisms underlying its\neffectiveness remain unclear. In this study, we investigate the efficacy of\nhyperparameter-tuned multi-stage SD in binary classification tasks with noisy\nlabeled Gaussian mixture data, utilizing a replica theory. Our findings reveals\nthat the primary driver of SD's performance improvement is denoising through\nhard pseudo-labels, with the most notable gains observed in moderately sized\ndatasets. We also demonstrate the efficacy of practical heuristics, such as\nearly stopping for extracting meaningful signal and bias fixation for\nimbalanced data. These results provide both theoretical guarantees and\npractical insights, advancing our understanding and application of SD in noisy\nsettings.",
        "Promoting active mobility like cycling relies on the availability of\nwell-connected, high-quality bicycle networks. However, expanding these\nnetworks over an extended planning horizon presents one of the most complex\nchallenges in transport science. This complexity arises from the intricate\ninteractions between infrastructure availability and usage, such as network\nspillover effects and mode choice substitutions. In this paper, we approach the\nproblem from two perspectives: direct optimization methods, which generate\nnear-optimal solutions using operations research techniques, and conceptual\nheuristics, which offer intuitive and scalable algorithms grounded in network\nscience. Specifically, we compare direct welfare optimization with an inverse\nnetwork percolation approach to planning cycle superhighway extensions in\nCopenhagen. Interestingly, while the more complex optimization models yield\nbetter overall welfare results, the improvements over simpler methods are\nsmall. More importantly, we demonstrate that the increased complexity of\nplanning approaches generally makes them more vulnerable to input uncertainty,\nreflecting the bias-variance tradeoff. This issue is particularly relevant in\nthe context of long-term planning, where conditions change during the\nimplementation of the planned infrastructure expansions. Therefore, while\nplanning bicycle infrastructure is important and renders exceptionally high\nbenefit-cost ratios, considerations of robustness and ease of implementation\nmay justify the use of more straightforward network-based methods.",
        "This paper introduces a novel method for solving the single-source\nlocalization problem, specifically addressing the case of trilateration. We\nformulate the problem as a weighted least-squares problem in the squared\ndistances and demonstrate how suitable weights are chosen to accommodate\ndifferent noise distributions. By transforming this formulation into an\neigenvalue problem, we leverage existing eigensolvers to achieve a fast,\nnumerically stable, and easily implemented solver. Furthermore, our theoretical\nanalysis establishes that the globally optimal solution corresponds to the\nlargest real eigenvalue, drawing parallels to the existing literature on the\ntrust-region subproblem. Unlike previous works, we give special treatment to\ndegenerate cases, where multiple and possibly infinitely many solutions exist.\nWe provide a geometric interpretation of the solution sets and design the\nproposed method to handle these cases gracefully. Finally, we validate against\na range of state-of-the-art methods using synthetic and real data,\ndemonstrating how the proposed method is among the fastest and most numerically\nstable.",
        "We model spontaneous Raman scattering noise in polarization-encoded quantum\ncommunication channels co-propagating with classical signals using the\ndepolarization channel. Utilizing NetSquid simulations, we validate the model\nagainst demonstrations of qubit transmission, entanglement distribution, and\nteleportation.",
        "Over-the-air computation (AirComp) has recently emerged as a pivotal\ntechnique for communication-efficient federated learning (FL) in\nresource-constrained wireless networks. Though AirComp leverages the\nsuperposition property of multiple access channels for computation, it\ninherently limits its ability to manage inter-task interference in multi-task\ncomputing. In this paper, we propose a quantized analog beamforming scheme at\nthe receiver to enable simultaneous multi-task FL. Specifically, inspiring by\nthe favorable propagation and channel hardening properties of large-scale\nantenna arrays, a targeted analog beamforming method in closed form is proposed\nfor statistical interference elimination. Analytical results reveal that the\ninterference power vanishes by an order of $\\mathcal{O}\\left(1\/N_r\\right)$ with\nthe number of analog phase shifters, $N_r$, irrespective of their quantization\nprecision. Numerical results demonstrate the effectiveness of the proposed\nanalog beamforming method and show that the performance upper bound of ideal\nlearning without errors can be achieved by increasing the number of\nlow-precision analog phase shifters.",
        "We introduce the first formal model capturing the elicitation of unverifiable\ninformation from a party (the \"source\") with implicit signals derived by other\nplayers (the \"observers\"). Our model is motivated in part by applications in\ndecentralized physical infrastructure networks (a.k.a. \"DePIN\"), an emerging\napplication domain in which physical services (e.g., sensor information,\nbandwidth, or energy) are provided at least in part by untrusted and\nself-interested parties. A key challenge in these signal network applications\nis verifying the level of service that was actually provided by network\nparticipants.\n  We first establish a condition called source identifiability, which we show\nis necessary for the existence of a mechanism for which truthful signal\nreporting is a strict equilibrium. For a converse, we build on techniques from\npeer prediction to show that in every signal network that satisfies the source\nidentifiability condition, there is in fact a strictly truthful mechanism,\nwhere truthful signal reporting gives strictly higher total expected payoff\nthan any less informative equilibrium. We furthermore show that this truthful\nequilibrium is in fact the unique equilibrium of the mechanism if there is\npositive probability that any one observer is unconditionally honest (e.g., if\nan observer were run by the network owner). Also, by extending our condition to\ncoalitions, we show that there are generally no collusion-resistant mechanisms\nin the settings that we consider.\n  We apply our framework and results to two DePIN applications: proving\nlocation, and proving bandwidth. In the location-proving setting observers\nlearn (potentially enlarged) Euclidean distances to the source. Here, our\ncondition has an appealing geometric interpretation, implying that the source's\nlocation can be truthfully elicited if and only if it is guaranteed to lie\ninside the convex hull of the observers.",
        "Molecular dynamics (MD) simulations are essential for understanding\nbiomolecular systems but remain challenging to automate. Recent advances in\nlarge language models (LLM) have demonstrated success in automating complex\nscientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an\nagentic LLM assistant capable of automating MD workflows. MDCrow uses\nchain-of-thought over 40 expert-designed tools for handling and processing\nfiles, setting up simulations, analyzing the simulation outputs, and retrieving\nrelevant information from literature and databases. We assess MDCrow's\nperformance across 25 tasks of varying required subtasks and difficulty, and we\nevaluate the agent's robustness to both difficulty and prompt style.\n\\texttt{gpt-4o} is able to complete complex tasks with low variance, followed\nclosely by \\texttt{llama3-405b}, a compelling open-source model. While prompt\nstyle does not influence the best models' performance, it has significant\neffects on smaller models.",
        "Dense bacterial suspensions exhibit turbulent-like flows at low Reynolds\nnumbers, driven by the activity of the microswimmers. In this study, we develop\na model system to examine these dynamics using motile colloids that mimic\nbacterial locomotion. The colloids are powered by the Quincke instability,\nwhich causes them to spontaneously roll in a random-walk pattern when exposed\nto a square-wave electric field. We experimentally investigate the flow\ndynamics in dense suspensions of these Quincke random walkers under quasi\ntwo-dimensional conditions, where the particle size is comparable to the gap\nbetween the electrodes. Our results reveal an energy spectrum scaling at high\nwavenumbers as $ \\sim k^{-4}$, which holds across a broad range of activity\nlevels -- controlled by the field strength -- and particle concentrations. We\nobserve that velocity time correlations decay within a single period of the\nsquare-wave field, yet an anti-correlation appears between successive field\napplications, indicative of a dynamic structural memory of the ensemble.",
        "It is shown that a compact $n$-dimensional K\\\"ahler manifold with\n$\\frac{n}{2}$-positive Calabi curvature operator has the rational cohomology of\ncomplex projective space. For even $n,$ this is sharp in the sense that the\ncomplex quadric with its symmetric metric has $\\frac{n}{2}$-nonnegative Calabi\ncurvature operator, yet $b_n =2.$ Furthermore, the compact K\\\"ahler manifolds\nwith $\\frac{n}{2}$-nonnegative Calabi curvature operator are completely\nclassified. In addition, the previously known results for the K\\\"ahler\ncurvature operator are improved when the metric is K\\\"ahler-Einstein.",
        "We conduct the first full Bayesian inference analysis for LISA parameter\nestimation incorporating the effects of subdominant harmonics and\nspin-precession through a full time domain response. The substantial\ncomputational demands of using time domain waveforms for LISA are significantly\nmitigated by implementing a novel Python version of the IMRPhenomT family of\nwaveform models and the LISA response with GPU acceleration. This time domain\nresponse alleviates the theoretical necessity of developing specific transfer\nfunctions to approximate the LISA response in the Fourier domain for each\nspecific type of system and allows for the use of unequal arms configurations\nand realistic LISA orbits. Our analysis includes a series of zero-noise\ninjections for a Massive Black Hole Binary with aligned and precessing spins.\nWe investigate the impact of including subdominant harmonics, compare equal and\nunequal arm configurations, and analyze different Time-Delay-Interferometry\n(TDI) configurations. We utilize full and uniform priors, with a lower\nfrequency cutoff of 0.1mHz, and a signal duration of approximately two months,\nsampled every 5 seconds. The sampler is initialized based on Fisher estimates.\nOur results demonstrate LISA capability to measure the two spin magnitudes and\nthe primary spin tilt angle, alongside sky localization, with percent-level\nprecision, while component masses are determined with sub-percent accuracy.",
        "Suppose $\\kappa$ is a regular cardinal and $\\bar a=\\langle \\mu_i: i<\\kappa\n\\rangle$ is a non-decreasing sequence of regular cardinals. We study the set of\npossible cofinalities of cuts Pcut$(\\bar a)=\\{(\\lambda_1, \\lambda_2):$ for some\nultrafilter $D$ on $\\kappa$, $(\\lambda_1, \\lambda_2)$ is the cofinality of a\ncut of $\\prod\\limits_{i<\\kappa} \\mu_i \/ D \\}$.",
        "Scientific discovery relies on scientists generating novel hypotheses that\nundergo rigorous experimental validation. To augment this process, we introduce\nan AI co-scientist, a multi-agent system built on Gemini 2.0. The AI\nco-scientist is intended to help uncover new, original knowledge and to\nformulate demonstrably novel research hypotheses and proposals, building upon\nprior evidence and aligned to scientist-provided research objectives and\nguidance. The system's design incorporates a generate, debate, and evolve\napproach to hypothesis generation, inspired by the scientific method and\naccelerated by scaling test-time compute. Key contributions include: (1) a\nmulti-agent architecture with an asynchronous task execution framework for\nflexible compute scaling; (2) a tournament evolution process for self-improving\nhypotheses generation. Automated evaluations show continued benefits of\ntest-time compute, improving hypothesis quality. While general purpose, we\nfocus development and validation in three biomedical areas: drug repurposing,\nnovel target discovery, and explaining mechanisms of bacterial evolution and\nanti-microbial resistance. For drug repurposing, the system proposes candidates\nwith promising validation findings, including candidates for acute myeloid\nleukemia that show tumor inhibition in vitro at clinically applicable\nconcentrations. For novel target discovery, the AI co-scientist proposed new\nepigenetic targets for liver fibrosis, validated by anti-fibrotic activity and\nliver cell regeneration in human hepatic organoids. Finally, the AI\nco-scientist recapitulated unpublished experimental results via a parallel in\nsilico discovery of a novel gene transfer mechanism in bacterial evolution.\nThese results, detailed in separate, co-timed reports, demonstrate the\npotential to augment biomedical and scientific discovery and usher an era of AI\nempowered scientists.",
        "The one-dimensional toy models proposed for the three-dimensional electron\nmagnetohydrodynamics in our previous work share some similarities with the\noriginal dynamics under certain symmetry. We continue to study the\nwell-posedness issue and explore the potential singularity formation scenario\nfor these models.",
        "The Unit Weibull distribution with parameters $\\alpha$ and $\\beta$ is\nconsidered to study in the context of dual generalized order statistics. For\nthe analysis purpose, Bayes estimators based on symmetric and asymmetric loss\nfunctions are obtained. The methods which are utilized for Bayesian estimation\nare approximation and simulation tools such as Lindley, Tierney-Kadane and\nMarkov chain Monte Carlo methods. The authors have considered squared error\nloss function as symmetric and LINEX and general entropy loss function as\nasymmetric loss functions. After presenting the mathematical results, a\nsimulation study is conducted to exhibit the performances of various derived\nestimators. As this study is considered for the dual generalized order\nstatistics that is unification of models based distinct ordered random variable\nsuch as order statistics, record values, etc. This provides flexibility in our\nresults and in continuation of this, the cotton production data of USA is\nanalyzed for both submodels of ordered random variables: order statistics and\nrecord values."
      ]
    }
  },
  {
    "id":2412.20675,
    "research_type":"applied",
    "start_id":"b21",
    "start_title":"LSTM Based Bearing Fault Diagnosis of Electrical Machines using Motor Current Signal",
    "start_abstract":"Rolling element bearings are one of the most critical components rotating machinery, with bearing faults amounting up to 50% in electrical machines. Therefore, fault diagnosis has attracted attention many researchers. Typically, is performed using vibration signals from machine. In addition, by deep learning algorithms on signals, detection accuracy close 100% can be achieved. However, measurement requires an additional sensor, which not present majority Nevertheless, alternative approach, stator current used for diagnosis. paper emphasizes current. The signal processing signature extraction that buried underneath noise signal. uses Paderborn University damaged dataset, contains data healthy, real inner raceway and outer different severity. For redundant frequencies filtered, then filtered eight features extracted time time-frequency domain wavelet packet decomposition (WPD). Then, these well known algorithm Long Short-Term Memory (LSTM), classification made. LSTM mostly speech recognition due its coherence, but this paper, ability also demonstrated 96%, outperforms perform method developed independent speed loading conditions.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Discriminative feature learning using a multiscale convolutional capsule network from attitude data for fault diagnosis of industrial robots"
      ],
      "abstract":[
        "Effective fault diagnosis is important to ensure the reliability, safety, and efficiency of industrial robots. This article proposes a simple yet effective data acquisition strategy based on transmission mechanism analysis, using only one attitude sensor mounted on an end effector or an output component to monitor the attitude of all transmission components. Unlike widely used vibration-monitoring signals, attitude signals can provide fault features reflecting spatial relationships. Using one attitude sensor facilitates the data collection, but weakens fault features and introduces strong background noise in attitude signals. To learn discriminative features from the attitude data collected by the attitude sensor, a multiscale convolutional capsule network (MCCN) is proposed. In MCCN, integrating low-level and high-level features in a convolutional neural network (CNN) as multiscale features is conductive to noise reduction and robust feature extraction, and a capsule network (CapsNet) is used to recognize the spatial relationships in attitude data. The extracted multiscale features in CNN and the spatial-relational features in CapsNet are fused for effective fault diagnosis of industrial robots. The performance of MCCN is evaluated by attaching a softmax-based classifier and integrating it into different transfer learning frameworks to diagnose faults in industrial robots under single and variable working conditions, respectively. Fault diagnosis experiments were conducted on a 6-axis series industrial robot and a parallel robot-driven 3D printer. The superiority of the proposed MCCN was demonstrated by comparing its performance with the other feature learning methods."
      ],
      "categories":[
        "eess.SP"
      ]
    },
    "list":{
      "title":[
        "Love numbers beyond GR from the modified Teukolsky equation",
        "Formation of Be star decretion discs through boundary layer effects",
        "Action representability in categories of unitary algebras",
        "Impact of the Pandemic on Currency Circulation in Brazil: Projections\n  using the SARIMA Model",
        "A precise asymptotic analysis of learning diffusion models: theory and\n  insights",
        "PINK: physical-informed machine learning for lattice thermal\n  conductivity",
        "Search for doubly charmed dibaryons in baryon-baryon scattering",
        "Nonlinear Nanophotonics for High-Dimensional Quantum States",
        "From discrete-time policies to continuous-time diffusion samplers:\n  Asymptotic equivalences and faster training",
        "Linear-Quadratic Optimal Control for Mean-Field Stochastic Differential\n  Equations in Infinite-Horizon with Regime Switching",
        "Pair correlations of one-dimensional model sets and monstrous\n  covariograms of Rauzy fractals",
        "A population synthesis study of the Gaia 100 pc unresolved white\n  dwarf-main sequence binary population",
        "Max-Min Fairness for IRS-Assisted Secure Two-Way Communications",
        "On Coordinated Drone-Courier Logistics for Intra-city Express Services",
        "Extinction and Extirpation Conditions in Coalescent and Ecotonal\n  Metacommunities",
        "The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model",
        "Robust design of bicycle infrastructure networks",
        "Single-Source Localization as an Eigenvalue Problem",
        "Simulating Raman Scattering Impairments with Depolarization Noise in\n  Quantum-Classical Links",
        "Quantized Analog Beamforming Enabled Multi-task Federated Learning\n  Over-the-air",
        "Incentive-Compatible Recovery from Manipulated Signals, with\n  Applications to Decentralized Physical Infrastructure",
        "MDCrow: Automating Molecular Dynamics Workflows with Large Language\n  Models",
        "Turbulent-like flows in quasi two-dimensional dense suspensions of\n  motile colloids",
        "Vanishing theorems for Hodge numbers and the Calabi curvature operator",
        "GPU-accelerated LISA parameter estimation with full time domain response",
        "On the number of cofinalities of cuts in ultraproducts of linear orders",
        "Towards an AI co-scientist",
        "Well-posedness and blowup of 1D electron magnetohydrodynamics",
        "Bayesian estimation of Unit-Weibull distribution based on dual\n  generalized order statistics with application to the Cotton Production Data"
      ],
      "abstract":[
        "We obtain the full set of tidal Love numbers of non-rotating black holes in\nan effective field theory extension of general relativity. We achieve our\nresults using a recently introduced modified Teukolsky equation that describes\nthe perturbations of black holes in this theory. We show how to identify the\nLove numbers and their beta functions in a systematic and gauge invariant way,\napplying analytic continuation on the angular number $\\ell$ when necessary. We\nobserve that there are three types of Love numbers: electric, magnetic, and a\n``mixing'' type, associated to parity-breaking theories, that we identify here\nfor the first time. The modified Teukolsky equation proves to be very useful as\nit allows us to obtain all the different Love numbers in a unified framework.\nWe compare our results with previous literature that utilized the\nRegge-Wheeler-Zerilli equations to compute Love numbers, finding perfect\nagreement. The method introduced here paves the way towards the computation of\nLove numbers of rotating black holes beyond general relativity.",
        "Be stars are rapidly rotating, with angular frequency around $0.7-0.8$ of\ntheir Keplerian break up frequency, as a result of significant accretion during\nthe earlier stellar evolution of a companion star. Material from the equator of\nthe Be star is ejected and forms a decretion disc, although the mechanism for\nthe disc formation has remained elusive. We find one-dimensional steady state\ndecretion disc solutions that smoothly transition from a rapidly rotating star\nthat is in hydrostatic balance. Boundary layer effects in a geometrically thick\ndisc which connects to a rotationally flattened star enable the formation of a\ndecretion disc at stellar spin rates below the break up rate. For a disc with\nan aspect ratio $H\/R\\approx 0.1$ at the inner edge, the torque from the disc on\nthe star slows the stellar spin to the observed range and mass ejection\ncontinues at a rate consistent with observed decretion rates. The critical\nrotation rate, to which the star slows down to, decreases as the disc aspect\nratio increases. More generally, steady state accretion and decretion disc\nsolutions can be found for all stellar spin rates. The outcome for a particular\nsystem depends upon the balance between the decretion rate and any external\ninfall accretion rate.",
        "In a recent article [13], G. Janelidze introduced the concept of ideally\nexact categories as a generalization of semi-abelian categories, aiming to\nincorporate relevant examples of non-pointed categories, such as the categories\n$\\textbf{Ring}$ and $\\textbf{CRing}$ of unitary (commutative) rings. He also\nextended the notion of action representability to this broader framework,\nproving that both $\\textbf{Ring}$ and $\\textbf{CRing}$ are action\nrepresentable.\n  This article investigates the representability of actions of unitary\nnon-associative algebras. After providing a detailed description of the monadic\nadjunction associated with any category of unitary algebra, we use the\nconstruction of the external weak actor [4] in order to prove that the\ncategories of unitary (commutative) associative algebras and that of unitary\nalternative algebras are action representable. The result is then extended for\nunitary (commutative) Poisson algebras, where the explicit construction of the\nuniversal strict general actor is employed.",
        "This study analyzes the impact of the COVID-19 pandemic on currency\ncirculation in Brazil by comparing actual data from 2000 to 2023 with\ncounterfactual projections using the\n\\textbf{SARIMA(3,1,1)(3,1,4)\\textsubscript{12}} model. The model was selected\nbased on an extensive parameter search, balancing accuracy and simplicity, and\nvalidated through the metrics MAPE, RMSE, and AIC. The results indicate a\nsignificant deviation between projected and observed values, with an average\ndifference of R\\$ 47.57 billion (13.95\\%). This suggests that the pandemic,\nemergency policies, and the introduction of \\textit{Pix} had a substantial\nimpact on currency circulation. The robustness of the SARIMA model was\nconfirmed, effectively capturing historical trends and seasonality, though\nfindings emphasize the importance of considering exogenous variables, such as\ninterest rates and macroeconomic policies, in future analyses. Future research\nshould explore multivariate models incorporating economic indicators, long-term\nanalysis of post-pandemic currency circulation trends, and studies on public\ncash-holding behavior. The results reinforce the need for continuous monitoring\nand econometric modeling to support decision-making in uncertain economic\ncontexts.",
        "In this manuscript, we consider the problem of learning a flow or\ndiffusion-based generative model parametrized by a two-layer auto-encoder,\ntrained with online stochastic gradient descent, on a high-dimensional target\ndensity with an underlying low-dimensional manifold structure. We derive a\ntight asymptotic characterization of low-dimensional projections of the\ndistribution of samples generated by the learned model, ascertaining in\nparticular its dependence on the number of training samples. Building on this\nanalysis, we discuss how mode collapse can arise, and lead to model collapse\nwhen the generative model is re-trained on generated synthetic data.",
        "Lattice thermal conductivity ($\\kappa_L$) is crucial for efficient thermal\nmanagement in electronics and energy conversion technologies. Traditional\nmethods for predicting \\k{appa}L are often computationally expensive, limiting\ntheir scalability for large-scale material screening. Empirical models, such as\nthe Slack model, offer faster alternatives but require time-consuming\ncalculations for key parameters such as sound velocity and the Gruneisen\nparameter. This work presents a high-throughput framework, physical-informed\nkappa (PINK), which combines the predictive power of crystal graph\nconvolutional neural networks (CGCNNs) with the physical interpretability of\nthe Slack model to predict \\k{appa}L directly from crystallographic information\nfiles (CIFs). Unlike previous approaches, PINK enables rapid, batch predictions\nby extracting material properties such as bulk and shear modulus from CIFs\nusing a well-trained CGCNN model. These properties are then used to compute the\nnecessary parameters for $\\kappa_L$ calculation through a simplified physical\nformula. PINK was applied to a dataset of 377,221 stable materials, enabling\nthe efficient identification of promising candidates with ultralow $\\kappa_L$\nvalues, such as Ag$_3$Te$_4$W and Ag$_3$Te$_4$Ta. The platform, accessible via\na user-friendly interface, offers an unprecedented combination of speed,\naccuracy, and scalability, significantly accelerating material discovery for\nthermal management and energy conversion applications.",
        "We perform a systematical investigation of the doubly charmed dibaryon system\nwith quantum numbers $IJ=01$, and strangeness numbers $S=0$, $-2$ and $-4$ in\nthe framework of the chiral quark model. Two resonance states with strangeness\nnumbers $S=-2$ is obtained in the $\\Lambda\\Omega_{cc}$ scattering channel,\nwhich are $\\Xi_{cc}^{\\ast}\\Xi$ with resonance mass 5081 MeV and decay width 0.3\nMeV, and the $\\Xi_{c}\\Xi_{c}^{\\ast}$ state with the mass 5213 MeV and decay\nwidth 19.8 MeV, respectively. These two predicted charmed dibaryon candidates\nare worth searching for experimentally. Besides, we would like to emphasize\nthat the multi-channel coupling calculation is important to confirm the\nexistence of multiquark states. The coupling can shift the energy of the\nresonance, give the width to the resonance and even destroy the resonance.\nTherefore, to provide the necessary information for experiments to search for\nexotic hadron states, the coupling calculation between the bound channels and\nopen channels is indispensable.",
        "Quantum nanophotonics merges the precision of nanoscale light manipulation\nwith the capabilities of quantum technologies, offering a pathway for enhanced\nlight-matter interaction and compact realization of quantum devices. Here, we\nshow how a recently-demonstrated nonlinear nanophotonic process can be employed\nto selectively create photonic high-dimensional quantum states (qudits). We\nutilize the nonlinearity on the surface of the nanophotonic device to dress,\nthrough the polarization of the pump field, the near-field modes carrying\nangular momentum and their superpositions. We then use this approach for the\nrealization of a multilevel quantum key distribution protocol, which doubles\nthe key rate compared to standard schemes. This idea is an important step\ntowards experimental realizations of quantum state generation and manipulation\nthrough nonlinearity within nanophotonic platforms, and enables new\ncapabilities for on-chip quantum devices.",
        "We study the problem of training neural stochastic differential equations, or\ndiffusion models, to sample from a Boltzmann distribution without access to\ntarget samples. Existing methods for training such models enforce time-reversal\nof the generative and noising processes, using either differentiable simulation\nor off-policy reinforcement learning (RL). We prove equivalences between\nfamilies of objectives in the limit of infinitesimal discretization steps,\nlinking entropic RL methods (GFlowNets) with continuous-time objects (partial\ndifferential equations and path space measures). We further show that an\nappropriate choice of coarse time discretization during training allows greatly\nimproved sample efficiency and the use of time-local objectives, achieving\ncompetitive performance on standard sampling benchmarks with reduced\ncomputational cost.",
        "This paper is concerned with stochastic linear quadratic (LQ, for short)\noptimal control problems in an infinite horizon with conditional mean-field\nterm in a switching regime environment. The orthogonal decomposition introduced\nin [21] has been adopted. Desired algebraic Riccati equations (AREs, for short)\nand a system of backward stochastic differential equations (BSDEs, for short)\nin infinite time horizon with the coefficients depending on the Markov chain\nhave been derived. The determination of closed-loop optimal strategy follows\nfrom the solvability of ARE and BSDE. Moreover, the solvability of BSDEs leads\nto a characterization of open-loop solvability of the optimal control problem.",
        "The averaged distance structure of one-dimensional regular model sets is\ndetermined via their pair correlation functions. The latter lead to\ncovariograms and cross covariograms of the windows, which give continuous\nfunctions in internal space. While they are simple tent-shaped, piecewise\nlinear functions for intervals, the typical case for inflation systems leads to\nconvolutions of Rauzy fractals, which are difficult to compute. In the presence\nof an inflation structure, an alternative path is possible via the exact\nrenormalisation structures of the pair correlation functions. We introduce this\napproach and derive two concrete examples, which display an unexpectedly\ncomplex and wild behaviour.",
        "Binary stars consisting of a white dwarf and a main sequence star (WDMS) are\nvaluable for studying key astrophysical questions. However, observational\nbiases strongly affect the known population, particularly unresolved systems\nwhere the main sequence star outshines the white dwarf. This work aims to\ncomprehensively simulate the population of unresolved WDMS binaries within 100\npc of the Sun and to compare the outcome with the currently most complete\nvolume-limited sample available from Gaia data. We employ a population\nsynthesis code, MRBIN, extensively developed by our group and based on Monte\nCarlo techniques, which uses a standard binary stellar evolutionary code\nadapted to cover a wide range of stars across all ages, masses, and\nmetallicities. Selection criteria matching those of Gaia observations are\napplied to generate synthetic populations comparable to the observed WDMS\nsample. The synthetic data accurately populate the expected regions in the Gaia\ncolor-magnitude diagram. However, simulations predict a lower number of\nextremely low-mass white dwarfs, suggesting potential issues in observed mass\nderivations. Additionally, our analysis constrains the common envelope\nefficiency to 0.1-0.4, consistent with previous findings, and estimates a total\ncompleteness of about 25% for the observed sample, confirming the strong\nobservational limitations for unresolved WDMS.",
        "This paper investigates an intelligent reflective surface (IRS) assisted\nsecure multi-user two-way communication system. The aim of this paper is to\nenhance the physical layer security by optimizing the minimum secrecy-rate\namong all user-pairs in the presence of a malicious user. The optimization\nproblem is converted into an alternating optimization problem consisting of two\nsub-problems. Transmit power optimization is handled using a fractional\nprogramming method, whereas IRS phase shift optimization is handled with\nsemi-definite programming. The convergence of the proposed algorithm is\ninvestigated numerically. The performance gain in minimum secrecy-rate is\nquantified for four different user configurations in comparison to the baseline\nscheme. Results indicate a 3.6-fold gain in minimum secrecy rate over the\nbaseline scheme when the IRS is positioned near a legitimate user, even when\nthe malicious user is located close to the same legitimate user.",
        "Problem definition: Drones, despite being acknowledged as a transformative\nforce in the city logistics sector, are unable to execute the\n\\textit{last-meter delivery} (unloading goods directly to customers' doorsteps)\ndue to airspace restrictions and safety concerns. To leverage advancements and\novercome the limitations of drones in providing intra-city express services, we\nintroduce a coordinated drone-courier logistics system where drones operate\nwithin a closed network among vertiports, while couriers connect customers to\nthe drone delivery system. This paper aims to shed light on this coordinated\nsystem in terms of system feasibility, network interactivity, and long-term\nsustainability. Methodology\/Results: We develop an integrated optimization\nmodel to optimize the network planning of the coordinated logistics system. The\ninterplay between network planning and tactical operations is mirrored by a\nqueueing network model, resulting in the nonlinear and nonconvex (partially\nconvex and partially concave) feasible region of the optimization model. An\niterative exact algorithm that tightens lower and upper bounds by adaptively\nrefining the linear approximations of nonlinear constraints is developed to\nprovide optimality-guaranteed solutions with finite convergence. The\ncomputational experiments demonstrate the scalability and robustness of our\nalgorithm across various network configurations and scenarios.Managerial\nimplications: The case study, based on a real-world dataset from SF Express, a\nlogistics giant in China, validates that the coordinated logistics system\nefficiently attains cost and time savings by leveraging the effective turnover\nof drones and the coordination between drones and couriers. The optimal network\ndesign features a concentrated structure, streamlining demand consolidation and\nreducing deadhead repositioning.",
        "Here we present extinction, extirpation and coexistence conditions where \/\nwhen two communities combine. We consider one specific model where two\ncommunities coalesce, and another model where the communities coexist side by\nside, blending in a transitionary zone called the ecotone. Specifically, (1) we\nanalytically calculate the shifts in abundances as a function of mixing\nstrength. (2) Obtain a critical value for the mixing strength leading to\nextinction. (3) Derive an inequality condition for full coexistent mixing. (4)\nfind how the individual communities penetrate into one other as a function of\nmixing strength. (5) derive the conditions for one species to cross the ecotone\nand invade an neighboring community and (6) conditions for a native species to\nget extirpated. Lastly, (7) we spatially investigate the species richness\nwithin the ecotone and derive a condition that determines whether the ecotone\nwill have higher or lower richness compared to its surrounding habitats.",
        "Self-distillation (SD), a technique where a model refines itself from its own\npredictions, has garnered attention as a simple yet powerful approach in\nmachine learning. Despite its widespread use, the mechanisms underlying its\neffectiveness remain unclear. In this study, we investigate the efficacy of\nhyperparameter-tuned multi-stage SD in binary classification tasks with noisy\nlabeled Gaussian mixture data, utilizing a replica theory. Our findings reveals\nthat the primary driver of SD's performance improvement is denoising through\nhard pseudo-labels, with the most notable gains observed in moderately sized\ndatasets. We also demonstrate the efficacy of practical heuristics, such as\nearly stopping for extracting meaningful signal and bias fixation for\nimbalanced data. These results provide both theoretical guarantees and\npractical insights, advancing our understanding and application of SD in noisy\nsettings.",
        "Promoting active mobility like cycling relies on the availability of\nwell-connected, high-quality bicycle networks. However, expanding these\nnetworks over an extended planning horizon presents one of the most complex\nchallenges in transport science. This complexity arises from the intricate\ninteractions between infrastructure availability and usage, such as network\nspillover effects and mode choice substitutions. In this paper, we approach the\nproblem from two perspectives: direct optimization methods, which generate\nnear-optimal solutions using operations research techniques, and conceptual\nheuristics, which offer intuitive and scalable algorithms grounded in network\nscience. Specifically, we compare direct welfare optimization with an inverse\nnetwork percolation approach to planning cycle superhighway extensions in\nCopenhagen. Interestingly, while the more complex optimization models yield\nbetter overall welfare results, the improvements over simpler methods are\nsmall. More importantly, we demonstrate that the increased complexity of\nplanning approaches generally makes them more vulnerable to input uncertainty,\nreflecting the bias-variance tradeoff. This issue is particularly relevant in\nthe context of long-term planning, where conditions change during the\nimplementation of the planned infrastructure expansions. Therefore, while\nplanning bicycle infrastructure is important and renders exceptionally high\nbenefit-cost ratios, considerations of robustness and ease of implementation\nmay justify the use of more straightforward network-based methods.",
        "This paper introduces a novel method for solving the single-source\nlocalization problem, specifically addressing the case of trilateration. We\nformulate the problem as a weighted least-squares problem in the squared\ndistances and demonstrate how suitable weights are chosen to accommodate\ndifferent noise distributions. By transforming this formulation into an\neigenvalue problem, we leverage existing eigensolvers to achieve a fast,\nnumerically stable, and easily implemented solver. Furthermore, our theoretical\nanalysis establishes that the globally optimal solution corresponds to the\nlargest real eigenvalue, drawing parallels to the existing literature on the\ntrust-region subproblem. Unlike previous works, we give special treatment to\ndegenerate cases, where multiple and possibly infinitely many solutions exist.\nWe provide a geometric interpretation of the solution sets and design the\nproposed method to handle these cases gracefully. Finally, we validate against\na range of state-of-the-art methods using synthetic and real data,\ndemonstrating how the proposed method is among the fastest and most numerically\nstable.",
        "We model spontaneous Raman scattering noise in polarization-encoded quantum\ncommunication channels co-propagating with classical signals using the\ndepolarization channel. Utilizing NetSquid simulations, we validate the model\nagainst demonstrations of qubit transmission, entanglement distribution, and\nteleportation.",
        "Over-the-air computation (AirComp) has recently emerged as a pivotal\ntechnique for communication-efficient federated learning (FL) in\nresource-constrained wireless networks. Though AirComp leverages the\nsuperposition property of multiple access channels for computation, it\ninherently limits its ability to manage inter-task interference in multi-task\ncomputing. In this paper, we propose a quantized analog beamforming scheme at\nthe receiver to enable simultaneous multi-task FL. Specifically, inspiring by\nthe favorable propagation and channel hardening properties of large-scale\nantenna arrays, a targeted analog beamforming method in closed form is proposed\nfor statistical interference elimination. Analytical results reveal that the\ninterference power vanishes by an order of $\\mathcal{O}\\left(1\/N_r\\right)$ with\nthe number of analog phase shifters, $N_r$, irrespective of their quantization\nprecision. Numerical results demonstrate the effectiveness of the proposed\nanalog beamforming method and show that the performance upper bound of ideal\nlearning without errors can be achieved by increasing the number of\nlow-precision analog phase shifters.",
        "We introduce the first formal model capturing the elicitation of unverifiable\ninformation from a party (the \"source\") with implicit signals derived by other\nplayers (the \"observers\"). Our model is motivated in part by applications in\ndecentralized physical infrastructure networks (a.k.a. \"DePIN\"), an emerging\napplication domain in which physical services (e.g., sensor information,\nbandwidth, or energy) are provided at least in part by untrusted and\nself-interested parties. A key challenge in these signal network applications\nis verifying the level of service that was actually provided by network\nparticipants.\n  We first establish a condition called source identifiability, which we show\nis necessary for the existence of a mechanism for which truthful signal\nreporting is a strict equilibrium. For a converse, we build on techniques from\npeer prediction to show that in every signal network that satisfies the source\nidentifiability condition, there is in fact a strictly truthful mechanism,\nwhere truthful signal reporting gives strictly higher total expected payoff\nthan any less informative equilibrium. We furthermore show that this truthful\nequilibrium is in fact the unique equilibrium of the mechanism if there is\npositive probability that any one observer is unconditionally honest (e.g., if\nan observer were run by the network owner). Also, by extending our condition to\ncoalitions, we show that there are generally no collusion-resistant mechanisms\nin the settings that we consider.\n  We apply our framework and results to two DePIN applications: proving\nlocation, and proving bandwidth. In the location-proving setting observers\nlearn (potentially enlarged) Euclidean distances to the source. Here, our\ncondition has an appealing geometric interpretation, implying that the source's\nlocation can be truthfully elicited if and only if it is guaranteed to lie\ninside the convex hull of the observers.",
        "Molecular dynamics (MD) simulations are essential for understanding\nbiomolecular systems but remain challenging to automate. Recent advances in\nlarge language models (LLM) have demonstrated success in automating complex\nscientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an\nagentic LLM assistant capable of automating MD workflows. MDCrow uses\nchain-of-thought over 40 expert-designed tools for handling and processing\nfiles, setting up simulations, analyzing the simulation outputs, and retrieving\nrelevant information from literature and databases. We assess MDCrow's\nperformance across 25 tasks of varying required subtasks and difficulty, and we\nevaluate the agent's robustness to both difficulty and prompt style.\n\\texttt{gpt-4o} is able to complete complex tasks with low variance, followed\nclosely by \\texttt{llama3-405b}, a compelling open-source model. While prompt\nstyle does not influence the best models' performance, it has significant\neffects on smaller models.",
        "Dense bacterial suspensions exhibit turbulent-like flows at low Reynolds\nnumbers, driven by the activity of the microswimmers. In this study, we develop\na model system to examine these dynamics using motile colloids that mimic\nbacterial locomotion. The colloids are powered by the Quincke instability,\nwhich causes them to spontaneously roll in a random-walk pattern when exposed\nto a square-wave electric field. We experimentally investigate the flow\ndynamics in dense suspensions of these Quincke random walkers under quasi\ntwo-dimensional conditions, where the particle size is comparable to the gap\nbetween the electrodes. Our results reveal an energy spectrum scaling at high\nwavenumbers as $ \\sim k^{-4}$, which holds across a broad range of activity\nlevels -- controlled by the field strength -- and particle concentrations. We\nobserve that velocity time correlations decay within a single period of the\nsquare-wave field, yet an anti-correlation appears between successive field\napplications, indicative of a dynamic structural memory of the ensemble.",
        "It is shown that a compact $n$-dimensional K\\\"ahler manifold with\n$\\frac{n}{2}$-positive Calabi curvature operator has the rational cohomology of\ncomplex projective space. For even $n,$ this is sharp in the sense that the\ncomplex quadric with its symmetric metric has $\\frac{n}{2}$-nonnegative Calabi\ncurvature operator, yet $b_n =2.$ Furthermore, the compact K\\\"ahler manifolds\nwith $\\frac{n}{2}$-nonnegative Calabi curvature operator are completely\nclassified. In addition, the previously known results for the K\\\"ahler\ncurvature operator are improved when the metric is K\\\"ahler-Einstein.",
        "We conduct the first full Bayesian inference analysis for LISA parameter\nestimation incorporating the effects of subdominant harmonics and\nspin-precession through a full time domain response. The substantial\ncomputational demands of using time domain waveforms for LISA are significantly\nmitigated by implementing a novel Python version of the IMRPhenomT family of\nwaveform models and the LISA response with GPU acceleration. This time domain\nresponse alleviates the theoretical necessity of developing specific transfer\nfunctions to approximate the LISA response in the Fourier domain for each\nspecific type of system and allows for the use of unequal arms configurations\nand realistic LISA orbits. Our analysis includes a series of zero-noise\ninjections for a Massive Black Hole Binary with aligned and precessing spins.\nWe investigate the impact of including subdominant harmonics, compare equal and\nunequal arm configurations, and analyze different Time-Delay-Interferometry\n(TDI) configurations. We utilize full and uniform priors, with a lower\nfrequency cutoff of 0.1mHz, and a signal duration of approximately two months,\nsampled every 5 seconds. The sampler is initialized based on Fisher estimates.\nOur results demonstrate LISA capability to measure the two spin magnitudes and\nthe primary spin tilt angle, alongside sky localization, with percent-level\nprecision, while component masses are determined with sub-percent accuracy.",
        "Suppose $\\kappa$ is a regular cardinal and $\\bar a=\\langle \\mu_i: i<\\kappa\n\\rangle$ is a non-decreasing sequence of regular cardinals. We study the set of\npossible cofinalities of cuts Pcut$(\\bar a)=\\{(\\lambda_1, \\lambda_2):$ for some\nultrafilter $D$ on $\\kappa$, $(\\lambda_1, \\lambda_2)$ is the cofinality of a\ncut of $\\prod\\limits_{i<\\kappa} \\mu_i \/ D \\}$.",
        "Scientific discovery relies on scientists generating novel hypotheses that\nundergo rigorous experimental validation. To augment this process, we introduce\nan AI co-scientist, a multi-agent system built on Gemini 2.0. The AI\nco-scientist is intended to help uncover new, original knowledge and to\nformulate demonstrably novel research hypotheses and proposals, building upon\nprior evidence and aligned to scientist-provided research objectives and\nguidance. The system's design incorporates a generate, debate, and evolve\napproach to hypothesis generation, inspired by the scientific method and\naccelerated by scaling test-time compute. Key contributions include: (1) a\nmulti-agent architecture with an asynchronous task execution framework for\nflexible compute scaling; (2) a tournament evolution process for self-improving\nhypotheses generation. Automated evaluations show continued benefits of\ntest-time compute, improving hypothesis quality. While general purpose, we\nfocus development and validation in three biomedical areas: drug repurposing,\nnovel target discovery, and explaining mechanisms of bacterial evolution and\nanti-microbial resistance. For drug repurposing, the system proposes candidates\nwith promising validation findings, including candidates for acute myeloid\nleukemia that show tumor inhibition in vitro at clinically applicable\nconcentrations. For novel target discovery, the AI co-scientist proposed new\nepigenetic targets for liver fibrosis, validated by anti-fibrotic activity and\nliver cell regeneration in human hepatic organoids. Finally, the AI\nco-scientist recapitulated unpublished experimental results via a parallel in\nsilico discovery of a novel gene transfer mechanism in bacterial evolution.\nThese results, detailed in separate, co-timed reports, demonstrate the\npotential to augment biomedical and scientific discovery and usher an era of AI\nempowered scientists.",
        "The one-dimensional toy models proposed for the three-dimensional electron\nmagnetohydrodynamics in our previous work share some similarities with the\noriginal dynamics under certain symmetry. We continue to study the\nwell-posedness issue and explore the potential singularity formation scenario\nfor these models.",
        "The Unit Weibull distribution with parameters $\\alpha$ and $\\beta$ is\nconsidered to study in the context of dual generalized order statistics. For\nthe analysis purpose, Bayes estimators based on symmetric and asymmetric loss\nfunctions are obtained. The methods which are utilized for Bayesian estimation\nare approximation and simulation tools such as Lindley, Tierney-Kadane and\nMarkov chain Monte Carlo methods. The authors have considered squared error\nloss function as symmetric and LINEX and general entropy loss function as\nasymmetric loss functions. After presenting the mathematical results, a\nsimulation study is conducted to exhibit the performances of various derived\nestimators. As this study is considered for the dual generalized order\nstatistics that is unification of models based distinct ordered random variable\nsuch as order statistics, record values, etc. This provides flexibility in our\nresults and in continuation of this, the cotton production data of USA is\nanalyzed for both submodels of ordered random variables: order statistics and\nrecord values."
      ]
    }
  },
  {
    "id":2411.10403,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Sparse MRI: The application of compressed sensing for rapid MR imaging",
    "start_abstract":"Abstract The sparsity which is implicit in MR images exploited to significantly undersample k \u2010space. Some such as angiograms are already sparse the pixel representation; other, more complicated have a representation some transform domain\u2013for example, terms of spatial finite\u2010differences or their wavelet coefficients. According recently developed mathematical theory compressed\u2010sensing, with can be recovered from randomly undersampled \u2010space data, provided an appropriate nonlinear recovery scheme used. Intuitively, artifacts due random undersampling add noise\u2010like interference. In domain significant coefficients stand out above A thresholding recover coefficients, effectively recovering image itself. this article, practical incoherent schemes and analyzed by means aliasing Incoherence introduced pseudo\u2010random variable\u2010density phase\u2010encodes. reconstruction performed minimizing \u2113 1 norm transformed image, subject data fidelity constraints. Examples demonstrate improved resolution accelerated acquisition for multislice fast spin\u2010echo brain imaging 3D contrast enhanced angiography. Magn Reson Med, 2007. \u00a9 2007 Wiley\u2010Liss, Inc.",
    "start_categories":[
      "stat.CO",
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Cardiovascular Flow Measurement with Phase-Contrast MR Imaging: Basic Facts and Implementation"
      ],
      "abstract":[
        "Phase-contrast magnetic resonance (MR) imaging is a well-known but undervalued method of obtaining quantitative information on blood flow. Applications this technique in cardiovascular MR are expanding. According to the sequences available, phase-contrast measurement can be performed breath hold or during normal respiration. Prospective as well retrospective gating techniques used. Common errors include mismatched encoding velocity, deviation plane, inadequate temporal resolution, spatial accelerated flow and misregistration, phase offset errors. Flow measurements most precise if plane perpendicular vessel interest set through-plane The sequence should repeated at least once, with high velocity used initially. If peak has estimated, an adapted velocity. overall error comprises prescription that occur image analysis data. With imaging, reduced less than 10%, acceptable level for routine clinical use."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Evaluating Answer Reranking Strategies in Time-sensitive Question\n  Answering",
        "On the Potential Galactic Origin of the Ultra-High-Energy Event\n  KM3-230213A",
        "Plantation Monitoring Using Drone Images: A Dataset and Performance\n  Review",
        "ML-Based Optimum Number of CUDA Streams for the GPU Implementation of\n  the Tridiagonal Partition Method",
        "Exact Decoding of Repetition Code under Circuit Level Noise",
        "Audio-to-Image Encoding for Improved Voice Characteristic Detection\n  Using Deep Convolutional Neural Networks",
        "Context-Enhanced Memory-Refined Transformer for Online Action Detection",
        "AI-assisted design of experiments at the frontiers of computation:\n  methods and new perspectives",
        "BioMamba: Leveraging Spectro-Temporal Embedding in Bidirectional Mamba\n  for Enhanced Biosignal Classification",
        "CDI3D: Cross-guided Dense-view Interpolation for 3D Reconstruction",
        "Quantum teleportation between simulated binary black holes",
        "EDGE: Efficient Data Selection for LLM Agents via Guideline\n  Effectiveness",
        "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay\n  Scoring Capabilities of Multimodal Large Language Models",
        "Definable Obstruction Theory",
        "Towards more accurate $B_{(s)}\\rightarrow\\pi(K)$ and\n  $D_{(s)}\\rightarrow\\pi(K)$ form factors",
        "Faster-Than-Nyquist Equalization with Convolutional Neural Networks",
        "Interplay of Kondo Physics with Incommensurate Charge Density Waves in\n  CeTe$_3$",
        "Rescaled Einstein-Gauss-Bonnet Gravity Inflation",
        "Unsupervised CP-UNet Framework for Denoising DAS Data with Decay Noise",
        "A Neural Symbolic Model for Space Physics",
        "Vibration Analysis and Mitigation in Semiconductor Motion Stages Using\n  DMAIC Methodology- A Case Study",
        "Col-OLHTR: A Novel Framework for Multimodal Online Handwritten Text\n  Recognition",
        "A stable phase-locking-free single beam optical lattice with multiple\n  configurations",
        "Enhanced Min-Sum Decoding of Quantum Codes Using Previous Iteration\n  Dynamics",
        "Scaling Semantic Categories: Investigating the Impact on Vision\n  Transformer Labeling Performance",
        "A general approach to quantum integration of cross sections in\n  high-energy physics",
        "Modeling metaorder impact with a Non-Markovian Zero Intelligence model",
        "Physics, Environment and Environmental Education; Perceptions from\n  trainee Natural Science teachers",
        "Leveraging Chain of Thought towards Empathetic Spoken Dialogue without\n  Corresponding Question-Answering Data"
      ],
      "abstract":[
        "Despite advancements in state-of-the-art models and information retrieval\ntechniques, current systems still struggle to handle temporal information and\nto correctly answer detailed questions about past events. In this paper, we\ninvestigate the impact of temporal characteristics of answers in Question\nAnswering (QA) by exploring several simple answer selection techniques. Our\nfindings emphasize the role of temporal features in selecting the most relevant\nanswers from diachronic document collections and highlight differences between\nexplicit and implicit temporal questions.",
        "The KM3NeT observatory detected the most energetic neutrino candidate ever\nobserved, with an energy between 72 PeV and 2.6 EeV at the 90% confidence\nlevel. The observed neutrino is likely of cosmic origin. In this article, it is\ninvestigated if the neutrino could have been produced within the Milky Way.\nConsidering the low fluxes of the Galactic diffuse emission at these energies,\nthe lack of a nearby potential Galactic particle accelerator in the direction\nof the event and the difficulty to accelerate particles to such high energies\nin Galactic systems, we conclude that if the event is indeed cosmic, it is most\nlikely of extragalactic origin.",
        "Automatic monitoring of tree plantations plays a crucial role in agriculture.\nFlawless monitoring of tree health helps farmers make informed decisions\nregarding their management by taking appropriate action. Use of drone images\nfor automatic plantation monitoring can enhance the accuracy of the monitoring\nprocess, while still being affordable to small farmers in developing countries\nsuch as India. Small, low cost drones equipped with an RGB camera can capture\nhigh-resolution images of agricultural fields, allowing for detailed analysis\nof the well-being of the plantations. Existing methods of automated plantation\nmonitoring are mostly based on satellite images, which are difficult to get for\nthe farmers. We propose an automated system for plantation health monitoring\nusing drone images, which are becoming easier to get for the farmers. We\npropose a dataset of images of trees with three categories: ``Good health\",\n``Stunted\", and ``Dead\". We annotate the dataset using CVAT annotation tool,\nfor use in research purposes. We experiment with different well-known CNN\nmodels to observe their performance on the proposed dataset. The initial low\naccuracy levels show the complexity of the proposed dataset. Further, our study\nrevealed that, depth-wise convolution operation embedded in a deep CNN model,\ncan enhance the performance of the model on drone dataset. Further, we apply\nstate-of-the-art object detection models to identify individual trees to better\nmonitor them automatically.",
        "This paper presents a heuristic for finding the optimum number of CUDA\nstreams by using tools common to the modern AI-oriented approaches and applied\nto the parallel partition algorithm. A time complexity model for the GPU\nrealization of the partition method is built. Further, a refined time\ncomplexity model for the partition algorithm being executed on multiple CUDA\nstreams is formulated. Computational experiments for different SLAE sizes are\nconducted, and the optimum number of CUDA streams for each of them is found\nempirically. Based on the collected data a model for the sum of the times for\nthe non-dominant GPU operations (that take part in the stream overlap) is\nformulated using regression analysis. A fitting non-linear model for the\noverhead time connected with the creation of CUDA streams is created.\nStatistical analysis is done for all the built models. An algorithm for finding\nthe optimum number of CUDA streams is formulated. Using this algorithm,\ntogether with the two models mentioned above, predictions for the optimum\nnumber of CUDA streams are made. Comparing the predicted values with the actual\ndata, the algorithm is deemed to be acceptably good.",
        "Repetition code forms a fundamental basis for quantum error correction\nexperiments. To date, it stands as the sole code that has achieved large\ndistances and extremely low error rates. Its applications span the spectrum of\nevaluating hardware limitations, pinpointing hardware defects, and detecting\nrare events. However, current methods for decoding repetition codes under\ncircuit level noise are suboptimal, leading to inaccurate error correction\nthresholds and introducing additional errors in event detection. In this work,\nwe establish that repetition code under circuit level noise has an exact\nsolution, and we propose an optimal maximum likelihood decoding algorithm\ncalled planar. The algorithm is based on the exact solution of the spin glass\npartition function on planar graphs and has polynomial computational\ncomplexity. Through extensive numerical experiments, we demonstrate that our\nalgorithm uncovers the exact threshold for depolarizing noise and realistic\nsuperconductor SI1000 noise. Furthermore, we apply our method to analyze data\nfrom recent quantum memory experiments conducted by Google Quantum AI,\nrevealing that part of the error floor was attributed to the decoding algorithm\nused by Google. Finally, we implemented the repetition code quantum memory on\nsuperconducting systems with a 72-qubit quantum chip lacking reset gates,\ndemonstrating that even with an unknown error model, the proposed algorithm\nachieves a significantly lower logical error rate than the matching-based\nalgorithm.",
        "This paper introduces a novel audio-to-image encoding framework that\nintegrates multiple dimensions of voice characteristics into a single RGB image\nfor speaker recognition. In this method, the green channel encodes raw audio\ndata, the red channel embeds statistical descriptors of the voice signal\n(including key metrics such as median and mean values for fundamental\nfrequency, spectral centroid, bandwidth, rolloff, zero-crossing rate, MFCCs,\nRMS energy, spectral flatness, spectral contrast, chroma, and harmonic-to-noise\nratio), and the blue channel comprises subframes representing these features in\na spatially organized format. A deep convolutional neural network trained on\nthese composite images achieves 98% accuracy in speaker classification across\ntwo speakers, suggesting that this integrated multi-channel representation can\nprovide a more discriminative input for voice recognition tasks.",
        "Online Action Detection (OAD) detects actions in streaming videos using past\nobservations. State-of-the-art OAD approaches model past observations and their\ninteractions with an anticipated future. The past is encoded using short- and\nlong-term memories to capture immediate and long-range dependencies, while\nanticipation compensates for missing future context. We identify a\ntraining-inference discrepancy in existing OAD methods that hinders learning\neffectiveness. The training uses varying lengths of short-term memory, while\ninference relies on a full-length short-term memory. As a remedy, we propose a\nContext-enhanced Memory-Refined Transformer (CMeRT). CMeRT introduces a\ncontext-enhanced encoder to improve frame representations using additional\nnear-past context. It also features a memory-refined decoder to leverage\nnear-future generation to enhance performance. CMeRT achieves state-of-the-art\nin online detection and anticipation on THUMOS'14, CrossTask, and\nEPIC-Kitchens-100.",
        "Designing the next generation colliders and detectors involves solving\noptimization problems in high-dimensional spaces where the optimal solutions\nmay nest in regions that even a team of expert humans would not explore.\n  Resorting to Artificial Intelligence to assist the experimental design\nintroduces however significant computational challenges in terms of generation\nand processing of the data required to perform such optimizations: from the\nsoftware point of view, differentiable programming makes the exploration of\nsuch spaces with gradient descent feasible; from the hardware point of view,\nthe complexity of the resulting models and their optimization is prohibitive.\nTo scale up to the complexity of the typical HEP collider experiment, a change\nin paradigma is required.\n  In this contribution I will describe the first proofs-of-concept of\ngradient-based optimization of experimental design and implementations in\nneuromorphic hardware architectures, paving the way to more complex challenges.",
        "Biological signals, such as electroencephalograms (EEGs) and\nelectrocardiograms (ECGs), play a pivotal role in numerous clinical practices,\nsuch as diagnosing brain and cardiac arrhythmic diseases. Existing methods for\nbiosignal classification rely on Attention-based frameworks with dense Feed\nForward layers, which lead to inefficient learning, high computational\noverhead, and suboptimal performance. In this work, we introduce BioMamba, a\nSpectro-Temporal Embedding strategy applied to the Bidirectional Mamba\nframework with Sparse Feed Forward layers to enable effective learning of\nbiosignal sequences. By integrating these three key components, BioMamba\neffectively addresses the limitations of existing methods. Extensive\nexperiments demonstrate that BioMamba significantly outperforms\nstate-of-the-art methods with marked improvement in classification performance.\nThe advantages of the proposed BioMamba include (1) Reliability: BioMamba\nconsistently delivers robust results, confirmed across six evaluation metrics.\n(2) Efficiency: We assess both model and training efficiency, the BioMamba\ndemonstrates computational effectiveness by reducing model size and resource\nconsumption compared to existing approaches. (3) Generality: With the capacity\nto effectively classify a diverse set of tasks, BioMamba demonstrates\nadaptability and effectiveness across various domains and applications.",
        "3D object reconstruction from single-view image is a fundamental task in\ncomputer vision with wide-ranging applications. Recent advancements in Large\nReconstruction Models (LRMs) have shown great promise in leveraging multi-view\nimages generated by 2D diffusion models to extract 3D content. However,\nchallenges remain as 2D diffusion models often struggle to produce dense images\nwith strong multi-view consistency, and LRMs tend to amplify these\ninconsistencies during the 3D reconstruction process. Addressing these issues\nis critical for achieving high-quality and efficient 3D reconstruction. In this\npaper, we present CDI3D, a feed-forward framework designed for efficient,\nhigh-quality image-to-3D generation with view interpolation. To tackle the\naforementioned challenges, we propose to integrate 2D diffusion-based view\ninterpolation into the LRM pipeline to enhance the quality and consistency of\nthe generated mesh. Specifically, our approach introduces a Dense View\nInterpolation (DVI) module, which synthesizes interpolated images between main\nviews generated by the 2D diffusion model, effectively densifying the input\nviews with better multi-view consistency. We also design a tilt camera pose\ntrajectory to capture views with different elevations and perspectives.\nSubsequently, we employ a tri-plane-based mesh reconstruction strategy to\nextract robust tokens from these interpolated and original views, enabling the\ngeneration of high-quality 3D meshes with superior texture and geometry.\nExtensive experiments demonstrate that our method significantly outperforms\nprevious state-of-the-art approaches across various benchmarks, producing 3D\ncontent with enhanced texture fidelity and geometric accuracy.",
        "The quantum description of a black hole predicts that quantum information\nhidden behind the event horizon can be teleported outside almost\ninstantaneously. In this work, we demonstrate that a chiral spin-chain model,\nwhich naturally simulates a binary black hole system, can realise this\nteleportation process. Our system captures two essential components of this\nprotocol: Hawking radiation, which generates the necessary entanglement between\nthe black holes, and optimal scrambling, which enables high-fidelity\nteleportation on short timescales. Through numerical simulations, we quantify\nthe key timescales governing the process, including the Page time, radiation\ntime, scrambling time, and butterfly velocity, showing their universal\ndependence on the chiral coupling strength. Our results establish the\nfeasibility of simulating quantum properties of black holes within condensed\nmatter systems, offering an experimentally accessible platform for probing\notherwise inaccessible high-energy phenomena.",
        "Large Language Models (LLMs) have shown remarkable capabilities as AI agents.\nHowever, existing methods for enhancing LLM-agent abilities often lack a focus\non data quality, leading to inefficiencies and suboptimal results in both\nfine-tuning and prompt engineering. To address this issue, we introduce EDGE, a\nnovel approach for identifying informative samples without needing golden\nanswers. We propose the Guideline Effectiveness (GE) metric, which selects\nchallenging samples by measuring the impact of human-provided guidelines in\nmulti-turn interaction tasks. A low GE score indicates that the human expertise\nrequired for a sample is missing from the guideline, making the sample more\ninformative. By selecting samples with low GE scores, we can improve the\nefficiency and outcomes of both prompt engineering and fine-tuning processes\nfor LLMs. Extensive experiments validate the performance of our method. Our\nmethod achieves competitive results on the HotpotQA and WebShop and datasets,\nrequiring 75\\% and 50\\% less data, respectively, while outperforming existing\nmethods. We also provide a fresh perspective on the data quality of LLM-agent\nfine-tuning.",
        "Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research. Our\ndataset and code will be available upon acceptance.",
        "A series of recent papers by Bergfalk, Lupini and Panagiotopoulus developed\nthe foundations of a field known as `definable algebraic topology,' in which\nclassical cohomological invariants are enriched by viewing them as groups with\na Polish cover. This allows one to apply techniques from descriptive set theory\nto the study of cohomology theories. In this paper, we will establish a\n`definable' version of a classical theorem from obstruction theory, and use\nthis to study the potential complexity of the homotopy relation on the space of\ncontinuous maps $C(X, |K|)$, where $X$ is a locally compact Polish space, and K\nis a locally finite countable simplicial complex. We will also characterize the\nSolecki Groups of the Cech cohomology of X, which are the canonical chain of\nsubgroups with a Polish cover that are least among those of a given complexity.",
        "We present progress on the calculation of scalar, vector, and tensor form\nfactors for the following meson decays: $B\\rightarrow\\pi$, $B_s\\rightarrow K$,\n$D\\rightarrow\\pi$ and $D_s\\rightarrow K$. This calculation uses the MILC HISQ\ngluon field ensembles with HISQ valence quarks. We generate ensembles of\ncorrelator data with varying lattice spacings, some as small as 0.044 fm. Some\nensembles have a strange-to-light quark mass ratio of 5:1 and others use the\nphysical light quark mass. The fully-relativistic, heavy-HISQ approach is used\nfor the heavy quark, with simulation masses ranging from the charm to near the\nbottom. This heavy-HISQ approach provides nearly full coverage of the kinematic\nrange.",
        "Faster-than-Nyquist (FTN) signaling aims at improving the spectral efficiency\nof wireless communication systems by exceeding the boundaries set by the\nNyquist-Shannon sampling theorem. 50 years after its first introduction in the\nscientific literature, wireless communications have significantly changed, but\nspectral efficiency remains one of the key challenges. To adopt FTN signaling,\ninter-symbol interference (ISI) patterns need to be equalized at the receiver.\nMotivated by the pattern recognition capabilities of convolutional neural\nnetworks with skip connections, we propose such deep learning architecture for\nISI equalization and symbol demodulation in FTN receivers. We investigate the\nperformance of the proposed model considering quadrature phase shift keying\nmodulation and low density parity check coding, and compare it to a set of\nbenchmarks, including frequency-domain equalization, a\nquadratic-programming-based receiver, and an equalization scheme based on a\ndeep neural network. We show that our receiver outperforms any benchmark,\nachieving error rates comparable to those in additive white Gaussian noise\nchannel, and higher effective throughput, thanks to the increased spectral\nefficiency of FTN signaling. With a compression factor of 60% and code rate\n3\/4, the proposed model achieves a peak effective throughput of 2.5 Mbps at\njust 10dB of energy per bit over noise power spectral density ratio, with other\nreceivers being limited by error floors due to the strong inter-symbol\ninterference. To promote reproducibility in deep learning for wireless\ncommunications, our code is open source at the repository provided in the\nreferences.",
        "CeTe$_3$ is a 2-dimensional (2D) Van der Waals (VdW) material with\nincommensurate charge density waves (CDW), extremely high transition\ntemperature ($T_{CDW}$) and a large momentum-dependent CDW gap that leaves a\nsignificant portion of the Fermi surface intact. It is also considered to be a\nweak Kondo system, a property unexpected for a material with incommensurate\nCDW, where each atomic site is slightly different. Here, we study the\nproperties of the CDW state in several RTe$_3$ (R is rare earth) materials and\nexamine the hybridization of itinerant states with the localized Ce $4f$\nmultiplet in CeTe$_3$ by using angle resolved photoemission spectroscopy\n(ARPES). We find that the renormalization of the itinerant states originating\nfrom the hybridization with the localized $4f$ states at $-260$ meV extends to\nthe Fermi level. This, with remnants of another localized state at the Fermi\nlevel, supports the characterization of CeTe$_3$ as a weak Kondo material.\nFurthermore, we uncover a $k$-dependence of the hybridization with the states\n$-260$ meV, indicating that similar effect could be the reason for discrepancy\nbetween the heavy masses in specific heat and light ones in Shubnikov de Haas\noscillations observed in other heavy fermion materials.",
        "We study the inflationary phenomenology of a rescaled Einstein-Gauss-Bonnet\ngravity. In this framework, the gravitational constant of the Einstein-Hilbert\nterm is rescaled due to effective terms active in the high curvature era.\nBasically, the total theory is an $F(R,G,\\phi)$ theory with the Gauss-Bonnet\npart contributing only a non-minimal coupling to the scalar field, so it is a\ntheory with string theory origins and with a non-trivial $F(R)$ gravity part.\nThe $F(R)$ gravity part in the high curvature regime contributes only a\nrescaled Einstein-Hilbert term and thus the resulting theory is effectively a\nrescaled version of a standard Einstein-Gauss-Bonnet theory. We develop the\nformalism of rescaled Einstein-Gauss-Bonnet gravity, taking in account the\nGW170817 constraints on the gravitational wave speed. We show explicitly how\nthe rescaled theory affects directly the primordial scalar and tensor\nperturbations, and how the slow-roll and observational indices of inflation are\naffected by the rescaling of the theory. We perform a thorough phenomenological\nanalysis of several models of interest and we show that is it possible to\nobtain viable inflationary theories compatible with the latest Planck data.\nAlso among the studied models there are cases that yield a relatively large\nblue tilted tensor spectral index and we demonstrate that these models can lead\nto detectable primordial gravitational waves in the future gravitational wave\nexperiments. Some of the scenarios examined, for specific values of the\nreheating temperature may be detectable by SKA, LISA, BBO, DECIGO and the\nEinstein Telescope.",
        "Distributed acoustic sensor (DAS) technology leverages optical fiber cables\nto detect acoustic signals, providing cost-effective and dense monitoring\ncapabilities. It offers several advantages including resistance to extreme\nconditions, immunity to electromagnetic interference, and accurate detection.\nHowever, DAS typically exhibits a lower signal-to-noise ratio (S\/N) compared to\ngeophones and is susceptible to various noise types, such as random noise,\nerratic noise, level noise, and long-period noise. This reduced S\/N can\nnegatively impact data analyses containing inversion and interpretation. While\nartificial intelligence has demonstrated excellent denoising capabilities, most\nexisting methods rely on supervised learning with labeled data, which imposes\nstringent requirements on the quality of the labels. To address this issue, we\ndevelop a label-free unsupervised learning (UL) network model based on\nContext-Pyramid-UNet (CP-UNet) to suppress erratic and random noises in DAS\ndata. The CP-UNet utilizes the Context Pyramid Module in the encoding and\ndecoding process to extract features and reconstruct the DAS data. To enhance\nthe connectivity between shallow and deep features, we add a Connected Module\n(CM) to both encoding and decoding section. Layer Normalization (LN) is\nutilized to replace the commonly employed Batch Normalization (BN),\naccelerating the convergence of the model and preventing gradient explosion\nduring training. Huber-loss is adopted as our loss function whose parameters\nare experimentally determined. We apply the network to both the 2-D synthetic\nand filed data. Comparing to traditional denoising methods and the latest UL\nframework, our proposed method demonstrates superior noise reduction\nperformance.",
        "In this study, we unveil a new AI model, termed PhyE2E, to discover physical\nformulas through symbolic regression. PhyE2E simplifies symbolic regression by\ndecomposing it into sub-problems using the second-order derivatives of an\noracle neural network, and employs a transformer model to translate data into\nsymbolic formulas in an end-to-end manner. The resulting formulas are refined\nthrough Monte-Carlo Tree Search and Genetic Programming. We leverage a large\nlanguage model to synthesize extensive symbolic expressions resembling real\nphysics, and train the model to recover these formulas directly from data. A\ncomprehensive evaluation reveals that PhyE2E outperforms existing\nstate-of-the-art approaches, delivering superior symbolic accuracy, precision\nin data fitting, and consistency in physical units. We deployed PhyE2E to five\napplications in space physics, including the prediction of sunspot numbers,\nsolar rotational angular velocity, emission line contribution functions,\nnear-Earth plasma pressure, and lunar-tide plasma signals. The physical\nformulas generated by AI demonstrate a high degree of accuracy in fitting the\nexperimental data from satellites and astronomical telescopes. We have\nsuccessfully upgraded the formula proposed by NASA in 1993 regarding solar\nactivity, and for the first time, provided the explanations for the long cycle\nof solar activity in an explicit form. We also found that the decay of\nnear-Earth plasma pressure is proportional to r^2 to Earth, where subsequent\nmathematical derivations are consistent with satellite data from another\nindependent study. Moreover, we found physical formulas that can describe the\nrelationships between emission lines in the extreme ultraviolet spectrum of the\nSun, temperatures, electron densities, and magnetic fields. The formula\nobtained is consistent with the properties that physicists had previously\nhypothesized it should possess.",
        "Motion stages are critical in semiconductor manufacturing equipment for\nprocesses like die bonding, wafer loading, and chip packaging, as their\nperformance must meet the industry's stringent precision requirements.\nVibration, a significant yet often overlooked adversary to precision motion\nstages, is challenging to identify and mitigate due to its subtle nature. This\nstudy, conducted at a motion stage manufacturer facing frequent\nvibration-related complaints, proposes a novel approach to resolving vibration\nissues. By leveraging the DMAIC methodology, it introduces VIBGUARD, an active\nvibration monitoring and mitigation solution, instead of solely focusing on\ntraditional hardware vibration control. This comprehensive strategy enhances\nvalue and competitiveness, increasing UPH (units per hour) by 15.3% from 8,500\nto 9,800 and reducing downtime by 68.2% from 2.2 to 0.7 occurrences per month.\nThis case study and the DMAIC methodology offer valuable resources for quality\ncontrol and problem analysis in the semiconductor industry.",
        "Online Handwritten Text Recognition (OLHTR) has gained considerable attention\nfor its diverse range of applications. Current approaches usually treat OLHTR\nas a sequence recognition task, employing either a single trajectory or image\nencoder, or multi-stream encoders, combined with a CTC or attention-based\nrecognition decoder. However, these approaches face several drawbacks: 1)\nsingle encoders typically focus on either local trajectories or visual regions,\nlacking the ability to dynamically capture relevant global features in\nchallenging cases; 2) multi-stream encoders, while more comprehensive, suffer\nfrom complex structures and increased inference costs. To tackle this, we\npropose a Collaborative learning-based OLHTR framework, called Col-OLHTR, that\nlearns multimodal features during training while maintaining a single-stream\ninference process. Col-OLHTR consists of a trajectory encoder, a\nPoint-to-Spatial Alignment (P2SA) module, and an attention-based decoder. The\nP2SA module is designed to learn image-level spatial features through\ntrajectory-encoded features and 2D rotary position embeddings. During training,\nan additional image-stream encoder-decoder is collaboratively trained to\nprovide supervision for P2SA features. At inference, the extra streams are\ndiscarded, and only the P2SA module is used and merged before the decoder,\nsimplifying the process while preserving high performance. Extensive\nexperimental results on several OLHTR benchmarks demonstrate the\nstate-of-the-art (SOTA) performance, proving the effectiveness and robustness\nof our design.",
        "Optical lattices formed by interfering laser beams are widely used to trap\nand manipulate atoms for quantum simulation, metrology, and computation. To\nstabilize optical lattices in experiments, it is usually challenging to\nimplement delicate phase-locking systems with complicated optics and\nelectronics to reduce the relative phase fluctuation of multiple laser beams.\nHere we report a phase-locking-free scheme to implement optical lattices by\npassing a single laser beam through a prism with n-fold symmetric facets and\nlarge apex angles. The scheme ensures a stable optical lattice since the\ninterference occurs among different deflected parts of a single laser beam\nwithout any moving component. Various lattice configurations, including a\ntriangular lattice and a quasi-crystal lattice with ten-fold symmetry are\ndemonstrated. In both cases, stability measurements show a change of lattice\nconstant in less than 1.14%, and a drift of lattice position in less than\n1.61%.",
        "In this paper, we propose a novel message-passing decoding approach that\nleverages the degeneracy of quantum low-density parity-check codes to enhance\ndecoding performance, eliminating the need for serial scheduling or\npost-processing. Our focus is on two-block Calderbank-Shor-Steane (CSS) codes,\nwhich are composed of symmetric stabilizers that hinder the performance of\nconventional iterative decoders with uniform update rules. Specifically, our\nanalysis shows that, under the isolation assumption, the min-sum decoder fails\nto converge when constant-weight errors are applied to symmetric stabilizers,\nas variable-to-check messages oscillate in every iteration. To address this, we\nintroduce a decoding technique that exploits this oscillatory property by\napplying distinct update rules: variable nodes in one block utilize messages\nfrom previous iterations, while those in the other block are updated\nconventionally. Logical error-rate results demonstrate that the proposed\ndecoder significantly outperforms the normalized min-sum decoder and achieves\ncompetitive performance with belief propagation enhanced by order-zero ordered\nstatistics decoding, all while maintaining linear complexity in the code's\nblock length.",
        "This study explores the impact of scaling semantic categories on the image\nclassification performance of vision transformers (ViTs). In this specific\ncase, the CLIP server provided by Jina AI is used for experimentation. The\nresearch hypothesizes that as the number of ground truth and artificially\nintroduced semantically equivalent categories increases, the labeling accuracy\nof ViTs improves until a theoretical maximum or limit is reached. A wide\nvariety of image datasets were chosen to test this hypothesis. These datasets\nwere processed through a custom function in Python designed to evaluate the\nmodel's accuracy, with adjustments being made to account for format differences\nbetween datasets. By exponentially introducing new redundant categories, the\nexperiment assessed accuracy trends until they plateaued, decreased, or\nfluctuated inconsistently. The findings show that while semantic scaling\ninitially increases model performance, the benefits diminish or reverse after\nsurpassing a critical threshold, providing insight into the limitations and\npossible optimization of category labeling strategies for ViTs.",
        "We present universal \\emph{building blocks} for the quantum integration of\ngeneric cross sections in high-energy physics. We make use of Fourier Quantum\nMonte Carlo Integration as implemented in {\\sc Quantinuum}'s Quantum Monte\nCarlo Integration engine to provide an extendable methodology for generating\nefficient circuits that can implement generic cross-section calculations,\nproviding a quadratic speed-up in root mean-squared error convergence with\nrespect to classical Monte Carlo integration. We focus on a concrete example of\na $1\\to 3$ decay process to illustrate our work.",
        "Devising models of the limit order book that realistically reproduce the\nmarket response to exogenous trades is extremely challenging and fundamental in\norder to test trading strategies. We propose a novel explainable model for\nsmall tick assets, the Non-Markovian Zero Intelligence, which is a variant of\nthe well-known Zero Intelligence model. The main modification is that the\nprobability of limit orders' signs (buy\/sell) is not constant but is a function\nof the exponentially weighted mid-price return, representing the past price\ndynamics, and can be interpreted as the reaction of traders with reservation\nprices to the price trend. With numerical simulations and analytical arguments,\nwe show that the model predicts a concave price path during a metaorder\nexecution and to a price reversion after the execution ends, as empirically\nobserved. We analyze in-depth the mechanism at the root of the arising\nconcavity, the components which constitute the price impact in our model, and\nthe dependence of the results on the two main parameters, namely the time scale\nand the strength of the reaction of traders to the price trend.",
        "Environmental Education (EE) is vital for shaping citizens who understand and\nvalue sustainability as an epistemological and practical alternative to\nmitigate current environmental issues. This research was prompted by the\nexploration of the relationship between EE and the physical sciences,\nconnections that are often overlooked in curriculums and in the teaching\nprocesses of both this science and EE. It is essential to emphasize that\nphysics provides conceptual frameworks and methodological tools that can\nenhance the understanding of environmental phenomena from a broad and\nmultidimensional perspective. To delve into these connections, a study with a\nhermeneutic interpretative nuance was conducted. Through a questionnaire, the\nperceptions of prospective teachers in the natural sciences field regarding\nthis topic were gathered. The findings revealed that a significant number of\nthem recognize and value the correlation between physics and EE. From their\nperspective, this linkage is not only crucial for a comprehensive view of\nenvironmental dynamics but also to encourage students to develop critical,\narticulated, and well-founded thinking about environmental balance. The\nresearch also highlighted the didactic opportunities presented when\nintertwining physics with EE. By associating physical concepts with real\nenvironmental issues, learning can be reinforced, making it meaningful and\nenduring over time. This interdisciplinary fusion also holds the potential to\nincrease students' motivation and interest, fostering a more active and engaged\nattitude in their educational journey",
        "Empathetic dialogue is crucial for natural human-computer interaction,\nallowing the dialogue system to respond in a more personalized and emotionally\naware manner, improving user satisfaction and engagement. The emergence of\nlarge language models (LLMs) has revolutionized dialogue generation by\nharnessing their powerful capabilities and shown its potential in multimodal\ndomains. Many studies have integrated speech with text-based LLMs to take\nspeech question as input and output text response. However, the lack of spoken\nquestion-answering datasets that include speech style information to supervised\nfine-tuning (SFT) limits the performance of these systems. As a result, while\nthese systems excel at understanding speech content, they often struggle to\ngenerate empathetic responses. In response, we propose a novel approach that\ncircumvents the need for question-answering data, called Listen, Perceive, and\nExpress (LPE). Our method employs a two-stage training process, initially\nguiding the LLM to listen the content and perceive the emotional aspects of\nspeech. Subsequently, we utilize Chain-of-Thought (CoT) prompting to unlock the\nmodel's potential for expressing empathetic responses based on listened spoken\ncontent and perceived emotional cues. We employ experiments to prove the\neffectiveness of proposed method. To our knowledge, this is the first attempt\nto leverage CoT for speech-based dialogue."
      ]
    }
  },
  {
    "id":2411.10403,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"MoDL: Model-Based Deep Learning Architecture for Inverse Problems",
    "start_abstract":"We introduce a model-based image reconstruction framework with convolution neural network (CNN)-based regularization prior. The proposed formulation provides systematic approach for deriving deep architectures inverse problems the arbitrary structure. Since forward model is explicitly accounted for, smaller fewer parameters sufficient to capture information compared direct inversion approaches. Thus, reducing demand training data and time. we rely on end-to-end weight sharing across iterations, CNN weights are customized model, thus offering improved performance over approaches that pre-trained denoisers. Our experiments show decoupling of number iterations from complexity offered by this benefits, including lower data, reduced risk overfitting, implementations significantly memory footprint. propose enforce data-consistency using numerical optimization blocks, such as conjugate gradients algorithm within network. This offers faster convergence per iteration, methods proximal steps consistency. translates performance, primarily when available GPU restricts iterations.",
    "start_categories":[
      "stat.CO",
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Cardiovascular Flow Measurement with Phase-Contrast MR Imaging: Basic Facts and Implementation"
      ],
      "abstract":[
        "Phase-contrast magnetic resonance (MR) imaging is a well-known but undervalued method of obtaining quantitative information on blood flow. Applications this technique in cardiovascular MR are expanding. According to the sequences available, phase-contrast measurement can be performed breath hold or during normal respiration. Prospective as well retrospective gating techniques used. Common errors include mismatched encoding velocity, deviation plane, inadequate temporal resolution, spatial accelerated flow and misregistration, phase offset errors. Flow measurements most precise if plane perpendicular vessel interest set through-plane The sequence should repeated at least once, with high velocity used initially. If peak has estimated, an adapted velocity. overall error comprises prescription that occur image analysis data. With imaging, reduced less than 10%, acceptable level for routine clinical use."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Evaluating Answer Reranking Strategies in Time-sensitive Question\n  Answering",
        "On the Potential Galactic Origin of the Ultra-High-Energy Event\n  KM3-230213A",
        "Plantation Monitoring Using Drone Images: A Dataset and Performance\n  Review",
        "ML-Based Optimum Number of CUDA Streams for the GPU Implementation of\n  the Tridiagonal Partition Method",
        "Exact Decoding of Repetition Code under Circuit Level Noise",
        "Audio-to-Image Encoding for Improved Voice Characteristic Detection\n  Using Deep Convolutional Neural Networks",
        "Context-Enhanced Memory-Refined Transformer for Online Action Detection",
        "AI-assisted design of experiments at the frontiers of computation:\n  methods and new perspectives",
        "BioMamba: Leveraging Spectro-Temporal Embedding in Bidirectional Mamba\n  for Enhanced Biosignal Classification",
        "CDI3D: Cross-guided Dense-view Interpolation for 3D Reconstruction",
        "Quantum teleportation between simulated binary black holes",
        "EDGE: Efficient Data Selection for LLM Agents via Guideline\n  Effectiveness",
        "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay\n  Scoring Capabilities of Multimodal Large Language Models",
        "Definable Obstruction Theory",
        "Towards more accurate $B_{(s)}\\rightarrow\\pi(K)$ and\n  $D_{(s)}\\rightarrow\\pi(K)$ form factors",
        "Faster-Than-Nyquist Equalization with Convolutional Neural Networks",
        "Interplay of Kondo Physics with Incommensurate Charge Density Waves in\n  CeTe$_3$",
        "Rescaled Einstein-Gauss-Bonnet Gravity Inflation",
        "Unsupervised CP-UNet Framework for Denoising DAS Data with Decay Noise",
        "A Neural Symbolic Model for Space Physics",
        "Vibration Analysis and Mitigation in Semiconductor Motion Stages Using\n  DMAIC Methodology- A Case Study",
        "Col-OLHTR: A Novel Framework for Multimodal Online Handwritten Text\n  Recognition",
        "A stable phase-locking-free single beam optical lattice with multiple\n  configurations",
        "Enhanced Min-Sum Decoding of Quantum Codes Using Previous Iteration\n  Dynamics",
        "Scaling Semantic Categories: Investigating the Impact on Vision\n  Transformer Labeling Performance",
        "A general approach to quantum integration of cross sections in\n  high-energy physics",
        "Modeling metaorder impact with a Non-Markovian Zero Intelligence model",
        "Physics, Environment and Environmental Education; Perceptions from\n  trainee Natural Science teachers",
        "Leveraging Chain of Thought towards Empathetic Spoken Dialogue without\n  Corresponding Question-Answering Data"
      ],
      "abstract":[
        "Despite advancements in state-of-the-art models and information retrieval\ntechniques, current systems still struggle to handle temporal information and\nto correctly answer detailed questions about past events. In this paper, we\ninvestigate the impact of temporal characteristics of answers in Question\nAnswering (QA) by exploring several simple answer selection techniques. Our\nfindings emphasize the role of temporal features in selecting the most relevant\nanswers from diachronic document collections and highlight differences between\nexplicit and implicit temporal questions.",
        "The KM3NeT observatory detected the most energetic neutrino candidate ever\nobserved, with an energy between 72 PeV and 2.6 EeV at the 90% confidence\nlevel. The observed neutrino is likely of cosmic origin. In this article, it is\ninvestigated if the neutrino could have been produced within the Milky Way.\nConsidering the low fluxes of the Galactic diffuse emission at these energies,\nthe lack of a nearby potential Galactic particle accelerator in the direction\nof the event and the difficulty to accelerate particles to such high energies\nin Galactic systems, we conclude that if the event is indeed cosmic, it is most\nlikely of extragalactic origin.",
        "Automatic monitoring of tree plantations plays a crucial role in agriculture.\nFlawless monitoring of tree health helps farmers make informed decisions\nregarding their management by taking appropriate action. Use of drone images\nfor automatic plantation monitoring can enhance the accuracy of the monitoring\nprocess, while still being affordable to small farmers in developing countries\nsuch as India. Small, low cost drones equipped with an RGB camera can capture\nhigh-resolution images of agricultural fields, allowing for detailed analysis\nof the well-being of the plantations. Existing methods of automated plantation\nmonitoring are mostly based on satellite images, which are difficult to get for\nthe farmers. We propose an automated system for plantation health monitoring\nusing drone images, which are becoming easier to get for the farmers. We\npropose a dataset of images of trees with three categories: ``Good health\",\n``Stunted\", and ``Dead\". We annotate the dataset using CVAT annotation tool,\nfor use in research purposes. We experiment with different well-known CNN\nmodels to observe their performance on the proposed dataset. The initial low\naccuracy levels show the complexity of the proposed dataset. Further, our study\nrevealed that, depth-wise convolution operation embedded in a deep CNN model,\ncan enhance the performance of the model on drone dataset. Further, we apply\nstate-of-the-art object detection models to identify individual trees to better\nmonitor them automatically.",
        "This paper presents a heuristic for finding the optimum number of CUDA\nstreams by using tools common to the modern AI-oriented approaches and applied\nto the parallel partition algorithm. A time complexity model for the GPU\nrealization of the partition method is built. Further, a refined time\ncomplexity model for the partition algorithm being executed on multiple CUDA\nstreams is formulated. Computational experiments for different SLAE sizes are\nconducted, and the optimum number of CUDA streams for each of them is found\nempirically. Based on the collected data a model for the sum of the times for\nthe non-dominant GPU operations (that take part in the stream overlap) is\nformulated using regression analysis. A fitting non-linear model for the\noverhead time connected with the creation of CUDA streams is created.\nStatistical analysis is done for all the built models. An algorithm for finding\nthe optimum number of CUDA streams is formulated. Using this algorithm,\ntogether with the two models mentioned above, predictions for the optimum\nnumber of CUDA streams are made. Comparing the predicted values with the actual\ndata, the algorithm is deemed to be acceptably good.",
        "Repetition code forms a fundamental basis for quantum error correction\nexperiments. To date, it stands as the sole code that has achieved large\ndistances and extremely low error rates. Its applications span the spectrum of\nevaluating hardware limitations, pinpointing hardware defects, and detecting\nrare events. However, current methods for decoding repetition codes under\ncircuit level noise are suboptimal, leading to inaccurate error correction\nthresholds and introducing additional errors in event detection. In this work,\nwe establish that repetition code under circuit level noise has an exact\nsolution, and we propose an optimal maximum likelihood decoding algorithm\ncalled planar. The algorithm is based on the exact solution of the spin glass\npartition function on planar graphs and has polynomial computational\ncomplexity. Through extensive numerical experiments, we demonstrate that our\nalgorithm uncovers the exact threshold for depolarizing noise and realistic\nsuperconductor SI1000 noise. Furthermore, we apply our method to analyze data\nfrom recent quantum memory experiments conducted by Google Quantum AI,\nrevealing that part of the error floor was attributed to the decoding algorithm\nused by Google. Finally, we implemented the repetition code quantum memory on\nsuperconducting systems with a 72-qubit quantum chip lacking reset gates,\ndemonstrating that even with an unknown error model, the proposed algorithm\nachieves a significantly lower logical error rate than the matching-based\nalgorithm.",
        "This paper introduces a novel audio-to-image encoding framework that\nintegrates multiple dimensions of voice characteristics into a single RGB image\nfor speaker recognition. In this method, the green channel encodes raw audio\ndata, the red channel embeds statistical descriptors of the voice signal\n(including key metrics such as median and mean values for fundamental\nfrequency, spectral centroid, bandwidth, rolloff, zero-crossing rate, MFCCs,\nRMS energy, spectral flatness, spectral contrast, chroma, and harmonic-to-noise\nratio), and the blue channel comprises subframes representing these features in\na spatially organized format. A deep convolutional neural network trained on\nthese composite images achieves 98% accuracy in speaker classification across\ntwo speakers, suggesting that this integrated multi-channel representation can\nprovide a more discriminative input for voice recognition tasks.",
        "Online Action Detection (OAD) detects actions in streaming videos using past\nobservations. State-of-the-art OAD approaches model past observations and their\ninteractions with an anticipated future. The past is encoded using short- and\nlong-term memories to capture immediate and long-range dependencies, while\nanticipation compensates for missing future context. We identify a\ntraining-inference discrepancy in existing OAD methods that hinders learning\neffectiveness. The training uses varying lengths of short-term memory, while\ninference relies on a full-length short-term memory. As a remedy, we propose a\nContext-enhanced Memory-Refined Transformer (CMeRT). CMeRT introduces a\ncontext-enhanced encoder to improve frame representations using additional\nnear-past context. It also features a memory-refined decoder to leverage\nnear-future generation to enhance performance. CMeRT achieves state-of-the-art\nin online detection and anticipation on THUMOS'14, CrossTask, and\nEPIC-Kitchens-100.",
        "Designing the next generation colliders and detectors involves solving\noptimization problems in high-dimensional spaces where the optimal solutions\nmay nest in regions that even a team of expert humans would not explore.\n  Resorting to Artificial Intelligence to assist the experimental design\nintroduces however significant computational challenges in terms of generation\nand processing of the data required to perform such optimizations: from the\nsoftware point of view, differentiable programming makes the exploration of\nsuch spaces with gradient descent feasible; from the hardware point of view,\nthe complexity of the resulting models and their optimization is prohibitive.\nTo scale up to the complexity of the typical HEP collider experiment, a change\nin paradigma is required.\n  In this contribution I will describe the first proofs-of-concept of\ngradient-based optimization of experimental design and implementations in\nneuromorphic hardware architectures, paving the way to more complex challenges.",
        "Biological signals, such as electroencephalograms (EEGs) and\nelectrocardiograms (ECGs), play a pivotal role in numerous clinical practices,\nsuch as diagnosing brain and cardiac arrhythmic diseases. Existing methods for\nbiosignal classification rely on Attention-based frameworks with dense Feed\nForward layers, which lead to inefficient learning, high computational\noverhead, and suboptimal performance. In this work, we introduce BioMamba, a\nSpectro-Temporal Embedding strategy applied to the Bidirectional Mamba\nframework with Sparse Feed Forward layers to enable effective learning of\nbiosignal sequences. By integrating these three key components, BioMamba\neffectively addresses the limitations of existing methods. Extensive\nexperiments demonstrate that BioMamba significantly outperforms\nstate-of-the-art methods with marked improvement in classification performance.\nThe advantages of the proposed BioMamba include (1) Reliability: BioMamba\nconsistently delivers robust results, confirmed across six evaluation metrics.\n(2) Efficiency: We assess both model and training efficiency, the BioMamba\ndemonstrates computational effectiveness by reducing model size and resource\nconsumption compared to existing approaches. (3) Generality: With the capacity\nto effectively classify a diverse set of tasks, BioMamba demonstrates\nadaptability and effectiveness across various domains and applications.",
        "3D object reconstruction from single-view image is a fundamental task in\ncomputer vision with wide-ranging applications. Recent advancements in Large\nReconstruction Models (LRMs) have shown great promise in leveraging multi-view\nimages generated by 2D diffusion models to extract 3D content. However,\nchallenges remain as 2D diffusion models often struggle to produce dense images\nwith strong multi-view consistency, and LRMs tend to amplify these\ninconsistencies during the 3D reconstruction process. Addressing these issues\nis critical for achieving high-quality and efficient 3D reconstruction. In this\npaper, we present CDI3D, a feed-forward framework designed for efficient,\nhigh-quality image-to-3D generation with view interpolation. To tackle the\naforementioned challenges, we propose to integrate 2D diffusion-based view\ninterpolation into the LRM pipeline to enhance the quality and consistency of\nthe generated mesh. Specifically, our approach introduces a Dense View\nInterpolation (DVI) module, which synthesizes interpolated images between main\nviews generated by the 2D diffusion model, effectively densifying the input\nviews with better multi-view consistency. We also design a tilt camera pose\ntrajectory to capture views with different elevations and perspectives.\nSubsequently, we employ a tri-plane-based mesh reconstruction strategy to\nextract robust tokens from these interpolated and original views, enabling the\ngeneration of high-quality 3D meshes with superior texture and geometry.\nExtensive experiments demonstrate that our method significantly outperforms\nprevious state-of-the-art approaches across various benchmarks, producing 3D\ncontent with enhanced texture fidelity and geometric accuracy.",
        "The quantum description of a black hole predicts that quantum information\nhidden behind the event horizon can be teleported outside almost\ninstantaneously. In this work, we demonstrate that a chiral spin-chain model,\nwhich naturally simulates a binary black hole system, can realise this\nteleportation process. Our system captures two essential components of this\nprotocol: Hawking radiation, which generates the necessary entanglement between\nthe black holes, and optimal scrambling, which enables high-fidelity\nteleportation on short timescales. Through numerical simulations, we quantify\nthe key timescales governing the process, including the Page time, radiation\ntime, scrambling time, and butterfly velocity, showing their universal\ndependence on the chiral coupling strength. Our results establish the\nfeasibility of simulating quantum properties of black holes within condensed\nmatter systems, offering an experimentally accessible platform for probing\notherwise inaccessible high-energy phenomena.",
        "Large Language Models (LLMs) have shown remarkable capabilities as AI agents.\nHowever, existing methods for enhancing LLM-agent abilities often lack a focus\non data quality, leading to inefficiencies and suboptimal results in both\nfine-tuning and prompt engineering. To address this issue, we introduce EDGE, a\nnovel approach for identifying informative samples without needing golden\nanswers. We propose the Guideline Effectiveness (GE) metric, which selects\nchallenging samples by measuring the impact of human-provided guidelines in\nmulti-turn interaction tasks. A low GE score indicates that the human expertise\nrequired for a sample is missing from the guideline, making the sample more\ninformative. By selecting samples with low GE scores, we can improve the\nefficiency and outcomes of both prompt engineering and fine-tuning processes\nfor LLMs. Extensive experiments validate the performance of our method. Our\nmethod achieves competitive results on the HotpotQA and WebShop and datasets,\nrequiring 75\\% and 50\\% less data, respectively, while outperforming existing\nmethods. We also provide a fresh perspective on the data quality of LLM-agent\nfine-tuning.",
        "Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research. Our\ndataset and code will be available upon acceptance.",
        "A series of recent papers by Bergfalk, Lupini and Panagiotopoulus developed\nthe foundations of a field known as `definable algebraic topology,' in which\nclassical cohomological invariants are enriched by viewing them as groups with\na Polish cover. This allows one to apply techniques from descriptive set theory\nto the study of cohomology theories. In this paper, we will establish a\n`definable' version of a classical theorem from obstruction theory, and use\nthis to study the potential complexity of the homotopy relation on the space of\ncontinuous maps $C(X, |K|)$, where $X$ is a locally compact Polish space, and K\nis a locally finite countable simplicial complex. We will also characterize the\nSolecki Groups of the Cech cohomology of X, which are the canonical chain of\nsubgroups with a Polish cover that are least among those of a given complexity.",
        "We present progress on the calculation of scalar, vector, and tensor form\nfactors for the following meson decays: $B\\rightarrow\\pi$, $B_s\\rightarrow K$,\n$D\\rightarrow\\pi$ and $D_s\\rightarrow K$. This calculation uses the MILC HISQ\ngluon field ensembles with HISQ valence quarks. We generate ensembles of\ncorrelator data with varying lattice spacings, some as small as 0.044 fm. Some\nensembles have a strange-to-light quark mass ratio of 5:1 and others use the\nphysical light quark mass. The fully-relativistic, heavy-HISQ approach is used\nfor the heavy quark, with simulation masses ranging from the charm to near the\nbottom. This heavy-HISQ approach provides nearly full coverage of the kinematic\nrange.",
        "Faster-than-Nyquist (FTN) signaling aims at improving the spectral efficiency\nof wireless communication systems by exceeding the boundaries set by the\nNyquist-Shannon sampling theorem. 50 years after its first introduction in the\nscientific literature, wireless communications have significantly changed, but\nspectral efficiency remains one of the key challenges. To adopt FTN signaling,\ninter-symbol interference (ISI) patterns need to be equalized at the receiver.\nMotivated by the pattern recognition capabilities of convolutional neural\nnetworks with skip connections, we propose such deep learning architecture for\nISI equalization and symbol demodulation in FTN receivers. We investigate the\nperformance of the proposed model considering quadrature phase shift keying\nmodulation and low density parity check coding, and compare it to a set of\nbenchmarks, including frequency-domain equalization, a\nquadratic-programming-based receiver, and an equalization scheme based on a\ndeep neural network. We show that our receiver outperforms any benchmark,\nachieving error rates comparable to those in additive white Gaussian noise\nchannel, and higher effective throughput, thanks to the increased spectral\nefficiency of FTN signaling. With a compression factor of 60% and code rate\n3\/4, the proposed model achieves a peak effective throughput of 2.5 Mbps at\njust 10dB of energy per bit over noise power spectral density ratio, with other\nreceivers being limited by error floors due to the strong inter-symbol\ninterference. To promote reproducibility in deep learning for wireless\ncommunications, our code is open source at the repository provided in the\nreferences.",
        "CeTe$_3$ is a 2-dimensional (2D) Van der Waals (VdW) material with\nincommensurate charge density waves (CDW), extremely high transition\ntemperature ($T_{CDW}$) and a large momentum-dependent CDW gap that leaves a\nsignificant portion of the Fermi surface intact. It is also considered to be a\nweak Kondo system, a property unexpected for a material with incommensurate\nCDW, where each atomic site is slightly different. Here, we study the\nproperties of the CDW state in several RTe$_3$ (R is rare earth) materials and\nexamine the hybridization of itinerant states with the localized Ce $4f$\nmultiplet in CeTe$_3$ by using angle resolved photoemission spectroscopy\n(ARPES). We find that the renormalization of the itinerant states originating\nfrom the hybridization with the localized $4f$ states at $-260$ meV extends to\nthe Fermi level. This, with remnants of another localized state at the Fermi\nlevel, supports the characterization of CeTe$_3$ as a weak Kondo material.\nFurthermore, we uncover a $k$-dependence of the hybridization with the states\n$-260$ meV, indicating that similar effect could be the reason for discrepancy\nbetween the heavy masses in specific heat and light ones in Shubnikov de Haas\noscillations observed in other heavy fermion materials.",
        "We study the inflationary phenomenology of a rescaled Einstein-Gauss-Bonnet\ngravity. In this framework, the gravitational constant of the Einstein-Hilbert\nterm is rescaled due to effective terms active in the high curvature era.\nBasically, the total theory is an $F(R,G,\\phi)$ theory with the Gauss-Bonnet\npart contributing only a non-minimal coupling to the scalar field, so it is a\ntheory with string theory origins and with a non-trivial $F(R)$ gravity part.\nThe $F(R)$ gravity part in the high curvature regime contributes only a\nrescaled Einstein-Hilbert term and thus the resulting theory is effectively a\nrescaled version of a standard Einstein-Gauss-Bonnet theory. We develop the\nformalism of rescaled Einstein-Gauss-Bonnet gravity, taking in account the\nGW170817 constraints on the gravitational wave speed. We show explicitly how\nthe rescaled theory affects directly the primordial scalar and tensor\nperturbations, and how the slow-roll and observational indices of inflation are\naffected by the rescaling of the theory. We perform a thorough phenomenological\nanalysis of several models of interest and we show that is it possible to\nobtain viable inflationary theories compatible with the latest Planck data.\nAlso among the studied models there are cases that yield a relatively large\nblue tilted tensor spectral index and we demonstrate that these models can lead\nto detectable primordial gravitational waves in the future gravitational wave\nexperiments. Some of the scenarios examined, for specific values of the\nreheating temperature may be detectable by SKA, LISA, BBO, DECIGO and the\nEinstein Telescope.",
        "Distributed acoustic sensor (DAS) technology leverages optical fiber cables\nto detect acoustic signals, providing cost-effective and dense monitoring\ncapabilities. It offers several advantages including resistance to extreme\nconditions, immunity to electromagnetic interference, and accurate detection.\nHowever, DAS typically exhibits a lower signal-to-noise ratio (S\/N) compared to\ngeophones and is susceptible to various noise types, such as random noise,\nerratic noise, level noise, and long-period noise. This reduced S\/N can\nnegatively impact data analyses containing inversion and interpretation. While\nartificial intelligence has demonstrated excellent denoising capabilities, most\nexisting methods rely on supervised learning with labeled data, which imposes\nstringent requirements on the quality of the labels. To address this issue, we\ndevelop a label-free unsupervised learning (UL) network model based on\nContext-Pyramid-UNet (CP-UNet) to suppress erratic and random noises in DAS\ndata. The CP-UNet utilizes the Context Pyramid Module in the encoding and\ndecoding process to extract features and reconstruct the DAS data. To enhance\nthe connectivity between shallow and deep features, we add a Connected Module\n(CM) to both encoding and decoding section. Layer Normalization (LN) is\nutilized to replace the commonly employed Batch Normalization (BN),\naccelerating the convergence of the model and preventing gradient explosion\nduring training. Huber-loss is adopted as our loss function whose parameters\nare experimentally determined. We apply the network to both the 2-D synthetic\nand filed data. Comparing to traditional denoising methods and the latest UL\nframework, our proposed method demonstrates superior noise reduction\nperformance.",
        "In this study, we unveil a new AI model, termed PhyE2E, to discover physical\nformulas through symbolic regression. PhyE2E simplifies symbolic regression by\ndecomposing it into sub-problems using the second-order derivatives of an\noracle neural network, and employs a transformer model to translate data into\nsymbolic formulas in an end-to-end manner. The resulting formulas are refined\nthrough Monte-Carlo Tree Search and Genetic Programming. We leverage a large\nlanguage model to synthesize extensive symbolic expressions resembling real\nphysics, and train the model to recover these formulas directly from data. A\ncomprehensive evaluation reveals that PhyE2E outperforms existing\nstate-of-the-art approaches, delivering superior symbolic accuracy, precision\nin data fitting, and consistency in physical units. We deployed PhyE2E to five\napplications in space physics, including the prediction of sunspot numbers,\nsolar rotational angular velocity, emission line contribution functions,\nnear-Earth plasma pressure, and lunar-tide plasma signals. The physical\nformulas generated by AI demonstrate a high degree of accuracy in fitting the\nexperimental data from satellites and astronomical telescopes. We have\nsuccessfully upgraded the formula proposed by NASA in 1993 regarding solar\nactivity, and for the first time, provided the explanations for the long cycle\nof solar activity in an explicit form. We also found that the decay of\nnear-Earth plasma pressure is proportional to r^2 to Earth, where subsequent\nmathematical derivations are consistent with satellite data from another\nindependent study. Moreover, we found physical formulas that can describe the\nrelationships between emission lines in the extreme ultraviolet spectrum of the\nSun, temperatures, electron densities, and magnetic fields. The formula\nobtained is consistent with the properties that physicists had previously\nhypothesized it should possess.",
        "Motion stages are critical in semiconductor manufacturing equipment for\nprocesses like die bonding, wafer loading, and chip packaging, as their\nperformance must meet the industry's stringent precision requirements.\nVibration, a significant yet often overlooked adversary to precision motion\nstages, is challenging to identify and mitigate due to its subtle nature. This\nstudy, conducted at a motion stage manufacturer facing frequent\nvibration-related complaints, proposes a novel approach to resolving vibration\nissues. By leveraging the DMAIC methodology, it introduces VIBGUARD, an active\nvibration monitoring and mitigation solution, instead of solely focusing on\ntraditional hardware vibration control. This comprehensive strategy enhances\nvalue and competitiveness, increasing UPH (units per hour) by 15.3% from 8,500\nto 9,800 and reducing downtime by 68.2% from 2.2 to 0.7 occurrences per month.\nThis case study and the DMAIC methodology offer valuable resources for quality\ncontrol and problem analysis in the semiconductor industry.",
        "Online Handwritten Text Recognition (OLHTR) has gained considerable attention\nfor its diverse range of applications. Current approaches usually treat OLHTR\nas a sequence recognition task, employing either a single trajectory or image\nencoder, or multi-stream encoders, combined with a CTC or attention-based\nrecognition decoder. However, these approaches face several drawbacks: 1)\nsingle encoders typically focus on either local trajectories or visual regions,\nlacking the ability to dynamically capture relevant global features in\nchallenging cases; 2) multi-stream encoders, while more comprehensive, suffer\nfrom complex structures and increased inference costs. To tackle this, we\npropose a Collaborative learning-based OLHTR framework, called Col-OLHTR, that\nlearns multimodal features during training while maintaining a single-stream\ninference process. Col-OLHTR consists of a trajectory encoder, a\nPoint-to-Spatial Alignment (P2SA) module, and an attention-based decoder. The\nP2SA module is designed to learn image-level spatial features through\ntrajectory-encoded features and 2D rotary position embeddings. During training,\nan additional image-stream encoder-decoder is collaboratively trained to\nprovide supervision for P2SA features. At inference, the extra streams are\ndiscarded, and only the P2SA module is used and merged before the decoder,\nsimplifying the process while preserving high performance. Extensive\nexperimental results on several OLHTR benchmarks demonstrate the\nstate-of-the-art (SOTA) performance, proving the effectiveness and robustness\nof our design.",
        "Optical lattices formed by interfering laser beams are widely used to trap\nand manipulate atoms for quantum simulation, metrology, and computation. To\nstabilize optical lattices in experiments, it is usually challenging to\nimplement delicate phase-locking systems with complicated optics and\nelectronics to reduce the relative phase fluctuation of multiple laser beams.\nHere we report a phase-locking-free scheme to implement optical lattices by\npassing a single laser beam through a prism with n-fold symmetric facets and\nlarge apex angles. The scheme ensures a stable optical lattice since the\ninterference occurs among different deflected parts of a single laser beam\nwithout any moving component. Various lattice configurations, including a\ntriangular lattice and a quasi-crystal lattice with ten-fold symmetry are\ndemonstrated. In both cases, stability measurements show a change of lattice\nconstant in less than 1.14%, and a drift of lattice position in less than\n1.61%.",
        "In this paper, we propose a novel message-passing decoding approach that\nleverages the degeneracy of quantum low-density parity-check codes to enhance\ndecoding performance, eliminating the need for serial scheduling or\npost-processing. Our focus is on two-block Calderbank-Shor-Steane (CSS) codes,\nwhich are composed of symmetric stabilizers that hinder the performance of\nconventional iterative decoders with uniform update rules. Specifically, our\nanalysis shows that, under the isolation assumption, the min-sum decoder fails\nto converge when constant-weight errors are applied to symmetric stabilizers,\nas variable-to-check messages oscillate in every iteration. To address this, we\nintroduce a decoding technique that exploits this oscillatory property by\napplying distinct update rules: variable nodes in one block utilize messages\nfrom previous iterations, while those in the other block are updated\nconventionally. Logical error-rate results demonstrate that the proposed\ndecoder significantly outperforms the normalized min-sum decoder and achieves\ncompetitive performance with belief propagation enhanced by order-zero ordered\nstatistics decoding, all while maintaining linear complexity in the code's\nblock length.",
        "This study explores the impact of scaling semantic categories on the image\nclassification performance of vision transformers (ViTs). In this specific\ncase, the CLIP server provided by Jina AI is used for experimentation. The\nresearch hypothesizes that as the number of ground truth and artificially\nintroduced semantically equivalent categories increases, the labeling accuracy\nof ViTs improves until a theoretical maximum or limit is reached. A wide\nvariety of image datasets were chosen to test this hypothesis. These datasets\nwere processed through a custom function in Python designed to evaluate the\nmodel's accuracy, with adjustments being made to account for format differences\nbetween datasets. By exponentially introducing new redundant categories, the\nexperiment assessed accuracy trends until they plateaued, decreased, or\nfluctuated inconsistently. The findings show that while semantic scaling\ninitially increases model performance, the benefits diminish or reverse after\nsurpassing a critical threshold, providing insight into the limitations and\npossible optimization of category labeling strategies for ViTs.",
        "We present universal \\emph{building blocks} for the quantum integration of\ngeneric cross sections in high-energy physics. We make use of Fourier Quantum\nMonte Carlo Integration as implemented in {\\sc Quantinuum}'s Quantum Monte\nCarlo Integration engine to provide an extendable methodology for generating\nefficient circuits that can implement generic cross-section calculations,\nproviding a quadratic speed-up in root mean-squared error convergence with\nrespect to classical Monte Carlo integration. We focus on a concrete example of\na $1\\to 3$ decay process to illustrate our work.",
        "Devising models of the limit order book that realistically reproduce the\nmarket response to exogenous trades is extremely challenging and fundamental in\norder to test trading strategies. We propose a novel explainable model for\nsmall tick assets, the Non-Markovian Zero Intelligence, which is a variant of\nthe well-known Zero Intelligence model. The main modification is that the\nprobability of limit orders' signs (buy\/sell) is not constant but is a function\nof the exponentially weighted mid-price return, representing the past price\ndynamics, and can be interpreted as the reaction of traders with reservation\nprices to the price trend. With numerical simulations and analytical arguments,\nwe show that the model predicts a concave price path during a metaorder\nexecution and to a price reversion after the execution ends, as empirically\nobserved. We analyze in-depth the mechanism at the root of the arising\nconcavity, the components which constitute the price impact in our model, and\nthe dependence of the results on the two main parameters, namely the time scale\nand the strength of the reaction of traders to the price trend.",
        "Environmental Education (EE) is vital for shaping citizens who understand and\nvalue sustainability as an epistemological and practical alternative to\nmitigate current environmental issues. This research was prompted by the\nexploration of the relationship between EE and the physical sciences,\nconnections that are often overlooked in curriculums and in the teaching\nprocesses of both this science and EE. It is essential to emphasize that\nphysics provides conceptual frameworks and methodological tools that can\nenhance the understanding of environmental phenomena from a broad and\nmultidimensional perspective. To delve into these connections, a study with a\nhermeneutic interpretative nuance was conducted. Through a questionnaire, the\nperceptions of prospective teachers in the natural sciences field regarding\nthis topic were gathered. The findings revealed that a significant number of\nthem recognize and value the correlation between physics and EE. From their\nperspective, this linkage is not only crucial for a comprehensive view of\nenvironmental dynamics but also to encourage students to develop critical,\narticulated, and well-founded thinking about environmental balance. The\nresearch also highlighted the didactic opportunities presented when\nintertwining physics with EE. By associating physical concepts with real\nenvironmental issues, learning can be reinforced, making it meaningful and\nenduring over time. This interdisciplinary fusion also holds the potential to\nincrease students' motivation and interest, fostering a more active and engaged\nattitude in their educational journey",
        "Empathetic dialogue is crucial for natural human-computer interaction,\nallowing the dialogue system to respond in a more personalized and emotionally\naware manner, improving user satisfaction and engagement. The emergence of\nlarge language models (LLMs) has revolutionized dialogue generation by\nharnessing their powerful capabilities and shown its potential in multimodal\ndomains. Many studies have integrated speech with text-based LLMs to take\nspeech question as input and output text response. However, the lack of spoken\nquestion-answering datasets that include speech style information to supervised\nfine-tuning (SFT) limits the performance of these systems. As a result, while\nthese systems excel at understanding speech content, they often struggle to\ngenerate empathetic responses. In response, we propose a novel approach that\ncircumvents the need for question-answering data, called Listen, Perceive, and\nExpress (LPE). Our method employs a two-stage training process, initially\nguiding the LLM to listen the content and perceive the emotional aspects of\nspeech. Subsequently, we utilize Chain-of-Thought (CoT) prompting to unlock the\nmodel's potential for expressing empathetic responses based on listened spoken\ncontent and perceived emotional cues. We employ experiments to prove the\neffectiveness of proposed method. To our knowledge, this is the first attempt\nto leverage CoT for speech-based dialogue."
      ]
    }
  },
  {
    "id":2411.10403,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Cardiovascular Flow Measurement with Phase-Contrast MR Imaging: Basic Facts and Implementation",
    "start_abstract":"Phase-contrast magnetic resonance (MR) imaging is a well-known but undervalued method of obtaining quantitative information on blood flow. Applications this technique in cardiovascular MR are expanding. According to the sequences available, phase-contrast measurement can be performed breath hold or during normal respiration. Prospective as well retrospective gating techniques used. Common errors include mismatched encoding velocity, deviation plane, inadequate temporal resolution, spatial accelerated flow and misregistration, phase offset errors. Flow measurements most precise if plane perpendicular vessel interest set through-plane The sequence should repeated at least once, with high velocity used initially. If peak has estimated, an adapted velocity. overall error comprises prescription that occur image analysis data. With imaging, reduced less than 10%, acceptable level for routine clinical use.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b14",
        "b1"
      ],
      "title":[
        "Sparse MRI: The application of compressed sensing for rapid MR imaging",
        "MoDL: Model-Based Deep Learning Architecture for Inverse Problems"
      ],
      "abstract":[
        "Abstract The sparsity which is implicit in MR images exploited to significantly undersample k \u2010space. Some such as angiograms are already sparse the pixel representation; other, more complicated have a representation some transform domain\u2013for example, terms of spatial finite\u2010differences or their wavelet coefficients. According recently developed mathematical theory compressed\u2010sensing, with can be recovered from randomly undersampled \u2010space data, provided an appropriate nonlinear recovery scheme used. Intuitively, artifacts due random undersampling add noise\u2010like interference. In domain significant coefficients stand out above A thresholding recover coefficients, effectively recovering image itself. this article, practical incoherent schemes and analyzed by means aliasing Incoherence introduced pseudo\u2010random variable\u2010density phase\u2010encodes. reconstruction performed minimizing \u2113 1 norm transformed image, subject data fidelity constraints. Examples demonstrate improved resolution accelerated acquisition for multislice fast spin\u2010echo brain imaging 3D contrast enhanced angiography. Magn Reson Med, 2007. \u00a9 2007 Wiley\u2010Liss, Inc.",
        "We introduce a model-based image reconstruction framework with convolution neural network (CNN)-based regularization prior. The proposed formulation provides systematic approach for deriving deep architectures inverse problems the arbitrary structure. Since forward model is explicitly accounted for, smaller fewer parameters sufficient to capture information compared direct inversion approaches. Thus, reducing demand training data and time. we rely on end-to-end weight sharing across iterations, CNN weights are customized model, thus offering improved performance over approaches that pre-trained denoisers. Our experiments show decoupling of number iterations from complexity offered by this benefits, including lower data, reduced risk overfitting, implementations significantly memory footprint. propose enforce data-consistency using numerical optimization blocks, such as conjugate gradients algorithm within network. This offers faster convergence per iteration, methods proximal steps consistency. translates performance, primarily when available GPU restricts iterations."
      ],
      "categories":[
        "stat.CO",
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "CausalGeD: Blending Causality and Diffusion for Spatial Gene Expression\n  Generation",
        "Pressure-Induced Structural and Dielectric Changes in Liquid Water at\n  Room Temperature",
        "AugGen: Synthetic Augmentation Can Improve Discriminative Models",
        "CAPOS: The bulge Cluster APOGEE Survey VII: First detailed chemical\n  analysis of NGC 6316",
        "Single-Satellite-Based Geolocation of Broadcast GNSS Spoofers from Low\n  Earth Orbit",
        "The Impact of Building-Induced Visibility Restrictions on Intersection\n  Accidents",
        "Scientific literature cited in patents: A Technology Transfer indicator\n  in Portuguese universities",
        "An atomistic approach for modeling of polarizability and Raman\n  scattering of water clusters and liquid water",
        "VEGA: Voids idEntification using Genetic Algorithm",
        "Sobol-CPI: a Doubly Robust Conditional Permutation Importance Statistic",
        "AdaCS: Adaptive Normalization for Enhanced Code-Switching ASR",
        "An Optimal Cascade Feature-Level Spatiotemporal Fusion Strategy for\n  Anomaly Detection in CAN Bus",
        "X-ray Thomson scattering studies on spin-singlet stabilization of highly\n  compressed H-like Be ions heated to two million degrees Kelvin",
        "Joint Optimization of Resource Allocation and Radar Receiver Selection\n  in Integrated Communication-Radar Systems",
        "Personalize Your LLM: Fake it then Align it",
        "New envelope equations for shallow water waves and modulational\n  instability",
        "Numerical analysis of a finite volume method for a 1-D wave equation\n  with non smooth wave speed and localized Kelvin-Voigt damping",
        "Measurement-Based Modeling and Analysis of UAV Air-Ground Channels at 1\n  and 4 GHz",
        "Direct Detection of Fast-Moving Low-Mass Dark Matter",
        "Join the Chat: How Curiosity Sparks Participation in Telegram Groups",
        "On zero-sum Ramsey numbers modulo 3",
        "Towards Explainable Spoofed Speech Attribution and Detection:a\n  Probabilistic Approach for Characterizing Speech Synthesizer Components",
        "A Minimalist Example of Edge-of-Stability and Progressive Sharpening",
        "Traversable Wormhole in AdS and Entanglement",
        "MoireDB: Formula-generated Interference-fringe Image Dataset",
        "Chiral and deconfinement thermal transitions at finite quark spin\n  polarization in lattice QCD simulations",
        "Continuous Observability Assurance in Cloud-Native Applications",
        "Towards shell model interactions with credible uncertainties"
      ],
      "abstract":[
        "The integration of single-cell RNA sequencing (scRNA-seq) and spatial\ntranscriptomics (ST) data is crucial for understanding gene expression in\nspatial context. Existing methods for such integration have limited\nperformance, with structural similarity often below 60\\%, We attribute this\nlimitation to the failure to consider causal relationships between genes. We\npresent CausalGeD, which combines diffusion and autoregressive processes to\nleverage these relationships. By generalizing the Causal Attention Transformer\nfrom image generation to gene expression data, our model captures regulatory\nmechanisms without predefined relationships. Across 10 tissue datasets,\nCausalGeD outperformed state-of-the-art baselines by 5- 32\\% in key metrics,\nincluding Pearson's correlation and structural similarity, advancing both\ntechnical and biological insights.",
        "Understanding the pressure-dependent dielectric properties of water is\ncrucial for a wide range of scientific and practical applications. In this\nstudy, we employ a deep neural network trained on density functional theory\ndata to investigate the dielectric properties of liquid water at room\ntemperature across a pressure range of 0.1 MPa to 1000 MPa. We observe a\nnonlinear increase in the static dielectric constant $\\epsilon_0$ with\nincreasing pressure, a trend that is qualitatively consistent with experimental\nobservations. This increase in $\\epsilon_0$ is primarily attributed to the\nincrease in water density under compression, which enhances collective dipole\nfluctuations within the hydrogen-bonding network as well as the dielectric\nresponse. Despite the increase in $\\epsilon_0$, our results reveal a decrease\nin the Kirkwood correlation factor $G_K$ with increasing pressure. This\ndecrease in $G_K$ is attributed to pressure-induced structural distortions in\nthe hydrogen-bonding network, which weaken dipolar correlations by disrupting\nthe ideal tetrahedral arrangement of water molecules.",
        "The increasing dependence on large-scale datasets in machine learning\nintroduces significant privacy and ethical challenges. Synthetic data\ngeneration offers a promising solution; however, most current methods rely on\nexternal datasets or pre-trained models, which add complexity and escalate\nresource demands. In this work, we introduce a novel self-contained synthetic\naugmentation technique that strategically samples from a conditional generative\nmodel trained exclusively on the target dataset. This approach eliminates the\nneed for auxiliary data sources. Applied to face recognition datasets, our\nmethod achieves 1--12\\% performance improvements on the IJB-C and IJB-B\nbenchmarks. It outperforms models trained solely on real data and exceeds the\nperformance of state-of-the-art synthetic data generation baselines. Notably,\nthese enhancements often surpass those achieved through architectural\nimprovements, underscoring the significant impact of synthetic augmentation in\ndata-scarce environments. These findings demonstrate that carefully integrated\nsynthetic data not only addresses privacy and resource constraints but also\nsubstantially boosts model performance. Project page\nhttps:\/\/parsa-ra.github.io\/auggen",
        "As part of the bulge Cluster APOgee Survey (CAPOS), high-resolution, high\nSignal-to-Noise Ratio Near-Infrared spectroscopy, we aim to conduct the most\nrobust chemical study to date for NGC 6316, deriving abundances for a number of\nelements with a variety of nucleosynthetic origins, most of which have never\nbeen studied before in this cluster. We use the Brussels Automatic Code for\nCharacterizing High accuracy Spectra (BACCHUS) with atmospheric parameters\nphotometrically obtained in order to determine, for the first time, abundances\nfor C, N, O, Mg, Al, Si, P, K, Ca, Ti, V, Cr, Mn, Fe, Ni and Ce for this\ncluster. We obtained a mean metallicity [Fe\/H] = -0.87 +- 0.02, finding no\nindication of an intrinsic metallicity spread. Our metallicity agrees with the\nmost recent values from other studies, revising earlier values that were ~0.5\ndex metal-richer. With this new value, this cluster, long believed to be a\nmember of the classical metal-rich group of bulge GCs around -0.5, now falls in\nthe dominant bulge globular cluster peak around [Fe\/H] = -1. The cluster\npresents a clear C-N anticorrelation. We also found a [{\\alpha}\/Fe] = 0.3 +-\n0.02. Our abundances show similar behaviour to other in situ globular clusters\nwith comparable metallicity. We obtained E(B-V) = 0.71 and (M-m)_0 = 15.32 +-\n0.05 by isochrone fitting, in good agreement with the recent determinations\nfrom other works. We derive an overall metallicity [M\/H] = -0.6 +- 0.05 by\nisochrone fitting, in agreement with our abundance determination. According to\nthe mean [Mg\/Fe] and [Al\/Fe] abundances from first population stars, NGC 6316\nis an in-situ globular cluster, in accordance with various dynamical\nclassifications.",
        "This paper presents an analysis and experimental demonstration of\nsingle-satellite single-pass geolocation of a terrestrial broadcast Global\nNavigation Satellite System (GNSS) spoofer from Low Earth Orbit (LEO). The\nproliferation of LEO-based GNSS receivers offers the prospect of unprecedented\nspectrum awareness, enabling persistent GNSS interference detection and\ngeolocation. Accurate LEO-based single-receiver emitter geolocation is possible\nwhen a range-rate time history can be extracted for the emitter. This paper\npresents a technique crafted specifically for indiscriminate broadcast-type\nGNSS spoofing signals. Furthermore, it explores how unmodeled oscillator\ninstability and worst-case spoofer-introduced signal variations degrade the\ngeolocation estimate. The proposed geolocation technique is validated by a\ncontrolled experiment, in partnership with Spire Global, in which a LEO-based\nreceiver captures broadcast GNSS spoofing signals transmitted from a known\nground station on a non-GNSS frequency band.",
        "Traffic accidents, especially at intersections, are a major road safety\nconcern. Previous research has extensively studied intersection-related\naccidents, but the effect of building-induced visibility restrictions at\nintersections on accident rates has been under-explored, particularly in urban\ncontexts. Using OpenStreetMap data, the UK's geographic and accident datasets,\nand the UK Traffic Count Dataset, we formulated a novel approach to estimate\naccident risk at intersections. This method factors in the area visible to\ndrivers, accounting for views blocked by buildings - a distinctive aspect in\ntraffic accident analysis. Our findings reveal a notable correlation between\nthe road visible percentage and accident frequency. In the model, the\ncoefficient for \"road visible percentage\" is 1.7450, implying a strong positive\nrelationship. Incorporating this visibility factor enhances the model's\nexplanatory power, with increased R-square values and reduced AIC and BIC,\nindicating a better data fit. This study underscores the essential role of\narchitectural layouts in road safety and suggests that urban planning\nstrategies should consider building-induced visibility restrictions. Such\nconsideration could be an effective approach to mitigate accident rates at\nintersections. This research opens up new avenues for innovative, data-driven\nurban planning and traffic management strategies, highlighting the importance\nof visibility enhancements for safer roads.",
        "The study aims to identify the process of transfer from science to technology\nthat occurs in the main Portuguese public universities. The methodology was\nbased on the analysis of the scientific literature cited in patents. Data was\nobtained from the Lens patent database. 10,514 scientific articles cited in\npatents were retrieved. A descriptive analysis of the data was performed.\nScience maps were created to visualize the main research trends. The results\nshowed a valuable impact of academic research in certain scientific\ndisciplines, such as Chemistry, Biology, Materials Sciences and Medicine. The\nmain research fronts were cancer, nanoparticles, biomaterials, tissue\nengineering or molecular biology. In conclusion, the research produced by\nPortuguese universities has generated relevant knowledge for patented\ninventions and the science-technology flow within specific areas.",
        "In this work, we develop a framework for atomistic modeling of electronic\npolarizability to predict the Raman spectra of hydrogen-bonded clusters and\nliquids from molecular dynamics (MD) simulations. The total polarizability of\nthe system is assumed to arise from contributions of both the monomer unit and\nintermolecular interactions. The generalized bond-polarizability model (GBPM),\ninspired by the classic bond-polarizability model, effectively describes the\nelectronic polarizability of a monomer. To account for the electronic\npolarizability arising from intermolecular interactions, we use a basis set of\nrapidly decaying functions of interatomic distances. We apply this model to\ncalculate the electronic polarizability and Raman spectra of water clusters\n((H2O)r, r = 2, 3, 4, 5, 6) and liquid water. The computational results are\ncompared with the results of quantum-mechanical calculations for clusters and\nto experimental data for the liquid. It is demonstrated that this simple and\nphysically motivated model, which relies on a small number of parameters,\nperforms well for clusters at both low and high temperatures, capturing strong\nanharmonic effects. Moreover, its high transferability suggests its\napplicability to other water clusters. These results suggest that a\nhierarchical approach based on the Jacob's ladder of increasingly sophisticated\nand accurate atomistic polarizability models incorporating additional effects\ncan be used for efficient modeling of Raman spectra from MD simulations of\nclusters, liquids and solids.",
        "Cosmic voids, the nearly empty regions nestled between walls and filaments,\nare recognized for their extensive applications in the field of cosmology and\nastrophysics. However, a consensus on the definition of voids remains elusive,\nas various void-finding methods identify different types of voids, each\ndiffering in shape and density based on the method that were used. In this\npaper, we introduce an innovative void identification method that utilizes\nGenetic Algorithm analysis. VEGA employs the Voronoi tessellation technique and\nthe Convex Hull algorithm to partition the dataset plane into distinct regions\nand calculate the volume of each region. For the first time, VEGA integrates\nGenetic Algorithm analysis with the luminosity density contrast function to\nidentify and locate the possible void region candidates. This method utilizes a\nset of grid points, which enhances the implementation of Voronoi tessellation\nand enables VEGA to more effectively access the dataset space for the\nidentification of void regions candidates, finding the center and the ultimate\nstructure of voids. Finally, we applied the VEGA and Aikio-M\\\"ah\\\"onen (AM)\nmethods to the same test dataset and compared the cosmic voids identified by\nVEGA with those identified by the AM method. This comparison demonstrated that\nthe VEGA void-finding method yields reliable results and can be effectively\napplied to various particle distributions.",
        "Conditional Permutation Importance (CPI) has been recently introduced for\nVariable Importance analysis with good empirical results. In this work, we\nfirst provide theoretical guarantees for CPI. We establish a double robustness\nproperty to detect null covariates, making it a suitable model for variable\nselection. We then present a modified and still computationally efficient\nversion, Sobol-CPI, that aims to estimate a well-known variable importance\nmeasure, the Total Sobol Index (TSI). We prove that it is nonparametrically\nefficient, and we provide a procedure to control the type-I error. Through\nnumerical experiments, we show that Sobol-CPI preserves the double robustness\nproperty in practice.",
        "Intra-sentential code-switching (CS) refers to the alternation between\nlanguages that happens within a single utterance and is a significant challenge\nfor Automatic Speech Recognition (ASR) systems. For example, when a Vietnamese\nspeaker uses foreign proper names or specialized terms within their speech. ASR\nsystems often struggle to accurately transcribe intra-sentential CS due to\ntheir training on monolingual data and the unpredictable nature of CS. This\nissue is even more pronounced for low-resource languages, where limited data\navailability hinders the development of robust models. In this study, we\npropose AdaCS, a normalization model integrates an adaptive bias attention\nmodule (BAM) into encoder-decoder network. This novel approach provides a\nrobust solution to CS ASR in unseen domains, thereby significantly enhancing\nour contribution to the field. By utilizing BAM to both identify and normalize\nCS phrases, AdaCS enhances its adaptive capabilities with a biased list of\nwords provided during inference. Our method demonstrates impressive performance\nand the ability to handle unseen CS phrases across various domains. Experiments\nshow that AdaCS outperforms previous state-of-the-art method on Vietnamese CS\nASR normalization by considerable WER reduction of 56.2% and 36.8% on the two\nproposed test sets.",
        "Autonomous vehicles represent a revolutionary advancement driven by the\nintegration of artificial intelligence within intelligent transportation\nsystems. However, they remain vulnerable due to the absence of robust security\nmechanisms in the Controller Area Network (CAN) bus. In order to mitigate the\nsecurity issue, many machine learning models and strategies have been proposed,\nwhich primarily focus on a subset of dominant patterns of anomalies and lack\nrigorous evaluation in terms of reliability and robustness. Therefore, to\naddress the limitations of previous works and mitigate the security\nvulnerability in CAN bus, the current study develops a model based on the\nintrinsic nature of the problem to cover all dominant patterns of anomalies. To\nachieve this, a cascade feature-level fusion strategy optimized by a\ntwo-parameter genetic algorithm is proposed to combine temporal and spatial\ninformation. Subsequently, the model is evaluated using a paired t-test to\nensure reliability and robustness. Finally, a comprehensive comparative\nanalysis conducted on two widely used datasets advocates that the proposed\nmodel outperforms other models and achieves superior accuracy and F1-score,\ndemonstrating the best performance among all models presented to date.",
        "Experiments at the US National Ignition Facility (NIF) [D\\\"{o}ppner et al.,\nNature {\\bf 618}, 270-275 (2023)] have created highly compressed hot\nhydrogen-like Be plasmas. Published analyses of the the NIF experiment have\nused finite-$T$ multi-atom density-functional theory (DFT) with Molecular\ndynamics (MD), and Path-Integral Monte Carlo (PIMC) simulations. These methods\nare very expensive to implement and often lack physical transparency. Here we\n(i) relate their results to simpler first-principles average-atom results, (ii)\nestablish the feasibility of rapid data analysis, with good accuracy and gain\nin physical transparency, and (iii) show that the NIF experiment reveals\nhigh-$T$ spin-singlet pairing of hydrogen-like Be ions with near neighbours.\nOur analysis predicts such stabilization over a wide range of compressed\ndensities for temperatures close to two million Kelvin. Calculations of\nstructure factors $S(k)$ for electrons or ions, the Raleigh weight and other\nquantities of interest to X-ray Thomson scattering are presented. We find that\nthe NIF data at the scattering wavevector $k_{sc}$ of 7.89 \\AA$^{-1}$ are more\nconsistent with a density of $20\\pm2$ g\/cm$^3$, mean ionization $\\bar{Z}=$3.25,\nat a temperature of $\\simeq$ 1,800,000 K than the 34 g\/cm$^3, \\bar{Z}=3.4$\nproposed by the NIF team. The relevance of ion-electron coupled-modes in\nstudying small $k_{sc}$ data is indicated.",
        "In this paper, we investigate a distributed multi-input multi-output and\northogonal frequency division multiplexing (MIMO-OFDM) dual-function\nradar-communication (DFRC) system, which enables simultaneous communication and\nsensing in different subcarrier sets. To obtain the best tradeoff between\ncommunication and sensing performance, we first derive Cramer-Rao Bound (CRB)\nof targets in the detection area, and then maximize the transmission rate by\njointly optimizing the power\/subcarriers allocation and the selection of radar\nreceivers under the constraints of detection performance and total transmit\npower. To tackle the non-convex mixed integer programming problem, we decompose\nthe original problem into a semidefinite programming (SDP) problem and a convex\nquadratic integer problem and solve them iteratively. The numerical results\ndemonstrate the effectiveness of our proposed algorithm, as well as the\nperformance improvement brought by optimizing radar receivers selection.",
        "Personalizing large language models (LLMs) is essential for delivering\ntailored interactions that improve user experience. Many existing\npersonalization methods require fine-tuning LLMs for each user, rendering them\nprohibitively expensive for widespread adoption. Although retrieval-based\napproaches offer a more compute-efficient alternative, they still depend on\nlarge, high-quality datasets that are not consistently available for all users.\nTo address this challenge, we propose CHAMELEON, a scalable and efficient\npersonalization approach that uses (1) self-generated personal preference data\nand (2) representation editing to enable quick and cost-effective\npersonalization. Our experiments on various tasks, including those from the\nLaMP personalization benchmark, show that CHAMELEON efficiently adapts models\nto personal preferences, improving instruction-tuned models and outperforms two\npersonalization baselines by an average of 40% across two model architectures.",
        "The dynamics of wave groups is studied for long waves, using the framework of\nthe Benjamin-Bona-Mahony (BBM) equation and its generalizations. It is shown\nthat the dynamics are richer than the corresponding results obtained just from\nthe Korteweg-de Vries-type equation. First, a reduction to a nonlinear\nSchr\\\"odinger equation is obtained for weakly nonlinear wave packets, and it is\ndemonstrated that either the focusing or the defocusing case can be obtained.\nThis is in contrast to the corresponding reduction for the Korteweg-de Vries\nequation, where only the defocusing case is found. The focusing regime displays\nmodulational instability responsible for the appearance of rogue waves. Next,\nthe condition for modulational instability is obtained in the case of one and\ntwo monochromatic waves in interaction at slow space-time coordinates with\nequal scalings. Other new envelope equations are obtained starting from the\ngeneral system describing shallow water waves found by Bona et al. [3]. A\npresumably integrable system is obtained form the integrable Kaup-Boussinesq\none.",
        "In this paper, we study the numerical solution of an elastic\/viscoelastic\nwave equation with non smooth wave speed and internal localized distributed\nKelvin-Voigt damping acting faraway from the boundary. Our method is based on\nthe Finite Volume Method (FVM) and we are interested in deriving the stability\nestimates and the convergence of the numerical solution to the continuous one.\nNumerical experiments are performed to confirm the theoretical study on the\ndecay rate of the solution to the null one when a localized damping acts.",
        "In the design of unmanned aerial vehicle (UAV) wireless communications, a\nbetter understanding of propagation characteristics and an accurate channel\nmodel are required. Measurements and comprehensive analysis for the UAV-based\nair-ground (AG) propagation channel in the vertical dimension are presented in\nthis letter. Based on the measurement data at 1 and 4 GHz, the large-scale and\nsmall-scale channel parameters are extracted in the line-of-sight (LOS) and\nnonLOS case, respectively. The altitude-dependent path loss model is proposed\nherein. Furthermore, shadow fading and fast fading are statistically analyzed\nfor comprehensively describing the fading behavior. Our results will be useful\nin the modeling of AG channels and the performance analysis for UAV-enabled\nwireless communication systems.",
        "We examine the signals produced by dark matter interactions with electrons,\nwhich play a crucial role in direct detection experiments employing heavy\ntarget materials, particularly in many well-motivated sub-GeV dark matter\nscenarios. When the momentum transfer to target electrons is comparable to or\nexceeds their binding energy, atomic effects related to electron ionization\nbecome essential for accurately determining signal rates - especially in the\ncase of fast-moving dark matter. In this paper, we revisit and extend the\natomic ionization formalism, systematically comparing different approaches used\nto formulate the ionization form factor and identifying their respective\ndomains of validity. As practical applications, we explore detection prospects\nin xenon target experiments. To illustrate our findings, we consider a specific\nscenario involving boosted dark matter, which often leads to high-momentum\nelectron recoils. Our analysis demonstrates that the choice of formalism can\nsignificantly influence the interpretation of experimental data, depending on\nthe regions of parameter space.",
        "This study delves into the mechanisms that spark user curiosity driving\nactive engagement within public Telegram groups. By analyzing approximately 6\nmillion messages from 29,196 users across 409 groups, we identify and quantify\nthe key factors that stimulate users to actively participate (i.e., send\nmessages) in group discussions. These factors include social influence,\nnovelty, complexity, uncertainty, and conflict, all measured through metrics\nderived from message sequences and user participation over time. After\nclustering the messages, we apply explainability techniques to assign\nmeaningful labels to the clusters. This approach uncovers macro categories\nrepresenting distinct curiosity stimulation profiles, each characterized by a\nunique combination of various stimuli. Social influence from peers and\ninfluencers drives engagement for some users, while for others, rare media\ntypes or a diverse range of senders and media sparks curiosity. Analyzing\npatterns, we found that user curiosity stimuli are mostly stable, but, as the\ntime between the initial message increases, curiosity occasionally shifts. A\ngraph-based analysis of influence networks reveals that users motivated by\ndirect social influence tend to occupy more peripheral positions, while those\nwho are not stimulated by any specific factors are often more central,\npotentially acting as initiators and conversation catalysts. These findings\ncontribute to understanding information dissemination and spread processes on\nsocial media networks, potentially contributing to more effective communication\nstrategies.",
        "We start with a systematic study of the zero-sum Ramsey numbers. For a graph\n$G$ with $0 \\ (\\!\\!\\!\\!\\mod 3)$ edges, the zero-sum Ramsey number is defined as\nthe smallest positive integer $R(G, \\mathbb{Z}_3)$ such that for every $n \\geq\nR(G, \\mathbb{Z}_3)$ and every edge-colouring $f$ of $K_n$ using $\\mathbb{Z}_3$,\nthere is a zero-sum copy of $G$ in $K_n$ coloured by $f$, that is: $\\sum_{e \\in\nE(G)} f(e) \\equiv 0 \\ (\\!\\!\\!\\!\\mod 3)$.\n  Only sporadic results are known for these Ramsey numbers, and we discover\nmany new ones. In particular we prove that for every forest $F$ on $n$ vertices\nand with $0 \\ (\\!\\!\\!\\!\\mod 3)$ edges, $R(F, \\mathbb{Z}_3) \\leq n+2$, and this\nbound is tight if all the vertices of $F$ have degrees $1 \\ (\\!\\!\\!\\!\\mod 3)$.\nWe also determine exact values of $R(T, \\mathbb{Z}_3)$ for infinite families of\ntrees.",
        "We propose an explainable probabilistic framework for characterizing spoofed\nspeech by decomposing it into probabilistic attribute embeddings. Unlike raw\nhigh-dimensional countermeasure embeddings, which lack interpretability, the\nproposed probabilistic attribute embeddings aim to detect specific speech\nsynthesizer components, represented through high-level attributes and their\ncorresponding values. We use these probabilistic embeddings with four\nclassifier back-ends to address two downstream tasks: spoofing detection and\nspoofing attack attribution. The former is the well-known bonafide-spoof\ndetection task, whereas the latter seeks to identify the source method\n(generator) of a spoofed utterance. We additionally use Shapley values, a\nwidely used technique in machine learning, to quantify the relative\ncontribution of each attribute value to the decision-making process in each\ntask. Results on the ASVspoof2019 dataset demonstrate the substantial role of\nduration and conversion modeling in spoofing detection; and waveform generation\nand speaker modeling in spoofing attack attribution. In the detection task, the\nprobabilistic attribute embeddings achieve $99.7\\%$ balanced accuracy and\n$0.22\\%$ equal error rate (EER), closely matching the performance of raw\nembeddings ($99.9\\%$ balanced accuracy and $0.22\\%$ EER). Similarly, in the\nattribution task, our embeddings achieve $90.23\\%$ balanced accuracy and\n$2.07\\%$ EER, compared to $90.16\\%$ and $2.11\\%$ with raw embeddings. These\nresults demonstrate that the proposed framework is both inherently explainable\nby design and capable of achieving performance comparable to raw CM embeddings.",
        "Recent advances in deep learning optimization have unveiled two intriguing\nphenomena under large learning rates: Edge of Stability (EoS) and Progressive\nSharpening (PS), challenging classical Gradient Descent (GD) analyses. Current\nresearch approaches, using either generalist frameworks or minimalist examples,\nface significant limitations in explaining these phenomena. This paper advances\nthe minimalist approach by introducing a two-layer network with a\ntwo-dimensional input, where one dimension is relevant to the response and the\nother is irrelevant. Through this model, we rigorously prove the existence of\nprogressive sharpening and self-stabilization under large learning rates, and\nestablish non-asymptotic analysis of the training dynamics and sharpness along\nthe entire GD trajectory. Besides, we connect our minimalist example to\nexisting works by reconciling the existence of a well-behaved ``stable set\"\nbetween minimalist and generalist analyses, and extending the analysis of\nGradient Flow Solution sharpness to our two-dimensional input scenario. These\nfindings provide new insights into the EoS phenomenon from both parameter and\ninput data distribution perspectives, potentially informing more effective\noptimization strategies in deep learning practice.",
        "A traversable wormhole generally violates the averaged null energy condition,\nusually requiring exotic matter. Recently, it has been found that the\ntraversable wormhole can be realized by non-exotic matter in\nEinstein-Dirac-Maxwell theories in flat space. This paper generalizes\ndiscussions to the AdS spacetime and finds traversable wormholes with spherical\nand planar topologies. Furthermore, based on the AdS\/CFT correspondence, we\ncompute the entanglement entropy of strips and disks on two AdS boundaries of\nthe wormhole. We find that entanglement entropy undergoes a phase transition as\nthe subsystem size increases.",
        "Image recognition models have struggled to treat recognition robustness to\nreal-world degradations. In this context, data augmentation methods like PixMix\nimprove robustness but rely on generative arts and feature visualizations\n(FVis), which have copyright, drawing cost, and scalability issues. We propose\nMoireDB, a formula-generated interference-fringe image dataset for image\naugmentation enhancing robustness. MoireDB eliminates copyright concerns,\nreduces dataset assembly costs, and enhances robustness by leveraging illusory\npatterns. Experiments show that MoireDB augmented images outperforms\ntraditional Fractal arts and FVis-based augmentations, making it a scalable and\neffective solution for improving model robustness against real-world\ndegradations.",
        "We study the effect of finite spin quark density on the chiral and\ndeconfinement thermal transitions using numerical simulations of lattice QCD\nwith two dynamical light quarks. The finite spin density is introduced by the\nquark spin potential in the canonical formulation of the spin operator. We show\nthat both chiral and deconfinement temperatures are decreasing functions of the\nspin potential. We determine the parabolic curvatures of transition\ntemperatures in a limit of physical quark masses.",
        "When faults occur in microservice applications -- as they inevitably do --\ndevelopers depend on observability data to quickly identify and diagnose the\nissue. To collect such data, microservices need to be instrumented and the\nrespective infrastructure configured. This task is often underestimated and\nerror-prone, typically relying on many ad-hoc decisions. However, some of these\ndecisions can significantly affect how quickly faults are detected and also\nimpact the cost and performance of the application.\n  Given its importance, we emphasize the need for a method to guide the\nobservability design process. In this paper, we build on previous work and\nintegrate our observability experiment tool OXN into a novel method for\ncontinuous observability assurance. We demonstrate its use and discuss future\ndirections.",
        "Background: The nuclear shell model offers realistic predictions of nuclear\nstructure starting from (quasi-) proton and neutron degrees of freedom, but\nrelies on coupling constants (interaction matrix elements) that must be fit to\nexperiment. To extend the shell model's applicability across the nuclear chart,\nand specifically toward the driplines, we must first be able to efficiently\ntest new interaction matrix elements and assign credible uncertainties.\n  Purpose: We develop and test a framework to efficiently fit new shell model\ninteractions and obtain credible uncertainties. We further demonstrate its use\nby validating the uncertainty estimates of the known \\textit{sd}-shell\neffective interactions.\n  Methods: We use eigenvector continuation to emulate solutions to the exact\nshell model. First, we use the emulator to replicate earlier results using a\nwell-known linear-combination chi-squared minimization algorithm. Then, we\nemploy a modern Markov Chain Monte Carlo method to test for nonlinearities in\nthe observable posterior distributions, which previous sensitivity analyses\nprecluded.\n  Results: The emulator reproduces the USDB interaction within a small margin\nof error, allowing for the quantification of the matrix element uncertainty.\nHowever, we find that to obtain credible predictive intervals the model defect\nof the shell model itself, rather than experimental or emulator\nuncertainty\/error, must be taken into account.\n  Conclusions: Eigenvector continuation can be used to accelerate fitting shell\nmodel interactions. We confirm that the linear approximation used to develop\ninteractions in the past is indeed sufficient. However, we find that typical\nassumptions about the likelihood function must be modified in order to obtain a\ncredible uncertainty-quantified interaction."
      ]
    }
  },
  {
    "id":2411.15202,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"The Great Barrier Reef: an environmental history",
    "start_abstract":"Reconstructing changes in the Great Barrier Reef 15 3 The natural context of 33 4 spread European settlement coastal Queensland 43 5 beche--de ... mer, pearl shell and trochus fisheries 55 6 Impacts on marine turtles 72 7 dugongs 95 8 whales, sharks fish 9 impacts coral collecting 10 guano rock phosphate mining 11 vi Contents 12 Other reefs 13 Changes island biota 14 Conclusion",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Ilf-lstm: Enhanced loss function in lstm to predict the sea surface temperature"
      ],
      "abstract":[
        "Globe's primary issue is global warming, water temperatures have accompanied it as the sea surface temperature, and it is the primary attribute to balance the energy on the earth's surface. Sea surface temperature prediction is vital to climate forecast. Downwelling currents carry some of this heat to the ocean's bottom layers, which are also heating, covering far behind the increase in sea surface temperature. In deep learning models, the correct loss function will try to reduce the error and converge fast. The proposed improved loss function correctly estimates how close the predictions made by the long short-term memory match the observed values in the training data. This research considers location-specific sea surface temperature predictions using the improved loss function in the long short-term memory neural network at six different locations around India for daily, weekly, and monthly time horizons. Most existing research concentrated on periodic forecasts, but this paper focused on daily, weekly, and monthly predictions. The improved loss function\u2014long short-term memory, achieved 98.7% accuracy, and this improved loss function overcomes the limitations of the existing techniques and reduces the processing time to\u2009~\u20090.35 s. In this research, the sea surface temperature prediction using the improved loss function in the long short-term memory neural network gives better results than the standard prediction models and other existing techniques by considering the long-time dependencies and obtaining features from the spatial data."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Resolution Invariant Autoencoder",
        "Extending Dense Passage Retrieval with Temporal Information",
        "Exploring constraints on the core radius and density jumps inside Earth\n  using atmospheric neutrino oscillations",
        "The Kodaira dimension of Hilbert modular threefolds",
        "Some NP Complete Problems Based on Algebra and Algebraic Geometry",
        "Lagrangian chaos and unique ergodicity for stochastic primitive\n  equations",
        "Simpliciality of vector-valued function spaces",
        "Graphy'our Data: Towards End-to-End Modeling, Exploring and Generating\n  Report from Raw Data",
        "Block Flow: Learning Straight Flow on Data Blocks",
        "LiDAR Remote Sensing Meets Weak Supervision: Concepts, Methods, and\n  Perspectives",
        "Training Consistency Models with Variational Noise Coupling",
        "Formulas as Processes, Deadlock-Freedom as Choreographies (Extended\n  Version)",
        "CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from\n  Motion-Blurred Images",
        "Optimal Control of Fluid Restless Multi-armed Bandits: A Machine\n  Learning Approach",
        "Central-moment-based discrete Boltzmann modeling of compressible flows",
        "Accuracy Can Lie: On the Impact of Surrogate Model in Configuration\n  Tuning",
        "Resilient Distributed Control for Uncertain Nonlinear Interconnected\n  Systems under Network Anomaly",
        "YUNet: Improved YOLOv11 Network for Skyline Detection",
        "Bounded powers of edge ideals: regularity and linear quotients",
        "Ukrainian contribution to particle physics: historical perspective and\n  prospects",
        "Double-Scaled SYK, QCD, and the Flat Space Limit of de Sitter Space",
        "Cyclicity of sliding cycles with singularities of regularized piecewise\n  smooth visible-invisible two-folds",
        "Robust Cross-Etiology and Speaker-Independent Dysarthric Speech\n  Recognition",
        "Deep learning-based holography for T-linear resistivity",
        "GNSS\/GPS Spoofing and Jamming Identification Using Machine Learning and\n  Deep Learning",
        "Structure factors and quantum geometry in multiband BCS superconductors",
        "Patch-Depth Fusion: Dichotomous Image Segmentation via Fine-Grained\n  Patch Strategy and Depth Integrity-Prior",
        "Foundation Inference Models for Stochastic Differential Equations: A\n  Transformer-based Approach for Zero-shot Function Estimation",
        "A Volumetric Approach to Privacy of Dynamical Systems"
      ],
      "abstract":[
        "Deep learning has significantly advanced medical imaging analysis, yet\nvariations in image resolution remain an overlooked challenge. Most methods\naddress this by resampling images, leading to either information loss or\ncomputational inefficiencies. While solutions exist for specific tasks, no\nunified approach has been proposed. We introduce a resolution-invariant\nautoencoder that adapts spatial resizing at each layer in the network via a\nlearned variable resizing process, replacing fixed spatial down\/upsampling at\nthe traditional factor of 2. This ensures a consistent latent space resolution,\nregardless of input or output resolution. Our model enables various downstream\ntasks to be performed on an image latent whilst maintaining performance across\ndifferent resolutions, overcoming the shortfalls of traditional methods. We\ndemonstrate its effectiveness in uncertainty-aware super-resolution,\nclassification, and generative modelling tasks and show how our method\noutperforms conventional baselines with minimal performance loss across\nresolutions.",
        "Temporal awareness is crucial in many information retrieval tasks,\nparticularly in scenarios where the relevance of documents depends on their\nalignment with the query's temporal context. Traditional retrieval methods such\nas BM25 and Dense Passage Retrieval (DPR) excel at capturing lexical and\nsemantic relevance but fall short in addressing time-sensitive queries. To\nbridge this gap, we introduce the temporal retrieval model that integrates\nexplicit temporal signals by incorporating query timestamps and document dates\ninto the representation space. Our approach ensures that retrieved passages are\nnot only topically relevant but also temporally aligned with user intent. We\nevaluate our approach on two large-scale benchmark datasets, ArchivalQA and\nChroniclingAmericaQA, achieving substantial performance gains over standard\nretrieval baselines. In particular, our model improves Top-1 retrieval accuracy\nby 6.63% and NDCG@10 by 3.79% on ArchivalQA, while yielding a 9.56% boost in\nTop-1 retrieval accuracy and 4.68% in NDCG@10 on ChroniclingAmericaQA.\nAdditionally, we introduce a time-sensitive negative sampling strategy, which\nrefines the model's ability to distinguish between temporally relevant and\nirrelevant documents during training. Our findings highlight the importance of\nexplicitly modeling time in retrieval systems and set a new standard for\nhandling temporally grounded queries.",
        "Atmospheric neutrinos, through their weak interactions, can serve as an\nindependent tool for exploring the internal structure of Earth. The information\nobtained would be complementary to that provided by seismic and gravitational\nmeasurements. The Earth matter effects in neutrino oscillations depend upon the\nenergy of neutrinos and the electron density distribution that they encounter\nduring their journey through Earth, and hence, can be used to probe the inner\nstructure of Earth. In this contribution, we demonstrate how well an\natmospheric neutrino experiment, such as an iron calorimeter detector (ICAL),\nwould simultaneously constrain the density jumps inside Earth and determine the\nlocation of the core-mantle boundary. In this work, we employ a five-layered\ndensity model of Earth, where the layer densities and core radius are modified\nto explore the parameter space, ensuring that the mass and moment of inertia of\nEarth remain constant while satisfying the hydrostatic equilibrium condition.\nWe further demonstrate that the charge identification capability of an\nICAL-like detector would play a crucial role in obtaining these correlated\nconstraints.",
        "Following a method introduced by Thomas-Vasquez and developed by Grundman, we\nprove that many Hilbert modular threefolds of arithmetic genus $0$ and $1$ are\nof general type, and that some are of nonnegative Kodaira dimension. The new\ningredient is a detailed study of the geometry and combinatorics of totally\npositive integral elements $x$ of a fractional ideal $I$ in a totally real\nnumber field $K$ with the property that $\\mathop{\\mathrm{tr}} xy <\n\\mathop{\\mathrm{min}} I \\mathop{\\mathrm{tr}} y$ for some $y \\gg 0 \\in K$.",
        "This paper describes several new problems and ideas concerning algebraic\ngeometry and complexity theory. It first uses the idea of coloring graphs with\nelements of finite fields. This procedure then shows that graph coloring\nproblems can be converted into membership problems for a new family of\nalgebraic varieties, coloring varieties, which are closely related to\ndeterminantal varieties. This in turn shows that the problem of NP vs P can be\nconverted into questions of if certain polynomials of large degree over finite\nfields have low multiplicative complexity.",
        "We show that the Lagrangian flow associated with the stochastic 3D primitive\nequations (PEs) with non-degenerate noise is chaotic, i.e., the corresponding\ntop Lyapunov exponent is strictly positive almost surely. This result builds on\nthe landmark work by Bedrossian, Blumenthal, and Punshon-Smith on Lagrangian\nchaos in stochastic fluid mechanics. Our primary contribution is establishing\nan instance where Lagrangian chaos can be proven for a fluid flow with\nsupercritical energy, a key characteristic of 3D fluid dynamics. For the 3D\nPEs, establishing the existence of the top Lyapunov exponent is already a\nchallenging task. We address this difficulty by deriving new estimates for the\ninvariant measures of the 3D PEs, which capture the anisotropic smoothing in\nthe dynamics of the PEs. As a by-product of our results, we also obtain the\nfirst uniqueness result for invariant measures of stochastic PEs.",
        "We investigate integral representation of vector-valued function spaces,\ni.e., of subspaces $H\\subset C(K,E)$, where $K$ is a compact space and $E$ is a\n(real or complex) Banach space. We point out that there are two possible ways\nof generalizing representation theorems known from the scalar case -- either\none may represent (all) functionals from $H^*$ using $E^*$-valued vector\nmeasures on $K$ (as it is done in the literature) or one may represent (some)\noperators from $L(H,E)$ by scalar measures on $K$ using the Bochner integral.\nThese two ways lead to two different notions of simpliciality which we call\n`vector simpliciality' and `weak simpliciality'. It turns out that these two\nnotions are in general incomparable. Moreover, the weak simpliciality is not\naffected by renorming the target space $E$, while vector simpliciality may be\naffected. Further, if $H$ contains constants, vector simpliciality is strictly\nstronger and admits several characterizations (partially analogous to the\ncharacterizations known in the scalar case). We also study orderings of\nmeasures inspired by C.J.K.~Batty which may be (in special cases) used to\ncharacterize $H$-boundary measures. Finally, we give a finer version of\nrepresentation theorem using positive measures on $K\\times B_{E^*}$ and\ncharacterize uniqueness in this case.",
        "Large Language Models (LLMs) have recently demonstrated remarkable\nperformance in tasks such as Retrieval-Augmented Generation (RAG) and\nautonomous AI agent workflows. Yet, when faced with large sets of unstructured\ndocuments requiring progressive exploration, analysis, and synthesis, such as\nconducting literature survey, existing approaches often fall short. We address\nthis challenge -- termed Progressive Document Investigation -- by introducing\nGraphy, an end-to-end platform that automates data modeling, exploration and\nhigh-quality report generation in a user-friendly manner. Graphy comprises an\noffline Scrapper that transforms raw documents into a structured graph of Fact\nand Dimension nodes, and an online Surveyor that enables iterative exploration\nand LLM-driven report generation. We showcase a pre-scrapped graph of over\n50,000 papers -- complete with their references -- demonstrating how Graphy\nfacilitates the literature-survey scenario. The demonstration video can be\nfound at https:\/\/youtu.be\/uM4nzkAdGlM.",
        "Flow-matching models provide a powerful framework for various applications,\noffering efficient sampling and flexible probability path modeling. These\nmodels are characterized by flows with low curvature in learned generative\ntrajectories, which results in reduced truncation error at each sampling step.\nTo further reduce curvature, we propose block matching. This novel approach\nleverages label information to partition the data distribution into blocks and\nmatch them with a prior distribution parameterized using the same label\ninformation, thereby learning straighter flows. We demonstrate that the\nvariance of the prior distribution can control the curvature upper bound of\nforward trajectories in flow-matching models. By designing flexible\nregularization strategies to adjust this variance, we achieve optimal\ngeneration performance, effectively balancing the trade-off between maintaining\ndiversity in generated samples and minimizing numerical solver errors. Our\nresults demonstrate competitive performance with models of the same parameter\nscale.Code is available at \\url{https:\/\/github.com\/wpp13749\/block_flow}.",
        "LiDAR (Light Detection and Ranging) enables rapid and accurate acquisition of\nthree-dimensional spatial data, widely applied in remote sensing areas such as\nsurface mapping, environmental monitoring, urban modeling, and forestry\ninventory. LiDAR remote sensing primarily includes data interpretation and\nLiDAR-based inversion. However, LiDAR interpretation typically relies on dense\nand precise annotations, which are costly and time-consuming. Similarly, LiDAR\ninversion depends on scarce supervisory signals and expensive field surveys for\nannotations. To address this challenge, weakly supervised learning has gained\nsignificant attention in recent years, with many methods emerging to tackle\nLiDAR remote sensing tasks using incomplete, inaccurate, and inexact\nannotations, as well as annotations from other domains. Existing review\narticles treat LiDAR interpretation and inversion as separate tasks. This\nreview, for the first time, adopts a unified weakly supervised learning\nperspective to systematically examine research on both LiDAR interpretation and\ninversion. We summarize the latest advancements, provide a comprehensive review\nof the development and application of weakly supervised techniques in LiDAR\nremote sensing, and discuss potential future research directions in this field.",
        "Consistency Training (CT) has recently emerged as a promising alternative to\ndiffusion models, achieving competitive performance in image generation tasks.\nHowever, non-distillation consistency training often suffers from high variance\nand instability, and analyzing and improving its training dynamics is an active\narea of research. In this work, we propose a novel CT training approach based\non the Flow Matching framework. Our main contribution is a trained\nnoise-coupling scheme inspired by the architecture of Variational Autoencoders\n(VAE). By training a data-dependent noise emission model implemented as an\nencoder architecture, our method can indirectly learn the geometry of the\nnoise-to-data mapping, which is instead fixed by the choice of the forward\nprocess in classical CT. Empirical results across diverse image datasets show\nsignificant generative improvements, with our model outperforming baselines and\nachieving the state-of-the-art (SoTA) non-distillation CT FID on CIFAR-10, and\nattaining FID on par with SoTA on ImageNet at $64 \\times 64$ resolution in\n2-step generation. Our code is available at https:\/\/github.com\/sony\/vct .",
        "We introduce a novel approach to studying properties of processes in the\n{\\pi}-calculus based on a processes-as-formulas interpretation, by establishing\na correspondence between specific sequent calculus derivations and computation\ntrees in the reduction semantics of the recursion-free {\\pi}-calculus. Our\nmethod provides a simple logical characterisation of deadlock-freedom for the\nrecursion- and race-free fragment of the {\\pi}-calculus, supporting key\nfeatures such as cyclic dependencies and an independence of the name\nrestriction and parallel operators. Based on this technique, we establish a\nstrong completeness result for a nontrivial choreographic language: all\ndeadlock-free and race-free finite {\\pi}-calculus processes composed in\nparallel at the top level can be faithfully represented by a choreography. With\nthese results, we show how the paradigm of computation-as-derivation extends\nthe reach of logical methods for the study of concurrency, by bridging\nimportant gaps between logic, the expressiveness of the {\\pi}-calculus, and the\nexpressiveness of choreographic languages.",
        "3D Gaussian Splatting (3DGS) has gained significant attention for their\nhigh-quality novel view rendering, motivating research to address real-world\nchallenges. A critical issue is the camera motion blur caused by movement\nduring exposure, which hinders accurate 3D scene reconstruction. In this study,\nwe propose CoMoGaussian, a Continuous Motion-Aware Gaussian Splatting that\nreconstructs precise 3D scenes from motion-blurred images while maintaining\nreal-time rendering speed. Considering the complex motion patterns inherent in\nreal-world camera movements, we predict continuous camera trajectories using\nneural ordinary differential equations (ODEs). To ensure accurate modeling, we\nemploy rigid body transformations, preserving the shape and size of the object\nbut rely on the discrete integration of sampled frames. To better approximate\nthe continuous nature of motion blur, we introduce a continuous motion\nrefinement (CMR) transformation that refines rigid transformations by\nincorporating additional learnable parameters. By revisiting fundamental camera\ntheory and leveraging advanced neural ODE techniques, we achieve precise\nmodeling of continuous camera trajectories, leading to improved reconstruction\naccuracy. Extensive experiments demonstrate state-of-the-art performance both\nquantitatively and qualitatively on benchmark datasets, which include a wide\nrange of motion blur scenarios, from moderate to extreme blur.",
        "We propose a machine learning approach to the optimal control of fluid\nrestless multi-armed bandits (FRMABs) with state equations that are either\naffine or quadratic in the state variables. By deriving fundamental properties\nof FRMAB problems, we design an efficient machine learning based algorithm.\nUsing this algorithm, we solve multiple instances with varying initial states\nto generate a comprehensive training set. We then learn a state feedback policy\nusing Optimal Classification Trees with hyperplane splits (OCT-H). We test our\napproach on machine maintenance, epidemic control and fisheries control\nproblems. Our method yields high-quality state feedback policies and achieves a\nspeed-up of up to 26 million times compared to a direct numerical algorithm for\nfluid problems.",
        "In this work, a central-moment-based discrete Boltzmann method (CDBM) is\nconstructed for fluid flows with variable specific heat ratios. The central\nkinetic moments are employed to calculate the equilibrium discrete velocity\ndistribution function in the CDBM. In comparison to previous incompressible\ncentral-moment-based lattice Boltzmann method, the CDBM possesses the\ncapability of investigating compressible flows with thermodynamic\nnonequilibrium effects beyond conventional hydrodynamic models. Unlike all\nexisting DBMs which are constructed in raw-moment space, the CDBM stands out by\ndirectly providing the nonequilibrium effects related to the thermal\nfluctuation. The proposed method has been rigorously validated using benchmarks\nof the Sod shock tube, Lax shock tube, shock wave phenomena, two-dimensional\nsound wave, and the Taylor-Green vortex flow. The numerical results exhibit an\nexceptional agreement with theoretical predictions.",
        "To ease the expensive measurements during configuration tuning, it is natural\nto build a surrogate model as the replacement of the system, and thereby the\nconfiguration performance can be cheaply evaluated. Yet, a stereotype therein\nis that the higher the model accuracy, the better the tuning result would be.\nThis \"accuracy is all\" belief drives our research community to build more and\nmore accurate models and criticize a tuner for the inaccuracy of the model\nused. However, this practice raises some previously unaddressed questions,\ne.g., Do those somewhat small accuracy improvements reported in existing work\nreally matter much to the tuners? What role does model accuracy play in the\nimpact of tuning quality? To answer those related questions, we conduct one of\nthe largest-scale empirical studies to date-running over the period of 13\nmonths 24*7-that covers 10 models, 17 tuners, and 29 systems from the existing\nworks while under four different commonly used metrics, leading to 13,612 cases\nof investigation. Surprisingly, our key findings reveal that the accuracy can\nlie: there are a considerable number of cases where higher accuracy actually\nleads to no improvement in the tuning outcomes (up to 58% cases under certain\nsetting), or even worse, it can degrade the tuning quality (up to 24% cases\nunder certain setting). We also discover that the chosen models in most\nproposed tuners are sub-optimal and that the required % of accuracy change to\nsignificantly improve tuning quality varies according to the range of model\naccuracy. Deriving from the fitness landscape analysis, we provide in-depth\ndiscussions of the rationale behind, offering several lessons learned as well\nas insights for future opportunities. Most importantly, this work poses a clear\nmessage to the community: we should take one step back from the natural\n\"accuracy is all\" belief for model-based configuration tuning.",
        "We address a distributed adaptive control methodology for nonlinear\ninterconnected systems possibly affected by network anomalies. In the framework\nof adaptive approximation, the distributed controller and parameter estimator\nare designed by exploiting a backstepping approach. The stability of the\ndistributed control system under anomalies is analyzed, where both local and\nneighboring anomaly effects are considered. To quantify the resilience of the\ninterconnected system under the action of network anomalies, we derive bounds\non the duration of each anomaly and the resting time between two consecutive\nanomalies. Specifically, when each anomaly duration is smaller than our\ndesigned upper bound, the interconnected system controlled by the distributed\napproximation-based controller remains asymptotically stable. Moreover, if the\nresting time between two consecutive anomalies is larger than the proposed\nbound, then all signals of the control system are guaranteed to be bounded. In\nthe paper, we show that under the action of the proposed distributed adaptive\ncontroller, the interconnected system remains stable in the presence of network\nanomalies, with both the qualitative and quantitative resilient conditions.\nExtensive simulation results show the effectiveness of our theoretical results.",
        "Skyline detection plays an important role in geolocalizaion, flight control,\nvisual navigation, port security, etc. The appearance of the sky and non-sky\nareas are variable, because of different weather or illumination environment,\nwhich brings challenges to skyline detection. In this research, we proposed the\nYUNet algorithm, which improved the YOLOv11 architecture to segment the sky\nregion and extract the skyline in complicated and variable circumstances. To\nimprove the ability of multi-scale and large range contextual feature fusion,\nthe YOLOv11 architecture is extended as an UNet-like architecture, consisting\nof an encoder, neck and decoder submodule. The encoder extracts the multi-scale\nfeatures from the given images. The neck makes fusion of these multi-scale\nfeatures. The decoder applies the fused features to complete the prediction\nrebuilding. To validate the proposed approach, the YUNet was tested on\nSkyfinder and CH1 datasets for segmentation and skyline detection respectively.\nOur test shows that the IoU of YUnet segmentation can reach 0.9858, and the\naverage error of YUnet skyline detection is just 1.36 pixels. The\nimplementation is published at\nhttps:\/\/github.com\/kuazhangxiaoai\/SkylineDet-YOLOv11Seg.git.",
        "Let $S=K[x_1, \\ldots,x_n]$ denote the polynomial ring in $n$ variables over a\nfield $K$ and let $I \\subset S$ be a monomial ideal. For a vector\n$\\mathfrak{c}\\in\\mathbb{N}^n$, we set $I_{\\mathfrak{c}}$ to be the ideal\ngenerated by monomials belonging to $I$ whose exponent vectors are\ncomponentwise bounded above by $\\mathfrak{c}$. Also, let\n$\\delta_{\\mathfrak{c}}(I)$ be the largest integer $k$ such that\n$(I^k)_{\\mathfrak{c}}\\neq 0$. It is shown that for every graph $G$ with edge\nideal $I(G)$, the ideal $(I(G)^{\\delta_{\\mathfrak{c}}(I)})_{\\mathfrak{c}}$ is a\npolymatroidal ideal. Moreover, we show that for each integer $s=1, \\ldots\n\\delta_{\\mathfrak{c}}(I(G))$, the Castelnuovo--Mumford regularity of\n$(I(G)^s)_{\\mathfrak{c}}$ is bounded above by $\\delta_{\\mathfrak{c}}(I(G))+s$.",
        "Many world-known scientists and engineers like G. Breit, G. Budker, G.\nCharpak, G. Gamow, M. Goldhaber, A. Ioffe, S. Korolyov, E. Lifshitz, M.\nOstrogradsky, S. Timoshenko, V. Veksler were born in Ukraine, while some, like\nL. Landau and M. Bogolyubov, started their career there. Reclaiming their\nscientific legacy as well as that of many others helps to promote Ukrainian\ncontributions to particle physics both inside and outside of Ukraine and to\nmotivate the next generation of Ukrainian scientists in the time of war. We\nwill present the status of Ukrainian scientific infrastructure two years after\nthe start of the full-scale invasion and past, present and expected future\ncontributions of Ukrainian scientists to CERN.",
        "A surprising connection exists between double-scaled SYK at infinite\ntemperature, and large N QCD. The large N expansions of the two theories have\nthe same form; the 't Hooft limit of QCD parallels the fixed p limit of SYK\n(for a theory with p-fermion interactions), and the limit of fixed gauge\ncoupling g -- the flat space limit in AdS\/CFT -- parallels the double-scaled\nlimit of SYK. From the holographic perspective fixed g is the far more\ninteresting limit of gauge theory, but very little is known about it. DSSYK\nallows us to explore it in a more tractable example. The connection is\nillustrated by perturbative and non-perturbative DSSYK calculations, and\ncomparing the results with known properties of Yang Mills theory. The\ncorrespondence is largely independent of the conjectured duality between DSSYK\nand de Sitter space, but may have a good deal to tell us about it.",
        "In this paper we study the cyclicity of sliding cycles for regularized\npiecewise smooth visible-invisible two-folds, in the presence of singularities\nof the Filippov sliding vector field located away from two-folds. We obtain a\nslow-fast system after cylindrical blow-up and use a well-known connection\nbetween the divergence integral along orbits and transition maps for vector\nfields. Since properties of the divergence integral depend on the location and\nmultiplicity of singularities, we divide the sliding cycles into different\nclasses, which can then produce different types of cyclicity results. As an\nexample, we apply our results to regularized piecewise linear systems.",
        "In this paper, we present a speaker-independent dysarthric speech recognition\nsystem, with a focus on evaluating the recently released Speech Accessibility\nProject (SAP-1005) dataset, which includes speech data from individuals with\nParkinson's disease (PD). Despite the growing body of research in dysarthric\nspeech recognition, many existing systems are speaker-dependent and adaptive,\nlimiting their generalizability across different speakers and etiologies. Our\nprimary objective is to develop a robust speaker-independent model capable of\naccurately recognizing dysarthric speech, irrespective of the speaker.\nAdditionally, as a secondary objective, we aim to test the cross-etiology\nperformance of our model by evaluating it on the TORGO dataset, which contains\nspeech samples from individuals with cerebral palsy (CP) and amyotrophic\nlateral sclerosis (ALS). By leveraging the Whisper model, our\nspeaker-independent system achieved a CER of 6.99% and a WER of 10.71% on the\nSAP-1005 dataset. Further, in cross-etiology settings, we achieved a CER of\n25.08% and a WER of 39.56% on the TORGO dataset. These results highlight the\npotential of our approach to generalize across unseen speakers and different\netiologies of dysarthria.",
        "We employ deep learning within holographic duality to investigate $T$-linear\nresistivity, a hallmark of strange metals. Utilizing Physics-Informed Neural\nNetworks, we incorporate boundary data for $T$-linear resistivity and bulk\ndifferential equations into a loss function. This approach allows us to derive\ndilaton potentials in Einstein-Maxwell-Dilaton-Axion theories, capturing\nessential features of strange metals, such as $T$-linear resistivity and linear\nspecific heat scaling. We also explore the impact of the resistivity slope on\ndilaton potentials. Regardless of slope, dilaton potentials exhibit universal\nexponential growth at low temperatures, driving $T$-linear resistivity and\nmatching infrared geometric analyses. At a specific slope, our method\nrediscovers the Gubser-Rocha model, a well-known holographic model of strange\nmetals. Additionally, the robustness of $T$-linear resistivity at higher\ntemperatures correlates with the asymptotic AdS behavior of the dilaton\ncoupling to the Maxwell term. Our findings suggest that deep learning could\nhelp uncover mechanisms in holographic condensed matter systems and advance our\nunderstanding of strange metals.",
        "The increasing reliance on Global Navigation Satellite Systems (GNSS),\nparticularly the Global Positioning System (GPS), underscores the urgent need\nto safeguard these technologies against malicious threats such as spoofing and\njamming. As the backbone for positioning, navigation, and timing (PNT) across\nvarious applications including transportation, telecommunications, and\nemergency services GNSS is vulnerable to deliberate interference that poses\nsignificant risks. Spoofing attacks, which involve transmitting counterfeit\nGNSS signals to mislead receivers into calculating incorrect positions, can\nresult in serious consequences, from navigational errors in civilian aviation\nto security breaches in military operations. Furthermore, the lack of inherent\nsecurity measures within GNSS systems makes them attractive targets for\nadversaries. While GNSS\/GPS jamming and spoofing systems consist of numerous\ncomponents, the ability to distinguish authentic signals from malicious ones is\nessential for maintaining system integrity. Recent advancements in machine\nlearning and deep learning provide promising avenues for enhancing detection\nand mitigation strategies against these threats. This paper addresses both\nspoofing and jamming by tackling real-world challenges through machine\nlearning, deep learning, and computer vision techniques. Through extensive\nexperiments on two real-world datasets related to spoofing and jamming\ndetection using advanced algorithms, we achieved state of the art results. In\nthe GNSS\/GPS jamming detection task, we attained approximately 99% accuracy,\nimproving performance by around 5% compared to previous studies. Additionally,\nwe addressed a challenging tasks related to spoofing detection, yielding\nresults that underscore the potential of machine learning and deep learning in\nthis domain.",
        "We consider multiband BCS superconductors that exhibit time-reversal symmetry\nand uniform pairing, and analyze their dynamic density and spin structure\nfactors using linear-response theory within the mean-field BCS-BEC crossover\nframework at zero temperature. Our results for the multi-orbital Hubbard model\nsatisfy the associated f-sum rules in several limits. In particular, in the\nstrong-coupling limit, they coincide with those of a weakly-interacting Bose\ngas of Cooper pairs, where the low-energy collective Goldstone modes serve as\nBogoliubov phonons. We further reveal that the quantum-geometric origin of the\nlow-energy structure factors, along with related observables such as the\nsuperfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be\ntraced all the way back to the effective-mass theorem for Bloch bands in this\nlimit. As an illustration, we investigate the pyrochlore-Hubbard model\nnumerically and demonstrate that the Goldstone modes are the only relevant\ncollective degrees of freedom in the flat-band regime.",
        "Dichotomous Image Segmentation (DIS) is a high-precision object segmentation\ntask for high-resolution natural images. The current mainstream methods focus\non the optimization of local details but overlook the fundamental challenge of\nmodeling the integrity of objects. We have found that the depth integrity-prior\nimplicit in the the pseudo-depth maps generated by Depth Anything Model v2 and\nthe local detail features of image patches can jointly address the above\ndilemmas. Based on the above findings, we have designed a novel Patch-Depth\nFusion Network (PDFNet) for high-precision dichotomous image segmentation. The\ncore of PDFNet consists of three aspects. Firstly, the object perception is\nenhanced through multi-modal input fusion. By utilizing the patch fine-grained\nstrategy, coupled with patch selection and enhancement, the sensitivity to\ndetails is improved. Secondly, by leveraging the depth integrity-prior\ndistributed in the depth maps, we propose an integrity-prior loss to enhance\nthe uniformity of the segmentation results in the depth maps. Finally, we\nutilize the features of the shared encoder and, through a simple depth\nrefinement decoder, improve the ability of the shared encoder to capture subtle\ndepth-related information in the images. Experiments on the DIS-5K dataset show\nthat PDFNet significantly outperforms state-of-the-art non-diffusion methods.\nDue to the incorporation of the depth integrity-prior, PDFNet achieves or even\nsurpassing the performance of the latest diffusion-based methods while using\nless than 11% of the parameters of diffusion-based methods. The source code at\nhttps:\/\/github.com\/Tennine2077\/PDFNet.",
        "Stochastic differential equations (SDEs) describe dynamical systems where\ndeterministic flows, governed by a drift function, are superimposed with random\nfluctuations dictated by a diffusion function. The accurate estimation (or\ndiscovery) of these functions from data is a central problem in machine\nlearning, with wide application across natural and social sciences alike. Yet\ncurrent solutions are brittle, and typically rely on symbolic regression or\nBayesian non-parametrics. In this work, we introduce FIM-SDE (Foundation\nInference Model for SDEs), a transformer-based recognition model capable of\nperforming accurate zero-shot estimation of the drift and diffusion functions\nof SDEs, from noisy and sparse observations on empirical processes of different\ndimensionalities. Leveraging concepts from amortized inference and neural\noperators, we train FIM-SDE in a supervised fashion, to map a large set of\nnoisy and discretely observed SDE paths to their corresponding drift and\ndiffusion functions. We demonstrate that one and the same (pretrained) FIM-SDE\nachieves robust zero-shot function estimation (i.e. without any parameter\nfine-tuning) across a wide range of synthetic and real-world processes, from\ncanonical SDE systems (e.g. double-well dynamics or weakly perturbed Hopf\nbifurcations) to human motion recordings and oil price and wind speed\nfluctuations.",
        "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. For linear scalar systems, we derive an explicit formulation\nof the uncertainty set. For multi-dimensional linear systems, we develop an\napproximate computation method leveraging interval analysis. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\nthe inference attack."
      ]
    }
  },
  {
    "id":2411.15202,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Ilf-lstm: Enhanced loss function in lstm to predict the sea surface temperature",
    "start_abstract":"Globe's primary issue is global warming, water temperatures have accompanied it as the sea surface temperature, and it is the primary attribute to balance the energy on the earth's surface. Sea surface temperature prediction is vital to climate forecast. Downwelling currents carry some of this heat to the ocean's bottom layers, which are also heating, covering far behind the increase in sea surface temperature. In deep learning models, the correct loss function will try to reduce the error and converge fast. The proposed improved loss function correctly estimates how close the predictions made by the long short-term memory match the observed values in the training data. This research considers location-specific sea surface temperature predictions using the improved loss function in the long short-term memory neural network at six different locations around India for daily, weekly, and monthly time horizons. Most existing research concentrated on periodic forecasts, but this paper focused on daily, weekly, and monthly predictions. The improved loss function\u2014long short-term memory, achieved 98.7% accuracy, and this improved loss function overcomes the limitations of the existing techniques and reduces the processing time to\u2009~\u20090.35 s. In this research, the sea surface temperature prediction using the improved loss function in the long short-term memory neural network gives better results than the standard prediction models and other existing techniques by considering the long-time dependencies and obtaining features from the spatial data.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "The Great Barrier Reef: an environmental history"
      ],
      "abstract":[
        "Reconstructing changes in the Great Barrier Reef 15 3 The natural context of 33 4 spread European settlement coastal Queensland 43 5 beche--de ... mer, pearl shell and trochus fisheries 55 6 Impacts on marine turtles 72 7 dugongs 95 8 whales, sharks fish 9 impacts coral collecting 10 guano rock phosphate mining 11 vi Contents 12 Other reefs 13 Changes island biota 14 Conclusion"
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Existence of Viscosity Solutions to Abstract Cauchy Problems via\n  Nonlinear Semigroups",
        "On the Baum-Connes conjecture for $D_{\\infty}$",
        "Abundance of spin liquids in the $S=1$ bilinear-biquadratic model on the\n  pyrochlore lattice, and its application to $\\mathrm{NaCaNi}_2\\mathrm{F}_7$",
        "Coherence DeepClean: Toward autonomous denoising of gravitational-wave\n  detector data",
        "On Stein spaces with finite homotopy rank-sum",
        "Distribution amplitudes of heavy-light pseudo-scalar and vector mesons\n  from Dyson-Schwinger equations framework",
        "Effects of particle angularity on granular self-organization",
        "Random matrices acting on sets: Independent columns",
        "Preons, Braid Topology, and Representations of Fundamental Particles",
        "COMPLETED CYCLES LEAKY HURWITZ NUMBERS",
        "Oriented diameter of the complete tripartite graph (III)",
        "Trends and Reversion in Financial Markets on Time Scales from Minutes to\n  Decades",
        "Thermostats without conjugate points",
        "Rethinking Approximate Gaussian Inference in Classification",
        "Dynamically Learning to Integrate in Recurrent Neural Networks",
        "Ultrafast neural sampling with spiking nanolasers",
        "Probing the Merger Rates of Supermassive Black Holes and Galaxies with\n  Gravitational Waves",
        "Discovering Dynamics with Kolmogorov Arnold Networks: Linear Multistep\n  Method-Based Algorithms and Error Estimation",
        "Probing $D_s^*$-meson longitudinal twist-2 LCDA",
        "Gradient-based Explanations for Deep Learning Survival Models",
        "Puncture loops on a non-orientable surface",
        "Understanding the Capabilities and Limitations of Weak-to-Strong\n  Generalization",
        "On Fractional Generalizations of the Logistic Map and their Applications",
        "Thermoelectric and heavy quark transport coefficients of hot QCD matter\n  in the presence of magnetic field",
        "When Do Transformers Outperform Feedforward and Recurrent Networks? A\n  Statistical Perspective",
        "Observation of the $\\Lambda_b^0 \\to J\/\\psi \\Xi^- K^+$ and $\\Xi_b^0 \\to\n  J\/\\psi \\Xi^- \\pi^+$ decays",
        "Study of 14.1 MeV Neutron Moderation in Beryllium",
        "Characterising the Surface Resistance of Laser-Treated LHC Beam Screens\n  with the Shielded Pair Method",
        "Towards an integrative approach to the study of brain-environment\n  interactions in human and non-human primate"
      ],
      "abstract":[
        "In this work, we provide conditions for nonlinear monotone semigroups on\nlocally convex vector lattices to give rise to a generalized notion of\nviscosity solutions to a related nonlinear partial differential equation. The\nsemigroup needs to satisfy a convexity estimate, so called $K$-convexity,\nw.r.t. another family of operators, defined on a potentially larger locally\nconvex vector lattice. We then show that, under mild continuity requirements on\nthe bounding family of operators, the semigroup yields viscosity solutions to\nthe abstract Cauchy problem given in terms of its generator in the larger\nlocally convex vector lattice. We apply our results to drift control problems\nfor infinite-dimensional L\\'evy processes and robust optimal control problems\nfor infinite-dimensional Ornstein-Uhlenbeck processes.",
        "We make an exposition of the proof of the Baum-Connes conjecture for the\ninfinite dihedral group following the ideas of Higson and Kasparov.",
        "Long considered the ''poor cousins'' of spin-1\/2 systems, magnets built of\nspin-1 moments have recently come to fore as a rich source of novel phases of\nmatter. Here we explore the phases which arise in a spin-1 magnet on the\npyrochlore lattice, once biquadratic interactions are taken into account. Using\na combination of variational and Monte Carlo techniques, built around the exact\ntreatment of spin-1 at the level of a single site, we uncover seven distinct\nspin liquid phases. Dynamical calculations for one of these spin liquids are\nshown to be in good agreement with inelastic neutron scattering on the spin-1\npyrochlore $\\mathrm{NaCaNi}_2\\mathrm{F}_7$. These results suggest that the\nrange of spin liquid phases found in spin-1 pyrochlores may be even richer than\nin materials with (pseudo-)spin-1\/2 moments.",
        "Technical and environmental noise in ground-based laser interferometers\ndesigned for gravitational-wave observations like Advanced LIGO, Advanced Virgo\nand KAGRA, can manifest as narrow (<1Hz) or broadband ($10'$s or even $100'$s\nof Hz) spectral lines and features in the instruments' strain amplitude\nspectral density. When the sources of this noise cannot be identified or\nremoved, in cases where there are witness sensors sensitive to this noise\nsource, denoising of the gravitational-wave strain channel can be performed in\nsoftware, enabling recovery of instrument sensitivity over affected frequency\nbands. This noise hunting and removal process can be particularly challenging\ndue to the wealth of auxiliary channels monitoring the interferometry and the\nenvironment and the non-linear couplings that may be present. In this work, we\npresent a comprehensive analysis approach and corresponding cyberinfrastructure\nto promptly identify and remove noise in software using machine learning\ntechniques. The approach builds on earlier work (referred to as DeepClean) in\nusing machine learning methods for linear and non-linear regression of noise.\nWe demonstrate how this procedure can be operated and optimized in a tandem\nfashion close to online data taking; it starts off with a coherence monitoring\nanalysis that first singles out and prioritizes witness channels that can then\nbe used by DeepClean. The resulting denoised strain by DeepClean reflects a\n1.4\\% improvement in the binary neutron star range, which can translate into a\n4.3\\% increase in the sensitive volume. This cyber infrastructure we refer to\nas Coherence DeepClean, or CDC, is a significant step toward autonomous\noperations of noise subtraction for ground-based interferometers.",
        "A topological space (not necessarily simply connected) is said to have finite\nhomotopy rank-sum if the sum of the ranks of all higher homotopy groups (from\nthe second homotopy group onward) is finite. In this article, we consider Stein\nspaces of arbitrary dimension satisfying the above rational homotopy theoretic\nproperty, although most of this article focuses on Stein surfaces only. We\ncharacterize all Stein surfaces satisfying the finite homotopy rank-sum\nproperty. In particular, if such a Stein surface is affine and every element of\nits fundamental group is finite, it is either simply connected or has a\nfundamental group of order $2$. A detailed classification of the smooth complex\naffine surfaces of the non-general type satisfying the finite homotopy rank-sum\nproperty is obtained. It turns out that these affine surfaces are\nEilenberg--MacLane spaces whenever the fundamental group is infinite.",
        "We systematically investigate leading-twist distribution amplitudes of ground\nstate heavy-light pseudo-scalar and vector mesons, the results of $B^*$,\n$B^*_s$, $B_c^*$ mesons are reported for the first time within the\nDyson-Schwinger equations framework. A novel numerical method for calculating\nMellin moments is proposed, which can avoid extrapolation or fitting in\nprevious similar studies. Based on it, we calculate the first eight Mellin\nmoments of mesons and reconstruct their distribution amplitudes. It is found\nthat, in flavor-asymmetric systems, distribution amplitude $\\phi(x)$ is skewed\nto one side, with the position of the maximum $\\sim M^f_E\/(M^f_E+M^g_E)$, where\n$M_E$ is Euclidean constituent quark mass and $f\/g$ denote the flavor of\nheavier\/lighter quark in the meson, respectively. For systems with the same\nvalence quark structure, the first Mellin moments follow the relation $\\langle\n\\xi \\rangle_{0^-} < \\langle \\xi \\rangle^{\\|}_{1^-} < \\langle \\xi\n\\rangle^{\\perp}_{1^-}$, where $\\xi = 2x - 1$ and $x$ is the momentum fraction\ncarried by the heavier quark. Our predictions can be compared with experimental\ndata and further theoretical calculations in the future, and the results of\nlight mesons such as $\\pi$, $K$, $\\rho$ are consistent with recent lattice\ndata.",
        "Recent studies of two-dimensional poly-disperse disc systems revealed a\ncoordinated self-organisation of cell stresses and shapes, with certain\ndistributions collapsing onto a master form for many processes, size\ndistributions, friction coefficients, and cell orders. Here we examine the\neffects of grain angularity on the indicators of self-organisation, using\nsimulations of bi-disperse regular $N$-polygons and varying $N$ systematically.\nWe find that: the strong correlation between local cell stresses and\norientations, as well as the collapses of the conditional distributions of\nscaled cell stress ratios to a master Weibull form for all cell orders $k$, are\nindependent of angularity and friction coefficient. In contrast, increasing\nangularity makes the collapses of the conditional distributions sensitive to\nchanges in the friction coefficient.",
        "We study random matrices with independent subgaussian columns. Assuming each\ncolumn has a fixed Euclidean norm, we establish conditions under which such\nmatrices act as near-isometries when restricted to a given subset of their\ndomain. We show that, with high probability, the maximum distortion caused by\nsuch a matrix is proportional to the Gaussian complexity of the subset, scaled\nby the subgaussian norm of the matrix columns. This linear dependence on the\nsubgaussian norm is a new phenomenon, as random matrices with independent rows\nor independent entries typically exhibit superlinear dependence. As a\nconsequence, normalizing the columns of random sparse matrices leads to\nstronger embedding guarantees.",
        "In particle phenomenology, preon models study compositional rules of standard\nmodel interactions. In spite of empirical success, mathematical underpinnings\nof preon models in terms of group representation theory have not been fully\nworked out. Here, we address this issue while clarifying the relation between\ndifferent preon models. In particular, we focus on two prominent models:\nBilson-Thompson's helon model, and Lambek's 4-vector model. We determine the\nmapping between helon model particle states and representation theory of Lie\nalgebras. Braided ribbon diagrams of the former represent on-shell states of\nspinors of the Lorentz group. Braids correspond to chirality, and twists, to\ncharges. We note that this model captures only the $SU(3)_c\\times U(1)_{em}$\nsector of the standard model. We then map the twists of helon diagrams to the\nweight polytope of $SU(3)_c \\times U(1)_{em}$. The braid structure maps to\nchiral states of fermions. We also show that Lambek's 4-vector can be recovered\nfrom helon diagrams. Alongside, we introduce a new 5-vector representation\nderived from the weight lattice. This representation contains both, the correct\ninteractions found in 4-vectors and the inclusion of chirality found in helons.\nAdditionally, we demonstrate topological analogues of CPT transformations in\nhelon diagrams. Interestingly, the braid diagrams of the helon model are the\nonly ones that are self-consistent with CPT invariance. In contrast to\nfield-theoretic approaches, the compositional character of preon models offers\nan analogous particle-centric perspective on fundamental interactions.",
        "We introduce $(r+1)$-completed cycles $k$-leaky Hurwitz numbers and prove\npiecewise polynomiality as well as establishing their chamber polynomiality\nstructure and their wall crossing formulae. For $k=0$ the results recover\nprevious results of Shadrin-Spitz-Zvonkine. The specialization for $r=1$\nrecovers Hurwitz numbers that are close to the ones studied by\nCavalieri-Markwig-Ranganathan and Cavalieri-Markwig-Schmitt. The ramifications\ndiffer by a lower order torus correction, natural from the Fock space\nperspective, not affecting the genus zero enumeration, nor the enumeration for\nleaky parameter values $k = \\pm 1$ in all genera.",
        "Given a bridgeless graph $G$, let $\\mathbb{D}(G)$ be the set of all strong\norientations of $G$, and define the oriented diameter $f(G)$ of $G$ to be the\nminimum of diameters $diam(D)$ among all the strong orientations $D\\in\n\\mathbb{D}(G)$, i.e., $f(G)=\\min\\{diam(D)\\mid D\\in \\mathbb{D}(G)\\}$. In this\npaper, we determine the oriented diameter of complete tripartite graph\n$K(3,p,q)$ for $p\\geqslant 5$. Combining with the previous results, the\noriented diameter of complete tripartite graph $K(3,p,q)$ are known.",
        "We empirically analyze the reversion of financial market trends with time\nhorizons ranging from minutes to decades. The analysis covers equities,\ninterest rates, currencies and commodities and combines 14 years of futures\ntick data, 30 years of daily futures prices, 330 years of monthly asset prices,\nand yearly financial data since medieval times.\n  Across asset classes, we find that markets are in a trending regime on time\nscales that range from a few hours to a few years, while they are in a\nreversion regime on shorter and longer time scales. In the trending regime,\nweak trends tend to persist, which can be explained by herding behavior of\ninvestors. However, in this regime trends tend to revert before they become\nstrong enough to be statistically significant, which can be interpreted as a\nreturn of asset prices to their intrinsic value. In the reversion regime, we\nfind the opposite pattern: weak trends tend to revert, while those trends that\nbecome statistically significant tend to persist.\n  Our results provide a set of empirical tests of theoretical models of\nfinancial markets. We interpret them in the light of a recently proposed\nlattice gas model, where the lattice represents the social network of traders,\nthe gas molecules represent the shares of financial assets, and efficient\nmarkets correspond to the critical point. If this model is accurate, the\nlattice gas must be near this critical point on time scales from 1 hour to a\nfew days, with a correlation time of a few years.",
        "We generalize Hopf's theorem to thermostats: the total thermostat curvature\nof a thermostat without conjugate points is non-positive, and vanishes only if\nthe thermostat curvature is identically zero. We further show that, if the\nthermostat curvature is zero, then the flow has no conjugate points, and the\nGreen bundles collapse almost everywhere. Given a thermostat without conjugate\npoints, we prove that the Green bundles are transversal everywhere if and only\nif it admits a dominated splitting. Finally, we provide an example showing that\nHopf's rigidity theorem on the 2-torus cannot be extended to thermostats. It is\nalso the first example of a thermostat with a dominated splitting which is not\nAnosov.",
        "In classification tasks, softmax functions are ubiquitously used as output\nactivations to produce predictive probabilities. Such outputs only capture\naleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian\ninference methods have been proposed, which output Gaussian distributions over\nthe logit space. Predictives are then obtained as the expectations of the\nGaussian distributions pushed forward through the softmax. However, such\nsoftmax Gaussian integrals cannot be solved analytically, and Monte Carlo (MC)\napproximations can be costly and noisy. We propose a simple change in the\nlearning objective which allows the exact computation of predictives and enjoys\nimproved training dynamics, with no runtime or memory overhead. This framework\nis compatible with a family of output activation functions that includes the\nsoftmax, as well as element-wise normCDF and sigmoid. Moreover, it allows for\napproximating the Gaussian pushforwards with Dirichlet distributions by\nanalytic moment matching. We evaluate our approach combined with several\napproximate Gaussian inference methods (Laplace, HET, SNGP) on large- and\nsmall-scale datasets (ImageNet, CIFAR-10), demonstrating improved uncertainty\nquantification capabilities compared to softmax MC sampling. Code is available\nat https:\/\/github.com\/bmucsanyi\/probit.",
        "Learning to remember over long timescales is fundamentally challenging for\nrecurrent neural networks (RNNs). While much prior work has explored why RNNs\nstruggle to learn long timescales and how to mitigate this, we still lack a\nclear understanding of the dynamics involved when RNNs learn long timescales\nvia gradient descent. Here we build a mathematical theory of the learning\ndynamics of linear RNNs trained to integrate white noise. We show that when the\ninitial recurrent weights are small, the dynamics of learning are described by\na low-dimensional system that tracks a single outlier eigenvalue of the\nrecurrent weights. This reveals the precise manner in which the long timescale\nassociated with white noise integration is learned. We extend our analyses to\nRNNs learning a damped oscillatory filter, and find rich dynamical equations\nfor the evolution of a conjugate pair of outlier eigenvalues. Taken together,\nour analyses build a rich mathematical framework for studying dynamical\nlearning problems salient for both machine learning and neuroscience.",
        "Owing to their significant advantages in terms of bandwidth, power efficiency\nand especially speed, optical neuromorphic systems have arisen as interesting\nalternatives to conventional semiconductor devices. Recently, photonic crystal\nnanolasers with excitable behaviour were first demonstrated. Depending on the\npumping strength, they emit short optical pulses -- spikes -- at various\nintervals on a nanosecond timescale. In this theoretical work, we show how\nnetworks of such photonic spiking neurons can be used for Bayesian inference\nthrough sampling from learned probability distributions. We provide a detailed\nderivation of translation rules from conventional sampling networks such as\nBoltzmann machines to photonic spiking networks and demonstrate their\nfunctionality across a range of generative tasks. Finally, we provide estimates\nof processing speed and power consumption, for which we expect improvements of\nseveral orders of magnitude over current state-of-the-art neuromorphic systems.",
        "The mergers of galaxies and supermassive black holes (SMBHs) are key drivers\nof galaxy evolution, contributing to the growth of both galaxies and their\ncentral black holes. Current projects like Pulsar Timing Arrays (PTAs) and\nupcoming missions such as the Laser Interferometer Space Antenna (LISA), Taiji,\nand Tianqin are designed to detect gravitational waves (GWs) emitted by SMBH\nbinaries during their inspiral and merger phases. We investigate the capability\nto probe the merger rates of SMBHs and their host galaxies by combining current\nPTA detections and mock GW data for LISA-like detectors, while incorporating\nobservational constraints from the $M_{\\bullet}-M_*$ relationship and galaxy\nstellar mass functions. Our findings highlight the critical role of GW\ndetections with LISA-like detectors in exploring the merger rates of galaxies\nand SMBHs and the timescale of SMBH mergers. Additionally, incorporating PTA\nconstraints on the stochastic gravitational wave background further refines\nmodel parameters and reduces uncertainties. Gravitational wave detections offer\nan independent method for estimating galaxy merger rates, providing a valuable\nconsistency check against rates derived from galaxy pair observations and\ncosmological simulations. Furthermore, comparing SMBH mass assembly through\nmergers with growth via accretion provides key insights into the evolutionary\nhistory of SMBHs, with the timescale of SMBH binary mergers playing a\nsignificant role in shaping their merger rates and merger mass assembly.",
        "Uncovering the underlying dynamics from observed data is a critical task in\nvarious scientific fields. Recent advances have shown that combining deep\nlearning techniques with linear multistep methods (LMMs) can be highly\neffective for this purpose. In this work, we propose a novel framework that\nintegrates Kolmogorov Arnold Networks (KANs) with LMMs for the discovery and\napproximation of dynamical systems' vector fields. Specifically, we begin by\nestablishing precise error bounds for two-layer B-spline KANs when\napproximating the governing functions of dynamical systems. Leveraging the\napproximation capabilities of KANs, we demonstrate that for certain families of\nLMMs, the total error is constrained within a specific range that accounts for\nboth the method's step size and the network's approximation accuracy.\nAdditionally, we analyze the difference between the numerical solution obtained\nfrom solving the ordinary differential equations with the fitted vector fields\nand the true solution of the dynamical system. To validate our theoretical\nresults, we provide several numerical examples that highlight the effectiveness\nof our approach.",
        "In this paper, we carry on an investigation of the semileptonic decays\n$B_s\\to D_s^*\\ell \\bar\\nu_{\\ell}$. Firstly, we derive the moments of the\n$D_s^*$-meson longitudinal leading-twist light-cone distribution amplitude\n(LCDA) based on QCD sum rules within background field theory framework.\nConsidering the contributions of the vacuum condensates up to dimension-six,\nits first ten non-zero $\\xi$-moments are given. Meanwhile, we construct the\n$D_s^*$-meson longitudinal leading-twist LCDA by using the light-cone harmonic\noscillator model. Then, using those moments, we fix the model parameters\n$\\alpha_{2;D_s^*}$ and $B_1^{2;D_s^*}$ by the least square method and apply\nthem to calculate $B_s \\to D_s^*$ transition form factors $A_1(q^2), A_2(q^2)$\nand $V(q^2)$ that are derived by using the QCD light-cone sum rules. At the\nlarge recoil region, we obtain $A_1(0) =0.632_{-0.135}^{+0.228}, A_2(0)\n=0.706_{-0.092}^{+0.109}$ and $V(0) =0.647_{-0.069}^{+0.076}$. Those form\nfactors are then extrapolated to the allowed whole physical $q^2$-region\nthrough the simplified series expansion. Finally, we obtain the branching\nfractions for the two decay channels $B_s\\to D_s^*\\ell\\bar\\nu_\\ell$, $\\it i.e.$\n${\\cal B}(B_s^0 \\to D_s^{*+}e^-\\bar\\nu_e)=(5.45_{-1.57}^{+2.15})\\times\n10^{-2}$, ${\\cal B}(B_s^0 \\to\nD_s^{*+}\\mu^-\\bar\\nu_\\mu)=(5.43_{-1.57}^{+2.14})\\times 10^{-2}$.",
        "Deep learning survival models often outperform classical methods in\ntime-to-event predictions, particularly in personalized medicine, but their\n\"black box\" nature hinders broader adoption. We propose a framework for\ngradient-based explanation methods tailored to survival neural networks,\nextending their use beyond regression and classification. We analyze the\nimplications of their theoretical assumptions for time-dependent explanations\nin the survival setting and propose effective visualizations incorporating the\ntemporal dimension. Experiments on synthetic data show that gradient-based\nmethods capture the magnitude and direction of local and global feature\neffects, including time dependencies. We introduce GradSHAP(t), a\ngradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and\nSurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply\nthese methods to medical data with multi-modal inputs, revealing relevant\ntabular features and visual patterns, as well as their temporal dynamics.",
        "On a connected surface $N$ with negative Euler characteristic, the free\nhomotopy class of a loop obtained by smoothing an intersection of two closed\ngeodesics may wind around a puncture. Chas and Kabiraj showed that this\nphenomenon does not occur when the surface $N$ is orientable. In this paper, we\nprove that it occurs when $N$ is non-orientable and both geodesics involved in\nthe smoothing are actually one-sided. In particular, we study a loop obtained\nby traversing a one-sided closed geodesic and the $m$-th power of another\none-sided closed geodesic for odd $m$. Then we show that its free homotopy\nclass may wind aroud a puncture at most two values of $m$. Furthermore, if two\nsuch $m$'s exist, they are consecutive odd integers.",
        "Weak-to-strong generalization, where weakly supervised strong models\noutperform their weaker teachers, offers a promising approach to aligning\nsuperhuman models with human values. To deepen the understanding of this\napproach, we provide theoretical insights into its capabilities and\nlimitations. First, in the classification setting, we establish upper and lower\ngeneralization error bounds for the strong model, identifying the primary\nlimitations as stemming from the weak model's generalization error and the\noptimization objective itself. Additionally, we derive lower and upper bounds\non the calibration error of the strong model. These theoretical bounds reveal\ntwo critical insights: (1) the weak model should demonstrate strong\ngeneralization performance and maintain well-calibrated predictions, and (2)\nthe strong model's training process must strike a careful balance, as excessive\noptimization could undermine its generalization capability by over-relying on\nthe weak supervision signals. Finally, in the regression setting, we extend the\nwork of Charikar et al. (2024) to a loss function based on Kullback-Leibler\n(KL) divergence, offering guarantees that the strong student can outperform its\nweak teacher by at least the magnitude of their disagreement. We conduct\nsufficient experiments to validate our theory.",
        "The regular logistic map was introduced in 1960s, served as an example of a\ncomplex system, and was used as an instrument to demonstrate and investigate\nthe period doubling cascade of bifurcations scenario of transition to chaos. In\nthis paper, we review various fractional generalizations of the logistic map\nand their applications.",
        "The aim of this thesis is twofold: a) A comprehensive study of the\nthermoelectric response in QGP in the absence and presence of a background\nmagnetic field, b) Exploring the dynamics of heavy quarks traversing in QGP in\nthe presence of a weak background magnetic field.\n  We have evaluated the strength of the thermoelectric response in QGP\nquantified by the Seebeck and Nernst coefficients, first in the absence of a\nbackground magnetic field, and then in the presence of a strong magnetic field.\nThis is followed by the evaluation of the coefficients in the presence of a\nweak magnetic field. Each of the above-mentioned scenarios is investigated\nunder the assumption that the QGP is isotropic. This assumption is then relaxed\nby using an anisotropic distribution for the quarks, and the calculations are\nrepeated. The formalism adopted in the calculation of these coefficients is\nthat of kinetic theory, particularly, the relativistic Boltzmann equation in\nthe relaxation time approximation.\n  The other part of this thesis deals with heavy quark (HQ) dynamics in the\nQGP. HQs have been recognised as very good probes of the QGP owing to their\nlarge masses. We have calculated the HQ energy loss $dE\/dx$, longitudinal and\ntransverse momentum diffusion coeffcients $\\kappa L\/T^3$ , and spatial\ndiffusion coefficient $D_s$ , to leading order in the strong coupling $\\alpha$,\nfor both charm and bottom quarks. We consider Coulomb scattering of the HQ with\nthermal quarks and gluons to evaluate the scattering rate from which, all the\naforementioned coefficients are obtained. We find that the values of $\\kappa$'s\nincrease in the presence of a weak magnetic field (compared to the $B = 0$\ncase), and the anisotropy therein is also heightened in the presence of the\nmagnetic field. $D_s$ is found to decrease in the presence of magnetic field,\ncompared to its value at $B = 0$.",
        "Theoretical efforts to prove advantages of Transformers in comparison with\nclassical architectures such as feedforward and recurrent neural networks have\nmostly focused on representational power. In this work, we take an alternative\nperspective and prove that even with infinite compute, feedforward and\nrecurrent networks may suffer from larger sample complexity compared to\nTransformers, as the latter can adapt to a form of dynamic sparsity.\nSpecifically, we consider a sequence-to-sequence data generating model on\nsequences of length $N$, in which the output at each position depends only on\n$q$ relevant tokens with $q \\ll N$, and the positions of these tokens are\ndescribed in the input prompt. We prove that a single-layer Transformer can\nlearn this model if and only if its number of attention heads is at least $q$,\nin which case it achieves a sample complexity almost independent of $N$, while\nrecurrent networks require $N^{\\Omega(1)}$ samples on the same problem. If we\nsimplify this model, recurrent networks may achieve a complexity almost\nindependent of $N$, while feedforward networks still require $N$ samples.\nConsequently, our proposed sparse retrieval model illustrates a natural\nhierarchy in sample complexity across these architectures.",
        "The first observation of the $\\Xi_b^0 \\to J\/\\psi \\Xi^- \\pi^+$ decay and the\nmost precise measurement of the branching fraction of the $\\Lambda_b^0 \\to\nJ\/\\psi \\Xi^- K^+$ decay are reported, using proton-proton collision data from\nthe LHCb experiment collected in 2016--2018 at a centre-of-mass energy of\n13~TeV, corresponding to an integrated luminosity of 5.4~fb$^{-1}$. Using the\n$\\Lambda_b^0 \\to J\/\\psi \\Lambda$ and $\\Xi_b^0 \\to J\/\\psi \\Xi^-$ decays as\nnormalisation channels, the ratios of branching fractions are measured to be:\n\\[ \\frac{\\mathcal{B}(\\Lambda_b^0 \\to J\/\\psi \\Xi^- K^+)}{\\mathcal{B}(\\Lambda_b^0\n\\to J\/\\psi \\Lambda)} = (1.17 \\pm 0.14 \\pm 0.08)\\times 10^{-2} \\, , \\] \\[\n\\frac{\\mathcal{B}(\\Xi_b^0 \\to J\/\\psi \\Xi^- \\pi^+)}{\\mathcal{B}(\\Xi_b^0 \\to\nJ\/\\psi \\Xi^-)} = (11.9 \\pm 1.4 \\pm 0.6)\\times 10^{-2}\\, , \\] where the first\nuncertainty is statistical and the second systematic.",
        "This study investigates the moderation of 14.1 MeV neutrons in a natural\nberyllium moderator arranged in a spherical geometry. The neutron interactions\nand moderation efficiency were analyzed using Monte Carlo simulations with the\nGEANT4 toolkit. Various sphere radii were tested to determine the optimal\nmoderator thickness for neutron thermalization.",
        "The presence of strong electron clouds in the quadrupole magnetic field\nregions of the Large Hadron Collider (LHC) leads to considerable heating that\nposes challenges for the cryogenic cooling system, and under certain conditions\nto proton beam quality deterioration. Research is being conducted on\nlaser-treated inner beam screen surfaces for the upgraded High-Luminosity LHC\nto mitigate this issue. Laser-induced surface structuring, a technique that\neffectively roughens surfaces, has been shown to reduce secondary electron\nemission; an essential factor in controlling electron cloud formation.\nConversely, the resulting surface roughening also alters the material's surface\nimpedance, potentially impacting beam stability and increasing beam-induced\nresistive wall heating. Different laser treatment patterns have been applied to\nLHC beam screens to estimate this potential impact and assessed for their\nmicrowave responses.",
        "By retracing my scientific journey that began 20 years ago, I highlight in\nthis thesis the need to consider the organization of the brain, which is\ncertainly globally hierarchical, but also highly distributed and mixed in the\nneuronal response of its different areas (by focusing on the sensorimotor\ncortex-basal ganglia network). I also emphasize the importance of adopting\nbehavioral paradigms that reflect as much as possible the characteristics of\nthe scenarios encountered in the real life of animals. And finally, I mention\nthe importance of \"disintegrating\" the way data analysis is traditionally\ncarried out, and of taking into account the dynamic nature of behavior, for\nexample by favoring the study of behavioral variables and so-called \"latent\"\nneuronal activities. I conclude this thesis by presenting the vision of my\nideal laboratory in a perspective of 5 to 10 years from today. This laboratory\nwould have two experimental devices, a first, classic, for carrying out tests\nin a constrained and therefore unnatural environment, the data of which would\nbe compared to those from a second device called \"naturalistic\" in which the\nanimals could potentially express their entire behavioral repertoire. The tasks\ntested would be characterized by strong ecological principles, such as in those\nsimulating the properties of foraging, and the data would make it possible to\ntest hypotheses based on the progressive addition of ecological components in\nthese tasks, all this in order to maintain control over the interpretability of\nthese data which are complex by nature."
      ]
    }
  },
  {
    "id":2411.06741,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Learning Polynomials with Neural Networks",
    "start_abstract":"We study the effectiveness of learning low degree polynomials using neural networks by gradient descent method. While have been shown to great expressive power, and has widely used in practice for networks, few theoretical guarantees are known such methods. In particular, it is well that can get stuck at local minima, even simple classes target functions. this paper, we present several positive results support networks. focus on twolayer where bottom layer a set non-linear hidden nodes, top node linear function, similar Barron (1993). First show randomly initialized network with sufficiently many units, generic algorithm learns any polynomial, assuming initialize weights randomly. Secondly, if use complex-valued (the function still be real), then under suitable conditions, there no robust minima: always escape minimum performing random perturbation. This property does not hold real-valued weights. Thirdly, discuss whether sparse learned small size dependent sparsity function.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b20"
      ],
      "title":[
        "Physics-informed machine learning"
      ],
      "abstract":[
        "Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems."
      ],
      "categories":[
        "physics.chem-ph"
      ]
    },
    "list":{
      "title":[
        "Non-Commutative fluid: an alternative source of cosmic acceleration",
        "Gradient estimates for the fractional $p$-Poisson equation",
        "Optimizing compilation of error correction codes for 2xN quantum dot\n  arrays and its NP-hardness",
        "FOSS solution for Molecular Dynamics Simulation Automation and\n  Collaboration with MDSGAT",
        "Leveraging the Bias-Variance Tradeoff in Quantum Chemistry for Accurate\n  Negative Singlet-Triplet Gap Predictions: A Case for Double-Hybrid DFT",
        "Galaxy mass profiles with convolutional neural networks",
        "Auto-Balancer: Harnessing idle network resources for enhanced market\n  stability",
        "On a planetary forcing of global seismicity",
        "Magnetic Interactions in the Polar Ferrimagnet with a Bipartite\n  Structure",
        "Chiral supersolid and dissipative time crystal in Rydberg-dressed\n  Bose-Einstein condensates with Raman-induced spin-orbit coupling",
        "Sub-MHz Radio Background from Ultralight Dark Photon Dark Matter",
        "Kolyvagin's conjecture for modular forms at non-ordinary primes",
        "Global $C^{1,\\alpha}$ regularity for Monge-Amp\\`ere equations on planar\n  convex domains",
        "Investigation of Inverse Velocity Dispersion in a Solar Energetic\n  Particle Event Observed by Solar Orbiter",
        "Averaging over the circles the gaussian free field in the Poincar{\\'e}\n  disk",
        "Functional limit theorems for Gaussian-fed queueing network in light and\n  heavy traffic",
        "Orbits of very distant asteroid satellites",
        "VEGA: Voids idEntification using Genetic Algorithm",
        "Can circumstellar interaction explain the strange light curve features\n  of Type Ib\/c supernovae?",
        "Entropic costs of the quantum-to-classical transition in a microscopic\n  clock",
        "Partial Resolution of the Erd\\\"os-Straus, Sierpinski, and Generalized\n  Erd\\\"os-Straus Conjectures Using New Analytical Formulas",
        "Irreducible components of affine Lusztig varieties",
        "Network Goodness-of-Fit for the block-model family",
        "A multi-component phase-field model for T1 precipitates in Al-Cu-Li\n  alloys",
        "Framed Blob Monoids",
        "Two- and three-meson scattering amplitudes with physical quark masses\n  from lattice QCD",
        "Magnetic imaging under high pressure with a spin-based quantum sensor\n  integrated in a van der Waals heterostructure",
        "Multi-Scale Conformal Prediction: A Theoretical Framework with Coverage\n  Guarantees",
        "Observable Primordial Gravitational Waves from Non-minimally Coupled\n  $R^2$ Palatini Modified Gravity"
      ],
      "abstract":[
        "We have developed a Hubble function based on Newtonian Cosmology using\nnon-commutative fluid equations. Our Hubble function contains cosmic fluids\nwith the signature of a new cosmological parameter $\\sigma$, motivated by a\nnon-commutative Poisson bracket structure. Interestingly, this Hubble function\ndoes not include any external fluid content related to dark energy or the\nCosmological constant; the parameter $\\sigma$ acts as the source of accelerated\nexpansion. In this work, we aim to explain the phenomenon of the accelerating\nexpansion of the universe without \"dark energy\". Additionally, we have verified\nthe observational bounds for $\\sigma$ to assess its potential in explaining the\naccelerated expansion.",
        "We consider local weak solutions to the fractional $p$-Poisson equation of\norder $s$, i.e. $\\left( - \\Delta_p\\right)^s u = f$. In the range $p>1$ and\n$s\\in \\big(\\frac{p-1}{p},1\\big)$ we prove Calder\\'on & Zygmund type estimates\nat the gradient level. More precisely, we show for any $q>1$ that\n\\begin{equation*}\n  f\\in L^{\\frac{qp}{p-1}}_{\\rm loc}\n  \\quad\\Longrightarrow\\quad\n  \\nabla u\\in L^{qp}_{\\rm loc}. \\end{equation*} The qualitative result is\naccompanied by a local quantitative estimate.",
        "The ability to physically move qubits within a register allows the design of\nhardware-specific error correction codes which can achieve fault-tolerance\nwhile respecting other constraints. In particular, recent advancements have\ndemonstrated the shuttling of electron and hole spin qubits through a quantum\ndot array with high fidelity. Exploiting this, we design an error correction\narchitecture, consisting merely of two parallel quantum dot arrays, an\nexperimentally validated architecture compatible with classical wiring and\ncontrol constraints. We develop a suite of heuristic methods for compiling any\nstabilizer error-correcting code's syndrome-extraction circuit to run with a\nminimal number of shuttling operations. In simulation, these heuristics show\nthat fault tolerance can be achieved on several contemporary quantum\nerror-correcting codes requiring only modestly-optimistic noise parameters.\nFurthermore, we demonstrate how constant column-weight qLDPC codes can be\ncompiled in a provably minimal number of shuttles that scales constantly with\ncode size using Shor-style syndrome extraction. In addition, we provide a proof\nof the NP hardness of minimizing the number of shuttle operations for codes not\nin that class.",
        "The process of setting up and successfully running Molecular Dynamics\nSimulations (MDS) is outlined to be incredibly labour and computationally\nexpensive with a very high barrier to entry for newcomers wishing to utilise\nthe benefits and insights of MDS. Here, presented, is a unique Free and\nOpen-Source Software (FOSS) solution that aims to not only reduce the barrier\nof entry for new Molecular Dynamics (MD) users, but also significantly reduce\nthe setup time and hardware utilisation overhead for even highly experienced MD\nresearchers. This is accomplished through the creation of the Molecular\nDynamics Simulation Generator and Analysis Tool (MDSGAT) which currently serves\nas a viable alternative to other restrictive or privatised MDS Graphical\nsolutions with a unique design that allows for seamless collaboration and\ndistribution of exact MD simulation setups and initialisation parameters\nthrough a single setup file. This solution is designed from the start with a\nmodular mindset allowing for additional software expansion to incorporate\nnumerous extra MDS packages and analysis methods over time",
        "Molecules that violate Hund's rule -- having first excited singlet state\n(S$_1$) below the triplet state (T$_1$) -- are rare yet promising as efficient\nlight emitters. Their high-throughput identification demands exceptionally\naccurate excited-state modeling to minimize false positives and negatives.\nBenchmarking twelve S$_1$-T$_1$ energy gaps, we find that local variants of\nADC(2) and CC2 deliver excellent accuracy and speed for screening medium-sized\nmolecules. Notably, while double-hybrid DFT approximations (e.g., B2GP-PLYP and\nPBE-QIDH) exhibit high mean errors ($>100$ meV) despite very low standard\ndeviations ($\\approx10$ meV), exploring their parameter space reveals that a\nconfiguration with 75% exchange and 55% correlation reduces the mean error to\nbelow $5$ meV -- albeit with increased variance. Using this low-bias\nparameterization as an internal reference, we correct the systematic error\nwhile maintaining low variance, effectively combining the strengths of both\nlow-bias and low-variance DFT parameterizations to enhance overall accuracy.\nOur findings suggest that low-variance DFT methods -- often overlooked due to\nhigh bias -- can serve as reliable tools for predictive modeling in\nfirst-principles molecular design.",
        "Determining the dynamical mass profiles of dispersion-supported galaxies is\nparticularly challenging due to projection effects and the unknown shape of\ntheir velocity anisotropy profile. Our goal is to develop a machine learning\nalgorithm capable of recovering dynamical mass profiles of dispersion-supported\ngalaxies from line-of-sight stellar data. Traditionally, this task relies on\ntime-consuming methods that require profile parameterization and assume\ndynamical equilibrium and spherical symmetry. We train a convolutional neural\nnetwork model using various sets of cosmological hydrodynamical simulations of\ngalaxies. By extracting projected stellar data from the simulated galaxies and\nfeeding it into the model, we obtain the posterior distribution of the\ndynamical mass profile at ten different radii. Additionally, we evaluate the\nperformance of existing literature mass estimators on our dataset. Our model\nachieves more accurate results than any literature mass estimator while also\nproviding enclosed mass estimates at radii where no previous estimators exist.\nWe confirm that the posterior distributions produced by the model are\nwell-calibrated, ensuring they provide meaningful uncertainties. However,\nissues remain, as the method loses performance when trained on one set of\nsimulations and applied to another, highlighting the importance of improving\nthe generalization of ML methods trained on specific galaxy simulations.",
        "We propose a mechanism embedded into the foundational infrastructure of a\nblockchain network, designed to improve the utility of idle network resources,\nwhilst enhancing market microstructure efficiency during block production by\nleveraging both network-owned and external capital. By systematically seeking\nto use idle network resources for internally capture arbitrageable\ninefficiencies, the mechanism mitigates extractable value leakage, reduces\nexecution frictions, and improves price formation across venues. This framework\noptimises resource allocation by incentivising an ordered set of transactions\nto be identified and automatically executed at the end of each block,\nredirecting any realised arbitrage income - to marketplaces operating on the\nhost blockchain network (and other stakeholders), which may have otherwise been\nextracted as rent by external actors. Crucially, this process operates without\nintroducing additional inventory risk, ensuring that the network remains a\nneutral facilitator of price discovery. While the systematic framework\ngoverning the distribution of these internally captured returns is beyond the\nscope of this work, reinvesting them to support the ecosystem deployed on the\nhost blockchain network is envisioned to endogenously enhance liquidity,\nstrengthen transactional efficiency, and promote the organic adoption of the\nblockchain for end users. This mechanism is designed specifically for Supra's\nblockchain and seeks to maximally utilise its highly efficient automation\nframework to enhance the blockchain network's efficiency.",
        "We have explored the temporal variability of the seismicity at global scale\nover the last 124 years, as well as its potential drivers. To achieve this, we\nconstructed and analyzed an averaged global seismicity curve for earthquakes of\nmagnitude equal or greater than 6 since 1900. Using Singular Spectrum Analysis,\nwe decomposed this curve and compared the extracted pseudo-cycles with two\nglobal geophysical parameters associated with Earth's tides: length-of-day\nvariations and sea-level changes. Our results reveal that these three\ngeophysical phenomena can be be explained with 90% accuracy, as the sum of up\nto seven periodic components, largely aligned with planetary ephemerides: 1\nyear, 3.4 years (Quasi-Biennial Oscillation, QBO), $\\sim$11 years, $\\sim$14\nyears, $\\sim$18.6 years (lunar nodal cycle), $\\sim$33 years, and $\\sim$60\nyears. We discuss these results in the framework of Laplace's theory, with a\nparticular focus on the phase relationships between seismicity, length-of-day\nvariations, and sea-level changes to further elucidate the underlying physical\nmechanisms. Finally,integrating observations from seismogenic regions, we\npropose a trigger mechanism based on solid Earth-hydrosphere interactions,\nemphasizing the key role of water-rock interactions in modulating earthquake\noccurrence.",
        "The polar magnets A$_2$Mo$_3$O$_8$ (A=Fe, Mn, Co, and Ni) feature a bipartite\nstructure, where the magnetic A$^{2+}$ ions occupy two different sites with\noctahedral and tetrahedral oxygen coordinations. This bipartite structure\nprovides a platform for the emergence of nontrivial magnetoelectric (ME)\neffects and intriguing excitation behaviors, and thus creates significant\nresearch interest. In this study, we conduct inelastic neutron scattering\nmeasurements on single crystals of Mn$_2$Mo$_3$O$_8$, an L-type ferrimagnet in\nthe A$_2$Mo$_3$O$_8$ family, to investigate its spin dynamics. The obtained\nmagnetic excitation spectra reveal two distinct magnon dispersions\ncorresponding to the octahedral and tetrahedral spins in Mn$_2$Mo$_3$O$_8$.\nThese magnon bands can be well described by a spin Hamiltonian including\nHeisenberg and single-ion anisotropy terms. Employing our effective spin model,\nwe successfully reproduce the unusual temperature dependence of the L-type\nferrimagnetic susceptibility through self-consistent mean-field theory. This\nresearch reveals the significance of the bipartite structure in determining the\nexcitation properties of the polar magnets $\\rm{A_{2}Mo_{3}O_{8}}$ and provides\nvaluable insights into the spin dynamics of L-type ferrimagnets.",
        "Spin-orbit coupling (SOC) is one of the key factors that affect the chiral\nsymmetry of matter by causing the spatial symmetry breaking of the system. We\nfind that Raman-induced SOC can induce a chiral supersolid phase with a helical\nantiskyrmion lattice in balanced Rydberg-dressed two-component Bose-Einstein\ncondensates (BECs) in a harmonic trap by modulating the Raman coupling\nstrength, strong contrast with the mirror symmetric supersolid phase containing\nskyrmion-antiskyrmion lattice pair for the case of Rashba SOC. Two ground-state\nphase diagrams are presented as a function of the Rydberg interaction strength\nand the SOC strength, as well as that of the Rydberg interaction strength and\nthe Raman coupling strength, respectively. It is shown that the interplay among\nRaman-induced SOC, soft-core long-range Rydberg interactions, and contact\ninteractions favors rich ground-state structures including half-quantum vortex\nphase, stripe supersolid phase, toroidal stripe phase with a central\nAnderson-Toulouse coreless vortex, checkerboard supersolid phase, mirror\nsymmetric supersolid phase, chiral supersolid phase and standing-wave\nsupersolid phase. In addition, the effects of rotation and in-plane quadrupole\nmagnetic field on the ground state of the system are analyzed. In these two\ncases, the chiral supersolid phase is broken and the ground state tends to form\na miscible phase. Furthermore, the stability and superfluid properties of the\ntwo-component BECs with Raman-induced SOC and Rydberg interactions in free\nspace are revealed by solving the Bogoliubov-de Gennes equation. Finally, we\ndemonstrate that when the initial state is a chiral supersolid phase the\nrotating harmonic trapped system sustains dissipative continuous time crystal\nby studying the rotational dynamic behaviors of the system.",
        "Dark photons are a well-motivated candidate for dark matter, but their\ndetection becomes challenging for ultralight masses with both experimental and\nastrophysical probes. In this work, we propose a new approach to explore this\nregime through the dark inverse Compton scattering of ultralight dark photons\nwith cosmic ray electrons and positrons. We show this process generates a\npotentially observable background radiation that is most prominent at\nfrequencies below MHz. We compute this effect using the latest cosmic ray\nmodels and radio absorption maps. Comparing it to observations of the Milky\nWay's radio spectrum from Explorer 43, Radio Astronomy Explorer 2, and the\nParker Solar Probe, we place leading constraints on the kinetic mixing of dark\nphoton dark matter for masses $\\lesssim 2 \\times 10^{-17} \\ \\rm eV$.",
        "In this article we prove a version of Kolyvagin's conjecture for modular\nforms at non-ordinary primes. In particular, we generalize the work of Wang on\na converse to a higher weight Gross-Zagier-Kolyvagin theorem in order to prove\nthe conjecture under the hypothesis that some Selmer group has rank one. The\nmain ingredients that we use in non-ordinary setting are the signed Selmer\ngroups introduced by Lei, Loeffler and Zerbes. We will also use a result of\nWan, i.e., the $p$-part of the Tamagawa number conjecture for non-ordinary\nmodular forms with analytic rank zero. Starting from the rank one case we will\nshow how to prove the full version of the conjecture.",
        "In this paper, we establish the global H\\\"older gradient estimate for\nsolutions to the Dirichlet problem of the Monge-Amp\\`ere equation $\\det D^2u =\nf$ on strictly convex but not uniformly convex domain $\\Omega$.",
        "Inverse velocity dispersion (IVD) events, characterized by higher-energy\nparticles arriving later than lower-energy particles, challenge the classical\nunderstanding of SEP events and are increasingly observed by spacecraft, such\nas Parker Solar Probe (PSP) and Solar Orbiter (SolO). However, the mechanisms\nunderlying IVD events remain poorly understood. This study aims to investigate\nthe physical processes responsible for long-duration IVD events by analyzing\nthe SEP event observed by SolO on 2022 June 7. We explore the role of evolving\nshock connectivity, particle acceleration at interplanetary (IP) shocks, and\ncross-field transport in shaping the observed particle profiles.We utilize data\nfrom Energetic Particle Detector (EPD) suite onboard SolO to analyze the\ncharacteristics of the IVD, and model the event using the Heliospheric\nEnergetic Particle Acceleration and Transport (HEPAT) model. The IVD event\nexhibited a distinct and long-duration IVD signature, across proton energies\nfrom 1 to 20 MeV and lasting for approximately 10 hours. Simulations suggest\nthat evolving shock connectivity and the evolution of shock play a primary role\nin the IVD signature, with SolO transitioning from shock flank to nose over\ntime, resulting in a gradual increase in maximum particle energy along the\nfield line. Furthermore, model results show that limited cross-field diffusion\ncan influence both the nose energy and the duration of the IVD event. This\nstudy demonstrates that long-duration IVD events are primarily driven by\nevolving magnetic connectivity along a non-uniform shock that evolves over\ntime, where the connection moves to more efficient acceleration sites as the\nshock propagates farther from the Sun. Other mechanisms, such as acceleration\ntime at the shock, may also contribute to the observed IVD features.",
        "The gaussian free field on the unit disk $D$ can be seen as a two-dimensional\nversion of the Brownian bridge on the interval [0,1]. It is intrinsically\nassociated with the Sobolev space $H_0^1 (D)$. To define the latter, we can\nchoose any metric conformally equivalent to the Euclidean metric on $D$. This\nnote is an introduction to the gaussian free field on the unit disk whose aim\nis to highlight some of the conveniences offered by hyperbolic geometryon $D$\nto describe the first properties of this probabilistic object.",
        "We consider a queueing network operating under a strictly upper-triangular\nrouting matrix with per column at most one non-negative entry. The root node is\nfed by a Gaussian process with stationary increments. Our aim is to\ncharacterize the distribution of the multivariate stationary workload process\nunder a specific scaling of the queue's service rates. In the main results of\nthis paper we identify, under mild conditions on the standard deviation\nfunction of the driving Gaussian process, in both light and heavy traffic\nparameterization, the limiting law of an appropriately scaled version (in both\ntime and space) of the joint stationary workload process. In particular, we\ndevelop conditions under which specific queueing processes of the network\neffectively decouple, i.e., become independent in the limiting regime.",
        "The very wide binary asteroid (VWBA) population is a small subset of the\npopulation of known binary and multiple asteroids made of systems with very\nwidely orbiting satellites and long orbital periods, on the order of tens to\nhundreds of days. The origin of these systems is debatable, and most members of\nthis population are poorly characterized. We have compiled all available\nhigh-angular resolution imaging archival data of VWBA systems from large\nground- and space-based telescopes. We measure the astrometric positions of the\nsatellite relative to the primary and analyze the dynamics of the satellites\nusing the Genoid genetic algorithm. Additionally, we use a NEATM thermal model\nto estimate the diameters of two systems, and we model the orbit of Litva's\ninner satellite using photometric lightcurve observations. We determine the\neffective diameters of binary systems Christophedumas and Alconrad to be 4.7 +\n0.4km and 5.2 + 0.3km respectively. We determine new orbital solutions for five\nsystems, Huenna, Litva, (3548) Eurybates, Pauling, and Alconrad. We find a\nsignificantly eccentric best-fit orbital solution for the outer satellite of\nLitva, a moderately eccentric solution for Alconrad, and a nearly circular\nsolution for Pauling. We also confirm previously reported orbital solutions for\n(379) Huenna and Eurybates. It is unlikely that BYORP expansion could be solely\nresponsible for the formation of VWBAs. It is possible that the satellites of\nthese systems were formed through YORP spin-up and then later scattered onto\nvery wide orbits. Additionally, we find that some members of the population are\nunlikely to have formed satellites through YORP spin-up, and a collisional\nformation history is favored. In particular, this applies to VWBAs within large\ndynamical families, or large VWBA systems such as Huenna and NASA's Lucy\nmission target Eurybates.",
        "Cosmic voids, the nearly empty regions nestled between walls and filaments,\nare recognized for their extensive applications in the field of cosmology and\nastrophysics. However, a consensus on the definition of voids remains elusive,\nas various void-finding methods identify different types of voids, each\ndiffering in shape and density based on the method that were used. In this\npaper, we introduce an innovative void identification method that utilizes\nGenetic Algorithm analysis. VEGA employs the Voronoi tessellation technique and\nthe Convex Hull algorithm to partition the dataset plane into distinct regions\nand calculate the volume of each region. For the first time, VEGA integrates\nGenetic Algorithm analysis with the luminosity density contrast function to\nidentify and locate the possible void region candidates. This method utilizes a\nset of grid points, which enhances the implementation of Voronoi tessellation\nand enables VEGA to more effectively access the dataset space for the\nidentification of void regions candidates, finding the center and the ultimate\nstructure of voids. Finally, we applied the VEGA and Aikio-M\\\"ah\\\"onen (AM)\nmethods to the same test dataset and compared the cosmic voids identified by\nVEGA with those identified by the AM method. This comparison demonstrated that\nthe VEGA void-finding method yields reliable results and can be effectively\napplied to various particle distributions.",
        "Context. The evolution and the surrounding of stripped-envelope supernova\nprogenitors are still under debate: some studies suggest single-star, while\nothers prefer massive binary progenitors. Moreover, the basic physical\nproperties of the exploding star and its interaction with circumstellar matter\ncould significantly modify the overall light curve features of these objects.\nTo better understand the effect of stellar evolution and circumstellar\ninteraction, systematic hydrodynamic calculations are needed.\n  Aims. Here, we test the hypothesis that circumstellar matter generated by an\nextreme episodic $\\eta$ Carinae-like eruption that occurs days or weeks before\nthe supernova explosion may explain the controversies related to the general\nlight curve features of stripped-envelope supernovae.\n  Methods. In this work, we present our bolometric light curve calculations of\nboth single- and binary progenitors generated by hydrodynamic simulations via\nMESA and SNEC. We also studied the effect of an interaction with a close,\nlow-mass circumstellar matter assumed to be created just a few days or weeks\nbefore the explosion. Beyond generating a model light curve grid, we compared\nour results with some observational data.\n  Results. We found that merely the shape of the supernova light curve could\nindicate that the cataclysmic death of the massive star happened in a binary\nsystem or was related to the explosion of a single star. Moreover, our study\nalso shows that a confined dense circumstellar matter may be responsible for\nthe strange light curve features (bumps, re-brightening, or steeper tail) of\nsome Type Ib\/c supernovae.",
        "We experimentally realize a quantum clock by using a charge sensor to count\ncharges tunneling through a double quantum dot (DQD). Individual tunneling\nevents are used as the clock's ticks. We quantify the clock's precision while\nmeasuring the power dissipated by the DQD and, separately, the charge sensor in\nboth direct-current and radio-frequency readout modes. This allows us to probe\nthe thermodynamic cost of creating ticks microscopically and recording them\nmacroscopically, which we refer to as the quantum-to-classical transition. Our\nexperiment is the first to explore the interplay between the entropy produced\nby a microscopic clockwork and its macroscopic measurement apparatus. We show\nthat the latter contribution not only dwarfs the former but also unlocks\ngreatly increased precision, because the measurement record can be exploited to\noptimally estimate time even when the DQD is at equilibrium. Our results\nsuggest that the entropy produced by the amplification and measurement of a\nclock's ticks, which has often been ignored in the literature, is the most\nimportant and fundamental thermodynamic cost of timekeeping at the quantum\nscale.",
        "This article proposes a unified analytical approach leading to a partial\nresolution of the Erdos-Straus, Sierpinski conjectures, and their\ngeneralization. We introduce two new analytical formulas under specific\nconditions of divisibility and the existence of perfect squares. Under these\nconditions, the formulas verify the conjectures even for very large numerical\nvalues. Moreover, our method reduces the problem to the search for a suitable\nperfect square, thereby opening the way to a complete proof of these\nconjectures. Notably, our second formula significantly improves upon Mordell's\nwork by demonstrating analytically the conjecture in the majority of cases\nwhere Mordell's approach fails. Furthermore, these formulas are highly\nversatile, as they provide, under the established conditions, a systematic\nmethod to decompose any fraction a\/n into a sum of three Egyptian fractions. In\nconclusion, we present open questions and conjectures to the mathematical\ncommunity regarding the generalization of these formulas.",
        "Let $\\breve{G}$ be a loop group and $\\tilde W$ be its Iwahori-Weyl group. The\naffine Lusztig variety $Y_w(\\gamma)$ describes the intersection of the Bruhat\ncell $\\mathcal{I} \\dot{w} \\mathcal{I}$ for $w \\in \\tilde W$ with the conjugacy\nclass of $\\gamma \\in \\breve{G}$, while the affine Deligne-Lusztig variety\n$X_w(b)$ describes the intersection of the Bruhat cell $\\mathcal{I} \\dot{w}\n\\mathcal{I}$ with the Frobenius-twisted conjugacy class of $b \\in \\breve{G}$.\nAlthough the geometric connections between these varieties are unknown,\nnumerical relations exist in their geometric properties.\n  This paper explores the irreducible components of affine Lusztig varieties.\nThe centralizer of $\\g$ acts on $Y_w(\\g)$ and the Frobenius-twisted centralizer\nof $b$ acts on $X_w(b)$. We relate the number of orbits on the top-dimensional\ncomponents of $Y_w(\\gamma)$ to the numbers of orbits on top-dimensional\ncomponents of $X_w(b)$ and the affine Springer fibers. For split groups and\nelements $\\gamma$ with integral Newton points, we show that, for most $w$, the\nnumbers of orbits for the affine Lusztig variety and the associated affine\nDeligne-Lusztig variety match. Moreover, for these $\\g$, we verify Chi's\nconjecture that the number of top-dimensional components in $Y_\\mu(\\gamma)$\nwithin the affine Grassmannian equals to the dimension of a specific weight\nspace in a representation of the Langlands dual group.",
        "The block-model family has four popular network models (SBM, DCBM, MMSBM, and\nDCMM). A fundamental problem is, how well each of these models fits with real\nnetworks. We propose GoF-MSCORE as a new Goodness-of-Fit (GoF) metric for DCMM\n(the broadest one among the four), with two main ideas. The first is to use\ncycle count statistics as a general recipe for GoF. The second is a novel\nnetwork fitting scheme. GoF-MSCORE is a flexible GoF approach, and we further\nextend it to SBM, DCBM, and MMSBM. This gives rise to a series of GoF metrics\ncovering each of the four models in the block-model family.\n  We show that for each of the four models, if the assumed model is correct,\nthen the corresponding GoF metric converges to $N(0, 1)$ as the network sizes\ndiverge. We also analyze the powers and show that these metrics are optimal in\nmany settings. In comparison, many other GoF ideas face challenges: they may\nlack a parameter-free limiting null, or are non-optimal in power, or face an\nanalytical hurdle. Note that a parameter-free limiting null is especially\ndesirable as many network models have a large number of unknown parameters. The\nlimiting nulls of our GoF metrics are always $N(0,1)$, which are parameter-free\nas desired.\n  For 12 frequently-used real networks, we use the proposed GoF metrics to show\nthat DCMM fits well with almost all of them. We also show that SBM, DCBM, and\nMMSBM do not fit well with many of these networks, especially when the networks\nare relatively large. To complement with our study on GoF, we also show that\nthe DCMM is nearly as broad as the rank-$K$ network model. Based on these\nresults, we recommend the DCMM as a promising model for undirected networks.",
        "In this study, the role of elastic and interfacial energies in the shape\nevolution of T1 precipitates in Al-Cu-Li alloys is investigated using\nphase-field modeling. We employ a formulation considering the stoichiometric\nnature of the precipitate phase explicitly, including coupled equation systems\nfor various order parameters. Inputs such as elastic properties are derived\nfrom DFT calculations, while chemical potentials are obtained from CALPHAD\ndatabases. This methodology provides a framework that is consistent with the\nderived chemical potentials to study the interplay of thermodynamic, kinetic,\nand elastic effects on T1 precipitate evolution in Al-Cu-Li alloys. It is shown\nthat diffusion-controlled lengthening and interface-controlled thickening are\nimportant mechanisms to describe the growth of T1 precipitates. Furthermore,\nthe study illustrates that the precipitate shape is significantly influenced by\nthe anisotropy in interfacial energy and linear reaction rate, however, elastic\neffects only play a minor role.",
        "We introduce and study blob and framed blob monoids. In particular, several\nrealizations of these monoids are given. We compute the cardinality of the\nframed blob monoid and derive some combinatorial formulas involving this\ncardinality.",
        "We study systems of two and three mesons composed of pions and kaons at\nmaximal isospin using four CLS ensembles with $a\\approx 0.063\\;$fm, including\none with approximately physical quark masses. Using the stochastic\nLaplacian-Heaviside method, we determine the energy spectrum of these systems\nincluding many levels in different momentum frames and irreducible\nrepresentations. Using the relativistic two- and three-body finite-volume\nformalism, we constrain the two and three-meson K matrices, including not only\nthe leading $s$ wave, but also $p$ and $d$ waves. By solving the three-body\nintegral equations, we determine, for the first time, the physical-point\nscattering amplitudes for $3\\pi^+$, $3K^+$, $\\pi^+\\pi^+ K^+$ and $K^+ K^+\n\\pi^+$ systems. These are determined for total angular momentum $J^P=0^-$,\n$1^+$, and $2^-$. We also obtain accurate results for $2\\pi^+$, $\\pi^+ K^+$,\nand $2K^+$ phase shifts. We compare our results to Chiral Perturbation Theory,\nand to phenomenological fits.",
        "Pressure is a powerful thermodynamic parameter for tuning the magnetic\nproperties of van der Waals magnets owing to their weak interlayer bonding.\nHowever, local magnetometry measurements under high pressure still remain\nelusive for this important class of emerging materials. Here we introduce a\nmethod enabling in situ magnetic imaging of van der Waals magnets under high\npressure with sub-micron spatial resolution. Our approach relies on a quantum\nsensing platform based on boron-vacancy (V$_\\text{B}^-$) centers in hexagonal\nboron nitride (hBN), which can be placed in atomic contact of any type of\ntwo-dimensional (2D) material within a van der Waals heterostructure. We first\nshow that the V$_\\text{B}^-$ center can be used as a magnetic field sensor up\nto pressures of a few GPa, a pressure range for which the properties of a wide\nvariety of van der Waals magnets are efficiently altered. We then use\nV$_\\text{B}^-$ centers in a thin hBN layer to perform magnetic imaging of a van\nder Waals magnet under pressure. As a proof of concept, we study the\npressure-dependent magnetization in micrometer-sized flakes of $1T$-CrTe$_2$,\nwhose evolution is explained by a shift of the Curie temperature. Besides\nproviding a new path for studying pressure-induced phase transitions in van der\nWaals magnets, this work also opens up interesting perspectives for exploring\nthe physics of 2D superconductors under pressure via local measurements of the\nMeissner effect.",
        "We propose a multi-scale extension of conformal prediction, an approach that\nconstructs prediction sets with finite-sample coverage guarantees under minimal\nstatistical assumptions. Classic conformal prediction relies on a single notion\nof conformity, overlooking the multi-level structures that arise in\napplications such as image analysis, hierarchical data exploration, and\nmulti-resolution time series modeling. In contrast, the proposed framework\ndefines a distinct conformity function at each relevant scale or resolution,\nproducing multiple conformal predictors whose prediction sets are then\nintersected to form the final multi-scale output. We establish theoretical\nresults confirming that the multi-scale prediction set retains the marginal\ncoverage guarantees of the original conformal framework and can, in fact, yield\nsmaller or more precise sets in practice. By distributing the total miscoverage\nprobability across scales in proportion to their informative power, the method\nfurther refines the set sizes. We also show that dependence between scales can\nlead to conservative coverage, ensuring that the actual coverage exceeds the\nnominal level. Numerical experiments in a synthetic classification setting\ndemonstrate that multi-scale conformal prediction achieves or surpasses the\nnominal coverage level while generating smaller prediction sets compared to\nsingle-scale conformal methods.",
        "We probe the spectrum of primordial gravitational waves (GWs) produced during\nthe eras of hyperkination, kination, and reheating in a non-minimally coupled,\n$\\mathcal{L} \\propto (1+ \\xi \\chi \/M_{\\text{Pl}})^t (R+\\alpha R^2)$, modified\ngravity using the Palatini formulation. We consider a runaway potential, which\ngives an era of kinetic domination after the end of inflation. The coupling\norder $t$ is varied to examine a large class of theories up to $\\chi^2 R^2$.\nFor models with $t>0$, reheating is not achieved naturally; hence, we\nsupplement such theories with a reheating mechanism based on the interaction of\ninflaton and radiation produced at the end of inflation due to cosmological\nexpansion. We demonstrate that the energy density of the GWs is enhanced as a\nfunction of the coupling during kination for all considered theories, and a\nshort-lived phase of hyperkination truncates the boost and avoids the\nover-production of GWs. Hyperkination, and thus the $R^2$ term, should be\ndeemed necessary in all theories with a runaway potential as it prevents the GW\nenhancement during kination from destabilizing the Big Bang Nucleosynthesis.\nThe spectrum remains flat for the period of hyperkination and reheating. We\nexamine the available parameter space for which the theories remain valid and\nplace bounds on the Hubble parameter ($H$) and radiation energy density\n($\\Omega_r^{\\text{end}}$) at the end of inflation. We find that as we decrease\nthe order of the coupling, the spectra shift towards a more observable regime\nof future GW experiments. The observation of the plateau during reheating will\nconstrain the $H$ and $\\Omega_r^{\\text{end}}$ values, while the spectral shape\nof the boost obtained during kination will confirm the nature of the theory.\nThe bounds from hyperkination lie in the kHz-GHz frequency range whose\ndetection can be positively anticipated via resonant cavities."
      ]
    }
  },
  {
    "id":2411.06741,
    "research_type":"applied",
    "start_id":"b20",
    "start_title":"Physics-informed machine learning",
    "start_abstract":"Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.",
    "start_categories":[
      "physics.chem-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Learning Polynomials with Neural Networks"
      ],
      "abstract":[
        "We study the effectiveness of learning low degree polynomials using neural networks by gradient descent method. While have been shown to great expressive power, and has widely used in practice for networks, few theoretical guarantees are known such methods. In particular, it is well that can get stuck at local minima, even simple classes target functions. this paper, we present several positive results support networks. focus on twolayer where bottom layer a set non-linear hidden nodes, top node linear function, similar Barron (1993). First show randomly initialized network with sufficiently many units, generic algorithm learns any polynomial, assuming initialize weights randomly. Secondly, if use complex-valued (the function still be real), then under suitable conditions, there no robust minima: always escape minimum performing random perturbation. This property does not hold real-valued weights. Thirdly, discuss whether sparse learned small size dependent sparsity function."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Towards Data-Efficient Pretraining for Atomic Property Prediction",
        "MMXU: A Multi-Modal and Multi-X-ray Understanding Dataset for Disease\n  Progression",
        "Activation Steering in Neural Theorem Provers",
        "Optimal Actuator Attacks on Autonomous Vehicles Using Reinforcement\n  Learning",
        "Spectral Unmixing Comparison with Sparse, Iterative and Mixed Integer\n  Programming Models",
        "Convergence analysis for a variant of manifold proximal point algorithm\n  based on Kurdyka-{\\L}ojasiewicz property",
        "Structure-Aware Correspondence Learning for Relative Pose Estimation",
        "Fixed point results for single and multi-valued three-points\n  contractions",
        "Data-driven Control of T-Product-based Dynamical Systems",
        "Exploring the Effects of Level of Control in the Initialization of\n  Shared Whiteboarding Sessions in Collaborative Augmented Reality",
        "Unveiling Hidden Pivotal Players with GoalNet: A GNN-Based Soccer Player\n  Evaluation System",
        "Incremental Approximate Single-Source Shortest Paths with Predictions",
        "How does the restriction of representations change under translations? A\n  story for the general linear groups and the unitary groups",
        "Understanding SGD with Exponential Moving Average: A Case Study in\n  Linear Regression",
        "Modeling of stochastic processes in $L_p(T)$ using orthogonal\n  polynomials",
        "ImageScope: Unifying Language-Guided Image Retrieval via Large\n  Multimodal Model Collective Reasoning",
        "Mutating ordered $\\tau$-rigid modules with applications to Nakayama\n  algebras",
        "Comparing Human Expertise and Large Language Models Embeddings in\n  Content Validity Assessment of Personality Tests",
        "Deep ARTMAP: Generalized Hierarchical Learning with Adaptive Resonance\n  Theory",
        "A Novel Observer Design for LuGre Friction Estimation and Control",
        "Real Time Control of Tandem-Wing Experimental Platform Using Concerto\n  Reinforcement Learning",
        "Re-examining Double Descent and Scaling Laws under Norm-based Capacity\n  via Deterministic Equivalence",
        "Towards Auto-Regressive Next-Token Prediction: In-Context Learning\n  Emerges from Generalization",
        "Efficient Phishing URL Detection Using Graph-based Machine Learning and\n  Loopy Belief Propagation",
        "Robust Federated Learning with Global Sensitivity Estimation for\n  Financial Risk Management",
        "A binary PSO based ensemble under-sampling model for rebalancing\n  imbalanced training data",
        "Language-Parametric Reference Synthesis (Extended)",
        "In Praise of Stubbornness: The Case for Cognitive-Dissonance-Aware\n  Knowledge Updates in LLMs",
        "Multiple Linked Tensor Factorization"
      ],
      "abstract":[
        "This paper challenges the recent paradigm in atomic property prediction that\nlinks progress to growing dataset sizes and computational resources. We show\nthat pretraining on a carefully selected, task-relevant dataset can match or\neven surpass large-scale pretraining, while using as little as 1\/24th of the\ncomputational cost. We introduce the Chemical Similarity Index (CSI), a novel\nmetric inspired by computer vision's Fr\\'echet Inception Distance, for\nmolecular graphs which quantifies the alignment between upstream pretraining\ndatasets and downstream tasks. By selecting the most relevant dataset with\nminimal CSI distance, we show that models pretrained on a smaller, focused\ndataset consistently outperform those pretrained on massive, mixed datasets\nsuch as JMP, even when those larger datasets include the relevant dataset.\nCounterintuitively, we also find that indiscriminately adding more data can\ndegrade model performance when the additional data poorly aligns with the task\nat hand. Our findings highlight that quality often outperforms quantity in\npretraining for atomic property prediction.",
        "Large vision-language models (LVLMs) have shown great promise in medical\napplications, particularly in visual question answering (MedVQA) and diagnosis\nfrom medical images. However, existing datasets and models often fail to\nconsider critical aspects of medical diagnostics, such as the integration of\nhistorical records and the analysis of disease progression over time. In this\npaper, we introduce MMXU (Multimodal and MultiX-ray Understanding), a novel\ndataset for MedVQA that focuses on identifying changes in specific regions\nbetween two patient visits. Unlike previous datasets that primarily address\nsingle-image questions, MMXU enables multi-image questions, incorporating both\ncurrent and historical patient data. We demonstrate the limitations of current\nLVLMs in identifying disease progression on MMXU-\\textit{test}, even those that\nperform well on traditional benchmarks. To address this, we propose a\nMedRecord-Augmented Generation (MAG) approach, incorporating both global and\nregional historical records. Our experiments show that integrating historical\nrecords significantly enhances diagnostic accuracy by at least 20\\%, bridging\nthe gap between current LVLMs and human expert performance. Additionally, we\nfine-tune models with MAG on MMXU-\\textit{dev}, which demonstrates notable\nimprovements. We hope this work could illuminate the avenue of advancing the\nuse of LVLMs in medical diagnostics by emphasizing the importance of historical\ncontext in interpreting medical images. Our dataset is released at\n\\href{https:\/\/github.com\/linjiemu\/MMXU}{https:\/\/github.com\/linjiemu\/MMXU}.",
        "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.",
        "With the increasing prevalence of autonomous vehicles (AVs), their\nvulnerability to various types of attacks has grown, presenting significant\nsecurity challenges. In this paper, we propose a reinforcement learning\n(RL)-based approach for designing optimal stealthy integrity attacks on AV\nactuators. We also analyze the limitations of state-of-the-art RL-based secure\ncontrollers developed to counter such attacks. Through extensive simulation\nexperiments, we demonstrate the effectiveness and efficiency of our proposed\nmethod.",
        "Hyperspectral unmixing is the analytical process of determining the pure\nmaterials and estimating the proportions of such materials composed within an\nobserved mixed pixel spectrum. We can unmix mixed pixel spectra using linear\nand nonlinear mixture models. Ordinary least squares (OLS) regression serves as\nthe foundation for many linear mixture models employed in Hyperspectral Image\nanalysis. Though variations of OLS are implemented, studies rarely address the\nunderlying assumptions that affect results. This paper provides an in depth\ndiscussion on the assumptions inherently endorsed by the application of OLS\nregression. We also examine variations of OLS models stemming from highly\neffective approaches in spectral unmixing -- sparse regression, iterative\nfeature search strategies and Mathematical programming. These variations are\ncompared to a novel unmixing approach called HySUDeB. We evaluated each\napproach's performance by computing the average error and precision of each\nmodel. Additionally, we provide a taxonomy of the molecular structure of each\nmineral to derive further understanding into the detection of the target\nmaterials.",
        "We incorporate an iteratively reweighted strategy in the manifold proximal\npoint algorithm (ManPPA) in [12] to solve an enhanced sparsity inducing model\nfor identifying sparse yet nonzero vectors in a given subspace. We establish\nthe global convergence of the whole sequence generated by our algorithm by\nassuming the Kurdyka-Lojasiewicz (KL) properties of suitable potential\nfunctions. We also study how the KL exponents of the different potential\nfunctions are related. More importantly, when our enhanced model and algorithm\nreduce, respectively, to the model and ManPPA with constant stepsize considered\nin [12], we show that the sequence generated converges linearly as long as the\noptimal value of the model is positive, and converges finitely when the limit\nof the sequence lies in a set of weak sharp minima. Our results improve [13,\nTheorem 2.4], which asserts local quadratic convergence in the presence of weak\nsharp minima when the constant stepsize is small.",
        "Relative pose estimation provides a promising way for achieving\nobject-agnostic pose estimation. Despite the success of existing 3D\ncorrespondence-based methods, the reliance on explicit feature matching suffers\nfrom small overlaps in visible regions and unreliable feature estimation for\ninvisible regions. Inspired by humans' ability to assemble two object parts\nthat have small or no overlapping regions by considering object structure, we\npropose a novel Structure-Aware Correspondence Learning method for Relative\nPose Estimation, which consists of two key modules. First, a structure-aware\nkeypoint extraction module is designed to locate a set of kepoints that can\nrepresent the structure of objects with different shapes and appearance, under\nthe guidance of a keypoint based image reconstruction loss. Second, a\nstructure-aware correspondence estimation module is designed to model the\nintra-image and inter-image relationships between keypoints to extract\nstructure-aware features for correspondence estimation. By jointly leveraging\nthese two modules, the proposed method can naturally estimate 3D-3D\ncorrespondences for unseen objects without explicit feature matching for\nprecise relative pose estimation. Experimental results on the CO3D, Objaverse\nand LineMOD datasets demonstrate that the proposed method significantly\noutperforms prior methods, i.e., with 5.7{\\deg}reduction in mean angular error\non the CO3D dataset.",
        "In this paper, we are concerned with the study of the existence of fixed\npoints for single and multi-valued three-points contractions. Namely, we first\nintroduce a new class of single-valued mappings defined on a metric space\nequipped with three metrics. A fixed point theorem is established for such\nmappings. The obtained result recovers that established recently by the second\nauthor [J. Fixed Point Theory Appl. 25 (2023) 74] for the class of\nsingle-valued mappings contracting perimeters of triangles. We next extend our\nstudy by introducing the class of multivalued three points contractions. A\nfixed point theorem, which is a multi-valued version of that obtained in the\nabove reference, is established. Some examples showing the validity of our\nobtained results are provided.",
        "Data-driven control is a powerful tool that enables the design and\nimplementation of control strategies directly from data without explicitly\nidentifying the underlying system dynamics. While various data-driven control\ntechniques, such as stabilization, linear quadratic regulation, and model\npredictive control, have been extensively developed, these methods are not\ninherently suited for multi-linear dynamical systems, where the states are\nrepresented as higher-order tensors. In this article, we propose a novel\nframework for data-driven control of T-product-based dynamical systems (TPDSs),\nwhere the system evolution is governed by the T-product between a third-order\ndynamic tensor and a third-order state tensor. In particular, we offer\nnecessary and sufficient conditions to determine the data informativity for\nsystem identification, stabilization by state feedback, and T-product quadratic\nregulation of TPDSs with detailed complexity analyses. Finally, we validate our\nframework through numerical examples.",
        "Augmented Reality (AR) collaboration can benefit from a shared 2D surface,\nsuch as a whiteboard. However, many features of each collaborators physical\nenvironment must be considered in order to determine the best placement and\nshape of the shared surface. We explored the effects of three methods for\nbeginning a collaborative whiteboarding session with varying levels of user\ncontrol: MANUAL, DISCRETE CHOICE, and AUTOMATIC by conducting a simulated AR\nstudy within Virtual Reality (VR). In the MANUAL method, users draw their own\nsurfaces directly in the environment until they agree on the placement; in the\nDISCRETE CHOICE method, the system provides three options for whiteboard size\nand location; and in the AUTOMATIC method, the system automatically creates a\nwhiteboard that fits within each collaborators environment. We evaluate these\nthree conditions in a study in which two collaborators used each method to\nbegin collaboration sessions. After establishing a session, the users worked\ntogether to complete an affinity diagramming task using the shared whiteboard.\nWe found that the majority of participants preferred to have direct control\nduring the initialization of a new collaboration session, despite the\nadditional workload induced by the Manual method.",
        "Soccer analysis tools emphasize metrics such as expected goals, leading to an\noverrepresentation of attacking players' contributions and overlooking players\nwho facilitate ball control and link attacks. Examples include Rodri from\nManchester City and Palhinha who just transferred to Bayern Munich. To address\nthis bias, we aim to identify players with pivotal roles in a soccer team,\nincorporating both spatial and temporal features.\n  In this work, we introduce a GNN-based framework that assigns individual\ncredit for changes in expected threat (xT), thus capturing overlooked yet vital\ncontributions in soccer. Our pipeline encodes both spatial and temporal\nfeatures in event-centric graphs, enabling fair attribution of non-scoring\nactions such as defensive or transitional plays. We incorporate centrality\nmeasures into the learned player embeddings, ensuring that ball-retaining\ndefenders and defensive midfielders receive due recognition for their overall\nimpact. Furthermore, we explore diverse GNN variants-including Graph Attention\nNetworks and Transformer-based models-to handle long-range dependencies and\nevolving match contexts, discussing their relative performance and\ncomputational complexity. Experiments on real match data confirm the robustness\nof our approach in highlighting pivotal roles that traditional attacking\nmetrics typically miss, underscoring the model's utility for more comprehensive\nsoccer analytics.",
        "The algorithms-with-predictions framework has been used extensively to\ndevelop online algorithms with improved beyond-worst-case competitive ratios.\nRecently, there is growing interest in leveraging predictions for designing\ndata structures with improved beyond-worst-case running times. In this paper,\nwe study the fundamental data structure problem of maintaining approximate\nshortest paths in incremental graphs in the algorithms-with-predictions model.\nGiven a sequence $\\sigma$ of edges that are inserted one at a time, the goal is\nto maintain approximate shortest paths from the source to each vertex in the\ngraph at each time step. Before any edges arrive, the data structure is given a\nprediction of the online edge sequence $\\hat{\\sigma}$ which is used to ``warm\nstart'' its state.\n  As our main result, we design a learned algorithm that maintains\n$(1+\\epsilon)$-approximate single-source shortest paths, which runs in\n$\\tilde{O}(m \\eta \\log W\/\\epsilon)$ time, where $W$ is the weight of the\nheaviest edge and $\\eta$ is the prediction error. We show these techniques\nimmediately extend to the all-pairs shortest-path setting as well. Our\nalgorithms are consistent (performing nearly as fast as the offline algorithm)\nwhen predictions are nearly perfect, have a smooth degradation in performance\nwith respect to the prediction error and, in the worst case, match the best\noffline algorithm up to logarithmic factors.\n  As a building block, we study the offline incremental approximate\nsingle-source shortest-paths problem. In this problem, the edge sequence\n$\\sigma$ is known a priori and the goal is to efficiently return the length of\nthe shortest paths in the intermediate graph $G_t$ consisting of the first $t$\nedges, for all $t$. Note that the offline incremental problem is defined in the\nworst-case setting (without predictions) and is of independent interest.",
        "We present a new approach to symmetry breaking for pairs of real forms of\n$(GL(n, \\mathbb{C}), GL(n-1, \\mathbb{C}))$. While translation functors are a\nuseful tool for studying a family of representations of a single reductive\ngroup $G$, when applied to a pair of groups $G \\supset G'$,translation functors\ncan significantly alter the nature of symmetry breaking between the\nrepresentations of $G$ and $G'$, even within the same Weyl chamber of the\ndirect product group $G \\times G'$. We introduce the concept of \\lq\\lq{fences\nfor the interlacing pattern}\\rq\\rq,which provides a refinement of the usual\nnotion of \\lq\\lq{walls for Weyl chambers}\\rq\\rq. We then present a theorem that\nstates that multiplicity is constant unless these \\lq\\lq{fences}\\rq\\rq\\ are\ncrossed. This general theorem is illustrated with examples of both tempered and\nnon-tempered representations. Additionally,we provide a new non-vanishing\ntheorem of period integrals for pairs of reductive symmetric spaces,which is\nfurther strengthened through this approach.",
        "Exponential moving average (EMA) has recently gained significant popularity\nin training modern deep learning models, especially diffusion-based generative\nmodels. However, there have been few theoretical results explaining the\neffectiveness of EMA. In this paper, to better understand EMA, we establish the\nrisk bound of online SGD with EMA for high-dimensional linear regression, one\nof the simplest overparameterized learning tasks that shares similarities with\nneural networks. Our results indicate that (i) the variance error of SGD with\nEMA is always smaller than that of SGD without averaging, and (ii) unlike SGD\nwith iterate averaging from the beginning, the bias error of SGD with EMA\ndecays exponentially in every eigen-subspace of the data covariance matrix.\nAdditionally, we develop proof techniques applicable to the analysis of a broad\nclass of averaging schemes.",
        "In this paper, models that approximate stochastic processes from the space\n$Sub_\\varphi(\\Omega)$ with given reliability and accuracy in $L_p(T)$ are\nconsidered for some specific functions $\\varphi(t)$. For processes that are\ndecomposited in series using orthonormal bases, such models are constructed in\nthe case where elements of such decomposition cannot be found explicitly.",
        "With the proliferation of images in online content, language-guided image\nretrieval (LGIR) has emerged as a research hotspot over the past decade,\nencompassing a variety of subtasks with diverse input forms. While the\ndevelopment of large multimodal models (LMMs) has significantly facilitated\nthese tasks, existing approaches often address them in isolation, requiring the\nconstruction of separate systems for each task. This not only increases system\ncomplexity and maintenance costs, but also exacerbates challenges stemming from\nlanguage ambiguity and complex image content, making it difficult for retrieval\nsystems to provide accurate and reliable results. To this end, we propose\nImageScope, a training-free, three-stage framework that leverages collective\nreasoning to unify LGIR tasks. The key insight behind the unification lies in\nthe compositional nature of language, which transforms diverse LGIR tasks into\na generalized text-to-image retrieval process, along with the reasoning of LMMs\nserving as a universal verification to refine the results. To be specific, in\nthe first stage, we improve the robustness of the framework by synthesizing\nsearch intents across varying levels of semantic granularity using\nchain-of-thought (CoT) reasoning. In the second and third stages, we then\nreflect on retrieval results by verifying predicate propositions locally, and\nperforming pairwise evaluations globally. Experiments conducted on six LGIR\ndatasets demonstrate that ImageScope outperforms competitive baselines.\nComprehensive evaluations and ablation studies further confirm the\neffectiveness of our design.",
        "A mutation operation for $\\tau$-exceptional sequences of modules over any\nfinite-dimensional algebra was recently introduced, generalising the mutation\nfor exceptional sequences of modules over hereditary algebras. We interpret\nthis mutation in terms of TF-ordered $\\tau$-rigid modules, which are in\nbijection with $\\tau$-exceptional sequences. As an application we show that the\nmutation is transitive for Nakayama algebras, by providing an explicit\ncombinatorial description of mutation over this class of algebras.",
        "In this article we explore the application of Large Language Models (LLMs) in\nassessing the content validity of psychometric instruments, focusing on the Big\nFive Questionnaire (BFQ) and Big Five Inventory (BFI). Content validity, a\ncornerstone of test construction, ensures that psychological measures\nadequately cover their intended constructs. Using both human expert evaluations\nand advanced LLMs, we compared the accuracy of semantic item-construct\nalignment. Graduate psychology students employed the Content Validity Ratio\n(CVR) to rate test items, forming the human baseline. In parallel,\nstate-of-the-art LLMs, including multilingual and fine-tuned models, analyzed\nitem embeddings to predict construct mappings. The results reveal distinct\nstrengths and limitations of human and AI approaches. Human validators excelled\nin aligning the behaviorally rich BFQ items, while LLMs performed better with\nthe linguistically concise BFI items. Training strategies significantly\ninfluenced LLM performance, with models tailored for lexical relationships\noutperforming general-purpose LLMs. Here we highlights the complementary\npotential of hybrid validation systems that integrate human expertise and AI\nprecision. The findings underscore the transformative role of LLMs in\npsychological assessment, paving the way for scalable, objective, and robust\ntest development methodologies.",
        "This paper presents Deep ARTMAP, a novel extension of the ARTMAP architecture\nthat generalizes the self-consistent modular ART (SMART) architecture to enable\nhierarchical learning (supervised and unsupervised) across arbitrary\ntransformations of data. The Deep ARTMAP framework operates as a divisive\nclustering mechanism, supporting an arbitrary number of modules with\ncustomizable granularity within each module. Inter-ART modules regulate the\nclustering at each layer, permitting unsupervised learning while enforcing a\none-to-many mapping from clusters in one layer to the next. While Deep ARTMAP\nreduces to both ARTMAP and SMART in particular configurations, it offers\nsignificantly enhanced flexibility, accommodating a broader range of data\ntransformations and learning modalities.",
        "Dynamic components of the friction may directly impact the stability and\nperformance of the motion control systems. The LuGre model is a prevalent\nfriction model utilized to express this dynamic behavior. Since the LuGre model\nis very comprehensive, friction compensation based on it might be challenging.\nInspired by this, we develop a novel observer to estimate and compensate for\nLuGre friction. Furthermore, we present a Lyapunov stability analysis to show\nthat observer dynamics are asymptotically stable under certain conditions.\nCompared to its counterparts, the proposed observer constitutes a simple and\nstandalone scheme that can be utilized with arbitrary control inputs in a\nstraightforward way. As a primary difference, the presented observer estimates\nvelocity and uses the velocity error to estimate friction in addition to\ncontrol input. The extensive simulations revealed that the introduced observer\nenhances position and velocity tracking performance in the presence of\nfriction.",
        "This paper introduces the CRL2RT algorithm, an advanced reinforcement\nlearning method aimed at improving the real-time control performance of the\nDirect-Drive Tandem-Wing Experimental Platform (DDTWEP). Inspired by dragonfly\nflight, DDTWEP's tandem wing structure causes nonlinear and unsteady\naerodynamic interactions, leading to complex load behaviors during pitch, roll,\nand yaw maneuvers. These complexities challenge stable motion control at high\nfrequencies (2000 Hz). To overcome these issues, we developed the CRL2RT\nalgorithm, which combines classical control elements with reinforcement\nlearning-based controllers using a time-interleaved architecture and a\nrule-based policy composer. This integration ensures finite-time convergence\nand single-life adaptability. Experimental results under various conditions,\nincluding different flapping frequencies and yaw disturbances, show that CRL2RT\nachieves a control frequency surpassing 2500 Hz on standard CPUs. Additionally,\nwhen integrated with classical controllers like PID, Adaptive PID, and Model\nReference Adaptive Control (MRAC), CRL2RT enhances tracking performance by\n18.3% to 60.7%. These findings demonstrate CRL2RT's broad applicability and\nsuperior performance in complex real-time control scenarios, validating its\neffectiveness in overcoming existing control strategy limitations and advancing\nrobust, efficient real-time control for biomimetic aerial vehicles.",
        "We investigate double descent and scaling laws in terms of weights rather\nthan the number of parameters. Specifically, we analyze linear and random\nfeatures models using the deterministic equivalence approach from random matrix\ntheory. We precisely characterize how the weights norm concentrate around\ndeterministic quantities and elucidate the relationship between the expected\ntest error and the norm-based capacity (complexity). Our results rigorously\nanswer whether double descent exists under norm-based capacity and reshape the\ncorresponding scaling laws. Moreover, they prompt a rethinking of the\ndata-parameter paradigm - from under-parameterized to over-parameterized\nregimes - by shifting the focus to norms (weights) rather than parameter count.",
        "Large language models (LLMs) have demonstrated remarkable in-context learning\n(ICL) abilities. However, existing theoretical analysis of ICL primarily\nexhibits two limitations: (a) Limited i.i.d. Setting. Most studies focus on\nsupervised function learning tasks where prompts are constructed with i.i.d.\ninput-label pairs. This i.i.d. assumption diverges significantly from real\nlanguage learning scenarios where prompt tokens are interdependent. (b) Lack of\nEmergence Explanation. Most literature answers what ICL does from an implicit\noptimization perspective but falls short in elucidating how ICL emerges and the\nimpact of pre-training phase on ICL. In our paper, to extend (a), we adopt a\nmore practical paradigm, auto-regressive next-token prediction (AR-NTP), which\nclosely aligns with the actual training of language models. Specifically,\nwithin AR-NTP, we emphasize prompt token-dependency, which involves predicting\neach subsequent token based on the preceding sequence. To address (b), we\nformalize a systematic pre-training and ICL framework, highlighting the\nlayer-wise structure of sequences and topics, alongside a two-level\nexpectation. In conclusion, we present data-dependent, topic-dependent and\noptimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs,\ninvestigating that ICL emerges from the generalization of sequences and topics.\nOur theory is supported by experiments on numerical linear dynamic systems,\nsynthetic GINC and real-world language datasets.",
        "The proliferation of mobile devices and online interactions have been\nthreatened by different cyberattacks, where phishing attacks and malicious\nUniform Resource Locators (URLs) pose significant risks to user security.\nTraditional phishing URL detection methods primarily rely on URL string-based\nfeatures, which attackers often manipulate to evade detection. To address these\nlimitations, we propose a novel graph-based machine learning model for phishing\nURL detection, integrating both URL structure and network-level features such\nas IP addresses and authoritative name servers. Our approach leverages Loopy\nBelief Propagation (LBP) with an enhanced convergence strategy to enable\neffective message passing and stable classification in the presence of complex\ngraph structures. Additionally, we introduce a refined edge potential mechanism\nthat dynamically adapts based on entity similarity and label relationships to\nfurther improve classification accuracy. Comprehensive experiments on\nreal-world datasets demonstrate our model's effectiveness by achieving F1 score\nof up to 98.77\\%. This robust and reproducible method advances phishing\ndetection capabilities, offering enhanced reliability and valuable insights in\nthe field of cybersecurity.",
        "In decentralized financial systems, robust and efficient Federated Learning\n(FL) is promising to handle diverse client environments and ensure resilience\nto systemic risks. We propose Federated Risk-Aware Learning with Central\nSensitivity Estimation (FRAL-CSE), an innovative FL framework designed to\nenhance scalability, stability, and robustness in collaborative financial\ndecision-making. The framework's core innovation lies in a central acceleration\nmechanism, guided by a quadratic sensitivity-based approximation of global\nmodel dynamics. By leveraging local sensitivity information derived from robust\nrisk measurements, FRAL-CSE performs a curvature-informed global update that\nefficiently incorporates second-order information without requiring repeated\nlocal re-evaluations, thereby enhancing training efficiency and improving\noptimization stability. Additionally, distortion risk measures are embedded\ninto the training objectives to capture tail risks and ensure robustness\nagainst extreme scenarios. Extensive experiments validate the effectiveness of\nFRAL-CSE in accelerating convergence and improving resilience across\nheterogeneous datasets compared to state-of-the-art baselines.",
        "Ensemble technique and under-sampling technique are both effective tools used\nfor imbalanced dataset classification problems. In this paper, a novel ensemble\nmethod combining the advantages of both ensemble learning for biasing\nclassifiers and a new under-sampling method is proposed. The under-sampling\nmethod is named Binary PSO instance selection; it gathers with ensemble\nclassifiers to find the most suitable length and combination of the majority\nclass samples to build a new dataset with minority class samples. The proposed\nmethod adopts multi-objective strategy, and contribution of this method is a\nnotable improvement of the performances of imbalanced classification, and in\nthe meantime guaranteeing a best integrity possible for the original dataset.\nWe experimented the proposed method and compared its performance of processing\nimbalanced datasets with several other conventional basic ensemble methods.\nExperiment is also conducted on these imbalanced datasets using an improved\nversion where ensemble classifiers are wrapped in the Binary PSO instance\nselection. According to experimental results, our proposed methods outperform\nsingle ensemble methods, state-of-the-art under-sampling methods, and also\ncombinations of these methods with the traditional PSO instance selection\nalgorithm.",
        "Modern Integrated Development Environments (IDEs) offer automated\nrefactorings to aid programmers in developing and maintaining software.\nHowever, implementing sound automated refactorings is challenging, as\nrefactorings may inadvertently introduce name-binding errors or cause\nreferences to resolve to incorrect declarations. To address these issues,\nprevious work by Sch\\\"afer et al. proposed replacing concrete references with\nlocked references to separate binding preservation from transformation. Locked\nreferences vacuously resolve to a specific declaration, and after\ntransformation must be replaced with concrete references that also resolve to\nthat declaration. Synthesizing these references requires a faithful inverse of\nthe name lookup functions of the underlying language.\n  Manually implementing such inverse lookup functions is challenging due to the\ncomplex name-binding features in modern programming languages. Instead, we\npropose to automatically derive this function from type system specifications\nwritten in the Statix meta-DSL. To guide the synthesis of qualified references\nwe use scope graphs, which represent the binding structure of a program, to\ninfer their names and discover their syntactic structure.\n  We evaluate our approach by synthesizing concrete references for locked\nreferences in 2528 Java, 196 ChocoPy, and 49 Featherweight Generic Java test\nprograms. Our approach yields a principled language-parametric method for\nsynthesizing references.",
        "Despite remarkable capabilities, large language models (LLMs) struggle to\ncontinually update their knowledge without catastrophic forgetting. In\ncontrast, humans effortlessly integrate new information, detect conflicts with\nexisting beliefs, and selectively update their mental models. This paper\nintroduces a cognitive-inspired investigation paradigm to study continual\nknowledge updating in LLMs. We implement two key components inspired by human\ncognition: (1) Dissonance and Familiarity Awareness, analyzing model behavior\nto classify information as novel, familiar, or dissonant; and (2) Targeted\nNetwork Updates, which track neural activity to identify frequently used\n(stubborn) and rarely used (plastic) neurons. Through carefully designed\nexperiments in controlled settings, we uncover a number of empirical findings\ndemonstrating the potential of this approach. First, dissonance detection is\nfeasible using simple activation and gradient features, suggesting potential\nfor cognitive-inspired training. Second, we find that non-dissonant updates\nlargely preserve prior knowledge regardless of targeting strategy, revealing\ninherent robustness in LLM knowledge integration. Most critically, we discover\nthat dissonant updates prove catastrophically destructive to the model's\nknowledge base, indiscriminately affecting even information unrelated to the\ncurrent updates. This suggests fundamental limitations in how neural networks\nhandle contradictions and motivates the need for new approaches to knowledge\nupdating that better mirror human cognitive mechanisms.",
        "In biomedical research and other fields, it is now common to generate high\ncontent data that are both multi-source and multi-way. Multi-source data are\ncollected from different high-throughput technologies while multi-way data are\ncollected over multiple dimensions, yielding multiple tensor arrays.\nIntegrative analysis of these data sets is needed, e.g., to capture and\nsynthesize different facets of complex biological systems. However, despite\ngrowing interest in multi-source and multi-way factorization techniques,\nmethods that can handle data that are both multi-source and multi-way are\nlimited. In this work, we propose a Multiple Linked Tensors Factorization\n(MULTIFAC) method extending the CANDECOMP\/PARAFAC (CP) decomposition to\nsimultaneously reduce the dimension of multiple multi-way arrays and\napproximate underlying signal. We first introduce a version of the CP\nfactorization with L2 penalties on the latent factors, leading to rank\nsparsity. When extended to multiple linked tensors, the method automatically\nreveals latent components that are shared across data sources or individual to\neach data source. We also extend the decomposition algorithm to its\nexpectation-maximization (EM) version to handle incomplete data with\nimputation. Extensive simulation studies are conducted to demonstrate\nMULTIFAC's ability to (i) approximate underlying signal, (ii) identify shared\nand unshared structures, and (iii) impute missing data. The approach yields an\ninterpretable decomposition on multi-way multi-omics data for a study on\nearly-life iron deficiency."
      ]
    }
  },
  {
    "id":2411.18259,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Physics-informed machine learning",
    "start_abstract":"Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Quantifying the performance of machine learning models in materials discovery"
      ],
      "abstract":[
        "The predictive capabilities of machine learning (ML) models used in materials discovery are typically measured using simple statistics such as the root-mean-square error (RMSE) or coefficient determination ($r^2$) between ML-predicted property values and their known values. A tempting assumption is that with low should be effective at guiding discovery, conversely, high give poor performance. However, we observe no clear connection exists a \"static\" quantity averaged across an entire training set, RMSE, ML model's ability to dynamically guide iterative (and often extrapolative) novel targeted properties. In this work, simulate sequential (SL)-guided process demonstrate decoupling traditional model metrics performance discoveries. We show depends strongly on (1) target range within distribution (e.g., whether 1st 10th decile material desired); (2) incorporation uncertainty estimates SL acquisition function; (3) scientist interested one many targets; (4) how iterations allowed. To overcome limitations static robustly capture performance, recommend Discovery Yield ($DY$), measure high-performing were discovered during SL, Probability ($DP$), likelihood discovering any point process."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "On the speed of coming down from infinity for (sub)critical branching\n  processes with pairwise interactions",
        "Comment on \"Optimal conversion of Kochen-Specker sets into bipartite\n  perfect quantum strategies\"",
        "A simple extrapolation criterion with an application to wavelet\n  characterization of various function spaces",
        "Predicted Neutrino Signal Features of Core-Collapse Supernovae",
        "A Space Mapping approach for the calibration of financial models with\n  the application to the Heston model",
        "Euclid Quick Data Release (Q1), A first look at the fraction of bars in\n  massive galaxies at $z<1$",
        "H$\\alpha$ Variability of AB Aur b with the Hubble Space Telescope:\n  Probing the Nature of a Protoplanet Candidate with Accretion Light Echoes",
        "Surfaces in 4-manifolds and extendible mapping classes",
        "Discrete Level Set Persistence for Finite Discrete Functions",
        "Monge-Kantorovich quantiles and ranks for image data",
        "Interpreting and Steering Protein Language Models through Sparse\n  Autoencoders",
        "Minimax rates of convergence for the nonparametric estimation of the\n  diffusion coefficient from time-homogeneous SDE paths",
        "Stochastic Equilibrium Raman Spectroscopy (STERS)",
        "The Light Neutralino Dark Matter in the Generalized Minimal Supergravity\n  (GmSUGRA)",
        "Fusion Dynamics of Majorana Zero Modes",
        "Error bounds for composite quantum hypothesis testing and a new\n  characterization of the weighted Kubo-Ando geometric means",
        "Part-Time Penalties and Heterogeneous Retirement Decisions",
        "Environmental Co-design: Fish-Blade Collision Model for Hydrokinetic\n  Turbines",
        "Noise equals endogenous control",
        "On regular charged black holes in three dimensions",
        "Advances in modeling complex materials: The rise of neuroevolution\n  potentials",
        "A numerical scheme for a multi-scale model of thrombus in arteries",
        "Parameter Symmetry Breaking and Restoration Determines the Hierarchical\n  Learning in AI Systems",
        "ELT-METIS imaging simulations for disks and envelopes associated with FU\n  Ori-type objects",
        "On meromorphic solutions of certain Fermat-type difference and analogues\n  equations concerning open problems",
        "Accelerating Combinatorial Electrocatalyst Discovery with Bayesian\n  Optimization: A Case Study in the Quaternary System Ni-Pd-Pt-Ru for the\n  Oxygen Evolution Reaction",
        "On the hydrostatic approximation of 3D Oldroyd-B model",
        "Metrological symmetries in singular quantum multi-parameter estimation",
        "Desorption-induced decoherence of nanoparticle motion"
      ],
      "abstract":[
        "In this paper, we investigate the phenomenon of coming down from infinity for\n(sub)critical cooperative branching processes with pairwise interactions (BPI\nprocesses for short) under appropriate conditions. BPI processes are\ncontinuous-time Markov chains that extend pure branching dynamics by\nincorporating additional mechanisms that allow both competition and cooperation\nevents between pairs of individuals.\n  Specifically, we focus on characterising the speed at which BPI processes\nevolve when starting from a very large initial population in the subcritical\nand critical cooperative regimes. Further, in the subcritical cooperative\nregime, we analyse their second-order fluctuations.",
        "A recent paper of Trandafir and Cabello [Phys. Rev. A, 111, 022408 (2025)]\ncontains a number of errors, inconsistencies, and inefficiencies. They are too\nnumerous to be listed here, so we identify and discuss them in the main body of\nthe comment.",
        "The aim of this paper is to obtain an extrapolation result without using\nconvexification. What is new about this criterion is that the convexification\nof Banach spaces does not come into play. As an application, a characterization\nof ball Banach function spaces in terms of wavelets can be obtained. The result\ncan be formulated so that we can take into account the smoothness property of\nthe function spaces under consideration. The same technique can be used for the\nproof of the vector-valued inequalities for example. Also, the result in the\npresent paper refines a recent result on the extension operator.",
        "In this paper, we examine the neutrino signals from 24 initially\nnon-rotating, three-dimensional core-collapse supernova (CCSN) simulations\ncarried to late times. We find that not only does the neutrino luminosity\nsignal encode information about each stage of the CCSN process, but that the\nmonotonic dependence of the luminosity peak height with compactness enables one\nto infer the progenitor core structure from the neutrino signal. We highlight a\nsystematic relationship between the luminosity peak height with its timing.\nAdditionally, we emphasize that the total energy radiated in neutrinos is\nmonotonic with progenitor compactness, and that the mean neutrino energy\ncontains a unique spiral SASI signature for nonexploding, BH-forming models. We\nalso find that neutrino emissions are not isotropic and that the anisotropy\nincreases roughly with progenitor compactness. To assess the detectability of\nthese neutrino signal features, we provide examples of the event rates for our\nmodels for the JUNO, DUNE, SK, and IceCube detectors using the SNEWPY software,\nand find that many of the trends in the luminosity signal can be detectable\nacross several detectors and oscillation models. Finally, we discuss\ncorrelations between the radiated neutrino energy and the evolution of the\ngravitational-wave f-mode.",
        "We present a novel approach for parameter calibration of the Heston model for\npricing an Asian put option, namely space mapping. Since few parameters of the\nHeston model can be directly extracted from real market data, calibration to\nreal market data is implicit and therefore a challenging task. In addition,\nsome of the parameters in the model are non-linear, which makes it difficult to\nfind the global minimum of the optimization problem within the calibration. Our\napproach is based on the idea of space mapping, exploiting the residuum of a\ncoarse surrogate model that allows optimization and a fine model that needs to\nbe calibrated. In our case, the pricing of an Asian option using the Heston\nmodel SDE is the fine model, and the surrogate is chosen to be the Heston model\nPDE pricing a European option. We formally derive a gradient descent algorithm\nfor the PDE constrained calibration model using well-known techniques from\noptimization with PDEs. Our main goal is to provide evidence that the space\nmapping approach can be useful in financial calibration tasks. Numerical\nresults underline the feasibility of our approach.",
        "Stellar bars are key structures in disc galaxies, driving angular momentum\nredistribution and influencing processes such as bulge growth and star\nformation. Quantifying the bar fraction as a function of redshift and stellar\nmass is therefore important for constraining the physical processes that drive\ndisc formation and evolution across the history of the Universe. Leveraging the\nunprecedented resolution and survey area of the Euclid Q1 data release combined\nwith the Zoobot deep-learning model trained on citizen-science labels, we\nidentify 7711 barred galaxies with $M_* \\gtrsim 10^{10}M_\\odot$ in a\nmagnitude-selected sample $I_E < 20.5$ spanning $63.1 deg^2$. We measure a mean\nbar fraction of $0.2-0.4$, consistent with prior studies. At fixed redshift,\nmassive galaxies exhibit higher bar fractions, while lower-mass systems show a\nsteeper decline with redshift, suggesting earlier disc assembly in massive\ngalaxies. Comparisons with cosmological simulations (e.g., TNG50, Auriga)\nreveal a broadly consistent bar fraction, but highlight overpredictions for\nhigh-mass systems, pointing to potential over-efficiency in central stellar\nmass build-up in simulations. These findings demonstrate Euclid's\ntransformative potential for galaxy morphology studies and underscore the\nimportance of refining theoretical models to better reproduce observed trends.\nFuture work will explore finer mass bins, environmental correlations, and\nadditional morphological indicators.",
        "Giant planets generate accretion luminosity as they form. Much of this energy\nis radiated in strong H$\\alpha$ line emission, which has motivated direct\nimaging surveys at optical wavelengths to search for accreting protoplanets.\nHowever, compact disk structures can mimic accreting planets by scattering\nemission from the host star. This can complicate the interpretation of\nH$\\alpha$ point sources, especially if the host star itself is accreting. We\ndescribe an approach to distinguish accreting protoplanets from scattered-light\ndisk features using \"accretion light echoes.\" This method relies on variable\nH$\\alpha$ emission from a stochastically accreting host star to search for a\ndelayed brightness correlation with a candidate protoplanet. We apply this\nmethod to the candidate protoplanet AB Aur b with a dedicated Hubble Space\nTelescope Wide Field Camera 3 program designed to sequentially sample the host\nstar and the candidate planet in H$\\alpha$ while accounting for the light\ntravel time delay and orbital geometry of the source within the protoplanetary\ndisk. Across five epochs spanning 14 months, AB Aur b is over 20 times more\nvariable than its host star; AB Aur's H$\\alpha$ emission changes by 15% while\nAB Aur b varies by 330%. These brightness changes are not correlated, which\nrules out unobstructed scattered starlight from the host star as the only\nsource of AB Aur b's H$\\alpha$ emission and is consistent with tracing emission\nfrom an independently accreting protoplanet, inner disk shadowing effects, or a\nphysically evolving compact disk structure. More broadly, accretion light\nechoes offer a novel tool to explore the nature of protoplanet candidates with\nwell-timed observations of the host star prior to deep imaging in H$\\alpha$.",
        "We study smooth proper embeddings of compact orientable surfaces in compact\norientable $4$-manifolds and elements in the mapping class group of that\nsurface which are induced by diffeomorphisms of the ambient $4$-manifolds. We\ncall such mapping classes extendible. An embedding for which all mapping\nclasses are extendible is called flexible. We show that for most of the\nsurfaces there exists no flexible embedding in a $4$-manifold with homology\ntype of a $4$-ball or of a $4$-sphere. As an application of our method, we\naddress a question of Etnyre and Lekili and show that there exists no simple\nopen book decomposition of $S^5$ with a spin page where all $3$-dimensional\nopen books admit open book embeddings. We also provide many constructions and\ncriteria for extendible and non-extendible mapping classes, and discuss a\nconnection between extendibility and sliceness of links in a homology $4$-ball\nwith $S^3$ boundary. Finally, we give a new generating set of the group of\nextendible mapping classes for the trivial embedding of a closed genus $g$\nsurface in $S^4$, consisting of $3g$ generators. This improves a previous\nresult of Hirose giving a generating set of size $6g-1$.",
        "We study sublevel set and superlevel set persistent homology on discrete\nfunctions through the perspective of finite ordered sets of both linearly\nordered and cyclically ordered domains. Finite ordered sets also serve as the\ncodomain of our functions making all arguments finite and discrete. We prove\nduality of filtrations of sublevel sets and superlevel sets that undergirths a\nrange of duality results of sublevel set persistent homology without the need\nto invoke complications of continuous functions or classical Morse theory. We\nshow that Morse-like behavior can be achieved for flat extrema without assuming\ngenericity. Additionally, we show that with inversion of order, one can compute\nsublevel set persistence from superlevel set persistence, and vice versa via a\nduality result that does not require the boundary to be treated as a special\ncase. Furthermore, we discuss aspects of barcode construction rules, surgery of\ncircular and linearly ordered sets, as well as surgery on auxiliary structures\nsuch as box snakes, which segment the ordered set by extrema and monotones.",
        "This paper defines quantiles, ranks and statistical depths for image data by\nleveraging ideas from measure transportation. The first step is to embed a\ndistribution of images in a tangent space, with the framework of linear optimal\ntransport. Therein, Monge-Kantorovich quantiles are shown to provide a\nmeaningful ordering of image data, with outward images having unusual shapes.\nNumerical experiments showcase the relevance of the proposed procedure, for\ndescriptive analysis, outlier detection or statistical testing.",
        "The rapid advancements in transformer-based language models have\nrevolutionized natural language processing, yet understanding the internal\nmechanisms of these models remains a significant challenge. This paper explores\nthe application of sparse autoencoders (SAE) to interpret the internal\nrepresentations of protein language models, specifically focusing on the ESM-2\n8M parameter model. By performing a statistical analysis on each latent\ncomponent's relevance to distinct protein annotations, we identify potential\ninterpretations linked to various protein characteristics, including\ntransmembrane regions, binding sites, and specialized motifs.\n  We then leverage these insights to guide sequence generation, shortlisting\nthe relevant latent components that can steer the model towards desired targets\nsuch as zinc finger domains. This work contributes to the emerging field of\nmechanistic interpretability in biological sequence models, offering new\nperspectives on model steering for sequence design.",
        "Consider a diffusion process X, solution of a time-homogeneous stochastic\ndifferential equation. We assume that the diffusion process X is observed at\ndiscrete times, at high frequency, which means that the time step tends toward\nzero. In addition, the drift and diffusion coefficients of the process X are\nassumed to be unknown. In this paper, we study the minimax rates of convergence\nof the nonparametric estimators of the square of the diffusion coefficient. Two\nobservation schemes are considered depending on the estimation interval. The\nsquare of the diffusion coefficient is estimated on the real line from repeated\nobservations of the process X, where the number of diffusion paths tends to\ninfinity. For the case of a compact estimation interval, we study the\nnonparametric estimation of the square of the diffusion coefficient constructed\nfrom a single diffusion path on one side and from repeated observations on the\nother side, where the number of trajectories tends to infinity. In each of\nthese cases, we establish minimax convergence rates of the risk of estimation\nof the diffusion coefficient over a space of Holder functions.",
        "We theoretically propose a new method in cavity- and surface-enhanced Raman\nspectroscopy (SERS) with improved temporal resolution in the measurement of\nstochastic Raman spectral fluctuations. Our approach combines Fourier\nspectroscopy and photon correlation to decouple the integration time from the\ntemporal resolution. Using statistical optics simulations, we establish the\nrelationship between time resolution and Raman signal strength, revealing that\ntypical Raman spectral fluctuations, commensurate with molecular conformational\ndynamics, can theoretically be resolved on micro- to millisecond timescales.\nThe method can further extract average single-molecule dynamics from small\nsub-ensembles, thereby potentially mitigating challenges in achieving strictly\nsingle-molecule isolation on SERS substrates.",
        "We investigate both the $Z$ and $H$ poles solutions for the Higgsino mass\nparameter $\\mu>0$ and $\\mu<0$ for the neutralino dark matter in light of the\nLHC supersymmetry searches and the direct detection dark matter experiments,\nLUX-ZEPLIN (LZ), in the Generalized Minimal Supergravity (GmSUGRA). Our study\nindicates that the latest experimental constraints from the LHC and LZ\nCollaborations exclude the light Higgsinos in the $Z$ and $H$ pole regions for\nthe $\\mu>0$ case. Interestingly, for the $\\mu < 0$ case, a very light Higgsinos\ncan still be consistent with the current constraints from the electroweakino\nsearches and LZ experiment in the $Z$ and $H$ poles. Consequently, the $\\mu <\n0$ case appears more promising and thus requires the dedicated efforts to make\ndefinitive conclusions about their current status from the experimental\nCollaborations. In this framework, our findings indicate a deviation of up to\n$2\\sigma$ from the central value of \\( a_\\mu \\equiv (g-2)_\\mu\/2 \\), resonating\nwith the experimental results reported by CMD and BDM.",
        "Braiding and fusion of Majorana zero modes are key elements of any future\ntopological Majorana-based quantum computer. Here, we investigate the fusion\ndynamics of Majorana zero modes in the spinless Kitaev model, as well as in a\nspinfull model describing magnet-superconductor hybrid structures. We consider\nvarious scenarios allowing us to reproduce the fusion rules of the Ising anyon\nmodel. Particular emphasis is given to the charge of the fermion obtained after\nfusing two Majorana zero modes: as long as it remains on the superconductor,\ncharge quantization is absent. When moving the fermion to a non-superconducting\nregion, such as a quantum dot, nearly-quantized charge can be measured. Our\nfindings confirm for both platforms that fusion dynamics of Majorana zero modes\ncan indeed be used for the readout of Majorana qubits.",
        "The optimal error exponents of binary composite i.i.d. state discrimination\nare trivially bounded by the worst-case pairwise exponents of discriminating\nindividual elements of the sets representing the two hypotheses, and in the\nfinite-dimensional classical case, these bounds in fact give exact single-copy\nexpressions for the error exponents. In contrast, in the non-commutative case,\nthe optimal exponents are only known to be expressible in terms of regularized\ndivergences, resulting in formulas that, while conceptually relevant,\npractically not very useful. In this paper, we develop further an approach\ninitiated in [Mosonyi, Szil\\'agyi, Weiner, IEEE Trans. Inf. Th.\n68(2):1032--1067, 2022] to give improved single-copy bounds on the error\nexponents by comparing not only individual states from the two hypotheses, but\nalso various unnormalized positive semi-definite operators associated to them.\nHere, we show a number of equivalent characterizations of such operators giving\nvalid bounds, and show that in the commutative case, considering weighted\ngeometric means of the states, and in the case of two states per hypothesis,\nconsidering weighted Kubo-Ando geometric means, are optimal for this approach.\nAs a result, we give a new characterization of the weighted Kubo-Ando geometric\nmeans as the only $2$-variable operator geometric means that are block\nadditive, tensor multiplicative, and satisfy the arithmetic-geometric mean\ninequality. We also extend our results to composite quantum channel\ndiscrimination, and show an analogous optimality property of the weighted\nKubo-Ando geometric means of two quantum channels, a notion that seems to be\nnew. We extend this concept to defining the notion of superoperator perspective\nfunction and establish some of its basic properties, which may be of\nindependent interest.",
        "Older male workers exhibit diverse retirement behaviors across occupations\nand respond differently to policy changes, influenced significantly by the\npart-time penalty, wage reduction faced by part-time workers compared to their\nfull-time counterparts. Many older individuals reduce their working hours, and\nin occupations with high part-time penalties, they tend to retire earlier, as\nobserved in data from Japan and the United States. This study develops a\ngeneral equilibrium model that incorporates occupational choices, endogenous\nlabor supply, highlighting that the impact on the retirement decision is\namplified by the presence of assets and pensions. I find that cutting\nemployees' pension benefits reduce aggregate labor supply in occupations with\nhigh part-time penalties in Japan, reducing overall welfare across the economy.\nIn contrast, increasing income tax credits and exempting pension form income\ntax boost labor supply across all occupations and enhance welfare by raising\ndisposable wages relative to the reservation wage. Reducing part-time penalties\nin high-penalty occupations also stimulate the labor supply in high-penalty\noccupations and improve long-term welfare.",
        "A major challenge in the deployment of hydrokinetic turbines in aquatic\nenvironments is the risk of fish collisions. Traditional fish collision models\noften oversimplify this risk by neglecting critical factors, such as the\nthickness of the turbine and accessory structures. Additionally, variations in\nfish size and species are frequently overlooked. This study addresses these\ngaps by developing a swimming mechanics-based fish-blade collision model. Using\na Lagrangian particle tracking approach, we simulate fish movements and\nevaluate collision risks with a representative hydrokinetic turbine, both with\nand without ducts. The model is applied to the velocity field at Baton Rouge,\nLouisiana, allowing for the assessment of collision risks across different fish\nspecies. The results offer valuable insights for turbine siting, optimization\nof turbine placement, and evaluation of protective designs to reduce\nenvironmental impacts in complex flow environments.",
        "Stochastic systems have a control-theoretic interpretation in which noise\nplays the role of endogenous control. In the weak-noise limit, relevant at low\ntemperatures or in large populations, control is optimal and an exact\nmathematical mapping from noise to control is described, where the maximizing\nthe probability of a state becomes the control objective. In Langevin dynamics\nnoise is identified directly with control, while in general Markov jump\nprocesses, which include chemical reaction networks and electronic circuits, we\nuse the Doi-Zel'dovich-Grassberger-Goldenfeld-Peliti path integral to identify\nthe `response' or `tilt' field $\\pi$ as control, which is proportional to the\nnoise in the semiclassical limit. This solves the longstanding problem of\ninterpreting $\\pi$. We illustrate the mapping on multistable chemical reaction\nnetworks and systems with unstable fixed points. The noise-control mapping\nbuilds intuition for otherwise puzzling phenomena of stochastic systems: why\nthe probability is generically a non-smooth function of state out of thermal\nequilibrium; why biological mechanisms can work better in the presence of\nnoise; and how agentic behavior emerges naturally without recourse to\nmysticism.",
        "As argued in arXiv:2104.10172, introducing a non-minimally coupled scalar\nfield, three-dimensional Einstein gravity can be extended by infinite families\nof theories which admit simple analytic generalizations of the charged BTZ\nblack hole. Depending on the gravitational couplings, the solutions may\ndescribe black holes with one or several horizons and with curvature or\nBTZ-like singularities. In other cases, the metric function behaves as\n$f(r)\\overset{(r\\rightarrow 0)}{\\sim} \\mathcal{O}(r^{2s})$ with $s\\geq 1$, and\nthe black holes are completely regular -- a feature unique to three dimensions.\nRegularity arises generically i.e., without requiring any fine-tuning of\nparameters. In this paper we show that all these theories satisfy Birkhoff\ntheorems, so that the most general spherically-symmetric solutions are given by\nthe corresponding static black holes. We perform a thorough characterization of\nthe Penrose diagrams of the solutions, finding a rich structure which includes,\nin particular, cases which tessellate the plane and others in which the\ndiagrams cannot be drawn in a single plane. We also study the motion of probe\nparticles on the black holes, finding that observers falling to regular black\nholes reach the center after a finite proper time. Contrary to the singular\ncases, the particles are not torn apart by tidal forces, so they oscillate\nbetween antipodal points describing many-universe orbits. We argue that in\nthose cases the region $r=0$ can be interpreted as a horizon with vanishing\nsurface gravity, giving rise to generic inner-extremal regular black hole\nsolutions. We also analyze the deep interior region of the solutions\nidentifying the presence of Kasner eons and the conditions under which they\ntake place. Finally, we construct new black hole solutions in the case in which\ninfinite towers of terms are included in the action.",
        "Interatomic potentials are essential for driving molecular dynamics (MD)\nsimulations, directly impacting the reliability of predictions regarding the\nphysical and chemical properties of materials. In recent years, machine-learned\npotentials (MLPs), trained against first-principles calculations, have become a\nnew paradigm in materials modeling as they provide a desirable balance between\naccuracy and computational cost. The neuroevolution potential (NEP) approach,\nimplemented in the open-source GPUMD software, has emerged as a promising\nmachine-learned potential, exhibiting impressive accuracy and exceptional\ncomputational efficiency. This review provides a comprehensive discussion on\nthe methodological and practical aspects of the NEP approach, along with a\ndetailed comparison with other representative state-of-the-art MLP approaches\nin terms of training accuracy, property prediction, and computational\nefficiency. We also demonstrate the application of the NEP approach to perform\naccurate and efficient MD simulations, addressing complex challenges that\ntraditional force fields typically can not tackle. Key examples include\nstructural properties of liquid and amorphous materials, chemical order in\ncomplex alloy systems, phase transitions, surface reconstruction, material\ngrowth, primary radiation damage, fracture in two-dimensional materials,\nnanoscale tribology, and mechanical behavior of compositionally complex alloys\nunder various mechanical loadings. This review concludes with a summary and\nperspectives on future extensions to further advance this rapidly evolving\nfield.",
        "In this article, the time-discretization of the fluid structure interaction\nmodel in the three-dimensional boundary domain is taken into account, which\nexplains the mechanical interaction between the blood flow and the Hookean\nelasticity. The interface between the two phases is given by a soft transition\nlayer and spreads to the finite thickness. On the implicit Euler scheme for\nthis discretization, We derive a variety of priori estimates and then use the\nFaedo-Galerkin method to prove the local well-poseedness results.",
        "The dynamics of learning in modern large AI systems is hierarchical, often\ncharacterized by abrupt, qualitative shifts akin to phase transitions observed\nin physical systems. While these phenomena hold promise for uncovering the\nmechanisms behind neural networks and language models, existing theories remain\nfragmented, addressing specific cases. In this paper, we posit that parameter\nsymmetry breaking and restoration serve as a unifying mechanism underlying\nthese behaviors. We synthesize prior observations and show how this mechanism\nexplains three distinct hierarchies in neural networks: learning dynamics,\nmodel complexity, and representation formation. By connecting these\nhierarchies, we highlight symmetry -- a cornerstone of theoretical physics --\nas a potential fundamental principle in modern AI.",
        "We investigate the detectability of extended mid-infrared (MIR) emission\nassociated with FU-Ori type objects (FUors) using the METIS coronagraphs on the\n39-m Extremely Large Telescope (ELT). The imaging simulations were made for\nthree representative filters ($\\lambda$=3.8, 4.8, and 11.3 micron) of the METIS\ninstrument. We demonstrate that the detectability of the extended MIR emission\nusing these coronagraphs is highly dependent on the uncertain nature of the\ncentral FUor and its circumstellar environment in various contexts. These\ncontexts are: (A) whether the central radiation source is either a flat\nself-luminous accretion disk or a star at near-infrared (NIR) wavelengths, (B)\nthe size of the accretion disk for the bright central MIR emission at\nmilliarcsecond scales, (C) whether the extended emission is due to either an\noptically thick disk or an optically thin envelope, and (D) dust grain models.\nObservations at $\\lambda$=3.8 micron will allow us to detect the extended\nemission in many cases, while the number of cases with detection may\nsignificantly decrease toward longer wavelengths due to the fainter nature of\nthe extended emission and high thermal background noise. In some cases, the\npresence of a binary companion can significantly hamper detections of the\nextended MIR emission. NIR and MIR imaging observations at existing 8-m class\ntelescopes, prior to the METIS observations, will be useful for (1) reducing\nthe many model uncertainties and (2) searching for binary companions associated\nwith FUors, therefore determining the best observing strategy using METIS.",
        "In this paper, we have found that some certain Fermat-type shift and\ndifference equations have the meromorphic solutions generated by Riccati type\nfunctions. Also we have solved the open problems posed by Liu and Yang (A note\non meromorphic solutions of Fermat types equations, An. Stiint. Univ. Al. I.\nCuza Lasi Mat. (N. S.), 62(2)(1), 317-325 (2016)). We have fortified the claims\nby some examples.",
        "The discovery of high-performance electrocatalysts is crucial for advancing\nsustainable energy technologies. Compositionally complex solid solutions\ncomprising multiple metals offer promising catalytic properties, yet their\nexploration is challenging due to the combinatorial explosion of possible\ncompositions. In this work, we combine combinatorial sputtering of thin-film\nmaterials libraries and their high-throughput characterization with Bayesian\noptimization to efficiently explore the quaternary composition space\nNi-Pd-Pt-Ru for the oxygen evolution reaction in alkaline media. Using this\nmethod, the global activity optimum of pure Ru was identified after covering\nless than 20% of the complete composition space with six materials libraries.\nSix additional libraries were fabricated to validate the activity trend. The\nresulting dataset is used to formulate general guidelines for the efficient\ncomposition space exploration using combinatorial synthesis paired with\nBayesian optimization.",
        "In this paper, we study the hydrostatic approximation for the 3D Oldroyd-B\nmodel. Firstly, we derive the hydrostatic approximate system for this model and\nprove the global well-posedness of the limit system with small analytic initial\ndata in horizontal variable. Then we justify the hydrostatic limit strictly\nfrom the re-scaled Oldroyd-B model to the hydrostatic Oldroyd-B model and\nobtain the precise convergence rate.",
        "The theoretical foundation of quantum sensing is rooted in the Cram\\'er-Rao\nformalism, which establishes quantitative precision bounds for a given quantum\nprobe. In many practical scenarios, where more than one parameter is unknown,\nthe multi-parameter Cram\\'er-Rao bound (CRB) applies. Since this is a matrix\ninequality involving the inverse of the quantum Fisher information matrix\n(QFIM), the formalism breaks down when the QFIM is singular. In this paper, we\nexamine the physical origins of such singularities, showing that they result\nfrom an over-parametrization on the metrological level. This is itself caused\nby emergent metrological symmetries, whereby the same set of measurement\noutcomes are obtained for different combinations of system parameters. Although\nthe number of effective parameters is equal to the number of non-zero QFIM\neigenvalues, the Cram\\'er-Rao formalism typically does not provide information\nabout the effective parameter encoding. Instead, we demonstrate through a\nseries of concrete examples that Bayesian estimation can provide deep insights.\nIn particular, the metrological symmetries appear in the Bayesian posterior\ndistribution as lines of persistent likelihood running through the space of\nunknown parameters. These lines are contour lines of the effective parameters\nwhich, through suitable parameter transformations, can be estimated and follow\ntheir own effective CRBs.",
        "We derive the quantum master equation predicting how the translational and\nrotational dynamics of a nanoparticle is affected by the emission of surface\nadsorbates. This is motivated by recent experiments which prepared the motion\nof internally hot silica particles in the deep quantum regime. In the limit of\na well localized nanoparticle the ro-translational dynamics can be\ncharacterized by diffusion rates in quantitative agreement with classical\nexpectations. The theory is also suited to describe the decoherence effect of\noutgassing and sublimation."
      ]
    }
  },
  {
    "id":2411.18259,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Quantifying the performance of machine learning models in materials discovery",
    "start_abstract":"The predictive capabilities of machine learning (ML) models used in materials discovery are typically measured using simple statistics such as the root-mean-square error (RMSE) or coefficient determination ($r^2$) between ML-predicted property values and their known values. A tempting assumption is that with low should be effective at guiding discovery, conversely, high give poor performance. However, we observe no clear connection exists a \"static\" quantity averaged across an entire training set, RMSE, ML model's ability to dynamically guide iterative (and often extrapolative) novel targeted properties. In this work, simulate sequential (SL)-guided process demonstrate decoupling traditional model metrics performance discoveries. We show depends strongly on (1) target range within distribution (e.g., whether 1st 10th decile material desired); (2) incorporation uncertainty estimates SL acquisition function; (3) scientist interested one many targets; (4) how iterations allowed. To overcome limitations static robustly capture performance, recommend Discovery Yield ($DY$), measure high-performing were discovered during SL, Probability ($DP$), likelihood discovering any point process.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Physics-informed machine learning"
      ],
      "abstract":[
        "Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Attention to Trajectory: Trajectory-Aware Open-Vocabulary Tracking",
        "One citation, one vote! A new approach for analysing\n  check-all-that-apply (CATA) data in sensometrics, using L1 norm methods",
        "Split-n-Chain: Privacy-Preserving Multi-Node Split Learning with\n  Blockchain-Based Auditability",
        "Analysis of kinematics of mechanisms containing revolute joints",
        "Controllable Emotion Generation with Emotion Vectors",
        "Automated Refactoring of Non-Idiomatic Python Code: A Differentiated\n  Replication with LLMs",
        "DanmuA11y: Making Time-Synced On-Screen Video Comments (Danmu)\n  Accessible to Blind and Low Vision Users via Multi-Viewer Audio Discussions",
        "Mixing Any Cocktail with Limited Ingredients: On the Structure of Payoff\n  Sets in Multi-Objective MDPs and its Impact on Randomised Strategies",
        "Asymptotic Optimism of Random-Design Linear and Kernel Regression Models",
        "On Nash Equilibria in Play-Once and Terminal Deterministic Graphical\n  Games",
        "Optimal generalisation and learning transition in extensive-width\n  shallow neural networks near interpolation",
        "MAST-Pro: Dynamic Mixture-of-Experts for Adaptive Segmentation of\n  Pan-Tumors with Knowledge-Driven Prompts",
        "VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion\n  Control",
        "A class of anisotropic diffusion-transport equations in non-divergence\n  form",
        "A conjecture on monomial realizations and polyhedral realizations for\n  crystal bases",
        "Maximal Representation Dimensions of Algebraic Tori of Fixed Dimension\n  Over Arbitrary Fields",
        "Duals of limiting interpolation spaces",
        "THOR: A Non-Speculative Value Dependent Timing Side Channel Attack\n  Exploiting Intel AMX",
        "MHAF-YOLO: Multi-Branch Heterogeneous Auxiliary Fusion YOLO for accurate\n  object detection",
        "HybridLinker: Topology-Guided Posterior Sampling for Enhanced Diversity\n  and Validity in 3D Molecular Linker Generation",
        "Utilizing API Response for Test Refinement",
        "Runway capacity expansion planning for public airports under demand\n  uncertainty",
        "Exponentially accurate spectral Monte Carlo method for linear PDEs and\n  their error estimates",
        "Problems on handlebody groups",
        "Recursive decoding of projective Reed-Muller codes",
        "SkyRover: A Modular Simulator for Cross-Domain Pathfinding",
        "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion\n  for Video Understanding",
        "Debate Helps Weak-to-Strong Generalization",
        "Toeplitz Operators on Fock Space $F_{\\alpha}^{\\infty}$"
      ],
      "abstract":[
        "Open-Vocabulary Multi-Object Tracking (OV-MOT) aims to enable approaches to\ntrack objects without being limited to a predefined set of categories. Current\nOV-MOT methods typically rely primarily on instance-level detection and\nassociation, often overlooking trajectory information that is unique and\nessential for object tracking tasks. Utilizing trajectory information can\nenhance association stability and classification accuracy, especially in cases\nof occlusion and category ambiguity, thereby improving adaptability to novel\nclasses. Thus motivated, in this paper we propose \\textbf{TRACT}, an\nopen-vocabulary tracker that leverages trajectory information to improve both\nobject association and classification in OV-MOT. Specifically, we introduce a\n\\textit{Trajectory Consistency Reinforcement} (\\textbf{TCR}) strategy, that\nbenefits tracking performance by improving target identity and category\nconsistency. In addition, we present \\textbf{TraCLIP}, a plug-and-play\ntrajectory classification module. It integrates \\textit{Trajectory Feature\nAggregation} (\\textbf{TFA}) and \\textit{Trajectory Semantic Enrichment}\n(\\textbf{TSE}) strategies to fully leverage trajectory information from visual\nand language perspectives for enhancing the classification results. Extensive\nexperiments on OV-TAO show that our TRACT significantly improves tracking\nperformance, highlighting trajectory information as a valuable asset for\nOV-MOT. Code will be released.",
        "A unified framework is provided for analysing check-all-that-apply (CATA)\nproduct data following the ``one citation, one vote\" principle. CATA data arise\nfrom studies where A consumers evaluate P products by describing samples by\nchecking all of the T terms that apply. Giving every citation the same weight,\nregardless of the assessor, product, or term, leads to analyses based on the L1\nnorm where the median absolute deviation is the measure of dispersion. Five\npermutation tests are proposed to answer the following questions. Do any\nproducts differ? For which terms do products differ? Within each of the terms,\nwhich products differ? Which product pairs differ? On which terms does each\nproduct pair differ? Additionally, we show how products and terms can be\nclustered following the ``one citation, one vote\" principle and how L1-norm\nprincipal component analysis (L1-norm PCA) can be applied to visualize CATA\nresults in few dimensions. Together, the permutation tests, clustering methods,\nand L1-norm PCA provide a unified approach. The proposed methods are\nillustrated using a data set in which 100 consumers evaluated 11 products using\n34 CATA terms.R code is provided to perform the analyses.",
        "Deep learning, when integrated with a large amount of training data, has the\npotential to outperform machine learning in terms of high accuracy. Recently,\nprivacy-preserving deep learning has drawn significant attention of the\nresearch community. Different privacy notions in deep learning include privacy\nof data provided by data-owners and privacy of parameters and\/or\nhyperparameters of the underlying neural network. Federated learning is a\npopular privacy-preserving execution environment where data-owners participate\nin learning the parameters collectively without leaking their respective data\nto other participants. However, federated learning suffers from certain\nsecurity\/privacy issues. In this paper, we propose Split-n-Chain, a variant of\nsplit learning where the layers of the network are split among several\ndistributed nodes. Split-n-Chain achieves several privacy properties:\ndata-owners need not share their training data with other nodes, and no nodes\nhave access to the parameters and hyperparameters of the neural network (except\nthat of the respective layers they hold). Moreover, Split-n-Chain uses\nblockchain to audit the computation done by different nodes. Our experimental\nresults show that: Split-n-Chain is efficient, in terms of time required to\nexecute different phases, and the training loss trend is similar to that for\nthe same neural network when implemented in a monolithic fashion.",
        "Kinematics of rigid bodies can be analyzed in many different ways. The\nadvantage of using Euler parameters is that the resulting equations are\npolynomials and hence computational algebra, in particular Gr\\\"obner bases, can\nbe used to study them. The disadvantage of the Gr\\\"obner basis methods is that\nthe computational complexity grows quite fast in the worst case in the number\nof variables and the degree of polynomials. In the present article we show how\nto simplify computations when the mechanism contains revolute joints. The idea\nis based on the fact that the ideal representing the constraints of the\nrevolute joint is not prime. Choosing the appropriate prime component reduces\nsignificantly the computational cost. We illustrate the method by applying it\nto the well known Bennett's and Bricard's mechanisms, but it can be applied to\nany mechanism which has revolute joints.",
        "In recent years, technologies based on large-scale language models (LLMs)\nhave made remarkable progress in many fields, especially in customer service,\ncontent creation, and embodied intelligence, showing broad application\npotential. However, The LLM's ability to express emotions with proper tone,\ntiming, and in both direct and indirect forms is still insufficient but\nsignificant. Few works have studied on how to build the controlable emotional\nexpression capability of LLMs. In this work, we propose a method for emotion\nexpression output by LLMs, which is universal, highly flexible, and well\ncontrollable proved with the extensive experiments and verifications. This\nmethod has broad application prospects in fields involving emotions output by\nLLMs, such as intelligent customer service, literary creation, and home\ncompanion robots. The extensive experiments on various LLMs with different\nmodel-scales and architectures prove the versatility and the effectiveness of\nthe proposed method.",
        "In the Python ecosystem, the adoption of idiomatic constructs has been\nfostered because of their expressiveness, increasing productivity and even\nefficiency, despite controversial arguments concerning familiarity or\nunderstandability issues. Recent research contributions have proposed\napproaches -- based on static code analysis and transformation -- to\nautomatically identify and enact refactoring opportunities of non-idiomatic\ncode into idiomatic ones. Given the potential recently offered by Large\nLanguage Models (LLMs) for code-related tasks, in this paper, we present the\nresults of a replication study in which we investigate GPT-4 effectiveness in\nrecommending and suggesting idiomatic refactoring actions. Our results reveal\nthat GPT-4 not only identifies idiomatic constructs effectively but frequently\nexceeds the benchmark in proposing refactoring actions where the existing\nbaseline failed. A manual analysis of a random sample shows the correctness of\nthe obtained recommendations. Our findings underscore the potential of LLMs to\nachieve tasks where, in the past, implementing recommenders based on complex\ncode analyses was required.",
        "By overlaying time-synced user comments on videos, Danmu creates a\nco-watching experience for online viewers. However, its visual-centric design\nposes significant challenges for blind and low vision (BLV) viewers. Our\nformative study identified three primary challenges that hinder BLV viewers'\nengagement with Danmu: the lack of visual context, the speech interference\nbetween comments and videos, and the disorganization of comments. To address\nthese challenges, we present DanmuA11y, a system that makes Danmu accessible by\ntransforming it into multi-viewer audio discussions. DanmuA11y incorporates\nthree core features: (1) Augmenting Danmu with visual context, (2) Seamlessly\nintegrating Danmu into videos, and (3) Presenting Danmu via multi-viewer\ndiscussions. Evaluation with twelve BLV viewers demonstrated that DanmuA11y\nsignificantly improved Danmu comprehension, provided smooth viewing\nexperiences, and fostered social connections among viewers. We further\nhighlight implications for enhancing commentary accessibility in video-based\nsocial media and live-streaming platforms.",
        "We consider multi-dimensional payoff functions in Markov decision processes,\nand ask whether a given expected payoff vector can be achieved or not. In\ngeneral, pure strategies (i.e., not resorting to randomisation) do not suffice\nfor this problem.\n  We study the structure of the set of expected payoff vectors of all\nstrategies given a multi-dimensional payoff function and its consequences\nregarding randomisation requirements for strategies. In particular, we prove\nthat for any payoff for which the expectation is well-defined under all\nstrategies, it is sufficient to mix (i.e., randomly select a pure strategy at\nthe start of a play and committing to it for the rest of the play) finitely\nmany pure strategies to approximate any expected payoff vector up to any\nprecision. Furthermore, for any payoff for which the expected payoff is finite\nunder all strategies, any expected payoff can be obtained exactly by mixing\nfinitely many strategies.",
        "We derived the closed-form asymptotic optimism of linear regression models\nunder random designs, and generalizes it to kernel ridge regression. Using\nscaled asymptotic optimism as a generic predictive model complexity measure, we\nstudied the fundamental different behaviors of linear regression model, tangent\nkernel (NTK) regression model and three-layer fully connected neural networks\n(NN). Our contribution is two-fold: we provided theoretical ground for using\nscaled optimism as a model predictive complexity measure; and we show\nempirically that NN with ReLUs behaves differently from kernel models under\nthis measure. With resampling techniques, we can also compute the optimism for\nregression models with real data.",
        "We consider finite $n$-person deterministic graphical games and study the\nexistence of pure stationary Nash-equilibrium in such games. We assume that all\ninfinite plays are equivalent and form a unique outcome, while each terminal\nposition is a separate outcome. It is known that for $n=2$ such a game always\nhas a Nash equilibrium, while that may not be true for $n > 2$.\n  A game is called {\\em play-once} if each player controls a unique position\nand {\\em terminal} if any terminal outcome is better than the infinite one for\neach player. We prove in this paper that play-once games have Nash equilibria.\n  We also show that terminal games have Nash equilibria if they have at most\nthree terminals.",
        "We consider a teacher-student model of supervised learning with a\nfully-trained 2-layer neural network whose width $k$ and input dimension $d$\nare large and proportional. We compute the Bayes-optimal generalisation error\nof the network for any activation function in the regime where the number of\ntraining data $n$ scales quadratically with the input dimension, i.e., around\nthe interpolation threshold where the number of trainable parameters $kd+k$ and\nof data points $n$ are comparable. Our analysis tackles generic weight\ndistributions. Focusing on binary weights, we uncover a discontinuous phase\ntransition separating a \"universal\" phase from a \"specialisation\" phase. In the\nfirst, the generalisation error is independent of the weight distribution and\ndecays slowly with the sampling rate $n\/d^2$, with the student learning only\nsome non-linear combinations of the teacher weights. In the latter, the error\nis weight distribution-dependent and decays faster due to the alignment of the\nstudent towards the teacher network. We thus unveil the existence of a highly\npredictive solution near interpolation, which is however potentially hard to\nfind.",
        "Accurate tumor segmentation is crucial for cancer diagnosis and treatment.\nWhile foundation models have advanced general-purpose segmentation, existing\nmethods still struggle with: (1) limited incorporation of medical priors, (2)\nimbalance between generic and tumor-specific features, and (3) high\ncomputational costs for clinical adaptation. To address these challenges, we\npropose MAST-Pro (Mixture-of-experts for Adaptive Segmentation of pan-Tumors\nwith knowledge-driven Prompts), a novel framework that integrates dynamic\nMixture-of-Experts (D-MoE) and knowledge-driven prompts for pan-tumor\nsegmentation. Specifically, text and anatomical prompts provide domain-specific\npriors, guiding tumor representation learning, while D-MoE dynamically selects\nexperts to balance generic and tumor-specific feature learning, improving\nsegmentation accuracy across diverse tumor types. To enhance efficiency, we\nemploy Parameter-Efficient Fine-Tuning (PEFT), optimizing MAST-Pro with\nsignificantly reduced computational overhead. Experiments on multi-anatomical\ntumor datasets demonstrate that MAST-Pro outperforms state-of-the-art\napproaches, achieving up to a 5.20% improvement in average DSC while reducing\ntrainable parameters by 91.04%, without compromising accuracy.",
        "Despite significant advancements in video generation, inserting a given\nobject into videos remains a challenging task. The difficulty lies in\npreserving the appearance details of the reference object and accurately\nmodeling coherent motions at the same time. In this paper, we propose\nVideoAnydoor, a zero-shot video object insertion framework with high-fidelity\ndetail preservation and precise motion control. Starting from a text-to-video\nmodel, we utilize an ID extractor to inject the global identity and leverage a\nbox sequence to control the overall motion. To preserve the detailed appearance\nand meanwhile support fine-grained motion control, we design a pixel warper. It\ntakes the reference image with arbitrary key-points and the corresponding\nkey-point trajectories as inputs. It warps the pixel details according to the\ntrajectories and fuses the warped features with the diffusion U-Net, thus\nimproving detail preservation and supporting users in manipulating the motion\ntrajectories. In addition, we propose a training strategy involving both videos\nand static images with a weighted loss to enhance insertion quality.\nVideoAnydoor demonstrates significant superiority over existing methods and\nnaturally supports various downstream applications (e.g., talking head\ngeneration, video virtual try-on, multi-region editing) without task-specific\nfine-tuning.",
        "We generalize Einstein's probabilistic method for the Brownian motion to\nstudy compressible fluids in porous media. The multi-dimensional case is\nconsidered with general probability distribution functions. By relating the\nexpected displacement per unit time with the velocity of the fluid, we derive\nan anisotropic diffusion equation in non-divergence form that contains a\ntransport term. Under the Darcy law assumption, a corresponding nonlinear\npartial differential equations for the density function is obtained. The\nclassical solutions of this equation are studied, and the maximum and strong\nmaximum principles are established. We also obtain exponential decay estimates\nfor the solutions for all time, and particularly, their exponential convergence\nas time tends to infinity. Our analysis uses some transformations of the\nBernstein-Cole--Hopf type which are explicitly constructed even for very\ngeneral equation of state. Moreover, the Lemma of Growth in time is proved and\nutilized in order to achieve the above decaying estimates.",
        "Crystal bases are powerful combinatorial tools in the representation theory\nof quantum groups $U_q(\\mathfrak{g})$ for a symmetrizable Kac-Moody algebras\n$\\mathfrak{g}$. The polyhedral realizations are combinatorial descriptions of\nthe crystal base $B(\\infty)$ for Verma modules in terms of the set of integer\npoints of a polyhedral cone, which equals the string cone when $\\mathfrak{g}$\nis finite dimensional simple. It is a fundamental and natural problem to find\nexplicit forms of the polyhedral cone. The monomial realization expresses\ncrystal bases $B(\\lambda)$ of integrable highest weight representations as\nLaurent monomials with double indexed variables. In this paper, we give a\nconjecture between explicit forms of the polyhedral cones and monomial\nrealizations. We prove the conjecture is true when $\\mathfrak{g}$ is a\nclassical Lie algebra, a rank $2$ Kac-Moody algebra or a classical affine Lie\nalgebra.",
        "We define the representation dimension of an algebraic torus $T$ to be the\nminimal positive integer $r$ such that there exists a faithful embedding $T\n\\hookrightarrow \\operatorname{GL}_r$. Given a positive integer $n$, there\nexists a maximal representation dimension of all $n$-dimensional algebraic tori\nover all fields. In this paper, we use the theory of group actions on lattices\nto find lower bounds on this maximum for all $n$. Further, we find the exact\nmaximum value for irreducible tori for all $n \\in \\left\\lbrace 1, 2, \\dots, 10,\n11, 13, 17, 19, 23\\right\\rbrace$ and conjecturally infinitely many primes $n$.",
        "The aim of the paper is to establish duals of the limiting real interpolation\n$K$- and $J$-spaces $(X_0,X_1)_{0,q,v;K}$ and $(X_0,X_1)_{0,q,v;J}$, where\n$(X_0,X_1)$ is a compatible couple of Banach spaces, $1\\le q<\\infty$, $v$ is a\nslowly varying function on the interval $(0,\\infty)$, and the symbols $K$ and\n$J$ stand for the Peetre $K$- and $J$-functionals. In the case of the classical\nreal interpolation method $(X_0,X_1)_{\\theta,q}$, where $\\theta \\in (0,1)$ and\n$1\\le q < \\infty$, this problem was solved by Lions and Peetre.",
        "The rise of on-chip accelerators signifies a major shift in computing, driven\nby the growing demands of artificial intelligence (AI) and specialized\napplications. These accelerators have gained popularity due to their ability to\nsubstantially boost performance, cut energy usage, lower total cost of\nownership (TCO), and promote sustainability. Intel's Advanced Matrix Extensions\n(AMX) is one such on-chip accelerator, specifically designed for handling tasks\ninvolving large matrix multiplications commonly used in machine learning (ML)\nmodels, image processing, and other computational-heavy operations. In this\npaper, we introduce a novel value-dependent timing side-channel vulnerability\nin Intel AMX. By exploiting this weakness, we demonstrate a software-based,\nvalue-dependent timing side-channel attack capable of inferring the sparsity of\nneural network weights without requiring any knowledge of the confidence score,\nprivileged access or physical proximity. Our attack method can fully recover\nthe sparsity of weights assigned to 64 input elements within 50 minutes, which\nis 631% faster than the maximum leakage rate achieved in the Hertzbleed attack.",
        "Due to the effective multi-scale feature fusion capabilities of the Path\nAggregation FPN (PAFPN), it has become a widely adopted component in YOLO-based\ndetectors. However, PAFPN struggles to integrate high-level semantic cues with\nlow-level spatial details, limiting its performance in real-world applications,\nespecially with significant scale variations. In this paper, we propose\nMHAF-YOLO, a novel detection framework featuring a versatile neck design called\nthe Multi-Branch Auxiliary FPN (MAFPN), which consists of two key modules: the\nSuperficial Assisted Fusion (SAF) and Advanced Assisted Fusion (AAF). The SAF\nbridges the backbone and the neck by fusing shallow features, effectively\ntransferring crucial low-level spatial information with high fidelity.\nMeanwhile, the AAF integrates multi-scale feature information at deeper neck\nlayers, delivering richer gradient information to the output layer and further\nenhancing the model learning capacity. To complement MAFPN, we introduce the\nGlobal Heterogeneous Flexible Kernel Selection (GHFKS) mechanism and the\nReparameterized Heterogeneous Multi-Scale (RepHMS) module to enhance feature\nfusion. RepHMS is globally integrated into the network, utilizing GHFKS to\nselect larger convolutional kernels for various feature layers, expanding the\nvertical receptive field and capturing contextual information across spatial\nhierarchies. Locally, it optimizes convolution by processing both large and\nsmall kernels within the same layer, broadening the lateral receptive field and\npreserving crucial details for detecting smaller targets. The source code of\nthis work is available at: https:\/\/github.com\/yang-0201\/MHAF-YOLO.",
        "Linker generation is critical in drug discovery applications such as lead\noptimization and PROTAC design, where molecular fragments are assembled into\ndiverse drug candidates. Existing methods fall into PC-Free and PC-Aware\ncategories based on their use of 3D point clouds (PC). PC-Free models\nprioritize diversity but suffer from lower validity due to overlooking PC\nconstraints, while PC-Aware models ensure higher validity but restrict\ndiversity by enforcing strict PC constraints. To overcome these trade-offs\nwithout additional training, we propose HybridLinker, a framework that enhances\nPC-Aware inference by providing diverse bonding topologies from a pretrained\nPC-Free model as guidance. At its core, we propose LinkerDPS, the first\ndiffusion posterior sampling (DPS) method operating across PC-Free and PC-Aware\nspaces, bridging molecular topology with 3D point clouds via an energy-inspired\nfunction. By transferring the diverse sampling distribution of PC-Free models\ninto the PC-Aware distribution, HybridLinker significantly and consistently\nsurpasses baselines, improving both validity and diversity in foundational\nmolecular design and applied property optimization tasks, establishing a new\nDPS framework in the molecular and graph domains beyond imaging.",
        "Most of the web services are offered in the form of RESTful APIs. This has\nled to an active research interest in API testing to ensure the reliability of\nthese services. While most of the testing techniques proposed in the past rely\non the API specification to generate the test cases, a major limitation of such\nan approach is that in the case of an incomplete or inconsistent specification,\nthe test cases may not be realistic in nature and would result in a lot of 4xx\nresponse due to invalid input. This is indicative of poor test quality.\nLearning-based approaches may learn about valid inputs but often require a\nlarge number of request-response pairs to learn the constraints, making it\ninfeasible to be readily used in the industry. To address this limitation, this\npaper proposes a dynamic test refinement approach that leverages the response\nmessage. The response is used to infer the point in the API testing flow where\na test scenario fix is required. Using an intelligent agent, the approach adds\nconstraints to the API specification that are further used to generate a test\nscenario accounting for the learned constraint from the response. Following a\ngreedy approach, the iterative learning and refinement of test scenarios are\nobtained from the API testing system. The proposed approach led to a decrease\nin the number of 4xx responses, taking a step closer to generating more\nrealistic test cases with high coverage that would aid in functional testing. A\nhigh coverage was obtained from a lesser number of API requests, as compared\nwith the state-of-the-art search-based API Testing tools.",
        "Flight delay is a significant issue affecting air travel. The runway system,\nfrequently falling short of demand, serves as a bottleneck. As demand\nincreases, runway capacity expansion becomes imperative to mitigate congestion.\nHowever, the decision to expand runway capacity is challenging due to inherent\nuncertainties in demand forecasts. This paper presents a novel approach to\nmodeling air traffic demand growth as a jump diffusion process, incorporating\ntwo layers of uncertainty: Geometric Brownian Motion (GBM) for continuous\nvariability and a Poisson process to capture the impact of crisis events, such\nas natural disasters or public health emergencies, on decision-making. We\npropose a real options model to jointly evaluate the interrelated factors of\noptimal runway capacity and investment timing under uncertainty, with\ninvestment timing linked to trigger demand. The findings suggest that increased\nuncertainty indicates more conservative decision-making. Furthermore, the\nrelationship between optimal investment timing and expansion size is complex:\nif the expansion size remains unchanged, the trigger demand decreases as the\ndemand growth rate increases; if the expansion size experiences a jump, the\ntrigger demand also exhibits a sharp rise. This work provides valuable insights\nfor airport authorities for informed capacity expansion decision-making.",
        "This paper introduces a spectral Monte Carlo iterative method (SMC) for\nsolving linear Poisson and parabolic equations driven by $\\alpha$-stable L\\'evy\nprocess with $\\alpha\\in (0,2)$, which was initially proposed and developed by\nGobet and Maire in their pioneering works (Monte Carlo Methods Appl 10(3-4),\n275--285, 2004, and SIAM J Numer Anal 43(3), 1256--1275, 2005) for the case\n$\\alpha=2$. The novel method effectively integrates multiple computational\ntechniques, including the interpolation based on generalized Jacobi functions\n(GJFs), space-time spectral methods, control variates techniques, and a novel\nwalk-on-sphere method (WOS). The exponential convergence of the error bounds is\nrigorously established through finite iterations for both Poisson and parabolic\nequations involving the integral fractional Laplacian operator. Remarkably, the\nproposed space-time spectral Monte Carlo method (ST-SMC) for the parabolic\nequation is unified for both $\\alpha\\in(0,2)$ and $\\alpha=2$. Extensive\nnumerical results are provided to demonstrate the spectral accuracy and\nefficiency of the proposed method, thereby validating the theoretical findings.",
        "We survey a number of constructions and open problems related to the\nhandlebody group, with a focus on recent trends in geometric group theory,\n(co)homological properties, and its relationship to outer automorphism groups\nof free groups. We also briefly describe how the \\emph{cheap\n$\\alpha$-rebuilding property} of Abert, Bergeron, Fraczyk, and Gaboriau can be\napplied using the disc complex to deduce results about the homology growth of\nthe handlebody group.",
        "We give a recursive decoding algorithm for projective Reed-Muller codes\nmaking use of a decoder for affine Reed-Muller codes. We determine the number\nof errors that can be corrected in this way, which is the current highest for\ndecoders of projective Reed-Muller codes. We show when we can decode up to the\nerror correction capability of these codes, and we compute the order of\ncomplexity of the algorithm, which is given by that of the chosen decoder for\naffine Reed-Muller codes.",
        "Unmanned Aerial Vehicles (UAVs) and Automated Guided Vehicles (AGVs)\nincreasingly collaborate in logistics, surveillance, inspection tasks and etc.\nHowever, existing simulators often focus on a single domain, limiting\ncross-domain study. This paper presents the SkyRover, a modular simulator for\nUAV-AGV multi-agent pathfinding (MAPF). SkyRover supports realistic agent\ndynamics, configurable 3D environments, and convenient APIs for external\nsolvers and learning methods. By unifying ground and aerial operations, it\nfacilitates cross-domain algorithm design, testing, and benchmarking.\nExperiments highlight SkyRover's capacity for efficient pathfinding and\nhigh-fidelity simulations in UAV-AGV coordination. Project is available at\nhttps:\/\/sites.google.com\/view\/mapf3d\/home.",
        "In this paper, we introduce LLaVA-Octopus, a novel video multimodal large\nlanguage model. LLaVA-Octopus adaptively weights features from different visual\nprojectors based on user instructions, enabling us to leverage the\ncomplementary strengths of each projector. We observe that different visual\nprojectors exhibit distinct characteristics when handling specific tasks. For\ninstance, some projectors excel at capturing static details, while others are\nmore effective at processing temporal information, and some are better suited\nfor tasks requiring temporal coherence. By dynamically adjusting feature\nweights according to user instructions, LLaVA-Octopus dynamically selects and\ncombines the most suitable features, significantly enhancing the model's\nperformance in multimodal tasks. Experimental results demonstrate that\nLLaVA-Octopus achieves excellent performance across multiple benchmarks,\nespecially in tasks such as video question answering, long video understanding,\nand comprehensive multi-choices benchmarks, highlighting its broad application\npotential.",
        "Common methods for aligning already-capable models with desired behavior rely\non the ability of humans to provide supervision. However, future superhuman\nmodels will surpass the capability of humans. Therefore, humans will only be\nable to weakly supervise superhuman models. This expected deficiency of human\nevaluation would weaken the safety of future AI systems. Scalable oversight and\nweak-to-strong generalization are two complementary approaches to tackle this\nissue. In this paper, we attempt to combine the strengths of these two\napproaches to further improve alignment. Specifically, we investigate ways of\nimproving human supervision with a strong pretrained model and then supervise\nthe strong model with enhanced weak human supervision. To make iterative\nempirical progress, we consider an analogy: can we use a strong model to\nimprove weak model supervision and then use it to supervise the strong model?\nWe empirically test it by finetuning a small weak model on ground truth labels\nwith the additional help from a large strong model, and then finetuning the\nstrong model on labels generated by the weak model. We find that debate can\nassist a weak model in extracting trustworthy information from an untrustworthy\nstrong model, which provides leverage as context on samples when training a\nweak model. We also show that an ensemble of weak models helps exploit long\narguments generated by strong model debaters and obtain a more robust\nsupervision estimate. Extensive experiments on the OpenAI weak-to-strong NLP\nbenchmarks show that the combination approach leads to better alignment, which\nindicates that debate has the potential to help weak-to-strong generalization.",
        "In this paper, we study necessary and sufficient conditions for a positive\nBorel measure $\\mu$ on the complex space $\\mathbb{C}$ to be a $(\\infty,q)$ or\n$(p,\\infty)$ (vanishing) Fock-Carleson measure through its Berezin transform.\nThen we discuss boundedness and compactness of the Toeplitz operator $T_{\\mu}$\nwith a positive Borel measure $\\mu$ as symbol on Fock space\n$F_{\\alpha}^{\\infty}$. Furthermore, we charaterize these properties of the\nToeplitz operator T_{\\varphi}$ with a symbol $\\varphi$ which is in $BMO$."
      ]
    }
  },
  {
    "id":2411.18253,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Multimodal data fusion for cancer biomarker discovery with deep learning",
    "start_abstract":"Technological advances have made it possible to study a patient from multiple angles with high-dimensional, high-throughput multiscale biomedical data. In oncology, massive amounts of data are being generated, ranging from molecular, histopathology, radiology to clinical records. The introduction of deep learning has greatly advanced the analysis of biomedical data. However, most approaches focus on single data modalities, leading to slow progress in methods to integrate complementary data types. Development of effective multimodal fusion approaches is becoming increasingly important as a single modality might not be consistent and sufficient to capture the heterogeneity of complex diseases to tailor medical care and improve personalized medicine. Many initiatives now focus on integrating these disparate modalities to unravel the biological processes involved in multifactorial diseases such as cancer. However, many obstacles remain, including lack of usable data as well as methods for clinical validation and interpretation. Here, we cover these current challenges and reflect on opportunities through deep learning to tackle data sparsity and scarcity, multimodal interpretability and standardization of datasets.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Artificial intelligence for predictive biomarker discovery in immuno-oncology: a systematic review"
      ],
      "abstract":[
        "Background: The widespread use of immune checkpoint inhibitors (ICIs) has revolutionised treatment of multiple cancer types. However, selecting patients who may benefit from ICI remains challenging. Artificial intelligence (AI) approaches allow exploitation of high-dimension oncological data in research and development of precision immuno-oncology. Materials and methods: We conducted a systematic literature review of peer-reviewed original articles studying the ICI efficacy prediction in cancer patients across five data modalities: genomics (including genomics, transcriptomics, and epigenomics), radiomics, digital pathology (pathomics), and real-world and multimodality data. Results: A total of 90 studies were included in this systematic review, with 80% published in 2021-2022. Among them, 37 studies included genomic, 20 radiomic, 8 pathomic, 20 real-world, and 5 multimodal data. Standard machine learning (ML) methods were used in 72% of studies, deep learning (DL) methods in 22%, and both in 6%. The most frequently studied cancer type was non-small-cell lung cancer (36%), followed by melanoma (16%), while 25% included pan-cancer studies. No prospective study design incorporated AI-based methodologies from the outset; rather, all implemented AI as a post hoc analysis. Novel biomarkers for ICI in radiomics and pathomics were identified using AI approaches, and molecular biomarkers have expanded past genomics into transcriptomics and epigenomics. Finally, complex algorithms and new types of AI-based markers, such as meta-biomarkers, are emerging by integrating multimodal\/multi-omics data. Conclusion: AI-based methods have expanded the horizon for biomarker discovery, demonstrating the power of integrating multimodal data from existing datasets to discover new meta-biomarkers. While most of the included studies showed promise for AI-based prediction of benefit from immunotherapy, none provided high-level evidence for immediate practice change. A priori planned prospective trial designs are needed to cover all lifecycle steps of these software biomarkers, from development and validation to integration into clinical practice."
      ],
      "categories":[
        "Immunotherapy"
      ]
    },
    "list":{
      "title":[
        "One-dimensional linear analysis and numerical simulations of Alfven\n  waves in a force-free magnetosphere around a Kerr black hole",
        "MEMSDuino: An Arduino-Based MEMS Switch Controller",
        "On binomial edge ideals of corona of graphs",
        "A Frequency-Domain Opportunistic Approach for Spectral-Efficient\n  Cell-Free Massive MIMO",
        "On finitely many base $q$ expansions",
        "Electronic structures of crystalline and amorphous GeSe and GeSbTe\n  compounds using machine learning empirical pseudopotentials",
        "DINAMO: Dynamic and INterpretable Anomaly MOnitoring for Large-Scale\n  Particle Physics Experiments",
        "Parallactic delay for geodetic VLBI and non-orthogonality of the\n  fundamental axes",
        "A knot-theoretic tour of dimension four",
        "Optimization Methods for Joint Eigendecomposition",
        "Homogeneous analytic Hilbert modules -- the case of non-transitive\n  action",
        "Detecting Destabilizing Nonlinearities in Absolute Stability Analysis of\n  Discrete-Time Feedback Systems",
        "Stability of propagating terraces in spatially periodic multistable\n  equations in $\\mathbb{R}^N$",
        "Mean-Field Analysis of Latent Variable Process Models on Dynamically\n  Evolving Graphs with Feedback Effects",
        "Coalescence production of sexaquark with three diquarks in high-energy\n  nuclear collisions",
        "Uniqueness of the strong positive solution for a general quasilinear\n  elliptic problem with variable exponents and homogeneous Neumann boundary\n  conditions using a generalization of the $p(x)$-D\\'{i}az-Saa inequality",
        "Reproducing EPR correlations without superluminal signalling: backward\n  conditional probabilities and Statistical Independence",
        "Linear Bandits with Partially Observable Features",
        "Correlative and in situ microscopy investigation of phase\n  transformation, crystal growth and degradation of antimony sulfide thin films",
        "Four-dimensional QCD equation of state at finite chemical potentials",
        "Defining Determinism",
        "Lunar Laser Ranging with High-Power CW Lasers",
        "Parameter Expanded Stochastic Gradient Markov Chain Monte Carlo",
        "ST-FlowNet: An Efficient Spiking Neural Network for Event-Based Optical\n  Flow Estimation",
        "A nodally bound-preserving finite element method for hyperbolic\n  convection-reaction problems",
        "On the Set of Balanced Games",
        "Modular transport in two-dimensional conformal field theory",
        "Statistical mechanics of a cold tracer in a hot bath",
        "Probabilistic Assessment of West Nile Virus Spillover Risk Using a\n  Compartmental Mechanistic Model"
      ],
      "abstract":[
        "We perform one-dimensional linear analysis and numerical simulations of the\npropagation of Alfven waves in a force-free magnetosphere along magnetic field\nlines around a spinning black hole. We use the results to investigate the\ndynamic process of wave propagation and energy transport for Alfven waves. As\nin a previous study using the Banados--Teitelboim--Zanelli spacetime (Koide et\nal. 2022), the Alfven wave induces a fast magnetosonic wave in the case of a\nspinning black hole. Energy conservation is confirmed when this additional\ninduced magnetosonic wave is considered. We also observe the reflection of the\ninwardly propagating Alfven wave around the static limit, which is prohibited\nin theory when using the eikonal approximation.",
        "Radio frequency cryogenic switches are a critical enabling technology for\nquantum information science for both calibration and high throughput testing of\nsamples. Traditionally, solenoid-based switches have been used [1,2], but a\ntransition is being made to MEMS-based (Micro Electro Mechanical Systems)\nswitches due to their lower power dissipation and smaller size, and to minimize\nthe risk that solenoid switches tend to produce current pulses that destroy\nexpensive cryogenic amplifiers and can cause electrostatic damage to devices.\nThese MEMS switches require a 90-volt signal to be applied to the control lines\nto determine the state of the switches. Switches exist that have built-in\nCMOS-based (Complimentary Metal Oxide Semiconductor) control electronics to\ndrive the 90 V, but these do not work at the cryogenic temperatures used in\nquantum information science.\n  There is no currently available room temperature control system with direct\ncontrol of the switches. The instrument presented here is a 19-inch rack-mount\ncontroller for a cryogenic MEMS switch network that allows a human operator to\nsee the state of the switch via a row of clearly marked indicator lights and to\nchange the state manually via buttons on an LED-based indicator board or\nautomatically via Python-based serial port commands to the Arduino, an open\nsource microcontroller platform available from multiple vendors. The design can\nalso be modified to control other switches that require either a large voltage\nor current to switch.",
        "For a simple graph $G$, let $J_G$ denote the corresponding binomial edge\nideal. This article considers the binomial edge ideal of the corona product of\ntwo connected graphs $G$ and $H$. The corona product of $G$ and $H$, denoted by\n$G\\circ H$, is a construction where each vertex of $G$ is connected (via the\nconing-off) to an entire copy of $H$. This is a direct generalization of a cone\nconstruction. Previous studies have shown that for $J_{G \\circ H}$ to be\nCohen-Macaulay, both $G$ and $H$ must be complete graphs. However, there are no\ngeneral formulae for the dimension, depth, or Castelnuovo-Mumford regularity of\n$J_{G\\circ H}$ for all graphs $G$ and $H$. In this article, we provide a\ngeneral formula for the dimension, depth and Castelnuovo-Mumford regularity of\nthe binomial edge ideals of certain corona and corona-type (somewhat a\ngeneralization of corona) products of special interests. Additionally, we study\nthe Cohen-Macaulayness, unmixedness and related properties of binomial edge\nideals corresponding to above class of graphs. We have also added a short note\non the reduction of the Bolognini-Macchia-Strazzanti Conjecture to all graphs\nwith a diameter of $3$.",
        "Constrained by weak signal strength and significant inter-cell interference,\nusers located at the cell edge in a cellular network suffer from inferior\nservice quality. Recently, cell-free massive MIMO (CFmMIMO) has gained\nconsiderable attention due to its capability to offer uniform quality of\nservice, alleviating the cell-edge problem. In contrast to previous studies\nfocused on narrow-band CFmMIMO systems, this paper studies wideband CFmMIMO\ncommunications against channel frequency selectivity. By exploiting the\nfrequency-domain flexibility offered by orthogonal frequency-division\nmultiplexing (OFDM), and leveraging a particular spatial characteristic in the\ncell-free structure -- namely, the near-far effect among distributed access\npoints (APs) -- we propose an opportunistic approach to boost spectral\nefficiency. The core concept lies in opportunistically activating nearby APs\nfor certain users across their assigned OFDM subcarriers while deactivating\ndistant APs to prevent power wastage and lower inter-user interference.\nFurthermore, this approach enables the use of downlink pilots by reducing the\nnumber of active APs per subcarrier to a small subset, thereby substantially\nimproving downlink performance through coherent detection at the user receiver.\nVerified by numerical results, our proposed approach demonstrates considerable\nperformance improvement compared to the two benchmark approaches.",
        "Given some integer $m \\geq 3$, we find the first explicit collection of\ncountably many intervals in $(1,2)$ such that for any $q$ in one of these\nintervals, the set of points with exactly $m$ base $q$ expansions is nonempty\nand moreover has positive Hausdorff dimension. Our method relies on an\napplication of a theorem proved by Falconer and Yavicoli, which guarantees that\nthe intersection of a family of compact subsets of $\\mathbb{R}^d$ has positive\nHausdorff dimension under certain conditions.",
        "The newly developed machine learning (ML) empirical pseudopotential (EP)\nmethod overcomes the poor transferability of the traditional EP method with the\nhelp of ML techniques while preserving its formal simplicity and computational\nefficiency. We apply the new method to binary and ternary systems such as GeSe\nand Ge-Sb-Te (GST) compounds, well-known materials for non-volatile\nphase-change memory and related technologies. Using a training set of {\\it ab\ninitio} electronic energy bands and rotation-covariant descriptors for various\nGeSe and GST compounds, we generate transferable EPs for Ge, Se, Sb, and Te. We\ndemonstrate that the new ML model accurately reproduces the energy bands and\nwavefunctions of structures outside the training set, closely matching\nfirst-principles calculations. This accuracy is achieved with significantly\nlower computational costs due to the elimination of self-consistency iterations\nand the reduced size of the plane-wave basis set. Notably, the method maintains\naccuracy even for diverse local atomic environments, such as amorphous phases\nor larger systems not explicitly included in the training set.",
        "Ensuring reliable data collection in large-scale particle physics experiments\ndemands Data Quality Monitoring (DQM) procedures to detect possible detector\nmalfunctions and preserve data integrity. Traditionally, this\nresource-intensive task has been handled by human shifters that struggle with\nfrequent changes in operational conditions. We present novel, interpretable,\nrobust, and scalable DQM algorithms designed to automate anomaly detection in\ntime-dependent settings. Our approach constructs evolving histogram templates\nwith built-in uncertainties, featuring both a statistical variant - extending\nthe classical Exponentially Weighted Moving Average (EWMA) - and a machine\nlearning (ML)-enhanced version that leverages a transformer encoder for\nimproved adaptability. Experimental validations on synthetic datasets\ndemonstrate the high accuracy, adaptability, and interpretability of these\nmethods, with the statistical variant being commissioned in the LHCb experiment\nat the Large Hadron Collider, underscoring its real-world impact. The code used\nin this study is available at https:\/\/github.com\/ArseniiGav\/DINAMO.",
        "The Gaia optical astrometric mission has measured the precise positions of\nmillions of objects in the sky, including extragalactic sources also observed\nby Very Long Baseline Interferometry (VLBI). In the recent Gaia EDR3 release,\nan effect of negative parallax with a magnitude of approximately -17 $\\mu$as\nwas reported, presumably due to technical reasons related to the relativistic\ndelay model. A recent analysis of a 30-year set of geodetic VLBI data revealed\na similar negative parallax with an amplitude of $-15.8 \\pm 0.5$ $ \\mu$as.\nSince both astrometric techniques, optical and radio, provide consistent\nestimates of this negative parallax, it is necessary to investigate the\npotential origin of this effect. We developed the extended group relativistic\ndelay model to incorporate the additional parallactic effect for radio sources\nat distances less than 1 Mpc and found that the apparent annual signal might\nappear due the non-orthogonality of the fundamental axes, which are defined by\nthe positions of the reference radio sources themselves. Unlike the\nconventional parallactic ellipse, the apparent annual effect in this case\nappears as a circular motion for all objects independently of their ecliptic\nlatitude. The measured amplitude of this circular effect is within a range of\n10-15 $\\mu$as that is consistent with the ICRF3 stability of the fundamental\naxis. This annual circular effect could also arise if a G\\\"odel-type\ncosmological metric were applied, suggesting that, in the future, this\nphenomenon could be used to indicate global cosmic rotation.",
        "These notes follow a lecture series at the \"Singularities and low dimensional\ntopology\" winter school at the R\\'enyi Institute in January 2023, with a target\naudience of graduate students in singularity theory and low-dimensional\ntopology. The lectures discuss the basics of four-dimensional manifold\ntopology, connecting this rich subject to knot theory on one side and to\ncontact, symplectic, and complex geometry (through Stein surfaces) on the other\nside of the spectrum.",
        "Joint diagonalization, the process of finding a shared set of approximate\neigenvectors for a collection of matrices, arises in diverse applications such\nas multidimensional harmonic analysis or quantum information theory. This task\nis typically framed as an optimization problem: minimizing a non-convex\nfunction that quantifies off-diagonal matrix elements across possible bases. In\nthis work, we introduce a suite of efficient algorithms designed to locate\nlocal minimizers of this functional. Our methods leverage the Hessian's\nstructure to bypass direct computation of second-order derivatives, evaluating\nit as either an operator or bilinear form - a strategy that remains\ncomputationally feasible even for large-scale applications. Additionally, we\ndemonstrate that this Hessian-based information enables precise estimation of\nparameters, such as step-size, in first-order optimization techniques like\nGradient Descent and Conjugate Gradient, and the design of second-order methods\nsuch as (Quasi-)Newton. The resulting algorithms for joint diagonalization\noutperform existing techniques, and we provide comprehensive numerical evidence\nof their superior performance.",
        "This work investigates analytic Hilbert modules $\\mathcal{H}$, over the\npolynomial ring, consisting of holomorphic functions on a $G$-space $\\Omega\n\\subset \\mathbb{C}^m$ that are homogeneous under the natural action of the\ngroup $G$. In a departure from the past studies of such questions, here we\ndon't assume transitivity of the group action. The primary finding reveals that\nunitary invariants such as curvature and the reproducing kernel of a\nhomogeneous analytic Hilbert module can be deduced from their values on a\nfundamental set $\\Lambda$ of the group action. Next, utilizing these\ntechniques, we examine the analytic Hilbert modules associated with the\nsymmetrized bi-disc $\\mathbb{G}_2$ and its homogeneity under the automorphism\ngroup of $\\mathbb{G}_2$. It follows from one of our main theorems that none of\nthe weighted Bergman metrics on the symmetrized bi-disc is K\\\"{a}hler-Einstein.",
        "This paper is concerned with the absolute stability analysis of discrete-time\nfeedback systems with slope-restricted nonlinearities. By employing static\nO'Shea-Zames-Falb multipliers in the framework of integral quadratic\nconstraints, we can obtain a certificate for the absolute stability in the form\nof a linear matrix inequality (LMI). However, since this LMI certificate is\nonly a sufficient condition, we cannot draw any definite conclusion if the LMI\nturns out to be infeasible. To address this issue, we focus on the dual LMI\nthat is feasible if and only if the original (primal) LMI is infeasible. As the\nmain result, if the dual solution satisfies a certain rank condition, we prove\nthat we can detect a destabilizing nonlinearity within the assumed class of\nslope-restricted nonlinearities as well as a non-zero equilibrium point of the\nresulting feedback system, thereby we can conclude that the system of interest\nis never absolutely stable. The effectiveness of the technical results is\ndemonstrated through numerical examples.",
        "In this paper, we study the large time behaviour of solutions of multistable\nreaction-diffusion equations in $\\mathbb{R}^N$, with a spatially periodic\nheterogeneity. By multistable, we mean that the problem admits a finite -- but\narbitrarily large -- number of stable, periodic steady states. In contrast with\nthe more classical monostable and bistable frameworks, which exhibit the\nemergence of a single travelling front in the long run, in the present case the\nlarge time dynamics is governed by a family of stacked travelling fronts,\ninvolving intermediate steady states, called propagating terrace. Their\nexistence in the multidimensional case has been established in our previous\nwork [13]. The first result of the present paper is their uniqueness. Next, we\nshow that the speeds of the propagating terraces in different directions\ndictate the spreading speeds of solutions of the Cauchy problem, for both\nplanar-like and compactly supported initial data. The latter case turns out to\nbe much more intricate than the former, due to the fact that the propagating\nterraces in distinct directions may involve different sets of intermediate\nsteady states. Another source of difficulty is that the Wulff shape of the\nspeeds of travelling fronts can be non-smooth, as we show in the bistable case\nusing a result of [4].",
        "In this paper, we study the asymptotic behavior of a class of dynamic\nco-evolving latent space networks. The model we study is subject to\nbi-directional feedback effects, meaning that at any given time, the latent\nprocess depends on its own value and the graph structure at the previous time\nstep, and the graph structure at the current time depends on the value of the\nlatent processes at the current time but also on the graph structure at the\nprevious time instance (sometimes called a persistence effect). We construct\nthe mean-field limit of this model, which we use to characterize the limiting\nbehavior of a random sample taken from the latent space network in the limit as\nthe number of nodes in the network diverges. From this limiting model, we can\nderive the limiting behavior of the empirical measure of the latent process and\nestablish the related graphon limit of the latent particle network process. We\nalso provide a description of the rich conditional probabilistic structure of\nthe limiting model. The inherent dependence structure complicates the\nmathematical analysis significantly. In the process of proving our main\nresults, we derive a general conditional propagation of chaos result, which is\nof independent interest. In addition, our novel approach of studying the\nlimiting behavior of random samples proves to be a very useful methodology for\nfully grasping the asymptotic behavior of co-evolving particle systems.\nNumerical results are included to illustrate the theoretical findings.",
        "The coalescence production of sexaquark, a hypothetical stable state with\nquark content $(uuddss)$, is investigated by the parton and hadron cascade\nmodel PACIAE in $pp$ collisions at $\\sqrt s = 7$ TeV. In this work, the compact\nsexaquark bound state of three diquarks is formed in the final partonic state\nby a two-step approach, which involves ``diquark\" formation via partonic\ncoalescence and sexaquark construction with dynamically constrained phase-space\ncoalescence model successively. The yields, yield ratios, and dependences of\nspatial parameters (the size of diquark $D_{0}$ and the radius of sexaquark\n$R_{0}$) of (anti-)sexaquark are predicted. The yields of a hadronic molecule\nH-dibaryon $\\mathrm{H}(\\Lambda\\Lambda)$ generated in the final hadronic state\nare also compared. These estimates provide references for future sexaquark\nsearches and other exotic state studies, such as dibaryons.",
        "In this paper, we study a generalization of the D\\'iaz-Saa inequality and its\napplications to nonlinear elliptic problems. We first present the necessary\nhypotheses and preliminary results before introducing an improved version of\nthe inequality, which holds in a broader functional setting and allows\napplications to problems with homogeneous Neumann boundary conditions. The\nsignificance of cases where the inequality becomes an equality is also\nanalyzed, leading to uniqueness results for certain classes of partial\ndifferential equations. Furthermore, we provide a detailed proof of a\nuniqueness theorem for strong positive solutions and illustrate our findings\nwith two concrete applications: a multiple-phase problem and an elliptic\nquasilinear equation relevant to image processing. The paper concludes with\npossible directions for future research.",
        "Bell's theorem states that no model that respects Local Causality and\nStatistical Independence can account for the correlations predicted by quantum\nmechanics via entangled states. This paper proposes a new approach, using\nbackward-in-time conditional probabilities, which relaxes conventional\nassumptions of temporal ordering while preserving Statistical Independence as a\n\"fine-tuning condition. It is shown how such models can account for EPR\/Bell\ncorrelations and, analogously, the GHZ predictions while nevertheless\nforbidding superluminal signalling.",
        "We introduce a novel linear bandit problem with partially observable\nfeatures, resulting in partial reward information and spurious estimates.\nWithout proper address for latent part, regret possibly grows linearly in\ndecision horizon $T$, as their influence on rewards are unknown. To tackle\nthis, we propose a novel analysis to handle the latent features and an\nalgorithm that achieves sublinear regret. The core of our algorithm involves\n(i) augmenting basis vectors orthogonal to the observed feature space, and (ii)\nintroducing an efficient doubly robust estimator. Our approach achieves a\nregret bound of $\\tilde{O}(\\sqrt{(d + d_h)T})$, where $d$ is the dimension of\nobserved features, and $d_h$ is the unknown dimension of the subspace of the\nunobserved features. Notably, our algorithm requires no prior knowledge of the\nunobserved feature space, which may expand as more features become hidden.\nNumerical experiments confirm that our algorithm outperforms both\nnon-contextual multi-armed bandits and linear bandit algorithms depending\nsolely on observed features.",
        "Antimony sulfide (Sb$_2$S$_3$), a compound of earth-abundant elements with\nhighly anisotropic, quasi-layered crystal structure, triggered growing interest\nas a solar absorber in photovoltaics and as a phase change material in memory\ndevices, yet challenges remain in achieving high-quality thin films with\ncontrolled nucleation and growth for optimal performance. Here, we investigate\nthe phase transformation, crystal structure and properties, growth and\ndegradation of atomic layer deposited Sb$_2$S$_3$ thin films using in situ TEM\nand correlative ex situ analysis. The as-deposited amorphous films crystallized\nat 243{\\deg}C, forming grains with an [100] out-of-plane texture and developed\ninto tens to hundreds of micrometer, leaves-shaped grains. Introducing an\nultra-thin ZnS interfacial layer increased nucleation density, and resulted in\na few micrometer-sized, more uniform grains while retaining the overall [100]\ntexture. In situ observations and subsequent crystal orientation analysis with\ncutting-edge 4D-STEM and EBSD revealed that the grains grew faster along the\n[010] ribbon direction and that the bare films underwent early-stage\ndegradation, forming holes in amorphous regions during annealing. The ZnS\ninterlayer mitigated degradation, stabilizing the films and improving their\nuniformity. These findings offer valuable insights for optimizing Sb$_2$S$_3$\nthin films for applications both as solar cell materials and phase change\nmaterials.",
        "Exploration of the QCD phase diagram is pivotal in particle and nuclear\nphysics. We construct a full four-dimensional equation of state of QCD with net\nbaryon, electric charge, and strangeness by extending the NEOS model beyond the\nconventional two-dimensional approximation. Lattice QCD calculations based on\nthe Taylor expansion method and the hadron resonance gas model are considered\nfor the construction. We also develop an efficient numerical method for\napplying the four-dimensional equation of state to relativistic hydrodynamic\nsimulations, which can be used for the analysis of nuclear collisions at beam\nenergy scan energies and for different nuclear species at the BNL Relativistic\nHeavy Ion Collider.",
        "Determinism is the thesis that the past determines the future, but efforts to\ndefine it precisely have exposed deep methodological disagreements. Standard\npossible-worlds formulations of determinism presuppose an \"agreement\" relation\nbetween worlds, but this relation can be understood in multiple ways -- none of\nwhich is particularly clear. We critically examine the proliferation of\ndefinitions of determinism in the recent literature, arguing that these\ndefinitions fail to deliver clear verdicts about actual scientific theories. We\nadvocate a return to a formal approach, in the logical tradition of Carnap,\nthat treats determinism as a property of scientific theories, rather than an\nelusive metaphysical doctrine.\n  We highlight two key distinctions: (1) the difference between qualitative and\n\"full\" determinism, as emphasized in recent discussions of physics and\nmetaphysics, and (2) the distinction between weak and strong formal conditions\non the uniqueness of world extensions. We argue that defining determinism in\nterms of metaphysical notions such as haecceities is unhelpful, whereas\nrigorous formal criteria -- such as Belot's D1 and D3 -- offer a tractable and\nscientifically relevant account. By clarifying what it means for a theory to be\ndeterministic, we set the stage for a fruitful interaction between physics and\nmetaphysics.",
        "We present a high-power continuous-wave (CW) lunar laser ranging (LLR)\ntechnique that has the potential to significantly improve Earth--Moon distance\nmeasurements. Using a 1 kW CW laser at 1064 nm and a 1 m-aperture telescope as\nan example, we develop a detailed link budget and analyze the prevailing noise\nsources to assess system performance when ranging to next-generation ~10 cm\ncorner-cube retroreflectors (CCRs). Unlike legacy arrays, these smaller CCRs\nare designed to yield lower intrinsic range errors, yet their reduced\nreflective area results in lower photon return rates, posing challenges for\npulsed LLR systems. The photon-rich CW approach, by providing continuous\nhigh-power illumination, overcomes this limitation, reducing shot noise and\nenabling sustained millimeter-level ranging with a pathway to sub-0.1 mm\nprecision. Furthermore, by alternating measurements between widely separated\nlunar reflectors, differential LLR mitigates common-mode station errors to\nachieve tens-of-micrometer precision, limited primarily by uncorrelated\natmospheric turbulence. This scalable approach -- integrating high-power CW\nlasers, narrowband filtering, and rapid atmospheric turbulence averaging --\nenables next-generation gravitational tests, precision lunar geodesy, and\nimproved lunar reference frames in support of planetary exploration.",
        "Bayesian Neural Networks (BNNs) provide a promising framework for modeling\npredictive uncertainty and enhancing out-of-distribution robustness (OOD) by\nestimating the posterior distribution of network parameters. Stochastic\nGradient Markov Chain Monte Carlo (SGMCMC) is one of the most powerful methods\nfor scalable posterior sampling in BNNs, achieving efficiency by combining\nstochastic gradient descent with second-order Langevin dynamics. However,\nSGMCMC often suffers from limited sample diversity in practice, which affects\nuncertainty estimation and model performance. We propose a simple yet effective\napproach to enhance sample diversity in SGMCMC without the need for tempering\nor running multiple chains. Our approach reparameterizes the neural network by\ndecomposing each of its weight matrices into a product of matrices, resulting\nin a sampling trajectory that better explores the target parameter space. This\napproach produces a more diverse set of samples, allowing faster mixing within\nthe same computational budget. Notably, our sampler achieves these improvements\nwithout increasing the inference cost compared to the standard SGMCMC.\nExtensive experiments on image classification tasks, including OOD robustness,\ndiversity, loss surface analyses, and a comparative study with Hamiltonian\nMonte Carlo, demonstrate the superiority of the proposed approach.",
        "Spiking Neural Networks (SNNs) have emerged as a promising tool for\nevent-based optical flow estimation tasks due to their ability to leverage\nspatio-temporal information and low-power capabilities. However, the\nperformance of SNN models is often constrained, limiting their application in\nreal-world scenarios. In this work, we address this gap by proposing a novel\nneural network architecture, ST-FlowNet, specifically tailored for optical flow\nestimation from event-based data. The ST-FlowNet architecture integrates\nConvGRU modules to facilitate cross-modal feature augmentation and temporal\nalignment of the predicted optical flow, improving the network's ability to\ncapture complex motion dynamics. Additionally, to overcome the challenges\nassociated with training SNNs, we introduce a novel approach to derive SNN\nmodels from pre-trained artificial neural networks (ANNs) through ANN-to-SNN\nconversion or our proposed BISNN method. Notably, the BISNN method alleviates\nthe complexities involved in biological parameter selection, further enhancing\nthe robustness of SNNs in optical flow estimation tasks. Extensive evaluations\non three benchmark event-based datasets demonstrate that the SNN-based\nST-FlowNet model outperforms state-of-the-art methods, delivering superior\nperformance in accurate optical flow estimation across a diverse range of\ndynamic visual scenes. Furthermore, the inherent energy efficiency of SNN\nmodels is highlighted, establishing a compelling advantage for their practical\ndeployment. Overall, our work presents a novel framework for optical flow\nestimation using SNNs and event-based data, contributing to the advancement of\nneuromorphic vision applications.",
        "In this article, we present a numerical approach to ensure the preservation\nof physical bounds on the solutions to linear and nonlinear hyperbolic\nconvection-reaction problems at the discrete level. We provide a rigorous\nframework for error analysis, formulating the discrete problem as a variational\ninequality and demonstrate optimal convergence rates in a natural norm. We\nsummarise extensive numerical experiments validating the effectiveness of the\nproposed methods in preserving physical bounds and preventing unphysical\noscillations, even in challenging scenarios involving highly nonlinear reaction\nterms.",
        "We study the geometric structure of the set of cooperative transferable\nutility games having a nonempty core, characterized by Bondareva and Shapley as\nbalanced games. We show that this set is a nonpointed polyhedral cone, and we\nfind the set of its extremal rays and facets. This study is also done for the\nset of balanced games whose value for the grand coalition is fixed, which\nyields an affine nonpointed polyhedral cone. Finally, the case of nonnegative\nbalanced games with fixed value for the grand coalition is tackled. This set is\na convex polytope, with remarkable properties. We characterize its vertices and\nfacets, study the adjacency structure of vertices, develop an algorithm for\ngenerating vertices in a random uniform way, and show that this polytope is\ncombinatorial and its adjacency graph is Hamiltonian. Last, we give a\ncharacterization of the set of games having a core reduced to a singleton.\nFunding: This work was supported by the Spanish Government [Grant\nPID2021-124933NB-I00].",
        "We study the quantum transport generated by the bipartite entanglement in\ntwo-dimensional conformal field theory at finite density with the $U(1) \\times\nU(1)$ symmetry associated to the conservation of the electric charge and of the\nhelicity. The bipartition given by an interval is considered, either on the\nline or on the circle. The continuity equations and the corresponding conserved\nquantities for the modular flows of the currents and of the energy-momentum\ntensor are derived. We investigate the mean values of the associated currents\nand their quantum fluctuations in the finite density representation, which\ndescribe the properties of the modular quantum transport. The modular analogues\nof the Johnson-Nyquist law and of the fluctuation-dissipation relation are\nfound, which encode the thermal nature of the modular evolution.",
        "We study the dynamics of a zero-temperature particle interacting linearly\nwith a bath of hot Brownian particles. Starting with the most general model of\na linearly-coupled bath, we eliminate the bath degrees of freedom exactly to\nmap the tracer dynamics onto a generalized Langevin equation, allowing for an\narbitrary external potential on the tracer. We apply this result to determine\nthe fate of a tracer connected by springs to $N$ identical bath particles or\ninserted within a harmonic chain of hot particles. In the former\n\"fully-connected\" case, we find the tracer to transition between an effective\nequilibrium regime at large $N$ and an FDT-violating regime at finite $N$,\nwhile in the latter \"loop\" model the tracer never satisfies an FDT. We then\nstudy the fully-connected model perturbatively for large but finite $N$,\ndemonstrating signatures of irreversibility such as ratchet currents,\nnon-Boltzmann statistics, and positive entropy production. Finally, we\nspecialize to harmonic external potentials on the tracer, allowing us to\nexactly solve the dynamics of both the tracer and the bath for an arbitrary\nlinear model. We apply our findings to show that a cold tracer in a hot lattice\nsuppresses the fluctuations of the lattice in a long-ranged manner, and we\ngeneralize this result to linear elastic field theories.",
        "This paper presents a novel probabilistic approach for assessing the risk of\nWest Nile Disease (WND) spillover to the human population. The assessment has\nbeen conducted under two different scenarios: (1) assessment of the onset of\nspillover, and (2) assessment of the severity of the epidemic after the onset\nof the disease. A compartmental model of differential equations is developed to\ndescribe the disease transmission mechanism, and a probability density function\nfor pathogen spillover to humans is derived based on the model for the\nassessment of the risk of the spillover onset and the severity of the epidemic.\nThe prediction strategy involves making a long-term forecast and then updating\nit with a short-term (lead time of two weeks or daily). The methodology is\ndemonstrated using detailed outbreak data from high-case counties in\nCalifornia, including Orange County, Los Angeles County, and Kern County. The\npredicted results are compared with actual infection dates reported by the\nCalifornia Department of Public Health for 2022-2024 to assess prediction\naccuracy. The performance accuracy is evaluated using a logarithmic scoring\nsystem and compared with one of the most renowned predictive models to assess\nits effectiveness. In all prediction scenarios, the model demonstrated strong\nperformance. Lastly, the method is applied to explore the impact of global\nwarming on spillover risk, revealing an increasing trend in the number of\nhigh-risk days and a shift toward a greater proportion of these days over time\nfor the onset of the disease."
      ]
    }
  },
  {
    "id":2411.18253,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Artificial intelligence for predictive biomarker discovery in immuno-oncology: a systematic review",
    "start_abstract":"Background: The widespread use of immune checkpoint inhibitors (ICIs) has revolutionised treatment of multiple cancer types. However, selecting patients who may benefit from ICI remains challenging. Artificial intelligence (AI) approaches allow exploitation of high-dimension oncological data in research and development of precision immuno-oncology. Materials and methods: We conducted a systematic literature review of peer-reviewed original articles studying the ICI efficacy prediction in cancer patients across five data modalities: genomics (including genomics, transcriptomics, and epigenomics), radiomics, digital pathology (pathomics), and real-world and multimodality data. Results: A total of 90 studies were included in this systematic review, with 80% published in 2021-2022. Among them, 37 studies included genomic, 20 radiomic, 8 pathomic, 20 real-world, and 5 multimodal data. Standard machine learning (ML) methods were used in 72% of studies, deep learning (DL) methods in 22%, and both in 6%. The most frequently studied cancer type was non-small-cell lung cancer (36%), followed by melanoma (16%), while 25% included pan-cancer studies. No prospective study design incorporated AI-based methodologies from the outset; rather, all implemented AI as a post hoc analysis. Novel biomarkers for ICI in radiomics and pathomics were identified using AI approaches, and molecular biomarkers have expanded past genomics into transcriptomics and epigenomics. Finally, complex algorithms and new types of AI-based markers, such as meta-biomarkers, are emerging by integrating multimodal\/multi-omics data. Conclusion: AI-based methods have expanded the horizon for biomarker discovery, demonstrating the power of integrating multimodal data from existing datasets to discover new meta-biomarkers. While most of the included studies showed promise for AI-based prediction of benefit from immunotherapy, none provided high-level evidence for immediate practice change. A priori planned prospective trial designs are needed to cover all lifecycle steps of these software biomarkers, from development and validation to integration into clinical practice.",
    "start_categories":[
      "Immunotherapy"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Multimodal data fusion for cancer biomarker discovery with deep learning"
      ],
      "abstract":[
        "Technological advances have made it possible to study a patient from multiple angles with high-dimensional, high-throughput multiscale biomedical data. In oncology, massive amounts of data are being generated, ranging from molecular, histopathology, radiology to clinical records. The introduction of deep learning has greatly advanced the analysis of biomedical data. However, most approaches focus on single data modalities, leading to slow progress in methods to integrate complementary data types. Development of effective multimodal fusion approaches is becoming increasingly important as a single modality might not be consistent and sufficient to capture the heterogeneity of complex diseases to tailor medical care and improve personalized medicine. Many initiatives now focus on integrating these disparate modalities to unravel the biological processes involved in multifactorial diseases such as cancer. However, many obstacles remain, including lack of usable data as well as methods for clinical validation and interpretation. Here, we cover these current challenges and reflect on opportunities through deep learning to tackle data sparsity and scarcity, multimodal interpretability and standardization of datasets."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "A New Era of Elections: Leveraging Blockchain for Fair and Transparent\n  Voting",
        "Twists of representations of complex reflection groups and rational\n  Cherednik algebras",
        "Generating Physically Realistic and Directable Human Motions from\n  Multi-Modal Inputs",
        "The GigaMIDI Dataset with Features for Expressive Music Performance\n  Detection",
        "On alternating-conjugate splitting methods",
        "Cluster algebras and skein algebras for surfaces",
        "Spring-mass behavior of solitons under the influence of an external\n  force field within the modified Korteweg-de Vries equation",
        "PanoGen++: Domain-Adapted Text-Guided Panoramic Environment Generation\n  for Vision-and-Language Navigation",
        "Subcodes of Second-Order Reed-Muller Codes via Recursive Subproducts",
        "Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive\n  Fine-tuning",
        "Robust Multi-Objective Preference Alignment with Online DPO",
        "The Striking Impact of Natural Hazard Risk on Global Green Hydrogen Cost",
        "Tunable magnon band topology and magnon orbital Nernst effect in\n  noncollinear antiferromagnets",
        "Automation of Electroweak Corrections",
        "Comparing strange and non-strange quark stars within resummed QCD at NLO",
        "Movable Antenna Aided Multiuser Communications: Antenna Position\n  Optimization Based on Statistical Channel Information",
        "ShapeShift: Towards Text-to-Shape Arrangement Synthesis with\n  Content-Aware Geometric Constraints",
        "A machine-learning study of phase transitions in Ising, Blume-Capel, and\n  Ising-metamagnet models",
        "GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields\n  through Efficient Dense 3D Point Tracking",
        "Study of SARS-CoV-2 Spike Protein by Surface Enhanced Raman Spectroscopy\n  and Transmission Electron Microscopy",
        "A Review of Brain-Computer Interface Technologies: Signal Acquisition\n  Methods and Interaction Paradigms",
        "Brain Effective Connectivity Estimation via Fourier Spatiotemporal\n  Attention",
        "MechIR: A Mechanistic Interpretability Framework for Information\n  Retrieval",
        "Irrationality of the Length Spectrum",
        "On the eigenvalues and Fu\\v{c}\\'{\\i}k spectrum of $p$-Laplace local and\n  nonlocal operator with mixed interpolated Hardy term",
        "Correlated flat-band physics in a bilayer kagome metal based on compact\n  molecular orbitals",
        "Continuous Approach to Phase (Norm) Retrieval Frames",
        "Gradient estimates for the fractional $p$-Poisson equation",
        "Pump-intensity-scaling of Two-Photon-Absorption and Photon Statistics of\n  Entangled-Photon Fields"
      ],
      "abstract":[
        "This study presents a blockchain-based voting system aimed at enhancing\nelection security, transparency, and integrity. Traditional voting methods face\ngrowing risks of tampering, making it crucial to explore innovative solutions.\nOur proposed system combines blockchain's immutable, decentralized ledger with\nadvanced voter identity verification techniques, including digital identity\nvalidation through Aadhaar and Driving Licenses (secured via BLAKE2b-512\nhashing), biometric fingerprint authentication, and a picture rotation pattern\nfor added security. Votes are recorded transparently and securely on a\nblockchain, with a consensus mechanism ensuring data integrity and reducing the\nrisk of unauthorized alterations. Security analysis indicates that this\nmulti-layered approach significantly reduces impersonation risks, while\nblockchain ensures accurate, private, and tamper-resistant vote recording. The\nfindings support that a blockchain-based voting system with robust identity\nchecks offers a trustworthy alternative to traditional methods, with potential\nfor even greater refinement in secure and transparent elections.",
        "Drinfeld twists, and the twists of Giaquinto and Zhang, allow for algebras\nand their modules to be deformed by a cocycle. We prove general results about\ncocycle twists of algebra factorisations and induced representations and apply\nthem to reflection groups and rational Cherednik algebras. In particular, we\ndescribe how a twist acts on characters of Coxeter groups of type $B_n$ and\n$D_n$ and relate them to characters of mystic reflection groups. This is used\nto characterise twists of standard modules of rational Cherednik algebras as\nstandard modules for certain braided Cherednik algebras. We introduce the\ncoinvariant algebra of a mystic reflection group and use a twist to show that\nan analogue of Chevalley's theorem holds for these noncommutative algebras. We\nalso discuss several cases where the negative braided Cherednik algebras are,\nand are not, isomorphic to rational Cherednik algebras.",
        "This work focuses on generating realistic, physically-based human behaviors\nfrom multi-modal inputs, which may only partially specify the desired motion.\nFor example, the input may come from a VR controller providing arm motion and\nbody velocity, partial key-point animation, computer vision applied to videos,\nor even higher-level motion goals. This requires a versatile low-level humanoid\ncontroller that can handle such sparse, under-specified guidance, seamlessly\nswitch between skills, and recover from failures. Current approaches for\nlearning humanoid controllers from demonstration data capture some of these\ncharacteristics, but none achieve them all. To this end, we introduce the\nMasked Humanoid Controller (MHC), a novel approach that applies multi-objective\nimitation learning on augmented and selectively masked motion demonstrations.\nThe training methodology results in an MHC that exhibits the key capabilities\nof catch-up to out-of-sync input commands, combining elements from multiple\nmotion sequences, and completing unspecified parts of motions from sparse\nmultimodal input. We demonstrate these key capabilities for an MHC learned over\na dataset of 87 diverse skills and showcase different multi-modal use cases,\nincluding integration with planning frameworks to highlight MHC's ability to\nsolve new user-defined tasks without any finetuning.",
        "The Musical Instrument Digital Interface (MIDI), introduced in 1983,\nrevolutionized music production by allowing computers and instruments to\ncommunicate efficiently. MIDI files encode musical instructions compactly,\nfacilitating convenient music sharing. They benefit Music Information Retrieval\n(MIR), aiding in research on music understanding, computational musicology, and\ngenerative music. The GigaMIDI dataset contains over 1.4 million unique MIDI\nfiles, encompassing 1.8 billion MIDI note events and over 5.3 million MIDI\ntracks. GigaMIDI is currently the largest collection of symbolic music in MIDI\nformat available for research purposes under fair dealing. Distinguishing\nbetween non-expressive and expressive MIDI tracks is challenging, as MIDI files\ndo not inherently make this distinction. To address this issue, we introduce a\nset of innovative heuristics for detecting expressive music performance. These\ninclude the Distinctive Note Velocity Ratio (DNVR) heuristic, which analyzes\nMIDI note velocity; the Distinctive Note Onset Deviation Ratio (DNODR)\nheuristic, which examines deviations in note onset times; and the Note Onset\nMedian Metric Level (NOMML) heuristic, which evaluates onset positions relative\nto metric levels. Our evaluation demonstrates these heuristics effectively\ndifferentiate between non-expressive and expressive MIDI tracks. Furthermore,\nafter evaluation, we create the most substantial expressive MIDI dataset,\nemploying our heuristic, NOMML. This curated iteration of GigaMIDI encompasses\nexpressively-performed instrument tracks detected by NOMML, containing all\nGeneral MIDI instruments, constituting 31% of the GigaMIDI dataset, totalling\n1,655,649 tracks.",
        "The new class of alternating-conjugate splitting methods is presented and\nanalyzed. They are obtained by concatenating a given composition involving\ncomplex coefficients with the same composition but with the complex conjugate\ncoefficients. We show that schemes of this type exhibit a good long time\nbehavior when applied to linear unitary and linear Hamiltonian systems, in\ncontrast with other methods based on complex coefficients, and study in detail\ntheir preservation properties. We also present new schemes within this class up\nto order 6 that exhibit a better efficiency than state-of-the-art splitting\nmethods with real coefficients for some classes of problems.",
        "We consider two algebras of curves associated to an oriented surface of\nfinite type - the cluster algebra from combinatorial algebra, and the skein\nalgebra from quantum topology. We focus on generalizations of cluster algebras\nand generalizations of skein algebras that include arcs whose endpoints are\nmarked points on the boundary or in the interior of the surface. We show that\nthe generalizations are closely related by maps that can be explicitly defined,\nand we explore the structural implications, including (non-)finite generation.\nWe also discuss open questions about the algebraic structure of the algebras.",
        "We investigate the interaction of solitons with an external periodic field\nwithin the framework of the modified Korteweg-de Vries (mKdV) equation. In the\ncase of small perturbation a simple dynamical system is used to describe the\nsoliton behaviour. Equilibrium points of this dynamical system are computed\nwhen the external force travels at a constant speed. Assuming that the external\nforce moves with sinusoidal speed, we demonstrate that the soliton behavior is\nqualitatively similar to the constant-speed case. Besides, a resonant frequency\nis derived from the asymptotic theory without using the classical broad force\napproximation. The results obtained from the dynamical system are compared with\nfully direct numerical simulations, which reveal that the soliton solution\nexhibits spiral-like behavior in the soliton amplitude versus soliton phase\nspace. Moreover, when the external force oscillates at the resonant frequency,\nthe trajectories in the soliton phase versus soliton amplitude exhibit chaotic\nbehavior.",
        "Vision-and-language navigation (VLN) tasks require agents to navigate\nthree-dimensional environments guided by natural language instructions,\noffering substantial potential for diverse applications. However, the scarcity\nof training data impedes progress in this field. This paper introduces\nPanoGen++, a novel framework that addresses this limitation by generating\nvaried and pertinent panoramic environments for VLN tasks. PanoGen++\nincorporates pre-trained diffusion models with domain-specific fine-tuning,\nemploying parameter-efficient techniques such as low-rank adaptation to\nminimize computational costs. We investigate two settings for environment\ngeneration: masked image inpainting and recursive image outpainting. The former\nmaximizes novel environment creation by inpainting masked regions based on\ntextual descriptions, while the latter facilitates agents' learning of spatial\nrelationships within panoramas. Empirical evaluations on room-to-room (R2R),\nroom-for-room (R4R), and cooperative vision-and-dialog navigation (CVDN)\ndatasets reveal significant performance enhancements: a 2.44% increase in\nsuccess rate on the R2R test leaderboard, a 0.63% improvement on the R4R\nvalidation unseen set, and a 0.75-meter enhancement in goal progress on the\nCVDN validation unseen set. PanoGen++ augments the diversity and relevance of\ntraining environments, resulting in improved generalization and efficacy in VLN\ntasks.",
        "We use a simple construction called `recursive subproducts' (that is known to\nyield good codes of lengths $n^m$, $n \\geq 3$) to identify a family of codes\nsandwiched between first-order and second-order Reed-Muller (RM) codes. These\ncodes are subcodes of multidimensional product codes that use first-order RM\ncodes as components. We identify the minimum weight codewords of all the codes\nin this family, and numerically determine the weight distribution of some of\nthem. While these codes have the same minimum distance and a smaller rate than\nsecond-order RM codes, they have significantly fewer minimum weight codewords.\nFurther, these codes can be decoded via modifications to known RM decoders\nwhich yield codeword error rates within 0.25 dB of second-order RM codes and\nbetter than CRC-aided Polar codes (in terms of $E_b\/N_o$ for lengths $256, 512,\n1024$), thereby offering rate adaptation options for RM codes in low-capacity\nscenarios.",
        "Recent advancements in large language models (LLMs) based on transformer\narchitectures have sparked significant interest in understanding their inner\nworkings. In this paper, we introduce a novel approach to modeling transformer\narchitectures using highly flexible non-autonomous neural ordinary differential\nequations (ODEs). Our proposed model parameterizes all weights of attention and\nfeed-forward blocks through neural networks, expressing these weights as\nfunctions of a continuous layer index. Through spectral analysis of the model's\ndynamics, we uncover an increase in eigenvalue magnitude that challenges the\nweight-sharing assumption prevalent in existing theoretical studies. We also\nleverage the Lyapunov exponent to examine token-level sensitivity, enhancing\nmodel interpretability. Our neural ODE transformer demonstrates performance\ncomparable to or better than vanilla transformers across various configurations\nand datasets, while offering flexible fine-tuning capabilities that can adapt\nto different architectural constraints.",
        "Multi-objective preference alignment of large language models (LLMs) is\ncritical for developing AI systems that are more configurable, personalizable,\nhelpful, and safe. However, optimizing model outputs to satisfy diverse\nobjectives with variable weights at inference time for truly personalized\nmodels presents a significant challenge. Existing approaches are either\ncomputationally expensive to train or do not sufficiently steer model\nbehaviors. This paper introduces the Multi-Objective Online DPO (MO-ODPO)\nalgorithm, designed to robustly and efficiently align model behaviors with\nmultiple, potentially conflicting human preferences. Our approach incorporates\na prompt conditioning mechanism, allowing us to train a single\npreference-conditional policy, that can adapt to new preference combinations at\ninference. Experiments on two popular benchmarks show that MO-ODPO\nPareto-dominates existing baselines while providing excellent inference-time\nsteerability between diverse objectives.",
        "Due to climate change, natural hazards that affect energy infrastructure will\nbecome more frequent in the future. However, to incorporate natural hazard risk\ninto infrastructure investment decisions, we develop an approach to translate\nthis risk into discount rates. Thus, our newly developed discount rate approach\nincorporates both economic risk and natural hazard risk. To illustrate the\nimpact of including the risk of natural hazards, we apply country-specific\ndiscount rates for hydrogen production costs. The country-specific relative\ndifference in hydrogen generation cost ranges from a 96% surplus in the\nPhilippines to a -63% cost reduction in Kyrgyzstan compared to a discount rate\nthat only consists of economic risks. The inclusion of natural hazard risk\nchanges the cost ranking of technologies as outcome of energy system models and\nthus policy recommendations. The derived discount rates for 254 countries\nworldwide are published in this publication for further use.",
        "We theoretically investigate the intrinsic magnon orbital Nernst effect (ONE)\nin noncollinear antiferromagnets with Kagom\\'e spin systems. Our analysis\nreveals that an externally applied magnetic field induces topological phase\ntransitions in the magnonic system, characterized by the closing and reopening\nof the band gap between distinct magnon bands. These transitions enable tunable\ncontrol of the magnon orbital Nernst effect with applied magnetic field, with a\npronounced enhancement in magnon orbital Nernst conductivity near the phase\ntransition points. This tunability presents a promising direction for\nexperimental detection of the magnon ONE.",
        "This dissertation addresses a topic that I have worked on over the past\ndecade: the automation of next-to-leading order electroweak corrections in the\nStandard Model of particle physics. After introducing the basic concepts and\ntechniques of next-to-leading order QCD calculations that underpin the\nMadGraph5_aMC@NLO framework, I present a few key features relevant to the\nautomated next-to-leading order electroweak contributions to short-distance\ncross sections, with an emphasis on the mixed QCD and electroweak coupling\nexpansions. These include the FKS subtraction, the renormalization and\nelectroweak input parameter schemes, and the complex mass scheme for dealing\nwith unstable particles. Issues related to the initial or final photons and\nleptons are also discussed. Two remaining challenges are highlighted if one\nwishes to go beyond next-to-leading order computations. Some phenomenological\napplications at the LHC are given to demonstrate the relevance of electroweak\ncorrections at colliders. Finally, an outlook on future studies concludes the\ndissertation.",
        "We employ the renormalization group optimized perturbation theory (RGOPT)\nresummation method to evaluate the equation of state (EoS) for strange\n($N_f=2+1$) and non-strange ($N_f=2$) cold quark matter at NLO. This allows us\nto obtain the mass-radius relation for pure quark stars and compare the results\nwith the predictions from perturbative QCD (pQCD) at NNLO. Choosing the\nrenormalization scale to generate maximum star masses of order $M=2 - 2.6\nM_\\odot$, we show that the RGOPT can produce mass-radius curves compatible with\nthe masses and radii of some recently observed pulsars, regardless of their\nstrangeness content. The scale values required to produce the desired maximum\nmasses are higher in the strange scenario since the EoS is softer in this case.\nThe possible reasons for such behavior are discussed. Our results also show\nthat, as expected, the RGOPT predictions for the relevant observables are less\nsensitive to scale variations than those furnished by pQCD.",
        "The movable antenna (MA) technology has attracted great attention recently\ndue to its promising capability in improving wireless channel conditions by\nflexibly adjusting antenna positions. To reap maximal performance gains of MA\nsystems, existing works mainly focus on MA position optimization to cater to\nthe instantaneous channel state information (CSI). However, the resulting\nreal-time antenna movement may face challenges in practical implementation due\nto the additional time overhead and energy consumption required, especially in\nfast time-varying channel scenarios. To address this issue, we propose in this\npaper a new approach to optimize the MA positions based on the users'\nstatistical CSI over a large timescale. In particular, we propose a general\nfield response based statistical channel model to characterize the random\nchannel variations caused by the local movement of users. Based on this model,\na two-timescale optimization problem is formulated to maximize the ergodic sum\nrate of multiple users, where the precoding matrix and the positions of MAs at\nthe base station (BS) are optimized based on the instantaneous and statistical\nCSI, respectively. To solve this non-convex optimization problem, a log-barrier\npenalized gradient ascent algorithm is developed to optimize the MA positions,\nwhere two methods are proposed to approximate the ergodic sum rate and its\ngradients with different complexities. Finally, we present simulation results\nto evaluate the performance of the proposed design and algorithms based on\npractical channels generated by ray-tracing. The results verify the performance\nadvantages of MA systems compared to their fixed-position antenna (FPA)\ncounterparts in terms of long-term rate improvement, especially for scenarios\nwith more diverse channel power distributions in the angular domain.",
        "While diffusion-based models excel at generating photorealistic images from\ntext, a more nuanced challenge emerges when constrained to using only a fixed\nset of rigid shapes, akin to solving tangram puzzles or arranging real-world\nobjects to match semantic descriptions. We formalize this problem as\nshape-based image generation, a new text-guided image-to-image translation task\nthat requires rearranging the input set of rigid shapes into non-overlapping\nconfigurations and visually communicating the target concept. Unlike\npixel-manipulation approaches, our method, ShapeShift, explicitly parameterizes\neach shape within a differentiable vector graphics pipeline, iteratively\noptimizing placement and orientation through score distillation sampling from\npretrained diffusion models. To preserve arrangement clarity, we introduce a\ncontent-aware collision resolution mechanism that applies minimal semantically\ncoherent adjustments when overlaps occur, ensuring smooth convergence toward\nphysically valid configurations. By bridging diffusion-based semantic guidance\nwith explicit geometric constraints, our approach yields interpretable\ncompositions where spatial relationships clearly embody the textual prompt.\nExtensive experiments demonstrate compelling results across diverse scenarios,\nwith quantitative and qualitative advantages over alternative techniques.",
        "We combine machine-learning (ML) techniques with Monte Carlo (MC) simulations\nand finite-size scaling (FSS) to study continuous and first-order phase\ntransitions in Ising, Blume-Capel, and Ising-metamagnet spin models. We go\nbeyond earlier studies that had concentrated on obtaining the\ncorrelation-length exponent $\\nu$. In particular, we show (a) how to combine\nneural networks (NNs), trained with data from MC simulations of Ising-type spin\nmodels on finite lattices, with FSS to obtain both thermal magnetic exponents\n$y_t = 1\/\\nu$ and $y_h$, respectively, at both critical and tricritical points,\n(b) how to obtain the NN counterpart of two-scale-factor universality at an\nIsing-type critical point, and (c) FSS at a first-order transition. We also\nobtain the FSS forms for the output of our trained NNs as functions of both the\ntemperature and the magnetic field.",
        "4D video control is essential in video generation as it enables the use of\nsophisticated lens techniques, such as multi-camera shooting and dolly zoom,\nwhich are currently unsupported by existing methods. Training a video Diffusion\nTransformer (DiT) directly to control 4D content requires expensive multi-view\nvideos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that\noptimizes a 4D representation and renders videos according to different 4D\nelements, such as camera pose and object motion editing, we bring pseudo 4D\nGaussian fields to video generation. Specifically, we propose a novel framework\nthat constructs a pseudo 4D Gaussian field with dense 3D point tracking and\nrenders the Gaussian field for all video frames. Then we finetune a pretrained\nDiT to generate videos following the guidance of the rendered video, dubbed as\nGS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense\n3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field\nconstruction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art\nsparse 3D point tracking method, in accuracy and accelerates the inference\nspeed by two orders of magnitude. During the inference stage, GS-DiT can\ngenerate videos with the same dynamic content while adhering to different\ncamera parameters, addressing a significant limitation of current video\ngeneration models. GS-DiT demonstrates strong generalization capabilities and\nextends the 4D controllability of Gaussian splatting to video generation beyond\njust camera poses. It supports advanced cinematic effects through the\nmanipulation of the Gaussian field and camera intrinsics, making it a powerful\ntool for creative video production. Demos are available at\nhttps:\/\/wkbian.github.io\/Projects\/GS-DiT\/.",
        "The spike protein (SP) of SARS-CoV-2 is the major molecular target for making\ndiagnostic tests, vaccines, and therapeutic development. We used a combination\nof transmission electron microscopy (TEM) and surface enhanced Raman microscopy\n(SERS) to study its structure. Using SERS on an aluminum substrate, we were\nable to detect a characteristic spectrum of SP mostly due to vibration of three\naromatic amino acids producing Raman shifts at 466 cm-1, 524 cm-1, 773 cm-1,\n831 cm-1, 1048 cm-1, 1308 cm-1, 1457 cm-1, and 1610 cm-1. Transmission Electron\nMicroscopy (TEM) of the SP showed periodic 2D-lattice orientation. The findings\nfrom this study have translational values for developing surface-enhanced Raman\nspectroscopy (SERS) based detectors for screening and testing SARS-CoV-2\nsignatures in diagnostic settings and contamination tracking.",
        "Brain-Computer Interface (BCI) technology facilitates direct communication\nbetween the human brain and external devices, representing a substantial\nadvancement in human-machine interaction. This review provides an in-depth\nanalysis of various BCI paradigms, including classic paradigms, current\nclassifications, and hybrid paradigms, each with distinct characteristics and\napplications. Additionally, we explore a range of signal acquisition methods,\nclassified into non-implantation, intervention, and implantation techniques,\nelaborating on their principles and recent advancements. By examining the\ninterdependence between paradigms and signal acquisition technologies, this\nreview offers a comprehensive perspective on how innovations in one domain\npropel progress in the other. The goal is to present insights into the future\ndevelopment of more efficient, user-friendly, and versatile BCI systems,\nemphasizing the synergy between paradigm design and signal acquisition\ntechniques and their potential to transform the field.",
        "Estimating brain effective connectivity (EC) from functional magnetic\nresonance imaging (fMRI) data can aid in comprehending the neural mechanisms\nunderlying human behavior and cognition, providing a foundation for disease\ndiagnosis. However, current spatiotemporal attention modules handle temporal\nand spatial attention separately, extracting temporal and spatial features\neither sequentially or in parallel. These approaches overlook the inherent\nspatiotemporal correlations present in real world fMRI data. Additionally, the\npresence of noise in fMRI data further limits the performance of existing\nmethods. In this paper, we propose a novel brain effective connectivity\nestimation method based on Fourier spatiotemporal attention (FSTA-EC), which\ncombines Fourier attention and spatiotemporal attention to simultaneously\ncapture inter-series (spatial) dynamics and intra-series (temporal)\ndependencies from high-noise fMRI data. Specifically, Fourier attention is\ndesigned to convert the high-noise fMRI data to frequency domain, and map the\ndenoised fMRI data back to physical domain, and spatiotemporal attention is\ncrafted to simultaneously learn spatiotemporal dynamics. Furthermore, through a\nseries of proofs, we demonstrate that incorporating learnable filter into fast\nFourier transform and inverse fast Fourier transform processes is\nmathematically equivalent to performing cyclic convolution. The experimental\nresults on simulated and real-resting-state fMRI datasets demonstrate that the\nproposed method exhibits superior performance when compared to state-of-the-art\nmethods.",
        "Mechanistic interpretability is an emerging diagnostic approach for neural\nmodels that has gained traction in broader natural language processing domains.\nThis paradigm aims to provide attribution to components of neural systems where\ncausal relationships between hidden layers and output were previously\nuninterpretable. As the use of neural models in IR for retrieval and evaluation\nbecomes ubiquitous, we need to ensure that we can interpret why a model\nproduces a given output for both transparency and the betterment of systems.\nThis work comprises a flexible framework for diagnostic analysis and\nintervention within these highly parametric neural systems specifically\ntailored for IR tasks and architectures. In providing such a framework, we look\nto facilitate further research in interpretable IR with a broader scope for\npractical interventions derived from mechanistic interpretability. We provide\npreliminary analysis and look to demonstrate our framework through an axiomatic\nlens to show its applications and ease of use for those IR practitioners\ninexperienced in this emerging paradigm.",
        "It is a classical result of Dal'Bo that the length spectrum of a\nnon-elementary Fuchsian group is non-arithmetic, namely, it generates a dense\nadditive subgroup of $\\mathbb{R}$. In this note we observe the following\nextension of this fact: a non-elementary Fuchsian group contains two elements\nwhose lengths are linearly independent over $\\mathbb{Q}$.",
        "In this article, we are concerned with the eigenvalue problem driven by the\nmixed local and nonlocal $p$-Laplacian operator having the interpolated Hardy\nterm \\begin{equation*} \\mathcal{T}(u) :=- \\Delta_p u + (- \\Delta_p)^s u - \\mu\n\\frac{|u|^{p-2}u}{|x|^{p \\theta}}, \\end{equation*} where $0<s<1<p<N$, $\\theta\n\\in [s,1]$, and $\\mu \\in (0,\\mu_0(\\theta))$. First, we establish a mixed\ninterpolated Hardy inequality and then show the existence of eigenvalues and\ntheir properties. We also investigate the Fu\\v{c}\\'{\\i}k spectrum, the\nexistence of the first nontrivial curve in the Fu\\v{c}\\'{\\i}k spectrum, and\nprove some of its properties. Moreover, we study the shape optimization of the\ndomain with respect to the first two eigenvalues, the regularity of the\neigenfunctions, the Faber-Krahn inequality, and a variational characterization\nof the second eigenvalue.",
        "Flat bands, when located close to the Fermi energy, can considerably enhance\nthe influence of electron correlations on the low energy physics in kagome and\nother frustrated-lattice metals. A major challenge in describing the\ninteraction effects in such bulk materials is that the flat band is often\nintermixed with a large number of other bands. Here we show that the recently\nintroduced notion of compact molecular orbitals (CMOs) enable a path forward in\ndescribing the dominant effect of the Coulomb interactions in spite of the\ncomplexity of the bandstructure. Our materials-based analysis allows for the\nunderstanding of the scanning-tunneling-microscopy experiment [J. C. Souza et\nal., preprint (2024)] of the bilayer kagome metal Ni$_3$In in terms of the CMO\nnotion. From the resulting CMO, an effective Anderson lattice model can be set\nup. This CMO-based approach enables the calculation of correlation effects that\nis difficult to do based on the atomic orbitals. Furthermore, it suggests an\nenriched phase diagram for the strange metal physics of the kagome metal, which\ncan be tested by future experiments. We discuss the implications of our results\nfor the general correlation physics of flat band systems and beyond.",
        "This paper investigates the properties of continuous frames, with a\nparticular focus on phase retrieval and norm retrieval in the context of\nHilbert spaces. We introduce the concept of continuous near-Riesz bases and\nprove their invariance under invertible operators. Some equivalent conditions\nfor phase and norm retrieval property of continuous frames are presented. We\nstudy the stability of phase retrieval under perturbations. Furthermore, tensor\nproduct frames for separable Hilbert spaces are studied, and we establish the\nequivalence of phase retrieval and norm retrieval properties between components\nand their tensor products.",
        "We consider local weak solutions to the fractional $p$-Poisson equation of\norder $s$, i.e. $\\left( - \\Delta_p\\right)^s u = f$. In the range $p>1$ and\n$s\\in \\big(\\frac{p-1}{p},1\\big)$ we prove Calder\\'on & Zygmund type estimates\nat the gradient level. More precisely, we show for any $q>1$ that\n\\begin{equation*}\n  f\\in L^{\\frac{qp}{p-1}}_{\\rm loc}\n  \\quad\\Longrightarrow\\quad\n  \\nabla u\\in L^{qp}_{\\rm loc}. \\end{equation*} The qualitative result is\naccompanied by a local quantitative estimate.",
        "We use a non-perturbative theoretical approach to the parametric\ndown-conversion (PDC) process, which generates entangled-photon field for an\narbitrarily strong pump-pulse. This approach can be used to evaluate\nmulti-point field correlation functions to compute nonlinear spectroscopic\nsignals induced by a strong pump. The entangled-photon statistics is studied\nusing Glauber's $g^{(2)}$ function, which helps understand the significance of\nthe photon entanglement-time and the pump-pulse intensity on spectroscopic\nsignals. Under the non-perturbative treatment of the entangled field, the\ntwo-photon absorption (TPA) signal shows linear to strongly non-linear growth\nwith the pump intensity, rather than linear to quadratic scaling reported\npreviously. An increase in the range of pump intensity for the linear scaling\nis observed as the pump band-width is increased. We propose an experimental\nscheme that can select contributions to the TPA signal that arise solely from\ninteractions with the entangled photons, and filter out unentangled photon\ncontributions, which are dominant at higher pump intensities, paving a way to\nexplore the entanglement effects at higher intensities."
      ]
    }
  },
  {
    "id":2411.17617,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"ALL-Net: Anatomical information lesion-wise loss function integrated into neural network for multiple sclerosis lesion segmentation",
    "start_abstract":"Accurate detection and segmentation of multiple sclerosis (MS) brain lesions on magnetic resonance images are important for disease diagnosis treatment. This is a challenging task as vary greatly in size, shape, location, image contrast. The objective our study was to develop an algorithm based deep convolutional neural network integrated with anatomic information lesion-wise loss function (ALL-Net) fast accurate automated MS lesions. Distance transformation mapping used construct module that encoded lesion-specific anatomical information. To overcome the lesion size imbalance during training improve small lesions, developed which individual were modeled spheres equal size. On ISBI-2015 longitudinal challenge dataset (19 subjects total), ALL-Net achieved overall score 93.32 amongst top performing methods. larger Cornell (176 significantly improved both voxel-wise metrics (Dice improvement 3.9% 35.3% p-values ranging from p < 0.01 0.0001, AUC precision-recall curve 2.1% 29.8%) (lesion-wise F1 12.6% 29.8% all ROC 1.4% 20.0%) compared leading publicly available tools.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "Deep learning-Based 3D inpainting of brain MR images"
      ],
      "abstract":[
        "Abstract The detailed anatomical information of the brain provided by 3D magnetic resonance imaging (MRI) enables various neuroscience research. However, due to long scan time for MR images, 2D images are mainly obtained in clinical environments. purpose this study is generate from a sparsely sampled using an inpainting deep neural network that has U-net-like structure and DenseNet sub-blocks. To train network, not only fidelity loss but also perceptual based on VGG were considered. Various methods used assess overall similarity between inpainted original data. In addition, morphological analyzes performed investigate whether data produced local features similar diagnostic ability was evaluated investigating pattern changes disease groups. Brain anatomy details efficiently recovered proposed network. voxel-based analysis gray matter volume cortical thickness, differences observed small clusters. method will be useful utilizing advanced neuroimaging techniques with MRI"
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "On the Ising Phase Transition in the Infrared-Divergent Spin Boson Model",
        "Rotational decoherence dynamics in ultracold molecules induced by a\n  tunable spin environment: The Central Rotor Model",
        "Pseudorapidity density distributions of charged particles and transverse\n  momentum spectra of identified particles in pp collisions in PACIAE 4.0 model",
        "Laser-based aberration corrector",
        "Dissociated Neuronal Cultures as Model Systems for Self-Organized\n  Prediction",
        "Geometric origin of supercurrents in Berry phase: Formula for computing\n  currents from wavefunctions with correlation and particle number variation",
        "Center vortices and the $\\mathrm{SU}(3)$ conformal window",
        "High-aspect-ratio silica meta-optics for high-intensity structured light",
        "Reporting on pTP sublimation during evaporation deposition",
        "Observation of a zero-energy excitation mode in the open Dicke model",
        "X-ray Thomson scattering studies on spin-singlet stabilization of highly\n  compressed H-like Be ions heated to two million degrees Kelvin",
        "Comment on: \"2005 VL1 is not Venera-2\"",
        "An Approach to Use Depletion Charges for Modifying Band Profiles for\n  Field-Effect Transistors",
        "On the Gauge Invariance of Secondary Gravitational Waves",
        "A physical model approach to order lot sizing",
        "Data driven discovery of human mobility models",
        "Refining Au\/Sb alloyed ohmic contacts in undoped Si\/SiGe strained\n  quantum wells",
        "Incompleteness of Bell's theorem",
        "Proceedings of the Erice Workshop: A new baseline for the hybrid,\n  asymmetric, linear Higgs factory HALHF",
        "An Adaptive Collocation Point Strategy For Physics Informed Neural\n  Networks via the QR Discrete Empirical Interpolation Method",
        "Demystifying 5G Polar and LDPC Codes: A Comprehensive Review and\n  Foundations",
        "Scalable Discovery of Fundamental Physical Laws: Learning\n  Magnetohydrodynamics from 3D Turbulence Data",
        "Modular covariant torus partition functions of dense $A_1^{(1)}$ and\n  dilute $A_2^{(2)}$ loop models",
        "Realistic predictions for Gaia black hole discoveries: comparison of\n  isolated binary and dynamical formation models",
        "Precision Higgs Constraints in U(1) Extensions of the Standard Model\n  with a Light Z'-Boson",
        "Bidirectional quantitative scattering microscopy",
        "The Motzkin subproduct system",
        "Sub-GeV Dark Matter Direct Detection with Neutrino Observatories",
        "Statistical inference for interacting innovation processes and related\n  general results"
      ],
      "abstract":[
        "We prove absence of ground states in the infrared-divergent spin boson model\nat large coupling. Our key argument reduces the proof to verifying long range\norder in the dual one-dimensional continuum Ising model, i.e., to showing that\nthe respective two point function is lower bounded by a strictly positive\nconstant. We can then use known results from percolation theory to establish\nlong range order at large coupling. Combined with the known existence of ground\nstates at small coupling, our result proves that the spin boson model undergoes\na phase transition with respect to the coupling strength. We also present an\nexpansion for the vacuum overlap of the spin boson ground state in terms of the\nIsing $n$-point functions, which implies that the phase transition is unique,\ni.e., that there is a critical coupling constant below which a ground state\nexists and above which none can exist.",
        "We show that quantum rotational wavepacket dynamics in molecules can be\ndescribed by a new system-environment model, which consists of a rotational\nsubsystem coupled to a magnetically tunable spin bath formed by the nuclear\nspins within the molecule. The central rotor model shares similarities with the\nparadigmatic central spin model, but features much richer rotational dynamics\nthat is sensitive to the molecule's environment, which can be initiated and\nprobed with short laser pulses used to control molecular orientation and\nalignment. We present numerical simulations of the nuclear-spin-bath-induced\nrotational decoherence dynamics of KRb molecules, which exhibit remarkable\nsensitivity to an external magnetic field. Our results show that ultracold\nmolecular gases provide a natural platform for the experimental realization of\nthe CRM.",
        "The pseudorapidity density distributions of charged particles and the\ntransverse momentum spectra of identified particles in proton-proton (pp)\ncollisions at the center-of-mass energies ranging from $\\sqrt{s}=200$ GeV to 13\nTeV have been systematically studied using the newly released parton and\ncascade model PACIAE 4.0 based on PYTHIA 8.3. The available experimental data\nare well reproduced across all analyzed aspects. This theoretical method can be\neasily extended to anywhere the experimental data for pp collisions are\ncurrently unavailable. Furthermore, since pp collisions serve as the baseline\nfor heavy-ion collisions, our results can provide a valuable resource for both\nexperimentalists and theorists.",
        "Aberration correctors are essential elements for achieving atomic resolution\nin state-of-the-art electron microscopes. Conventional correctors are based on\na series of multipolar electron lenses, but more versatile alternatives are\nintensively sought. Here we suggest spatially tailored intense laser pulses as\none such alternative. Our simulations demonstrate that the free-space\nelectron-photon interaction can be used to compensate for spherical and\nchromatic aberrations of subsequent electron lenses. We show a significant\nimprovement in the simulated electron probe sizes and discuss the prospects of\nutilizing the tailored laser fields as a platform for novel electron optics in\nultrafast electron microscope setups.",
        "Dissociated neuronal cultures provide a simplified yet effective model system\nfor investigating self-organized prediction and information processing in\nneural networks. This review consolidates current research demonstrating that\nthese in vitro networks display fundamental computational capabilities,\nincluding predictive coding, adaptive learning, goal-directed behavior, and\ndeviance detection. We examine how these cultures develop critical dynamics\noptimized for information processing, detail the mechanisms underlying learning\nand memory formation, and explore the relevance of the free energy principle\nwithin these systems. Building on these insights, we discuss how findings from\ndissociated neuronal cultures inform the design of neuromorphic and reservoir\ncomputing architectures, with the potential to enhance energy efficiency and\nadaptive functionality in artificial intelligence. The reduced complexity of\nneuronal cultures allows for precise manipulation and systematic investigation,\nbridging theoretical frameworks with practical implementations in bio-inspired\ncomputing. Finally, we highlight promising future directions, emphasizing\nadvancements in three-dimensional culture techniques, multi-compartment models,\nand brain organoids that deepen our understanding of hierarchical and\npredictive processes in both biological and artificial systems. This review\naims to provide a comprehensive overview of how dissociated neuronal cultures\ncontribute to neuroscience and artificial intelligence, ultimately paving the\nway for biologically inspired computing solutions.",
        "The complexity of itinerant and many-body nature in Bardeen-Cooper-Schrieffer\n(BCS) wavefunctions has traditionally led to the use of coarse-grained order\nparameters for describing currents in superconductors (SC), rather than\ndirectly utilizing wavefunctions. In this work, we introduce a phase-based\nformula that enables the direct computation of currents from microscopic\nwavefunctions, accounting for correlation and particle number variations.\nInterestingly, the formulation draws parallels with insulators, suggesting a\nunified framework for understanding (intra-band) charge transport across two\nextremes of conductivity. A group velocity current\n$J_{band}{\\propto}\\frac{1}{\\hbar}{\\partial}_kE(k)$ is derived from Berry phase,\nindependent of wave package dynamics, robust against correlation. Additionally,\nwe identify a correlation-driven contribution, $J_{corr}$, which reveals that\nthe pairing correlations ${\\langle}c_kc_{-k}{\\rangle}$ among dancing partners\nprovide a current component beyond the velocity operator.",
        "A novel approach for estimating the lower end of the $\\mathrm{SU}(3)$\nconformal window is presented through the study of center vortex geometry and\nits dependence on the number of fermion flavors $N_f$. Values ranging from $N_f\n= 2$--$8$ are utilized to infer an upper limit for vortex behavior in the low\n$N_f$ phase, which may inform the transition to the conformal window. The\nsimulations are performed at a single lattice spacing and pion mass, both fixed\nfor all $N_f$. Visualizations of the center vortex structure in\nthree-dimensional slices of the lattice reveal a growing roughness in the\nvortex matter as a function of $N_f$, embodied by an increase in the density of\nvortex matter in the percolating cluster and a simultaneous reduction in\nsecondary clusters disconnected from the percolating cluster in 3D slices. This\nis quantified by various bulk properties, including the vortex and branching\npoint densities. A correlation of the vortex structure reveals a turning point\nnear $N_f \\simeq 5$ past which a randomness in the vortex field becomes the\ndominant aspect of its evolution with $N_f$. As a byproduct, extrapolations to\nthe vortex content of a uniform-random gauge field provide a critical point at\nwhich there must be a drastic shift in vacuum field structure. A precise\nestimate for the critical value is extracted as $N_f^* = 11.43(16)(17)$, close\nto various other estimates.",
        "Structured light and high-intensity ultrafast lasers are two rapidly\nadvancing frontiers in photonics, yet their intersection remains largely\nunexplored. While ultrafast lasers continue to push the boundaries of peak\nintensities, structured light has enabled unprecedented control over light's\nspatial, temporal, and polarization properties. However, the lack of robust\noptical devices capable of bridging structured light with the high-intensity\ndomain has constrained progress in combining these directions. Here, we\ndemonstrate high-aspect-ratio silica meta-optics, which close this gap by\ncombining silica's extraordinary damage resistance with the advanced phase and\npolarization control offered by metasurfaces. By leveraging anisotropic etching\ntechniques, we fabricate nanopillars exceeding 3 $\\mu$m in height with aspect\nratios up to 14, enabling precise manipulation of complex light fields at\nintensities far beyond the thresholds of conventional metasurfaces. We showcase\ntheir functionality in generating vortex beams and achieving polarization\nmanipulation with large phase retardance at challenging long-visible\nwavelengths. High-aspect-ratio silica meta-optics unlock structured\nlaser-matter interactions in extreme regimes, that will surpass plasma\nionization thresholds and enable applications such as relativistic particle\nacceleration and high-harmonic generation with structured beams, for both\ntabletop ultrafast systems and large-scale laser facilities.",
        "Noble liquid detectors rely on wavelength shifter materials, such as\np-terphenyl (pTP) and Tetraphenyl-butadiene (TPB), which are widely used in\nneutrino and dark matter experiments. Given their importance, a thorough\nunderstanding and characterization of these compounds are essential for\noptimizing experimental techniques and enhancing detector performance. In this\nstudy, we report a novel phenomenon in which commonly used wavelength shifters\nundergo spontaneous sublimation under high vacuum conditions. We quantify the\nsublimation rates of pTP and TPB as a function of pressure and temperature,\nassessing their impact on material growth and physical properties.\nAdditionally, we investigate how variations in film thickness and growth rate\ninfluence the sublimation process. These findings provide critical insights\ninto improving the handling and preparation of wavelength shifters during the\nfabrication of light detectors for these experiments, ensuring their stability\nand performance in low-background photodetection systems.",
        "Approaching phase boundaries in many-body systems can give rise to intriguing\nsignatures in their excitation spectra. Here, we explore the excitation\nspectrum of a Bose-Einstein condensate strongly coupled to an optical cavity\nand pumped by an optical standing wave, which simulates the famous\nDicke-Hepp-Lieb phase transition of the open Dicke model with dissipation\narising due to photon leakage from the cavity. For weak dissipation, the\nexcitation spectrum displays two strongly polaritonic modes. Close to the phase\nboundary, we observe an intriguing regime where the lower-energetic of these\nmodes, instead of showing the expected roton-type mode softening, is found to\napproach and persist at zero energy, well before the critical pump strength for\nthe Dicke-Hepp-Lieb transition boundary is reached. Hence, a peculiar situation\narises, where an excitation is possible at zero energy cost, but nevertheless\nno instability of the system is created.",
        "Experiments at the US National Ignition Facility (NIF) [D\\\"{o}ppner et al.,\nNature {\\bf 618}, 270-275 (2023)] have created highly compressed hot\nhydrogen-like Be plasmas. Published analyses of the the NIF experiment have\nused finite-$T$ multi-atom density-functional theory (DFT) with Molecular\ndynamics (MD), and Path-Integral Monte Carlo (PIMC) simulations. These methods\nare very expensive to implement and often lack physical transparency. Here we\n(i) relate their results to simpler first-principles average-atom results, (ii)\nestablish the feasibility of rapid data analysis, with good accuracy and gain\nin physical transparency, and (iii) show that the NIF experiment reveals\nhigh-$T$ spin-singlet pairing of hydrogen-like Be ions with near neighbours.\nOur analysis predicts such stabilization over a wide range of compressed\ndensities for temperatures close to two million Kelvin. Calculations of\nstructure factors $S(k)$ for electrons or ions, the Raleigh weight and other\nquantities of interest to X-ray Thomson scattering are presented. We find that\nthe NIF data at the scattering wavevector $k_{sc}$ of 7.89 \\AA$^{-1}$ are more\nconsistent with a density of $20\\pm2$ g\/cm$^3$, mean ionization $\\bar{Z}=$3.25,\nat a temperature of $\\simeq$ 1,800,000 K than the 34 g\/cm$^3, \\bar{Z}=3.4$\nproposed by the NIF team. The relevance of ion-electron coupled-modes in\nstudying small $k_{sc}$ data is indicated.",
        "I show that the small differences between the orbital parameters of the dark\ncomet 2005 VL1 and the Venera 2 spacecraft (reported in arXiv:2503.07972) are\nof the magnitude expected from gravitational deflection by a close encounter of\nVenera 2 with Venus.",
        "We present the study of using depletion charges for tailoring lateral band\nprofiles and applying it to the promising gate-all-around field-effect\ntransistors (GAAFET). Specifically, we introduce heavily p-type doped Si next\nto the channel, but outside the channel, of a transistor. They are connected to\nthe heavily n-type doped source and drain for generating the depletion charges.\nThe finite difference method was used for simulations and the results show\nsignificant modifications of the conduction band along the channel. The\ndepletion charges act as built-in electrodes capable of significantly modifying\nthe band profiles of field-effect transistors. Quantum confinement within the\nchannel has been attempted with different approaches, such as additional\nelectrodes and point contacts. The results presented show two aspects of this\napproach, namely, realizing quantum confinement in an all-Si structure and\ntailoring band profiles within channels to modify their transport properties.",
        "Second-order tensor perturbations induced by primordial fluctuations play a\ncrucial role in probing small-scale physics, but gauge dependence of their\nenergy density has remained a fundamental challenge in cosmological\nperturbation theory. We address this issue by introducing a boundary\ncondition-based filtering method that extracts physical radiation through the\nSommerfeld criterion. We demonstrate that after filtering non-physical modes,\nthe energy density of secondary gravitational waves becomes gauge-invariant and\nexhibits physically consistent behavior in the sub-horizon limit. This approach\nprovides a unified framework for both adiabatic and isocurvature perturbations,\nenhancing theoretical predictions and observational signatures of early\nuniverse physics.",
        "The growing need for companies to reduce costs and maximize profits has led\nto an increased focus on logistics activities. Among these, inventory\nmanagement plays a crucial role in minimizing organizational expenses by\noptimizing the storage and transportation of materials. In this context, this\nstudy introduces an optimization model for the lot-sizing problem based on a\nphysical system approach. By establishing that the material supply problem is\nisomorphic to a one-dimensional mechanical system of point particles connected\nby elastic elements, we leverage this analogy to derive cost optimization\nconditions naturally and obtain an exact solution. This approach determines lot\nsizes that minimize the combined ordering and inventory holding costs in a\nsignificantly shorter time, eliminating the need for heuristic methods. The\noptimal lot sizes are defined in terms of the parameter $ \\gamma = 2C_O \/ C_H\n$, which represents the relationship between the ordering cost per order ($ C_O\n$) and the holding cost per period for the material required in one period ($\nC_H $). This parameter fully dictates the system's behavior: when $ \\gamma \\leq\n1 $, the optimal strategy is to place one order per period, whereas for $\n\\gamma > 1 $, the number of orders $ N $ is reduced relative to the planning\nhorizon $ M $, meaning $ N < M $. By formulating the total cost function in\nterms of the intensive variable $ N\/M $, we consolidate the entire optimization\nproblem into a single function of $ \\gamma $. This eliminates the need for\ncomplex algorithms, enabling faster and more precise purchasing decisions. The\nproposed model was validated through a real-world case study and benchmarked\nagainst classical algorithms, demonstrating superior cost optimization and\nreduced execution time. These findings underscore the potential of this\napproach for improving material lot-sizing strategies.",
        "Human mobility is a fundamental aspect of social behavior, with broad\napplications in transportation, urban planning, and epidemic modeling. However,\nfor decades new mathematical formulas to model mobility phenomena have been\nscarce and usually discovered by analogy to physical processes, such as the\ngravity model and the radiation model. These sporadic discoveries are often\nthought to rely on intuition and luck in fitting empirical data. Here, we\npropose a systematic approach that leverages symbolic regression to\nautomatically discover interpretable models from human mobility data. Our\napproach finds several well-known formulas, such as the distance decay effect\nand classical gravity models, as well as previously unknown ones, such as an\nexponential-power-law decay that can be explained by the maximum entropy\nprinciple. By relaxing the constraints on the complexity of model expressions,\nwe further show how key variables of human mobility are progressively\nincorporated into the model, making this framework a powerful tool for\nrevealing the underlying mathematical structures of complex social phenomena\ndirectly from observational data.",
        "Shallow undoped Si\/SiGe quantum wells are the leading platforms for hosting\nquantum processors based on spin-qubits. The ohmic contacts to the electron gas\nin these systems are accomplished by ion-implantation technique since the\nconventional Au\/Sb alloyed contacts present a rough surface consisting of sharp\nislands and pits. These sharp protrusions cause electrical discharge across the\ngate-dielectric between the ohmic contacts and the accumulation-gates causing\ndevice break-down. A clear understanding of the surface morphology, elemental,\ncompositional and electrical characterization of the alloyed region would\nenable one to engineer a smoother post alloyed surface. In this work, we find\nthat the rough surface morphology is a cumulative effect of the Au\/Si eutectic\nreaction and the threading dislocations inherent in the heterostructure. The\nstructural, elemental, and chemical-state analysis show that the inverted\npyramidal pits are resulting from the enhanced Au\/Si eutectic reaction at the\nthreading dislocations stemming from the heterostructure interface, while, the\nsharp protrusions causing accumulation gate-leakage are gold-rich\nprecipitations. The protrusions are removed using an aqua regia treatment prior\nto the deposition of the gate-oxide and gate electrode. Exploiting a Hall bar\ndevice, we analyse the mobility and carrier concentration of the undoped\nSi\/SiGe consisting of Au\/Sb alloyed contacts down to 1.5 K. The measured\nmobility ~10^5 cm^2\/Vs and carrier concentration of ~10^11\/cm^2are comparable\nto the reported values on similar high-mobility heterostructures confirming the\nefficacy of our modified Au\/Sb alloy technique in accomplishing high-efficiency\ncontacts to undoped Si\/SiGe heterostructures.",
        "In the paper it is reported that Bell's correlation formula allows an\nEinstein local hidden variables explanation. The key is the application of\nPetis integration.",
        "The HALHF collaboration has discussed a new baseline for the project, taking\ninto account comments from the accelerator community on various aspects of the\noriginal design. In particular, these concerned the practicality of the\ndual-purpose linac to accelerate both colliding positron bunches and the drive\nbeams required for the plasma linac. In addition, many other aspects of the\nproject were also considered; the discussion and conclusions are documented in\nthis paper. Finally, a new baseline is outlined that has been optimised and\naddresses several weaknesses in the original design, has higher luminosity,\nreduced centre-of-mass energy boost and additional features such as positron\npolarization as well as electron polarization. Although HALHF has become longer\nand more expensive, it remains significantly smaller and cheaper than other\nmature Higgs factory designs currently under discussion.",
        "Physics-informed neural networks (PINNs) have gained significant attention\nfor solving forward and inverse problems related to partial differential\nequations (PDEs). While advancements in loss functions and network\narchitectures have improved PINN accuracy, the impact of collocation point\nsampling on their performance remains underexplored. Fixed sampling methods,\nsuch as uniform random sampling and equispaced grids, can fail to capture\ncritical regions with high solution gradients, limiting their effectiveness for\ncomplex PDEs. Adaptive methods, inspired by adaptive mesh refinement from\ntraditional numerical methods, address this by dynamically updating collocation\npoints during training but may overlook residual dynamics between updates,\npotentially losing valuable information. To overcome this limitation, we\npropose an adaptive collocation point selection strategy utilizing the QR\nDiscrete Empirical Interpolation Method (QR-DEIM), a reduced-order modeling\ntechnique for efficiently approximating nonlinear functions. Our results on\nbenchmark PDEs, including the wave, Allen-Cahn, and Burgers' equations,\ndemonstrate that our QR-DEIM-based approach improves PINN accuracy compared to\nexisting methods, offering a promising direction for adaptive collocation point\nstrategies.",
        "This paper serves as a comprehensive guide for practitioners and scholars\naiming to understand the channel coding and decoding schemes integral to the 5G\nNR standard, with a particular focus on LDPC and polar codes. We start by\nexplaining the design procedures that underlie these channel codes, offering\nfundamental information from the perspectives of both encoding and decoding. In\norder to determine the present status of research in this area, we also provide\na thorough literature review. Notably, we add comprehensive, standard-specific\ninformation to these foundational evaluations that is frequently difficult to\nextract from technical specification documents. The significance of reviewing\nand refining the foundations of the aforementioned codes lies in their\npotential to serve as candidate error-correcting codes for the future 6G\nstandard and beyond.",
        "The discovery of dynamical models from data represents a crucial step in\nadvancing our understanding of physical systems. Library-based sparse\nregression has emerged as a powerful method for inferring governing equations\ndirectly from spatiotemporal data, but current model-agnostic implementations\nremain computationally expensive, limiting their applicability to data that\nlack substantial complexity. To overcome these challenges, we introduce a\nscalable framework that enables efficient discovery of complex dynamical models\nacross a wide range of applications. We demonstrate the capabilities of our\napproach, by ``discovering'' the equations of magnetohydrodynamics (MHD) from\nsynthetic data generated by high-resolution simulations of turbulent MHD flows\nwith viscous and Ohmic dissipation. Using a library of candidate terms that is\n$\\gtrsim 10$ times larger than those in previous studies, we accurately recover\nthe full set of MHD equations, including the subtle dissipative terms that are\ncritical to the dynamics of the system. Our results establish sparse regression\nas a practical tool for uncovering fundamental physical laws from complex,\nhigh-dimensional data without assumptions on the underlying symmetry or the\nform of any governing equation.",
        "Yang-Baxter integrable dense $A_1^{(1)}$ and dilute $A_2^{(2)}$ loop models\nare considered on the torus in their simplest physical regimes. A combination\nof boundary conditions $(h,v)$ is applied in the horizontal and vertical\ndirections with $h,v=0$ and $1$ for periodic and antiperiodic boundary\nconditions respectively. The fugacities of non-contractible and contractible\nloops are denoted by $\\alpha$ and $\\beta$ respectively where $\\beta$ is simply\nrelated to the crossing parameter $\\lambda$. At roots of unity, when\n$\\lambda\/\\pi\\in\\mathbb Q$, these models are the dense ${\\cal LM}(p,p')$ and\ndilute ${\\cal DLM}(p,p')$ logarithmic minimal models with $p,p'$ coprime\nintegers. We conjecture the scaling limits of the transfer matrix traces in the\nstandard modules with $d$ defects and deduce the conformal partition functions\n${\\cal Z}_{\\textrm{dense}}^{(h,v)}(\\alpha)$ and ${\\cal\nZ}_{\\textrm{dilute}}^{(h,v)}(\\alpha)$ using Markov traces. These are expressed\nin terms of functions ${\\cal Z}_{m,m'}(g)$ known from the Coulomb gas arguments\nof Di Francesco, Saleur and Zuber and subsequently as sesquilinear forms in\nVerma characters. Crucially, we find that the partition functions are identical\nfor the dense and dilute models. The coincidence of these conformal partition\nfunctions provides compelling evidence that, for given $(p,p')$, these dense\nand dilute theories lie in the same universality class. In root of unity cases\nwith $\\alpha=2$, the $(h,v)$ modular covariant partition functions are also\nexpressed as sesquilinear forms in affine $u(1)$ characters involving\ngeneralized Bezout conjugates. These also give the modular covariant partition\nfunctions for the 6-vertex and Izergin-Korepin 19-vertex models in the\ncorresponding regimes.",
        "Astrometry from Gaia has enabled discovery of three dormant black holes (BHs)\nin au-scale binaries. Numerous models have been proposed to explain their\nformation, including several that have forecasted Gaia detections. However,\nprevious works have used simplified detectability metrics that do not capture\nkey elements of the Gaia astrometric orbit selection function. We apply a\nrealistic forward-model of Gaia astrometric orbit catalogs to BH binary\npopulations generated through (a) isolated binary evolution (IBE) and (b)\ndynamical formation in star clusters. For both formation channels, we analyze\nbinary populations in a simulated Milky Way-like galaxy with a realistic\nmetallicity-dependent star formation history and 3D dust map. We generate epoch\nastrometry for each binary from the Gaia scanning law and fit it with the\ncascade of astrometric models used in Gaia DR3. The IBE model of Chawla et al.\n(2022) predicts that no BH binaries should have been detected in DR3 and thus\nsignificantly underpredicts the formation rate of Gaia BHs. In contrast, the\ndynamical model of Di Carlo et al. (2024) overpredicts the number of BHs\nreceiving DR3 orbital solutions by a factor of $\\sim$8. The two models predict\nvery different orbital period distributions, with the IBE model predicting only\nbinaries that avoided common envelope evolution and have $P_{\\text{orb}}\n\\gtrsim 2,000$ d to be detectable, and the dynamical formation model predicting\na period distribution that is roughly log-uniform. Adopting the dynamical\nchannel as a fiducial model and rescaling by a factor of 1\/8 to match DR3, we\npredict that $\\sim$30 BH binaries will be detected in Gaia DR4, representing\n$\\sim0.1\\%$ of Milky Way BHs with luminous companions in au-scale orbits.",
        "Anomaly free $U(1)$ extensions of the standard model (SM) predict a new\nneutral gauge boson $Z'$. When the $Z'$ obtains its mass from the spontaneous\nbreaking of the new $U(1)$ symmetry by a new complex scalar field, the model\nalso predicts a second real scalar $s$ and the searches for the new scalar and\nthe new gauge boson become intertwined. We present the computation of\nproduction cross sections and decay widths of such a scalar $s$ in models with\na light $Z'$ boson, when the decay $h\\to Z' Z'$ may have a sizeable branching\nratio. We show how Higgs signal strength measurement in this channel can\nprovide stricter exclusion bounds on the parameters of the model than those\nobtained from the total signal strength for Higgs boson production.",
        "Quantitative phase microscopy (QPM) and interferometric scattering (iSCAT)\nmicroscopy are powerful label-free imaging techniques and are widely used for\nbiomedical applications. Each method, however, possesses distinct limitations:\nQPM, which measures forward scattering (FS), excels at imaging microscale\nstructures but struggles with rapidly moving nanoscale objects, while iSCAT,\nbased on backward scattering (BS), is highly sensitive to nanoscale dynamics\nbut lacks the ability to image microscale structures comprehensively. Here, we\nintroduce bidirectional quantitative scattering microscopy (BiQSM), an\ninnovative approach that integrates FS and BS detection using off-axis digital\nholography with bidirectional illumination and spatial-frequency multiplexing.\nBiQSM achieves spatiotemporal consistency and a dynamic range 14 times wider\nthan QPM, enabling simultaneous imaging of nanoscale and microscale cellular\ncomponents. We demonstrate BiQSM's ability to reveal spatiotemporal behaviors\nof intracellular structures, with FS-BS correlation analysis providing insights\ninto proteins, lipids, and membranes. Time-lapse imaging of dying cells further\nhighlights BiQSM's potential as a label-free tool for monitoring cellular vital\nstates through structural and motion-related changes. By bridging the strengths\nof QPM and iSCAT, BiQSM advances quantitative cellular imaging and opens new\navenues for studying dynamic biological processes.",
        "We introduce a subproduct system of finite-dimensional Hilbert spaces using\nthe Motzkin planar algebra and its Jones-Wenzl idempotents, which generalizes\nthe Temperley-Lieb subproduct system of Habbestad and Neshveyev. We provide a\ndescription of the corresponding Toeplitz C$^*$-algebra as a universal\nC$^*$-algebra, defined in terms of generators and relations, and we highlight\nproperties of its representation theory.",
        "We present a new technique for sub-GeV dark matter (DM) searches and a new\nuse of neutrino observatories. DM-electron scattering can excite or ionize\ntarget molecules in the observatory, which then produce light that can be\ndetected by its photomultiplier tubes (PMTs). While individual DM scatterings\nare undetectable, the aggregate rate from many independent scatterings can be\nisolated from the total PMT dark rate using the expected DM annual modulation.\nWe showcase this technique with the example of JUNO, a 20,000 ton scintillator\ndetector, showing that its sensitivity in some mass ranges exceeds all other\ntechniques and reaches key particle-theory targets.",
        "Given the importance of understanding how different innovation processes\naffect each other, we have introduced a model for a finite system of\ninteracting innovation processes. The present work focuses on the second-order\nasymptotic properties of the model and illustrates how to leverage the\ntheoretical results in order to make statistical inference on the intensity of\nthe interaction. This methodology is presented within a general framework in\nthe supplementary material to ensure its broad applicability across various\ncontexts. We apply the proposed tools to two real data sets (from Reddit and\nGutenberg)."
      ]
    }
  },
  {
    "id":2411.17617,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"Deep learning-Based 3D inpainting of brain MR images",
    "start_abstract":"Abstract The detailed anatomical information of the brain provided by 3D magnetic resonance imaging (MRI) enables various neuroscience research. However, due to long scan time for MR images, 2D images are mainly obtained in clinical environments. purpose this study is generate from a sparsely sampled using an inpainting deep neural network that has U-net-like structure and DenseNet sub-blocks. To train network, not only fidelity loss but also perceptual based on VGG were considered. Various methods used assess overall similarity between inpainted original data. In addition, morphological analyzes performed investigate whether data produced local features similar diagnostic ability was evaluated investigating pattern changes disease groups. Brain anatomy details efficiently recovered proposed network. voxel-based analysis gray matter volume cortical thickness, differences observed small clusters. method will be useful utilizing advanced neuroimaging techniques with MRI",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "ALL-Net: Anatomical information lesion-wise loss function integrated into neural network for multiple sclerosis lesion segmentation"
      ],
      "abstract":[
        "Accurate detection and segmentation of multiple sclerosis (MS) brain lesions on magnetic resonance images are important for disease diagnosis treatment. This is a challenging task as vary greatly in size, shape, location, image contrast. The objective our study was to develop an algorithm based deep convolutional neural network integrated with anatomic information lesion-wise loss function (ALL-Net) fast accurate automated MS lesions. Distance transformation mapping used construct module that encoded lesion-specific anatomical information. To overcome the lesion size imbalance during training improve small lesions, developed which individual were modeled spheres equal size. On ISBI-2015 longitudinal challenge dataset (19 subjects total), ALL-Net achieved overall score 93.32 amongst top performing methods. larger Cornell (176 significantly improved both voxel-wise metrics (Dice improvement 3.9% 35.3% p-values ranging from p < 0.01 0.0001, AUC precision-recall curve 2.1% 29.8%) (lesion-wise F1 12.6% 29.8% all ROC 1.4% 20.0%) compared leading publicly available tools."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MI-DETR: An Object Detection Model with Multi-time Inquiries Mechanism",
        "Non-linear Partition of Unity method",
        "Linear statistics at the microscopic scale for the 2D Coulomb gas",
        "The role of effective mass and long-range interactions in the band-gap\n  renormalization of photo-excited semiconductors",
        "Machine-learning potentials for structurally and chemically complex MAB\n  phases: strain hardening and ripplocation-mediated plasticity",
        "On Elephant Random Walk with Random Memory",
        "Theoretical Data-Driven MobilePosenet: Lightweight Neural Network for\n  Accurate Calibration-Free 5-DOF Magnet Localization",
        "Security and Quality in LLM-Generated Code: A Multi-Language,\n  Multi-Model Analysis",
        "ProJudge: A Multi-Modal Multi-Discipline Benchmark and\n  Instruction-Tuning Dataset for MLLM-based Process Judges",
        "Analysis of Information Loss on Composition Measurement in Stiff\n  Chemically Reacting Systems",
        "ReVeil: Unconstrained Concealed Backdoor Attack on Deep Neural Networks\n  using Machine Unlearning",
        "Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education",
        "Non-ergodic Phase Transition in the Global Hysteresis of the Frustrated\n  Magnet DyRu2Si2",
        "Evaluation for Regression Analyses on Evolving Data Streams",
        "Many-body perturbation theory for moir\\'{e} systems",
        "Zooming into the horizon region of black hole-type objects",
        "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling",
        "Murmurations and Sato-Tate Conjectures for High Rank Zetas of Elliptic\n  Curves II: Beyond Riemann Hypothesis",
        "Weak mixing angle under $\\text{U}(1, 3)$ colored gravity",
        "An Elementary Microscopic Model of Sympatric Speciation",
        "Performance Analysis of Traditional VQA Models Under Limited\n  Computational Resources",
        "Compressing Language Models for Specialized Domains",
        "Efficient and Accurate Estimation of Lipschitz Constants for Hybrid\n  Quantum-Classical Decision Models",
        "Bruhat-Tits buildings and $p$-adic period domains",
        "Big algebra in type $A$ for the coordinate ring of the matrix space",
        "Nonlinear optical response in a ferromagnetic insulating manganite:\n  Pr$_{0.8}$Ca$_{0.2}$MnO$_{3}$",
        "Can Large Language Models Outperform Non-Experts in Poetry Evaluation? A\n  Comparative Study Using the Consensual Assessment Technique",
        "Nucleation line in three-component Bose-Einstein condensates in\n  Gross-Pitaevskii theory",
        "A Unified Framework for Entropy Search and Expected Improvement in\n  Bayesian Optimization"
      ],
      "abstract":[
        "Based on analyzing the character of cascaded decoder architecture commonly\nadopted in existing DETR-like models, this paper proposes a new decoder\narchitecture. The cascaded decoder architecture constrains object queries to\nupdate in the cascaded direction, only enabling object queries to learn\nrelatively-limited information from image features. However, the challenges for\nobject detection in natural scenes (e.g., extremely-small, heavily-occluded,\nand confusingly mixed with the background) require an object detection model to\nfully utilize image features, which motivates us to propose a new decoder\narchitecture with the parallel Multi-time Inquiries (MI) mechanism. MI enables\nobject queries to learn more comprehensive information, and our MI based model,\nMI-DETR, outperforms all existing DETR-like models on COCO benchmark under\ndifferent backbones and training epochs, achieving +2.3 AP and +0.6 AP\nimprovements compared to the most representative model DINO and SOTA model\nRelation-DETR under ResNet-50 backbone. In addition, a series of diagnostic and\nvisualization experiments demonstrate the effectiveness, rationality, and\ninterpretability of MI.",
        "This paper introduces the Non-linear Partition of Unity Method, a novel\ntechnique integrating Radial Basis Function interpolation and Weighted\nEssentially Non-Oscillatory algorithms. It addresses challenges in\nhigh-accuracy approximations, particularly near discontinuities, by adapting\nweights dynamically. The method is rooted in the Partition of Unity framework,\nenabling efficient decomposition of large datasets into subproblems while\nmaintaining accuracy. Smoothness indicators and compactly supported functions\nensure precision in regions with discontinuities. Error bounds are calculated\nand validate its effectiveness, showing improved interpolation in discontinuous\nand smooth regions. Some numerical experiments are performed to check the\ntheoretical results.",
        "We consider the classical Coulomb gas in two dimensions at the inverse\ntemperature $\\beta=2$, confined within a droplet of radius $R$ by a\nrotationally invariant potential $U(r)$. For $U(r)\\sim r^2$ this describes the\neigenvalues of the complex Ginibre ensemble of random matrices. We study linear\nstatistics of the form ${\\cal L}_N = \\sum_{i=1}^N f(|{\\bf x}_i|)$, where ${\\bf\nx}_i$'s are the positions of the $N$ particles, in the large $N$ limit with\n$R=O(1)$. It is known that for smooth functions $f(r)$ the variance ${\\rm Var}\n\\,{\\cal L}_N= O(1)$, while for an indicator function relevant for the disk\ncounting statistics, all cumulants of ${\\cal L}_N$ of order $q \\geq 2$ behave\nas $\\sim \\sqrt{N}$. In addition, for smooth functions, it was shown that the\ncumulants of ${\\cal L}_N$ of order $q \\geq 3$ scale as $\\sim N^{2-q}$.\nSurprisingly it was found that they depend only on $f'(|\\bf x|)$ and its\nderivatives evaluated exactly at the boundary of the droplet. To understand\nthis property, and interpolate between the two behaviors (smooth versus\nstep-like), we study the microscopic linear statistics given by $f(r) \\to\nf_N(r) = \\phi((r-\\hat r) \\sqrt{N}\/\\xi)$, which probes the fluctuations at the\nscale of the inter-particle distance. We compute the cumulants of ${\\cal L}_N$\nat large $N$ for a fixed $\\phi(u)$ at arbitrary $\\xi$. For large $\\xi$ they\nmatch the predictions for smooth functions which shows that the leading\ncontribution in that case comes from a boundary layer of size $1\/\\sqrt{N}$ near\nthe boundary of the droplet. Finally we show that the full probability\ndistribution of ${\\cal L}_N$ take two distinct large deviation forms, in the\nregime ${\\cal L}_N \\sim \\sqrt{N}$ and ${\\cal L}_N \\sim N$ respectively. We also\ndiscuss applications of our results to fermions in a rotating harmonic trap and\nto the Ginibre symplectic ensemble.",
        "Understanding how to control changes in electronic structure and related\ndynamical renormalizations by external driving fields is the key for\nunderstanding ultrafast spectroscopy and applications in electronics. Here we\nfocus on the band-gap's modulation by external electric fields and uncover the\neffect of band dispersion on the gap renormalization. We employ the Green's\nfunction formalism using the real-time Dyson expansion to account for dynamical\ncorrelations induced by photodoping. The many-body formalism captures the\ndynamics of systems with long-range interactions, carrier mobility, and\nvariable electron and hole effective mass. We also demonstrate that mean-field\nsimulations based on the Hartree-Fock Hamiltonian, which lacks dynamical\ncorrelations, yields a qualitatively incorrect picture of band-gap\nrenormalization. We find the trend that increasing effective mass, thus\ndecreasing mobility, leads to as much as a 6\\% enhancement in band-gap\nrenormalization. Further, the renormalization is strongly dependent on the\ndegree of photodoping. As the screening induced by free electrons and holes\neffectively reduces any long-range and interband interactions for highly\nexcited systems, we show that there is a specific turnover point with minimal\nband-gap. We further demonstrate that the optical gap renormalization follows\nthe same trend though its magnitude is altered by the Moss-Burstein effect.",
        "Though offering unprecedented pathways to molecular dynamics (MD) simulations\nof technologically-relevant materials and conditions, machine-learning\ninteratomic potentials (MLIPs) are typically trained for ``simple'' materials\nand properties with minor size effects. Our study of MAB phases (MABs) -\nalternating transition metal boride (MB) and group A element layers -\nexemplifies that MLIPs for complex materials can be fitted and used in a\nhigh-throughput fashion: for predicting structural and mechanical properties\nacross a large chemical\/phase\/temperature space. Considering group 4-6\ntransition metal based MABs, with A=Al and the 222, 212, and 314 type phases,\nthree MLIPs are trained and tested, including lattice and elastic constants\ncalculations at temperatures $T\\in\\{0,300,1200\\}$ K, extrapolation grade and\nenergy (force, stress) error analysis for $\\approx{3\\cdot10^6}$ ab initio MD\nsnapshots. Subsequently, nanoscale tensile tests serve to quantify upper limits\nof strength and toughness attainable in single-crystal MABs at 300~K as well as\ntheir temperature evolution. In-plane tensile deformation is characterised by\nrelatively high strength, {110}$\\langle001\\rangle$ type slipping, and failure\nby shear banding. The response to [001] loading is softer, triggers work\nhardening, and failure by kinking and layer delamination. Furthermore,\nW$_2$AlB$_2$ able to retard fracture via ripplocations and twinning from 300 up\nto 1200~K.",
        "In this paper, we introduce the elephant random walk (ERW) with memory\nconsisting of randomly selected steps from its history. It is a time-changed\nvariant of the standard elephant random walk with memory consisting of its full\nhistory. At each time point, the time changing component is the composition of\ntwo uniformly distributed independent random variables with support over all\nthe past steps. Several conditional distributional properties including the\nconditional mean increments and conditional displacement of ERW with random\nmemory are obtained. Using these conditional results, we derive the recursive\nand explicit expressions for the mean increments and mean displacement of the\nwalk.",
        "Permanent magnet tracking using the external sensor array is crucial for the\naccurate localization of wireless capsule endoscope robots. Traditional\ntracking algorithms, based on the magnetic dipole model and Levenberg-Marquardt\n(LM) algorithm, face challenges related to computational delays and the need\nfor initial position estimation. More recently proposed neural network-based\napproaches often require extensive hardware calibration and real-world data\ncollection, which are time-consuming and labor-intensive. To address these\nchallenges, we propose MobilePosenet, a lightweight neural network architecture\nthat leverages depthwise separable convolutions to minimize computational cost\nand a channel attention mechanism to enhance localization accuracy. Besides,\nthe inputs to the network integrate the sensors' coordinate information and\nrandom noise, compensating for the discrepancies between the theoretical model\nand the actual magnetic fields and thus allowing MobilePosenet to be trained\nentirely on theoretical data. Experimental evaluations conducted in a \\(90\n\\times 90 \\times 80\\) mm workspace demonstrate that MobilePosenet exhibits\nexcellent 5-DOF localization accuracy ($1.54 \\pm 1.03$ mm and $2.24 \\pm\n1.84^{\\circ}$) and inference speed (0.9 ms) against state-of-the-art methods\ntrained on real-world data. Since network training relies solely on theoretical\ndata, MobilePosenet can eliminate the hardware calibration and real-world data\ncollection process, improving the generalizability of this permanent magnet\nlocalization method and the potential for rapid adoption in different clinical\nsettings.",
        "Artificial Intelligence (AI)-driven code generation tools are increasingly\nused throughout the software development lifecycle to accelerate coding tasks.\nHowever, the security of AI-generated code using Large Language Models (LLMs)\nremains underexplored, with studies revealing various risks and weaknesses.\nThis paper analyzes the security of code generated by LLMs across different\nprogramming languages. We introduce a dataset of 200 tasks grouped into six\ncategories to evaluate the performance of LLMs in generating secure and\nmaintainable code. Our research shows that while LLMs can automate code\ncreation, their security effectiveness varies by language. Many models fail to\nutilize modern security features in recent compiler and toolkit updates, such\nas Java 17. Moreover, outdated methods are still commonly used, particularly in\nC++. This highlights the need for advancing LLMs to enhance security and\nquality while incorporating emerging best practices in programming languages.",
        "As multi-modal large language models (MLLMs) frequently exhibit errors when\nsolving scientific problems, evaluating the validity of their reasoning\nprocesses is critical for ensuring reliability and uncovering fine-grained\nmodel weaknesses. Since human evaluation is laborious and costly, prompting\nMLLMs as automated process judges has become a common practice. However, the\nreliability of these model-based judges remains uncertain. To address this, we\nintroduce ProJudgeBench, the first comprehensive benchmark specifically\ndesigned for evaluating abilities of MLLM-based process judges. ProJudgeBench\ncomprises 2,400 test cases and 50,118 step-level labels, spanning four\nscientific disciplines with diverse difficulty levels and multi-modal content.\nIn ProJudgeBench, each step is meticulously annotated by human experts for\ncorrectness, error type, and explanation, enabling a systematic evaluation of\njudges' capabilities to detect, classify and diagnose errors. Evaluation on\nProJudgeBench reveals a significant performance gap between open-source and\nproprietary models. To bridge this gap, we further propose ProJudge-173k, a\nlarge-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning\nstrategy that encourages models to explicitly reason through problem-solving\nbefore assessing solutions. Both contributions significantly enhance the\nprocess evaluation capabilities of open-source models. All the resources will\nbe released to foster future research of reliable multi-modal process\nevaluation.",
        "Gas sampling methods have been crucial for the advancement of combustion\nscience, enabling analysis of reaction kinetics and pollutant formation.\nHowever, the measured composition can deviate from the true one because of the\npotential residual reactions in the sampling probes. This study formulates the\ninitial composition estimation in stiff chemically reacting systems as a\nBayesian inference problem, solved using the No-U-Turn Sampler (NUTS).\nInformation loss arises from the restriction of system dynamics by low\ndimensional attracting manifold, where constrained evolution causes initial\nperturbations to decay or vanish in fast eigen-directions in composition space.\nThis study systematically investigates the initial value inference in\ncombustion systems and successfully validates the methodological framework in\nthe Robertson toy system and hydrogen autoignition. Furthermore, a gas sample\ncollected from a one-dimensional hydrogen diffusion flame is analyzed to\ninvestigate the effect of frozen temperature on information loss. The research\nhighlights the importance of species covariance information from observations\nin improving estimation accuracy and identifies how the rank reduction in the\nsensitivity matrix leads to inference failures. Critical failure times for\nspecies inference in the Robertson and hydrogen autoignition systems are\nanalyzed, providing insights into the limits of inference reliability and its\nphysical significance.",
        "Backdoor attacks embed hidden functionalities in deep neural networks (DNN),\ntriggering malicious behavior with specific inputs. Advanced defenses monitor\nanomalous DNN inferences to detect such attacks. However, concealed backdoors\nevade detection by maintaining a low pre-deployment attack success rate (ASR)\nand restoring high ASR post-deployment via machine unlearning. Existing\nconcealed backdoors are often constrained by requiring white-box or black-box\naccess or auxiliary data, limiting their practicality when such access or data\nis unavailable. This paper introduces ReVeil, a concealed backdoor attack\ntargeting the data collection phase of the DNN training pipeline, requiring no\nmodel access or auxiliary data. ReVeil maintains low pre-deployment ASR across\nfour datasets and four trigger patterns, successfully evades three popular\nbackdoor detection methods, and restores high ASR post-deployment through\nmachine unlearning.",
        "In AI-facilitated teaching, leveraging various query styles to interpret\nabstract text descriptions is crucial for ensuring high-quality teaching.\nHowever, current retrieval models primarily focus on natural text-image\nretrieval, making them insufficiently tailored to educational scenarios due to\nthe ambiguities in the retrieval process. In this paper, we propose a diverse\nexpression retrieval task tailored to educational scenarios, supporting\nretrieval based on multiple query styles and expressions. We introduce the STEM\nEducation Retrieval Dataset (SER), which contains over 24,000 query pairs of\ndifferent styles, and the Uni-Retrieval, an efficient and style-diversified\nretrieval vision-language model based on prompt tuning. Uni-Retrieval extracts\nquery style features as prototypes and builds a continuously updated Prompt\nBank containing prompt tokens for diverse queries. This bank can updated during\ntest time to represent domain-specific knowledge for different subject\nretrieval scenarios. Our framework demonstrates scalability and robustness by\ndynamically retrieving prompt tokens based on prototype similarity, effectively\nfacilitating learning for unknown queries. Experimental results indicate that\nUni-Retrieval outperforms existing retrieval models in most retrieval tasks.\nThis advancement provides a scalable and precise solution for diverse\neducational needs.",
        "Some frustrated magnets exhibit a huge hysteresis called \"global hysteresis\n(GH)\", where the magnetic plateaus appearing in the increasing field process\nare skipped in the decreasing field process from the high magnetic field state.\nIn this paper, we focused on the frustrated magnet DyRu2Si2 and measured\nmagnetization relaxations from two plateau states inside the GH loop, the\nphases III and IV, and investigated the phase transitions into them. As a\nresult of the relaxation measurements, no relaxation is observed in the phase\nIII, whereas long-time relaxations of more than 105 sec are observed at the\nphase IV plateau. Moreover, a Mpemba-effect-like relaxation phenomenon where\nthe relaxation from an initial state prepared in the zero-field-cooled\ncondition overtakes that from an initial state prepared in the field-cooled\ncondition is observed. These results indicate that the phase IV is the\nnon-ergodic state with a complex free-energy landscape with multiple local\nminima, while the phase III has a simple free energy structure. Therefore, the\nIII-IV phase transition is considered to be the ergodic to non-ergodic phase\ntransition. Although this type of phase transition typically occurs in random\nglassy systems, the phase IV in DyRu2Si2 has a regular long-range ordered\nmagnetic structure and yet exhibits non-ergodic properties, which is highly\nnontrivial. Our findings open the possibility of observing non-ergodic states\nin frustrated magnets with regular long-range orders.",
        "The paper explores the challenges of regression analysis in evolving data\nstreams, an area that remains relatively underexplored compared to\nclassification. We propose a standardized evaluation process for regression and\nprediction interval tasks in streaming contexts. Additionally, we introduce an\ninnovative drift simulation strategy capable of synthesizing various drift\ntypes, including the less-studied incremental drift. Comprehensive experiments\nwith state-of-the-art methods, conducted under the proposed process, validate\nthe effectiveness and robustness of our approach.",
        "Moir\\'{e} systems such as magic-angle twisted bilayer graphene have attracted\nsignificant attention due to their ability to host correlated phenomena\nincluding superconductivity and strongly correlated insulating states. By\ndefining the single-particle Green's function in the band basis, we\nsystematically develop a many-body perturbation theory framework to address\ncorrelations beyond the usual mean-field Hartree-Fock approaches. As a specific\nexample, we first analyze twisted bilayer graphene within the Hartree-Fock\napproximation. We derive analytical solutions for symmetry-breaking states at\ninteger fillings and the finite-temperature metal-insulator transition that\nclosely match previously known numerical results in the literature. Moving\nbeyond Hartree-Fock, we incorporate self-consistent GW corrections\ndemonstrating that first-order diagrams significantly overestimate the\nfilling-dependent fluctuations in the electronic compressibility. This\nframework provides a comprehensive pathway for exploring strong electronic\ncorrelations in moir\\'{e} systems beyond mean-field, giving new insights into\nthe interplay of symmetry breaking and electron correlations.",
        "A universal prediction of quantum gravity is that the dynamics of general\nrelativity is augmented by interactions that are of higher order in the\nspacetime curvature. Numerical explorations indicate that such terms may have a\ndrastic impact on black hole-type solutions by modifying the geometry close to\nthe would-be event horizon in a substantial way. In this work, we perform the\nfirst systematic investigation of this blow-up mechanism within general\nrelativity supplemented by quadratic gravity terms, the Goroff-Sagnotti\ncounterterm, the combination of the two, and Einstein-Cubic Gravity. By\nstudying linear perturbations of the Schwarzschild solution close to the\nSchwarzschild radius, we discover the following picture: the higher-derivative\nterms giving rise to extra degrees of freedom play a distinguished role. Once\ncouplings associated with these terms enter the solution in the asymptotically\nflat region, a blow-up mechanism removes the event horizon and one deals with\neither a naked singularity or a wormhole. We believe that this finding is\nhighly relevant when constraining the coefficients appearing in the Wilsonian\ndescription of gravity by observations.",
        "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications.",
        "As a continuation of our earlier paper, we offer a new approach to\nmurmurations and Sato-Tate laws for higher rank zetas of elliptic curves. Our\napproach here does not depend on the Riemann hypothesis for the so-called\na-invariant in rank n>2 even for the Sato-Tate law, rather, on a much refined\nstructure, a similar version of which was already observed by Zagier and the\nsenior author when the rank n Riemann hypothesis was established. Namely,\ninstead of the rank n Riemann hypothesis bounds, we use much stronger\nasymptotic bounds. Accordingly, rank n Sato-Tate law can be established and\nrank n murmuration can be formulated equally well, similar to the corresponding\nstructures in the abelian framework for Artin zetas of elliptic curves.",
        "Colored gravity, based on $\\text{U}(1,3)$ symmetry, emerges naturally in the\ncomplexification of Lorentzian manifolds and integrates U(1) electromagnetism\nas a subcase. This work explores the viability of also including strong and\nelectroweak interactions under the $\\text{U}(1,3)$ gauge group of colored\ngravity. We identify specific generators linked to leptonic and quark\ninteractions and embed the standard Higgs mechanism. Crucially, the weak mixing\nangle ($\\sin^2\\theta_W$) is predicted to exhibit about $\\sim0.231$ for\nlepton-lepton interactions (close to observations) and $\\sim0.222$ for\nhadron-lepton interactions, which is in 3$\\sigma$ tension with some\nobservations. These findings open pathways for reconciling experimental data\nwith colored gravity and suggest avenues for quantum correction studies.",
        "Using as a narrative theme the example of Darwin's finches, a microscopic\nagent-based model is introduces to study sympatric speciation as a result of\ncompetition for resources in the same ecological niche. Varying competition\namong individuals and resource distribution, the model exhibits some of the\nmain features of evolutionary branching processing. The model can be extended\nto include spatial effects, different genetic loci, sexual mating and\nrecombination, etc.",
        "In real-world applications where computational resources are limited,\neffectively integrating visual and textual information for Visual Question\nAnswering (VQA) presents significant challenges. This paper investigates the\nperformance of traditional models under computational constraints, focusing on\nenhancing VQA performance, particularly for numerical and counting questions.\nWe evaluate models based on Bidirectional GRU (BidGRU), GRU, Bidirectional LSTM\n(BidLSTM), and Convolutional Neural Networks (CNN), analyzing the impact of\ndifferent vocabulary sizes, fine-tuning strategies, and embedding dimensions.\nExperimental results show that the BidGRU model with an embedding dimension of\n300 and a vocabulary size of 3000 achieves the best overall performance without\nthe computational overhead of larger models. Ablation studies emphasize the\nimportance of attention mechanisms and counting information in handling complex\nreasoning tasks under resource limitations. Our research provides valuable\ninsights for developing more efficient VQA models suitable for deployment in\nenvironments with limited computational capacity.",
        "Compression techniques such as pruning and quantization offer a solution for\nmore efficient deployment of language models (LMs), albeit with small\nperformance drops in benchmark performance. However, general-purpose LM\ncompression methods can negatively affect performance in specialized domains\n(e.g. biomedical or legal). Recent work has sought to address this, yet\nrequires computationally expensive full-parameter fine-tuning. To this end, we\npropose cross-calibration, a novel training-free approach for improving the\ndomain performance of compressed LMs. Our approach effectively leverages\nHessian-based sensitivity to identify weights that are influential for both\nin-domain and general performance. Through extensive experimentation, we\ndemonstrate that cross-calibration substantially outperforms existing\napproaches on domain-specific tasks, without compromising general performance.\nNotably, these gains come without additional computational overhead, displaying\nremarkable potential towards extracting domain-specialized compressed models\nfrom general-purpose LMs.",
        "In this paper, we propose a novel framework for efficiently and accurately\nestimating Lipschitz constants in hybrid quantum-classical decision models. Our\napproach integrates classical neural network with quantum variational circuits\nto address critical issues in learning theory such as fairness verification,\nrobust training, and generalization.\n  By a unified convex optimization formulation, we extend existing classical\nmethods to capture the interplay between classical and quantum layers. This\nintegrated strategy not only provide a tight bound on the Lipschitz constant\nbut also improves computational efficiency with respect to the previous\nmethods.",
        "Let $G$ be a connected reductive group over a $p$-adic local field $F$.\nR\\'emy-Thuillier-Werner constructed embeddings of the (reduced) Bruhat-Tits\nbuilding $\\mathcal{B}(G,F)$ into the Berkovich spaces associated to suitable\nflag varieties of $G$, generalizing the work of Berkovich in split case. They\ndefined compactifications of $\\mathcal{B}(G,F)$ by taking closure inside these\nBerkovich flag varieties. We show that, in the setting of a basic local Shimura\ndatum, the R\\'emy-Thuillier-Werner embedding factors through the associated\n$p$-adic Hodge-Tate period domain. Moreover, we compare the boundaries of the\nBerkovich compactification of $\\mathcal{B}(G,F)$ with non basic Newton strata.\nIn the case of $\\mathrm{GL}_n$ and the cocharacter $\\mu=(1^d, 0^{n-d})$ for an\ninteger $d$ which is coprime to $n$, we further construct a continuous\nretraction map from the $p$-adic period domain to the building. This reveals\nnew information on these $p$-adic period domains, which share many similarities\nwith the Drinfeld spaces.",
        "In this note we consider the big algebra recently introduced by Hausel for\nthe $\\mathrm{GL}_n$-action on the coordinate ring of the matrix space\n$\\operatorname{Mat}(n,r)$. In particular, we obtain explicit formulas for the\nbig algebra generators in terms of differential operators with polynomial\ncoefficients. We show that big algebras in type $A$ are commutative and relate\nthem to the Bethe subalgebra in the Yangian $\\mathrm{Y}(\\mathfrak{gl}_{n})$. We\napply these results to big algebras of symmetric powers of the standard\nrepresentation of $\\mathrm{GL}_n$.",
        "High harmonic generation from Pr$_{0.8}$Ca$_{0.2}$MnO$_{3}$ was investigated\nacross a high-temperature paramagnetic phase and a low-temperature\nferromagnetic phase. As the temperature decreases, the harmonic intensity\ngradually increases in the paramagnetic phase like that in different\ncomposition material Pr$_{0.6}$Ca$_{0.4}$MnO$_{3}$. However, it turns to a\ndecrease in the ferromagnetic phase. We propose a possible interpretation of\nthe anomaly around the ferromagnetic transition temperature considering the\nthermal fluctuation of orbital order and the metal-insulator phase separation\nin the ferromagnetic insulating phase.",
        "The Consensual Assessment Technique (CAT) evaluates creativity through\nholistic expert judgments. We investigate the use of two advanced Large\nLanguage Models (LLMs), Claude-3-Opus and GPT-4o, to evaluate poetry by a\nmethodology inspired by the CAT. Using a dataset of 90 poems, we found that\nthese LLMs can surpass the results achieved by non-expert human judges at\nmatching a ground truth based on publication venue, particularly when assessing\nsmaller subsets of poems. Claude-3-Opus exhibited slightly superior performance\nthan GPT-4o. We show that LLMs are viable tools for accurately assessing\npoetry, paving the way for their broader application into other creative\ndomains.",
        "By means of Gross-Pitaevskii theory we investigate the possibility of wetting\nphase transition in three-component Bose-Einstein condensates within the strong\nsegregation between components 1 and 2. Third component plays the role of a\nsurfactant, which can wet the interface formed by components 1 and 2. The case\nof symmetry of components 1 and 2, the equation for nucleation line and wetting\nphase diagram are studied. By linearizing the set of three-coupled\nGross-Pitaevskii equations, the wave functions of the components are found. An\nanalytical approximation for the surfactant thickness in the prewetting phase\nis presented. The results indicate that the surfactant thickness in the\nprewtting phase varies linearly with the chemical potential ratio in the\nlogarithmic scale.",
        "Bayesian optimization is a widely used method for optimizing expensive\nblack-box functions, with Expected Improvement being one of the most commonly\nused acquisition functions. In contrast, information-theoretic acquisition\nfunctions aim to reduce uncertainty about the function's optimum and are often\nconsidered fundamentally distinct from EI. In this work, we challenge this\nprevailing perspective by introducing a unified theoretical framework,\nVariational Entropy Search, which reveals that EI and information-theoretic\nacquisition functions are more closely related than previously recognized. We\ndemonstrate that EI can be interpreted as a variational inference approximation\nof the popular information-theoretic acquisition function, named Max-value\nEntropy Search. Building on this insight, we propose VES-Gamma, a novel\nacquisition function that balances the strengths of EI and MES. Extensive\nempirical evaluations across both low- and high-dimensional synthetic and\nreal-world benchmarks demonstrate that VES-Gamma is competitive with\nstate-of-the-art acquisition functions and in many cases outperforms EI and\nMES."
      ]
    }
  },
  {
    "id":2412.00252,
    "research_type":"basic",
    "start_id":"b15",
    "start_title":"Analytic perturbation theory for matrices and operators",
    "start_abstract":"Perturbation theory is the study of the behavior of mathematical objects under the influence of perturbations. It is not a well-defined mathematical topic with specific objects and methods, but rather a method of investigation. In this book, perturbation theory will be developed for linear operators. First, inter-est focuses on the properties of spectral objects, such as eigenvalues, eigenprojections, eigenvectors and Jordan vectors, under perturbation of the underlying operator. This study encompasses some difficult problems. On the one hand, variations of the spectral objects need to be calculated quantitatively. The spectral objects are assumed known for the unperturbed operator, the determination (or at least the approximation) of the spectral objects for the perturbed operator is at issue. This is the starting point for the perturbation theory of L. RAYLEIGH [1] (see also R. COURANT and D. HTT.BEBT [1, p. 296 sqq]). On the other hand, the spectral objects often undergo abrupt qualitative changes, even in the case of small perturbations. These changes cause significant complications. Usually, the behavior of the spectral objects depends strongly on the assumptions about the nature of the perturbation. For example, one can assume continuity, differ-entiability, smoothness (i.e. arbitrary differentiability) or analyticity. In the following discussion, only the case of analytic (holomorphic) perturbations will be investigated, even if this strong restriction is applied, the problems remain difficult enough. On the one hand, solving problems of perturbation theory is of conceptual interest. The study of intrinsic spectral properties of a linear operator undergoing perturbation leads to deeper insights and understanding of the structure of the operator. It also leads to the development of new tools for further investigations. On the other hand, applications (inside and outside of mathematics) lead to new questions in perturbation theory. One of the first calculations of perturbation theory was given by L. Rayleigh, who determined the eigenfrequencies and eigenmodes of an oscillating string, fixed at x = 0 and x = n, whose elasticity modulus is constant and whose mass density Q(X) has only a small deviation from a constant value for all x, 0 ^ x 5S n. (That is, the density o(x) is of the form Q(X) = p0 + ea(x), where a(x) is a given function and where e is a small perturbation parameter.) Actually, as this example indicates, the starting point for the development of per-turbation theory was the study of perturbations of spectral objects (eigenvalues and eigenvectors) for concrete classes of operators, for example, Fredholm integral operators or Sturm-Liouville differential operators (for example, see L. LIECHTENSTEIN [1]).",
    "start_categories":[
      "Analytic Perturbation Theory "
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Localization and landscape functions for graph laplacians"
      ],
      "abstract":[
        "We discuss explicit landscape functions for quantum graphs. By a 'landscape function' Upsilon(x) we mean a function that controls the localization properties of normalized eigenfunctions psi(x) through a pointwise inequality of the form |psi(x)| le Upsilon(x). The ideal Upsilon is a function that a) responds to the potential energy V(x) and to the structure of the graph in some formulaic way; b) is small in examples where eigenfunctions are suppressed by the tunneling effect, and c) relatively large in regions where eigenfunctions may - or may not - be concentrated, as observed in specific examples. It turns out that the connectedness of a graph can present a barrier to the existence of universal landscape functions in the high-energy r\u00e9gime, as we show with simple examples. We therefore apply different methods in different r\u00e9gimes determined by the values of the potential energy V(x) and the eigenvalue parameter E."
      ],
      "categories":[
        "Anderson Localization"
      ]
    },
    "list":{
      "title":[
        "GSVC: Efficient Video Representation and Compression Through 2D Gaussian\n  Splatting",
        "On finite approximations of transitive graphs",
        "Solvable Dynamics of Self-Supervised Word Embeddings and the Emergence\n  of Analogical Reasoning",
        "How to Tune a Multilingual Encoder Model for Germanic Languages: A Study\n  of PEFT, Full Fine-Tuning, and Language Adapters",
        "Dynamic realization of emergent high-dimensional optical vortices",
        "Rigidity results for free boundary hypersurfaces in initial data sets\n  with boundary",
        "Int2Int: a framework for mathematics with transformers",
        "Eliciting Language Model Behaviors with Investigator Agents",
        "Applying the Liouville-Lanczos Method of Time-Dependent\n  Density-Functional Theory to Warm Dense Matter",
        "TigerVector: Supporting Vector Search in Graph Databases for Advanced\n  RAGs",
        "qReduMIS: A Quantum-Informed Reduction Algorithm for the Maximum\n  Independent Set Problem",
        "Algebra and geometry of ASM weak order",
        "BPS invariants from framed links",
        "Overcoming Semantic Dilution in Transformer-Based Next Frame Prediction",
        "Non-convergence to the optimal risk for Adam and stochastic gradient\n  descent optimization in the training of deep neural networks",
        "On the Role of Individual Differences in Current Approaches to\n  Computational Image Aesthetics",
        "UAV-Enabled IoT Networks: A SWIPT Energy Harvesting Architecture with\n  Relay Support for Disaster Response",
        "JWST's little red dots: an emerging population of young, low-mass AGN\n  cocooned in dense ionized gas",
        "Understanding the Quality-Diversity Trade-off in Diffusion Language\n  Models",
        "Advanced Weakly-Supervised Formula Exploration for Neuro-Symbolic\n  Mathematical Reasoning",
        "Cooperative Behavior in Pre-State Societies: An Agent-Based Approach of\n  the Aksum Civilization",
        "Improved SED-Fitting Assumptions Result in Inside-Out Quenching at\n  $z\\sim0.5$ and Quenching at All Radii Simultaneously at $z\\sim1$",
        "Displacement-Sparse Neural Optimal Transport",
        "Quantifying Correlations of Machine Learning Models",
        "Machine learning the vanishing order of rational L-functions",
        "Fairness Aware Reinforcement Learning via Proximal Policy Optimization",
        "Gravitational Effects of a Small Primordial Black Hole Passing Through\n  the Human Body",
        "An Ontology for Social Determinants of Education (SDoEd) based on\n  Human-AI Collaborative Approach",
        "Benchmarking MedMNIST dataset on real quantum hardware"
      ],
      "abstract":[
        "3D Gaussian splats have emerged as a revolutionary, effective, learned\nrepresentation for static 3D scenes. In this work, we explore using 2D Gaussian\nsplats as a new primitive for representing videos. We propose GSVC, an approach\nto learning a set of 2D Gaussian splats that can effectively represent and\ncompress video frames. GSVC incorporates the following techniques: (i) To\nexploit temporal redundancy among adjacent frames, which can speed up training\nand improve the compression efficiency, we predict the Gaussian splats of a\nframe based on its previous frame; (ii) To control the trade-offs between file\nsize and quality, we remove Gaussian splats with low contribution to the video\nquality; (iii) To capture dynamics in videos, we randomly add Gaussian splats\nto fit content with large motion or newly-appeared objects; (iv) To handle\nsignificant changes in the scene, we detect key frames based on loss\ndifferences during the learning process. Experiment results show that GSVC\nachieves good rate-distortion trade-offs, comparable to state-of-the-art video\ncodecs such as AV1 and VVC, and a rendering speed of 1500 fps for a 1920x1080\nvideo.",
        "In this note we answer a question of Johannes Carmesin, which was circulated\nat the Oberwolfach Workshop on \"Graph Theory\" in January 2025. We provide a\nunimodular, locally finite, and vertex-transitive graph without any perfect\nfinite $r$-local model for $r \\in \\mathbb N$ large enough.",
        "The remarkable success of large language models relies on their ability to\nimplicitly learn structured latent representations from the pretraining corpus.\nAs a simpler surrogate for representation learning in language modeling, we\nstudy a class of solvable contrastive self-supervised algorithms which we term\nquadratic word embedding models. These models resemble the word2vec algorithm\nand perform similarly on downstream tasks. Our main contributions are\nanalytical solutions for both the training dynamics (under certain\nhyperparameter choices) and the final word embeddings, given in terms of only\nthe corpus statistics. Our solutions reveal that these models learn orthogonal\nlinear subspaces one at a time, each one incrementing the effective rank of the\nembeddings until model capacity is saturated. Training on WikiText, we find\nthat the top subspaces represent interpretable concepts. Finally, we use our\ndynamical theory to predict how and when models acquire the ability to complete\nanalogies.",
        "This paper investigates the optimal use of the multilingual encoder model\nmDeBERTa for tasks in three Germanic languages -- German, Swedish, and\nIcelandic -- representing varying levels of presence and likely data quality in\nmDeBERTas pre-training data. We compare full fine-tuning with the\nparameter-efficient fine-tuning (PEFT) methods LoRA and Pfeiffer bottleneck\nadapters, finding that PEFT is more effective for the higher-resource language,\nGerman. However, results for Swedish and Icelandic are less consistent. We also\nobserve differences between tasks: While PEFT tends to work better for question\nanswering, full fine-tuning is preferable for named entity recognition.\nInspired by previous research on modular approaches that combine task and\nlanguage adapters, we evaluate the impact of adding PEFT modules trained on\nunstructured text, finding that this approach is not beneficial.",
        "The dimensionality of vortical structures has recently been extended beyond\ntwo dimensions, providing higher-order topological characteristics and\nrobustness for high-capacity information processing and turbulence control. The\ngeneration of high-dimensional vortical structures has mostly been demonstrated\nin classical systems through the complex interference of fluidic, acoustic, or\nelectromagnetic waves. However, natural materials rarely support three- or\nhigher-dimensional vortical structures and their physical interactions. Here,\nwe present a high-dimensional gradient thickness optical cavity (GTOC) in which\nthe optical coupling of planar metal-dielectric multilayers implements\ntopological interactions across multiple dimensions. Topological interactions\nin high-dimensional GTOC construct non-trivial topological phases, which induce\nhigh-dimensional vortical structures in generalized parameter space in three,\nfour dimensions, and beyond. These emergent high-dimensional vortical\nstructures are observed under electro-optic tomography as optical vortex\ndynamics in two-dimensional real-space, employing the optical thicknesses of\nthe dielectric layers as synthetic dimensions. We experimentally demonstrate\nemergent vortical structures, optical vortex lines and vortex rings, in a\nthree-dimensional generalized parameter space and their topological\ntransitions. Furthermore, we explore four-dimensional vortical structures,\ntermed optical vortex sheets, which provide the programmability of real-space\noptical vortex dynamics. Our findings hold significant promise for emulating\nhigh-dimensional physics and developing active topological photonic devices.",
        "In this work, we present several rigidity results for compact free boundary\nhypersurfaces in initial data sets with boundary. Specifically, in the first\npart of the paper, we extend the local splitting theorems from [G. J. Galloway\nand H. C. Jang, Some scalar curvature warped product splitting theorems, Proc.\nAm. Math. Soc. 148 (2020), no. 6, 2617-2629] to the setting of manifolds with\nboundary. To achieve this, we build on the approach of the original paper,\nutilizing results on free boundary marginally outer trapped surfaces (MOTS)\napplied to specific initial data sets. In the second part, we extend the main\nresults from [A. Barros and C. Cruz, Free boundary hypersurfaces with\nnon-positive Yamabe invariant in mean convex manifolds, J. Geom. Anal. 30\n(2020), no. 4, 3542-3562] to the context of free boundary MOTS in initial data\nsets with boundary.",
        "This paper documents Int2Int, an open source code base for using transformers\non problems of mathematical research, with a focus on number theory and other\nproblems involving integers. Int2Int is a complete PyTorch implementation of a\ntransformer architecture, together with training and evaluation loops, and\nclasses and functions to represent, generate and decode common mathematical\nobjects. Ancillary code for data preparation, and Jupyter Notebooks for\nvisualizing experimental results are also provided. This document presents the\nmain features of Int2Int, serves as its user manual, and provides guidelines on\nhow to extend it. Int2Int is released under the MIT licence, at\nhttps:\/\/github.com\/FacebookResearch\/Int2Int.",
        "Language models exhibit complex, diverse behaviors when prompted with\nfree-form text, making it difficult to characterize the space of possible\noutputs. We study the problem of behavior elicitation, where the goal is to\nsearch for prompts that induce specific target behaviors (e.g., hallucinations\nor harmful responses) from a target language model. To navigate the\nexponentially large space of possible prompts, we train investigator models to\nmap randomly-chosen target behaviors to a diverse distribution of outputs that\nelicit them, similar to amortized Bayesian inference. We do this through\nsupervised fine-tuning, reinforcement learning via DPO, and a novel Frank-Wolfe\ntraining objective to iteratively discover diverse prompting strategies. Our\ninvestigator models surface a variety of effective and human-interpretable\nprompts leading to jailbreaks, hallucinations, and open-ended aberrant\nbehaviors, obtaining a 100% attack success rate on a subset of AdvBench\n(Harmful Behaviors) and an 85% hallucination rate.",
        "Ab initio modeling of dynamic structure factors (DSF) and related density\nresponse properties in the warm dense matter (WDM) regime is a challenging\ncomputational task. The DSF, convolved with a probing X-ray beam and instrument\nfunction, is measured in X-ray Thomson scattering (XRTS) experiments, which\nallows for the study of electronic structure properties at the microscopic\nlevel. Among the various ab initio methods, linear response time-dependent\ndensity functional theory (LR-TDDFT) is a key framework for simulating the DSF.\nThe standard approach in LR-TDDFT for computing the DSF relies on the orbital\nrepresentation. A significant drawback of this method is the unfavorable\nscaling of the number of required empty bands as the wavenumber increases,\nmaking LR-TDDFT impractical for modeling XRTS measurements over large energy\nscales, such as in backward scattering geometry. We consider and test an\nalternative approach that employs the Liouville-Lanczos (LL) method for\nsimulating the DSF. This approach does not require empty states and allows the\nDSF at large momentum transfer values and over a broad frequency range to be\naccessed. We compare the results obtained from the LL method with those from\nthe standard LR-TDDFT within the projector augmented-wave formalism for\nisochorically heated aluminum and warm dense hydrogen. Additionally, we utilize\nexact path integral Monte Carlo (PIMC) results for the imaginary-time\ndensity-density correlation function (ITCF) of warm dense hydrogen to\nrigorously benchmark the LL approach. We discuss the application of the LL\nmethod for calculating DSFs and ITCFs at different wavenumbers, the effects of\npseudopotentials, and the role of Lorentzian smearing. The successful\nvalidation of the LL method under WDM conditions makes it a valuable addition\nto the ab initio simulation landscape, supporting experimental efforts and\nadvancing WDM theory.",
        "In this paper, we introduce TigerVector, a system that integrates vector\nsearch and graph query within TigerGraph, a Massively Parallel Processing (MPP)\nnative graph database. We extend the vertex attribute type with the embedding\ntype. To support fast vector search, we devise an MPP index framework that\ninteroperates efficiently with the graph engine. The graph query language GSQL\nis enhanced to support vector type expressions and enable query compositions\nbetween vector search results and graph query blocks. These advancements\nelevate the expressive power and analytical capabilities of graph databases,\nenabling seamless fusion of unstructured and structured data in ways previously\nunattainable. Through extensive experiments, we demonstrate TigerVector's\nhybrid search capability, scalability, and superior performance compared to\nother graph databases (including Neo4j and Amazon Neptune) and a highly\noptimized specialized vector database (Milvus). TigerVector was integrated into\nTigerGraph v4.2, the latest release of TigerGraph, in December 2024.",
        "We propose and implement a quantum-informed reduction algorithm for the\nmaximum independent set problem that integrates classical kernelization\ntechniques with information extracted from quantum devices. Our larger\nframework consists of dedicated application, algorithm, and hardware layers,\nand easily generalizes to the maximum weight independent set problem. In this\nhybrid quantum-classical framework, which we call qReduMIS, the quantum\ncomputer is used as a co-processor to inform classical reduction logic about\nfrozen vertices that are likely (or unlikely) to be in large independent sets,\nthereby opening up the reduction space after removal of targeted subgraphs. We\nsystematically assess the performance of qReduMIS based on experiments with up\nto 231 qubits run on Rydberg quantum hardware available through Amazon Braket.\nOur experiments show that qReduMIS can help address fundamental performance\nlimitations faced by a broad set of (quantum) solvers including Rydberg quantum\ndevices. We outline implementations of qReduMIS with alternative platforms,\nsuch as superconducting qubits or trapped ions, and we discuss potential future\nextensions.",
        "Much of modern Schubert calculus is centered on Schubert varieties in the\ncomplete flag variety and on their classes in its integral cohomology ring.\nUnder the Borel isomorphism, these classes are represented by distinguished\npolynomials called Schubert polynomials, introduced by Lascoux and\nSch\\\"utzenberger.\n  Knutson and Miller showed that Schubert polynomials are multidegrees of\nmatrix Schubert varieties, affine varieties introduced by Fulton, which are\nclosely related to Schubert varieties. Many roads to studying Schubert\npolynomials pass through unions and intersections of matrix Schubert varieties.\nThe third author showed that the natural indexing objects of arbitrary\nintersections of matrix Schubert varieties are alternating sign matrices\n(ASMs). Every ASM variety is expressible as a union of matrix Schubert\nvarieties.\n  Many fundamental algebro-geometric invariants (e.g., codimension, degree, and\nCastelnuovo--Mumford regularity) are well understood combinatorially for matrix\nSchubert varieties, substantially via the combinatorics of strong Bruhat order\non $S_n$. The extension of strong order to ASM(n), the set of $n \\times n$\nASMs, has so far not borne as much algebro-geometric fruit for ASM varieties.\n  Hamaker and Reiner proposed an extension of weak Bruhat order from $S_n$ to\nASM(n), which they studied from a combinatorial perspective. In the present\npaper, we place this work on algebro-geometric footing. We use weak order on\nASMs to give a characterization of codimension of ASM varieties. We also show\nthat weak order operators commute with K-theoretic divided difference operators\nand that they satisfy the same derivative formula that facilitated the first\ngeneral combinatorial computation of Castelnuovo--Mumford regularity of matrix\nSchubert varieties. Finally, we build from these results to generalizations\nthat apply to arbitrary unions of matrix Schubert varieties.",
        "In this article, we investigate the BPS invariants associated with framed\nlinks. We extend the relationship between the algebraic curve (i.e. dual\n$A$-polynomial) and the BPS invariants of a knot investigated in \\cite{GKS} to\nthe case of a framed knot. With the help of the framing change formula for the\ndual $A$-polynomial of a framed knot, we give several explicit formulas for the\nextremal $A$-polynomials and the BPS invariants of framed knots. As to the\nframed links, we present several numerical calculations for the Ooguri-Vafa\ninvariants and BPS invariants for framed Whitehead links and Borromean rings\nand verify the integrality property for them.",
        "Next-frame prediction in videos is crucial for applications such as\nautonomous driving, object tracking, and motion prediction. The primary\nchallenge in next-frame prediction lies in effectively capturing and processing\nboth spatial and temporal information from previous video sequences. The\ntransformer architecture, known for its prowess in handling sequence data, has\nmade remarkable progress in this domain. However, transformer-based next-frame\nprediction models face notable issues: (a) The multi-head self-attention (MHSA)\nmechanism requires the input embedding to be split into $N$ chunks, where $N$\nis the number of heads. Each segment captures only a fraction of the original\nembeddings information, which distorts the representation of the embedding in\nthe latent space, resulting in a semantic dilution problem; (b) These models\npredict the embeddings of the next frames rather than the frames themselves,\nbut the loss function based on the errors of the reconstructed frames, not the\npredicted embeddings -- this creates a discrepancy between the training\nobjective and the model output. We propose a Semantic Concentration Multi-Head\nSelf-Attention (SCMHSA) architecture, which effectively mitigates semantic\ndilution in transformer-based next-frame prediction. Additionally, we introduce\na loss function that optimizes SCMHSA in the latent space, aligning the\ntraining objective more closely with the model output. Our method demonstrates\nsuperior performance compared to the original transformer-based predictors.",
        "Despite the omnipresent use of stochastic gradient descent (SGD) optimization\nmethods in the training of deep neural networks (DNNs), it remains, in\nbasically all practically relevant scenarios, a fundamental open problem to\nprovide a rigorous theoretical explanation for the success (and the\nlimitations) of SGD optimization methods in deep learning. In particular, it\nremains an open question to prove or disprove convergence of the true risk of\nSGD optimization methods to the optimal true risk value in the training of\nDNNs. In one of the main results of this work we reveal for a general class of\nactivations, loss functions, random initializations, and SGD optimization\nmethods (including, for example, standard SGD, momentum SGD, Nesterov\naccelerated SGD, Adagrad, RMSprop, Adadelta, Adam, Adamax, Nadam, Nadamax, and\nAMSGrad) that in the training of any arbitrary fully-connected feedforward DNN\nit does not hold that the true risk of the considered optimizer converges in\nprobability to the optimal true risk value. Nonetheless, the true risk of the\nconsidered SGD optimization method may very well converge to a strictly\nsuboptimal true risk value.",
        "Image aesthetic assessment (IAA) evaluates image aesthetics, a task\ncomplicated by image diversity and user subjectivity. Current approaches\naddress this in two stages: Generic IAA (GIAA) models estimate mean aesthetic\nscores, while Personal IAA (PIAA) models adapt GIAA using transfer learning to\nincorporate user subjectivity. However, a theoretical understanding of transfer\nlearning between GIAA and PIAA, particularly concerning the impact of group\ncomposition, group size, aesthetic differences between groups and individuals,\nand demographic correlations, is lacking. This work establishes a theoretical\nfoundation for IAA, proposing a unified model that encodes individual\ncharacteristics in a distributional format for both individual and group\nassessments. We show that transferring from GIAA to PIAA involves\nextrapolation, while the reverse involves interpolation, which is generally\nmore effective for machine learning. Experiments with varying group\ncompositions, including sub-sampling by group size and disjoint demographics,\nreveal significant performance variation even for GIAA, indicating that mean\nscores do not fully eliminate individual subjectivity. Performance variations\nand Gini index analysis reveal education as the primary factor influencing\naesthetic differences, followed by photography and art experience, with\nstronger individual subjectivity observed in artworks than in photos. Our model\nuniquely supports both GIAA and PIAA, enhancing generalization across\ndemographics.",
        "Due to the wide application of unmanned aerial vehicles (UAVs) as relays to\nestablish Disaster Response Networks (DRNs), an effective model of energy\nharvesting (EH) and energy consumption for the UAV-aided Disaster Response\nNetwork (DRN) is rising to be a challenging issue. This is mainly manifest in\nInternet of Things (IoT) scenarios where multiple users are looking to\ncommunicate with the UAV. In this paper, the possibility of connecting an UAV\nwith several users is investigated where the UAV as a relay receives data from\na DRN and delivers to another network considering two IoT scenarios. The first\nscenario represents a conventional method with limited UAV energy where low\ncommunication rates and inadequate service coverage for all users are\nchallenges. But in the second scenario, a Simultaneous Wireless Information and\nPower Transmission (SWIPT) technique is used to serve users. Considering\npotential limitations in transmission energy of users within disaster networks,\nthe SWIPT technique is applied to maximize energy acquisition by the UAV,\nleading to improve the efficiency of the investigated scenario. Finally, the\nrequired energy of the UAV to serve the largest number of users in the shortest\npossible time is clarified. Furthermore, by Considering the relationship\nbetween energy and UAV flight time and defining the UAV flight time\noptimization problem, optimal network parameters are obtained. Simulation\nresults show the effectiveness of the proposed scenario.",
        "JWST has uncovered large numbers of compact galaxies at high redshift with\nbroad hydrogen\/helium lines. These include the enigmatic population known as\n\"little red dots\" (LRDs). Their nature is debated, but they are thought to be\npowered by supermassive black holes (SMBHs) or intense star formation. They\nexhibit unusual properties for SMBHs, such as black holes that are overmassive\nfor their host galaxies and extremely weak X-ray and radio emission. Using the\nhighest-quality JWST spectra, we show here that the lines are broadened by\nelectron scattering with a narrow intrinsic line core. The data require high\nelectron column densities and compact sizes (light days), which, when coupled\nwith their high luminosities can only be explained by SMBH accretion. The\nnarrow intrinsic cores of the lines imply upper limits on the black hole masses\nof $10^{5-7}$ $M_{\\odot}$, two orders of magnitude lower than previous\nestimates. These are among the lowest mass SMBHs known at high redshift and\nsuggest that this is a population of young, rapidly growing SMBHs. They are\nenshrouded in a dense cocoon of ionized gas, probably related to their youth,\nfrom which they are accreting close to the Eddington limit. Reprocessed nebular\nemission from the dense cocoon dominates the optical spectrum, explaining most\nLRD spectral characteristics and helping to suppress radio and X-ray emission.",
        "Diffusion models have seen immense success in modelling continuous data\nacross a range of domains such as vision and audio. Despite the challenges of\nadapting diffusion models to discrete data, recent work explores their\napplication to text generation by working in the continuous embedding space.\nHowever, these models lack a natural means to control the inherent trade-off\nbetween quality and diversity as afforded by the temperature hyperparameter in\nautoregressive models, hindering understanding of model performance and\nrestricting generation quality. This work proposes the use of classifier-free\nguidance and stochastic clamping for manipulating the quality-diversity\ntrade-off on sequence-to-sequence tasks, demonstrating that these techniques\nmay be used to improve the performance of a diffusion language model.",
        "In recent years, neuro-symbolic methods have become a popular and powerful\napproach that augments artificial intelligence systems with the capability to\nperform abstract, logical, and quantitative deductions with enhanced precision\nand controllability. Recent studies successfully performed symbolic reasoning\nby leveraging various machine learning models to explicitly or implicitly\npredict intermediate labels that provide symbolic instructions. However, these\nintermediate labels are not always prepared for every task as a part of\ntraining data, and pre-trained models, represented by Large Language Models\n(LLMs), also do not consistently generate valid symbolic instructions with\ntheir intrinsic knowledge. On the other hand, existing work developed\nalternative learning techniques that allow the learning system to autonomously\nuncover optimal symbolic instructions. Nevertheless, their performance also\nexhibits limitations when faced with relatively huge search spaces or more\nchallenging reasoning problems. In view of this, in this work, we put forward\nan advanced practice for neuro-symbolic reasoning systems to explore the\nintermediate labels with weak supervision from problem inputs and final\noutputs. Our experiments on the Mathematics dataset illustrated the\neffectiveness of our proposals from multiple aspects.",
        "This study intends to test the hypothesis that, contrary to traditional\ninterpretation, the social structure of the polity of Aksum - especially in its\nearly stages - was not characterized by a vertical hierarchy with highly\ncentralized administrative power, and that the leaders mentioned in the few\navailable inscriptions were predominantly ritual leaders with religious rather\nthan coercive political authority. This hypothesis, suggested by the available\narchaeological evidence, is grounded in Charles Stanish's model, which posits\nthat pre-state societies could achieve cooperative behavior without the\npresence of coercive authority. Using agent-based modeling applied to data\ninspired by the Aksum civilization, we examine the dynamics of cooperation in\nthe presence and absence of a Public Goods Game. Results show that while\ncooperative behavior can emerge in the short term without coercive power, it\nmay not be sustainable over the long term, suggesting a need for centralized\nauthority to foster stable, complex societies. These findings provide insights\ninto the evolutionary pathways that lead to state formation and complex social\nstructures.",
        "Many studies conclude that galaxies quench from the inside-out by examining\nprofiles of specific star-formation rate (sSFR). These are usually measured by\nfitting spectral energy distributions (SEDs) assuming a fixed dust law and\nuniform priors on all parameters. Here, we examine the effects of more\nphysically motivated priors: a flexible dust law, an exponential prior on the\ndust attenuation $A_V$, and Gaussian priors that favor extended star-formation\nhistories. This results in model colors that better trace observations. We then\nperform radial SED fits to multiband flux profiles measured from Hubble Space\nTelescope images for 1,440 galaxies at $0.4<z<1.5$ of stellar masses\n$10^{10}-10^{11.5}\\ M_{\\odot}$ using both the traditional and the more\nphysically motivated assumptions. The latter results in star formation rate and\n$A_V$ profiles that agree with measurements from spectroscopy and $A_V$\nprofiles that behave correctly as a function of inclination. Since green valley\ngalaxies at $z\\sim1.3$ are expected to evolve into quiescent galaxies at\n$z\\sim0.9$, we compare their sSFR profiles using the more physically motivated\nassumptions. Their slopes are similar at all masses ($0.06 -\n0.08~\\textrm{dex}~\\textrm{kpc}^{-1}$), and the normalizations for the quiescent\ngalaxies are lower. Therefore, the sSFR profiles decline with time as quenching\noccurs at all radii simultaneously. We compare profiles of green valley\ngalaxies at $z\\sim0.9$ and quiescent galaxies at $z\\sim0.5$. The former are\nshallower at all masses by $\\sim0.1~\\textrm{dex}~\\textrm{kpc}^{-1}$. The sSFR\nprofiles steepen with time as galaxies quench from the inside-out. In summary,\nat $z\\sim0.9-1.3$, galaxies quench at all radii simultaneously, and at\n$z\\sim0.5-0.9$, they quench from the inside-out.",
        "Optimal Transport (OT) theory seeks to determine the map $T:X \\to Y$ that\ntransports a source measure $P$ to a target measure $Q$, minimizing the cost\n$c(\\mathbf{x}, T(\\mathbf{x}))$ between $\\mathbf{x}$ and its image\n$T(\\mathbf{x})$. Building upon the Input Convex Neural Network OT solver and\nincorporating the concept of displacement-sparse maps, we introduce a sparsity\npenalty into the minimax Wasserstein formulation, promote sparsity in\ndisplacement vectors $\\Delta(\\mathbf{x}) := T(\\mathbf{x}) - \\mathbf{x}$, and\nenhance the interpretability of the resulting map. However, increasing sparsity\noften reduces feasibility, causing $T_{\\#}(P)$ to deviate more significantly\nfrom the target measure. In low-dimensional settings, we propose a heuristic\nframework to balance the trade-off between sparsity and feasibility by\ndynamically adjusting the sparsity intensity parameter during training. For\nhigh-dimensional settings, we directly constrain the dimensionality of\ndisplacement vectors by enforcing $\\dim(\\Delta(\\mathbf{x})) \\leq l$, where $l <\nd$ for $X \\subseteq \\mathbb{R}^d$. Among maps satisfying this constraint, we\naim to identify the most feasible one. This goal can be effectively achieved by\nadapting our low-dimensional heuristic framework without resorting to\ndimensionality reduction. We validate our method on both synthesized sc-RNA and\nreal 4i cell perturbation datasets, demonstrating improvements over existing\nmethods.",
        "Machine Learning models are being extensively used in safety critical\napplications where errors from these models could cause harm to the user. Such\nrisks are amplified when multiple machine learning models, which are deployed\nconcurrently, interact and make errors simultaneously. This paper explores\nthree scenarios where error correlations between multiple models arise,\nresulting in such aggregated risks. Using real-world data, we simulate these\nscenarios and quantify the correlations in errors of different models. Our\nfindings indicate that aggregated risks are substantial, particularly when\nmodels share similar algorithms, training datasets, or foundational models.\nOverall, we observe that correlations across models are pervasive and likely to\nintensify with increased reliance on foundational models and widely used public\ndatasets, highlighting the need for effective mitigation strategies to address\nthese challenges.",
        "In this paper, we study the vanishing order of rational $L$-functions from a\ndata scientific perspective. Each $L$-function is represented in our data by\nfinitely many Dirichlet coefficients, the normalisation of which depends on the\ncontext. We observe murmuration-like patterns in averages across our dataset,\nfind that PCA clusters rational $L$-functions by their vanishing order, and\nrecord that LDA and neural networks may accurately predict this quantity.",
        "Fairness in multi-agent systems (MAS) focuses on equitable reward\ndistribution among agents in scenarios involving sensitive attributes such as\nrace, gender, or socioeconomic status. This paper introduces fairness in\nProximal Policy Optimization (PPO) with a penalty term derived from demographic\nparity, counterfactual fairness, and conditional statistical parity. The\nproposed method balances reward maximisation with fairness by integrating two\npenalty components: a retrospective component that minimises disparities in\npast outcomes and a prospective component that ensures fairness in future\ndecision-making. We evaluate our approach in the Allelopathic Harvest game, a\ncooperative and competitive MAS focused on resource collection, where some\nagents possess a sensitive attribute. Experiments demonstrate that fair-PPO\nachieves fairer policies across all fairness metrics than classic PPO. Fairness\ncomes at the cost of reduced rewards, namely the Price of Fairness, although\nagents with and without the sensitive attribute renounce comparable amounts of\nrewards. Additionally, the retrospective and prospective penalties effectively\nchange the agents' behaviour and improve fairness. These findings underscore\nthe potential of fair-PPO to address fairness challenges in MAS.",
        "The gravitational effects of a primordial black hole (PBH) passing through\nthe human body are examined, with the goal of determining the minimum mass\nnecessary to produce significant injury or death. Two effects are examined: the\ndamage caused by a shock wave propagating outward from the black hole\ntrajectory, and the dissociation of brain cells from tidal forces produced by\nthe black hole on its passage through the human body. It is found that the\nformer is the dominant effect, with a cutoff mass for serious injury or death\nof approximately $M_{PBH} > 1.4 \\times 10^{17} {\\rm g}$. The number density of\nprimordial black holes with a mass above this cutoff is far too small to\nproduce any observable effects on the human population.",
        "The use of computational ontologies is well-established in the field of\nMedical Informatics. The topic of Social Determinants of Health (SDoH) has also\nreceived extensive attention. Work at the intersection of ontologies and SDoH\nhas been published. However, a standardized framework for Social Determinants\nof Education (SDoEd) is lacking. In this paper, we are closing the gap by\nintroducing an SDoEd ontology for creating a precise conceptualization of the\ninterplay between life circumstances of students and their possible educational\nachievements. The ontology was developed utilizing suggestions from\nChatGPT-3.5-010422 and validated using peer-reviewed research articles. The\nfirst version of developed ontology was evaluated by human experts in the field\nof education and validated using standard ontology evaluation software. This\nversion of the SDoEd ontology contains 231 domain concepts, 10 object\nproperties, and 24 data properties",
        "Quantum machine learning (QML) has emerged as a promising domain to leverage\nthe computational capabilities of quantum systems to solve complex\nclassification tasks. In this work, we present the first comprehensive QML\nstudy by benchmarking the MedMNIST-a diverse collection of medical imaging\ndatasets on a 127-qubit real IBM quantum hardware, to evaluate the feasibility\nand performance of quantum models (without any classical neural networks) in\npractical applications. This study explores recent advancements in quantum\ncomputing such as device-aware quantum circuits, error suppression, and\nmitigation for medical image classification. Our methodology is comprised of\nthree stages: preprocessing, generation of noise-resilient and\nhardware-efficient quantum circuits, optimizing\/training of quantum circuits on\nclassical hardware, and inference on real IBM quantum hardware. Firstly, we\nprocess all input images in the preprocessing stage to reduce the spatial\ndimension due to quantum hardware limitations. We generate hardware-efficient\nquantum circuits using backend properties expressible to learn complex patterns\nfor medical image classification. After classical optimization of QML models,\nwe perform inference on real quantum hardware. We also incorporate advanced\nerror suppression and mitigation techniques in our QML workflow, including\ndynamical decoupling (DD), gate twirling, and matrix-free measurement\nmitigation (M3) to mitigate the effects of noise and improve classification\nperformance. The experimental results showcase the potential of quantum\ncomputing for medical imaging and establish a benchmark for future advancements\nin QML applied to healthcare."
      ]
    }
  },
  {
    "id":2412.00252,
    "research_type":"basic",
    "start_id":"b5",
    "start_title":"Localization and landscape functions for graph laplacians",
    "start_abstract":"We discuss explicit landscape functions for quantum graphs. By a 'landscape function' Upsilon(x) we mean a function that controls the localization properties of normalized eigenfunctions psi(x) through a pointwise inequality of the form |psi(x)| le Upsilon(x). The ideal Upsilon is a function that a) responds to the potential energy V(x) and to the structure of the graph in some formulaic way; b) is small in examples where eigenfunctions are suppressed by the tunneling effect, and c) relatively large in regions where eigenfunctions may - or may not - be concentrated, as observed in specific examples. It turns out that the connectedness of a graph can present a barrier to the existence of universal landscape functions in the high-energy r\u00e9gime, as we show with simple examples. We therefore apply different methods in different r\u00e9gimes determined by the values of the potential energy V(x) and the eigenvalue parameter E.",
    "start_categories":[
      "Anderson Localization"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Analytic perturbation theory for matrices and operators"
      ],
      "abstract":[
        "Perturbation theory is the study of the behavior of mathematical objects under the influence of perturbations. It is not a well-defined mathematical topic with specific objects and methods, but rather a method of investigation. In this book, perturbation theory will be developed for linear operators. First, inter-est focuses on the properties of spectral objects, such as eigenvalues, eigenprojections, eigenvectors and Jordan vectors, under perturbation of the underlying operator. This study encompasses some difficult problems. On the one hand, variations of the spectral objects need to be calculated quantitatively. The spectral objects are assumed known for the unperturbed operator, the determination (or at least the approximation) of the spectral objects for the perturbed operator is at issue. This is the starting point for the perturbation theory of L. RAYLEIGH [1] (see also R. COURANT and D. HTT.BEBT [1, p. 296 sqq]). On the other hand, the spectral objects often undergo abrupt qualitative changes, even in the case of small perturbations. These changes cause significant complications. Usually, the behavior of the spectral objects depends strongly on the assumptions about the nature of the perturbation. For example, one can assume continuity, differ-entiability, smoothness (i.e. arbitrary differentiability) or analyticity. In the following discussion, only the case of analytic (holomorphic) perturbations will be investigated, even if this strong restriction is applied, the problems remain difficult enough. On the one hand, solving problems of perturbation theory is of conceptual interest. The study of intrinsic spectral properties of a linear operator undergoing perturbation leads to deeper insights and understanding of the structure of the operator. It also leads to the development of new tools for further investigations. On the other hand, applications (inside and outside of mathematics) lead to new questions in perturbation theory. One of the first calculations of perturbation theory was given by L. Rayleigh, who determined the eigenfrequencies and eigenmodes of an oscillating string, fixed at x = 0 and x = n, whose elasticity modulus is constant and whose mass density Q(X) has only a small deviation from a constant value for all x, 0 ^ x 5S n. (That is, the density o(x) is of the form Q(X) = p0 + ea(x), where a(x) is a given function and where e is a small perturbation parameter.) Actually, as this example indicates, the starting point for the development of per-turbation theory was the study of perturbations of spectral objects (eigenvalues and eigenvectors) for concrete classes of operators, for example, Fredholm integral operators or Sturm-Liouville differential operators (for example, see L. LIECHTENSTEIN [1])."
      ],
      "categories":[
        "Analytic Perturbation Theory "
      ]
    },
    "list":{
      "title":[
        "GSVC: Efficient Video Representation and Compression Through 2D Gaussian\n  Splatting",
        "On finite approximations of transitive graphs",
        "Solvable Dynamics of Self-Supervised Word Embeddings and the Emergence\n  of Analogical Reasoning",
        "How to Tune a Multilingual Encoder Model for Germanic Languages: A Study\n  of PEFT, Full Fine-Tuning, and Language Adapters",
        "Dynamic realization of emergent high-dimensional optical vortices",
        "Rigidity results for free boundary hypersurfaces in initial data sets\n  with boundary",
        "Int2Int: a framework for mathematics with transformers",
        "Eliciting Language Model Behaviors with Investigator Agents",
        "Applying the Liouville-Lanczos Method of Time-Dependent\n  Density-Functional Theory to Warm Dense Matter",
        "TigerVector: Supporting Vector Search in Graph Databases for Advanced\n  RAGs",
        "qReduMIS: A Quantum-Informed Reduction Algorithm for the Maximum\n  Independent Set Problem",
        "Algebra and geometry of ASM weak order",
        "BPS invariants from framed links",
        "Overcoming Semantic Dilution in Transformer-Based Next Frame Prediction",
        "Non-convergence to the optimal risk for Adam and stochastic gradient\n  descent optimization in the training of deep neural networks",
        "On the Role of Individual Differences in Current Approaches to\n  Computational Image Aesthetics",
        "UAV-Enabled IoT Networks: A SWIPT Energy Harvesting Architecture with\n  Relay Support for Disaster Response",
        "JWST's little red dots: an emerging population of young, low-mass AGN\n  cocooned in dense ionized gas",
        "Understanding the Quality-Diversity Trade-off in Diffusion Language\n  Models",
        "Advanced Weakly-Supervised Formula Exploration for Neuro-Symbolic\n  Mathematical Reasoning",
        "Cooperative Behavior in Pre-State Societies: An Agent-Based Approach of\n  the Aksum Civilization",
        "Improved SED-Fitting Assumptions Result in Inside-Out Quenching at\n  $z\\sim0.5$ and Quenching at All Radii Simultaneously at $z\\sim1$",
        "Displacement-Sparse Neural Optimal Transport",
        "Quantifying Correlations of Machine Learning Models",
        "Machine learning the vanishing order of rational L-functions",
        "Fairness Aware Reinforcement Learning via Proximal Policy Optimization",
        "Gravitational Effects of a Small Primordial Black Hole Passing Through\n  the Human Body",
        "An Ontology for Social Determinants of Education (SDoEd) based on\n  Human-AI Collaborative Approach",
        "Benchmarking MedMNIST dataset on real quantum hardware"
      ],
      "abstract":[
        "3D Gaussian splats have emerged as a revolutionary, effective, learned\nrepresentation for static 3D scenes. In this work, we explore using 2D Gaussian\nsplats as a new primitive for representing videos. We propose GSVC, an approach\nto learning a set of 2D Gaussian splats that can effectively represent and\ncompress video frames. GSVC incorporates the following techniques: (i) To\nexploit temporal redundancy among adjacent frames, which can speed up training\nand improve the compression efficiency, we predict the Gaussian splats of a\nframe based on its previous frame; (ii) To control the trade-offs between file\nsize and quality, we remove Gaussian splats with low contribution to the video\nquality; (iii) To capture dynamics in videos, we randomly add Gaussian splats\nto fit content with large motion or newly-appeared objects; (iv) To handle\nsignificant changes in the scene, we detect key frames based on loss\ndifferences during the learning process. Experiment results show that GSVC\nachieves good rate-distortion trade-offs, comparable to state-of-the-art video\ncodecs such as AV1 and VVC, and a rendering speed of 1500 fps for a 1920x1080\nvideo.",
        "In this note we answer a question of Johannes Carmesin, which was circulated\nat the Oberwolfach Workshop on \"Graph Theory\" in January 2025. We provide a\nunimodular, locally finite, and vertex-transitive graph without any perfect\nfinite $r$-local model for $r \\in \\mathbb N$ large enough.",
        "The remarkable success of large language models relies on their ability to\nimplicitly learn structured latent representations from the pretraining corpus.\nAs a simpler surrogate for representation learning in language modeling, we\nstudy a class of solvable contrastive self-supervised algorithms which we term\nquadratic word embedding models. These models resemble the word2vec algorithm\nand perform similarly on downstream tasks. Our main contributions are\nanalytical solutions for both the training dynamics (under certain\nhyperparameter choices) and the final word embeddings, given in terms of only\nthe corpus statistics. Our solutions reveal that these models learn orthogonal\nlinear subspaces one at a time, each one incrementing the effective rank of the\nembeddings until model capacity is saturated. Training on WikiText, we find\nthat the top subspaces represent interpretable concepts. Finally, we use our\ndynamical theory to predict how and when models acquire the ability to complete\nanalogies.",
        "This paper investigates the optimal use of the multilingual encoder model\nmDeBERTa for tasks in three Germanic languages -- German, Swedish, and\nIcelandic -- representing varying levels of presence and likely data quality in\nmDeBERTas pre-training data. We compare full fine-tuning with the\nparameter-efficient fine-tuning (PEFT) methods LoRA and Pfeiffer bottleneck\nadapters, finding that PEFT is more effective for the higher-resource language,\nGerman. However, results for Swedish and Icelandic are less consistent. We also\nobserve differences between tasks: While PEFT tends to work better for question\nanswering, full fine-tuning is preferable for named entity recognition.\nInspired by previous research on modular approaches that combine task and\nlanguage adapters, we evaluate the impact of adding PEFT modules trained on\nunstructured text, finding that this approach is not beneficial.",
        "The dimensionality of vortical structures has recently been extended beyond\ntwo dimensions, providing higher-order topological characteristics and\nrobustness for high-capacity information processing and turbulence control. The\ngeneration of high-dimensional vortical structures has mostly been demonstrated\nin classical systems through the complex interference of fluidic, acoustic, or\nelectromagnetic waves. However, natural materials rarely support three- or\nhigher-dimensional vortical structures and their physical interactions. Here,\nwe present a high-dimensional gradient thickness optical cavity (GTOC) in which\nthe optical coupling of planar metal-dielectric multilayers implements\ntopological interactions across multiple dimensions. Topological interactions\nin high-dimensional GTOC construct non-trivial topological phases, which induce\nhigh-dimensional vortical structures in generalized parameter space in three,\nfour dimensions, and beyond. These emergent high-dimensional vortical\nstructures are observed under electro-optic tomography as optical vortex\ndynamics in two-dimensional real-space, employing the optical thicknesses of\nthe dielectric layers as synthetic dimensions. We experimentally demonstrate\nemergent vortical structures, optical vortex lines and vortex rings, in a\nthree-dimensional generalized parameter space and their topological\ntransitions. Furthermore, we explore four-dimensional vortical structures,\ntermed optical vortex sheets, which provide the programmability of real-space\noptical vortex dynamics. Our findings hold significant promise for emulating\nhigh-dimensional physics and developing active topological photonic devices.",
        "In this work, we present several rigidity results for compact free boundary\nhypersurfaces in initial data sets with boundary. Specifically, in the first\npart of the paper, we extend the local splitting theorems from [G. J. Galloway\nand H. C. Jang, Some scalar curvature warped product splitting theorems, Proc.\nAm. Math. Soc. 148 (2020), no. 6, 2617-2629] to the setting of manifolds with\nboundary. To achieve this, we build on the approach of the original paper,\nutilizing results on free boundary marginally outer trapped surfaces (MOTS)\napplied to specific initial data sets. In the second part, we extend the main\nresults from [A. Barros and C. Cruz, Free boundary hypersurfaces with\nnon-positive Yamabe invariant in mean convex manifolds, J. Geom. Anal. 30\n(2020), no. 4, 3542-3562] to the context of free boundary MOTS in initial data\nsets with boundary.",
        "This paper documents Int2Int, an open source code base for using transformers\non problems of mathematical research, with a focus on number theory and other\nproblems involving integers. Int2Int is a complete PyTorch implementation of a\ntransformer architecture, together with training and evaluation loops, and\nclasses and functions to represent, generate and decode common mathematical\nobjects. Ancillary code for data preparation, and Jupyter Notebooks for\nvisualizing experimental results are also provided. This document presents the\nmain features of Int2Int, serves as its user manual, and provides guidelines on\nhow to extend it. Int2Int is released under the MIT licence, at\nhttps:\/\/github.com\/FacebookResearch\/Int2Int.",
        "Language models exhibit complex, diverse behaviors when prompted with\nfree-form text, making it difficult to characterize the space of possible\noutputs. We study the problem of behavior elicitation, where the goal is to\nsearch for prompts that induce specific target behaviors (e.g., hallucinations\nor harmful responses) from a target language model. To navigate the\nexponentially large space of possible prompts, we train investigator models to\nmap randomly-chosen target behaviors to a diverse distribution of outputs that\nelicit them, similar to amortized Bayesian inference. We do this through\nsupervised fine-tuning, reinforcement learning via DPO, and a novel Frank-Wolfe\ntraining objective to iteratively discover diverse prompting strategies. Our\ninvestigator models surface a variety of effective and human-interpretable\nprompts leading to jailbreaks, hallucinations, and open-ended aberrant\nbehaviors, obtaining a 100% attack success rate on a subset of AdvBench\n(Harmful Behaviors) and an 85% hallucination rate.",
        "Ab initio modeling of dynamic structure factors (DSF) and related density\nresponse properties in the warm dense matter (WDM) regime is a challenging\ncomputational task. The DSF, convolved with a probing X-ray beam and instrument\nfunction, is measured in X-ray Thomson scattering (XRTS) experiments, which\nallows for the study of electronic structure properties at the microscopic\nlevel. Among the various ab initio methods, linear response time-dependent\ndensity functional theory (LR-TDDFT) is a key framework for simulating the DSF.\nThe standard approach in LR-TDDFT for computing the DSF relies on the orbital\nrepresentation. A significant drawback of this method is the unfavorable\nscaling of the number of required empty bands as the wavenumber increases,\nmaking LR-TDDFT impractical for modeling XRTS measurements over large energy\nscales, such as in backward scattering geometry. We consider and test an\nalternative approach that employs the Liouville-Lanczos (LL) method for\nsimulating the DSF. This approach does not require empty states and allows the\nDSF at large momentum transfer values and over a broad frequency range to be\naccessed. We compare the results obtained from the LL method with those from\nthe standard LR-TDDFT within the projector augmented-wave formalism for\nisochorically heated aluminum and warm dense hydrogen. Additionally, we utilize\nexact path integral Monte Carlo (PIMC) results for the imaginary-time\ndensity-density correlation function (ITCF) of warm dense hydrogen to\nrigorously benchmark the LL approach. We discuss the application of the LL\nmethod for calculating DSFs and ITCFs at different wavenumbers, the effects of\npseudopotentials, and the role of Lorentzian smearing. The successful\nvalidation of the LL method under WDM conditions makes it a valuable addition\nto the ab initio simulation landscape, supporting experimental efforts and\nadvancing WDM theory.",
        "In this paper, we introduce TigerVector, a system that integrates vector\nsearch and graph query within TigerGraph, a Massively Parallel Processing (MPP)\nnative graph database. We extend the vertex attribute type with the embedding\ntype. To support fast vector search, we devise an MPP index framework that\ninteroperates efficiently with the graph engine. The graph query language GSQL\nis enhanced to support vector type expressions and enable query compositions\nbetween vector search results and graph query blocks. These advancements\nelevate the expressive power and analytical capabilities of graph databases,\nenabling seamless fusion of unstructured and structured data in ways previously\nunattainable. Through extensive experiments, we demonstrate TigerVector's\nhybrid search capability, scalability, and superior performance compared to\nother graph databases (including Neo4j and Amazon Neptune) and a highly\noptimized specialized vector database (Milvus). TigerVector was integrated into\nTigerGraph v4.2, the latest release of TigerGraph, in December 2024.",
        "We propose and implement a quantum-informed reduction algorithm for the\nmaximum independent set problem that integrates classical kernelization\ntechniques with information extracted from quantum devices. Our larger\nframework consists of dedicated application, algorithm, and hardware layers,\nand easily generalizes to the maximum weight independent set problem. In this\nhybrid quantum-classical framework, which we call qReduMIS, the quantum\ncomputer is used as a co-processor to inform classical reduction logic about\nfrozen vertices that are likely (or unlikely) to be in large independent sets,\nthereby opening up the reduction space after removal of targeted subgraphs. We\nsystematically assess the performance of qReduMIS based on experiments with up\nto 231 qubits run on Rydberg quantum hardware available through Amazon Braket.\nOur experiments show that qReduMIS can help address fundamental performance\nlimitations faced by a broad set of (quantum) solvers including Rydberg quantum\ndevices. We outline implementations of qReduMIS with alternative platforms,\nsuch as superconducting qubits or trapped ions, and we discuss potential future\nextensions.",
        "Much of modern Schubert calculus is centered on Schubert varieties in the\ncomplete flag variety and on their classes in its integral cohomology ring.\nUnder the Borel isomorphism, these classes are represented by distinguished\npolynomials called Schubert polynomials, introduced by Lascoux and\nSch\\\"utzenberger.\n  Knutson and Miller showed that Schubert polynomials are multidegrees of\nmatrix Schubert varieties, affine varieties introduced by Fulton, which are\nclosely related to Schubert varieties. Many roads to studying Schubert\npolynomials pass through unions and intersections of matrix Schubert varieties.\nThe third author showed that the natural indexing objects of arbitrary\nintersections of matrix Schubert varieties are alternating sign matrices\n(ASMs). Every ASM variety is expressible as a union of matrix Schubert\nvarieties.\n  Many fundamental algebro-geometric invariants (e.g., codimension, degree, and\nCastelnuovo--Mumford regularity) are well understood combinatorially for matrix\nSchubert varieties, substantially via the combinatorics of strong Bruhat order\non $S_n$. The extension of strong order to ASM(n), the set of $n \\times n$\nASMs, has so far not borne as much algebro-geometric fruit for ASM varieties.\n  Hamaker and Reiner proposed an extension of weak Bruhat order from $S_n$ to\nASM(n), which they studied from a combinatorial perspective. In the present\npaper, we place this work on algebro-geometric footing. We use weak order on\nASMs to give a characterization of codimension of ASM varieties. We also show\nthat weak order operators commute with K-theoretic divided difference operators\nand that they satisfy the same derivative formula that facilitated the first\ngeneral combinatorial computation of Castelnuovo--Mumford regularity of matrix\nSchubert varieties. Finally, we build from these results to generalizations\nthat apply to arbitrary unions of matrix Schubert varieties.",
        "In this article, we investigate the BPS invariants associated with framed\nlinks. We extend the relationship between the algebraic curve (i.e. dual\n$A$-polynomial) and the BPS invariants of a knot investigated in \\cite{GKS} to\nthe case of a framed knot. With the help of the framing change formula for the\ndual $A$-polynomial of a framed knot, we give several explicit formulas for the\nextremal $A$-polynomials and the BPS invariants of framed knots. As to the\nframed links, we present several numerical calculations for the Ooguri-Vafa\ninvariants and BPS invariants for framed Whitehead links and Borromean rings\nand verify the integrality property for them.",
        "Next-frame prediction in videos is crucial for applications such as\nautonomous driving, object tracking, and motion prediction. The primary\nchallenge in next-frame prediction lies in effectively capturing and processing\nboth spatial and temporal information from previous video sequences. The\ntransformer architecture, known for its prowess in handling sequence data, has\nmade remarkable progress in this domain. However, transformer-based next-frame\nprediction models face notable issues: (a) The multi-head self-attention (MHSA)\nmechanism requires the input embedding to be split into $N$ chunks, where $N$\nis the number of heads. Each segment captures only a fraction of the original\nembeddings information, which distorts the representation of the embedding in\nthe latent space, resulting in a semantic dilution problem; (b) These models\npredict the embeddings of the next frames rather than the frames themselves,\nbut the loss function based on the errors of the reconstructed frames, not the\npredicted embeddings -- this creates a discrepancy between the training\nobjective and the model output. We propose a Semantic Concentration Multi-Head\nSelf-Attention (SCMHSA) architecture, which effectively mitigates semantic\ndilution in transformer-based next-frame prediction. Additionally, we introduce\na loss function that optimizes SCMHSA in the latent space, aligning the\ntraining objective more closely with the model output. Our method demonstrates\nsuperior performance compared to the original transformer-based predictors.",
        "Despite the omnipresent use of stochastic gradient descent (SGD) optimization\nmethods in the training of deep neural networks (DNNs), it remains, in\nbasically all practically relevant scenarios, a fundamental open problem to\nprovide a rigorous theoretical explanation for the success (and the\nlimitations) of SGD optimization methods in deep learning. In particular, it\nremains an open question to prove or disprove convergence of the true risk of\nSGD optimization methods to the optimal true risk value in the training of\nDNNs. In one of the main results of this work we reveal for a general class of\nactivations, loss functions, random initializations, and SGD optimization\nmethods (including, for example, standard SGD, momentum SGD, Nesterov\naccelerated SGD, Adagrad, RMSprop, Adadelta, Adam, Adamax, Nadam, Nadamax, and\nAMSGrad) that in the training of any arbitrary fully-connected feedforward DNN\nit does not hold that the true risk of the considered optimizer converges in\nprobability to the optimal true risk value. Nonetheless, the true risk of the\nconsidered SGD optimization method may very well converge to a strictly\nsuboptimal true risk value.",
        "Image aesthetic assessment (IAA) evaluates image aesthetics, a task\ncomplicated by image diversity and user subjectivity. Current approaches\naddress this in two stages: Generic IAA (GIAA) models estimate mean aesthetic\nscores, while Personal IAA (PIAA) models adapt GIAA using transfer learning to\nincorporate user subjectivity. However, a theoretical understanding of transfer\nlearning between GIAA and PIAA, particularly concerning the impact of group\ncomposition, group size, aesthetic differences between groups and individuals,\nand demographic correlations, is lacking. This work establishes a theoretical\nfoundation for IAA, proposing a unified model that encodes individual\ncharacteristics in a distributional format for both individual and group\nassessments. We show that transferring from GIAA to PIAA involves\nextrapolation, while the reverse involves interpolation, which is generally\nmore effective for machine learning. Experiments with varying group\ncompositions, including sub-sampling by group size and disjoint demographics,\nreveal significant performance variation even for GIAA, indicating that mean\nscores do not fully eliminate individual subjectivity. Performance variations\nand Gini index analysis reveal education as the primary factor influencing\naesthetic differences, followed by photography and art experience, with\nstronger individual subjectivity observed in artworks than in photos. Our model\nuniquely supports both GIAA and PIAA, enhancing generalization across\ndemographics.",
        "Due to the wide application of unmanned aerial vehicles (UAVs) as relays to\nestablish Disaster Response Networks (DRNs), an effective model of energy\nharvesting (EH) and energy consumption for the UAV-aided Disaster Response\nNetwork (DRN) is rising to be a challenging issue. This is mainly manifest in\nInternet of Things (IoT) scenarios where multiple users are looking to\ncommunicate with the UAV. In this paper, the possibility of connecting an UAV\nwith several users is investigated where the UAV as a relay receives data from\na DRN and delivers to another network considering two IoT scenarios. The first\nscenario represents a conventional method with limited UAV energy where low\ncommunication rates and inadequate service coverage for all users are\nchallenges. But in the second scenario, a Simultaneous Wireless Information and\nPower Transmission (SWIPT) technique is used to serve users. Considering\npotential limitations in transmission energy of users within disaster networks,\nthe SWIPT technique is applied to maximize energy acquisition by the UAV,\nleading to improve the efficiency of the investigated scenario. Finally, the\nrequired energy of the UAV to serve the largest number of users in the shortest\npossible time is clarified. Furthermore, by Considering the relationship\nbetween energy and UAV flight time and defining the UAV flight time\noptimization problem, optimal network parameters are obtained. Simulation\nresults show the effectiveness of the proposed scenario.",
        "JWST has uncovered large numbers of compact galaxies at high redshift with\nbroad hydrogen\/helium lines. These include the enigmatic population known as\n\"little red dots\" (LRDs). Their nature is debated, but they are thought to be\npowered by supermassive black holes (SMBHs) or intense star formation. They\nexhibit unusual properties for SMBHs, such as black holes that are overmassive\nfor their host galaxies and extremely weak X-ray and radio emission. Using the\nhighest-quality JWST spectra, we show here that the lines are broadened by\nelectron scattering with a narrow intrinsic line core. The data require high\nelectron column densities and compact sizes (light days), which, when coupled\nwith their high luminosities can only be explained by SMBH accretion. The\nnarrow intrinsic cores of the lines imply upper limits on the black hole masses\nof $10^{5-7}$ $M_{\\odot}$, two orders of magnitude lower than previous\nestimates. These are among the lowest mass SMBHs known at high redshift and\nsuggest that this is a population of young, rapidly growing SMBHs. They are\nenshrouded in a dense cocoon of ionized gas, probably related to their youth,\nfrom which they are accreting close to the Eddington limit. Reprocessed nebular\nemission from the dense cocoon dominates the optical spectrum, explaining most\nLRD spectral characteristics and helping to suppress radio and X-ray emission.",
        "Diffusion models have seen immense success in modelling continuous data\nacross a range of domains such as vision and audio. Despite the challenges of\nadapting diffusion models to discrete data, recent work explores their\napplication to text generation by working in the continuous embedding space.\nHowever, these models lack a natural means to control the inherent trade-off\nbetween quality and diversity as afforded by the temperature hyperparameter in\nautoregressive models, hindering understanding of model performance and\nrestricting generation quality. This work proposes the use of classifier-free\nguidance and stochastic clamping for manipulating the quality-diversity\ntrade-off on sequence-to-sequence tasks, demonstrating that these techniques\nmay be used to improve the performance of a diffusion language model.",
        "In recent years, neuro-symbolic methods have become a popular and powerful\napproach that augments artificial intelligence systems with the capability to\nperform abstract, logical, and quantitative deductions with enhanced precision\nand controllability. Recent studies successfully performed symbolic reasoning\nby leveraging various machine learning models to explicitly or implicitly\npredict intermediate labels that provide symbolic instructions. However, these\nintermediate labels are not always prepared for every task as a part of\ntraining data, and pre-trained models, represented by Large Language Models\n(LLMs), also do not consistently generate valid symbolic instructions with\ntheir intrinsic knowledge. On the other hand, existing work developed\nalternative learning techniques that allow the learning system to autonomously\nuncover optimal symbolic instructions. Nevertheless, their performance also\nexhibits limitations when faced with relatively huge search spaces or more\nchallenging reasoning problems. In view of this, in this work, we put forward\nan advanced practice for neuro-symbolic reasoning systems to explore the\nintermediate labels with weak supervision from problem inputs and final\noutputs. Our experiments on the Mathematics dataset illustrated the\neffectiveness of our proposals from multiple aspects.",
        "This study intends to test the hypothesis that, contrary to traditional\ninterpretation, the social structure of the polity of Aksum - especially in its\nearly stages - was not characterized by a vertical hierarchy with highly\ncentralized administrative power, and that the leaders mentioned in the few\navailable inscriptions were predominantly ritual leaders with religious rather\nthan coercive political authority. This hypothesis, suggested by the available\narchaeological evidence, is grounded in Charles Stanish's model, which posits\nthat pre-state societies could achieve cooperative behavior without the\npresence of coercive authority. Using agent-based modeling applied to data\ninspired by the Aksum civilization, we examine the dynamics of cooperation in\nthe presence and absence of a Public Goods Game. Results show that while\ncooperative behavior can emerge in the short term without coercive power, it\nmay not be sustainable over the long term, suggesting a need for centralized\nauthority to foster stable, complex societies. These findings provide insights\ninto the evolutionary pathways that lead to state formation and complex social\nstructures.",
        "Many studies conclude that galaxies quench from the inside-out by examining\nprofiles of specific star-formation rate (sSFR). These are usually measured by\nfitting spectral energy distributions (SEDs) assuming a fixed dust law and\nuniform priors on all parameters. Here, we examine the effects of more\nphysically motivated priors: a flexible dust law, an exponential prior on the\ndust attenuation $A_V$, and Gaussian priors that favor extended star-formation\nhistories. This results in model colors that better trace observations. We then\nperform radial SED fits to multiband flux profiles measured from Hubble Space\nTelescope images for 1,440 galaxies at $0.4<z<1.5$ of stellar masses\n$10^{10}-10^{11.5}\\ M_{\\odot}$ using both the traditional and the more\nphysically motivated assumptions. The latter results in star formation rate and\n$A_V$ profiles that agree with measurements from spectroscopy and $A_V$\nprofiles that behave correctly as a function of inclination. Since green valley\ngalaxies at $z\\sim1.3$ are expected to evolve into quiescent galaxies at\n$z\\sim0.9$, we compare their sSFR profiles using the more physically motivated\nassumptions. Their slopes are similar at all masses ($0.06 -\n0.08~\\textrm{dex}~\\textrm{kpc}^{-1}$), and the normalizations for the quiescent\ngalaxies are lower. Therefore, the sSFR profiles decline with time as quenching\noccurs at all radii simultaneously. We compare profiles of green valley\ngalaxies at $z\\sim0.9$ and quiescent galaxies at $z\\sim0.5$. The former are\nshallower at all masses by $\\sim0.1~\\textrm{dex}~\\textrm{kpc}^{-1}$. The sSFR\nprofiles steepen with time as galaxies quench from the inside-out. In summary,\nat $z\\sim0.9-1.3$, galaxies quench at all radii simultaneously, and at\n$z\\sim0.5-0.9$, they quench from the inside-out.",
        "Optimal Transport (OT) theory seeks to determine the map $T:X \\to Y$ that\ntransports a source measure $P$ to a target measure $Q$, minimizing the cost\n$c(\\mathbf{x}, T(\\mathbf{x}))$ between $\\mathbf{x}$ and its image\n$T(\\mathbf{x})$. Building upon the Input Convex Neural Network OT solver and\nincorporating the concept of displacement-sparse maps, we introduce a sparsity\npenalty into the minimax Wasserstein formulation, promote sparsity in\ndisplacement vectors $\\Delta(\\mathbf{x}) := T(\\mathbf{x}) - \\mathbf{x}$, and\nenhance the interpretability of the resulting map. However, increasing sparsity\noften reduces feasibility, causing $T_{\\#}(P)$ to deviate more significantly\nfrom the target measure. In low-dimensional settings, we propose a heuristic\nframework to balance the trade-off between sparsity and feasibility by\ndynamically adjusting the sparsity intensity parameter during training. For\nhigh-dimensional settings, we directly constrain the dimensionality of\ndisplacement vectors by enforcing $\\dim(\\Delta(\\mathbf{x})) \\leq l$, where $l <\nd$ for $X \\subseteq \\mathbb{R}^d$. Among maps satisfying this constraint, we\naim to identify the most feasible one. This goal can be effectively achieved by\nadapting our low-dimensional heuristic framework without resorting to\ndimensionality reduction. We validate our method on both synthesized sc-RNA and\nreal 4i cell perturbation datasets, demonstrating improvements over existing\nmethods.",
        "Machine Learning models are being extensively used in safety critical\napplications where errors from these models could cause harm to the user. Such\nrisks are amplified when multiple machine learning models, which are deployed\nconcurrently, interact and make errors simultaneously. This paper explores\nthree scenarios where error correlations between multiple models arise,\nresulting in such aggregated risks. Using real-world data, we simulate these\nscenarios and quantify the correlations in errors of different models. Our\nfindings indicate that aggregated risks are substantial, particularly when\nmodels share similar algorithms, training datasets, or foundational models.\nOverall, we observe that correlations across models are pervasive and likely to\nintensify with increased reliance on foundational models and widely used public\ndatasets, highlighting the need for effective mitigation strategies to address\nthese challenges.",
        "In this paper, we study the vanishing order of rational $L$-functions from a\ndata scientific perspective. Each $L$-function is represented in our data by\nfinitely many Dirichlet coefficients, the normalisation of which depends on the\ncontext. We observe murmuration-like patterns in averages across our dataset,\nfind that PCA clusters rational $L$-functions by their vanishing order, and\nrecord that LDA and neural networks may accurately predict this quantity.",
        "Fairness in multi-agent systems (MAS) focuses on equitable reward\ndistribution among agents in scenarios involving sensitive attributes such as\nrace, gender, or socioeconomic status. This paper introduces fairness in\nProximal Policy Optimization (PPO) with a penalty term derived from demographic\nparity, counterfactual fairness, and conditional statistical parity. The\nproposed method balances reward maximisation with fairness by integrating two\npenalty components: a retrospective component that minimises disparities in\npast outcomes and a prospective component that ensures fairness in future\ndecision-making. We evaluate our approach in the Allelopathic Harvest game, a\ncooperative and competitive MAS focused on resource collection, where some\nagents possess a sensitive attribute. Experiments demonstrate that fair-PPO\nachieves fairer policies across all fairness metrics than classic PPO. Fairness\ncomes at the cost of reduced rewards, namely the Price of Fairness, although\nagents with and without the sensitive attribute renounce comparable amounts of\nrewards. Additionally, the retrospective and prospective penalties effectively\nchange the agents' behaviour and improve fairness. These findings underscore\nthe potential of fair-PPO to address fairness challenges in MAS.",
        "The gravitational effects of a primordial black hole (PBH) passing through\nthe human body are examined, with the goal of determining the minimum mass\nnecessary to produce significant injury or death. Two effects are examined: the\ndamage caused by a shock wave propagating outward from the black hole\ntrajectory, and the dissociation of brain cells from tidal forces produced by\nthe black hole on its passage through the human body. It is found that the\nformer is the dominant effect, with a cutoff mass for serious injury or death\nof approximately $M_{PBH} > 1.4 \\times 10^{17} {\\rm g}$. The number density of\nprimordial black holes with a mass above this cutoff is far too small to\nproduce any observable effects on the human population.",
        "The use of computational ontologies is well-established in the field of\nMedical Informatics. The topic of Social Determinants of Health (SDoH) has also\nreceived extensive attention. Work at the intersection of ontologies and SDoH\nhas been published. However, a standardized framework for Social Determinants\nof Education (SDoEd) is lacking. In this paper, we are closing the gap by\nintroducing an SDoEd ontology for creating a precise conceptualization of the\ninterplay between life circumstances of students and their possible educational\nachievements. The ontology was developed utilizing suggestions from\nChatGPT-3.5-010422 and validated using peer-reviewed research articles. The\nfirst version of developed ontology was evaluated by human experts in the field\nof education and validated using standard ontology evaluation software. This\nversion of the SDoEd ontology contains 231 domain concepts, 10 object\nproperties, and 24 data properties",
        "Quantum machine learning (QML) has emerged as a promising domain to leverage\nthe computational capabilities of quantum systems to solve complex\nclassification tasks. In this work, we present the first comprehensive QML\nstudy by benchmarking the MedMNIST-a diverse collection of medical imaging\ndatasets on a 127-qubit real IBM quantum hardware, to evaluate the feasibility\nand performance of quantum models (without any classical neural networks) in\npractical applications. This study explores recent advancements in quantum\ncomputing such as device-aware quantum circuits, error suppression, and\nmitigation for medical image classification. Our methodology is comprised of\nthree stages: preprocessing, generation of noise-resilient and\nhardware-efficient quantum circuits, optimizing\/training of quantum circuits on\nclassical hardware, and inference on real IBM quantum hardware. Firstly, we\nprocess all input images in the preprocessing stage to reduce the spatial\ndimension due to quantum hardware limitations. We generate hardware-efficient\nquantum circuits using backend properties expressible to learn complex patterns\nfor medical image classification. After classical optimization of QML models,\nwe perform inference on real quantum hardware. We also incorporate advanced\nerror suppression and mitigation techniques in our QML workflow, including\ndynamical decoupling (DD), gate twirling, and matrix-free measurement\nmitigation (M3) to mitigate the effects of noise and improve classification\nperformance. The experimental results showcase the potential of quantum\ncomputing for medical imaging and establish a benchmark for future advancements\nin QML applied to healthcare."
      ]
    }
  },
  {
    "id":2411.06513,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Advances and Open Problems in Federated Learning",
    "start_abstract":"The term Federated Learning was coined as recently 2016 to describe a machine learning setting where multiple entities collaborate in solving problem, under the coordination of central server or service provider. Each client\u2019s raw data is stored locally and not exchanged transferred; instead, focused updates intended for immediate aggregation are used achieve objective. Since then, topic has gathered much interest across many different disciplines realization that these interdisciplinary problems likely requires just but techniques from distributed optimization, cryptography, security, differential privacy, fairness, compressed sensing, systems, information theory, statistics, more. This monograph contributions leading experts disciplines, who latest state-of-the art their perspective. These have been carefully curated into comprehensive treatment enables reader understand work done get pointers effort required solve before can become reality practical systems. Researchers working area systems will find this an enlightening read may inspire them on challenging issues outlined. up speed quickly easily what increasingly important topic: Learning.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "The future of digital health with federated learning"
      ],
      "abstract":[
        "Abstract Data-driven machine learning (ML) has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing data not fully exploited ML primarily because it sits silos privacy concerns restrict access to this data. However, without sufficient will be prevented reaching its full potential and, ultimately, making the transition research clinical practice. This paper considers key factors contributing issue, explores how federated (FL) may provide solution future of digital health highlights challenges considerations that need addressed."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Iterative phase retrieval algorithm for space-variant PSF in optical\n  systems with aberrations",
        "Network-Driven Global Stability Analysis: SVIRS Epidemic Model",
        "VAEs and GANs: Implicitly Approximating Complex Distributions with\n  Simple Base Distributions and Deep Neural Networks -- Principles, Necessity,\n  and Limitations",
        "The Equation of State of QCD up to very high temperatures",
        "Toward Large-Scale Distributed Quantum Long Short-Term Memory with\n  Modular Quantum Computers",
        "Domination Parameters of Graph Covers",
        "$L^2$-estimates on flat vector bundles and Pr\\'ekopa's theorem",
        "Unveiling potential candidates for rare-earth-free permanent magnet and\n  magnetocaloric effect applications: a high throughput screening in Fe-N\n  alloys",
        "QPEs as Lense-Thirring precession of super-Eddington flows",
        "On monotonicity of heat kernels: a new example and counterexamples",
        "3D Surface Reconstruction and Volume Approximation via the meshless\n  methods",
        "SOE's ESG Performance on Financial Flexibility: The Evidence from the\n  Hong Kong Stock Market",
        "Unsupervised optimal deep transfer learning for classification under\n  general conditional shift",
        "Multivariable Behavioral Change Modeling of Epidemics in the Presence of\n  Undetected Infections",
        "SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time\n  Series Forecasting",
        "Sabotage and Free Riding in Contests with a Group-Specific\n  Public-Good\/Bad Prize",
        "Purported quantitative support for multiple introductions of SARS-CoV-2\n  into humans is an artefact of an imbalanced hypothesis testing framework",
        "Stretching the Printability Metric in Direct-ink Writing with Highly\n  Extensible Yield-Stress Fluids",
        "On the Baum-Connes conjecture for $D_{\\infty}$",
        "Almost sharp variational estimates for discrete truncated operators of\n  Carleson type",
        "Markov fractions and the slopes of the exceptional bundles on $\\mathbb\n  P^2$",
        "Large-time estimates for the Dirichlet heat equation in exterior domains",
        "The Impact of Digitalisation and Sustainability on Inclusiveness:\n  Inclusive Growth Determinants",
        "A study of the Antlion Random Walk",
        "Signatures of modified gravity from the gravitational Aharonov-Bohm\n  effect",
        "Restricted type estimates on the Bergman projection of some singular\n  domains",
        "Sparse Orthogonal Matching Pursuit-based Parameter Estimation for\n  Integrated Sensing and Communications",
        "Explainable AI-Driven Neural Activity Analysis in Parkinsonian Rats\n  under Electrical Stimulation",
        "Benchmarking a magnon-scattering reservoir with modal and temporal\n  multiplexing"
      ],
      "abstract":[
        "Iterative phase retrieval algorithms are widely used in digital optics for\ntheir efficiency and simplicity. Conventionally, these algorithms do not\nconsider aberrations as they assume an ideal, aberration-free optical system.\nHere, we propose modified iterative phase retrieval algorithms that take into\naccount the space-invariant and space-variant point spread function of the\noptical system.",
        "An epidemic Susceptible-Vaccinated-Infected-Removed-Susceptible (SVIRS) model\nis presented on a weighted-undirected network with graph Laplacian diffusion.\nDisease-free equilibrium always exists while the existence and uniqueness of\nendemic equilibrium have been shown. When the basic reproduction number is\nbelow unity, the disease-free equilibrium is asymptotically globally stable.\nThe endemic equilibrium is asymptotically globally stable if the basic\nreproduction number is above unity. Numerical analysis is illustrated with a\nroad graph of the state of Minnesota. The effect of all important model\nparameters has been discussed.",
        "This tutorial focuses on the fundamental architectures of Variational\nAutoencoders (VAE) and Generative Adversarial Networks (GAN), disregarding\ntheir numerous variations, to highlight their core principles. Both VAE and GAN\nutilize simple distributions, such as Gaussians, as a basis and leverage the\npowerful nonlinear transformation capabilities of neural networks to\napproximate arbitrarily complex distributions. The theoretical basis lies in\nthat a linear combination of multiple Gaussians can almost approximate any\nprobability distribution, while neural networks enable further refinement\nthrough nonlinear transformations. Both methods approximate complex data\ndistributions implicitly. This implicit approximation is crucial because\ndirectly modeling high-dimensional distributions explicitly is often\nintractable. However, the choice of a simple latent prior, while\ncomputationally convenient, introduces limitations. In VAEs, the fixed Gaussian\nprior forces the posterior distribution to align with it, potentially leading\nto loss of information and reduced expressiveness. This restriction affects\nboth the interpretability of the model and the quality of generated samples.",
        "We present the non-perturbative computation of the entropy density in QCD for\ntemperatures ranging from 3 GeV up to the electro-weak scale, using $N_f=3$\nflavours of massless O$(a)$-improved Wilson fermions. We adopt a new strategy\ndesigned to be computationally efficient and based on formulating thermal QCD\nin a moving reference frame, where the fields satisfy shifted boundary\nconditions in the temporal direction and periodic boundary conditions along the\nspatial ones. In this setup the entropy density can be computed as the\nderivative of the free-energy density with respect to the shift parameter. For\neach physical temperature, we perform Monte Carlo simulations at four values of\nthe lattice spacing in order to extrapolate the numerical data of the entropy\ndensity to the continuum limit. We achieve a final accuracy of approximatively\n$0.5$-$1.0\\%$ and our results are compared with predictions from\nhigh-temperature perturbation theory.",
        "In this work, we introduce a Distributed Quantum Long Short-Term Memory\n(QLSTM) framework that leverages modular quantum computing to address\nscalability challenges on Noisy Intermediate-Scale Quantum (NISQ) devices. By\nembedding variational quantum circuits into LSTM cells, the QLSTM captures\nlong-range temporal dependencies, while a distributed architecture partitions\nthe underlying Variational Quantum Circuits (VQCs) into smaller, manageable\nsubcircuits that can be executed on a network of quantum processing units. We\nassess the proposed framework using nontrivial benchmark problems such as\ndamped harmonic oscillators and Nonlinear Autoregressive Moving Average\nsequences. Our results demonstrate that the distributed QLSTM achieves stable\nconvergence and improved training dynamics compared to classical approaches.\nThis work underscores the potential of modular, distributed quantum computing\narchitectures for large-scale sequence modelling, providing a foundation for\nthe future integration of hybrid quantum-classical solutions into advanced\nQuantum High-performance computing (HPC) ecosystems.",
        "A graph $G$ is a \\emph{cover} of a graph $F$ if there exists an onto mapping\n$\\pi : V(G) \\to V(F)$, called a (\\emph{covering}) \\emph{projection}, such that\n$\\pi$ maps the neighbours of any vertex $v$ in $G$ bijectively onto the\nneighbours of $\\pi(v)$ in $F$. This paper is the first attempt to study the\nconnection between domination parameters and graph covers. We focus on the\ndomination number, the total domination number, and the connected domination\nnumber. We prove tight upper bounds for the domination parameters of $G$.\nMoreover, we prove lower bounds for the domination parameters of $G$. Finally,\nwe propose a conjecture on the lower bound for the domination number of $G$ and\nprovide evidence to support the conjecture.",
        "In this paper, we will construct H\\\"ormander's $L^2$-estimate of the operator\n$d$ on a flat vector bundle over a $p$-convex Riemannian manifold and discuss\nsome geometric applications of it. In particular, we will generalize the\nclassical Pr\\'ekopa's theorem in convex analysis.",
        "Based on high-throughput density functional theory calculations, we have\nfound 49 ferromag-netic cases in FexN1-x (0<x<1) compounds, focusing especially\non permanent magnet and giant magnetocaloric effect applications. It is found\nthat 15 compounds are potential permanent mag-nets with a magneto-crystalline\nanisotropy energy more than 1 MJ\/m3, filling in the gap of appli-cation\nspectrum between high-performance and widely used permanents. Among the\npotential permanent magnets, Fe2N can be classified as a hard magnet while the\nother 14 compounds can be classified as semi-hard magnets. According to the\ncalculations of magnetic deformation proxy, 40 compounds are identified as\npotential giant magnetocaloric effect candidates. We suspect that Fe-N\ncompounds provide fine opportunities for applications in both rare-earth free\npermanent magnets and magnetocaloric effect.",
        "Quasi-periodic eruptions (QPEs) are a recently identified class of X-ray\ntransient associated with tidal disruption events by supermassive black holes,\nand for which there are multiple possible explanations. In this paper we\npresent a simple model which requires the black hole be spinning, be misaligned\nwith the accretion flow (both conditions of which are almost certainly met) and\nthat the accretion rate is a few times the Eddington limit. We speculate that\nthe resulting Lense-Thirring torques force the disc and entrained outflows to\nprecess, leading to increased X-ray flux when the wind-cone is oriented at\nlower inclinations to the observer. We test the range of parameters for which\nthis model could explain the period and brightness of the QPE events discovered\nthus far, and make qualitative comparisons between the observed X-ray spectra\nand lightcurves to those extracted from GR-RMHD simulations. Overall, we find\nsome areas of promising concordance, and identify challenges related to the\ndetails of current simulations.",
        "We discover a new, non-radial example of a manifold whose heat kernel\ndecreases monotonically along all minimal geodesics. We also classify the flat\ntori with this monotonicity property. Furthermore, we show that for a generic\nmetric on any smooth manifold the monotonicity property fails at large times.\nThis answers a recent question of Alonso-Or\\'an, Chamizo, Mart\\'inez, and Mas.",
        "In this paper, we propose several mathematical models for 3D surface\nreconstruction and volume estimation from a set of scattered cloud data. Three\nmeshless methods including the interpolation-based method by RBF, PDE-based\napproach by Kansa's method and the Method of Fundamental Solutions are employed\nand compared. For the optimal recovery of the surfaces, the selection of free\nparameters in related PDE models are further studied and analyzed. Besides,\nseveral criteria like distance are employed in above methods instead of the\nclassical parameter lambda determination strategy, which leads to a more\nreliable reconstruction performance. Finally, the volume estimation of 3D\nirregular objects is proposed based on the optimal reconstructed geometric\nmodels via proposed meshless methods. Numerous numerical examples are presented\nto demonstrate the effectiveness of the proposed surface reconstruction methods\nand the volume estimation strategy.",
        "As the global economic environment becomes increasingly unstable, enhancing\nfinancial flexibility to cope with risks has become the consensus of many\ncompanies. At the same time, environmental, social, and governance (ESG)\nperformance may be one of the effective ways. We studied the impact of a firm's\nESG performance on its financial flexibility with a sample of companies listed\non the Hong Kong stock market from 2018 to 2022. The empirical results show\nthat good environmental, social and governance performance can significantly\nimprove a firm's financial flexibility. In addition, this paper also finds that\nthe influence of ESG performance on financial flexibility is weak for\nstate-owned enterprises due to the influence of governance structure and market\ncharacteristics. Finally, the further analysis shows that there is a mediating\nrole played by financing constraints in this process. This study can provide\nbackground information for state-owned enterprises' governance, information\ndisclosure, and corporate operations. It also has guiding significance for\nrelevant investors, management and officials.",
        "Classifiers trained solely on labeled source data may yield misleading\nresults when applied to unlabeled target data drawn from a different\ndistribution. Transfer learning can rectify this by transferring knowledge from\nsource to target data, but its effectiveness frequently relies on stringent\nassumptions, such as label shift. In this paper, we introduce a novel General\nConditional Shift (GCS) assumption, which encompasses label shift as a special\nscenario. Under GCS, we demonstrate that both the target distribution and the\nshift function are identifiable. To estimate the conditional probabilities\n${\\bm\\eta}_P$ for source data, we propose leveraging deep neural networks\n(DNNs). Subsequent to transferring the DNN estimator, we estimate the target\nlabel distribution ${\\bm\\pi}_Q$ utilizing a pseudo-maximum likelihood approach.\nUltimately, by incorporating these estimates and circumventing the need to\nestimate the shift function, we construct our proposed Bayes classifier. We\nestablish concentration bounds for our estimators of both ${\\bm\\eta}_P$ and\n${\\bm\\pi}_Q$ in terms of the intrinsic dimension of ${\\bm\\eta}_P$ . Notably,\nour DNN-based classifier achieves the optimal minimax rate, up to a logarithmic\nfactor. A key advantage of our method is its capacity to effectively combat the\ncurse of dimensionality when ${\\bm\\eta}_P$ exhibits a low-dimensional\nstructure. Numerical simulations, along with an analysis of an Alzheimer's\ndisease dataset, underscore its exceptional performance.",
        "Epidemic models are invaluable tools to understand and implement strategies\nto control the spread of infectious diseases, as well as to inform public\nhealth policies and resource allocation. However, current modeling approaches\nhave limitations that reduce their practical utility, such as the exclusion of\nhuman behavioral change in response to the epidemic or ignoring the presence of\nundetected infectious individuals in the population. These limitations became\nparticularly evident during the COVID-19 pandemic, underscoring the need for\nmore accurate and informative models. Motivated by these challenges, we develop\na novel Bayesian epidemic modeling framework to better capture the complexities\nof disease spread by incorporating behavioral responses and undetected\ninfections. In particular, our framework makes three contributions: 1)\nleveraging additional data on hospitalizations and deaths in modeling the\ndisease dynamics, 2) accounting data uncertainty arising from the large\npresence of asymptomatic and undetected infections, and 3) allowing the\npopulation behavioral change to be dynamically influenced by multiple data\nsources (cases and deaths). We thoroughly investigate the properties of the\nproposed model via simulation, and illustrate its utility on COVID-19 data from\nMontreal and Miami.",
        "In recent work on time-series prediction, Transformers and even large\nlanguage models have garnered significant attention due to their strong\ncapabilities in sequence modeling. However, in practical deployments,\ntime-series prediction often requires operation in resource-constrained\nenvironments, such as edge devices, which are unable to handle the\ncomputational overhead of large models. To address such scenarios, some\nlightweight models have been proposed, but they exhibit poor performance on\nnon-stationary sequences. In this paper, we propose $\\textit{SWIFT}$, a\nlightweight model that is not only powerful, but also efficient in deployment\nand inference for Long-term Time Series Forecasting (LTSF). Our model is based\non three key points: (i) Utilizing wavelet transform to perform lossless\ndownsampling of time series. (ii) Achieving cross-band information fusion with\na learnable filter. (iii) Using only one shared linear layer or one shallow MLP\nfor sub-series' mapping. We conduct comprehensive experiments, and the results\nshow that $\\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on\nmultiple datasets, offering a promising method for edge computing and\ndeployment in this task. Moreover, it is noteworthy that the number of\nparameters in $\\textit{SWIFT-Linear}$ is only 25\\% of what it would be with a\nsingle-layer linear model for time-domain prediction. Our code is available at\nhttps:\/\/github.com\/LancelotXWX\/SWIFT.",
        "We study contests in which two groups compete to win (or not to win) a\ngroup-specific public-good\/bad prize. Each player in the groups can exert two\ntypes of effort: one to help her own group win the prize, and one to sabotage\nher own group's chances of winning it. The players in the groups choose their\neffort levels simultaneously and independently. We introduce a specific form of\ncontest success function that determines each group's probability of winning\nthe prize, taking into account players' sabotage activities. We show that two\ntypes of purestrategy Nash equilibrium occur, depending on parameter values:\none without sabotage activities and one with sabotage activities. In the first\ntype, only the highest-valuation player in each group expends positive effort,\nwhereas, in the second type, only the lowest-valuation player in each group\nexpends positive effort.",
        "A prominent report claimed substantial support for two introductions of\nSARS-CoV-2 into humans using a calculation that combined phylodynamic\ninferences and epidemic models. Inspection of the calculation identifies an\nimbalance in the hypothesis testing framework that confounds this result; the\nsingle-introduction model was tested against more stringent conditions than the\ntwo-introduction model. Here, I show that when the two-introduction model is\ntested against the same conditions, the support disappears.",
        "Direct-ink writing leverages the rheological complexity of yield-stress\nfluids to construct complex geometries, particularly those with large gaps\nacross internal structures. However, extensional rheological properties have\nrarely been considered in work that studies rheology-printability correlations.\nHere, we test our hypothesis that extensional properties correlate with\ndrawability, a key indicator of printability that signifies speed robustness,\nprinting resolution, and gap-spanning performance. We formulated cementitious\nsuspensions using hydroxyapatite (HAp) particles, independently tuning them for\nyield stress and extensibility, two crucial rheological properties, and\ntest-printed. To enhance extensibility, we incorporated hydroxypropyl\nmethylcellulose as a polymeric modifier, but this enhancement may decrease as\nyield stress increases, presenting a challenge in materials design. We\nmodulated particle interactions to achieve a wide range of yield stress and\nextensibility, allowing for rigorous testing of our hypothesis. This approach\ncreated inks with high extensibility and high yield stress, generally\nconsidered mutually exclusive properties. We evaluated correlations between\ndrawability and key rheological properties, finding the strongest positive\ncorrelation with extensional failure strains (strain-to-break) rather than\nyield stress. We establish a bijective property-manufacturing relationship\n(one-on-one mapping of shear yield stress to buildability and extensional\nstrain-to-break to drawability) by combining our findings on drawability with\nprevious studies on buildability. This relationship provides a comprehensive\nframework for designing high-performance inks that can be self-supporting,\ncapable of high-speed printing, and allow gap-spanning features.",
        "We make an exposition of the proof of the Baum-Connes conjecture for the\ninfinite dihedral group following the ideas of Higson and Kasparov.",
        "We establish $r$-variational estimates for discrete truncated Carleson-type\noperators on $\\ell^p$ for $1<p<\\infty$. Notably, these estimates are sharp and\nenhance the results obtained by Krause and Roos (J. Eur. Math. Soc. 2022, J.\nFunct. Anal. 2023), up to a logarithmic loss related to the scale. On the other\nhand, as $r$ approaches infinity, the consequences align with the estimates\nproved by Krause and Roos. Moreover, for the case of quadratic phases, we\nremove this logarithmic loss with respect to the scale, at the cost of\nincreasing $p$ slightly.",
        "We show that the Markov fractions introduced recently by Boris Springborn are\nprecisely the slopes of the exceptional vector bundles on $\\mathbb P^2$ studied\nin 1980s by Dr\\`ezet and Le Potier and by Rudakov. In particular, we provide a\nsimpler proof of Rudakov's result claiming that the ranks of the exceptional\nbundles on $\\mathbb P^2$ are Markov numbers.",
        "We give large-time asymptotic estimates, both in uniform and $L^1$ norms, for\nsolutions of the Dirichlet heat equation in the complement of a bounded open\nset of $\\mathbb{R}^d$ satisfying certain technical assumptions. We always\nassume that the initial datum has suitable finite moments (often, finite first\nmoment). All estimates include an explicit rate of approach to the asymptotic\nprofiles at the different scales natural to the problem, in analogy with the\nGaussian behaviour of the heat equation in the full space. As a consequence we\nobtain by an approximation procedure the asymptotic profile, with rates, for\nthe Dirichlet heat kernel in these exterior domains. The estimates on the rates\nare new even when the domain is the complement of the unit ball in\n$\\mathbb{R}^d$, except for previous results by Uchiyama in dimension 2, which\nwe are able to improve in some scales. We obtain that the heat kernel behaves\nasymptotically as the heat kernel in the full space, with a factor that takes\ninto account the shape of the domain through a harmonic profile, and a second\nfactor which accounts for the loss of mass through the boundary. The main ideas\nwe use come from entropy methods in PDE and probability, whose application\nseems to be new in the context of diffusion problems in exterior domains.",
        "Inclusiveness and economic development have been slowed by the pandemics and\nmilitary conflicts. This study investigates the main determinants of\ninclusiveness at the European level. A multi-method approach is used, with\nPrincipal Component Analysis (PCA) applied to create the Inclusiveness Index\nand Generalised Method of Moments (GMM) analysis used to investigate the\ndeterminants of inclusiveness. The data comprises a range of 22 years, from\n2000 to 2021, for 32 European countries. The determinants of inclusiveness and\ntheir effects were identified. First, economic growth, industrial upgrading,\nelectricity consumption, digitalisation, and the quantitative aspect of\ngovernance, all have a positive impact on inclusive growth in Europe. Second,\nthe level of CO2 emissions and inflation have a negative impact on\ninclusiveness. Tomorrow's inclusive and sustainable growth must include\ninvestments in renewable energy, digital infrastructure, inequality policies,\nsustainable governance, human capital, and inflation management. These findings\ncan help decision makers design inclusive growth policies.",
        "This paper treats a new type of random walk referred to as an Antlion Random\nWalk (ARW), which is motivated by mathematical modeling of the decision-making\nprocess using chaotic semiconductor lasers with memory parameters. We discuss\nthe dependency of the property of the probability distribution of ARWs on the\nmemory parameter $\\alpha$ and discuss uniqueness of them in contrast to the\nconventional, simple RWs through similarity to the normal distribution.",
        "To date, no observational confirmation of dark matter particles has been\nfound. In this paper, we put forward an alternative approach to inferring\nevidence for dark matter through modified gravity, without invoking fundamental\ndark matter particles. Specifically, we explore the possibility of extracting\nsignatures of Kaluza-Klein gravity through the gravitational Aharonov-Bohm\neffect. Kaluza-Klein theory has recently been proposed as an alternative to the\ndark sector, and predicts a tower of particles, including spin-0 and spin-1\ngravitons alongside the usual spin-2 gravitons, which can gravitationally\ncouple to matter. We thus analyze a quantum system in free fall around a\ngravitating body in the presence of a modified Yukawa-like gravitational\npotential, and determine the gravitational phase induced by the additional\ndegrees of freedom introduced by the Kaluza-Klein model. Our results reveal\nthat, in addition to the usual result from General Relativity, the quantum wave\nfunction of the system exhibits an additional effect: a splitting of the energy\nlevels with a new quantum number due to the extra vector gravitational degrees\nof freedom. The energy splitting difference between general relativity and\nKaluza-Klein gravity is found to be of the order of meV for an atomic system\nand eV for a nuclear system. Similar values also arise in generic modified\ngravity models and can be feasibly tested in the future. Numerical estimates\nfor the graviton mass are also provided, and potential imprints on\ngravitational waves are mentioned.",
        "We obtain (weighted) restricted type estimates for the Bergman projection\noperator on monomial polyhedra, a class of domains generalizing the Hartogs\ntriangle. From these estimates, we recapture $L^p$ boundedness results of the\nBergman projection on these domains. On some monomial polyhedra, we also\ndiscover that the Bergman projection could fail to be of weak type $(q_*,q_*)$\nwhere $q_*$ is the right endpoint of the interval of $L^p$-regularity of the\ndomain.",
        "Accurate parameter estimation such as angle of arrival (AOA) is essential to\nenhance the performance of integrated sensing and communication (ISAC) in\nmmWave multiple-input multiple-output (MIMO) systems. This work presents a\nsensing-aided communication channel estimation mechanism, where the sensing\nchannel shares the same AOA with the uplink communication channel. First, we\npropose a novel orthogonal matching pursuit (OMP)-based method for coarsely\nestimating the AOA in a sensing channel, offering improved accuracy compared to\nconventional methods that rely on rotational invariance techniques. Next, we\nrefine the coarse estimates obtained in the first step by modifying the\nSpace-Alternating Generalized Expectation Maximization algorithm for fine\nparameter estimation. Through simulations and mathematical analysis, we\ndemonstrate that scenarios with shared AOA achieve a better Cramer-Rao lower\nbound (CRLB) than those without sharing. This finding highlights the potential\nof leveraging joint sensing and communication channels to enhance parameter\nestimation accuracy, particularly in channel or location estimation\napplications.",
        "Parkinson's disease (PD) is a neurodegenerative disorder characterized by\nmotor dysfunction and abnormal neural oscillations. These symptoms can be\nmodulated through electrical stimulation. Traditional neural activity analysis\nin PD has typically relied on statistical methods, which often introduce bias\nowing to the need for expert-driven feature extraction. To address this\nlimitation, we explore an explainable artificial intelligence (XAI) approach to\nanalyze neural activity in Parkinsonian rats receiving electrical stimulation.\nElectrocorticogram (ECoG) signals were collected before and after electrical\nstimulation using graphene-based electrodes that enable less-invasive\nmonitoring and stimulation in PD. EEGNet, a convolutional neural network,\nclassified these ECoG signals into pre- and post-stimulation states. We applied\nlayer-wise relevance propagation, an XAI technique, to identify key neural\ninputs contributing to the model's decisions, incorporating the spatial\nelectrode information matched to the cortex map. The XAI analysis highlighted\narea-specific importance in beta and gamma frequency bands, which could not be\ndetected through mean comparison analyses relying on feature extraction. These\nfindings demonstrate the potential of XAI in analyzing neural dynamics in\nneurodegenerative disorders such as PD, suggesting that the integration of\ngraphene-based electrodes with advanced deep learning models offers a promising\nsolution for real-time PD monitoring and therapy.",
        "Physical reservoir computing has emerged as a powerful framework for\nexploiting the inherent nonlinear dynamics of physical systems to perform\ncomputational tasks. Recently, we presented the magnon-scattering reservoir,\nwhose internal nodes are given by the fundamental wave-like excitations of\nferromagnets called magnons. These excitations can be geometrically-quantized\nand, in response to an external stimulus, show transient nonlinear scattering\ndynamics that can be harnessed to perform memory and nonlinear transformation\ntasks. Here, we test a magnon-scattering reservoir in a single magnetic disk in\nthe vortex state towards two key performance indicators for physical reservoir\ncomputing, the short-term memory and parity-check tasks. Using time-resolved\nBrillouin-light-scattering microscopy, we measure the evolution of the\nreservoir's spectral response to an input sequence consisting of random binary\ninputs encoded in microwave pulses with two distinct frequencies. Two different\noutput spaces of the reservoir are defined, one based on the time-averaged\nfrequency spectra and another based on temporal multiplexing. Our results\ndemonstrate that the memory and nonlinear transformation capability do not\ndepend on the chosen read-out scheme as long as the dimension of the output\nspace is large enough to capture all nonlinear features provided by the\nmagnon-magnon interactions. This further shows that solely the nonlinear\nmagnons in the physical system, not the read-out, determine the reservoir's\ncapacity."
      ]
    }
  },
  {
    "id":2411.06513,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"The future of digital health with federated learning",
    "start_abstract":"Abstract Data-driven machine learning (ML) has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing data not fully exploited ML primarily because it sits silos privacy concerns restrict access to this data. However, without sufficient will be prevented reaching its full potential and, ultimately, making the transition research clinical practice. This paper considers key factors contributing issue, explores how federated (FL) may provide solution future of digital health highlights challenges considerations that need addressed.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Advances and Open Problems in Federated Learning"
      ],
      "abstract":[
        "The term Federated Learning was coined as recently 2016 to describe a machine learning setting where multiple entities collaborate in solving problem, under the coordination of central server or service provider. Each client\u2019s raw data is stored locally and not exchanged transferred; instead, focused updates intended for immediate aggregation are used achieve objective. Since then, topic has gathered much interest across many different disciplines realization that these interdisciplinary problems likely requires just but techniques from distributed optimization, cryptography, security, differential privacy, fairness, compressed sensing, systems, information theory, statistics, more. This monograph contributions leading experts disciplines, who latest state-of-the art their perspective. These have been carefully curated into comprehensive treatment enables reader understand work done get pointers effort required solve before can become reality practical systems. Researchers working area systems will find this an enlightening read may inspire them on challenging issues outlined. up speed quickly easily what increasingly important topic: Learning."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Metering Error Estimation of Fast-Charging Stations Using Charging Data\n  Analytics",
        "Discovering Directly-Follows Graph Model for Acyclic Processes",
        "The Impact of Artificial Intelligence on Emergency Medicine: A Review of\n  Recent Advances",
        "LlaMADRS: Prompting Large Language Models for Interview-Based Depression\n  Assessment",
        "Physical Layer Design for Ambient IoT",
        "Nice and precise $K^*(892) \\to K\\pi$ branching fractions",
        "S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate\n  Efficiency",
        "A plethora of long-range neutrino interactions probed by DUNE and T2HK",
        "SPRI: Aligning Large Language Models with Context-Situated Principles",
        "Discrete Markov Probabilistic Models",
        "Estimation of the generalized Laplace distribution and its projection\n  onto the circle",
        "Efficient Parallel Scheduling for Sparse Triangular Solvers",
        "Electron spin dynamics guide cell motility",
        "Towards More Trustworthy Deep Code Models by Enabling\n  Out-of-Distribution Detection",
        "Real-Time LiDAR Point Cloud Compression and Transmission for\n  Resource-constrained Robots",
        "How Well Can AI Build SD Models?",
        "Towards Interpretable Protein Structure Prediction with Sparse\n  Autoencoders",
        "HybriDNA: A Hybrid Transformer-Mamba2 Long-Range DNA Language Model",
        "A Unified Understanding and Evaluation of Steering Methods",
        "Pure $\\epsilon$-equilibrium in random games",
        "$B \\to \\rho \\ell \\bar{\\nu}$ resonance form factors from $B \\to \\pi\\pi\n  \\ell \\bar{\\nu}$ in lattice QCD",
        "Linear, nested, and quadratic ordered measures: Computation and\n  incorporation into optimization problems",
        "Fingerprint Matrix Concept for Detecting, Localizing and Characterizing\n  Targets in Complex Media",
        "Partitions with prescribed sum of reciprocals: asymptotic bounds",
        "The Role of Mobile and Social Media Services in Enhancing Freedom of\n  Expression: Opportunities, Challenges, and Prospects for Local Platform\n  Development in Uganda's Digital Ecosystem",
        "The Power of Perturbation under Sampling in Solving Extensive-Form Games",
        "You Can't Eat Your Cake and Have It Too: The Performance Degradation of\n  LLMs with Jailbreak Defense",
        "A Randomised Approach to Distributed Sorting",
        "RITHMS : An advanced stochastic framework for the simulation of\n  transgenerational hologenomic data"
      ],
      "abstract":[
        "Accurate electric energy metering (EEM) of fast charging stations (FCSs),\nserving as critical infrastructure in the electric vehicle (EV) industry and as\nsignificant carriers of vehicle-to-grid (V2G) technology, is the cornerstone\nfor ensuring fair electric energy transactions. Traditional on-site\nverification methods, constrained by their high costs and low efficiency,\nstruggle to keep pace with the rapid global expansion of FCSs. In response,\nthis paper adopts a data-driven approach and proposes the measuring performance\ncomparison (MPC) method. By utilizing the estimation value of state-of-charge\n(SOC) as a medium, MPC establishes comparison chains of EEM performance of\nmultiple FCSs. Therefore, the estimation of EEM errors for FCSs with high\nefficiency is enabled. Moreover, this paper summarizes the interfering factors\nof estimation results and establishes corresponding error models and\nuncertainty models. Also, a method for discriminating whether there are EEM\nperformance defects in FCSs is proposed. Finally, the feasibility of MPC method\nis validated, with results indicating that for FCSs with an accuracy grade of\n2\\%, the discriminative accuracy exceeds 95\\%. The MPC provides a viable\napproach for the online monitoring of EEM performance for FCSs, laying a\nfoundation for a fair and just electricity trading market.",
        "Process mining is the common name for a range of methods and approaches aimed\nat analysing and improving processes. Specifically, methods that aim to derive\nprocess models from event logs fall under the category of process discovery.\nWithin the range of processes, acyclic processes form a distinct category. In\nsuch processes, previously performed actions are not repeated, forming chains\nof unique actions. However, due to differences in the order of actions,\nexisting process discovery methods can provide models containing cycles even if\na process is acyclic. This paper presents a new process discovery algorithm\nthat allows to discover acyclic DFG models for acyclic processes. A model is\ndiscovered by partitioning an event log into parts that provide acyclic DFG\nmodels and merging them while avoiding the formation of cycles. The resulting\nalgorithm was tested both on real-life and artificial event logs. Absence of\ncycles improves model visual clarity and precision, also allowing to apply\ncycle-sensitive methods or visualisations to the model.",
        "Artificial Intelligence (AI) is revolutionizing emergency medicine by\nenhancing diagnostic processes and improving patient outcomes. This article\nprovides a review of the current applications of AI in emergency imaging\nstudies, focusing on the last five years of advancements. AI technologies,\nparticularly machine learning and deep learning, are pivotal in interpreting\ncomplex imaging data, offering rapid, accurate diagnoses and potentially\nsurpassing traditional diagnostic methods. Studies highlighted within the\narticle demonstrate AI's capabilities in accurately detecting conditions such\nas fractures, pneumothorax, and pulmonary diseases from various imaging\nmodalities including X-rays, CT scans, and MRIs. Furthermore, AI's ability to\npredict clinical outcomes like mechanical ventilation needs illustrates its\npotential in crisis resource optimization. Despite these advancements, the\nintegration of AI into clinical practice presents challenges such as data\nprivacy, algorithmic bias, and the need for extensive validation across diverse\nsettings. This review underscores the transformative potential of AI in\nemergency settings, advocating for a future where AI and clinical expertise\nsynergize to elevate patient care standards.",
        "This study introduces LlaMADRS, a novel framework leveraging open-source\nLarge Language Models (LLMs) to automate depression severity assessment using\nthe Montgomery-Asberg Depression Rating Scale (MADRS). We employ a zero-shot\nprompting strategy with carefully designed cues to guide the model in\ninterpreting and scoring transcribed clinical interviews. Our approach, tested\non 236 real-world interviews from the Context-Adaptive Multimodal Informatics\n(CAMI) dataset, demonstrates strong correlations with clinician assessments.\nThe Qwen 2.5--72b model achieves near-human level agreement across most MADRS\nitems, with Intraclass Correlation Coefficients (ICC) closely approaching those\nbetween human raters. We provide a comprehensive analysis of model performance\nacross different MADRS items, highlighting strengths and current limitations.\nOur findings suggest that LLMs, with appropriate prompting, can serve as\nefficient tools for mental health assessment, potentially increasing\naccessibility in resource-limited settings. However, challenges remain,\nparticularly in assessing symptoms that rely on non-verbal cues, underscoring\nthe need for multimodal approaches in future work.",
        "There is a growing demand for ultra low power and ultra low complexity\ndevices for applications which require maintenance-free and battery-less\noperation. One way to serve such applications is through backscatter devices,\nwhich communicate using energy harvested from ambient sources such as radio\nwaves transmitted by a reader. Traditional backscatter devices, such as RFID,\nare limited by range, interference, low connection density, and security\nissues. To address these problems, the Third Generation Partnership Project\n(3GPP) has started working on Ambient IoT (A-IoT). For the realization of A-IoT\ndevices, various aspects ranging from physical layer design, to the protocol\nstack, to the device architecture should be standardized. In this paper, we\nprovide an overview of the standardization efforts on the physical layer design\nfor A-IoT devices. The various physical channels and signals are discussed,\nfollowed by link level simulations to compare the performance of various\nconfigurations of reader to device and device to reader channels.",
        "Although discovered more than sixty years ago, direct measurement of the\n$K^*(892) \\to K\\pi$ branching fractions is a formidable challenge that has not\nbeen attempted. Typically they are assumed to obey the isospin limit in\nhundreds of particle data measurements. We show that an abundance of recent\namplitude analyses and other data, however, enables recovery of the ratios\n$\\mathcal{B}(K^{*+} \\to K^+ \\pi^0)\/\\mathcal{B}(K^{*+} \\to K_S^0 \\pi^+)$ and\n$4\\mathcal{B}(K^{*0} \\to K_S^0 \\pi^0)\/\\mathcal{B}(K^{*0} \\to K^+ \\pi^-)$ at\n$\\sim 5\\%$ precision.",
        "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious natural language processing (NLP) scenarios, but they still face\nchallenges when handling complex arithmetic and logical reasoning tasks. While\nChain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction\nstrategies have attempted to guide models in sequential, multi-step reasoning,\nMulti-agent Debate (MAD) has emerged as a viable approach for enhancing the\nreasoning capabilities of LLMs. By increasing both the number of agents and the\nfrequency of debates, the performance of LLMs improves significantly. However,\nthis strategy results in a significant increase in token costs, presenting a\nbarrier to scalability. To address this challenge, we introduce a novel\nsparsification strategy designed to reduce token costs within MAD. This\napproach minimizes ineffective exchanges of information and unproductive\ndiscussions among agents, thereby enhancing the overall efficiency of the\ndebate process. We conduct comparative experiments on multiple datasets across\nvarious models, demonstrating that our approach significantly reduces the token\ncosts in MAD to a considerable extent. Specifically, compared to MAD, our\napproach achieves an impressive reduction of up to 94.5\\% in token costs while\nmaintaining performance degradation below 2.0\\%.",
        "The next-generation neutrino oscillation experiments would be sensitive to\nthe new neutrino interactions that would strengthen the search for physics\nbeyond the Standard Model. In this context, we explore the capabilities of the\ntwo leading future long-baseline neutrino oscillation experiments, DUNE and\nT2HK, to search for new flavor-dependent neutrino interactions with electrons,\nprotons, and neutrons that could potentially modify neutrino flavor\ntransitions. We forecast their sensitivities in the context of long-range\nneutrino interactions mediated by a neutral vector boson lighter than\n$10^{-10}$ eV and sourced by the vast amount of nearby and distant matter in\nthe Earth, Moon, Sun, Milky Way, and local Universe. For the first time, we\nexplore a plethora of $U(1)^\\prime$ symmetries inducing the new interactions\nbuilt from the combination of lepton and baryon numbers. We find that in all\ncases, DUNE and T2HK may constrain or discover the existence of new long-range\nneutrino interaction, and in some favorable cases, may identify the new\n$U(1)^\\prime$ symmetry responsible for it. In this short proceeding, we only\nsummarize the prospects of constraining the new interaction in case of all our\ncandidate $U(1)^\\prime$ symmetries, which have been discussed in JHEP 09 (2024)\n055.",
        "Aligning Large Language Models to integrate and reflect human values,\nespecially for tasks that demand intricate human oversight, is arduous since it\nis resource-intensive and time-consuming to depend on human expertise for\ncontext-specific guidance. Prior work has utilized predefined sets of rules or\nprinciples to steer the behavior of models (Bai et al., 2022; Sun et al.,\n2023). However, these principles tend to be generic, making it challenging to\nadapt them to each individual input query or context. In this work, we present\nSituated-PRInciples (SPRI), a framework requiring minimal or no human effort\nthat is designed to automatically generate guiding principles in real-time for\neach input query and utilize them to align each response. We evaluate SPRI on\nthree tasks, and show that 1) SPRI can derive principles in a complex\ndomain-specific task that leads to on-par performance as expert-crafted ones;\n2) SPRI-generated principles lead to instance-specific rubrics that outperform\nprior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data\nleads to substantial improvement on truthfulness. We release our code and model\ngenerations at https:\/\/github.com\/honglizhan\/SPRI-public.",
        "This paper introduces the Discrete Markov Probabilistic Model (DMPM), a novel\nalgorithm for discrete data generation. The algorithm operates in the space of\nbits $\\{0,1\\}^d$, where the noising process is a continuous-time Markov chain\nthat can be sampled exactly via a Poissonian clock that flips labels uniformly\nat random. The time-reversal process, like the forward noise process, is a jump\nprocess, with its intensity governed by a discrete analogue of the classical\nscore function. Crucially, this intensity is proven to be the conditional\nexpectation of a function of the forward process, strengthening its theoretical\nalignment with score-based generative models while ensuring robustness and\nefficiency. We further establish convergence bounds for the algorithm under\nminimal assumptions and demonstrate its effectiveness through experiments on\nlow-dimensional Bernoulli-distributed datasets and high-dimensional binary\nMNIST data. The results highlight its strong performance in generating discrete\nstructures. This work bridges theoretical foundations and practical\napplications, advancing the development of effective and theoretically grounded\ndiscrete generative modeling.",
        "The generalized Laplace (GL) distribution, which falls in the larger family\nof generalized hyperbolic distributions, provides a versatile model to deal\nwith a variety of applications thanks to its shape parameters. The elliptically\nsymmetric GL admits a polar representation that can be used to yield a circular\ndistribution, which we call \\emph{projected} GL distribution. The latter does\nnot appear to have been considered yet in practical applications. In this\narticle, we explore an easy-to-implement maximum likelihood estimation strategy\nbased on Gaussian quadrature for the scale-mixture representation of the GL and\nits projection onto the circle. A simulation study is carried out to benchmark\nthe fitting routine against alternative estimation methods to assess its\nfeasibility, while the projected GL model is contrasted with other popular\ncircular distributions.",
        "We develop and analyze new scheduling algorithms for solving sparse\ntriangular linear systems (SpTRSV) in parallel. Our approach, which we call\nbarrier list scheduling, produces highly efficient synchronous schedules for\nthe forward- and backward-substitution algorithm. Compared to state-of-the-art\nbaselines HDagg and SpMP, we achieve a $3.24\\times$ and $1.45\\times$\ngeometric-mean speed-up, respectively. We achieve this by obtaining an up to\n$11\\times$ geometric-mean reduction in the number of synchronization barriers\nover HDagg, whilst maintaining a balanced workload, and by applying a matrix\nreordering step for locality. We show that our improvements are consistent\nacross a variety of input matrices and hardware architectures.",
        "Diverse organisms exploit the geomagnetic field (GMF) for migration.\nMigrating birds employ an intrinsically quantum mechanical mechanism for\ndetecting the geomagnetic field: absorption of a blue photon generates a\nradical pair whose two electrons precess at different rates in the magnetic\nfield, thereby sensitizing cells to the direction of the GMF. In this work,\nusing an in vitro injury model, we discovered a quantum-based mechanism of\ncellular migration. Specifically, we show that migrating cells detect the GMF\nvia an optically activated, electron spin-based mechanism. Cell injury provokes\nacute emission of blue photons, and these photons sensitize muscle progenitor\ncells to the magnetic field. We show that the magnetosensitivity of muscle\nprogenitor cells is (a) activated by blue light, but not by green or red light,\nand (b) disrupted by the application of an oscillatory field at the frequency\ncorresponding to the energy of the electron-spin\/magnetic field interaction. A\ncomprehensive analysis of protein expression reveals that the ability of blue\nphotons to promote cell motility is mediated by activation of calmodulin\ncalcium sensors. Collectively, these data suggest that cells possess a\nlight-dependent magnetic compass driven by electron spin dynamics.",
        "Numerous machine learning (ML) models have been developed, including those\nfor software engineering (SE) tasks, under the assumption that training and\ntesting data come from the same distribution. However, training and testing\ndistributions often differ, as training datasets rarely encompass the entire\ndistribution, while testing distribution tends to shift over time. Hence, when\nconfronted with out-of-distribution (OOD) instances that differ from the\ntraining data, a reliable and trustworthy SE ML model must be capable of\ndetecting them to either abstain from making predictions, or potentially\nforward these OODs to appropriate models handling other categories or tasks.\n  In this paper, we develop two types of SE-specific OOD detection models,\nunsupervised and weakly-supervised OOD detection for code. The unsupervised OOD\ndetection approach is trained solely on in-distribution samples while the\nweakly-supervised approach utilizes a tiny number of OOD samples to further\nenhance the detection performance in various OOD scenarios. Extensive\nexperimental results demonstrate that our proposed methods significantly\noutperform the baselines in detecting OOD samples from four different scenarios\nsimultaneously and also positively impact a main code understanding task.",
        "LiDARs are widely used in autonomous robots due to their ability to provide\naccurate environment structural information. However, the large size of point\nclouds poses challenges in terms of data storage and transmission. In this\npaper, we propose a novel point cloud compression and transmission framework\nfor resource-constrained robotic applications, called RCPCC. We iteratively fit\nthe surface of point clouds with a similar range value and eliminate redundancy\nthrough their spatial relationships. Then, we use Shape-adaptive DCT (SA-DCT)\nto transform the unfit points and reduce the data volume by quantizing the\ntransformed coefficients. We design an adaptive bitrate control strategy based\non QoE as the optimization goal to control the quality of the transmitted point\ncloud. Experiments show that our framework achieves compression rates of\n40$\\times$ to 80$\\times$ while maintaining high accuracy for downstream\napplications. our method significantly outperforms other baselines in terms of\naccuracy when the compression rate exceeds 70$\\times$. Furthermore, in\nsituations of reduced communication bandwidth, our adaptive bitrate control\nstrategy demonstrates significant QoE improvements. The code will be available\nat https:\/\/github.com\/HITSZ-NRSL\/RCPCC.git.",
        "Introduction: As system dynamics (SD) embraces automation, AI offers\nefficiency but risks bias from missing data and flawed models. Models that omit\nmultiple perspectives and data threaten model quality, whether created by\nhumans or with the assistance of AI. To reduce uncertainty about how well AI\ncan build SD models, we introduce two metrics for evaluation of AI-generated\ncausal maps: technical correctness (causal translation) and adherence to\ninstructions (conformance).\n  Approach: We developed an open source project called sd-ai to provide a basis\nfor collaboration in the SD community, aiming to fully harness the potential of\nAI based tools like ChatGPT for dynamic modeling. Additionally, we created an\nevaluation theory along with a comprehensive suite of tests designed to\nevaluate any such tools developed within the sd-ai ecosystem.\n  Results: We tested 11 different LLMs on their ability to do causal\ntranslation as well as conform to user instruction. gpt-4.5-preview was the top\nperformer, scoring 92.9% overall, excelling in both tasks. o1 scored 100% in\ncausal translation. gpt-4o identified all causal links but struggled with\npositive polarity in decreasing terms. While gpt-4.5-preview and o1 are most\naccurate, gpt-4o is the cheapest.\n  Discussion: Causal translation and conformance tests applied to the sd-ai\nengine reveal significant variations across lLLMs, underscoring the need for\ncontinued evaluation to ensure responsible development of AI tools for dynamic\nmodeling. To address this, an open collaboration among tool developers,\nmodelers, and stakeholders is launched to standardize measures for evaluating\nthe capacity of AI tools to improve the modeling process.",
        "Protein language models have revolutionized structure prediction, but their\nnonlinear nature obscures how sequence representations inform structure\nprediction. While sparse autoencoders (SAEs) offer a path to interpretability\nhere by learning linear representations in high-dimensional space, their\napplication has been limited to smaller protein language models unable to\nperform structure prediction. In this work, we make two key advances: (1) we\nscale SAEs to ESM2-3B, the base model for ESMFold, enabling mechanistic\ninterpretability of protein structure prediction for the first time, and (2) we\nadapt Matryoshka SAEs for protein language models, which learn hierarchically\norganized features by forcing nested groups of latents to reconstruct inputs\nindependently. We demonstrate that our Matryoshka SAEs achieve comparable or\nbetter performance than standard architectures. Through comprehensive\nevaluations, we show that SAEs trained on ESM2-3B significantly outperform\nthose trained on smaller models for both biological concept discovery and\ncontact map prediction. Finally, we present an initial case study demonstrating\nhow our approach enables targeted steering of ESMFold predictions, increasing\nstructure solvent accessibility while fixing the input sequence. To facilitate\nfurther investigation by the broader community, we open-source our code,\ndataset, pretrained models https:\/\/github.com\/johnyang101\/reticular-sae , and\nvisualizer https:\/\/sae.reticular.ai .",
        "Advances in natural language processing and large language models have\nsparked growing interest in modeling DNA, often referred to as the \"language of\nlife\". However, DNA modeling poses unique challenges. First, it requires the\nability to process ultra-long DNA sequences while preserving single-nucleotide\nresolution, as individual nucleotides play a critical role in DNA function.\nSecond, success in this domain requires excelling at both generative and\nunderstanding tasks: generative tasks hold potential for therapeutic and\nindustrial applications, while understanding tasks provide crucial insights\ninto biological mechanisms and diseases. To address these challenges, we\npropose HybriDNA, a decoder-only DNA language model that incorporates a hybrid\nTransformer-Mamba2 architecture, seamlessly integrating the strengths of\nattention mechanisms with selective state-space models. This hybrid design\nenables HybriDNA to efficiently process DNA sequences up to 131kb in length\nwith single-nucleotide resolution. HybriDNA achieves state-of-the-art\nperformance across 33 DNA understanding datasets curated from the BEND, GUE,\nand LRB benchmarks, and demonstrates exceptional capability in generating\nsynthetic cis-regulatory elements (CREs) with desired properties. Furthermore,\nwe show that HybriDNA adheres to expected scaling laws, with performance\nimproving consistently as the model scales from 300M to 3B and 7B parameters.\nThese findings underscore HybriDNA's versatility and its potential to advance\nDNA research and applications, paving the way for innovations in understanding\nand engineering the \"language of life\".",
        "Steering methods provide a practical approach to controlling large language\nmodels by applying steering vectors to intermediate activations, guiding\noutputs toward desired behaviors while avoiding retraining. Despite their\ngrowing importance, the field lacks a unified understanding and consistent\nevaluation across tasks and datasets, hindering progress. This paper introduces\na unified framework for analyzing and evaluating steering methods, formalizing\ntheir core principles and offering theoretical insights into their\neffectiveness. Through comprehensive empirical evaluations on multiple-choice\nand open-ended text generation tasks, we validate these insights, identifying\nkey factors that influence performance and demonstrating the superiority of\ncertain methods. Our work bridges theoretical and practical perspectives,\noffering actionable guidance for advancing the design, optimization, and\ndeployment of steering methods in LLMs.",
        "We show that for any $\\epsilon>0$ the probability that a randomly drawn game\nhas a pure $\\epsilon$-equilibrium goes to 1 as the number of agents gets large.\nThis contrasts sharply with the known fact that if $\\epsilon = 0$, that is, for\npure Nash equilibrium, the probability is asymptotically $1- 1\/e\\approx 0.63$.",
        "The decay $B \\to \\rho \\ell \\bar{\\nu}$ is an attractive process for\ndetermining the magnitude of the smallest CKM matrix element, $|V_{ub}|$, and\ncan provide new insights into the origin of the long-standing\nexclusive-inclusive discrepancy in determinations of this Standard-Model\nparameter. This requires a nonperturbative QCD calculation of the $B \\to \\rho$\nform factors $V$, $A_0$, $A_1$, and $A_{12}$. The unstable nature of the $\\rho$\nresonance has prevented precise lattice QCD calculations of these form factors\nto date. Here, we present the first lattice QCD calculation of the $B \\to \\rho$\nform factors in which the $\\rho$ is treated properly as a resonance in $P$-wave\n$\\pi \\pi$ scattering. To this end, we use the Lellouch-L\\\"uscher finite-volume\nformalism to compute the $B \\to \\pi \\pi$ form factors as a function of both\nmomentum transfer and $\\pi \\pi$ invariant mass, and then analytically continue\nto the $\\rho$ resonance pole. This calculation is performed with $2+1$\ndynamical quark flavors at a pion mass of approximately 320 MeV, and\ndemonstrates a clear path toward results at the physical point.",
        "In this paper we address a unified mathematical optimization framework to\ncompute a wide range of measures used in most operations research and data\nscience contexts. The goal is to embed such metrics within general optimization\nmodels allowing their efficient computation. We assess the usefulness of this\napproach applying it to three different families of measures, namely linear,\nnested, and quadratic ordered measures. Computational results are reported\nshowing the efficiency and accuracy of our methods as compared with standard\nimplementations in numerical software packages. Finally, we illustrate this\nmethodology by computing a number of optimal solutions with respect to\ndifferent metrics on three well-known linear and combinatorial optimization\nproblems: scenario analysis in linear programming, the traveling salesman and\nthe weighted multicover set problem.",
        "As waves propagate through a complex medium, they undergo multiple scattering\nevents. This phenomenon is detrimental to imaging, as it causes a full blurring\nof the image beyond a transport mean free path. Here, we show how to detect,\nlocalize, and characterize any scattering target through the reflection matrix\nof the complex medium in which this target is embedded and thus hidden from\ndirect view. More precisely, we introduce a fingerprint operator that contains\nthe specific signature of the target with respect to its environment. Applied\nto the recorded reflection matrix, this operator provides a likelihood index of\nthe target in any given state, despite the scattering fog induced by the\nsurrounding environment. This state can be the target position for localization\npurposes, its shape for characterization, or any other parameter that\ninfluences the target response. Our concept is versatile and broadly applicable\nto different type of waves for which multi-element technology allows a\nreflection matrix to be measured. We demonstrate this here explicitly by\nperforming different proof-of-concept experiments with ultrasound on targets\nburied inside a strongly scattering granular suspension, on lesion markers for\nclinical applications, and on the architecture of muscle tissue.",
        "In $1963$ Graham proved that every positive integer $n \\ge 78$ can be written\nas a sum of distinct positive integers $a_1, a_2, \\ldots, a_r$ for which\n$\\frac{1}{a_1} + \\frac{1}{a_2} + \\ldots + \\frac{1}{a_r}$ is equal to $1$. In\nthe same paper he managed to further generalize this, and showed that for all\npositive rationals $\\alpha$ and all positive integers $m$, there exists an\n$n_{\\alpha, m}$ such that every positive integer $n \\ge n_{\\alpha, m}$ has a\npartition with distinct parts, all larger than or equal to $m$, and such that\nthe sum of reciprocals is equal to $\\alpha$. No attempt was made to estimate\nthe quantity $n_{\\alpha, m}$, however. With $n_{\\alpha} := n_{\\alpha, 1}$, in\nthis paper we provide near-optimal upper bounds on $n_{\\alpha}$ and $n_{\\alpha,\nm}$, as well as bounds on the cardinality of the set $\\{\\alpha : n_{\\alpha} \\le\nn\\}$.",
        "Utilizing mobile and social media platforms is a transformative approach to\nenhancing freedom of expression and fostering digital engagement. However,\nUganda's digital ecosystem faces challenges such as restrictive legislation,\nfinancial barriers, and the absence of localized platforms tailored to cultural\ncontexts. This study employed a mixed-methods approach to explore how these\nplatforms influence public discourse, activism, and civic participation while\nhighlighting opportunities for local innovation. The research further\nidentified the critical need for regulatory reforms, investments in digital\nliteracy, and collaborative efforts to develop sustainable and culturally\nrelevant platforms, ensuring a more inclusive and empowered digital society.\n  Keywords: Freedom of Expression, Mobile Services, Social Media Platforms,\nLocal Digital Innovation, Uganda's Digital Ecosystem",
        "This paper investigates how perturbation does and does not improve the\nFollow-the-Regularized-Leader (FTRL) algorithm in imperfect-information\nextensive-form games. Perturbing the expected payoffs guarantees that the FTRL\ndynamics reach an approximate equilibrium, and proper adjustments of the\nmagnitude of the perturbation lead to a Nash equilibrium (\\textit{last-iterate\nconvergence}). This approach is robust even when payoffs are estimated using\nsampling -- as is the case for large games -- while the optimistic approach\noften becomes unstable. Building upon those insights, we first develop a\ngeneral framework for perturbed FTRL algorithms under \\textit{sampling}. We\nthen empirically show that in the last-iterate sense, the perturbed FTRL\nconsistently outperforms the non-perturbed FTRL. We further identify a\ndivergence function that reduces the variance of the estimates for perturbed\npayoffs, with which it significantly outperforms the prior algorithms on Leduc\npoker (whose structure is more asymmetric in a sense than that of the other\nbenchmark games) and consistently performs smooth convergence behavior on all\nthe benchmark games.",
        "With the rise of generative large language models (LLMs) like LLaMA and\nChatGPT, these models have significantly transformed daily life and work by\nproviding advanced insights. However, as jailbreak attacks continue to\ncircumvent built-in safety mechanisms, exploiting carefully crafted scenarios\nor tokens, the safety risks of LLMs have come into focus. While numerous\ndefense strategies--such as prompt detection, modification, and model\nfine-tuning--have been proposed to counter these attacks, a critical question\narises: do these defenses compromise the utility and usability of LLMs for\nlegitimate users? Existing research predominantly focuses on the effectiveness\nof defense strategies without thoroughly examining their impact on performance,\nleaving a gap in understanding the trade-offs between LLM safety and\nperformance. Our research addresses this gap by conducting a comprehensive\nstudy on the utility degradation, safety elevation, and exaggerated-safety\nescalation of LLMs with jailbreak defense strategies. We propose USEBench, a\nnovel benchmark designed to evaluate these aspects, along with USEIndex, a\ncomprehensive metric for assessing overall model performance. Through\nexperiments on seven state-of-the-art LLMs, we found that mainstream jailbreak\ndefenses fail to ensure both safety and performance simultaneously. Although\nmodel-finetuning performs the best overall, their effectiveness varies across\nLLMs. Furthermore, vertical comparisons reveal that developers commonly\nprioritize performance over safety when iterating or fine-tuning their LLMs.",
        "We introduce and analyse a new, extremely simple, randomised sorting\nalgorithm:\n  - choose a pair of indices $\\{i, j\\}$ according to some distribution $q$;\n  - sort the elements in positions $i$ and $j$ of the array in ascending order.\n  Choosing $q_{\\{i,j\\}} \\propto 1\/|j - i|$ yields an order-$n (\\log n)^2$\nsorting time. We call it the harmonic sorter.\n  The sorter trivially parallelises in the asynchronous setting, yielding a\nlinear speed-up. We also exhibit a low-communication, synchronous version with\na linear speed-up.\n  We compare and contrast this algorithm with other sorters, and discuss some\nof its benefits, particularly its robustness and amenability to parallelisation\nand distributed computing.",
        "A holobiont is made up of a host organism together with its microbiota. In\nthe context of animal breeding, as the holobiont can be viewed as the single\nunit upon which selection operates, integrating microbiota data into genomic\nprediction models may be a promising approach to improve predictions of\nphenotypic and genetic values. Nevertheless, there is a paucity of hologenomic\ntransgenerational data to address this hypothesis, and thus to fill this gap,\nwe propose a new simulation framework. Our approach, an R Implementation of a\nTransgenerational Hologenomic Model-based Simulator (RITHMS) is an open-source\npackage, builds upon the MoBPS package and incorporates distinctive\ncharacteristics of the microbiota, notably vertical and horizontal transmission\nas well as modulation due to the environment and host genetics. In addition,\nRITHMS can account for a variety of selection strategies and is adaptable to\ndifferent genetic architectures. We simulated transgenerational hologenomic\ndata using RITHMS under a wide variety of scenarios, varying heritability,\nmicrobiability, and microbiota heritability. We found that simulated data\naccurately reflected expected characteristics, notably based on microbial\ndiversity metrics, correlation between taxa, modulation of vertical and\nhorizontal transmission, response to environmental effects and the evolution of\nphenotypic values depending on selection strategy. Our results support the\nrelevance of our simulation framework and illustrate its possible use for\nbuilding a selection index balancing genetic gain and microbial diversity.\nRITHMS is an advanced, flexible tool for generating transgenerational\nhologenomic data that incorporate the complex interplay between genetics,\nmicrobiota and environment."
      ]
    }
  },
  {
    "id":2411.16464,
    "research_type":"basic",
    "start_id":"b2",
    "start_title":"Meeting Strangers and Friends of Friends: How Random Are Social Networks?",
    "start_abstract":"We present a dynamic model of network formation where nodes find other with whom to form links in two ways: some are found uniformly at random, while others by searching locally through the current structure (e.g., meeting friends friends). This combination processes results spectrum features exhibited large social networks, including presence more high- and low-degree than when formed independently having low distances between network, high clustering on local level. fit data from six networks impute relative ratio random network-based meetings link formation, which turns out vary dramatically across applications. show that as random\/network-based varies, resulting degree distributions can be ordered sense stochastic dominance, allows us infer how process affects average utility network. (JEL D85, Z13)",
    "start_categories":[
      "q-fin.EC"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "An agent-based spatial urban social network generator: A case study of beijing, china"
      ],
      "abstract":[
        "This paper proposes an agent-based spatial social network model, which combines a utility function and heuristic algorithms, to formulate friendships of agents in a given synthetic population comprising individuals and households, as well as their attributes and locations. In order to better and explicitly represent the real social networks, the model attempts to generate both close and somewhat close social networks by linking agents with either close or somewhat close friendships, fitting both distributions of network degree and transitivity, which are two basic characteristics of a network. Here, a utility function, which incorporates the similarity between agents in individual attributes (e.g., sex), as well as the spatial closeness of their residential locations and workplaces, is developed to judge whether a friendship between a pair of agents can be built. Furthermore, the social network model is developed as a key component of an agent-and Geographic Information System (GIS)-based virtual city creator that is a set of synthesis methods used to generate spatially disaggregate urban data. Finally, Beijing, China is used as a case study. Both close and somewhat close social networks are generated with the target and generated distributions well matched, and the generated networks are further analysed from a geographical perspective."
      ],
      "categories":[
        "cs.CE"
      ]
    },
    "list":{
      "title":[
        "Ferroelectric Properties of van der Waals Chalcogenides: DFT perspective",
        "Vision Transformers on the Edge: A Comprehensive Survey of Model\n  Compression and Acceleration Strategies",
        "Contact process for the spread of knowledge",
        "Site-Decorated Model for Unconventional Frustrated Magnets: Ultranarrow\n  Phase Crossover and Spin Reversal Transition",
        "Path-dependency and emergent computing under vectorial driving",
        "Enhancing the De-identification of Personally Identifiable Information\n  in Educational Data",
        "Multi-compartment diffusion-relaxation MR signal representation in the\n  spherical 3D-SHORE basis",
        "Advancing ATLAS DCS Data Analysis with a Modern Data Platform",
        "Non-linear Partition of Unity method",
        "Network fault costs based on minimum leaf spanning trees",
        "Study of long-term spectral evolution and X-ray and Gamma-ray\n  correlation of blazars seen by HAWC",
        "Optimized Relay Lens Design For High-Resolution Image Transmission In\n  Military Target Detection Systems",
        "Reinforcement Learning in Strategy-Based and Atari Games: A Review of\n  Google DeepMinds Innovations",
        "Detection of Somali-written Fake News and Toxic Messages on the Social\n  Media Using Transformer-based Language Models",
        "Managing target of opportunity (ToO) observations at Observatorio\n  Astrof\\'isico de Javalambre (OAJ)",
        "Landau-level composition of bound exciton states in magnetic field",
        "Foundations of Digital Circuits: Denotation, Operational, and Algebraic\n  Semantics",
        "AI-based Identity Fraud Detection: A Systematic Review",
        "On stability of exponentially subelliptic harmonic maps",
        "Numerical evaluation of Gaussian mixture entropy",
        "A Homology Theory for the Semimodules of Radical Submodules",
        "Annotating Scientific Uncertainty: A comprehensive model using\n  linguistic patterns and comparison with existing approaches",
        "Acc3D: Accelerating Single Image to 3D Diffusion Models via Edge\n  Consistency Guided Score Distillation",
        "Incorporating Backreaction in One-Loop Corrections in Ultra-Slow-Roll\n  Inflation",
        "Several-variable Kronecker limit formula over global function fields",
        "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation",
        "LSM Trees in Adversarial Environments",
        "Pulsar scattering as a probe for structures in the interstellar medium",
        "Orbital Signatures of Density Wave Transition in La3Ni2O7-delta and\n  La2PrNi2O7-delta RP-Nickelates Probed via in-situ X-ray Absorption Near-edge\n  Spectroscopy"
      ],
      "abstract":[
        "Layered materials with non-centrosymmetric stacking order are attracting\nincreasing interest due to the presence of ferroelectric polarization, which is\ndictated by weak interlayer hybridization of atomic orbitals. Here, we use\ndensity functional theory modelling to systematically build a library of van\nder Waals chalcogenides that exhibit substantial ferroelectric polarization.\nFor the most promising materials, we also analyse the pressure dependence of\nthe ferroelectric effect and charge accumulation of photo-induced electrons and\nholes at surfaces and internal twin boundaries in thin films of such materials.",
        "In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), tensor processing units (TPUs), and\nfield-programmable gate arrays (FPGAs). The goal is to inspire further research\nwith a contemporary guide on optimizing ViTs for efficient deployment on edge\ndevices.",
        "This paper is concerned with a natural variant of the contact process\nmodeling the spread of knowledge on the integer lattice. Each site is\ncharacterized by its knowledge, measured by a real number ranging from 0 =\nignorant to 1 = omniscient. Neighbors interact at rate $\\lambda$, which results\nin both neighbors attempting to teach each other a fraction $\\mu$ of their\nknowledge, and individuals die at rate one, which results in a new individual\nwith no knowledge. Starting with a single omniscient site, our objective is to\nstudy whether the total amount of knowledge on the lattice converges to zero\n(extinction) or remains bounded away from zero (survival). The process dies out\nwhen $\\lambda \\leq \\lambda_c$ and\/or $\\mu = 0$, where $\\lambda_c$ denotes the\ncritical value of the contact process. In contrast, we prove that, for all\n$\\lambda > \\lambda_c$, there is a unique phase transition in the direction of\n$\\mu$, and for all $\\mu > 0$, there is a unique phase transition in the\ndirection of $\\lambda$. Our proof of survival relies on block constructions\nshowing more generally convergence of the knowledge to infinity, while our\nproof of extinction relies on martingale techniques showing more generally an\nexponential decay of the knowledge.",
        "The site-decorated Ising model is introduced to advance the understanding and\nexperimental realization of the recently discovered one-dimensional\nfinite-temperature ultranarrow phase crossover in an external magnetic field,\nwhile mitigating the geometric complexities of traditional bond-decorated\nmodels. Furthermore, although higher-dimensional Ising models in an external\nfield remain unsolved, an exact solution for a novel spin-reversal transition\n-- driven by an exotic, hidden ``half-ice, half-fire'' state induced by site\ndecoration -- is derived. This transition, triggered by a slight variation in\ntemperature or magnetic field even in the weak-field limit, offers a promising\nroute toward energy-efficient applications such as data storage and processing.\nThe results establish site decoration as a compelling new avenue for materials\nand device design, particularly in systems such as mixed $d$-$f$ compounds,\noptical lattices, and neural networks.",
        "The sequential response of frustrated materials - ranging from crumpled\nsheets and amorphous media to metamaterials - reveals their memory effects and\nemergent computational potential. Despite their spatial extension, most studies\nrely on a single global stimulus, such as compression, effectively reducing the\nproblem to scalar driving. Here, we introduce vectorial driving by applying\nmultiple spatially localized stimuli to explore path-dependent, sequential\nresponses. We uncover a wealth of phenomena absent in scalar driving, including\nnon-Abelian responses, mixed-mode behavior, and chiral loop transients. We find\nthat such path dependencies arise from elementary motifs linked to fold\nsingularities, which connect triplets of states - ancestor, descendant, and\nsibling; and develop a general framework using pt-graphs to describe responses\nunder any vectorial driving protocol. Leveraging binarized vectorial driving,\nwe establish a natural connection to computation, showing that a single sample\ncan encode multiple sequential Boolean circuits, which are selectable by\ndriving strength and reprogrammable via additional inputs. Finally, we\nintroduce graph-based motifs to manage the complexity of high-dimensional\ndriving. Our work paves the way for strategies to explore, harness, and\nunderstand complex materials and memory, while advancing embodied intelligence\nand in-materia computing.",
        "Protecting Personally Identifiable Information (PII), such as names, is a\ncritical requirement in learning technologies to safeguard student and teacher\nprivacy and maintain trust. Accurate PII detection is an essential step toward\nanonymizing sensitive information while preserving the utility of educational\ndata. Motivated by recent advancements in artificial intelligence, our study\ninvestigates the GPT-4o-mini model as a cost-effective and efficient solution\nfor PII detection tasks. We explore both prompting and fine-tuning approaches\nand compare GPT-4o-mini's performance against established frameworks, including\nMicrosoft Presidio and Azure AI Language. Our evaluation on two public\ndatasets, CRAPII and TSCC, demonstrates that the fine-tuned GPT-4o-mini model\nachieves superior performance, with a recall of 0.9589 on CRAPII. Additionally,\nfine-tuned GPT-4o-mini significantly improves precision scores (a threefold\nincrease) while reducing computational costs to nearly one-tenth of those\nassociated with Azure AI Language. Furthermore, our bias analysis reveals that\nthe fine-tuned GPT-4o-mini model consistently delivers accurate results across\ndiverse cultural backgrounds and genders. The generalizability analysis using\nthe TSCC dataset further highlights its robustness, achieving a recall of\n0.9895 with minimal additional training data from TSCC. These results emphasize\nthe potential of fine-tuned GPT-4o-mini as an accurate and cost-effective tool\nfor PII detection in educational data. It offers robust privacy protection\nwhile preserving the data's utility for research and pedagogical analysis. Our\ncode is available on GitHub: https:\/\/github.com\/AnonJD\/PrivacyAI",
        "Modelling the diffusion-relaxation magnetic resonance (MR) signal obtained\nfrom multi-parametric sequences has recently gained immense interest in the\ncommunity due to new techniques significantly reducing data acquisition time. A\npreferred approach for examining the diffusion-relaxation MR data is to follow\nthe continuum modelling principle that employs kernels to represent the tissue\nfeatures, such as the relaxations or diffusion properties. However,\nconstructing reasonable dictionaries with predefined signal components depends\non the sampling density of model parameter space, thus leading to a geometrical\nincrease in the number of atoms per extra tissue parameter considered in the\nmodel. That makes estimating the contributions from each atom in the signal\nchallenging, especially considering diffusion features beyond the\nmono-exponential decay.\n  This paper presents a new Multi-Compartment diffusion-relaxation MR signal\nrepresentation based on the Simple Harmonic Oscillator-based Reconstruction and\nEstimation (MC-SHORE) representation, compatible with scattered acquisitions.\nThe proposed technique imposes sparsity constraint on the solution via the\n$\\ell_1$ norm and enables the estimation of the microstructural measures, such\nas the return-to-the-origin probability, and the orientation distribution\nfunction, depending on the compartments considered in a single voxel. The\nprocedure has been verified with in silico and in vivo data and enabled the\napproximation of the diffusion-relaxation MR signal more accurately than\nsingle-compartment non-Gaussian representations and multi-compartment\nmono-exponential decay techniques, maintaining a low number of atoms in the\ndictionary. Ultimately, the MC-SHORE procedure allows for separating\nintra-\/extra-axonal and free water contributions from the signal, thus reducing\nthe partial volume effect observable in the boundaries of the tissues.",
        "This paper presents a modern and scalable framework for analyzing Detector\nControl System (DCS) data from the ATLAS experiment at CERN. The DCS data,\nstored in an Oracle database via the WinCC OA system, is optimized for\ntransactional operations, posing challenges for large-scale analysis across\nextensive time periods and devices. To address these limitations, we developed\na data pipeline using Apache Spark, CERN's Hadoop service, and the CERN SWAN\nplatform. This framework integrates seamlessly with Python notebooks, providing\nan accessible and efficient environment for data analysis using\nindustry-standard tools. The approach has proven effective in troubleshooting\nData Acquisition (DAQ) links for the ATLAS New Small Wheel (NSW) detector,\ndemonstrating the value of modern data platforms in enabling detector experts\nto quickly identify and resolve critical issues.",
        "This paper introduces the Non-linear Partition of Unity Method, a novel\ntechnique integrating Radial Basis Function interpolation and Weighted\nEssentially Non-Oscillatory algorithms. It addresses challenges in\nhigh-accuracy approximations, particularly near discontinuities, by adapting\nweights dynamically. The method is rooted in the Partition of Unity framework,\nenabling efficient decomposition of large datasets into subproblems while\nmaintaining accuracy. Smoothness indicators and compactly supported functions\nensure precision in regions with discontinuities. Error bounds are calculated\nand validate its effectiveness, showing improved interpolation in discontinuous\nand smooth regions. Some numerical experiments are performed to check the\ntheoretical results.",
        "We study the fault-tolerance of networks from both the structural and\ncomputational point of view using the minimum leaf number of the corresponding\ngraph $G$, i.e. the minimum number of leaves of the spanning trees of $G$, and\nits vertex-deleted subgraphs. We investigate networks that are leaf-guaranteed,\ni.e. which satisfy a certain stability condition with respect to minimum leaf\nnumbers and vertex-deletion. Next to this, our main notion is the so-called\nfault cost, which is based on the number of vertices that have different\ndegrees in minimum leaf spanning trees of the network and its vertex-deleted\nsubgraphs. We characterise networks with vanishing fault cost via\nleaf-guaranteed graphs and describe, for any given network $N$, leaf-guaranteed\nnetworks containing $N$. We determine for all non-negative integers $k \\le 8$\nexcept $1$ the smallest network with fault cost $k$. We also give a detailed\ntreatment of the fault cost $1$ case, prove that there are infinitely many\n$3$-regular networks with fault cost $3$, and show that for any non-negative\ninteger $k$ there exists a network with fault cost exactly $k$.",
        "The HAWC Observatory collected 6 years of extensive data, providing an ideal\nplatform for long-term monitoring of blazars in the Very High Energy (VHE)\nband, without bias towards specific flux states. HAWC continuously monitors\nblazar activity at TeV energies, focusing on sources with a redshift of {z \\lt\n0.3}, based on the Third Fermi-LAT Catalog of High-Energy sources. We\nspecifically focused our analysis on Mrk 421 and Mrk 501, as they are the\nbrightest blazars observed by the HAWC Observatory. With a dataset of 2143\ndays, this work significantly extends the monitoring previously published,\nwhich was based on 511 days of observation. By utilizing HAWC data for the VHE\n{\\gamma}-ray emission in the 300 GeV to 100 TeV energy range, in conjunction\nwith Swift-XRT data for the 0.3 to 10 keV X-ray emission, we aim to explore\npotential correlations between these two bands. For Mrk 501, we found evidence\nof a long-term correlation. Additionally, we identified a period in the light\ncurve where the flux was very low for more than two years. On the other hand,\nour analysis of Mrk 421 measured a strong linear correlation for\nquasi-simultaneous observations collected by HAWC and Swift-XRT. This result is\nconsistent with a linear dependence and a multiple-zone synchrotron\nself-Compton model to explain the X-ray and the {\\gamma}-ray emission. Finally,\nas suggested by previous findings, we confirm a harder-when-brighter behavior\nin the spectral evolution of the flux properties for Mrk 421. These findings\ncontribute to the understanding of blazar emissions and their underlying\nmechanisms.",
        "The design and performance analysis of relay lenses that provide\nhigh-performance image transmission for target acquisition and tracking in\nmilitary optical systems. Relay lenses are critical components for clear and\nlossless image transmission over long distances. In this study, the optical\nperformance of a relay lens system designed and optimized using ZEMAX software\nis investigated in detail. The analysis focuses on important optical properties\nsuch as modulation transfer function (MTF), spot diagrams, Seidel diagram,\nfield curvature and distortion. The results show that the lens has significant\npotential in military applications for target detection and tracking with high\nresolution and low aberration.",
        "Reinforcement Learning (RL) has been widely used in many applications,\nparticularly in gaming, which serves as an excellent training ground for AI\nmodels. Google DeepMind has pioneered innovations in this field, employing\nreinforcement learning algorithms, including model-based, model-free, and deep\nQ-network approaches, to create advanced AI models such as AlphaGo, AlphaGo\nZero, and MuZero. AlphaGo, the initial model, integrates supervised learning\nand reinforcement learning to master the game of Go, surpassing professional\nhuman players. AlphaGo Zero refines this approach by eliminating reliance on\nhuman gameplay data, instead utilizing self-play for enhanced learning\nefficiency. MuZero further extends these advancements by learning the\nunderlying dynamics of game environments without explicit knowledge of the\nrules, achieving adaptability across various games, including complex Atari\ngames. This paper reviews the significance of reinforcement learning\napplications in Atari and strategy-based games, analyzing these three models,\ntheir key innovations, training processes, challenges encountered, and\nimprovements made. Additionally, we discuss advancements in the field of\ngaming, including MiniZero and multi-agent models, highlighting future\ndirections and emerging AI models from Google DeepMind.",
        "The fact that everyone with a social media account can create and share\ncontent, and the increasing public reliance on social media platforms as a news\nand information source bring about significant challenges such as\nmisinformation, fake news, harmful content, etc. Although human content\nmoderation may be useful to an extent and used by these platforms to flag\nposted materials, the use of AI models provides a more sustainable, scalable,\nand effective way to mitigate these harmful contents. However, low-resourced\nlanguages such as the Somali language face limitations in AI automation,\nincluding scarce annotated training datasets and lack of language models\ntailored to their unique linguistic characteristics. This paper presents part\nof our ongoing research work to bridge some of these gaps for the Somali\nlanguage. In particular, we created two human-annotated social-media-sourced\nSomali datasets for two downstream applications, fake news \\& toxicity\nclassification, and developed a transformer-based monolingual Somali language\nmodel (named SomBERTa) -- the first of its kind to the best of our knowledge.\nSomBERTa is then fine-tuned and evaluated on toxic content, fake news and news\ntopic classification datasets. Comparative evaluation analysis of the proposed\nmodel against related multilingual models (e.g., AfriBERTa, AfroXLMR, etc)\ndemonstrated that SomBERTa consistently outperformed these comparators in both\nfake news and toxic content classification tasks while achieving the best\naverage accuracy (87.99%) across all tasks. This research contributes to Somali\nNLP by offering a foundational language model and a replicable framework for\nother low-resource languages, promoting digital and AI inclusivity and\nlinguistic diversity.",
        "The Observatorio Astrof\\'isico de Javalambre (OAJ) is a Spanish astronomical\nICTS (Unique Scientific and Technical Infrastructures) located in the Sierra de\nJavalambre in Teruel (Spain). It has been particularly conceived for carrying\nout large-sky multi-filter surveys. As an ICTS, the OAJ offers Open Time to the\nastronomical community, offering more than 25% through Legacy Surveys, Regular\nPrograms (RP) and Director discretionary time (DDT). Regarding the RP, a new\ncall for proposals is made public each semester accepting only proposals under\nthe modality of Target of Opportunity (ToO).\n  This contribution summarizes how ToOs are managed at OAJ presenting the\ndifferent applications designed and implemented at the observatory to deal with\nthem: the Proposal Preparation portal (to request observing time), the Phase2\nObserving tool and the submitphase2 web service (to trigger the ToOs), the TAC\nTracking portal (for telescope operators to support the observations) and the\nTACData portal (to publish and offer the images and their data products).",
        "We present a theory that studies the state composition of a bound exciton in\nmagnetic field. Using a basis set made of products of free electron and hole\nwavefunctions in Landau gauge, we derive a secular equation which shows the\nrelation between Landau levels (LLs) of the electron and hole when a bound\nexciton is formed. Focusing on excitons in the light cone, we establish a\nscattering selection rule for the interaction of an electron in LL $n_\\text{e}$\nwith a hole in LL $n_\\text{h}$. We solve the resulting secular equation and\nidentify a simple pairing law, $n_\\text{e} = n_\\text{h} + l$, which informs us\non the construction of a bound exciton state with magnetic quantum number $l$,\nand on the interaction of the exciton magnetic moment with magnetic field. We\nobtain good agreement between theory results and recent measurements of the\ndiamagnetic shifts of exciton states in WSe$_2$ monolayers.",
        "This thesis details a project to define a fully compositional theory of\nsynchronous sequential circuits built from primitive components, motivated by\napplying techniques successfully used in programming languages to hardware.\n  The first part of the thesis defines the syntactic foundations of sequential\ncircuit morphisms, and then builds three different semantic theories:\ndenotational, operational and algebraic. We characterise the denotational\nsemantics of sequential circuits as certain causal stream functions, as well as\nproviding a link to existing circuit methodologies by mapping between circuit\nmorphisms, stream functions and Mealy machines. The operational semantics is\ndefined as a strategy for applying some global transformations followed by\nlocal reductions to demonstrate how a circuit processes a value, leading to a\nnotion of observational equivalence. The algebraic semantics consists of\nequations for bringing circuits into a pseudo-normal form, and then encoding\nbetween different state sets. This part of the thesis concludes with a\ndiscussion of some novel applications, such as those for using partial\nevaluation for digital circuits.\n  While mathematically rigorous, the categorical string diagram formalism is\nnot suited for reasoning computationally. The second part of this thesis\ndetails an extension of string diagram rewriting with hypergraphs so that it is\ncompatible with the traced comonoid structure present in the category of\ndigital circuits. We identify the properties that characterise cospans of\nhypergraphs corresponding to traced comonoid terms, and demonstrate how to\nidentify rewriting contexts valid for rewriting modulo traced comonoid\nstructure. We apply the graph rewriting framework to fixed point operators as\nwell as the operational semantics from the first part, and present a new\nhardware description language based on these theoretical developments.",
        "With the rapid development of digital services, a large volume of personally\nidentifiable information (PII) is stored online and is subject to cyberattacks\nsuch as Identity fraud. Most recently, the use of Artificial Intelligence (AI)\nenabled deep fake technologies has significantly increased the complexity of\nidentity fraud. Fraudsters may use these technologies to create highly\nsophisticated counterfeit personal identification documents, photos and videos.\nThese advancements in the identity fraud landscape pose challenges for identity\nfraud detection and society at large. There is a pressing need to review and\nunderstand identity fraud detection methods, their limitations and potential\nsolutions. This research aims to address this important need by using the\nwell-known systematic literature review method. This paper reviewed a selected\nset of 43 papers across 4 major academic literature databases. In particular,\nthe review results highlight the two types of identity fraud prevention and\ndetection methods, in-depth and open challenges. The results were also\nconsolidated into a taxonomy of AI-based identity fraud detection and\nprevention methods including key insights and trends. Overall, this paper\nprovides a foundational knowledge base to researchers and practitioners for\nfurther research and development in this important area of digital identity\nfraud.",
        "In this paper, we study the stability problem of exponentially subelliptic\nharmonic maps from sub-Riemannian manifolds to Riemannian manifolds. We derive\nthe rst and second variation formulas for exponentially subelliptic harmonic\nmaps, and apply these formulas to prove that if the target manifold has\nnonpositive curvature, the exponentially subelliptic harmonic map is stable.\nFurther, we obtain the instability of exponentially subelliptic harmonic maps\nwhen the target manifold is a sphere.",
        "We develop an approximation method for the differential entropy\n$h(\\mathbf{X})$ of a $q$-component Gaussian mixture in $\\mathbb{R}^n$. We\nprovide two examples of approximations using our method denoted by\n$\\bar{h}^{\\mathrm{Taylor}}_{C,m}(\\mathbf{X})$ and\n$\\bar{h}^{\\mathrm{Polyfit}}_{C,m}(\\mathbf{X})$. We show that\n$\\bar{h}^{\\mathrm{Taylor}}_{C,m}(\\mathbf{X})$ provides an easy to compute lower\nbound to $h(\\mathbf{X})$, while $\\bar{h}^{\\mathrm{Polyfit}}_{C,m}(\\mathbf{X})$\nprovides an accurate and efficient approximation to $h(\\mathbf{X})$.\n$\\bar{h}^{\\mathrm{Polyfit}}_{C,m}(\\mathbf{X})$ is more accurate than known\nbounds, and conjectured to be much more resilient than the approximation of [5]\nin high dimensions.",
        "Let $R$ be a commutative ring with identity, and let $\\R(R)$ denote the\nsemiring of radical ideals of $R$. The radical functor $\\R$, from the category\nof $R$-modules $R{-}\\boldsymbol{\\sf{Mod}}$ to the category of\n$\\R(R)$-semimodules $\\R(R){-}\\boldsymbol{\\sf{Semod}}$, maps any complex\n$\\M=(M_n, f_n)_{n\\geq 0}$ of $R$-modules to a complex $\\R(\\M)=(\\R(M_n),\n\\R(f_n))_{n\\geq 0}$ of $\\R(R)$-semimodules, where $\\R(M_n)$ consists of radical\nsubmodules of $M_n$, and the $\\R(R)$-semimodule homomorphisms\n$\\R(f_n):\\R(M_n)\\rightarrow \\R(M_{n-1})$ are defined by\n$\\R(f_n)(N)=\\rad(f_n(N))$. The $n$-th radical homology of the complex\n$(\\R(M_n), \\R(f_n))_{n\\geq 0}$, denoted $H_n(\\R(\\M))$, consists of radical\nsubmodules $N$ of $M_n$ such that $f_n(N)$ is contained in the radical of the\nzero submodule of $M_{n-1}$, and two such radical submodules are equivalent\nunder the Bourne relation modulo the image of $\\R(f_{n+1})$. $H_n(\\R(-))$ is\nregarded as a covariant functor from the category\n$\\boldsymbol{\\sf{Ch}}(R{-}\\boldsymbol{\\sf{Mod}})$ of chain complexes of\n$R$-modules to $\\R(R){-}\\boldsymbol{\\sf{Semod}}$, which acts identically on any\npair of homotopic maps of complexes of $R$-modules. In particular, if $\\M$ and\n$\\M'$ are homotopically equivalent, then $H_n(\\R(\\M))$ and $H_n(\\R(\\M'))$ are\nisomorphic $\\R(R)$-semimodules. We provide conditions under which $H_n(\\R(-))$\ninduces a long exact sequence of radical homology modules for any short exact\nsequence of complexes of $R$-modules, and satisfies the naturality condition\nfor exact homology sequences. Finally, we introduce a projective resolution for\nan $R$-module $M$ based on $\\R(R)$-semimodules and give conditions under which\nsuch a projective resolution exists and is unique up to a homotopy.",
        "UnScientify, a system designed to detect scientific uncertainty in scholarly\nfull text. The system utilizes a weakly supervised technique to identify\nverbally expressed uncertainty in scientific texts and their authorial\nreferences. The core methodology of UnScientify is based on a multi-faceted\npipeline that integrates span pattern matching, complex sentence analysis and\nauthor reference checking. This approach streamlines the labeling and\nannotation processes essential for identifying scientific uncertainty, covering\na variety of uncertainty expression types to support diverse applications\nincluding information retrieval, text mining and scientific document\nprocessing. The evaluation results highlight the trade-offs between modern\nlarge language models (LLMs) and the UnScientify system. UnScientify, which\nemploys more traditional techniques, achieved superior performance in the\nscientific uncertainty detection task, attaining an accuracy score of 0.808.\nThis finding underscores the continued relevance and efficiency of\nUnScientify's simple rule-based and pattern matching strategy for this specific\napplication. The results demonstrate that in scenarios where resource\nefficiency, interpretability, and domain-specific adaptability are critical,\ntraditional methods can still offer significant advantages.",
        "We present Acc3D to tackle the challenge of accelerating the diffusion\nprocess to generate 3D models from single images. To derive high-quality\nreconstructions through few-step inferences, we emphasize the critical issue of\nregularizing the learning of score function in states of random noise. To this\nend, we propose edge consistency, i.e., consistent predictions across the high\nsignal-to-noise ratio region, to enhance a pre-trained diffusion model,\nenabling a distillation-based refinement of the endpoint score function.\nBuilding on those distilled diffusion models, we propose an adversarial\naugmentation strategy to further enrich the generation detail and boost overall\ngeneration quality. The two modules complement each other, mutually reinforcing\nto elevate generative performance. Extensive experiments demonstrate that our\nAcc3D not only achieves over a $20\\times$ increase in computational efficiency\nbut also yields notable quality improvements, compared to the\nstate-of-the-arts.",
        "We investigate the one-loop quantum correction to the power spectrum of\nprimordial curvature perturbations in the ultra-slow-roll (USR) inflationary\nscenario, incorporating the backreaction effect from curvature perturbations.\nIn the spatially-flat gauge, we expand the background inflaton field up to\nsecond order and identify the one-loop level backreaction term in the action.\nUtilizing a gauge transformation, we derive the comoving curvature interaction\nHamiltonian in the presence of the backreaction term and calculate the one-loop\ncorrection using the in-in formalism. Our results reveal that the one-loop\nsuper-horizon corrections previously reported in the literature are canceled by\nthe backreaction contributions. This finding underscores the importance of\naccounting for the backreaction effects in the analysis of quantum corrections\nduring USR inflation.",
        "We establish Kronecker-type first and second limit formulas for\n\"non-holomorphic\" and \"Jacobi-type\" Eisenstein series over global function\nfields in the several-variable setting. Our main theorem demonstrates that the\nderivatives of these Eisenstein series can be understood as averaged integrals\nof certain period quantities along the associated \"Heegner cycles\" on Drinfeld\nmodular varieties. A key innovation lies in our use of the Berkovich analytic\nstructure of the Drinfeld period domains, which enables the parametrization of\nthe Heegner cycles in question by Euclidean \"parallelepiped\" regions. This\napproach also facilitates a unified and streamlined formulation and proof of\nour results. Finally, we apply these formulas to provide period interpretations\nof the \"Kronecker terms\" of Dedekind-Weil zeta functions and Dirichlet\n$L$-functions associated with ring and ray class characters.",
        "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation.",
        "The Log Structured Merge (LSM) Tree is a popular choice for key-value stores\nthat focus on optimized write throughput while maintaining performant,\nproduction-ready read latencies. To optimize read performance, LSM stores rely\non a probabilistic data structure called the Bloom Filter (BF). In this paper,\nwe focus on adversarial workloads that lead to a sharp degradation in read\nperformance by impacting the accuracy of BFs used within the LSM store. Our\nevaluation shows up to $800\\%$ increase in the read latency of lookups for\npopular LSM stores. We define adversarial models and security definitions for\nLSM stores. We implement adversary resilience into two popular LSM stores,\nLevelDB and RocksDB. We use our implementations to demonstrate how performance\ndegradation under adversarial workloads can be mitigated.",
        "Due to the inhomogeneity of electron number density, radio waves emitted by\npulsars undergo scattering as they pass through the interstellar medium (ISM).\nHowever, a connection between large-scale pulsar scattering data and the\nstructure of the Galactic ISM has yet to be established. In this paper, we\nexplore the capability of pulsar scattering time data in discovering structures\nin the ISM. Using a large dataset of scattering time measurements for 473\npulsars, we fit the pulsar reduced scattering intensity as a function of\nGalactic latitude and distance, constructing a smooth model of the Galactic\npulsar scattering distribution. By comparing this smooth distribution with\nobservational data, we identify two ISM structures responsible for pulsar\nscattering, one is associated with the Vela supernova remnant region within the\nGum Nebula, while the other is a newly discovered structure -- a distant\nsuperbubble, G38, located at a distance of 2.3 kpc with a size of ~50 pc.\nAnalysis of the correlation coefficient of the pulsar scattering distribution\nshows that the correlation is dominated by structures smaller than 0.15 kpc --\nthe closest separation approachable by the current dataset. As measurements of\nthe pulsar scattering time continue to increase in the future, they can\npotentially become an independent tool for exploring structures in the ISM.",
        "The report of superconductivity (SC) with Tc~80 K in bilayer\nRuddlesden-Popper (RP) nickelate La3Ni2O7-delta have sparked considerable\ninvestigations on its normal state properties and SC mechanism under pressure\nand at low temperature. It is believed that the density wave (DW) at ~150 K\nplays an important role in SC emergence, but its nature remains largely\nunderexplored. Here, we utilized temperature-dependent in-situ Ni K-edge X-ray\nAbsorption Near-edge Spectroscopy (XANES) to probe the Ni-3d\/4p electronic\nstates of La3Ni2O7-delta and La2PrNi2O7-delta samples down to 4.8 K, enabling\nus to witness the evolution of both in-plane d_(x^2-y^2)\/p_x (p_y) and\nout-of-plane d_(3z^2-r^2)\/p_z orbitals of NiO6 octahedron across the DW\ntransition. Main edge energy associated with Ni 4p orbital shows an anomalous\ndecline near DW transition, signifying the occurrence of lattice distortions as\na hallmark of charge density wave. Below DW transition, the enlarged crystal\nfield splitting (CFS) indicates an enhanced NiO6 octahedral distortion.\nIntriguingly, magnetic Pr substituents could activate the mutual interplay of\nd_(x^2-y^2) and d_(3z^2-r^2) orbitals. We discussed its relevance to the\nfavored bulk SC in the pressurized polycrystalline La2PrNi2O7-delta than\npristine."
      ]
    }
  },
  {
    "id":2411.16464,
    "research_type":"basic",
    "start_id":"b6",
    "start_title":"An agent-based spatial urban social network generator: A case study of beijing, china",
    "start_abstract":"This paper proposes an agent-based spatial social network model, which combines a utility function and heuristic algorithms, to formulate friendships of agents in a given synthetic population comprising individuals and households, as well as their attributes and locations. In order to better and explicitly represent the real social networks, the model attempts to generate both close and somewhat close social networks by linking agents with either close or somewhat close friendships, fitting both distributions of network degree and transitivity, which are two basic characteristics of a network. Here, a utility function, which incorporates the similarity between agents in individual attributes (e.g., sex), as well as the spatial closeness of their residential locations and workplaces, is developed to judge whether a friendship between a pair of agents can be built. Furthermore, the social network model is developed as a key component of an agent-and Geographic Information System (GIS)-based virtual city creator that is a set of synthesis methods used to generate spatially disaggregate urban data. Finally, Beijing, China is used as a case study. Both close and somewhat close social networks are generated with the target and generated distributions well matched, and the generated networks are further analysed from a geographical perspective.",
    "start_categories":[
      "cs.CE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Meeting Strangers and Friends of Friends: How Random Are Social Networks?"
      ],
      "abstract":[
        "We present a dynamic model of network formation where nodes find other with whom to form links in two ways: some are found uniformly at random, while others by searching locally through the current structure (e.g., meeting friends friends). This combination processes results spectrum features exhibited large social networks, including presence more high- and low-degree than when formed independently having low distances between network, high clustering on local level. fit data from six networks impute relative ratio random network-based meetings link formation, which turns out vary dramatically across applications. show that as random\/network-based varies, resulting degree distributions can be ordered sense stochastic dominance, allows us infer how process affects average utility network. (JEL D85, Z13)"
      ],
      "categories":[
        "q-fin.EC"
      ]
    },
    "list":{
      "title":[
        "Reducing Circuit Depth in Quantum State Preparation for Quantum\n  Simulation Using Measurements and Feedforward",
        "Insights from leptohadronic modelling of the brightest blazar flare",
        "Ultra-cold neutrons in qBounce experiments as laboratory for test of\n  chameleon field theories and cosmic acceleration",
        "Twenty years of Ne\\v{s}et\\v{r}il's classification programme of Ramsey\n  classes",
        "The algebraic and geometric classification of Jordan superalgebras",
        "Flipped Rotating Axion Non-minimally Coupled to Gravity: Baryogenesis\n  and Dark Matter",
        "Euclid Quick Data Release (Q1). The Strong Lensing Discovery Engine D --\n  Double-source-plane lens candidates",
        "NICER, NuSTAR and Insight-HXMT views to the newly discovered black hole\n  X-ray binary Swift J1727.8--1613",
        "Can supermassive stars form in protogalaxies due to internal\n  Lyman-Werner feedback?",
        "Experimental Realization of Special-Unitary Operations in Classical\n  Mechanics by Non-Adiabatic Evolutions",
        "Elasticity of a Freely Jointed Chain with Quenched Disorder",
        "Robust Cislunar Low-Thrust Trajectory Optimization under Uncertainties\n  via Sequential Covariance Steering",
        "Human-Agent Interaction in Synthetic Social Networks: A Framework for\n  Studying Online Polarization",
        "Quantile-Based Randomized Kaczmarz for Corrupted Tensor Linear Systems",
        "Free Perpetuities I: Existence, Subordination and Tail Asymptotics",
        "Coupled Hierarchical Structure Learning using Tree-Wasserstein Distance",
        "Gravitational lensing and shadows in the toron solution of Einstein's\n  equations using ray tracing methods",
        "The Thermodynamic Cost of Ignorance: Thermal State Preparation with One\n  Ancilla Qubit",
        "The external version of a subclassical logic",
        "Magnetic moment of electrons in systems with spin-orbit coupling",
        "Graph-Theoretic Analysis of $n$-Replica Time Evolution in the Brownian\n  Gaussian Unitary Ensemble",
        "Prediction of Binding Affinity for ErbB Inhibitors Using Deep Neural\n  Network Model with Morgan Fingerprints as Features",
        "On finitely many base $q$ expansions",
        "Polarization agnostic continuous variable quantum key distribution",
        "Broadband Absorption in Cadmium Telluride Thin-Film Solar Cells via\n  Composite Light Trapping Techniques",
        "From discrete-time policies to continuous-time diffusion samplers:\n  Asymptotic equivalences and faster training",
        "A Detection of Circumgalactic Dust at Megaparsec Scales with Maximum\n  Likelihood Estimation",
        "A quantum monomer-dimer model on Penrose tilings",
        "Phase space geometry of collective spin systems: Scaling and Fractality"
      ],
      "abstract":[
        "Reducing circuit depth and identifying an optimal trade-off between circuit\ndepth and width is crucial for successful quantum computation. In this context,\nmid-circuit measurement and feedforward have been shown to significantly reduce\nthe depth of quantum circuits, particularly in implementing logical gates. By\nleveraging these techniques, we propose several parallelization strategies that\nreduce quantum circuit depth at the expense of increasing width in preparing\nvarious quantum states relevant to quantum simulation. With measurements and\nfeedforward, we demonstrate that utilizing unary encoding as a bridge between\ntwo quantum states substantially reduces the circuit depth required for\npreparing quantum states, such as sparse quantum states and sums of Slater\ndeterminants within the first quantization framework, while maintaining an\nefficient circuit width. Additionally, we show that a coordinate Bethe ansatz,\ncharacterized by its high degree of freedom in its phase, can be\nprobabilistically prepared in a constant-depth quantum circuit using\nmeasurements and feedforward. We anticipate that our study will contribute to\nthe reduction of circuit depth in initial state preparation, particularly for\nquantum simulation, which is a critical step toward achieving quantum\nadvantage.",
        "The blazar 3C 454.3 experienced a major flare in November 2010 making it the\nbrightest $\\gamma$-ray source in the sky of the Fermi-LAT. We obtain seven\ndaily consecutive spectral-energy distributions (SEDs) of the flare in the\ninfra-red, optical, ultra-violet, X-ray and $\\gamma$-ray bands with publicly\navailable data. We simulate the physical conditions in the blazar and show that\nthe observed SEDs are well reproduced in the framework of a \"standing feature\"\nwhere the position of the emitting region is almost stationary, located beyond\nthe outer radius of the broad-line region and into which fresh blobs of\nrelativistically moving magnetized plasma are continuously injected. Meanwhile,\na model with a single \"moving blob\" does not describe the data well. We obtain\na robust upper limit to the amount of high-energy protons in the jet of 3C\n454.3 from the electromagnetic SED. We construct a neutrino light curve of 3C\n454.3 and estimate the expected neutrino yield at energies $\\geq 100$ TeV for\n3C 454.3 to be up to $6 \\times 10^{-3}$ $\\nu_{\\mu}$ per year. Finally, we\nextrapolate our model findings to the light curves of all Fermi-LAT\nflat-spectrum radio quasars. We find that next-generation neutrino telescopes\nare expected to detect approximately one multimessenger ($\\gamma + \\nu_{\\mu}$)\nflare per year from bright blazars with neutrino peak energy in the hundreds\nTeV -- hundreds PeV energy range and show that the electromagnetic flare peak\ncan precede the neutrino arrival by months to years.",
        "The accelerating expansion of the Universe, attributed to dark energy, has\nspurred interest in theories involving scalar fields such as chameleon field\ntheories. These fields, which couple to matter with density-dependent effective\nmass, offer a promising explanation for cosmic acceleration. Experiments\nleveraging ultra-cold neutrons (UCNs) provide an innovative approach to testing\nthese theories. The existence of a chameleon field, being responsible for the\ncurrent phase of cosmic acceleration, is investigated by analysing a free fall\nof ultra-cold neutrons from the gap between two mirrors after their bouncing\nbetween these two mirrors. We analyse a deformation of the wave functions of\nthe quantum gravitational states of ultra-cold neutrons, induced by a chameleon\nfield, and find a new upper bound $\\beta\\leq6.5\\times10^8$ on the\nchameleon-matter coupling constant $\\beta$ from the unitarity condition. This\nresult refines previous estimates and highlights the potential of ultra-cold\nneutron experiments as laboratories for exploring scalar field theories and\nfundamental physics.",
        "In the 1970s, structural Ramsey theory emerged as a new branch of\ncombinatorics. This development came with the isolation of the concepts of the\n$\\mathbf{A}$-Ramsey property and Ramsey class. Following the influential\nNe\\v{s}et\\v{r}il-R\\\"{o}dl theorem, several Ramsey classes have been identified.\nIn the 1980s Ne\\v{s}et\\v{r}il, inspired by a seminar of Lachlan, discovered a\ncrucial connection between Ramsey classes and Fra\\\"{\\i}ss\\'{e} classes and, in\nhis 1989 paper, connected the classification programme of homogeneous\nstructures to structural Ramsey theory. In 2005, Kechris, Pestov, and\nTodor\\v{c}evi\\'{c} revitalized the field by connecting Ramsey classes to\ntopological dynamics. This breakthrough motivated Ne\\v{s}et\\v{r}il to propose a\nprogram for classifying Ramsey classes. We review the progress made on this\nprogram in the past two decades, list open problems, and discuss recent\nextensions to new areas, namely the extension property for partial\nautomorphisms (EPPA), and big Ramsey structures.",
        "We give the algebraic and geometric classification of complex\nfour-dimensional Jordan superalgebras. In particular, we describe all\nirreducible components in the corresponding varieties.",
        "We demonstrate that the co-genesis of baryon asymmetry and dark matter can be\nachieved through the rotation of an axion-like particle, driven by a flip in\nthe vacuum manifold's direction at the end of inflation. This can occur if the\naxion has a periodic non-minimal coupling to gravity, while preserving the\ndiscrete shift symmetry. In non-oscillating inflation models, after inflation\nthere is typically a period of kination (with $w = 1$). In this case, it is\nshown that the vacuum manifold of the axion is flipped and the axion begins\nrotating in field space, because it can slide across the decreasing potential\nbarrier as in Ricci reheating. Such a rotating axion can generate the baryon\nasymmetry of the Universe through spontaneous baryogenesis, while at later\nepochs it can oscillate as dark matter. The period of kination makes the\nprimordial gravitational waves (GW) generated during inflation sharply\nblue-tilted which constrains the parameter space due to GW overproduction,\nwhile being testable by next generation CMB experiments. As a concrete example,\nwe show that such a cogenesis of baryon asymmetry and dark matter can be\nrealized for the axion as the Majoron in the Type-I seesaw setup, predicting\nmass ranges for the Majoron below sub eVs, with right-handed neutrino mass\nabove $\\mathcal{O}(10^{8})$ GeV. We also show that in order to avoid\nfragmentation of the axion condensate during the rotation, we require the\nnon-minimal coupling \\mbox{$\\xi \\sim (f\/m_P)^2 $} or somewhat larger, where $f$\nis the axion decay constant.",
        "Strong gravitational lensing systems with multiple source planes are powerful\ntools for probing the density profiles and dark matter substructure of the\ngalaxies. The ratio of Einstein radii is related to the dark energy equation of\nstate through the cosmological scaling factor $\\beta$. However, galaxy-scale\ndouble-source-plane lenses (DSPLs) are extremely rare. In this paper, we report\nthe discovery of four new galaxy-scale double-source-plane lens candidates in\nthe Euclid Quick Release 1 (Q1) data. These systems were initially identified\nthrough a combination of machine learning lens-finding models and subsequent\nvisual inspection from citizens and experts. We apply the widely-used {\\tt\nLensPop} lens forecasting model to predict that the full \\Euclid survey will\ndiscover 1700 DSPLs, which scales to $6 \\pm 3$ DSPLs in 63 deg$^2$, the area of\nQ1. The number of discoveries in this work is broadly consistent with this\nforecast. We present lens models for each DSPL and infer their $\\beta$ values.\nOur initial Q1 sample demonstrates the promise of \\Euclid to discover such rare\nobjects.",
        "Swift J1727.8--1613 is a black hole X-ray binary newly discovered in 2023. We\nperform spectral analysis with simultaneous Insight-HXMT, NICER and NuSTAR\nobservations when the source was approaching to the hard intermediate state.\nSuch a joint view reveals an additional hard component apart from the normally\nobserved hard component with reflection in the spectrum, to be distinguished\nfrom the usual black hole X-ray binary systems. By including this extra\ncomponent in the spectrum, we have measured a high spin of\n$0.98^{+0.02}_{-0.07}$ and an inclination of around $40^{+1.2}_{-0.8}$ degrees,\nwhich is consistent with NICER results reported before. However, we find that\nthe additional spectral component can not be exclusively determined due to the\nmodel degeneracy. Accordingly, a possible jet\/corona configuration is adjusted\nto account for the spectral fitting with different model trials. The extra\ncomponent may originate either from a relativistic jet or a jet base\/corona\nunderneath a slow jet.",
        "Population III stars are possible precursors to early massive and\nsupermassive black holes (BHs). The presence of soft UV Lyman Werner (LW)\nbackground radiation can suppress Population III star formation in minihalos\nand allow them to form in pristine atomic cooling halos. In the absence of\nmolecular hydrogen ($\\rm H_2$) cooling, atomic-cooling halos enable rapid\ncollapse with suppressed fragmentation. High background LW fluxes from\npreceding star-formation have been proposed to dissociate $\\rm H_2$. This flux\ncan be supplemented by LW radiation from one or more Population III star(s) in\nthe same halo, reducing the necessary background level. Here we consider\natomic-cooling halos in which multiple protostellar cores form close to one\nanother nearly simultaneously. We assess whether the first star's LW radiation\ncan dissociate nearby $\\rm H_2$, enabling the prompt formation of a second,\nsupermassive star (SMS) from warm, atomically-cooled gas. We use a set of\nhydrodynamical simulations with the code ENZO, with identical LW backgrounds\ncentered on a halo with two adjacent collapsing gas clumps. When an additional\nlarge local LW flux is introduced, we observe immediate reductions in both the\naccretion rates and the stellar masses that form within these clumps. While the\nLW flux reduces the $\\text{H}_2$ fraction and increases the gas temperature,\nthe halo core's potential well is too shallow to promptly heat the gas to\n$\\gtrsim$ 1000 K and increase the accretion rate onto the second protostar. We\nconclude that internal LW feedback inside atomic-cooling halos is unlikely to\nfacilitate the formation of SMSs or massive BH seeds.",
        "Artificial classical wave systems such as wave crystals and metamaterials\nhave demonstrated promising capabilities in simulating a wide range of quantum\nmechanical phenomena. Yet some gaps between quantum and classical worlds are\ngenerally considered fundamental and difficult to bridge. Dynamics obeying\nspecial unitary groups, e.g., electronic spins described by SU(2), color\nsymmetries of fundamental particles described by SU(3), are such examples. In\nthis work, we present the experimental realization of universal SU(2) and SU(3)\ndynamic operations in classical mechanical oscillator systems with temporally\nmodulated coupling terms. Our approach relies on the sequential execution of\nnon-adiabatic holonomic evolutions, which are typically used in constructing\nquantum-logic gates. The method is swift and purely geometric and can be\nextended to realize more sophisticated dynamic operations. Our results open a\nnew way for studying and simulating quantum phenomena in classical systems.",
        "We introduce a simple theoretical model, the Freely Jointed Chain with\nquenched hinges (qFJC), which captures the quenched disorder in the local\nbending stiffness of the polymer. In this article, we analyze the tensile\nelasticity of the qFJC in the Gibbs (fixed-force) ensemble. For finite-size\nsystems, we obtain a recurrence relation of the exact free energy, which allows\nus to calculate the exact force-extension relation numerically for an arbitrary\nsize of the system. In the thermodynamic limit, when $L({\\rm contour\n\\;length})\\gg L_p({\\rm persistence \\;length})$, we obtain a framework to deal\nwith quenched disorder in the polymer configuration. This allows us to obtain\nthe response function for the discrete and continuous qFJC in the thermodynamic\nlimit. It turns out that the extension of the continuous qFJC can be cast in a\nsimple form. Furthermore, we have applied our analysis to rod-coil multiblock\ncopolymers.",
        "Spacecraft operations are influenced by uncertainties such as dynamics\nmodeling, navigation, and maneuver execution errors. Although mission design\nhas traditionally incorporated heuristic safety margins to mitigate the effect\nof uncertainties, particularly before\/after crucial events, it is yet unclear\nwhether this practice will scale in the cislunar region, which features locally\nchaotic nonlinear dynamics and involves frequent lunar flybys. This paper\napplies chance-constrained covariance steering and sequential convex\nprogramming to simultaneously design an optimal trajectory and trajectory\ncorrection policy that can probabilistically guarantee safety constraints under\nthe assumed physical\/navigational error models. The results show that the\nproposed method can effectively control the state uncertainty in a highly\nnonlinear environment and provide a trajectory with better local stability\nproperties than a trajectory designed without considering uncertainties. The\nframework allows faster computation and lossless covariance propagation\ncompared to existing methods, enabling a rapid and accurate comparison of\n$\\Delta V_{99}$ costs for different uncertainty parameters. We demonstrate the\nalgorithm on several transfers in the Earth-Moon Circular Restricted Three Body\nProblem.",
        "Online social networks have dramatically altered the landscape of public\ndiscourse, creating both opportunities for enhanced civic participation and\nrisks of deepening social divisions. Prevalent approaches to studying online\npolarization have been limited by a methodological disconnect: mathematical\nmodels excel at formal analysis but lack linguistic realism, while language\nmodel-based simulations capture natural discourse but often sacrifice\nanalytical precision. This paper introduces an innovative computational\nframework that synthesizes these approaches by embedding formal opinion\ndynamics principles within LLM-based artificial agents, enabling both rigorous\nmathematical analysis and naturalistic social interactions. We validate our\nframework through comprehensive offline testing and experimental evaluation\nwith 122 human participants engaging in a controlled social network\nenvironment. The results demonstrate our ability to systematically investigate\npolarization mechanisms while preserving ecological validity. Our findings\nreveal how polarized environments shape user perceptions and behavior:\nparticipants exposed to polarized discussions showed markedly increased\nsensitivity to emotional content and group affiliations, while perceiving\nreduced uncertainty in the agents' positions. By combining mathematical\nprecision with natural language capabilities, our framework opens new avenues\nfor investigating social media phenomena through controlled experimentation.\nThis methodological advancement allows researchers to bridge the gap between\ntheoretical models and empirical observations, offering unprecedented\nopportunities to study the causal mechanisms underlying online opinion\ndynamics.",
        "The reconstruction of tensor-valued signals from corrupted measurements,\nknown as tensor regression, has become essential in many multi-modal\napplications such as hyperspectral image reconstruction and medical imaging. In\nthis work, we address the tensor linear system problem $\\mathcal{A}\n\\mathcal{X}=\\mathcal{B}$, where $\\mathcal{A}$ is a measurement operator,\n$\\mathcal{X}$ is the unknown tensor-valued signal, and $\\mathcal{B}$ contains\nthe measurements, possibly corrupted by arbitrary errors. Such corruption is\ncommon in large-scale tensor data, where transmission, sensory, or storage\nerrors are rare per instance but likely over the entire dataset and may be\narbitrarily large in magnitude. We extend the Kaczmarz method, a popular\niterative algorithm for solving large linear systems, to develop a Quantile\nTensor Randomized Kaczmarz (QTRK) method robust to large, sparse corruptions in\nthe observations $\\mathcal{B}$. This approach combines the tensor Kaczmarz\nframework with quantile-based statistics, allowing it to mitigate adversarial\ncorruptions and improve convergence reliability. We also propose and discuss\nthe Masked Quantile Randomized Kaczmarz (mQTRK) variant, which selectively\napplies partial updates to handle corruptions further. We present convergence\nguarantees, discuss the advantages and disadvantages of our approaches, and\ndemonstrate the effectiveness of our methods through experiments, including an\napplication for video deblurring.",
        "We study the free analogue of the classical affine fixed-point (or\nperpetuity) equation\n  \\[\n  \\mathbb{X} \\stackrel{d}{=} \\mathbb{A}^{1\/2}\\mathbb{X}\\,\\mathbb{A}^{1\/2} +\n\\mathbb{B},\n  \\] where $\\mathbb{X}$ is assumed to be $*$-free from the pair\n$(\\mathbb{A},\\mathbb{B})$, with $\\mathbb{A}\\ge 0$ and\n$\\mathbb{B}=\\mathbb{B}^*$. Our analysis covers both the subcritical regime,\nwhere $\\tau(\\mathbb{A})<1$, and the critical case $\\tau(\\mathbb{A})=1$, in\nwhich the solution $\\mathbb{X}$ is necessarily unbounded. When\n$\\tau(\\mathbb{A})=1$, we prove that the series defining $\\mathbb{X}$ converges\nbilaterally almost uniformly (and almost uniformly under additional tail\nassumptions), while the perpetuity fails to have higher moments even if all\nmoments of $\\mathbb{A}$ and $\\mathbb{B}$ exist.\n  Our approach relies on a detailed study of the asymptotic behavior of moments\nunder free multiplicative convolution, which reveals a markedly different\nbehavior from the classical setting. By employing subordination techniques for\nnon-commutative random variables, we derive precise asymptotic estimates for\nthe tail of the distributions of $\\mathbb{X}$ in both one-sided and symmetric\ncases. Interestingly, in the critical case, the free perpetuity exhibits a\npower-law tail behavior that mirrors the phenomenon observed in the celebrated\nKesten's theorem.",
        "In many applications, both data samples and features have underlying\nhierarchical structures. However, existing methods for learning these latent\nstructures typically focus on either samples or features, ignoring possible\ncoupling between them. In this paper, we introduce a coupled hierarchical\nstructure learning method using tree-Wasserstein distance (TWD). Our method\njointly computes TWDs for samples and features, representing their latent\nhierarchies as trees. We propose an iterative, unsupervised procedure to build\nthese sample and feature trees based on diffusion geometry, hyperbolic\ngeometry, and wavelet filters. We show that this iterative procedure converges\nand empirically improves the quality of the constructed trees. The method is\nalso computationally efficient and scales well in high-dimensional settings.\nOur method can be seamlessly integrated with hyperbolic graph convolutional\nnetworks (HGCN). We demonstrate that our method outperforms competing\napproaches in sparse approximation and unsupervised Wasserstein distance\nlearning on several word-document and single-cell RNA-sequencing datasets. In\naddition, integrating our method into HGCN enhances performance in link\nprediction and node classification tasks.",
        "We present a numerical and analytical study of the so-called `toron' solution\nof the stationary axisymmetric Einstein equations in vacuum expressed in terms\nof elliptic functions. The asymptotic behavior of this solution coincides with\nthe one of the NUT solution, i.e., it has a `gravimagnetic' mass known as the\nNUT parameter while the ordinary mass vanishes. The physical properties of this\nspacetime are studied via ray tracing. The results are compared to known\ngeodesic flows in Schwarzschild, Kerr and NUT spacetimes to discuss\nsimilarities and differences, with a particular emphasis on the comparison of\nNUT and toron spacetimes.",
        "In this work we investigate a model of thermalization wherein a single\nancillary qubit randomly interacts with the system to be thermalized. This not\nonly sheds light on the emergence of Gibbs states in nature, but also provides\na routine for preparing arbitrary thermal states on a digital quantum computer.\nFor desired $\\beta$ and random interaction $G$ the routine boils down to time\nindependent Hamiltonian simulation and is represented by the channel $\\Phi :\n\\rho \\mapsto \\mathbb{E}_G {\\rm Tr}_{\\rm Env} \\left[ e^{-i(H + \\alpha G)t}\n\\left(\\rho \\otimes \\frac{e^{-\\beta H_E}}{\\mathcal{Z}}\\right) e^{i (H + \\alpha\nG)t} \\right]$. We rigorously prove that these dynamics reduce to a Markov chain\nprocess in the weak-coupling regime with the thermal state as the approximate\nfixed point. We upper bound the total simulation time required in terms of the\nMarkov chain spectral gap $\\lambda_\\star$, which we compute exactly in the\nground state limit. These results are independent of any eigenvalue knowledge\nof the system, but we are further able to show that with knowledge of\neigenvalue differences $\\lambda_S(i) - \\lambda_S(j)$, then the total simulation\ntime is dramatically reduced. The ratio of the complete ignorance simulation\ncost to the perfect knowledge simulation cost scales as $\\widetilde{O}\n\\left({\\frac{\\|{H_S}\\|^7}{\\delta_{\\rm min}^7 \\epsilon^{3.5}\n\\lambda_\\star(\\beta)^{3.5}}}\\right)$, where $\\delta_{\\min}$ is related to the\neigenvalue differences of the system. Additionally, we provide more specific\nresults for single qubit and harmonic oscillator systems as well as numeric\nexperiments with hydrogen chains. In addition to the algorithmic merits, these\nresults can be viewed as broad extensions of the Repeated Interactions model to\ngeneric Hamiltonians with unknown interactions, giving a complete picture of\nthe thermalization process for quantum systems.",
        "A three-valued logic L is subclassical when it is defined by a single matrix\nhaving the classical two-element matrix as a subreduct. In this case, the\nlanguage of L can be expanded with special unary connectives, called external\noperators. The resulting logic L^e is the external version of L, a notion\noriginally introduced by D. Bochvar in 1938 with respect to his weak Kleene\nlogic. In this paper we study the semantic properties of the external version\nof a three-valued subclassical logic L. We determine sufficient and necessary\nconditions to turn a model of L into a model of L^e . Moreover, we establish\nsome distinctive semantic properties of L^e.",
        "Magnetic effects originating from spin-orbit coupling (SOC) have been\nattracting major attention. However, SOC contributions to the electron magnetic\nmoment operator are conventionally disregarded. In this work, we analyze\nrelativistic contributions to the latter operator, including those of the\nSOC-type: in vacuum, for the semiconductor 8 band Kane model, and for an\narbitrary system with two spectral branches. In this endeavor, we introduce a\nnotion of relativistic corrections to the operation\n$\\partial\/\\partial\\boldsymbol B$, where $\\boldsymbol B$ is an external magnetic\nfield. We highlight the difference between the magnetic moment and $-\\partial\nH\/\\partial\\boldsymbol B$, where $H$ is the system Hamiltonian. We suggest to\ncall this difference the abnormal magnetic moment. We demonstrate that the\nconventional splitting of the total magnetic moment into the spin and orbital\nparts becomes ambiguous when relativistic corrections are taken into account.\nThe latter also jeopardize the ``modern theory of orbital magnetization'' in\nits standard formulation. We derive a linear response Kubo formula for the\nkinetic magnetoelectric effect projected to individual branches of a two branch\nsystem. This allows us, in particular, to identify a source of this effect that\nstems from noncommutation of the position and $\\partial\/\\partial\\boldsymbol B$\noperators' components. This is an analog of the contribution to the Hall\nconductivity from noncommuting components of the position operator. We also\nreport several additional observations related to the electron magnetic moment\noperator in systems with SOC and other relativistic corrections.",
        "In this paper, we investigate the $n$-replica time evolution operator\n$\\mathcal{U}_n(t)\\equiv e^{\\mathcal{L}_nt} $ for the Brownian Gaussian Unitary\nEnsemble (BGUE) using a graph-theoretic approach. We examine the moments of the\ngenerating operator $\\mathcal{L}_n$, which governs the Euclidean time evolution\nwithin an auxiliary $D^{2n}$-dimensional Hilbert space, where $D$ represents\nthe dimension of the Hilbert space for the original system. Explicit\nrepresentations for the cases of $n = 2$ and $n = 3$ are derived, emphasizing\nthe role of graph categorization in simplifying calculations. Furthermore, we\npresent a general approach to streamline the calculation of time evolution for\narbitrary $n$, supported by a detailed example of $n = 4$. Our results\ndemonstrate that the $n$-replica framework not only facilitates the evaluation\nof various observables but also provides valuable insights into the\nrelationship between Brownian disordered systems and quantum information\ntheory.",
        "The ErbB receptor family, including EGFR and HER2, plays a crucial role in\ncell growth and survival and is associated with the progression of various\ncancers such as breast and lung cancer. In this study, we developed a deep\nlearning model to predict the binding affinity of ErbB inhibitors using\nmolecular fingerprints derived from SMILES representations. The SMILES\nrepresentations for each ErbB inhibitor were obtained from the ChEMBL database.\nWe first generated Morgan fingerprints from the SMILES strings and applied\nAutoDock Vina docking to calculate the binding affinity values. After filtering\nthe dataset based on binding affinity, we trained a deep neural network (DNN)\nmodel to predict binding affinity values from the molecular fingerprints. The\nmodel achieved significant performance, with a Mean Squared Error (MSE) of\n0.2591, Mean Absolute Error (MAE) of 0.3658, and an R-squared value of 0.9389\non the training set. Although performance decreased slightly on the test set (R\nsquared = 0.7731), the model still demonstrated robust generalization\ncapabilities. These results indicate that the deep learning approach is highly\neffective for predicting the binding affinity of ErbB inhibitors, offering a\nvaluable tool for virtual screening and drug discovery.",
        "Given some integer $m \\geq 3$, we find the first explicit collection of\ncountably many intervals in $(1,2)$ such that for any $q$ in one of these\nintervals, the set of points with exactly $m$ base $q$ expansions is nonempty\nand moreover has positive Hausdorff dimension. Our method relies on an\napplication of a theorem proved by Falconer and Yavicoli, which guarantees that\nthe intersection of a family of compact subsets of $\\mathbb{R}^d$ has positive\nHausdorff dimension under certain conditions.",
        "We introduce a polarization agnostic method for Gaussian-modulated coherent\nstate (GCMS) continuous-variable quantum key distribution (CVQKD). Due to the\nrandom and continuous nature of the GCMS protocol, Alice, the transmitter, can\nencode two distinct quadratures in each of two orthogonal polarization modes,\nsuch that Bob, the receiver, measures valid GCMS quadratures in a single\npolarization mode even when polarization changes occur during transmission.\nThis method does not require polarization correction in the optical domain,\ndoes not require monitoring both polarization modes, reduces loss by\neliminating optical components, and avoids the noise injected by polarization\ncorrection algorithms.",
        "Composite light-trapping structures offer a promising approach to achieving\nbroadband absorption and high efficiency in thin-film solar cells (TFSCs) in\norder to accelerate sustainable energy solutions. As the leading material in\nthin-film solar technology, cadmium telluride (CdTe) faces challenges from\nsurface reflective losses across the solar spectrum and weak absorption in the\nnear-infrared (NIR) range. This computational study addresses these limitations\nby employing a dual light trapping technique: the top surfaces of both the CdS\nand CdTe layers are tapered as nanocones (NCs), while germanium (Ge) spherical\nnanoparticles (NPs) are embedded within the CdTe absorber layer to enhance\nbroadband absorption. Numerical simulations using Finite-Difference Time Domain\n(FDTD) and other methods are used to optimize the parameters and configurations\nof both nanostructures, aiming to achieve peak optoelectronic performance. The\nresults show that a short-circuit current density ($J_{sc}$) of 35.38 mA\/$cm^2$\nand a power conversion efficiency (PCE) of 27.76% can be achieved with optimal\nnanocone (NC) texturing and spherical Ge nanoparticle (NP) configurations, a\n45.45% and 80.72% increase compared to baseline structure in $J_{sc}$ and PCE\nrespectively. To understand the enhancement mechanisms, the study includes\nanalyses using diffraction grating theory and Mie theory. Fabricability of\nthese structures is also evaluated. Furthermore, an additional study on the\neffects of incident angle variation and polarization change demonstrates that\nthe optimal structure is robust under practical conditions, maintaining\nconsistent performance.",
        "We study the problem of training neural stochastic differential equations, or\ndiffusion models, to sample from a Boltzmann distribution without access to\ntarget samples. Existing methods for training such models enforce time-reversal\nof the generative and noising processes, using either differentiable simulation\nor off-policy reinforcement learning (RL). We prove equivalences between\nfamilies of objectives in the limit of infinitesimal discretization steps,\nlinking entropic RL methods (GFlowNets) with continuous-time objects (partial\ndifferential equations and path space measures). We further show that an\nappropriate choice of coarse time discretization during training allows greatly\nimproved sample efficiency and the use of time-local objectives, achieving\ncompetitive performance on standard sampling benchmarks with reduced\ncomputational cost.",
        "One of the more surprising astrophysical discoveries of the last decade has\nbeen the presence of enormous quantities of dust at megaparsec distances from\ngalaxies, which has important implications for galaxy evolution, the\ncircumgalactic and intergalactic medium, and observational cosmology. In this\nwork, we present a novel method for studying these vast halos of circumgalactic\ndust: a maximum-likelihood estimator for dust-induced extinction of background\ngalaxies. This estimator can accommodate a broad range of archival photometric\ndata and can incorporate different dust reddening prescriptions, making it\napplicable to diverse galaxy types and redshifts. We apply the estimator to the\nredMaGiC catalog of luminous red galaxies, selected for their tight dispersion\nin color and well-constrained photometric redshifts, and measure the resulting\nextinction as a function of projected distance from WISExSuperCOSMOS and\nredMaGiC foreground galaxies. We detect significant dust-induced extinction\nprofiles extending to at least 1 megaparsec from galactic disks, with\nnoticeable differences between star-forming and quiescent galaxies:\nstar-forming galaxies exhibit a pronounced rise in extinction within the inner\n50 kiloparsecs and a steep decline beyond 1 megaparsec, while the quiescent\ngalaxies host little dust in the inner halo but have detectable extinction out\nto 30 megaparsecs. We test the robustness of our results using star catalogs\nand inverted foreground and background samples and find no evidence for\nsignificant systematic error. Our approach provides a powerful tool for\nstudying the interplay between circumgalactic dust, galaxy evolution, and\nlarge-scale structure, with potential applications in a number of astrophysical\nsubfields.",
        "We define a quantum monomer-dimer model in the space of maximal dimer\ncoverings of quasicrystalline Penrose tilings. Since Penrose tilings do not\nadmit perfect dimer coverings, as shown by F. Flicker et al., PRX 10, 011005\n(2020), monomers are necessarily present in our model. The model features a\nfrustration-free Rokhsar-Kivelson (RK) point where the ground state is a\nuniform superposition of all the exponentially many maximal dimer coverings,\ndespite the presence of a finite density of monomers. We map our model to a\n$\\mathbb{Z}_2$ gauge theory with matter and calculate various correlators to\ncharacterize the phase of the system at the RK point using classical Monte\nCarlo calculations. Specifically, we compute the dimer-dimer and vison-vison\ncorrelators, as well as open Wilson lines and closed Wilson loops corresponding\nto the monomers and the visons. We find that both the dimer-dimer and\nvison-vison correlators decay exponentially with distance. The open Wilson\nlines and closed Wilson loops decay exponentially with the same correlation\nlength, indicating that the gauge theory is in the confined phase, which\nimplies that the system is likely in an ordered phase.",
        "We examine the scaling of the inverse participation ratio of spin coherent\nstates in the energy basis of three collective spin systems: a bounded harmonic\noscillator, the Lipkin-Meshkov-Glick model, and the Quantum Kicked Top. The\nfinite-size quantum probing provides detailed insights into the structure of\nthe phase space, particularly the relationship between critical points in\nclassical dynamics and their quantum counterparts in collective spin systems.\nWe introduce a finite-size scaling mass exponent that makes it possible to\nidentify conditions under which a power-law behavior emerges, allowing to\nassign a fractal dimension to a coherent state. For the Quantum Kicked Top, the\nfractal dimension of coherent states -- when well-defined -- exhibits three\ngeneral behaviors: one related to the presence of critical points and two\nassociated with regular and chaotic dynamics. The finite-size scaling analysis\npaves the way toward exploring collective spin systems relevant to quantum\ntechnologies within the quantum-classical framework."
      ]
    }
  },
  {
    "id":2411.1726,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Differentiation of distal ureteral stones and pelvic phleboliths using a convolutional neural network",
    "start_abstract":"Abstract The objectives were to develop and validate a Convolutional Neural Network (CNN) using local features for differentiating distal ureteral stones from pelvic phleboliths, compare the CNN method with semi-quantitative radiologists\u2019 assessments evaluate whether assessment of calcification its surroundings is sufficient discriminating phleboliths in non-contrast-enhanced CT (NECT). We retrospectively included 341 consecutive patients acute renal colic stone on NECT showing either stone, phlebolith or both. A 2.5-dimensional (2.5D-CNN) model was used, where perpendicular axial, coronal sagittal images through each used as input data CNN. trained 384 calcifications, evaluated an unseen dataset 50 phleboliths. compared by seven radiologists who reviewed 5 \u00d7 cm image stack surrounding calcification, cut-off values based attenuation volume calcifications. differentiated sensitivity, specificity accuracy 94%, 90% 92% AUC 0.95. This similar majority vote 93% significantly higher ( p = 0.03) than mean radiologist 86%. 49%. In conclusion, features. However, more are needed reach optimal discrimination.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b27"
      ],
      "title":[
        "Micro-CT data of early physiological cancellous bone formation in the lumbar spine of female C57BL\/6 mice"
      ],
      "abstract":[
        "Micro-CT provides critical data for musculoskeletal research, yielding three-dimensional datasets containing distributions of mineral density. Using high-resolution scans, we quantified changes in the fine architecture of bone in the spine of young mice. This data is made available as a reference to physiological cancellous bone growth. The scans (n\u2009=\u200919) depict the extensive structural changes typical for female C57BL\/6 mice pups, aged 1-, 3-, 7-, 10- and 14-days post-partum, as they attain the\u00a0mature geometry. We reveal the micro-morphology down to individual\u00a0trabeculae in the spine that follow phases of mineral-tissue rearrangement in the growing lumbar vertebra on a micrometer length scale. Phantom data is provided to facilitate mineral density calibration. Conventional histomorphometry matched with our micro-CT data on selected samples confirms the validity and accuracy of our 3D scans. The data may thus serve as a reference for modeling normal bone growth and can be used to benchmark other experiments assessing the effects of biomaterials, tissue growth, healing, and regeneration. Measurement(s) bone growth \u2022 bone mineralization involved in bone maturation Technology Type(s) micro-computed tomography Factor Type(s) age Sample Characteristic - Organism Mus musculus Sample Characteristic - Environment biological_process Machine-accessible metadata file describing the reported data: https:\/\/doi.org\/10.6084\/m9.figshare.14062073"
      ],
      "categories":[
        "physics.med-ph"
      ]
    },
    "list":{
      "title":[
        "A variant of \\v{S}emrl's preserver theorem for singular matrices",
        "Optimal domain of Volterra operators in Korenblum spaces",
        "On general versions of the Petty projection inequality",
        "Applying the Liouville-Lanczos Method of Time-Dependent\n  Density-Functional Theory to Warm Dense Matter",
        "Representation Theorems for Convex Expectations and Semigroups on Path\n  Space",
        "Homotopical Entropy",
        "Metastability and Ostwald Step Rule in the Crystallisation of Diamond\n  and Graphite from Molten Carbon",
        "Multiwavelength Variability Analysis of the Blazar PKS 0727-11: A\n  $\\sim$168 Days Quasi-periodic Oscillation in Gamma-ray",
        "Chirality, Nonreciprocity and Symmetries for a Giant Atom",
        "The period-index problem for hyper-K\\\"ahler varieties via\n  hyperholomorphic bundles",
        "Constrained differential operators, Sobolev inequalities, and Riesz\n  potentials",
        "Benchmarking ANN extrapolations of the ground-state energies and radii\n  of Li isotopes",
        "On the Curvature and Topology of Compact Stationary Spacetimes",
        "Global Convergence and Rate Analysis of the Steepest Descent Method for\n  Uncertain Multiobjective Optimization via a Robust Optimization Approach",
        "Topological Invariants in Invasion Percolation",
        "A refined functorial universal tangle invariant",
        "Mosaic-skeleton approximation is all you need for Smoluchowski equations",
        "Searching for single production of vector-like quarks decaying into $Wb$\n  at a future muon-proton collider",
        "Impact of radiative accelerations on the stellar characterization of\n  FGK-type stars using spectroscopic and seismic constraints",
        "Measurement-based Simulation of Geometric Gates in Topological Qubits on\n  NISQ Devices",
        "The Complex Magnetic Field of the Extreme Galactic Center: PRIMA Science\n  Potential",
        "Dynamical behavior and bifurcation analysis for a theoretical model of\n  dengue fever transmission with incubation period and delayed recovery",
        "Facial structure of copositive and completely positive cones over a\n  second-order cone",
        "KLiNQ: Knowledge Distillation-Assisted Lightweight Neural Network for\n  Qubit Readout on FPGA",
        "$B\\to K\\sf{+}invisible$, dark matter, and $CP$ violation in hyperon\n  decays",
        "Chern numbers on positive vector bundles and combinatorics",
        "On the $L_2$-discrepancy of Latin hypercubes",
        "Quantum Cosmology and the Age of the Universe",
        "Dynamic Markov Blanket Detection for Macroscopic Physics Discovery"
      ],
      "abstract":[
        "For positive integers $1 \\leq k \\leq n$ let $M_n$ be the algebra of all $n\n\\times n$ complex matrices and $M_n^{\\le k}$ its subset consisting of all\nmatrices of rank at most $k$. We first show that whenever $k>\\frac{n}{2}$, any\ncontinuous spectrum-shrinking map $\\phi : M_n^{\\le k} \\to M_n$ (i.e.\n$\\mathrm{sp}(\\phi(X)) \\subseteq \\mathrm{sp}(X)$ for all $X \\in M_n^{\\le k}$)\neither preserves characteristic polynomials or takes only nilpotent values.\nMoreover, for any $k$ there exists a real analytic embedding of $M_n^{\\le k}$\ninto the space of $n\\times n$ nilpotent matrices for all sufficiently large\n$n$. This phenomenon cannot occur when $\\phi$ is injective and either $k > n -\n\\sqrt{n}$ or the image of $\\phi$ is contained in $M_n^{\\le k}$. We then\nestablish a main result of the paper -- a variant of \\v{S}emrl's preserver\ntheorem for $M_n^{\\le k}$: if $n \\geq 3$, any injective continuous map $\\phi\n:M_n^{\\le k} \\to M_n^{\\le k}$ that preserves commutativity and shrinks spectrum\nis of the form $\\phi(\\cdot)=T(\\cdot)T^{-1}$ or $\\phi(\\cdot)=T(\\cdot)^tT^{-1}$,\nfor some invertible matrix $T\\in M_n$. Moreover, when $k=n-1$, which\ncorresponds to the set of singular $n\\times n$ matrices, this result extends to\nmaps $\\phi$ which take values in $M_n$. Finally, we discuss the\nindispensability of assumptions in our main result.",
        "The aim of this article is to study the largest domain space $[T,X]$,\nwhenever it exists, of a given continuous linear operator $T\\colon X\\to X$,\nwhere $X\\subseteq H(\\mathbb{D})$ is a Banach space of analytic functions on the\nopen unit disc $\\mathbb{D}\\subseteq \\mathbb{C}$. That is, $[T,X]\\subseteq\nH(\\mathbb{D})$ is the \\textit{largest} Banach space of analytic functions\ncontaining $X$ to which $T$ has a continuous, linear, $X$-valued extension\n$T\\colon [T,X]\\to X$. The class of operators considered consists of generalized\nVolterra operators $T$ acting in the Korenblum growth Banach spaces\n$X:=A^{-\\gamma}$, for $\\gamma>0$. Previous studies dealt with the classical\nCes\\`aro operator $T:=C$ acting in the Hardy spaces $H^p$, $1\\leq p<\\infty$,\n\\cite{CR}, \\cite{CR1}, in $A^{-\\gamma}$, \\cite{ABR-R}, and more recently,\ngeneralized Volterra operators $T$ acting in $X:=H^p$, \\cite{BDNS}.",
        "The classical Petty projection inequality is an affine isoperimetric\ninequality which constitutes a cornerstone in the affine geometry of convex\nbodies. By extending the polar projection body to an inter-dimensional\noperator, Petty's inequality was generalized to the so-called $(L_p,Q)$\nsetting, where $Q$ is an $m$-dimensional compact convex set. In this work, we\nfurther extend the $(L_p,Q)$ Petty projection inequality to the broader realm\nof rotationally invariant measures with concavity properties, namely, those\nwith $\\gamma$-concave density (for $\\gamma\\geq-1\/nm$). Moreover, when $p=1$,\nand motivated by a contemporary empirical reinterpretation of Petty's result,\nwe explore empirical analogues of this inequality.",
        "Ab initio modeling of dynamic structure factors (DSF) and related density\nresponse properties in the warm dense matter (WDM) regime is a challenging\ncomputational task. The DSF, convolved with a probing X-ray beam and instrument\nfunction, is measured in X-ray Thomson scattering (XRTS) experiments, which\nallows for the study of electronic structure properties at the microscopic\nlevel. Among the various ab initio methods, linear response time-dependent\ndensity functional theory (LR-TDDFT) is a key framework for simulating the DSF.\nThe standard approach in LR-TDDFT for computing the DSF relies on the orbital\nrepresentation. A significant drawback of this method is the unfavorable\nscaling of the number of required empty bands as the wavenumber increases,\nmaking LR-TDDFT impractical for modeling XRTS measurements over large energy\nscales, such as in backward scattering geometry. We consider and test an\nalternative approach that employs the Liouville-Lanczos (LL) method for\nsimulating the DSF. This approach does not require empty states and allows the\nDSF at large momentum transfer values and over a broad frequency range to be\naccessed. We compare the results obtained from the LL method with those from\nthe standard LR-TDDFT within the projector augmented-wave formalism for\nisochorically heated aluminum and warm dense hydrogen. Additionally, we utilize\nexact path integral Monte Carlo (PIMC) results for the imaginary-time\ndensity-density correlation function (ITCF) of warm dense hydrogen to\nrigorously benchmark the LL approach. We discuss the application of the LL\nmethod for calculating DSFs and ITCFs at different wavenumbers, the effects of\npseudopotentials, and the role of Lorentzian smearing. The successful\nvalidation of the LL method under WDM conditions makes it a valuable addition\nto the ab initio simulation landscape, supporting experimental efforts and\nadvancing WDM theory.",
        "The objective of this paper is to investigate the connection between penalty\nfunctions from stochastic optimal control, convex semigroups from analysis and\nconvex expectations from probability theory. Our main result provides a\none-to-one relation between these objects. As an application, we use the\nrepresentation via penality functions and duality arguments to show that convex\nexpectations are determined by their finite dimensional distributions. To\nillustrate this structural result, we show that Hu and Peng's axiomatic\ndescription of $G$-L\\'evy processes in terms of finite dimensional\ndistributions extends uniquely to the control approach introduced by Neufeld\nand Nutz. Finally, we show that convex expectations with a Markovian structure\nare fully determined by their one-dimensional distributions, which give rise to\na classical semigroup on the state space.",
        "We present a \"homotopification\" of fundamental concepts from information\ntheory. Using homotopy type theory, we define homotopy types that behave\nanalogously to probability spaces, random variables, and the exponentials of\nShannon entropy and relative entropy. The original analytic theories emerge\nthrough homotopy cardinality, which maps homotopy types to real numbers and\ngeneralizes the cardinality of sets.",
        "The crystallisation of carbon from the melt under extreme conditions is\nhighly relevant to earth and planetary science, materials manufacturing, and\nnuclear fusion research. The thermodynamic conditions near the\ngraphite-diamond-liquid (GDL) triple point are especially of interest for\ngeological and technological applications, but high-pressure flash heating\nexperiments aiming to resolve this region of the phase diagram of carbon\nexhibit large discrepancies. Experimental challenges are often related to the\npersistence of metastable crystalline or glassy phases, superheated crystals,\nor supercooled liquids. A deeper understanding of the crystallisation kinetics\nof diamond and graphite is crucial for effectively interpreting the outcomes of\nthese experiments. Here, we reveal the microscopic mechanisms of diamond and\ngraphite nucleation from liquid carbon through molecular simulations with\nfirst-principles machine learning potentials. Our simulations accurately\nreproduce the experimental phase diagram of carbon in the region around the GDL\ntriple point and show that liquid carbon crystallises spontaneously upon\ncooling at constant pressure. Surprisingly, metastable graphite crystallises in\nthe domain of diamond thermodynamic stability at pressures above the triple\npoint. Furthermore, whereas diamond crystallises through a classical nucleation\npathway, graphite follows a two-step process in which low-density fluctuations\nforego ordering. Calculations of the nucleation rates of the two competing\nphases confirm this result and reveal a manifestation of Ostwald's step rule\nwhere the strong metastability of graphite hinders the transformation to the\nstable diamond phase. Our results provide a new key to interpreting melting and\nrecrystallisation experiments and shed light on nucleation kinetics in\npolymorphic materials with deep metastable states.",
        "We performed variability analysis of the multiwavelength light curves for the\nflat-spectrum radio quasar PKS 0727-11. Using the generalized Lomb-Scargle\nperiodogram, we identified a possible quasi-periodic oscillation (QPO) of\n$\\sim$ 168.6 days (persisted for 6 cycles, with a significance of $3.8\\sigma$)\nin the gamma-ray light curve during the flare period (MJD 54687-55738). It is\nthe first time that periodic variations have been detected in this source, and\nfurther supported by other methods: weighted wavelet $z$-transform, phase\ndispersion minimization, REDFIT, autoregressive integrated moving average\nmodel, and structure function analysis. Cross-correlation analysis shows that\nthere is a strong correlation between multi-band light variations, indicating\nthat gamma-ray and radio flares may originate from the same disturbance, and\nthe distance between the emission regions of gamma-ray and radio flares is\ncalculated based on the time lag. We demonstrate that QPO arising from the\nnon-ballistic helical jet motion driven by the orbital motion in a supermassive\nbinary black hole is a plausible physical explanation. In this scenario, the\nestimated mass of the primary black hole is\n$M\\sim3.66\\times10^8-5.79\\times10^{9}M_\\odot$.",
        "Chiral and nonreciprocal quantum devices are crucial for signal routing and\nprocessing in a quantum network. In this work, we study the chirality and\nnonreciprocity of a giant atom coupled to a one-dimensional waveguide. We\nclarify that the chiral emission of the giant atom is not directly related to\nthe time-reversal symmetry breaking but to the mirror-symmetry breaking. We\npropose a passive scheme to realize the chiral emission of a giant atom without\nbreaking time-reversal symmetry by extending the legs of the giant atom. We\nfind the time-reversal symmetry breaking via nonuniform coupling phases is\nartificial and thus cannot result in nonreciprocal single-photon scattering for\nthe giant atom. The nonreciprocity of the giant atom can be obtained by the\nexternal dissipation of the giant atom that truly breaks the time-reversal\nsymmetry. Our work clarifies the roles of symmetries in the chirality and\nnonreciprocity of giant-atom systems and paves the way for the design of\non-chip functional devices with superconducting giant atoms.",
        "We prove new bounds for the period-index problem for hyper-K\\\"ahler varieties\nof $K3^{[n]}$-type using projectively hyperholomorphic bundles constructed by\nMarkman. We show that $\\mathrm{dim}(X)$ is a bound for any $X$ of\n$K3^{[n]}$-type. We also show that the bound can be reduced to\n$\\frac{1}{2}\\mathrm{dim}(X)$, as conjectured by Huybrechts, when the Picard\nrank of $X$ is at least 3.",
        "Inequalities for Riesz potentials are well-known to be equivalent to Sobolev\ninequalities of the same order for domain norms \"far\" from $L^1$, but to be\nweaker otherwise. Recent contributions by Van Schaftingen, by Hernandez,\nRai\\c{t}\\u{a} and Spector, and by Stolyarov proved that this gap can be filled\nin Riesz potential inequalities for vector-valued functions in $L^1$ fulfilling\na co-canceling differential condition. This work demonstrates that such a\nproperty is not just peculiar to the space $L^1$. Indeed, under the same\ndifferential constraint, a Riesz potential inequality is shown to hold for any\ndomain and target rearrangement-invariant norms that render a Sobolev\ninequality of the same order true. This is based on a new interpolation\ninequality, which, via a kind of duality argument, yields a parallel property\nof Sobolev inequalities for any linear homogeneous elliptic canceling\ndifferential operator. Specifically, Sobolev inequalities involving the full\ngradient of a certain order share the same rearrangement-invariant domain and\ntarget spaces as their analogs for any other homogeneous elliptic canceling\ndifferential operator of equal order. As a consequence, Riesz potential\ninequalities under the co-canceling constraint and Sobolev inequalities for\nhomogeneous elliptic canceling differential operators are offered for general\nfamilies of rearrangement-invariant spaces, such as the Orlicz spaces and the\nLorentz-Zygmund spaces. Especially relevant instances of inequalities for\ndomain spaces neighboring $L^1$ are singled out.",
        "We present a comparison of model-space extrapolation methods for No-Core\nShell Model calculations of ground-state energies and root-mean-square radii in\nLi isotopes. In particular, we benchmark the latest machine learning tools\nagainst widely used exponential and infrared extrapolations for energies and\ncrossing point estimates for radii. Our findings demonstrate that machine\nlearning-based approaches provide reliable predictions with robust statistical\nuncertainties for both observables even in small model spaces. These\npredictions are compatible with established exponential and IR extrapolations\nof energies and mark a notable improvement over conventional radius estimates.",
        "Using the result of Petersen $\\&$ Wink '21, we find obstructions to the\ncurvature and topology of compact Lorentzian manifolds admitting a unit-length\ntimelike Killing vector field.",
        "In this article, we extend our previous work (Applicable Analysis, 2024, pp.\n1-25) on the steepest descent method for uncertain multiobjective optimization\nproblems. While that study established local convergence, it did not address\nglobal convergence and the rate of convergence of the steepest descent\nalgorithm. To bridge this gap, we provide rigorous proofs for both global\nconvergence and the linear convergence rate of the steepest descent algorithm.\nGlobal convergence analysis strengthens the theoretical foundation of the\nsteepest descent method for uncertain multiobjective optimization problems,\noffering deeper insights into its efficiency and robustness across a broader\nclass of optimization problems. These findings enhance the method's practical\napplicability and contribute to the advancement of robust optimization\ntechniques.",
        "Based on bond percolation theory, a method is presented here to calculate the\nrelationship between capillary pressure and saturation in porous media from\nfirst principles. The governing equations are formulated on the undirected\ngraph of the pore network. The graph is a simplified mathematical object that\naccounts for the topology of the pore structure. Thus, the calculation is\nextremely computationally efficient since it is mesh-free and voxel-free. Two\ntopological invariants are identified: The bond percolation threshold and the\nresidual saturation. Bond percolation theory is used to obtain a closed-form\npressure-saturation relation in terms of the geometry of the pores (pore throat\ndistribution) and material parameters (contact angle and interfacial tension),\nuniversal exponents, and topological invariants, based on scaling relations.",
        "The universal invariant with respect to a given ribbon Hopf algebra is a\ntangle invariant that dominates all the Reshetikhin-Turaev invariants built\nfrom the representation theory of the algebra. We construct a canonical strict\nmonoidal functor that encodes the universal invariant of upwards tangles and\nrefines the Kerler-Kauffman-Radford functorial invariant. Moreover, this\nfunctor preserves the braiding, twist and the open trace, the latter being a\nmild modification of Joyal-Street-Verity's notion of trace in a balanced\ncategory. We construct this functor using the more flexible XC-algebras, a\nclass which contains both ribbon Hopf algebras and endomorphism algebras of\nrepresentation of these.",
        "In this work we demonstrate a surprising way of exploitation of the\nmosaic--skeleton approximations for efficient numerical solving of aggregation\nequations with many applied kinetic kernels. The complexity of the evaluation\nof the right-hand side with $M$ nonlinear differential equations basing on the\nuse of the mosaic-skeleton approximations is $\\mathcal{O}(M \\log^2 M)$\noperations instead of $\\mathcal{O}(M^2)$ for the straightforward computation.\nThe class of kernels allowing to make fast and accurate computations via our\napproach is wider than analogous set of kinetic coefficients for effective\ncalculations with previously developed algorithms. This class covers the\naggregation problems arising in modelling of sedimentation, supersonic effects,\nturbulent flows, etc. We show that our approach makes it possible to study the\nsystems with $M=2^{20}$ nonlinear equations within a modest computing time.",
        "In this work, we investigate the single production of the Vector-Like Quarks\n(VLQs) $T$ and $Y$, with charge $+2\/3$e and $-4\/3$e, respectively, both\ndecaying into $Wb$ at a future $\\mu p$ collider with $\\sqrt{s}=5.29, 6.48$, and\n9.16 TeV. We focus on two final states, where the $W$ boson decays leptonically\nor hadronically (in the latter case, it is highly boosted, leading to a fat\n$W$-jet). By performing a detailed signal-to-background analysis in presence of\ndetector simulations, the $5\\sigma$ discovery prospects and 95\\% CL exclusion\nlimits are mapped over parameter space regions of the model accommodating such\nnew heavy quarks, assuming an integrated luminosity of 100 fb$^{-1}$ at each of\nthe aforementioned energies.",
        "Chemical transport mechanisms are fundamental processes in stellar evolution\nmodels. They are responsible for the chemical distribution, and their impact\ndetermines how accurately we can characterize stars. Radiative accelerations\nare one of these processes. They allow the accumulation of elements at\ndifferent depths in the star. We aim to assess the impact of radiative\naccelerations on the modeling of FGK-type stars and their impact on the\nprediction of surface abundances. To reduce the cost of the computation of\nradiative accelerations, we implemented the single-valued parameters (SVP)\nmethod in the stellar evolution code MESA. The SVP method is more efficient in\ncalculating radiative accelerations, which enables computations of large enough\ngrids of models for stellar characterization. Compared to models that include\natomic diffusion (with only gravitational settling), the inclusion of radiative\naccelerations has a small effect on the inference of fundamental properties,\nwith an impact of 2\\%, 0.7\\%, and 5\\% for mass, radius, and age. However, the\ntreatment of radiative accelerations is necessary to predict the chemical\ncomposition of and accurately characterize stars.",
        "While the adiabatic exchange of Majorana zero modes (MZMs) enables a\nnon-universal set of geometrically protected gates, realising an experimental\nimplementation of MZM braiding remains challenging. In an alternative proposal,\ncharge-parity measurement of two neighboring MZMs supports braiding by\nteleportation. Moreover, owing to the lack of definitive evidence of MZMs in\nsemiconducting systems, there have been several simulations of MZMs on NISQ\ndevices which more naturally lend themselves to braiding. In this work,\nmeasurement-based braiding about MZM Y-junctions are simulated by multi-qubit\nPauli-parity measurements of a logical qubit. Logical single-qubit geometric\n$S^{(\\dagger)}$-gates and entangling two-qubit gates is shown using\ntwo-physical-qubit joint measurements alone, whilst partial phase rotations\nsuch as a $T^{(\\dagger)}$-gates require at least one three-qubit joint\nmeasurement. These relatively small scale circuits offer both novel\nmeasurement-based geometric gates as well as a measurement-based demonstration\nof quantum Hamiltonian simulation.",
        "The Central Molecular Zone (CMZ) of the Galactic Center (GC) region of the\nMilky Way contains a substantial fraction of the molecular mass of the Galaxy\n>10e7 solar masses yet exhibits an order of magnitude lower star formation\nefficiency (SFE) than expected given the high densities found in this region.\nThere are multiple possible explanations for the depressed SFE in the CMZ, like\nfeedback, strong turbulence, longer free-fall timescales, and high magnetic\nfield strengths. It is currently unclear which of these mechanisms is the\ndominant inhibitor of star formation in the CMZ. It is important to understand\nthe star formation process in the extreme environment of the CMZ because it is\nthe only Galactic nuclear region we are able to study at high spatial\nresolutions with current observatories. One way to determine the relative\nimportance of the different SFE inhibiting mechanisms is through multi-spatial\nand multi-frequency polarimetric observations of the CMZ. Such observations\nwill provide insight into the behavior of the magnetic field in this unique\nenvironment. These observations will complement radio observations of\nnon-thermal structures revealing the magnetic field morphology and\npolarization. The PRobe far--Infrared Mission for Astrophysics (PRIMA) will be\nuniquely capable of contributing to such explorations by providing unique\nresolutions and frequencies for polarimetric observations. The PRIMAger\ninstrument will yield polarimetric observations covering the wavelength range\n80 -- 261 um with beam sizes ranging from 11 -- 28'', capabilities that\ncomplement existing and upcoming observatories.",
        "As offered by the World Health Organisation (WHO), close to half of the\npopulation in the world's resides in dengue-risk zones. Dengue viruses are\ntransmitted to individuals by Aedes mosquito species infected bite (Ae.\nAlbopictus of Ae. aegypti). These mosquitoes can transmit other viruses,\nincluding Zika and Chikungunya. In this research, a mathematical model is\nformulated to reflect different time delays considered in both extrinsic and\nintrinsic incubation periods, as well as in the recovery periods of infectious\nindividuals. Preliminary results for the non-delayed model including positivity\nand boundedness of solutions, non-dimensionalization and equalibria analysis\nare presented. The threshold parameter (reproduction number) of the model is\nobtained via next generation matrix schemes. The stability analysis of the\nmodel revealed that various dynamical behaviour can be observed depending on\ndelay parameters, where in particular the effect of delay in the recovery time\nof infectious individuals may lead to substantial changes in the dynamics. The\nideas presented in this paper can be applied to other infectious diseases,\nproviding qualitative evaluations for understanding time delays influencing the\ntransmission dynamics.",
        "We classify the faces of copositive and completely positive cones over a\nsecond-order cone and investigate their dimension and exposedness properties.\nThen we compute two parameters related to chains of faces of both cones. At the\nend, we discuss some possible extensions of the results with a view toward\nanalyzing the facial structure of general copositive and completely positive\ncones.",
        "Superconducting qubits are among the most promising candidates for building\nquantum information processors. Yet, they are often limited by slow and\nerror-prone qubit readout -- a critical factor in achieving high-fidelity\noperations. While current methods, including deep neural networks, enhance\nreadout accuracy, they typically lack support for mid-circuit measurements\nessential for quantum error correction, and they usually rely on large,\nresource-intensive network models. This paper presents KLiNQ, a novel qubit\nreadout architecture leveraging lightweight neural networks optimized via\nknowledge distillation. Our approach achieves around a 99% reduction in model\nsize compared to the baseline while maintaining a qubit-state discrimination\naccuracy of 91%. KLiNQ facilitates rapid, independent qubit-state readouts that\nenable mid-circuit measurements by assigning a dedicated, compact neural\nnetwork for each qubit. Implemented on the Xilinx UltraScale+ FPGA, our design\ncan perform the discrimination within 32ns. The results demonstrate that\ncompressed neural networks can maintain high-fidelity independent readout while\nenabling efficient hardware implementation, advancing practical quantum\ncomputing.",
        "Recently the Belle II Collaboration has reported a measurement of the $B^+\\to\nK^+\\nu\\bar\\nu$ rate that is higher than the standard-model expectation. Since\nthe emitted neutrinos are unobserved, the excess could be due to the $B^+$\ndecaying into a $K^+$ and a dark-matter pair. We entertain this possibility in\na two-Higgs-doublet model supplemented with a real singlet scalar boson acting\nas the dark matter. This model also accommodates strangeness-changing\ninteractions providing new sources of $CP$ violation which can affect hyperon\nand kaon nonleptonic transitions. We find that the resulting $CP$ violation in\nthe hyperon sector can be significant, reaching the current empirical bounds,\nafter taking into account constraints from kaon mixing and decay and from\ndark-matter relic-density data and direct searches including the Migdal effect.\nWe demonstrate that the hyperon and kaon processes are complementary probes of\nthis new-physics scenario. Its prediction for sizable hyperon $CP$ violation is\npotentially testable in ongoing experiments, such as BESIII, Belle II, and\nLHCb, and in next-generation ones like PANDA and at the Super Tau Charm\nFacility.",
        "Combinatorial ideas are developed in this article to study Chern numbers on\nample and numerically effective vector bundles. An effective lower bound for\nChern numbers of ample vector bundles is established, which makes some progress\ntowards a long-standing question. Along this line we prove that Chern numbers\non nef vector bundles obey reverse dominance ordering, which improves upon some\nclassical and recent results. We propose a simultaneous positivity question on\n(signed) Chern numbers of compact complex or K\\\"{a}hler manifolds whose\n(co)tangent bundles are semipositive in various senses, and show that it holds\ntrue for compact homogeneous complex manifolds.",
        "We investigate $L_2$-discrepancies of what we call weak Latin hypercubes. In\nthis case it turns out that there is a precise equivalence between the extreme\nand periodic $L_2$-discrepancy which follows from a much broader result about\ngeneralized energies for weighted point sets.\n  Motivated by this we study the asymptotics of the optimal $L_2$-discrepancy\nof weak Latin hypercubes. We determine asymptotically tight bounds for $d \\geq\n3$ and even the precise (dimension dependent) constant in front of the\ndominating term for $d \\geq 4$.",
        "In this article I study how the problem of time of canonical approaches to\nquantum gravity affects the simple minisuperspace models used in quantum\ncosmology. I follow some authors who have argued that this issue makes the\nquantization of general relativity problematic to conclude that the same\napplies in the case of quantum cosmology. In particular, I argue that temporal\nstructures are lost in quantization and that this is a problem, as they encode\npart of the empirical content of classical cosmology, such as the age of the\nuniverse.",
        "The free energy principle (FEP), along with the associated constructs of\nMarkov blankets and ontological potentials, have recently been presented as the\ncore components of a generalized modeling method capable of mathematically\ndescribing arbitrary objects that persist in random dynamical systems; that is,\na mathematical theory of ``every'' ``thing''. Here, we leverage the FEP to\ndevelop a mathematical physics approach to the identification of objects,\nobject types, and the macroscopic, object-type-specific rules that govern their\nbehavior. We take a generative modeling approach and use variational Bayesian\nexpectation maximization to develop a dynamic Markov blanket detection\nalgorithm that is capable of identifying and classifying macroscopic objects,\ngiven partial observation of microscopic dynamics. This unsupervised algorithm\nuses Bayesian attention to explicitly label observable microscopic elements\naccording to their current role in a given system, as either the internal or\nboundary elements of a given macroscopic object; and it identifies macroscopic\nphysical laws that govern how the object interacts with its environment.\nBecause these labels are dynamic or evolve over time, the algorithm is capable\nof identifying complex objects that travel through fixed media or exchange\nmatter with their environment. This approach leads directly to a flexible class\nof structured, unsupervised algorithms that sensibly partition complex\nmany-particle or many-component systems into collections of interacting\nmacroscopic subsystems, namely, ``objects'' or ``things''. We derive a few\nexamples of this kind of macroscopic physics discovery algorithm and\ndemonstrate its utility with simple numerical experiments, in which the\nalgorithm correctly labels the components of Newton's cradle, a burning fuse,\nthe Lorenz attractor, and a simulated cell."
      ]
    }
  },
  {
    "id":2411.1726,
    "research_type":"applied",
    "start_id":"b27",
    "start_title":"Micro-CT data of early physiological cancellous bone formation in the lumbar spine of female C57BL\/6 mice",
    "start_abstract":"Micro-CT provides critical data for musculoskeletal research, yielding three-dimensional datasets containing distributions of mineral density. Using high-resolution scans, we quantified changes in the fine architecture of bone in the spine of young mice. This data is made available as a reference to physiological cancellous bone growth. The scans (n\u2009=\u200919) depict the extensive structural changes typical for female C57BL\/6 mice pups, aged 1-, 3-, 7-, 10- and 14-days post-partum, as they attain the\u00a0mature geometry. We reveal the micro-morphology down to individual\u00a0trabeculae in the spine that follow phases of mineral-tissue rearrangement in the growing lumbar vertebra on a micrometer length scale. Phantom data is provided to facilitate mineral density calibration. Conventional histomorphometry matched with our micro-CT data on selected samples confirms the validity and accuracy of our 3D scans. The data may thus serve as a reference for modeling normal bone growth and can be used to benchmark other experiments assessing the effects of biomaterials, tissue growth, healing, and regeneration. Measurement(s) bone growth \u2022 bone mineralization involved in bone maturation Technology Type(s) micro-computed tomography Factor Type(s) age Sample Characteristic - Organism Mus musculus Sample Characteristic - Environment biological_process Machine-accessible metadata file describing the reported data: https:\/\/doi.org\/10.6084\/m9.figshare.14062073",
    "start_categories":[
      "physics.med-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Differentiation of distal ureteral stones and pelvic phleboliths using a convolutional neural network"
      ],
      "abstract":[
        "Abstract The objectives were to develop and validate a Convolutional Neural Network (CNN) using local features for differentiating distal ureteral stones from pelvic phleboliths, compare the CNN method with semi-quantitative radiologists\u2019 assessments evaluate whether assessment of calcification its surroundings is sufficient discriminating phleboliths in non-contrast-enhanced CT (NECT). We retrospectively included 341 consecutive patients acute renal colic stone on NECT showing either stone, phlebolith or both. A 2.5-dimensional (2.5D-CNN) model was used, where perpendicular axial, coronal sagittal images through each used as input data CNN. trained 384 calcifications, evaluated an unseen dataset 50 phleboliths. compared by seven radiologists who reviewed 5 \u00d7 cm image stack surrounding calcification, cut-off values based attenuation volume calcifications. differentiated sensitivity, specificity accuracy 94%, 90% 92% AUC 0.95. This similar majority vote 93% significantly higher ( p = 0.03) than mean radiologist 86%. 49%. In conclusion, features. However, more are needed reach optimal discrimination."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
        "Theory-to-Practice Gap for Neural Networks and Neural Operators",
        "Quantitative Derivation of the Two-Component Gross-Pitaevskii Equation\n  with Uniform-in-Time Convergence Rate",
        "Polynomial Time Learning-Augmented Algorithms for NP-hard Permutation\n  Problems",
        "AI-Empowered Catalyst Discovery: A Survey from Classical Machine\n  Learning Approaches to Large Language Models",
        "Characterizing Data Visualization Literacy: a Systematic Literature\n  Review",
        "Geodesic Diffusion Models for Medical Image-to-Image Generation",
        "Langevin Monte-Carlo Provably Learns Depth Two Neural Nets at Any Size\n  and Data",
        "Optimisation of space-time periodic eigenvalues",
        "The least balanced graphs and trees",
        "One Stack, Diverse Vehicles: Checking Safe Portability of Automated\n  Driving Software",
        "Are Large Language Models Good In-context Learners for Financial\n  Sentiment Analysis?",
        "Diffusion on Graph: Augmentation of Graph Structure for Node\n  Classification",
        "Prophet Inequalities for Bandits, Cabinets, and DAGs",
        "Fast Debiasing of the LASSO Estimator",
        "Accelerated Medicines Development using a Digital Formulator and a\n  Self-Driving Tableting DataFactory",
        "A Novel Spatiotemporal Correlation Anomaly Detection Method Based on\n  Time-Frequency-Domain Feature Fusion and a Dynamic Graph Neural Network in\n  Wireless Sensor Network",
        "Para-Holomorphic Algebroids and Para-Complex Connections",
        "FeatPCA: A feature subspace based principal component analysis technique\n  for enhancing clustering of single-cell RNA-seq data",
        "Seeing World Dynamics in a Nutshell",
        "Augmenting Image Annotation: A Human-LMM Collaborative Framework for\n  Efficient Object Selection and Label Generation",
        "Generalized Simple Graphical Rules for Assessing Selection Bias",
        "DBSCAN in domains with periodic boundary conditions",
        "Joint Cell Selection and Resource Allocation Games with Backhaul\n  Constraints",
        "Toward Foundation Models for Online Complex Event Detection in CPS-IoT:\n  A Case Study",
        "UnitCoder: Scalable Iterative Code Synthesis with Unit Test Guidance",
        "You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from\n  Video Demonstrations",
        "Robust Probabilistic Model Checking with Continuous Reward Domains",
        "KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large\n  Language Model"
      ],
      "abstract":[
        "Recent advancements in Neural Audio Codec (NAC) models have inspired their\nuse in various speech processing tasks, including speech enhancement (SE). In\nthis work, we propose a novel, efficient SE approach by leveraging the\npre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE\nmethods, which process discrete speech tokens using Language Models (LMs), we\nperform SE within the continuous embedding space of the pretrained NAC, which\nis highly compressed along the time dimension for efficient representation. Our\nlightweight SE model, optimized through an embedding-level loss, delivers\nresults comparable to SE baselines trained on larger datasets, with a\nsignificantly lower real-time factor of 0.005. Additionally, our method\nachieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer\nin a simulated cloud-based audio transmission environment. This work highlights\na new, efficient NAC-based SE solution, particularly suitable for cloud\napplications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission\nfrom IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting\/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to\nservers or lists, or reuse of any copyrighted component of this work in other\nworks.",
        "This work studies the sampling complexity of learning with ReLU neural\nnetworks and neural operators. For mappings belonging to relevant approximation\nspaces, we derive upper bounds on the best-possible convergence rate of any\nlearning algorithm, with respect to the number of samples. In the\nfinite-dimensional case, these bounds imply a gap between the parametric and\nsampling complexities of learning, known as the \\emph{theory-to-practice gap}.\nIn this work, a unified treatment of the theory-to-practice gap is achieved in\na general $L^p$-setting, while at the same time improving available bounds in\nthe literature. Furthermore, based on these results the theory-to-practice gap\nis extended to the infinite-dimensional setting of operator learning. Our\nresults apply to Deep Operator Networks and integral kernel-based neural\noperators, including the Fourier neural operator. We show that the\nbest-possible convergence rate in a Bochner $L^p$-norm is bounded by\nMonte-Carlo rates of order $1\/p$.",
        "We derive the time-dependent two-component Gross-Pitaevskii equation as an\neffective description of the dynamics of a dilute two-component Bose gas near\nits ground state, which exhibits a two-component mixture Bose-Einstein\ncondensate, in the Gross-Pitaevskii limit regime. Our main result establishes a\nuniform-in-time bound on the convergence rate between the many-body dynamics\nand the effective description, explicitly quantified in terms of the particle\nnumber $N$. This improves upon the works of Michelangeli and Olgliati [73, 85]\nby providing a sharper, $N$-dependent, time-independent convergence rate. Our\napproach also extends the framework of Benedikter, de Oliveira, and Schlein\n[10] to the multi-component Bose gas setting. More specifically, we develop the\nnecessary Bogoliubov theory to analyze the dynamics of multi-component Bose\ngases in the Gross-Pitaevskii regime.",
        "We consider a learning-augmented framework for NP-hard permutation problems.\nThe algorithm has access to predictions telling, given a pair $u,v$ of\nelements, whether $u$ is before $v$ or not in an optimal solution. Building on\nthe work of Braverman and Mossel (SODA 2008), we show that for a class of\noptimization problems including scheduling, network design and other graph\npermutation problems, these predictions allow to solve them in polynomial time\nwith high probability, provided that predictions are true with probability at\nleast $1\/2+\\epsilon$. Moreover, this can be achieved with a parsimonious access\nto the predictions.",
        "Catalysts are essential for accelerating chemical reactions and enhancing\nselectivity, which is crucial for the sustainable production of energy,\nmaterials, and bioactive compounds. Catalyst discovery is fundamental yet\nchallenging in computational chemistry and has garnered significant attention\ndue to the promising performance of advanced Artificial Intelligence (AI)\ntechniques. The development of Large Language Models (LLMs) notably accelerates\nprogress in the discovery of both homogeneous and heterogeneous catalysts,\nwhere their chemical reactions differ significantly in material phases,\ntemperature, dynamics, etc. However, there is currently no comprehensive survey\nthat discusses the progress and latest developments in both areas, particularly\nwith the application of LLM techniques. To address this gap, this paper\npresents a thorough and systematic survey of AI-empowered catalyst discovery,\nemploying a unified and general categorization for homogeneous and\nheterogeneous catalysts. We examine the progress of AI-empowered catalyst\ndiscovery, highlighting their individual advantages and disadvantages, and\ndiscuss the challenges faced in this field. Furthermore, we suggest potential\ndirections for future research from the perspective of computer science. Our\ngoal is to assist researchers in computational chemistry, computer science, and\nrelated fields in easily tracking the latest advancements, providing a clear\noverview and roadmap of this area. We also organize and make accessible\nrelevant resources, including article lists and datasets, in an open repository\nat\nhttps:\/\/github.com\/LuckyGirl-XU\/Awesome-Artificial-Intelligence-Empowered-Catalyst-Discovery.",
        "With the advent of the data era, and of new, more intelligent interfaces for\nsupporting decision making, there is a growing need to define, model and assess\nhuman ability and data visualizations usability for a better encoding and\ndecoding of data patterns. Data Visualization Literacy (DVL) is the ability of\nencoding and decoding data into and from a visual language. Although this\nability and its measurement are crucial for advancing human knowledge and\ndecision capacity, they have seldom been investigated, let alone\nsystematically. To address this gap, this paper presents a systematic\nliterature review comprising 43 reports on DVL, analyzed using the PRISMA\nmethodology. Our results include the identification of the purposes of DVL, its\nsatellite aspects, the models proposed, and the assessments designed to\nevaluate the degree of DVL of people. Eventually, we devise many research\ndirections including, among the most challenging, the definition of a\n(standard) unifying construct of DVL.",
        "Diffusion models transform an unknown data distribution into a Gaussian prior\nby progressively adding noise until the data become indistinguishable from pure\nnoise. This stochastic process traces a path in probability space, evolving\nfrom the original data distribution (considered as a Gaussian with near-zero\nvariance) to an isotropic Gaussian. The denoiser then learns to reverse this\nprocess, generating high-quality samples from random Gaussian noise. However,\nstandard diffusion models, such as the Denoising Diffusion Probabilistic Model\n(DDPM), do not ensure a geodesic (i.e., shortest) path in probability space.\nThis inefficiency necessitates the use of many intermediate time steps, leading\nto high computational costs in training and sampling. To address this\nlimitation, we propose the Geodesic Diffusion Model (GDM), which defines a\ngeodesic path under the Fisher-Rao metric with a variance-exploding noise\nscheduler. This formulation transforms the data distribution into a Gaussian\nprior with minimal energy, significantly improving the efficiency of diffusion\nmodels. We trained GDM by continuously sampling time steps from 0 to 1 and\nusing as few as 15 evenly spaced time steps for model sampling. We evaluated\nGDM on two medical image-to-image generation tasks: CT image denoising and MRI\nimage super-resolution. Experimental results show that GDM achieved\nstate-of-the-art performance while reducing training time by a 50-fold compared\nto DDPM and 10-fold compared to Fast-DDPM, with 66 times faster sampling than\nDDPM and a similar sampling speed to Fast-DDPM. These efficiency gains enable\nrapid model exploration and real-time clinical applications. Our code is\npublicly available at: https:\/\/github.com\/mirthAI\/GDM-VE.",
        "In this work, we will establish that the Langevin Monte-Carlo algorithm can\nlearn depth-2 neural nets of any size and for any data and we give\nnon-asymptotic convergence rates for it. We achieve this via showing that under\nTotal Variation distance and q-Renyi divergence, the iterates of Langevin Monte\nCarlo converge to the Gibbs distribution of Frobenius norm regularized losses\nfor any of these nets, when using smooth activations and in both classification\nand regression settings. Most critically, the amount of regularization needed\nfor our results is independent of the size of the net. This result combines\nseveral recent observations, like our previous papers showing that two-layer\nneural loss functions can always be regularized by a certain constant amount\nsuch that they satisfy the Villani conditions, and thus their Gibbs measures\nsatisfy a Poincare inequality.",
        "The goal of this paper is to provide a qualitative analysis of the\noptimisation of space-time periodic principal eigenvalues. Namely, considering\na fixed time horizon $T$ and the $d$-dimensional torus $\\mathbb{T}^d$, let, for\nany $m\\in L^\\infty((0,T)\\times\\mathbb{T}^d)$, $\\lambda(m)$ be the principal\neigenvalue of the operator $\\partial_t-\\Delta-m$ endowed with (time-space)\nperiodic boundary conditions. The main question we set out to answer is the\nfollowing: how to choose $m$ so as to minimise $\\lambda(m)$? This question\nstems from population dynamics. We prove that in several cases it is always\nbeneficial to rearrange $m$ with respect to time in a symmetric way, which is\nthe first comparison result for the rearrangement in time of parabolic\nequations. Furthermore, we investigate the validity (or lack thereof) of\nTalenti inequalities for the rearrangement in time of parabolic equations. The\nnumerical simulations which illustrate our results were obtained by developing\na framework within which it is possible to optimise criteria with respect to\nfunctions having a prescribed rearrangement (or distribution function).",
        "Given a connected graph, the principal eigenvector of the adjacency matrix\n(often called the Perron vector) can be used to assign positive weights to the\nvertices. A natural way to measure the homogeneousness of this vector is by\nconsidering the ratio of its $\\ell^1$ and $\\ell^2$ norms.\n  It is easy to see that the most balanced graphs in this sense (i.e., the ones\nwith the largest ratio) are the regular graphs. What about the least balanced\ngraphs with the smallest ratio? It was conjectured by R\\\"ucker, R\\\"ucker and\nGutman that, for any given $n \\geq 6$, among $n$-vertex connected graphs the\nsmallest ratio is achieved by the complete graph $K_4$ with a single path\n$P_{n-4}$ attached to one of its vertices. In this paper we confirm this\nconjecture.\n  We also verify the analogous conjecture for trees: for any given $n \\geq 8$,\namong $n$-vertex trees the smallest ratio is achieved by the star graph $S_5$\nwith a path $P_{n-5}$ attached to its central vertex.",
        "Integrating an automated driving software stack into vehicles with variable\nconfiguration is challenging, especially due to different hardware\ncharacteristics. Further, to provide software updates to a vehicle fleet in the\nfield, the functional safety of every affected configuration has to be ensured.\nThese additional demands for dependability and the increasing hardware\ndiversity in automated driving make rigorous automatic analysis essential. This\npaper addresses this challenge by using formal portability checking of adaptive\ncruise controller code for different vehicle configurations. Given a formal\nspecification of the safe behavior, models of target configurations are\nderived, which capture relevant effects of sensors, actuators and computing\nplatforms. A corresponding safe set is obtained and used to check if the\ndesired behavior is achievable on all targets. In a case study, portability\nchecking of a traditional and a neural network controller are performed\nautomatically within minutes for each vehicle hardware configuration. The check\nprovides feedback for necessary adaptations of the controllers, thus, allowing\nrapid integration and testing of software or parameter changes.",
        "Recently, large language models (LLMs) with hundreds of billions of\nparameters have demonstrated the emergent ability, surpassing traditional\nmethods in various domains even without fine-tuning over domain-specific data.\nHowever, when it comes to financial sentiment analysis (FSA)$\\unicode{x2013}$a\nfundamental task in financial AI$\\unicode{x2013}$these models often encounter\nvarious challenges, such as complex financial terminology, subjective human\nemotions, and ambiguous inclination expressions. In this paper, we aim to\nanswer the fundamental question: whether LLMs are good in-context learners for\nFSA? Unveiling this question can yield informative insights on whether LLMs can\nlearn to address the challenges by generalizing in-context demonstrations of\nfinancial document-sentiment pairs to the sentiment analysis of new documents,\ngiven that finetuning these models on finance-specific data is difficult, if\nnot impossible at all. To the best of our knowledge, this is the first paper\nexploring in-context learning for FSA that covers most modern LLMs (recently\nreleased DeepSeek V3 included) and multiple in-context sample selection\nmethods. Comprehensive experiments validate the in-context learning capability\nof LLMs for FSA.",
        "Graph diffusion models have recently been proposed to synthesize entire\ngraphs, such as molecule graphs. Although existing methods have shown great\nperformance in generating entire graphs for graph-level learning tasks, no\ngraph diffusion models have been developed to generate synthetic graph\nstructures, that is, synthetic nodes and associated edges within a given graph,\nfor node-level learning tasks. Inspired by the research in the computer vision\nliterature using synthetic data for enhanced performance, we propose Diffusion\non Graph (DoG), which generates synthetic graph structures to boost the\nperformance of GNNs. The synthetic graph structures generated by DoG are\ncombined with the original graph to form an augmented graph for the training of\nnode-level learning tasks, such as node classification and graph contrastive\nlearning (GCL). To improve the efficiency of the generation process, a Bi-Level\nNeighbor Map Decoder (BLND) is introduced in DoG. To mitigate the adverse\neffect of the noise introduced by the synthetic graph structures, a low-rank\nregularization method is proposed for the training of graph neural networks\n(GNNs) on the augmented graphs. Extensive experiments on various graph datasets\nfor semi-supervised node classification and graph contrastive learning have\nbeen conducted to demonstrate the effectiveness of DoG with low-rank\nregularization. The code of DoG is available at\nhttps:\/\/github.com\/Statistical-Deep-Learning\/DoG.",
        "A decisionmaker faces $n$ alternatives, each of which represents a potential\nreward. After investing costly resources into investigating the alternatives,\nthe decisionmaker may select one, or more generally a feasible subset, and\nobtain the associated reward(s). The objective is to maximize the sum of\nrewards minus total costs invested. We consider this problem under a general\nmodel of an alternative as a \"Markov Search Process,\" a type of undiscounted\nMarkov Decision Process on a finite acyclic graph. Even simple cases generalize\nNP-hard problems such as Pandora's Box with nonobligatory inspection.\n  Despite the apparently adaptive and interactive nature of the problem, we\nprove optimal prophet inequalities for this problem under a variety of\ncombinatorial constraints. That is, we give approximation algorithms that\ninteract with the alternatives sequentially, where each must be fully explored\nand either selected or else discarded before the next arrives. In particular,\nwe obtain a computationally efficient $\\frac{1}{2}-\\epsilon$ prophet inequality\nfor Combinatorial Markov Search subject to any matroid constraint. This result\nimplies incentive-compatible mechanisms with constant Price of Anarchy for\nserving single-parameter agents when the agents strategically conduct\nindependent, costly search processes to discover their values.",
        "In high-dimensional sparse regression, the \\textsc{Lasso} estimator offers\nexcellent theoretical guarantees but is well-known to produce biased estimates.\nTo address this, \\cite{Javanmard2014} introduced a method to ``debias\" the\n\\textsc{Lasso} estimates for a random sub-Gaussian sensing matrix\n$\\boldsymbol{A}$. Their approach relies on computing an ``approximate inverse\"\n$\\boldsymbol{M}$ of the matrix $\\boldsymbol{A}^\\top \\boldsymbol{A}\/n$ by\nsolving a convex optimization problem. This matrix $\\boldsymbol{M}$ plays a\ncritical role in mitigating bias and allowing for construction of confidence\nintervals using the debiased \\textsc{Lasso} estimates. However the computation\nof $\\boldsymbol{M}$ is expensive in practice as it requires iterative\noptimization. In the presented work, we re-parameterize the optimization\nproblem to compute a ``debiasing matrix\" $\\boldsymbol{W} :=\n\\boldsymbol{AM}^{\\top}$ directly, rather than the approximate inverse\n$\\boldsymbol{M}$. This reformulation retains the theoretical guarantees of the\ndebiased \\textsc{Lasso} estimates, as they depend on the \\emph{product}\n$\\boldsymbol{AM}^{\\top}$ rather than on $\\boldsymbol{M}$ alone. Notably, we\nprovide a simple, computationally efficient, closed-form solution for\n$\\boldsymbol{W}$ under similar conditions for the sensing matrix\n$\\boldsymbol{A}$ used in the original debiasing formulation, with an additional\ncondition that the elements of every row of $\\boldsymbol{A}$ have uncorrelated\nentries. Also, the optimization problem based on $\\boldsymbol{W}$ guarantees a\nunique optimal solution, unlike the original formulation based on\n$\\boldsymbol{M}$. We verify our main result with numerical simulations.",
        "Pharmaceutical tablet formulation and process development, traditionally a\ncomplex and multi-dimensional decision-making process, necessitates extensive\nexperimentation and resources, often resulting in suboptimal solutions. This\nstudy presents an integrated platform for tablet formulation and manufacturing,\nbuilt around a Digital Formulator and a Self-Driving Tableting DataFactory. By\ncombining predictive modelling, optimisation algorithms, and automation, this\nsystem offers a material-to-product approach to predict and optimise critical\nquality attributes for different formulations, linking raw material attributes\nto key blend and tablet properties, such as flowability, porosity, and tensile\nstrength. The platform leverages the Digital Formulator, an in-silico\noptimisation framework that employs a hybrid system of models - melding\ndata-driven and mechanistic models - to identify optimal formulation settings\nfor manufacturability. Optimised formulations then proceed through the\nself-driving Tableting DataFactory, which includes automated powder dosing,\ntablet compression and performance testing, followed by iterative refinement of\nprocess parameters through Bayesian optimisation methods. This approach\naccelerates the timeline from material characterisation to development of an\nin-specification tablet within 6 hours, utilising less than 5 grams of API, and\nmanufacturing small batch sizes of up to 1,440 tablets with augmented and mixed\nreality enabled real-time quality control within 24 hours. Validation across\nmultiple APIs and drug loadings underscores the platform's capacity to reliably\nmeet target quality attributes, positioning it as a transformative solution for\naccelerated and resource-efficient pharmaceutical development.",
        "Attention-based transformers have played an important role in wireless sensor\nnetwork (WSN) timing anomaly detection due to their ability to capture\nlong-term dependencies. However, there are several issues that must be\naddressed, such as the fact that their ability to capture long-term\ndependencies is not completely reliable, their computational complexity levels\nare high, and the spatiotemporal features of WSN timing data are not\nsufficiently extracted for detecting the correlation anomalies of multinode WSN\ntiming data. To address these limitations, this paper proposes a WSN anomaly\ndetection method that integrates frequency-domain features with dynamic graph\nneural networks (GNN) under a designed self-encoder reconstruction framework.\nFirst, the discrete wavelet transform effectively decomposes trend and seasonal\ncomponents of time series to solve the poor long-term reliability of\ntransformers. Second, a frequency-domain attention mechanism is designed to\nmake full use of the difference between the amplitude distributions of normal\ndata and anomalous data in this domain. Finally, a multimodal fusion-based\ndynamic graph convolutional network (MFDGCN) is designed by combining an\nattention mechanism and a graph convolutional network (GCN) to adaptively\nextract spatial correlation features. A series of experiments conducted on\npublic datasets and their results demonstrate that the anomaly detection method\ndesigned in this paper exhibits superior precision and recall than the existing\nmethods do, with an F1 score of 93.5%, representing an improvement of 2.9% over\nthat of the existing models.",
        "The goal of this paper is to develop the theory of Courant algebroids with\nintegrable para-Hermitian vector bundle structures by invoking the theory of\nLie bialgebroids. We consider the case where the underlying manifold has an\nalmost para-complex structure, and use this to define a notion of\npara-holomorphic algebroid. We investigate connections on para-holomorphic\nalgebroids and determine an appropriate sense in which they can be\npara-complex. Finally, we show through a series of examples how the theory of\nexact para-holomorphic algebroids with a para-complex connection is a\ngeneralization of both para-K\\\"{a}hler geometry and the theory of Poisson-Lie\ngroups.",
        "Single-cell RNA sequencing (scRNA-seq) has revolutionized our ability to\nanalyze gene expression at the cellular level. By providing data on gene\nexpression for each individual cell, scRNA-seq generates large datasets with\nthousands of genes. However, handling such high-dimensional data poses\ncomputational challenges due to increased complexity. Dimensionality reduction\nbecomes crucial for scRNA-seq analysis. Various dimensionality reduction\nalgorithms, including Principal Component Analysis (PCA), Uniform Manifold\nApproximation and Projection (UMAP), and t-Distributed Stochastic Neighbor\nEmbedding (t-SNE), are commonly used to address this challenge. These methods\ntransform the original high-dimensional data into a lower-dimensional\nrepresentation while preserving relevant information. In this paper we propose\n{\\methodname}. Instead of applying dimensionality reduction directly to the\nentire dataset, we divide it into multiple subspaces. Within each subspace, we\napply dimension reduction techniques, and then merge the reduced data.\n{\\methodname} offers four variations for subspacing. Our experimental results\ndemonstrate that clustering based on subspacing yields better accuracy than\nworking with the full dataset. Across a variety of scRNA-seq datasets,\n{\\methodname} consistently outperforms existing state-of-the-art clustering\ntools.",
        "We consider the problem of efficiently representing casually captured\nmonocular videos in a spatially- and temporally-coherent manner. While existing\napproaches predominantly rely on 2D\/2.5D techniques treating videos as\ncollections of spatiotemporal pixels, they struggle with complex motions,\nocclusions, and geometric consistency due to absence of temporal coherence and\nexplicit 3D structure. Drawing inspiration from monocular video as a projection\nof the dynamic 3D world, we explore representing videos in their intrinsic 3D\nform through continuous flows of Gaussian primitives in space-time. In this\npaper, we propose NutWorld, a novel framework that efficiently transforms\nmonocular videos into dynamic 3D Gaussian representations in a single forward\npass. At its core, NutWorld introduces a structured spatial-temporal aligned\nGaussian (STAG) representation, enabling optimization-free scene modeling with\neffective depth and flow regularization. Through comprehensive experiments, we\ndemonstrate that NutWorld achieves high-fidelity video reconstruction quality\nwhile enabling various downstream applications in real-time. Demos and code\nwill be available at https:\/\/github.com\/Nut-World\/NutWorld.",
        "Traditional image annotation tasks rely heavily on human effort for object\nselection and label assignment, making the process time-consuming and prone to\ndecreased efficiency as annotators experience fatigue after extensive work.\nThis paper introduces a novel framework that leverages the visual understanding\ncapabilities of large multimodal models (LMMs), particularly GPT, to assist\nannotation workflows. In our proposed approach, human annotators focus on\nselecting objects via bounding boxes, while the LMM autonomously generates\nrelevant labels. This human-AI collaborative framework enhances annotation\nefficiency by reducing the cognitive and time burden on human annotators. By\nanalyzing the system's performance across various types of annotation tasks, we\ndemonstrate its ability to generalize to tasks such as object recognition,\nscene description, and fine-grained categorization. Our proposed framework\nhighlights the potential of this approach to redefine annotation workflows,\noffering a scalable and efficient solution for large-scale data labeling in\ncomputer vision. Finally, we discuss how integrating LMMs into the annotation\npipeline can advance bidirectional human-AI alignment, as well as the\nchallenges of alleviating the \"endless annotation\" burden in the face of\ninformation overload by shifting some of the work to AI.",
        "Selection bias is a major obstacle toward valid causal inference in\nepidemiology. Over the past decade, several simple graphical rules based on\ncausal diagrams have been proposed as the sufficient identification conditions\nfor addressing selection bias and recovering causal effects. However, these\nsimple graphical rules are usually coupled with specific identification\nstrategies and estimators. In this article, we show two important cases of\nselection bias that cannot be addressed by these simple rules and their\nestimators: one case where selection is a descendant of a collider of the\ntreatment and the outcome, and the other case where selection is affected by\nthe mediator. To address selection bias in these two cases, we construct\nidentification formulas by the g-computation and the inverse probability\nweighting (IPW) methods based on single-world intervention graphs (SWIGs). They\nare generalized to recover the average treatment effect by adjusting for\npost-treatment upstream causes of selection. We propose two IPW estimators and\ntheir variance estimators to recover the average treatment effect in the\npresence of selection bias in these two cases. We conduct simulation studies to\nverify the performance of the estimators when the traditional crude\nselected-sample analysis returns erroneous contradictory conclusions to the\ntruth.",
        "Many scientific problems involve data that is embedded in a space with\nperiodic boundary conditions. This can for instance be related to an inherent\ncyclic or rotational symmetry in the data or a spatially extended periodicity.\nWhen analyzing such data, well-tailored methods are needed to obtain efficient\napproaches that obey the periodic boundary conditions of the problem. In this\nwork, we present a method for applying a clustering algorithm to data embedded\nin a periodic domain based on the DBSCAN algorithm, a widely used unsupervised\nmachine learning method that identifies clusters in data. The proposed method\ninternally leverages the conventional DBSCAN algorithm for domains with open\nboundaries, such that it remains compatible with all optimized implementations\nfor neighborhood searches in open domains. In this way, it retains the same\noptimized runtime complexity of $O(N\\log N)$. We demonstrate the workings of\nthe proposed method using synthetic data in one, two and three dimensions and\nalso apply it to a real-world example involving the clustering of bubbles in a\nturbulent flow. The proposed approach is implemented in a ready-to-use Python\npackage that we make publicly available.",
        "In this work we study the problem of user association and resource allocation\nto maximize the proportional fairness of a wireless network with limited\nbackhaul capacity. The optimal solution of this problem requires solving a\nmixed integer non-linear programming problem which generally cannot be solved\nin real time. We propose instead to model the problem as a potential game,\nwhich decreases dramatically the computational complexity and obtains a user\nassociation and resource allocation close to the optimal solution.\nAdditionally, the use of a game-theoretic approach allows an efficient\ndistribution of the computational burden among the computational resources of\nthe network.",
        "Complex events (CEs) play a crucial role in CPS-IoT applications, enabling\nhigh-level decision-making in domains such as smart monitoring and autonomous\nsystems. However, most existing models focus on short-span perception tasks,\nlacking the long-term reasoning required for CE detection. CEs consist of\nsequences of short-time atomic events (AEs) governed by spatiotemporal\ndependencies. Detecting them is difficult due to long, noisy sensor data and\nthe challenge of filtering out irrelevant AEs while capturing meaningful\npatterns. This work explores CE detection as a case study for CPS-IoT\nfoundation models capable of long-term reasoning. We evaluate three approaches:\n(1) leveraging large language models (LLMs), (2) employing various neural\narchitectures that learn CE rules from data, and (3) adopting a neurosymbolic\napproach that integrates neural models with symbolic engines embedding human\nknowledge. Our results show that the state-space model, Mamba, which belongs to\nthe second category, outperforms all methods in accuracy and generalization to\nlonger, unseen sensor traces. These findings suggest that state-space models\ncould be a strong backbone for CPS-IoT foundation models for long-span\nreasoning tasks.",
        "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, yet code generation remains a major challenge. Current\napproaches for obtaining high-quality code data primarily focus on (i)\ncollecting large-scale pre-training data and (ii) synthesizing instruction data\nthrough prompt engineering with powerful models. While pre-training data faces\nquality consistency issues, instruction-based synthesis suffers from limited\ninstruction diversity and inherent biases of LLMs. To address this gap, we\nintroduce UnitCoder, a systematic pipeline leveraging model-generated unit\ntests to both guide and validate the code generation process. Combined with\nlarge-scale package-based retrieval from pre-training corpus, we generate a\ndataset of 500K+ verifiable programs containing diverse API calls. Evaluations\non multiple Python benchmarks (BigCodeBench, HumanEval, MBPP) demonstrate that\nmodels fine-tuned on our synthetic data exhibit consistent performance\nimprovements. Notably, Llama3.1-8B and InternLM2.5-7B improve from 31\\% and\n28\\% to 40\\% and 39\\% success rates on BigCodeBench, respectively. Our work\npresents a scalable approach that leverages model-generated unit tests to guide\nthe synthesis of high-quality code data from pre-training corpora,\ndemonstrating the potential for producing diverse and high-quality\npost-training data at scale. All code and data will be released\n(https:\/\/github.com).",
        "Bimanual robotic manipulation is a long-standing challenge of embodied\nintelligence due to its characteristics of dual-arm spatial-temporal\ncoordination and high-dimensional action spaces. Previous studies rely on\npre-defined action taxonomies or direct teleoperation to alleviate or\ncircumvent these issues, often making them lack simplicity, versatility and\nscalability. Differently, we believe that the most effective and efficient way\nfor teaching bimanual manipulation is learning from human demonstrated videos,\nwhere rich features such as spatial-temporal positions, dynamic postures,\ninteraction states and dexterous transitions are available almost for free. In\nthis work, we propose the YOTO (You Only Teach Once), which can extract and\nthen inject patterns of bimanual actions from as few as a single binocular\nobservation of hand movements, and teach dual robot arms various complex tasks.\nFurthermore, based on keyframes-based motion trajectories, we devise a subtle\nsolution for rapidly generating training demonstrations with diverse variations\nof manipulated objects and their locations. These data can then be used to\nlearn a customized bimanual diffusion policy (BiDP) across diverse scenes. In\nexperiments, YOTO achieves impressive performance in mimicking 5 intricate\nlong-horizon bimanual tasks, possesses strong generalization under different\nvisual and spatial conditions, and outperforms existing visuomotor imitation\nlearning methods in accuracy and efficiency. Our project link is\nhttps:\/\/hnuzhy.github.io\/projects\/YOTO.",
        "Probabilistic model checking traditionally verifies properties on the\nexpected value of a measure of interest. This restriction may fail to capture\nthe quality of service of a significant proportion of a system's runs,\nespecially when the probability distribution of the measure of interest is\npoorly represented by its expected value due to heavy-tail behaviors or\nmultiple modalities. Recent works inspired by distributional reinforcement\nlearning use discrete histograms to approximate integer reward distribution,\nbut they struggle with continuous reward space and present challenges in\nbalancing accuracy and scalability. We propose a novel method for handling both\ncontinuous and discrete reward distributions in Discrete Time Markov Chains\nusing moment matching with Erlang mixtures. By analytically deriving\nhigher-order moments through Moment Generating Functions, our method\napproximates the reward distribution with theoretically bounded error while\npreserving the statistical properties of the true distribution. This detailed\ndistributional insight enables the formulation and robust model checking of\nquality properties based on the entire reward distribution function, rather\nthan restricting to its expected value. We include a theoretical foundation\nensuring bounded approximation errors, along with an experimental evaluation\ndemonstrating our method's accuracy and scalability in practical model-checking\nproblems.",
        "Drug discovery is a critical task in biomedical natural language processing\n(NLP), yet explainable drug discovery remains underexplored. Meanwhile, large\nlanguage models (LLMs) have shown remarkable abilities in natural language\nunderstanding and generation. Leveraging LLMs for explainable drug discovery\nhas the potential to improve downstream tasks and real-world applications. In\nthis study, we utilize open-source drug knowledge graphs, clinical trial data,\nand PubMed publications to construct a comprehensive dataset for the\nexplainable drug discovery task, named \\textbf{expRxRec}. Furthermore, we\nintroduce \\textbf{KEDRec-LM}, an instruction-tuned LLM which distills knowledge\nfrom rich medical knowledge corpus for drug recommendation and rationale\ngeneration. To encourage further research in this area, we will publicly\nrelease\\footnote{A copy is attached with this submission} both the dataset and\nKEDRec-LM."
      ]
    }
  },
  {
    "id":2412.14846,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"Evaluation of the Impact of Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) Definition for Radiation Treatment Planning (RTP) of Inoperable High Grade Gliomas (HGGs)",
    "start_abstract":"Aim and Background . Inoperable high-grade gliomas (HGGs) comprise a specific group of brain tumors portending very poor prognosis. In the absence surgical management, radiation therapy (RT) offers primary local treatment modality for inoperable HGGs. Optimal target definition planning (RTP) HGGs is difficult task given diffusely infiltrative nature disease. this context, detailed multimodality imaging information may add to accuracy in We evaluated impact Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) RTP study. Materials Methods Twenty-five patients with clinical diagnosis HGG were included GTV was based Computed Tomography- (CT-) simulation images only or both CT-simulation MR images, comparative assessment performed investigate incorporation MRI into Results Median volume acquired by using use CT 65.3 (39.6 - 94.3) cc 76.1 (46.8-108.9) cc, respectively. Incorporation has resulted median increase 12.61% (6%-19%) defined only, which statistically significant (p &lt; 0.05). Conclusion improve have implications dose escalation\/intensification strategies despite need further supporting evidence.",
    "start_categories":[
      "physics.med-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation"
      ],
      "abstract":[
        "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able process 2D images while data used in clinical practice consists of 3D volumes. In this work we propose an approach segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end MRI volumes depicting prostate, learns predict for whole volume at once. We introduce novel objective function, that optimise during training, Dice coefficient. way can deal with situations where there strong imbalance between number foreground background voxels. To cope limited annotated available augment applying random non-linear transformations histogram matching. show our experimental evaluation achieves good performances challenging test requiring fraction processing time needed by other previous methods."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Bidirectional Prototype-Reward co-Evolution for Test-Time Adaptation of\n  Vision-Language Models",
        "Velocity-free task-space regulator for robot manipulators with external\n  disturbances",
        "Learning from Active Human Involvement through Proxy Value Propagation",
        "Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID\n  Guidance",
        "EU-Nets: Enhanced, Explainable and Parsimonious U-Nets",
        "HoneypotNet: Backdoor Attacks Against Model Extraction",
        "Integral gains for non-autonomous Wazewski systems",
        "Multi-agent Auto-Bidding with Latent Graph Diffusion Models",
        "The Large-Scale Structure of Entanglement in Quantum Many-body Systems",
        "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
        "Equivariant localization in Batalin-Vilkovisky formalism",
        "Insights into dendritic growth mechanisms in batteries: A combined\n  machine learning and computational study",
        "General Stability Estimates in NonLocal Traffic Models for Several\n  Populations",
        "A non-D-continuum with weakly infinite-dimensional closed\n  set-aposyndetic Whitney levels",
        "We Can't Understand AI Using our Existing Vocabulary",
        "Evaluation of CGRA Toolchains",
        "FedSA: A Unified Representation Learning via Semantic Anchors for\n  Prototype-based Federated Learning",
        "SWIPTNet: A Unified Deep Learning Framework for SWIPT based on GNN and\n  Transfer Learning",
        "Understanding and Mitigating Distribution Shifts For Machine Learning\n  Force Fields",
        "Accurate 3D Grapevine Structure Extraction from High-Resolution Point\n  Clouds",
        "System-level Analysis of Dual-Mode Networked Sensing: ISAC Integration &\n  Coordination Gains",
        "Pseudo-cones and measure transport",
        "Infrastructure for AI Agents",
        "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model\n  Pretraining",
        "ProgCo: Program Helps Self-Correction of Large Language Models",
        "Global Solvability for the Compressible Hookean Viscoelastic Fluids with\n  a Free Boundary in Some Classes of Large Data",
        "Graded Courrent PDL",
        "An Efficient Approach to Fractional Analysis for Non-Linear Coupled\n  Thermo-Elastic Systems",
        "Options-Aware Dense Retrieval for Multiple-Choice query Answering"
      ],
      "abstract":[
        "Test-time adaptation (TTA) is crucial in maintaining Vision-Language Models\n(VLMs) performance when facing real-world distribution shifts, particularly\nwhen the source data or target labels are inaccessible. Existing TTA methods\nrely on CLIP's output probability distribution for feature evaluation, which\ncan introduce biases under domain shifts. This misalignment may cause features\nto be misclassified due to text priors or incorrect textual associations. To\naddress these limitations, we propose Bidirectional Prototype-Reward\nco-Evolution (BPRE), a novel TTA framework for VLMs that integrates feature\nquality assessment with prototype evolution through a synergistic feedback\nloop. BPRE first employs a Multi-Dimensional Quality-Aware Reward Module to\nevaluate feature quality and guide prototype refinement precisely. The\ncontinuous refinement of prototype quality through Prototype-Reward Interactive\nEvolution will subsequently enhance the computation of more robust\nMulti-Dimensional Quality-Aware Reward Scores. Through the bidirectional\ninteraction, the precision of rewards and the evolution of prototypes mutually\nreinforce each other, forming a self-evolving cycle. Extensive experiments are\nconducted across 15 diverse recognition datasets encompassing natural\ndistribution shifts and cross-dataset generalization scenarios. Results\ndemonstrate that BPRE consistently achieves superior average performance\ncompared to state-of-the-art methods across different model architectures, such\nas ResNet-50 and ViT-B\/16. By emphasizing comprehensive feature evaluation and\nbidirectional knowledge refinement, BPRE advances VLM generalization\ncapabilities, offering a new perspective on TTA.",
        "This paper addresses the problem of task-space robust regulation of robot\nmanipulators subject to external disturbances. A velocity-free control law is\nproposed by combining the internal model principle and the passivity-based\noutput-feedback control approach. The developed output-feedback controller\nensures not only asymptotic convergence of the regulation error but also\nsuppression of unwanted external step\/sinusoidal disturbances. The potential of\nthe proposed method lies in its simplicity, intuitively appealing, and simple\ngain selection criteria for synthesis of multi-joint robot manipulator control\nsystems.",
        "Learning from active human involvement enables the human subject to actively\nintervene and demonstrate to the AI agent during training. The interaction and\ncorrective feedback from human brings safety and AI alignment to the learning\nprocess. In this work, we propose a new reward-free active human involvement\nmethod called Proxy Value Propagation for policy optimization. Our key insight\nis that a proxy value function can be designed to express human intents,\nwherein state-action pairs in the human demonstration are labeled with high\nvalues, while those agents' actions that are intervened receive low values.\nThrough the TD-learning framework, labeled values of demonstrated state-action\npairs are further propagated to other unlabeled data generated from agents'\nexploration. The proxy value function thus induces a policy that faithfully\nemulates human behaviors. Human-in-the-loop experiments show the generality and\nefficiency of our method. With minimal modification to existing reinforcement\nlearning algorithms, our method can learn to solve continuous and discrete\ncontrol tasks with various human control devices, including the challenging\ntask of driving in Grand Theft Auto V. Demo video and code are available at:\nhttps:\/\/metadriverse.github.io\/pvp",
        "Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in\nreconstructing detailed 3D scenes within multi-view setups and the emergence of\nlarge 2D human foundation models, we introduce Arc2Avatar, the first SDS-based\nmethod utilizing a human face foundation model as guidance with just a single\nimage as input. To achieve that, we extend such a model for diverse-view human\nhead generation by fine-tuning on synthetic data and modifying its\nconditioning. Our avatars maintain a dense correspondence with a human face\nmesh template, allowing blendshape-based expression generation. This is\nachieved through a modified 3DGS approach, connectivity regularizers, and a\nstrategic initialization tailored for our task. Additionally, we propose an\noptional efficient SDS-based correction step to refine the blendshape\nexpressions, enhancing realism and diversity. Experiments demonstrate that\nArc2Avatar achieves state-of-the-art realism and identity preservation,\neffectively addressing color issues by allowing the use of very low guidance,\nenabled by our strong identity prior and initialization strategy, without\ncompromising detail. Please visit https:\/\/arc2avatar.github.io for more\nresources.",
        "In this study, we propose MHEX+, a framework adaptable to any U-Net\narchitecture. Built upon MHEX+, we introduce novel U-Net variants, EU-Nets,\nwhich enhance explainability and uncertainty estimation, addressing the\nlimitations of traditional U-Net models while improving performance and\nstability. A key innovation is the Equivalent Convolutional Kernel, which\nunifies consecutive convolutional layers, boosting interpretability. For\nuncertainty estimation, we propose the collaboration gradient approach,\nmeasuring gradient consistency across decoder layers. Notably, EU-Nets achieve\nan average accuracy improvement of 1.389\\% and a variance reduction of 0.83\\%\nacross all networks and datasets in our experiments, requiring fewer than 0.1M\nparameters.",
        "Model extraction attacks are one type of inference-time attacks that\napproximate the functionality and performance of a black-box victim model by\nlaunching a certain number of queries to the model and then leveraging the\nmodel's predictions to train a substitute model. These attacks pose severe\nsecurity threats to production models and MLaaS platforms and could cause\nsignificant monetary losses to the model owners. A body of work has proposed to\ndefend machine learning models against model extraction attacks, including both\nactive defense methods that modify the model's outputs or increase the query\noverhead to avoid extraction and passive defense methods that detect malicious\nqueries or leverage watermarks to perform post-verification. In this work, we\nintroduce a new defense paradigm called attack as defense which modifies the\nmodel's output to be poisonous such that any malicious users that attempt to\nuse the output to train a substitute model will be poisoned. To this end, we\npropose a novel lightweight backdoor attack method dubbed HoneypotNet that\nreplaces the classification layer of the victim model with a honeypot layer and\nthen fine-tunes the honeypot layer with a shadow model (to simulate model\nextraction) via bi-level optimization to modify its output to be poisonous\nwhile remaining the original performance. We empirically demonstrate on four\ncommonly used benchmark datasets that HoneypotNet can inject backdoors into\nsubstitute models with a high success rate. The injected backdoor not only\nfacilitates ownership verification but also disrupts the functionality of\nsubstitute models, serving as a significant deterrent to model extraction\nattacks.",
        "In this work we consider linear non-autonomous systems of Wazewski type on\nHilbert spaces and provide a new approach to study their stability properties\nby means of a decomposition into subsystems and conditions implied on the\ninterconnection properties. These conditions are of the small-gain type but the\nappoach is based on a conceptually new notion which we call integral gain. This\nnotion is introduced for the first time in this paper. We compare our approach\nwith known results from the literature and demonstrate advantages of our\nresults.",
        "This paper proposes a diffusion-based auto-bidding framework that leverages\ngraph representations to model large-scale auction environments. In such\nsettings, agents must dynamically optimize bidding strategies under constraints\ndefined by key performance indicator (KPI) metrics, all while operating in\ncompetitive environments characterized by uncertain, sparse, and stochastic\nvariables. To address these challenges, we introduce a novel approach combining\nlearnable graph-based embeddings with a planning-based latent diffusion model\n(LDM). By capturing patterns and nuances underlying the interdependence of\nimpression opportunities and the multi-agent dynamics of the auction\nenvironment, the graph representation enable expressive computations regarding\nauto-bidding outcomes. With reward alignment techniques, the LDM's posterior is\nfine-tuned to generate auto-bidding trajectories that maximize KPI metrics\nwhile satisfying constraint thresholds. Empirical evaluations on both\nreal-world and synthetic auction environments demonstrate significant\nimprovements in auto-bidding performance across multiple common KPI metrics, as\nwell as accuracy in forecasting auction outcomes.",
        "We show that the thermodynamic limit of a many-body system can reveal\nentanglement properties that are hard to detect in finite-size systems --\nsimilar to how phase transitions only sharply emerge in the thermodynamic\nlimit. The resulting operational entanglement properties are in one-to-one\ncorrespondence with abstract properties of the local observable algebras that\nemerge in the thermodynamic limit. These properties are insensitive to finite\nperturbations and hence describe the \\emph{large-scale structure of\nentanglement} of many-body systems. We formulate and discuss the emerging\nstructures and open questions, both for gapped and gapless many-body systems.\nIn particular, we show that every gapped phase of matter, even the trivial one,\nin $D\\geq 2$ dimensions contains models with the strongest possible bipartite\nlarge-scale entanglement. Conversely, we conjecture the existence of\ntopological phases of matter, where all representatives have the strongest form\nof entanglement.",
        "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
        "We derive equivariant localization formulas of Atiyah--Bott and cohomological\nfield theory types in the Batalin-Vilkovisky formalism and discuss their\napplications in Poisson geometry and quantum field theory.",
        "In recent years, researchers have increasingly sought batteries as an\nefficient and cost-effective solution for energy storage and supply, owing to\ntheir high energy density, low cost, and environmental resilience. However, the\nissue of dendrite growth has emerged as a significant obstacle in battery\ndevelopment. Excessive dendrite growth during charging and discharging\nprocesses can lead to battery short-circuiting, degradation of electrochemical\nperformance, reduced cycle life, and abnormal exothermic events. Consequently,\nunderstanding the dendrite growth process has become a key challenge for\nresearchers. In this study, we investigated dendrite growth mechanisms in\nbatteries using a combined machine learning approach, specifically a\ntwo-dimensional artificial convolutional neural network (CNN) model, along with\ncomputational methods. We developed two distinct computer models to predict\ndendrite growth in batteries. The CNN-1 model employs standard convolutional\nneural network techniques for dendritic growth prediction, while CNN-2\nintegrates additional physical parameters to enhance model robustness. Our\nresults demonstrate that CNN-2 significantly enhances prediction accuracy,\noffering deeper insights into the impact of physical factors on dendritic\ngrowth. This improved model effectively captures the dynamic nature of dendrite\nformation, exhibiting high accuracy and sensitivity. These findings contribute\nto the advancement of safer and more reliable energy storage systems.",
        "We prove global existence, uniqueness and $\\L1$ stability of solutions to\ngeneral systems of nonlocal conservation laws modeling multiclass vehicular\ntraffic. Each class follows its own speed law and has specific effects on the\nother classes' speeds. Moreover, general explicit dependencies of the speed\nlaws on space and time are allowed. Solutions are proved to depend continuously\n-- in suitable norms -- on all terms appearing in the equations, as well as on\nthe initial data. Numerical simulations show the relevance and the effects of\nthe nonlocal terms.",
        "In this paper, we introduce the new class of continua; weakly\ninfinite-dimensional closed set-aposyndetic continua. With this notion, we show\nthat there exists a non-D-continuum such that each positive Whitney level of\nthe hyperspace of the continuum is a weakly infinite-dimensional closed\nset-aposyndetic continuum. This result strengthens those of van Douwen and\nGoodykoontz [2], Illanes [7], and the main result of Illanes et al. [9].",
        "This position paper argues that, in order to understand AI, we cannot rely on\nour existing vocabulary of human words. Instead, we should strive to develop\nneologisms: new words that represent precise human concepts that we want to\nteach machines, or machine concepts that we need to learn. We start from the\npremise that humans and machines have differing concepts. This means\ninterpretability can be framed as a communication problem: humans must be able\nto reference and control machine concepts, and communicate human concepts to\nmachines. Creating a shared human-machine language through developing\nneologisms, we believe, could solve this communication problem. Successful\nneologisms achieve a useful amount of abstraction: not too detailed, so they're\nreusable in many contexts, and not too high-level, so they convey precise\ninformation. As a proof of concept, we demonstrate how a \"length neologism\"\nenables controlling LLM response length, while a \"diversity neologism\" allows\nsampling more variable responses. Taken together, we argue that we cannot\nunderstand AI using our existing vocabulary, and expanding it through\nneologisms creates opportunities for both controlling and understanding\nmachines better.",
        "Increasing demands for computing power also propel the need for\nenergy-efficient SoC accelerator architectures. One class for such accelerators\nare so-called processor arrays, which typically integrate a two-dimensional\nmesh of interconnected processing elements (PEs). Such arrays are specifically\ndesigned to accelerate the execution of multidimensional nested loops by\nexploiting the intrinsic parallelism of such loops. Coarse-grained\nreconfigurable arrays (CGRAs) belong to this class of accelerator\narchitectures. In this work, we analyze four toolchains for mapping loop\nprograms onto CGRAs and compare the resulting mappings wrt. performance, i.e.,\nlatency. While most toolchains succeed in simpler kernels like general matrix\nmultiplication, some struggle to find valid mappings for more complex loops\nlike a triangular solver. Furthermore, we observe that the considered CGRA\nmappers generally tend to underutilize the available PEs.",
        "Prototype-based federated learning has emerged as a promising approach that\nshares lightweight prototypes to transfer knowledge among clients with data\nheterogeneity in a model-agnostic manner. However, existing methods often\ncollect prototypes directly from local models, which inevitably introduce\ninconsistencies into representation learning due to the biased data\ndistributions and differing model architectures among clients. In this paper,\nwe identify that both statistical and model heterogeneity create a vicious\ncycle of representation inconsistency, classifier divergence, and skewed\nprototype alignment, which negatively impacts the performance of clients. To\nbreak the vicious cycle, we propose a novel framework named Federated Learning\nvia Semantic Anchors (FedSA) to decouple the generation of prototypes from\nlocal representation learning. We introduce a novel perspective that uses\nsimple yet effective semantic anchors serving as prototypes to guide local\nmodels in learning consistent representations. By incorporating semantic\nanchors, we further propose anchor-based regularization with margin-enhanced\ncontrastive learning and anchor-based classifier calibration to correct feature\nextractors and calibrate classifiers across clients, achieving intra-class\ncompactness and inter-class separability of prototypes while ensuring\nconsistent decision boundaries. We then update the semantic anchors with these\nconsistent and discriminative prototypes, which iteratively encourage clients\nto collaboratively learn a unified data representation with robust\ngeneralization. Extensive experiments under both statistical and model\nheterogeneity settings show that FedSA significantly outperforms existing\nprototype-based FL methods on various classification tasks.",
        "This paper investigates the deep learning based approaches for simultaneous\nwireless information and power transfer (SWIPT). The quality-of-service (QoS)\nconstrained sum-rate maximization problems are, respectively, formulated for\npower-splitting (PS) receivers and time-switching (TS) receivers and solved by\na unified graph neural network (GNN) based model termed SWIPT net (SWIPTNet).\nTo improve the performance of SWIPTNet, we first propose a single-type output\nmethod to reduce the learning complexity and facilitate the satisfaction of QoS\nconstraints, and then, utilize the Laplace transform to enhance input features\nwith the structural information. Besides, we adopt the multi-head attention and\nlayer connection to enhance feature extracting. Furthermore, we present the\nimplementation of transfer learning to the SWIPTNet between PS and TS\nreceivers. Ablation studies show the effectiveness of key components in the\nSWIPTNet. Numerical results also demonstrate the capability of SWIPTNet in\nachieving near-optimal performance with millisecond-level inference speed which\nis much faster than the traditional optimization algorithms. We also show the\neffectiveness of transfer learning via fast convergence and expressive\ncapability improvement.",
        "Machine Learning Force Fields (MLFFs) are a promising alternative to\nexpensive ab initio quantum mechanical molecular simulations. Given the\ndiversity of chemical spaces that are of interest and the cost of generating\nnew data, it is important to understand how MLFFs generalize beyond their\ntraining distributions. In order to characterize and better understand\ndistribution shifts in MLFFs, we conduct diagnostic experiments on chemical\ndatasets, revealing common shifts that pose significant challenges, even for\nlarge foundation models trained on extensive data. Based on these observations,\nwe hypothesize that current supervised training methods inadequately regularize\nMLFFs, resulting in overfitting and learning poor representations of\nout-of-distribution systems. We then propose two new methods as initial steps\nfor mitigating distribution shifts for MLFFs. Our methods focus on test-time\nrefinement strategies that incur minimal computational cost and do not use\nexpensive ab initio reference labels. The first strategy, based on spectral\ngraph theory, modifies the edges of test graphs to align with graph structures\nseen during training. Our second strategy improves representations for\nout-of-distribution systems at test-time by taking gradient steps using an\nauxiliary objective, such as a cheap physical prior. Our test-time refinement\nstrategies significantly reduce errors on out-of-distribution systems,\nsuggesting that MLFFs are capable of and can move towards modeling diverse\nchemical spaces, but are not being effectively trained to do so. Our\nexperiments establish clear benchmarks for evaluating the generalization\ncapabilities of the next generation of MLFFs. Our code is available at\nhttps:\/\/tkreiman.github.io\/projects\/mlff_distribution_shifts\/.",
        "Accurate 3D modelling of grapevines is crucial for precision viticulture,\nparticularly for informed pruning decisions and automated management\ntechniques. However, the intricate structure of grapevines poses significant\nchallenges for traditional skeletonization algorithms. This paper presents an\nadaptation of the Smart-Tree algorithm for 3D grapevine modelling, addressing\nthe unique characteristics of grapevine structures. We introduce a graph-based\nmethod for disambiguating skeletonization. Our method delineates individual\ncane skeletons, which are crucial for precise analysis and management. We\nvalidate our approach using annotated real-world grapevine point clouds,\ndemonstrating improvement of 15.8% in the F1 score compared to the original\nSmart-Tree algorithm. This research contributes to advancing 3D grapevine\nmodelling techniques, potentially enhancing both the sustainability and\nprofitability of grape production through more precise and automated\nviticulture practices",
        "This paper characterizes integration and coordination gains in dense\nmillimeter-wave ISAC networks through a dual-mode framework that combines\nmonostatic and multistatic sensing. A comprehensive system-level analysis is\nconducted, accounting for base station (BS) density, power allocation, antenna\nmisalignment, radar cross-section (RCS) fluctuations, clutter, bistatic\ngeometry, channel fading, and self-interference cancellation (SIC) efficiency.\nUsing stochastic geometry, coverage probabilities and ergodic rates for sensing\nand communication are derived, revealing tradeoffs among BS density, beamwidth,\nand power allocation. It is shown that the communication performance sustained\nreliable operation despite the overlaid sensing functionality. In contrast, the\nresults reveal the foundational role of spatial sensing diversity, driven by\nthe dual-mode operation, to compensate for the weak sensing reflections and\nvulnerability to imperfect SIC along with interference and clutter. To this\nend, we identify a system transition from monostatic to multistatic-dominant\nsensing operation as a function of the SIC efficiency. In the latter case,\nusing six multistatic BSs instead of a single bistatic receiver improved\nsensing coverage probability by over 100%, highlighting the coordination gain.\nMoreover, comparisons with pure communication networks confirm substantial\nintegration gain. Specifically, dual-mode networked sensing with four\ncooperative BSs can double throughput, while multistatic sensing alone improves\nthroughput by over 50%.",
        "A recent result on the Gauss image problem for pseudo-cones can be\ninterpreted as a measure transport, performed by the reverse radial Gauss map\nof a pseudo-cone. We find a cost function that is minimized by this transport\nmap, and we prove an analogue of Rockafellar's characterization of the\nsubdifferentials of convex functions.",
        "Increasingly many AI systems can plan and execute interactions in open-ended\nenvironments, such as making phone calls or buying online goods. As developers\ngrow the space of tasks that such AI agents can accomplish, we will need tools\nboth to unlock their benefits and manage their risks. Current tools are largely\ninsufficient because they are not designed to shape how agents interact with\nexisting institutions (e.g., legal and economic systems) or actors (e.g.,\ndigital service providers, humans, other AI agents). For example, alignment\ntechniques by nature do not assure counterparties that some human will be held\naccountable when a user instructs an agent to perform an illegal action. To\nfill this gap, we propose the concept of agent infrastructure: technical\nsystems and shared protocols external to agents that are designed to mediate\nand influence their interactions with and impacts on their environments. Agent\ninfrastructure comprises both new tools and reconfigurations or extensions of\nexisting tools. For example, to facilitate accountability, protocols that tie\nusers to agents could build upon existing systems for user authentication, such\nas OpenID. Just as the Internet relies on infrastructure like HTTPS, we argue\nthat agent infrastructure will be similarly indispensable to ecosystems of\nagents. We identify three functions for agent infrastructure: 1) attributing\nactions, properties, and other information to specific agents, their users, or\nother actors; 2) shaping agents' interactions; and 3) detecting and remedying\nharmful actions from agents. We propose infrastructure that could help achieve\neach function, explaining use cases, adoption, limitations, and open questions.\nMaking progress on agent infrastructure can prepare society for the adoption of\nmore advanced agents.",
        "Pretraining large language models (LLMs) on vast and heterogeneous datasets\nis crucial for achieving state-of-the-art performance across diverse downstream\ntasks. However, current training paradigms treat all samples equally,\noverlooking the importance or relevance of individual samples throughout the\ntraining process. Existing reweighting strategies, which primarily focus on\ngroup-level data importance, fail to leverage fine-grained instance-level\ninformation and do not adapt dynamically to individual sample importance as\ntraining progresses. In this paper, we introduce novel algorithms for dynamic,\ninstance-level data reweighting aimed at improving both the efficiency and\neffectiveness of LLM pretraining. Our methods adjust the weight of each\ntraining sample based on its loss value in an online fashion, allowing the\nmodel to dynamically focus on more informative or important samples at the\ncurrent training stage. In particular, our framework allows us to\nsystematically devise reweighting strategies deprioritizing redundant or\nuninformative data, which we find tend to work best. Furthermore, we develop a\nnew theoretical framework for analyzing the impact of loss-based reweighting on\nthe convergence of gradient-based optimization, providing the first formal\ncharacterization of how these strategies affect convergence bounds. We\nempirically validate our approach across a spectrum of tasks, from pretraining\n7B and 1.4B parameter LLMs to smaller-scale language models and linear\nregression problems, demonstrating that our loss-based reweighting approach can\nlead to faster convergence and significantly improved performance.",
        "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools.",
        "Recently Jiang-Jiang established a global (in time) existence result for\nunique strong solutions of the two-dimensional (2D) free-boundary problem of an\nincompressible Hookean viscoelastic fluid, the rest state of which is defined\nin a slab, in some classes of large data [28]. In particular, Jiang-Jiang's\nmathematical result shows that, if the initial free boundary is flat, the way\nthe elastic deformation under the large elasticity coefficient $\\kappa$ acts on\nthe free boundary prevents the natural tendency of the fluid to form\nsingularities, even when the initial velocity is properly large. However it is\nnot clear whether their result can be extended to the corresponding 3D case. In\nthis paper, we further find a similar result in the 3D stratified (immiscible)\ncompressible Hookean viscoelastic fluids in an infinite slab with two\nrestrictive conditions: that the elasticity coefficients of two fluids are\nequal, and that the initial density functions satisfy the asymptotic stability\ncondition in Lagrangian coordinates. These two restrictive conditions in the\ncompressible case contribute us to avoid the essential obstacles that would be\nfaced in the extension of Jiang-Jiang's result from two dimensions to our 3D\ncase. In addition, we can further obtain a new result regarding the vanishing\nphenomena of the nonlinear interactions of solutions with the fixed initial\nvelocity and the initial zero perturbation deformation. Such a new result\nroughly presents that the solutions of the problem considered by us can be\napproximated by the ones of a linear problem for sufficiently large $\\kappa$.",
        "Propositional Dynamic Logic, PDL, is a modal logic designed to formalize the\nreasoning about programs. By extending accessibility between states to states\nand state sets, concurrent propositional dynamic logic CPDL, is introduced to\ninclude concurrent programs due to Peleg and Goldblatt. We study a many-valued\ngeneralization of CPDL where the satisfiability and the reachability relation\nbetween states and state sets are graded over a finite {\\L}ukasiewicz chain.\nFinitely-valued dynamic logic has been shown to be useful in formalizing\nreasoning about program behaviors under uncertainty. We obtain completeness\nresults for all finitely valued PDL.",
        "Nonlinear thermoelastic systems play a crucial role in understanding thermal\nconductivity, stresses, elasticity, and temperature interactions. This research\nfocuses on finding solutions to these systems in their fractional forms, which\nis a significant aspect of the study. We consider various proposed models\nrelated to fractional thermoelasticity and derive results through sophisticated\nmethodologies. Numerical simulations are conducted for both fractional and\ninteger order thermoelastic coupled systems, with results presented in tables\nand graphs. The graphs indicate a close correspondence between the approximate\nand exact solutions. The solutions obtained demonstrate convergence for both\nfractional and integer order problems, ensuring accurate modeling. Furthermore,\nthe tables confirm that greater accuracy can be achieved by increasing the\nnumber of terms in the series of solutions.",
        "Long-context multiple-choice question answering tasks require robust\nreasoning over extensive text sources. Since most of the pre-trained\ntransformer models are restricted to processing only a few hundred words at a\ntime, successful completion of such tasks often relies on the identification of\nevidence spans, such as sentences, that provide supporting evidence for\nselecting the correct answer. Prior research in this domain has predominantly\nutilized pre-trained dense retrieval models, given the absence of supervision\nto fine-tune the retrieval process. This paper proposes a novel method called\nOptions Aware Dense Retrieval (OADR) to address these challenges. ORDA uses an\ninnovative approach to fine-tuning retrieval by leveraging query-options\nembeddings, which aim to mimic the embeddings of the oracle query (i.e., the\nquery paired with the correct answer) for enhanced identification of supporting\nevidence. Through experiments conducted on the QuALITY benchmark dataset, we\ndemonstrate that our proposed model surpasses existing baselines in terms of\nperformance and accuracy."
      ]
    }
  },
  {
    "id":2412.14846,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation",
    "start_abstract":"Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able process 2D images while data used in clinical practice consists of 3D volumes. In this work we propose an approach segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end MRI volumes depicting prostate, learns predict for whole volume at once. We introduce novel objective function, that optimise during training, Dice coefficient. way can deal with situations where there strong imbalance between number foreground background voxels. To cope limited annotated available augment applying random non-linear transformations histogram matching. show our experimental evaluation achieves good performances challenging test requiring fraction processing time needed by other previous methods.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Evaluation of the Impact of Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) Definition for Radiation Treatment Planning (RTP) of Inoperable High Grade Gliomas (HGGs)"
      ],
      "abstract":[
        "Aim and Background . Inoperable high-grade gliomas (HGGs) comprise a specific group of brain tumors portending very poor prognosis. In the absence surgical management, radiation therapy (RT) offers primary local treatment modality for inoperable HGGs. Optimal target definition planning (RTP) HGGs is difficult task given diffusely infiltrative nature disease. this context, detailed multimodality imaging information may add to accuracy in We evaluated impact Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) RTP study. Materials Methods Twenty-five patients with clinical diagnosis HGG were included GTV was based Computed Tomography- (CT-) simulation images only or both CT-simulation MR images, comparative assessment performed investigate incorporation MRI into Results Median volume acquired by using use CT 65.3 (39.6 - 94.3) cc 76.1 (46.8-108.9) cc, respectively. Incorporation has resulted median increase 12.61% (6%-19%) defined only, which statistically significant (p &lt; 0.05). Conclusion improve have implications dose escalation\/intensification strategies despite need further supporting evidence."
      ],
      "categories":[
        "physics.med-ph"
      ]
    },
    "list":{
      "title":[
        "Efficient sampling approaches based on generalized Golub-Kahan methods\n  for large-scale hierarchical Bayesian inverse problems",
        "Optimal Insurance under Endogenous Default and Background Risk",
        "Optimizing confidence in negative-partial-transpose-based entanglement\n  criteria",
        "On almost Gallai colourings in complete graphs",
        "Stabilization of an unstable reaction-diffusion PDE with input delay\n  despite state and input quantization",
        "Rigidity in a Fixed Number Field and a Directional $p$-Adic Littlewood\n  Conjecture for Algebraic Vectors",
        "Enhancing finite-difference based derivative-free optimization methods\n  with machine learning",
        "New Representations of Catalan's Constant, Apery's Constant and the\n  Euler Numbers Obtained from the Half Hyperbolic Secant Distribution",
        "The COSMOS-Web deep galaxy group catalog up to $z=3.7$",
        "Counting principal ideals of small norm in the simplest cubic fields",
        "Explaining the Unexplainable: A Systematic Review of Explainable AI in\n  Finance",
        "A new transcendence measure for the values of the exponential function\n  at algebraic arguments",
        "Sensitivity analysis of path-dependent options in an incomplete market\n  with pathwise functional Ito calculus",
        "Scattering resonances and pairing in a Rabi-coupled Fermi gas",
        "Feature Learning beyond the Lazy-Rich Dichotomy: Insights from\n  Representational Geometry",
        "Spatiotemporal Gaussian Optimization for 4D Cone Beam CT Reconstruction\n  from Sparse Projections",
        "Partial domination of middle graphs",
        "Lattice Boltzmann simulation reveals supercritical bifurcation in flow\n  mode transitions of power-law fluids in the four-roll mill",
        "Generic Structural Stability for $2 \\times 2$ Systems of Hyperbolic\n  Conservation Laws",
        "Analysis of the data on differential cross sections and spin density\n  matrix elements for $\\gamma p \\to \\rho^0 p$",
        "Ensemble Knowledge Distillation for Machine Learning Interatomic\n  Potentials",
        "Numerical verification of the Collatz conjecture for billion digit\n  random numbers",
        "Functional central limit theorem for the subgraph count of the voter\n  model on dynamic random graphs",
        "Multimaterial topology optimization for finite strain elastoplasticity:\n  theory, methods, and applications",
        "Exploring the wettability of liquid iron on refractory oxides with\n  sessile drop technique and density-functional derived Hamaker constants",
        "Characterizations of the semi-harmonious and harmonious quasi-projection\n  pairs on Hilbert $C^*$-modules",
        "Unveiling What Makes Saturn Ring: Quantifying the Amplitudes of Saturn's\n  Planetary Normal-Mode Oscillations and Trends in C-Ring Properties Using\n  Kronoseismology (VII)",
        "On the location of zeros of a quaternion polynomial",
        "Pauli Network Circuit Synthesis with Reinforcement Learning"
      ],
      "abstract":[
        "Uncertainty quantification for large-scale inverse problems remains a\nchallenging task. For linear inverse problems with additive Gaussian noise and\nGaussian priors, the posterior is Gaussian but sampling can be challenging,\nespecially for problems with a very large number of unknown parameters (e.g.,\ndynamic inverse problems) and for problems where computation of the square root\nand inverse of the prior covariance matrix are not feasible. Moreover, for\nhierarchical problems where several hyperparameters that define the prior and\nthe noise model must be estimated from the data, the posterior distribution may\nno longer be Gaussian, even if the forward operator is linear. Performing\nlarge-scale uncertainty quantification for these hierarchical settings requires\nnew computational techniques. In this work, we consider a hierarchical Bayesian\nframework where both the noise and prior variance are modeled as\nhyperparameters. Our approach uses Metropolis-Hastings independence sampling\nwithin Gibbs where the proposal distribution is based on generalized\nGolub-Kahan based methods. We consider two proposal samplers, one that uses a\nlow rank approximation to the conditional covariance matrix and another that\nuses a preconditioned Lanczos method. Numerical examples from seismic imaging,\ndynamic photoacoustic tomography, and atmospheric inverse modeling demonstrate\nthe effectiveness of the described approaches.",
        "This paper studies an optimal insurance problem for a utility-maximizing\nbuyer of insurance, subject to the seller's endogenous default and background\nrisk. An endogenous default occurs when the buyer's contractual indemnity\nexceeds the seller's available reserve, which is random due to the background\nrisk. We obtain an analytical solution to the optimal contract for two types of\ncontracts, differentiated by whether their indemnity functions depend on the\nseller's background risk. The results shed light on the joint effect of the\nseller's default and background risk on the buyer's insurance demand.",
        "A key requirement of any separable quantum state is that its density matrix\nhas a positive partial transpose. For continuous bipartite quantum states,\nviolation of this condition may be tested via the hierarchy of\nnegative-partial-transpose (NPT) based entanglement criteria introduced by\nShchukin and Vogel [Phys. Rev. Lett. 95, 230502 (2005)]. However, a procedure\nfor selecting the optimal NPT-based criterion is currently lacking. Here, we\ndevelop a framework to select the optimal criterion by determining the level of\nconfidence of criteria within the Shchukin and Vogel hierarchy for finite\nmeasurement number, environmental noise, and the optimal allocation of\nmeasurement resources. To demonstrate the utility of our approach, we apply our\nstatistical framework to prominent example Gaussian and non-Gaussian states,\nincluding the two-mode squeezed vacuum state, the quanta-subtracted two-mode\nsqueezed vacuum state, and the two-mode Schr\\\"odinger-cat state. Beyond\nbipartite inseparability tests, our framework can be applied to any Hermitian\nmatrix constructed of observable moments and thus can be utilized for a wide\nvariety of other nonclassicality criteria and multi-mode entanglement tests.",
        "For $t \\in \\mathbb{N}$, we say that a colouring of $E(K_n)$ is\n$\\textit{almost}$ $t$-$\\textit{Gallai}$ if no two rainbow $t$-cliques share an\nedge. Motivated by a lemma of Berkowitz on bounding the modulus of the\ncharacteristic function of clique counts in random graphs, we study the maximum\nnumber $\\tau_t(n)$ of rainbow $t$-cliques in an almost $t$-Gallai colouring of\n$E(K_n)$. For every $t \\ge 4$, we show that $n^{2-o(1)} \\leq \\tau_t(n) =\no(n^2)$. For $t=3$, surprisingly, the behaviour is substantially different. Our\nmain result establishes that $$\\left ( \\frac{1}{2}-o(1) \\right ) n\\log n \\le\n\\tau_3(n) = O\\big (n^{\\sqrt{2}}\\log n \\big ),$$ which gives the first\nnon-trivial improvements over the simple lower and upper bounds. Our proof\ncombines various applications of the probabilistic method and a generalisation\nof the edge-isoperimetric inequality for the hypercube.",
        "We solve the global asymptotic stability problem of an unstable\nreaction-diffusion Partial Differential Equation (PDE) subject to input delay\nand state quantization developing a switched predictor-feedback law. To deal\nwith the input delay, we reformulate the problem as an actuated transport PDE\ncoupled with the original reaction-diffusion PDE. Then, we design a quantized\npredictor-based feedback mechanism that employs a dynamic switching strategy to\nadjust the quantization range and error over time. The stability of the\nclosed-loop system is proven properly combining backstepping with a small-gain\napproach and input-to-state stability techniques, for deriving estimates on\nsolutions, despite the quantization effect and the system's instability. We\nalso extend this result to the input quantization case.",
        "Let $X_n$ be the space of unimodular lattices in $\\RR^n$ and let $A$ be the\nfull diagonal group in $\\on{SL}_n(\\RR)$. It is known that compact $A$-orbits\noriginate from moduls in totally real degree $n$ number fields. Our first\nresult shows that for a natural family of compact orbits $(Ax_k)_k$ all\noriginating from a fixed number field $K$, every weak limit of the Haar\nmeasures on those orbits $m_{Ax_k}$ must contain the Haar measure $m_{X_n}$ as\nan ergodic component. This result generalizes certain aspects of the work by\nAka and Shapira in \\cite{Shapira-Aka} to arbitrary dimensions, as well as\nelements from Shapira-Zheng in \\cite{shapira2021translates}.\n  For every vector $\\overline \\alpha\\in \\RR^n$ and for every rational\napproximation $(\\overline p,q)\\in \\RR^n\\times\\RR$ we can associate the\ndisplacement vector $q\\alpha-\\overline p$. We focus on algebraic vectors,\nnamely $\\overline \\alpha=(\\alpha_1,\\dots,\\alpha_n)$ such that $1, \\alpha_1,\n\\dots, \\alpha_n$ span a rank $n$ number field. For these vectors, we\ninvestigate the size of their displacements as well as the distribution of\ntheir directions. We establish that algebraic vector $\\overline \\alpha$ satisfy\nthe $p$-adic Littlewood Conjecture. Namely, we prove that \\begin{equation}\n  \\liminf_{k \\to \\infty} \\left( k \\|k\\|_p \\right)^{1\/n} \\| k (\\alpha_1, \\dots,\n\\alpha_n) \\|_\\infty = 0. \\end{equation} Additionally, we classify all limiting\ndistributions, with a special weighting, of the sequence of directions of the\ndefects in the $\\varepsilon$-approximations of $(\\alpha_1, \\dots, \\alpha_n)$.\nEach such limiting measure is expressed as the pushforward of an algebraic\nmeasure on $X_n$ to the sphere.\n  Our proof relies on estimates of the asymptotic orders of units in fixed\nnumber fields modulo families of natural numbers and on rigidity results from\n\\cite{ELMV1}.",
        "Derivative-Free Optimization (DFO) involves methods that rely solely on\nevaluations of the objective function. One of the earliest strategies for\ndesigning DFO methods is to adapt first-order methods by replacing gradients\nwith finite-difference approximations. The execution of such methods generates\na rich dataset about the objective function, including iterate points, function\nvalues, approximate gradients, and successful step sizes. In this work, we\npropose a simple auxiliary procedure to leverage this dataset and enhance the\nperformance of finite-difference-based DFO methods. Specifically, our procedure\ntrains a surrogate model using the available data and applies the gradient\nmethod with Armijo line search to the surrogate until it fails to ensure\nsufficient decrease in the true objective function, in which case we revert to\nthe original algorithm and improve our surrogate based on the new available\ninformation. As a proof of concept, we integrate this procedure with the\nderivative-free method proposed in (Optim. Lett. 18: 195--213, 2024). Numerical\nresults demonstrate significant performance improvements, particularly when the\napproximate gradients are also used to train the surrogates.",
        "New expressions and bounds for Catalan's and Apery's constants, derived from\nthe half hyperbolic secant distribution, are presented. These constants are\nobtained by using expressions for the Lorenz curve, the Gini and Theil indices,\nconvolutions and a mixture of distributions, among other approaches. The new\nexpressions are presented both in terms of integral (simple and double)\nrepresentation and also as an interesting series representation. Some of these\nfeatures are well known, while others are new. In addition, some integral\nrepresentations of Euler's numbers are obtained.",
        "Galaxy groups with $M_{tot} \\lesssim 10^{14}$ $M_\\odot$ and up to a few tens\nof members are the most common galaxy environment, marking the transition\nbetween field and massive clusters. Identifying groups plays a crucial role in\nunderstanding structure formation and galaxy evolution. Modern deep surveys\nallow us to build well-characterized samples of groups up to the regime where\nstructures were taking shape. We aimed to build the largest deep catalog of\ngalaxy groups to date over the COSMOS-Web field effective area of 0.45 deg$^2$,\nleveraging the deep high quality data of the new COSMOS-Web photometric catalog\nresulted from the James Webb Space Telescope observations of the COSMOS-Web\nfield. We performed the group search with the AMICO algorithm, a linear matched\nfilter based on an analytical model for the group signal. AMICO has already\nbeen tested in wide and deep field surveys, including COSMOS data up to $z=2$.\nIn this work, we tested the algorithm performances at even higher redshift and\nsearched for protocluster cores at $z>2$. We compiled a list of known\nprotoclusters in COSMOS at $2 \\leq z \\leq 3.7$, matched them with our\ndetections and studied the clustering of the detected cores. We estimated\npurity and completeness of our sample by creating data-driven mocks with the\nSinFoniA code and linked signal-to-noise to purity. We detected 1678 groups in\nthe COSMOS-Web field up to $z=3.7$, including lists of members extending nearly\ntwo magnitudes deeper than the previous AMICO-COSMOS catalog. 756 groups were\ndetected with purity of 80\\%. More than 500 groups have their redshift\nconfirmed by assigning spectroscopic counterparts. This group catalog offers a\nunique opportunity to explore galaxy evolution in different environments\nspanning $\\sim$12 Gyr and to study groups, from the least rich population to\nthe formation of the most massive clusters.",
        "We estimate the number of principal ideals $ I $ of norm $ \\mathrm{N}(I) \\leq\nx $ in the family of the simplest cubic fields. The advantage of our result is\nthat it provides the correct order of magnitude for arbitrary $ x \\geq 1 $,\neven when $ x $ is significantly smaller than the discriminant. In particular,\nit shows that there exist surprisingly many principal ideals of small norm.",
        "Practitioners and researchers trying to strike a balance between accuracy and\ntransparency center Explainable Artificial Intelligence (XAI) at the junction\nof finance. This paper offers a thorough overview of the changing scene of XAI\napplications in finance together with domain-specific implementations,\nmethodological developments, and trend mapping of research. Using bibliometric\nand content analysis, we find topic clusters, significant research, and most\noften used explainability strategies used in financial industries. Our results\nshow a substantial dependence on post-hoc interpretability techniques;\nattention mechanisms, feature importance analysis and SHAP are the most often\nused techniques among them. This review stresses the need of multidisciplinary\napproaches combining financial knowledge with improved explainability paradigms\nand exposes important shortcomings in present XAI systems.",
        "Let $P\\in \\mathbb Z[X]\\setminus\\{0\\}$ be of degree $\\delta\\ge 1$ and usual\nheight $H\\ge 1$, and let $\\alpha\\in \\overline{\\mathbb Q}^*$ be of degree $d\\ge\n2$. Mahler proved in 1931 the following transcendence measure for $e^\\alpha$:\nfor any $\\varepsilon\\&gt;0$, there exists $c\\&gt;0$ such that $\\vert\nP(e^\\alpha)\\vert\\&gt;c\/H^{\\mu(d,\\delta)+\\varepsilon}$ where the exponent\n$\\mu(d,\\delta)=(4d^2-2d)\\delta+2d-1$. Zheng obtained a better result in 1991\nwith $\\mu(d,\\delta)=(4d^2-2d)\\delta-1$. In this paper, we provide a new\nexplicit exponent $\\mu(d,\\delta)$ which improves on Zheng's transcendence\nmeasure for all $\\delta\\ge 2$ and all $d\\ge 2$. When $\\delta=1$, we recover his\nbound for all $d\\ge 2$, which had in fact already been obtained by Kappe in\n1966. Our improvement rests upon the optimization of an accessory parameter in\nSiegel's classical determinant method applied to Hermite-Pad{\\'e} approximants\nto powers of the exponential function.",
        "Functional It^o calculus is based on an extension of the classical It^o\ncalculus to functionals depending on the entire past evolution of the\nunderlying paths and not only on its current value. The calculus builds on\nFollmer's deterministic proof of the It^o formula, see [3], and a notion of\npathwise functional derivatives introduced by [5]. There are no smoothness\nassumptions required on the functionals, however, they are required to possess\ncertain directional derivatives which may be computed pathwise, see [6, 9, 8].\nUsing functional It^o calculus and the notion of quadratic variation, we derive\nthe functional It^o formula along with the Feynman-Kac formula for functional\nprocesses. Furthermore, we express the Greeks for path-dependent options as\nexpectations, which can be efficiently computed numerically using Monte Carlo\nsimulations. We illustrate these results by applying the formulae to digital\noptions within the Black-Scholes model framework.",
        "We investigate the possibility of using a Rabi drive to tune the interactions\nin an atomic Fermi gas. Specifically, we consider the scenario where two\nfermion species (spins) are Rabi coupled and interacting with a third uncoupled\nspecies. Using an exact calculation within a minimal low-energy model, we\nderive analytical expressions for the effective scattering length and effective\nrange that characterize the collisions between a Rabi-dressed atom and an atom\nfrom the third species. In particular, we find that new scattering resonances\nemerge in the Rabi-coupled system, which we demonstrate are linked to the\nexistence of hybrid two-body bound states. Furthermore, we show via a\ngeneralized Thouless criterion that the scattering properties have a direct\nimpact on the superfluid transitions in the Rabi-coupled Fermi gas. The\npresence of Rabi-induced resonances thus has implications for the investigation\nof many-body physics with driven atomic gases.",
        "The ability to integrate task-relevant information into neural\nrepresentations is a fundamental aspect of both biological and artificial\nintelligence. To enable theoretical analysis, recent work has examined whether\na network learns task-relevant features (rich learning) or resembles a random\nfeature model (or a kernel machine, i.e., lazy learning). However, this simple\nlazy-versus-rich dichotomy overlooks the possibility of various subtypes of\nfeature learning that emerge from different architectures, learning rules, and\ndata properties. Furthermore, most existing approaches emphasize weight\nmatrices or neural tangent kernels, limiting their applicability to\nneuroscience because they do not explicitly characterize representations.\n  In this work, we introduce an analysis framework based on representational\ngeometry to study feature learning. Instead of analyzing what are the learned\nfeatures, we focus on characterizing how task-relevant representational\nmanifolds evolve during the learning process. In both theory and experiment, we\nfind that when a network learns features useful for solving a task, the\ntask-relevant manifolds become increasingly untangled. Moreover, by tracking\nchanges in the underlying manifold geometry, we uncover distinct learning\nstages throughout training, as well as different learning strategies associated\nwith training hyperparameters, uncovering subtypes of feature learning beyond\nthe lazy-versus-rich dichotomy. Applying our method to neuroscience and machine\nlearning, we gain geometric insights into the structural inductive biases of\nneural circuits solving cognitive tasks and the mechanisms underlying\nout-of-distribution generalization in image classification. Our framework\nprovides a novel geometric perspective for understanding and quantifying\nfeature learning in both artificial and biological neural networks.",
        "In image-guided radiotherapy (IGRT), four-dimensional cone-beam computed\ntomography (4D-CBCT) is critical for assessing tumor motion during a patients\nbreathing cycle prior to beam delivery. However, generating 4D-CBCT images with\nsufficient quality requires significantly more projection images than a\nstandard 3D-CBCT scan, leading to extended scanning times and increased imaging\ndose to the patient. To address these limitations, there is a strong demand for\nmethods capable of reconstructing high-quality 4D-CBCT images from a 1-minute\n3D-CBCT acquisition. The challenge lies in the sparse sampling of projections,\nwhich introduces severe streaking artifacts and compromises image quality. This\npaper introduces a novel framework leveraging spatiotemporal Gaussian\nrepresentation for 4D-CBCT reconstruction from sparse projections, achieving a\nbalance between streak artifact reduction, dynamic motion preservation, and\nfine detail restoration. Each Gaussian is characterized by its 3D position,\ncovariance, rotation, and density. Two-dimensional X-ray projection images can\nbe rendered from the Gaussian point cloud representation via X-ray\nrasterization. The properties of each Gaussian were optimized by minimizing the\ndiscrepancy between the measured projections and the rendered X-ray\nprojections. A Gaussian deformation network is jointly optimized to deform\nthese Gaussian properties to obtain a 4D Gaussian representation for dynamic\nCBCT scene modeling. The final 4D-CBCT images are reconstructed by voxelizing\nthe 4D Gaussians, achieving a high-quality representation that preserves both\nmotion dynamics and spatial detail. The code and reconstruction results can be\nfound at https:\/\/github.com\/fuyabo\/4DGS_for_4DCBCT\/tree\/main",
        "For any graph $G=(V,E)$, a subset $S\\subseteq V$ is called {\\it an isolating\nset} of $G$ if $V\\setminus N_G[S]$ is an independent set of $G$, where\n$N_G[S]=S\\cup N_G(S)$, and {\\it the isolation number} of $G$, denoted by\n$\\iota(G)$, is the size of a smallest isolating set of $G$. In this article, we\nshow that the isolation number of the middle graph of $G$ is equal to the size\nof a smallest maximal matching of $G$.",
        "The four-roll mill has been traditionally viewed as a device generating\nsimple extensional flow with a central stagnation point. Our systematic\ninvestigation using a two-relaxation-time regularized lattice Boltzmann\n(TRT-RLB) model reveals unexpected richness in the flow physics, identifying\ntwo previously unreported supercritical bifurcation modes: a quadrifoliate\nvortex mode featuring four symmetrical counter-rotating vortices, and a\ndumbbell-shaped quad-vortex mode where vortices detach from but remain\nsymmetric about the stagnation point. The numerical framework, representing the\nfirst successful extension of TRT-RLB method to power-law fluid dynamics,\nenables comprehensive mapping of flow characteristics across Reynolds numbers\n($1 \\leq Re \\leq 50$), power-law indices ($0.7 \\leq n \\leq 1.3$), and geometric\nconfigurations. The transition from quadrifoliate vortex mode exhibits distinct\npathways depending on the power-law index: at relatively small $n$, the flow\nundergoes a direct supercritical bifurcation to simple extensional flow, while\nat relatively large $n$, it evolves through an intermediate dumbbell-shaped\nstate. Among geometric parameters, the roller radius $r$ emerges as the\ndominant factor controlling bifurcation points and vortex dimensions, whereas\nthe roller-container gap $\\delta$ exerts minimal influence on flow regimes. The\ntransitions between flow modes can be precisely characterized through the\nevolution of vortex dimensions and velocity gradients at the stagnation point,\nproviding quantitative criteria for flow regime identification. These findings\nenrich our fundamental understanding of bifurcation phenomena in extensional\ndevices and provide quantitative guidelines for achieving desired flow patterns\nin four-roll mill applications.",
        "This paper presents a proof of generic structural stability for Riemann\nsolutions to $2 \\times 2$ system of hyperbolic conservation laws in one spatial\nvariable, without diffusive terms. This means that for almost every left and\nright state, shocks and rarefaction solutions of the same type are preserved\nvia perturbations of the flux functions. The main assumptions for this proof\ninvolve standard assumptions on strict hyperbolicity and genuine non-linearity,\na technical assumption on directionality of rarefaction curve, and the regular\nmanifold (submersion) assumption motivated by concepts in differential\ntopology. We show that the structural stability of the Riemann solutions is\nrelated to the transversality of the Hugoniot loci and rarefaction curves in\nthe state space. The regular manifold assumption is required to invoke a\nvariant of a theorem from differential topology, Thom's parametric\ntransversality theorem, to illustrate the genericity of transversality of these\ncurves. This in turn implies the genericity of structural stability. We then\nillustrate the applications of this theorem to two examples: the p-system and a\n$2 \\times 2$ system governing the evolution of gravity-driven monodisperse\nparticle-laden thin films. In particular, we illustrate how one can verify all\nthe above assumptions for the former, and apply the theorem to different\nnumerical and physical aspects of the system governing the latter.",
        "The newly published data on spin density matrix elements from the GlueX\nCollaboration, along with the previously released differential cross-section\ndata from the CLAS Collaboration and the other two experiments for the $\\gamma\np \\to \\rho^0 p$ reaction, are systematically investigated using an effective\nLagrangian approach within the tree-level Born approximation. The model\ncombines contributions from $t$-channel meson exchanges ($\\pi$, $\\eta$, and\n$f_2$), $s$-channel nucleon ($N$) and nucleon resonance ($N^\\ast$) exchanges,\n$u$-channel $N$ exchange, and a generalized contact term to construct the\nscattering amplitudes. Regge propagators are employed for $t$-channel\namplitudes to incorporate the contributions from mesons with various spins\nlying on the same trajectories. The analysis shows that the background\ncontributions, dominated by the $f_2$-trajectory exchange, provide a\nsatisfactory description of the data in the high-energy and forward-angle\nregions. The inclusion of specific nucleon resonances, such as\n$N(2100)1\/2^{+}$, $N(2060)5\/2^{-}$, or $\\Delta(2000)5\/2^{+}$, significantly\nimproves the description of the differential cross-section data at\nnear-perpendicular scattering angles in the low-energy region. Predictions of\nphoton beam and target nucleon asymmetries are provided, offering valuable\ninsights to discriminate reaction mechanisms when corresponding data become\navailable in the future.",
        "Machine learning interatomic potentials (MLIPs) are a promising tool to\naccelerate atomistic simulations and molecular property prediction. The quality\nof MLIPs strongly depends on the quantity of available training data as well as\nthe quantum chemistry (QC) level of theory used to generate that data. Datasets\ngenerated with high-fidelity QC methods, such as coupled cluster, are typically\nrestricted to small molecules and may be missing energy gradients. With this\nlimited quantity of data, it is often difficult to train good MLIP models. We\npresent an ensemble knowledge distillation (EKD) method to improve MLIP\naccuracy when trained to energy-only datasets. In our EKD approach, first,\nmultiple teacher models are trained to QC energies and then used to generate\natomic forces for all configurations in the dataset. Next, a student MLIP is\ntrained to both QC energies and to ensemble-averaged forces generated by the\nteacher models. We apply this workflow on the ANI-1ccx dataset which consists\nof organic molecules with configuration energies computed at the coupled\ncluster level of theory. The resulting student MLIPs achieve new\nstate-of-the-art accuracy on the out-of-sample COMP6 benchmark and improved\nstability for molecular dynamics simulations. The EKD approach for MLIP is\nbroadly applicable for chemical, biomolecular and materials science\nsimulations.",
        "The Collatz conjecture, also known as the 3n+1 problem, is one of the most\npopular open problems in number theory. In this note, an algorithm for the\nverification of the Collatz conjecture is presented that works on a standard PC\nfor numbers with up to ten billion decimal places.",
        "In this paper we consider two-opinion voter models on dynamic random graphs,\nin which the joint dynamics of opinions and graphs acts as one-way feedback,\ni.e., edges appear and disappear over time depending on the opinions of the two\nconnected vertices, while the opinion dynamics does not depend on the edge\nprocess. Our goal is to investigate the joint evolution of the entries of a\nvoter subgraph count vector, i.e., vector of subgraphs where each vertex has a\nspecific opinion, in the regime that the number of vertices grows large. The\nmain result of this paper is a functional central limit theorem. In particular,\nwe prove that, under a proper centering and scaling, the joint functional of\nthe vector of subgraph counts converges to a specific multidimensional Gaussian\nprocess.",
        "Plasticity is inherent to many engineering materials such as metals. While it\ncan degrade the load-carrying capacity of structures via material yielding, it\ncan also protect structures through plastic energy dissipation. To fully\nharness plasticity, here we present the theory, method, and application of a\ntopology optimization framework that simultaneously optimizes structural\ngeometries and material phases to customize the stiffness, strength, and\nstructural toughness of designs experiencing finite strain elastoplasticity.\nThe framework accurately predicts structural responses by employing a rigorous,\nmechanics-based elastoplasticity theory that ensures isochoric plastic flow. It\nalso effectively identifies optimal material phase distributions using a\ngradient-based optimizer, where gradient information is obtained via a reversed\nadjoint method to address history dependence, along with automatic\ndifferentiation to compute the complex partial derivatives. We demonstrate the\nframework by optimizing a range of 2D and 3D elastoplastic structures,\nincluding energy-dissipating dampers, load-carrying beams, impact-resisting\nbumpers, and cold working profiled sheets. These optimized multimaterial\nstructures reveal important mechanisms for improving design performance under\nlarge deformation, such as the transition from kinematic to isotropic hardening\nwith increasing displacement amplitudes and the formation of twisted regions\nthat concentrate stress, enhancing plastic energy dissipation. Through the\nsuperior performance of these optimized designs, we demonstrate the framework's\neffectiveness in tailoring elastoplastic responses across various spatial\nconfigurations, material types, hardening behaviors, and combinations of\ncandidate materials. This work offers a systematic approach for optimizing\nnext-generation multimaterial structures with elastoplastic behaviors under\nlarge deformations.",
        "The macroscopic interactions of liquid iron and solid oxides, such as\nalumina, calcia, magnesia, silica, and zirconia manifest the behavior and\nefficiency of high-temperature metallurgical processes. The oxides serve dual\nroles, both as components of refractory materials in submerged entry nozzles\nand also as significant constituents of non-metallic inclusions in the melt. It\nis therefore crucial to understand the physicochemical interplay between the\nliquid and the oxides in order to address the nozzle clogging challenges, and\nthereby optimize cast iron and steel production. This paper presents a\nmethodology for describing these interactions by combining the materials'\ndielectric responses, computed within the density functional theory, with the\nCasimir-Lifshitz dispersion forces to generate the Hamaker constants. The\napproach provides a comprehensive understanding of the wettability of iron\nagainst these refractory oxides, revealing the complex relation between\nmolecular and macroscopic properties. Our theoretically determined crystalline\nstructures are confirmed by room-temperature X-ray diffraction, and the contact\nangles of liquid iron on the oxides are validated with a sessile drop system at\nthe temperature 1823 K. For comparison, we also present the wettability of the\noxides by a liquid tin-bismuth alloy. The findings are essential in advancing\nthe fundamental understanding of interfacial interactions in metallurgical\nscience, and are also pivotal in driving the development of more efficient and\nreliable steelmaking processes.",
        "For each adjointable idempotent $Q$ on a Hilbert $C^*$-module $H$, a specific\nprojection $m(Q)$ called the matched projection of $Q$ was introduced recently\ndue to the characterization of the minimum value among all the distances from\nprojections to $Q$. Inspired by the relationship between $m(Q)$ and $Q$,\nanother term called the quasi-projection pair $(P,Q)$ was also introduced\nrecently, where $P$ is a projection on $H$ satisfying $Q^*=(2P-I)Q(2P-I)$, in\nwhich $Q^*$ is the adjoint operator of the idempotent $Q$ and $I$ is the\nidentity operator on $H$. This paper aims to make systematical\ncharacterizations of the semi-harmonious and harmonious quasi-projection pairs\non Hilbert $C^*$-modules, and meanwhile to provide examples illustrating the\nnon-triviality of the associated characterizations.",
        "Certain spiral density waves in Saturn's rings are generated through\nresonances with planetary normal modes, making them valuable probes of Saturn's\ninternal structure. Previous research has primarily focused on the rotation\nrates of these waves. However, other characteristics of these waves also\ncontain valuable information about the planet's interior. In this work, we\ninvestigate the amplitudes of the waves across the C-ring by analyzing high\nsignal-to-noise profiles derived from phase-corrected averages of occultation\nprofiles obtained by Cassini's Visual and Infrared Mapping Spectrometer (VIMS).\nBy fitting these wave profiles to linear density wave models, we estimate the\nring's surface mass density, mass extinction coefficient and effective\nkinematic viscosity at 34 locations in the C-ring, as well as the amplitude of\nthe gravitational potential perturbations associated with 6 satellite\nresonances and 28 planetary normal mode resonances.\n  Our estimates of the C-ring's mass extinction coefficient, indicate that the\ntypical particle mass density is around 0.3 g\/cm^3 interior to 84,000 km, but\ncan get as low as 0.03 g\/cm^3 exterior to 84,000 km. We also find the ring's\nviscosity is reduced in the outer C-ring, which is consistent with the\nexceptionally high porosity of the particles in this region. Meanwhile, we find\nthe amplitudes of Saturn's normal modes are complex functions of frequency, l\nand m, implying that multiple factors influence how efficiently these modes are\nexcited. This analysis identified two primary sources of these normal-mode\noscillations: a deep source located close to Saturn's core, and a shallow\nsource residing near the surface.",
        "In this paper, we are concerned with the problem of locating the zeros of\npolynomials of a quaternionic variable with quaternionic coefficients. We\nderive some new Cauchy bounds for the zeros of a polynomial by virtue of\nmaximum modulus theorem. Our results will generalise some recently proved\nresults about the distribution of zeros of a quaternionic polynomial.",
        "We introduce a Reinforcement Learning (RL)-based method for re-synthesis of\nquantum circuits containing arbitrary Pauli rotations alongside Clifford\noperations. By collapsing each sub-block to a compact representation and then\nsynthesizing it step-by-step through a learned heuristic, we obtain circuits\nthat are both shorter and compliant with hardware connectivity constraints. We\nfind that the method is fast enough and good enough to work as an optimization\nprocedure: in direct comparisons on 6-qubit random Pauli Networks against\nstate-of-the-art heuristic methods, our RL approach yields over 2x reduction in\ntwo-qubit gate count, while executing in under 10 milliseconds per circuit. We\nfurther integrate the method into a collect-and-re-synthesize pipeline, applied\nas a Qiskit transpiler pass, where we observe average improvements of 20% in\ntwo-qubit gate count and depth, reaching up to 60% for many instances, across\nthe Benchpress benchmark. These results highlight the potential of RL-driven\nsynthesis to significantly improve circuit quality in realistic, large-scale\nquantum transpilation workloads."
      ]
    }
  },
  {
    "id":2412.16995,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Multi-objective performance optimization & thermodynamic analysis of solar powered supercritical co2 power cycles using machine learning methods & genetic algorithm",
    "start_abstract":"The present study is focused on multi-objective performance optimization & thermodynamic analysis from the perspectives of energy and exergy for Recompression, Partial Cooling & Main Compression Intercooling supercritical CO2 (sCO2) Brayton cycles for concentrated solar power (CSP) applications using machine learning algorithms. The novelty of this work lies in the integration of artificial neural networks (ANN) and genetic algorithms (GA) for optimizing the performance of advanced sCO2 power cycles considering climatic variation, which has significant implications for both the scientific community and engineering applications in the renewable energy sector. The methodology employed includes thermodynamic analysis based on energy, exergy & environmental factors including system performance optimization. The system is modelled for net power production of 15 MW thermal output utilizing equations for the energy and exergy balance for each component. Subsequently, thermodynamic model extracted dataset used for prediction & evaluation of Random Forest, XGBoost, KNN, AdaBoost, ANN and LightGBM algorithm. Finally, considering climate conditions, multi-objective optimization is carried out for the CSP integrated sCO2 Power cycle for optimal power output, exergy destruction, thermal and exergetic efficiency. Genetic algorithm and TOPSIS (technique for order of preference by similarity to ideal solution), multi-objective decision-making tool, were used to determine the optimum operating conditions. The major findings of this work reveal significant improvements in the performance of the advanced sCO2 cycle by 1.68 % and 7.87 % compared to conventional recompression and partial cooling cycle, respectively. This research could advance renewable energy technologies, particularly concentrated solar power, by improving power cycle designs to increase system efficiency and economic feasibility. Optimized advanced supercritical CO2 power cycles in concentrated solar power plants might increase renewable energy use and energy generation infrastructure, potentially opening new research avenues.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "A method for real-time optimal heliostat aiming strategy generation via deep learning"
      ],
      "abstract":[
        "Optimal aiming strategies are essential for efficient solar power tower technology operation. However, the high calculation complexity makes it difficult for existing optimization methods to solve the optimization problem in real-time directly. This work proposes a real-time optimal heliostat aiming strategy generation method via deep learning. First, a two-stage learning scheme where the neural network models are trained by genetic algorithm (GA) benchmark solutions to produce an optimal aiming strategy is presented. Then, an end-to-end model without needing GA solutions for training is developed and discussed. Furthermore, a robust end-to-end training method using randomly sampled flux maps is also proposed. The proposed models demonstrated comparable performance as GA with two orders of magnitude less computation time through case studies. Among the proposed models, the end-to-end model shows significantly better generalization ability than the pure data-driven two-stage model on the test set. A robust end-to-end model with data enhancement has better robustness on unseen flux maps."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Uncovering the Iceberg in the Sea: Fundamentals of Pulse Shaping and\n  Modulation Design for Random ISAC Signals",
        "Efficient Image Restoration via Latent Consistency Flow Matching",
        "Quantum Computer Controlled by Superconducting Digital Electronics at\n  Millikelvin Temperature",
        "ESPARGOS: An Ultra Low-Cost, Realtime-Capable Multi-Antenna WiFi Channel\n  Sounder",
        "Safety Verification of Nonlinear Stochastic Systems via Probabilistic\n  Tube",
        "Graph Neural Network Flavor Tagger and measurement of\n  $\\mathrm{sin}2\\beta$ at Belle II",
        "Remining Hard Negatives for Generative Pseudo Labeled Domain Adaptation",
        "Channel Resolvability Using Multiplicative Weight Update Algorithm",
        "DEFEND: A Large-scale 1M Dataset and Foundation Model for Tobacco\n  Addiction Prevention",
        "Which Code Statements Implement Privacy Behaviors in Android\n  Applications?",
        "Do Existing Testing Tools Really Uncover Gender Bias in Text-to-Image\n  Models?",
        "A Survey on the Optimization of Large Language Model-based Agents",
        "Algorithmic Clustering based on String Compression to Extract P300\n  Structure in EEG Signals",
        "Deep Reinforcement Learning based Triggering Function for Early\n  Classifiers of Time Series",
        "Decentralized Online Ensembles of Gaussian Processes for Multi-Agent\n  Systems",
        "On the convergence of split exponential integrators for semilinear\n  parabolic problems",
        "Voting Scheme to Strengthen Localization Security in Randomly Deployed\n  Wireless Sensor Networks",
        "Memristor Applications: Nanodevices Redefining Technological Landscapes",
        "Mean-Field Limits for Nearly Unstable Hawkes Processes",
        "GI-SLAM: Gaussian-Inertial SLAM",
        "Cold-Start Recommendation towards the Era of Large Language Models\n  (LLMs): A Comprehensive Survey and Roadmap",
        "Image-Space Gridding for Nonrigid Motion-Corrected MR Image\n  Reconstruction",
        "Refining local-type primordial non-Gaussianity: Sharpened $b_\\phi$\n  constraints through bias expansion",
        "TokenSim: Enabling Hardware and Software Exploration for Large Language\n  Model Inference Systems",
        "A Heliocentric-orbiting Objects Processing System (HOPS) for the Wide\n  Field Survey Telescope: Architecture, Processing Workflow, and Preliminary\n  Results",
        "Learning Fair Policies for Infectious Diseases Mitigation using Path\n  Integral Control",
        "Accelerating Linear Recurrent Neural Networks for the Edge with\n  Unstructured Sparsity",
        "Position: Open and Closed Large Language Models in Healthcare",
        "AdaPTS: Adapting Univariate Foundation Models to Probabilistic\n  Multivariate Time Series Forecasting"
      ],
      "abstract":[
        "Integrated Sensing and Communications (ISAC) is expected to play a pivotal\nrole in future 6G networks. To maximize time-frequency resource utilization, 6G\nISAC systems must exploit data payload signals, that are inherently random, for\nboth communication and sensing tasks. This paper provides a comprehensive\nanalysis of the sensing performance of such communication-centric ISAC signals,\nwith a focus on modulation and pulse shaping design to reshape the statistical\nproperties of their auto-correlation functions (ACFs), thereby improving the\ntarget ranging performance. We derive a closed-form expression for the\nexpectation of the squared ACF of random ISAC signals, considering arbitrary\nmodulation bases and constellation mappings within the Nyquist pulse shaping\nframework. The structure is metaphorically described as an ``iceberg hidden in\nthe sea\", where the ``iceberg'' represents the squared mean of the ACF of\nrandom ISAC signals, that is determined by the pulse shaping filter, and the\n``sea level'' characterizes the corresponding variance, caused by the\nrandomness of the data payload. Our analysis shows that, for QAM\/PSK\nconstellations with Nyquist pulse shaping, Orthogonal Frequency Division\nMultiplexing (OFDM) achieves the lowest ranging sidelobe level across all lags.\nBuilding on these insights, we propose a novel Nyquist pulse shaping design to\nenhance the sensing performance of random ISAC signals. Numerical results\nvalidate our theoretical findings, showing that the proposed pulse shaping\nsignificantly reduces ranging sidelobes compared to conventional root-raised\ncosine (RRC) pulse shaping, thereby improving the ranging performance.",
        "Recent advances in generative image restoration (IR) have demonstrated\nimpressive results. However, these methods are hindered by their substantial\nsize and computational demands, rendering them unsuitable for deployment on\nedge devices. This work introduces ELIR, an Efficient Latent Image Restoration\nmethod. ELIR operates in latent space by first predicting the latent\nrepresentation of the minimum mean square error (MMSE) estimator and then\ntransporting this estimate to high-quality images using a latent consistency\nflow-based model. Consequently, ELIR is more than 4x faster compared to the\nstate-of-the-art diffusion and flow-based approaches. Moreover, ELIR is also\nmore than 4x smaller, making it well-suited for deployment on\nresource-constrained edge devices. Comprehensive evaluations of various image\nrestoration tasks show that ELIR achieves competitive results, effectively\nbalancing distortion and perceptual quality metrics while offering improved\nefficiency in terms of memory and computation.",
        "Current superconducting quantum computing platforms face significant scaling\nchallenges, as individual signal lines are required for control of each qubit.\nThis wiring overhead is a result of the low level of integration between\ncontrol electronics at room temperature and qubits operating at millikelvin\ntemperatures, which raise serious doubts among technologists about whether\nutility-scale quantum computers can be built. A promising alternative is to\nutilize cryogenic, superconducting digital control electronics that coexist\nwith qubits. Here, we report the first multi-qubit system integrating this\ntechnology. The system utilizes digital demultiplexing, breaking the linear\nscaling of control lines to number of qubits. We also demonstrate single-qubit\nfidelities above 99%, and up to 99.9%. This work is a critical step forward in\nrealizing highly scalable chip-based quantum computers.",
        "Multi-antenna channel sounding is a technique for measuring the propagation\ncharacteristics of electromagnetic waves that is commonly employed for\nparameterizing channel models. Channel sounders are usually custom-built from\nmany Software Defined Radio receivers, making them expensive to procure and\ndifficult to operate, which constrains the set of users to a few specialized\nscientific institutions and industrial research laboratories. Recent\ndevelopments in Joint Communications and Sensing (JCaS) extend the possible\nuses of channel data to applications like human activity recognition, human\npresence detection, user localization and wireless Channel Charting, all of\nwhich are of great interest to security researchers, experts in industrial\nautomation and others. However, due to a lack of affordable, easy-to-use and\ncommercially available multi-antenna channel sounders, those scientific\ncommunities can be hindered by their lack of access to wireless channel\nmeasurements. To lower the barrier to entry for channel sounding, we develop an\nultra low-cost measurement hardware platform based on mass-produced WiFi chips,\nwhich is easily affordable to research groups and even hobbyists.",
        "We address the problem of safety verification for nonlinear stochastic\nsystems, specifically the task of certifying that system trajectories remain\nwithin a safe set with high probability. To tackle this challenge, we adopt a\nset-erosion strategy, which decouples the effects of stochastic disturbances\nfrom deterministic dynamics. This approach converts the stochastic safety\nverification problem on a safe set into a deterministic safety verification\nproblem on an eroded subset of the safe set. The success of this strategy\nhinges on the depth of erosion, which is determined by a probabilistic tube\nthat bounds the deviation of stochastic trajectories from their corresponding\ndeterministic trajectories. Our main contribution is the establishment of a\ntight bound for the probabilistic tube of nonlinear stochastic systems. To\nobtain a probabilistic bound for stochastic trajectories, we adopt a\nmartingale-based approach. The core innovation lies in the design of a novel\nenergy function associated with the averaged moment generating function, which\nforms an affine martingale, a generalization of the traditional c-martingale.\nUsing this energy function, we derive a precise bound for the probabilistic\ntube. Furthermore, we enhance this bound by incorporating the union-bound\ninequality for strictly contractive dynamics. By integrating the derived\nprobabilistic tubes into the set-erosion strategy, we demonstrate that the\nsafety verification problem for nonlinear stochastic systems can be reduced to\na deterministic safety verification problem. Our theoretical results are\nvalidated through applications in reachability-based safety verification and\nsafe controller synthesis, accompanied by several numerical examples that\nillustrate their effectiveness.",
        "We present GFlaT, a new algorithm that uses a graph-neural-network to\ndetermine the flavor of neutral B mesons produced in $\\mathrm{\\Upsilon(4S)}$\ndecays. We evaluate its performance using $B$ decays to flavor-specific\nhadronic final states reconstructed in a $362$ $\\mathrm{fb}^{-1}$ sample of\nelectron-positron collisions recorded at the $\\mathrm{\\Upsilon(4S)}$ resonance\nwith the Belle II detector at the SuperKEKB collider. We achieve an effective\ntagging efficiency of $(37.40 \\pm 0.43 \\pm 0.36) \\%$, where the first\nuncertainty is statistical and the second systematic, which is $18\\%$ better\nthan the previous Belle II algorithm. Demonstrating the algorithm, we use $B^0\n\\to J\/\\psi K_\\mathrm{S}^0$ decays to measure the direct and mixing-induced CP\nviolation parameters, $C = (-0.035 \\pm 0.026 \\pm 0.013)$ and $S = (0.724 \\pm\n0.035 \\pm 0.014)$, from which we obtain $\\beta = (23.2 \\pm 1.5 \\pm\n0.6)^{\\circ}$.",
        "Dense retrievers have demonstrated significant potential for neural\ninformation retrieval; however, they exhibit a lack of robustness to domain\nshifts, thereby limiting their efficacy in zero-shot settings across diverse\ndomains. A state-of-the-art domain adaptation technique is Generative Pseudo\nLabeling (GPL). GPL uses synthetic query generation and initially mined hard\nnegatives to distill knowledge from cross-encoder to dense retrievers in the\ntarget domain. In this paper, we analyze the documents retrieved by the\ndomain-adapted model and discover that these are more relevant to the target\nqueries than those of the non-domain-adapted model. We then propose refreshing\nthe hard-negative index during the knowledge distillation phase to mine better\nhard negatives. Our remining R-GPL approach boosts ranking performance in 13\/14\nBEIR datasets and 9\/12 LoTTe datasets. Our contributions are (i) analyzing hard\nnegatives returned by domain-adapted and non-domain-adapted models and (ii)\napplying the GPL training with and without hard-negative re-mining in LoTTE and\nBEIR datasets.",
        "We study the channel resolvability problem, which is used to prove strong\nconverse of identification via channel. Channel resolvability has been solved\nby only random coding in the literature. We prove channel resolvability using\nthe multiplicative weight update algorithm. This is the first approach to\nchannel resolvability using non-random coding.",
        "While tobacco advertising innovates at unprecedented speed, traditional\nsurveillance methods remain frozen in time, especially in the context of social\nmedia. The lack of large-scale, comprehensive datasets and sophisticated\nmonitoring systems has created a widening gap between industry advancement and\npublic health oversight. This paper addresses this critical challenge by\nintroducing Tobacco-1M, a comprehensive dataset of one million tobacco product\nimages with hierarchical labels spanning 75 product categories, and DEFEND, a\nnovel foundation model for tobacco product understanding. Our approach\nintegrates a Feature Enhancement Module for rich multimodal representation\nlearning, a Local-Global Visual Coherence mechanism for detailed feature\ndiscrimination, and an Enhanced Image-Text Alignment strategy for precise\nproduct characterization. Experimental results demonstrate DEFEND's superior\nperformance, achieving 83.1% accuracy in product classification and 73.8% in\nvisual question-answering tasks, outperforming existing methods by significant\nmargins. Moreover, the model exhibits robust zero-shot learning capabilities\nwith 45.6% accuracy on novel product categories. This work provides regulatory\nbodies and public health researchers with powerful tools for monitoring\nemerging tobacco products and marketing strategies, potentially revolutionizing\napproaches to tobacco control and public health surveillance.",
        "A \"privacy behavior\" in software is an action where the software uses\npersonal information for a service or a feature, such as a website using\nlocation to provide content relevant to a user. Programmers are required by\nregulations or application stores to provide privacy notices and labels\ndescribing these privacy behaviors. Although many tools and research prototypes\nhave been developed to help programmers generate these notices by analyzing the\nsource code, these approaches are often fairly coarse-grained (i.e., at the\nlevel of whole methods or files, rather than at the statement level). But this\nis not necessarily how privacy behaviors exist in code. Privacy behaviors are\nembedded in specific statements in code. Current literature does not examine\nwhat statements programmers see as most important, how consistent these views\nare, or how to detect them. In this paper, we conduct an empirical study to\nexamine which statements programmers view as most-related to privacy behaviors.\nWe find that expression statements that make function calls are most associated\nwith privacy behaviors, while the type of privacy label has little effect on\nthe attributes of the selected statements. We then propose an approach to\nautomatically detect these privacy-relevant statements by fine-tuning three\nlarge language models with the data from the study. We observe that the\nagreement between our approach and participants is comparable to or higher than\nan agreement between two participants. Our study and detection approach can\nhelp programmers understand which statements in code affect privacy in mobile\napplications.",
        "Text-to-Image (T2I) models have recently gained significant attention due to\ntheir ability to generate high-quality images and are consequently used in a\nwide range of applications. However, there are concerns about the gender bias\nof these models. Previous studies have shown that T2I models can perpetuate or\neven amplify gender stereotypes when provided with neutral text prompts.\nResearchers have proposed automated gender bias uncovering detectors for T2I\nmodels, but a crucial gap exists: no existing work comprehensively compares the\nvarious detectors and understands how the gender bias detected by them deviates\nfrom the actual situation. This study addresses this gap by validating previous\ngender bias detectors using a manually labeled dataset and comparing how the\nbias identified by various detectors deviates from the actual bias in T2I\nmodels, as verified by manual confirmation. We create a dataset consisting of\n6,000 images generated from three cutting-edge T2I models: Stable Diffusion XL,\nStable Diffusion 3, and Dreamlike Photoreal 2.0. During the human-labeling\nprocess, we find that all three T2I models generate a portion (12.48% on\naverage) of low-quality images (e.g., generate images with no face present),\nwhere human annotators cannot determine the gender of the person. Our analysis\nreveals that all three T2I models show a preference for generating male images,\nwith SDXL being the most biased. Additionally, images generated using prompts\ncontaining professional descriptions (e.g., lawyer or doctor) show the most\nbias. We evaluate seven gender bias detectors and find that none fully capture\nthe actual level of bias in T2I models, with some detectors overestimating bias\nby up to 26.95%. We further investigate the causes of inaccurate estimations,\nhighlighting the limitations of detectors in dealing with low-quality images.\nBased on our findings, we propose an enhanced detector...",
        "With the rapid development of Large Language Models (LLMs), LLM-based agents\nhave been widely adopted in various fields, becoming essential for autonomous\ndecision-making and interactive tasks. However, current work typically relies\non prompt design or fine-tuning strategies applied to vanilla LLMs, which often\nleads to limited effectiveness or suboptimal performance in complex\nagent-related environments. Although LLM optimization techniques can improve\nmodel performance across many general tasks, they lack specialized optimization\ntowards critical agent functionalities such as long-term planning, dynamic\nenvironmental interaction, and complex decision-making. Although numerous\nrecent studies have explored various strategies to optimize LLM-based agents\nfor complex agent tasks, a systematic review summarizing and comparing these\nmethods from a holistic perspective is still lacking. In this survey, we\nprovide a comprehensive review of LLM-based agent optimization approaches,\ncategorizing them into parameter-driven and parameter-free methods. We first\nfocus on parameter-driven optimization, covering fine-tuning-based\noptimization, reinforcement learning-based optimization, and hybrid strategies,\nanalyzing key aspects such as trajectory data construction, fine-tuning\ntechniques, reward function design, and optimization algorithms. Additionally,\nwe briefly discuss parameter-free strategies that optimize agent behavior\nthrough prompt engineering and external knowledge retrieval. Finally, we\nsummarize the datasets and benchmarks used for evaluation and tuning, review\nkey applications of LLM-based agents, and discuss major challenges and\npromising future directions. Our repository for related references is available\nat https:\/\/github.com\/YoungDubbyDu\/LLM-Agent-Optimization.",
        "P300 is an Event-Related Potential widely used in Brain-Computer Interfaces,\nbut its detection is challenging due to inter-subject and temporal variability.\nThis work introduces a clustering methodology based on Normalized Compression\nDistance (NCD) to extract the P300 structure, ensuring robustness against\nvariability. We propose a novel signal-to-ASCII transformation to generate\ncompression-friendly objects, which are then clustered using a hierarchical\ntree-based method and a multidimensional projection approach. Experimental\nresults on two datasets demonstrate the method's ability to reveal relevant\nP300 structures, showing clustering performance comparable to state-of-the-art\napproaches. Furthermore, analysis at the electrode level suggests that the\nmethod could assist in electrode selection for P300 detection. This\ncompression-driven clustering methodology offers a complementary tool for EEG\nanalysis and P300 identification.",
        "Early Classification of Time Series (ECTS) has been recognized as an\nimportant problem in many areas where decisions have to be taken as soon as\npossible, before the full data availability, while time pressure increases.\nNumerous ECTS approaches have been proposed, based on different triggering\nfunctions, each taking into account various pieces of information related to\nthe incoming time series and\/or the output of a classifier. Although their\nperformances have been empirically compared in the literature, no studies have\nbeen carried out on the optimality of these triggering functions that involve\n``man-tailored'' decision rules. Based on the same information, could there be\nbetter triggering functions? This paper presents one way to investigate this\nquestion by showing first how to translate ECTS problems into Reinforcement\nLearning (RL) ones, where the very same information is used in the state space.\nA thorough comparison of the performance obtained by ``handmade'' approaches\nand their ``RL-based'' counterparts has been carried out. A second question\ninvestigated in this paper is whether a different combination of information,\ndefining the state space in RL systems, can achieve even better performance.\nExperiments show that the system we describe, called \\textsc{Alert},\nsignificantly outperforms its state-of-the-art competitors on a large number of\ndatasets.",
        "Flexible and scalable decentralized learning solutions are fundamentally\nimportant in the application of multi-agent systems. While several recent\napproaches introduce (ensembles of) kernel machines in the distributed setting,\nBayesian solutions are much more limited. We introduce a fully decentralized,\nasymptotically exact solution to computing the random feature approximation of\nGaussian processes. We further address the choice of hyperparameters by\nintroducing an ensembling scheme for Bayesian multiple kernel learning based on\nonline Bayesian model averaging. The resulting algorithm is tested against\nBayesian and frequentist methods on simulated and real-world datasets.",
        "Splitting the exponential-like $\\varphi$ functions, which typically appear in\nexponential integrators, is attractive in many situations since it can\ndramatically reduce the computational cost of the procedure. However, depending\non the employed splitting, this can result in order reduction. The aim of this\npaper is to analyze different such split approximations. We perform the\nanalysis for semilinear problems in the abstract framework of commuting\nsemigroups and derive error bounds that depend, in particular, on whether the\nvector (to which the $\\varphi$ functions are applied) satisfies appropriate\nboundary conditions. We then present the convergence analysis for two split\nversions of a second-order exponential Runge--Kutta integrator in the context\nof analytic semigroups, and show that one suffers from order reduction while\nthe other does not. Numerical results for semidiscretized parabolic PDEs\nconfirm the theoretical findings.",
        "This work aspires to provide a trustworthy solution for target localization\nin adverse environments, where malicious nodes, capable of manipulating\ndistance measurements (i.e., performing spoofing attacks), are present, thus\nhindering accurate localization. Besides localization, its other goal is to\nidentify (detect) which of the nodes participating in the process are\nmalicious. This problem becomes extremely important with the forthcoming\nexpansion of IoT and smart cities applications, that depend on accurate\nlocalization, and the presence of malicious attackers can represent serious\nsecurity threats if not taken into consideration. This is the case with most\nexisting localization systems which makes them highly vulnerable to spoofing\nattacks. In addition, existing methods that are intended for adversarial\nsettings consider very specific settings or require additional knowledge about\nthe system model, making them only partially secure. Therefore, this work\nproposes a novel voting scheme based on clustering and weighted central mass to\nsecurely solve the localization problem and detect attackers. The proposed\nsolution has two main phases: 1) Choosing a cluster of suitable points of\ninterest by taking advantage of the problem geometry to assigning votes in\norder to localize the target, and 2) Attacker detection by exploiting the\nlocation estimate and basic statistics. The proposed method is assessed in\nterms of localization accuracy, success in attacker detection, and\ncomputational complexity in different settings. Computer simulations and\nreal-world experiments corroborate the effectiveness of the proposed scheme\ncompared to state-of-the-art methods, showing that it can accomplish an error\nreduction of $30~\\%$ and is capable of achieving almost perfect attacker\ndetection rate when the ratio between attacker intensity and noise standard\ndeviation is significant.",
        "A memristor, a two-terminal nanodevice, has garnered substantial attention in\nrecent years due to its distinctive properties and versatile applications.\nThese nanoscale components, characterized by their simplicity of manufacture,\nscalability in small dimensions, nonvolatile memory capabilities, and\nadaptability to low-power platforms, offer a wealth of opportunities for\ntechnological innovation. Memristors hold great promise in diverse fields,\nranging from advanced memory devices and neuromorphic computing to\nenergy-efficient circuits and more. As we delve into this report, our aim is to\nprovide a succinct but thorough exploration of the expanding landscape of\nmemristor applications. Through the meticulous examination of scholarly\nliterature, we systematically documented pivotal research milestones. By\npreserving historical consistency in our approach, we aim to unveil the\nintricate spectrum of possibilities that memristors offer, according to which\nthey can revolutionize and enhance various domains of electronics and\ncomputing.",
        "In this paper, we establish general scaling limits for nearly unstable Hawkes\nprocesses in a mean-field regime by extending the method introduced by Jaisson\nand Rosenbaum. Under a mild asymptotic criticality condition on the\nself-exciting kernels $\\{\\phi^n\\}$, specifically $\\|\\phi^n\\|_{L^1} \\to 1$, we\nfirst show that the scaling limits of these Hawkes processes are necessarily\nstochastic Volterra diffusions of affine type. Moreover, we establish a\npropagation of chaos result for Hawkes systems with mean-field interactions,\nhighlighting three distinct regimes for the limiting processes, which depend on\nthe asymptotics of $n(1-\\|\\phi^n\\|_{L^1})^2$. These results provide a\nsignificant generalization of the findings by Delattre, Fournier and Hoffmann.",
        "3D Gaussian Splatting (3DGS) has recently emerged as a powerful\nrepresentation of geometry and appearance for dense Simultaneous Localization\nand Mapping (SLAM). Through rapid, differentiable rasterization of 3D\nGaussians, many 3DGS SLAM methods achieve near real-time rendering and\naccelerated training. However, these methods largely overlook inertial data,\nwitch is a critical piece of information collected from the inertial\nmeasurement unit (IMU). In this paper, we present GI-SLAM, a novel\ngaussian-inertial SLAM system which consists of an IMU-enhanced camera tracking\nmodule and a realistic 3D Gaussian-based scene representation for mapping. Our\nmethod introduces an IMU loss that seamlessly integrates into the deep learning\nframework underpinning 3D Gaussian Splatting SLAM, effectively enhancing the\naccuracy, robustness and efficiency of camera tracking. Moreover, our SLAM\nsystem supports a wide range of sensor configurations, including monocular,\nstereo, and RGBD cameras, both with and without IMU integration. Our method\nachieves competitive performance compared with existing state-of-the-art\nreal-time methods on the EuRoC and TUM-RGBD datasets.",
        "Cold-start problem is one of the long-standing challenges in recommender\nsystems, focusing on accurately modeling new or interaction-limited users or\nitems to provide better recommendations. Due to the diversification of internet\nplatforms and the exponential growth of users and items, the importance of\ncold-start recommendation (CSR) is becoming increasingly evident. At the same\ntime, large language models (LLMs) have achieved tremendous success and possess\nstrong capabilities in modeling user and item information, providing new\npotential for cold-start recommendations. However, the research community on\nCSR still lacks a comprehensive review and reflection in this field. Based on\nthis, in this paper, we stand in the context of the era of large language\nmodels and provide a comprehensive review and discussion on the roadmap,\nrelated literature, and future directions of CSR. Specifically, we have\nconducted an exploration of the development path of how existing CSR utilizes\ninformation, from content features, graph relations, and domain information, to\nthe world knowledge possessed by large language models, aiming to provide new\ninsights for both the research and industrial communities on CSR. Related\nresources of cold-start recommendations are collected and continuously updated\nfor the community in\nhttps:\/\/github.com\/YuanchenBei\/Awesome-Cold-Start-Recommendation.",
        "Motion remains a major challenge in magnetic resonance (MR) imaging,\nparticularly in free-breathing cardiac MR imaging, where data are acquired over\nmultiple heartbeats at varying respiratory phases. We adopt a model-based\napproach for nonrigid motion correction, addressing two challenges: (a) motion\nrepresentation and (b) motion estimation. For motion representation, we derive\nimage-space gridding by adapting the nonuniform fast Fourier transform (NUFFT)\nto represent and compute nonrigid motion, which provides an exact\nforward-adjoint pair of linear operators. We then introduce nonrigid SENSE\noperators that incorporate nonrigid motion into the multi-coil MR acquisition\nmodel. For motion estimation, we employ both low-resolution 3D image-based\nnavigators (iNAVs) and high-resolution 3D self-navigating image-based\nnavigators (self-iNAVs). During each heartbeat, data are acquired along two\ntypes of non-Cartesian trajectories: a subset of a high-resolution trajectory\nthat sparsely covers 3D k-space, followed by a full low-resolution trajectory.\nWe reconstruct 3D iNAVs for each heartbeat using the full low-resolution data,\nwhich are then used to estimate bulk motion and identify the respiratory phase\nof each heartbeat. By combining data from multiple heartbeats within the same\nrespiratory phase, we reconstruct high-resolution 3D self-iNAVs, allowing\nestimation of nonrigid respiratory motion. For each respiratory phase, we\nconstruct the nonrigid SENSE operator, reformulating the nonrigid\nmotion-corrected reconstruction as a standard regularized inverse problem. In a\npreliminary study, the proposed method enhanced sharpness of the coronary\narteries and improved image quality in non-cardiac regions, outperforming\ntranslational motion-corrected reconstruction.",
        "Local-type primordial non-Gaussianity (PNG), predicted by many non-minimal\nmodels of inflation, creates a scale-dependent contribution to the power\nspectrum of large-scale structure (LSS) tracers. Its amplitude is characterized\nby the product $b_\\phi f_{\\rm NL}^{\\rm loc}$, where $b_\\phi$ is an\nastrophysical parameter dependent on the properties of the tracer. However,\n$b_\\phi$ exhibits significant secondary dependence on halo concentration and\nother astrophysical properties, which may bias and weaken the constraints on\n$f_{\\rm NL}^{\\rm loc}$. In this work, we demonstrate that incorporating\nknowledge of the relation between Lagrangian bias parameters and $b_\\phi$ can\nsignificantly enhance PNG constraints. We employ the Hybrid Effective Field\nTheory (HEFT) approach at the field-level and a linear regression model to seek\na connection between the bias parameters and $b_{\\phi}$ for halo and galaxy\nsamples, constructed using the \\textsc{AbacusSummit} simulation suite and\nmimicking the luminous red galaxies (LRGs) and quasi-stellar objects (QSOs) of\nthe Dark Energy Spectroscopic Instrument (DESI) survey. For the fixed-mass halo\nsamples, our full bias model reduces the uncertainty by more than 70\\%, with\nmost of that improvement coming from $b_\\nabla$, which we find to be an\nexcellent proxy for concentration. For the galaxy samples, our model reduces\nthe uncertainty on $b_\\phi$ by 80\\% for all tracers. By adopting\nLagrangian-bias informed priors on the parameter $b_\\phi$, future analyses can\nthus constrain $f_{\\rm NL}^{\\rm loc}$ with less bias and smaller errors.",
        "The increasing demand for large language model (LLM) serving has necessitated\nsignificant advancements in the optimization and profiling of LLM inference\nsystems. As these models become integral to a wide range of applications, the\nneed for efficient and scalable serving solutions has grown exponentially. This\nwork introduces TokenSim, a comprehensive hardware and software exploration\nsystem designed specifically for LLM inference. TokenSim is characterized by\nits support for extensible system optimizations including scheduling and memory\nmanagement. We validate the results with systems running with realworld\ndatasets, achieving an error rate of less than 1%. Furthermore, TokenSim\nfacilitates various insightful explorations into the performance and\noptimization of LLM serving systems.",
        "Wide-field surveys have markedly enhanced the discovery and study of solar\nsystem objects (SSOs). The 2.5-meter Wide Field Survey Telescope (WFST)\nrepresents the foremost facility dedicated to optical time-domain surveys in\nthe northern hemisphere. To fully exploit WFST's capabilities for SSO\ndetection, we have developed a heliocentric-orbiting objects processing system\n(HOPS) tailored for identifying these objects. This system integrates\nHelioLinC3D, an algorithm well suited for the WFST survey cadence,\ncharacterized by revisiting the same sky field twice on the majority of nights.\nIn this paper, we outline the architecture and processing flow of our SSO\nprocessing system. The application of the system to the WFST pilot survey data\ncollected between March and May 2024 demonstrates exceptional performance in\nterms of both temporal efficiency and completeness. A total of 658,489\nobservations encompassing 38,520 known asteroids have been documented, and 241\nnewly discovered asteroids have been assigned provisional designations. In\nparticular, 27% of these new discoveries were achieved using merely two\nobservations per night on three nights. The preliminary results not only\nilluminate the effectiveness of integrating HelioLinC3D within the SSO\nprocessing system, but also emphasize the considerable potential contributions\nof WFST to the field of solar system science.",
        "Infectious diseases pose major public health challenges to society,\nhighlighting the importance of designing effective policies to reduce economic\nloss and mortality. In this paper, we propose a framework for sequential\ndecision-making under uncertainty to design fairness-aware disease mitigation\npolicies that incorporate various measures of unfairness. Specifically, our\napproach learns equitable vaccination and lockdown strategies based on a\nstochastic multi-group SIR model. To address the challenges of solving the\nresulting sequential decision-making problem, we adopt the path integral\ncontrol algorithm as an efficient solution scheme. Through a case study, we\ndemonstrate that our approach effectively improves fairness compared to\nconventional methods and provides valuable insights for policymakers.",
        "Linear recurrent neural networks enable powerful long-range sequence modeling\nwith constant memory usage and time-per-token during inference. These\narchitectures hold promise for streaming applications at the edge, but\ndeployment in resource-constrained environments requires hardware-aware\noptimizations to minimize latency and energy consumption. Unstructured sparsity\noffers a compelling solution, enabling substantial reductions in compute and\nmemory requirements--when accelerated by compatible hardware platforms. In this\npaper, we conduct a scaling study to investigate the Pareto front of\nperformance and efficiency across inference compute budgets. We find that\nhighly sparse linear RNNs consistently achieve better efficiency-performance\ntrade-offs than dense baselines, with 2x less compute and 36% less memory at\niso-accuracy. Our models achieve state-of-the-art results on a real-time\nstreaming task for audio denoising. By quantizing our sparse models to\nfixed-point arithmetic and deploying them on the Intel Loihi 2 neuromorphic\nchip for real-time processing, we translate model compression into tangible\ngains of 42x lower latency and 149x lower energy consumption compared to a\ndense model on an edge GPU. Our findings showcase the transformative potential\nof unstructured sparsity, paving the way for highly efficient recurrent neural\nnetworks in real-world, resource-constrained environments.",
        "This position paper analyzes the evolving roles of open-source and\nclosed-source large language models (LLMs) in healthcare, emphasizing their\ndistinct contributions and the scientific community's response to their\ndevelopment. Due to their advanced reasoning capabilities, closed LLMs, such as\nGPT-4, have dominated high-performance applications, particularly in medical\nimaging and multimodal diagnostics. Conversely, open LLMs, like Meta's LLaMA,\nhave gained popularity for their adaptability and cost-effectiveness, enabling\nresearchers to fine-tune models for specific domains, such as mental health and\npatient communication.",
        "Pre-trained foundation models (FMs) have shown exceptional performance in\nunivariate time series forecasting tasks. However, several practical challenges\npersist, including managing intricate dependencies among features and\nquantifying uncertainty in predictions. This study aims to tackle these\ncritical limitations by introducing adapters; feature-space transformations\nthat facilitate the effective use of pre-trained univariate time series FMs for\nmultivariate tasks. Adapters operate by projecting multivariate inputs into a\nsuitable latent space and applying the FM independently to each dimension.\nInspired by the literature on representation learning and partially stochastic\nBayesian neural networks, we present a range of adapters and\noptimization\/inference strategies. Experiments conducted on both synthetic and\nreal-world datasets confirm the efficacy of adapters, demonstrating substantial\nenhancements in forecasting accuracy and uncertainty quantification compared to\nbaseline methods. Our framework, AdaPTS, positions adapters as a modular,\nscalable, and effective solution for leveraging time series FMs in multivariate\ncontexts, thereby promoting their wider adoption in real-world applications. We\nrelease the code at https:\/\/github.com\/abenechehab\/AdaPTS."
      ]
    }
  },
  {
    "id":2412.16995,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"A method for real-time optimal heliostat aiming strategy generation via deep learning",
    "start_abstract":"Optimal aiming strategies are essential for efficient solar power tower technology operation. However, the high calculation complexity makes it difficult for existing optimization methods to solve the optimization problem in real-time directly. This work proposes a real-time optimal heliostat aiming strategy generation method via deep learning. First, a two-stage learning scheme where the neural network models are trained by genetic algorithm (GA) benchmark solutions to produce an optimal aiming strategy is presented. Then, an end-to-end model without needing GA solutions for training is developed and discussed. Furthermore, a robust end-to-end training method using randomly sampled flux maps is also proposed. The proposed models demonstrated comparable performance as GA with two orders of magnitude less computation time through case studies. Among the proposed models, the end-to-end model shows significantly better generalization ability than the pure data-driven two-stage model on the test set. A robust end-to-end model with data enhancement has better robustness on unseen flux maps.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Multi-objective performance optimization & thermodynamic analysis of solar powered supercritical co2 power cycles using machine learning methods & genetic algorithm"
      ],
      "abstract":[
        "The present study is focused on multi-objective performance optimization & thermodynamic analysis from the perspectives of energy and exergy for Recompression, Partial Cooling & Main Compression Intercooling supercritical CO2 (sCO2) Brayton cycles for concentrated solar power (CSP) applications using machine learning algorithms. The novelty of this work lies in the integration of artificial neural networks (ANN) and genetic algorithms (GA) for optimizing the performance of advanced sCO2 power cycles considering climatic variation, which has significant implications for both the scientific community and engineering applications in the renewable energy sector. The methodology employed includes thermodynamic analysis based on energy, exergy & environmental factors including system performance optimization. The system is modelled for net power production of 15 MW thermal output utilizing equations for the energy and exergy balance for each component. Subsequently, thermodynamic model extracted dataset used for prediction & evaluation of Random Forest, XGBoost, KNN, AdaBoost, ANN and LightGBM algorithm. Finally, considering climate conditions, multi-objective optimization is carried out for the CSP integrated sCO2 Power cycle for optimal power output, exergy destruction, thermal and exergetic efficiency. Genetic algorithm and TOPSIS (technique for order of preference by similarity to ideal solution), multi-objective decision-making tool, were used to determine the optimum operating conditions. The major findings of this work reveal significant improvements in the performance of the advanced sCO2 cycle by 1.68 % and 7.87 % compared to conventional recompression and partial cooling cycle, respectively. This research could advance renewable energy technologies, particularly concentrated solar power, by improving power cycle designs to increase system efficiency and economic feasibility. Optimized advanced supercritical CO2 power cycles in concentrated solar power plants might increase renewable energy use and energy generation infrastructure, potentially opening new research avenues."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "A hybrid pressure formulation of the face-centred finite volume method\n  for viscous laminar incompressible flows",
        "Chemically-Accurate Prediction of the Ionisation Potential of Helium\n  Using a Quantum Processor",
        "Testing the QCD formation time with reconstructed parton splittings",
        "Cauchy Random Features for Operator Learning in Sobolev Space",
        "Grid-based exoplanet atmospheric mass loss predictions through neural\n  network",
        "Hyper-neutron stars from an ab initio calculation",
        "The Method of ${\\cal M}_{n}$-Extension: The KdV Equation",
        "Stronger Constraints on Primordial Black Holes as Dark Matter Derived\n  from the Thermal Evolution of the Intergalactic Medium over the Last Twelve\n  Billion Years",
        "Impulsive mixing of stellar populations in dwarf spheroidal galaxies",
        "A Flux-Tunable cavity for Dark matter detection",
        "Apparent teleportation of indistinguishable particles",
        "Long Lived Quasinormal Modes of Regular and Extreme Black Holes",
        "Positive Feedback: How a Synergy Between the Streaming Instability and\n  Dust Coagulation Forms Planetesimals",
        "Signs of Non-Monotonic Finite-Volume Corrections to $g_A$",
        "Metamaterials that learn to change shape",
        "Nonequilibrium Green's Function Formalism Applicable to Discrete\n  Impurities in Semiconductor Nanostructures",
        "Dark matter spiral arms in Milky Way-like halos",
        "Man-in-the-Middle Attacks Targeting Quantum Cryptography",
        "Ground States for the NLS on graphs with an attractive potential",
        "Non-local functionals, total variation, and Gamma-convergence with\n  respect to area-strict convergence",
        "Representation Theorems for Convex Expectations and Semigroups on Path\n  Space",
        "Totally bounded ultrametric spaces and locally finite trees",
        "Forecasting the Volatility of Energy Transition Metals",
        "Searching for continuous gravitational waves from highly deformed\n  compact objects with DECIGO",
        "Settling the no-$(k+1)$-in-line problem when $k$ is not small",
        "A Quantum Good Authentication Protocol",
        "Future collider sensitivities to $\\nu$SMEFT interactions",
        "On a theorem of Harder"
      ],
      "abstract":[
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "This work presents a hybrid pressure face-centred finite volume (FCFV) solver\nto simulate steady-state incompressible Navier-Stokes flows. The method\nleverages the robustness, in the incompressible limit, of the hybridisable\ndiscontinuous Galerkin paradigm for compressible and weakly compressible flows\nto derive the formulation of a novel, low-order face-based discretisation. The\nincompressibility constraint is enforced in a weak sense, by introducing an\ninter-cell mass flux defined in terms of a new, hybrid variable, representing\nthe pressure at the cell faces. This results in a new hybridisation strategy\nwhere cell variables (velocity, pressure and deviatoric strain rate tensor) are\nexpressed as a function of velocity and pressure at the barycentre of the cell\nfaces. The hybrid pressure formulation provides first-order convergence of all\nvariables, including the stress, independently of cell type, stretching and\ndistortion. Numerical benchmarks of Navier-Stokes flows at low and moderate\nReynolds numbers, in two and three dimensions, are presented to evaluate\naccuracy and robustness of the method. In particular, the hybrid pressure\nformulation outperforms the FCFV method when convective effects are relevant,\nachieving accurate predictions on significantly coarser meshes.",
        "Quantum computers have the potential to revolutionise our understanding of\nthe microscopic behaviour of materials and chemical processes by enabling\nhigh-accuracy electronic structure calculations to scale more efficiently than\nis possible using classical computers. Current quantum computing hardware\ndevices suffer from the dual challenges of noise and cost, which raises the\nquestion of what practical value these devices might offer before full fault\ntolerance is achieved and economies of scale enable cheaper access. Here we\nexamine the practical value of noisy quantum computers as tools for\nhigh-accuracy electronic structure, by using a Quantinuum ion-trap quantum\ncomputer to predict the ionisation potential of helium. By combining a series\nof techniques suited for use with current hardware including qubit-efficient\nencoding coupled with chemical insight, low-cost variational optimisation with\nhardware-adapted quantum circuits, and moments-based corrections, we obtain an\nionisation potential of 24.5536 (+0.0011, -0.0005) eV, which agrees with the\nexperimentally measured value to within true chemical accuracy, and with high\nstatistical confidence. The methods employed here can be generalised to predict\nother properties and expand our understanding of the value that might be\nprovided by near-term quantum computers.",
        "In high-energy elementary collisions the space-time ordering of parton\nbranching processes is not accessible experimentally. In contrast, in heavy-ion\ncollisions, parton showers interact with a spatially extended dense medium.\nThis sets a reference length scale with respect to which the space-time\nordering may be analysed. Here, we explore the possibility of identifying\nexperimental signatures of the QCD formation time, $\\tau_f$, on the level of a\nsingle parton splitting. Since heavy flavour offers an additional handle on\ntracing the propagation of individual quarks through the medium, we focus on\nthe $g\\to c\\bar{c}$ splitting. Combining adapted versions of the\nCambridge-Aachen and FlavourCone jet finding algorithms with grooming\ntechniques, we show how the kinematics of such splittings can be reconstructed\nwith high fidelity using either final state partons or hadrons, and how the\nformation time distribution of parton splittings can be constructed therefrom.\nMedium modification leads to a characteristic modification of this $\\tau_f$\ndistribution. This effect can be used to construct experimentally-accessible\nratios of $\\tau_f$ distributions, in which the sensitivity of the medium\nmodification to the QCD formation time becomes measurable.",
        "Operator learning is the approximation of operators between infinite\ndimensional Banach spaces using machine learning approaches. While most\nprogress in this area has been driven by variants of deep neural networks such\nas the Deep Operator Network and Fourier Neural Operator, the theoretical\nguarantees are often in the form of a universal approximation property.\nHowever, the existence theorems do not guarantee that an accurate operator\nnetwork is obtainable in practice. Motivated by the recent kernel-based\noperator learning framework, we propose a random feature operator learning\nmethod with theoretical guarantees and error bounds. The random feature method\ncan be viewed as a randomized approximation of a kernel method, which\nsignificantly reduces the computation requirements for training. We provide a\ngeneralization error analysis for our proposed random feature operator learning\nmethod along with comprehensive numerical results. Compared to kernel-based\nmethod and neural network methods, the proposed method can obtain similar or\nbetter test errors across benchmarks examples with significantly reduced\ntraining times. An additional advantages it that our implementation is simple\nand does require costly computational resources, such as GPU.",
        "The fast and accurate estimation of planetary mass-loss rates is critical for\nplanet population and evolution modelling. We use machine learning (ML) for\nfast interpolation across an existing large grid of hydrodynamic upper\natmosphere models, providing mass-loss rates for any planet inside the grid\nboundaries with superior accuracy compared to previously published\ninterpolation schemes. We consider an already available grid comprising about\n11000 hydrodynamic upper atmosphere models for training and generate an\nadditional grid of about 250 models for testing purposes. We develop the ML\ninterpolation scheme (dubbed \"atmospheric Mass Loss INquiry frameworK\"; MLink)\nusing a Dense Neural Network, further comparing the results with what was\nobtained employing classical approaches (e.g. linear interpolation and radial\nbasis function-based regression). Finally, we study the impact of the different\ninterpolation schemes on the evolution of a small sample of carefully selected\nsynthetic planets. MLink provides high-quality interpolation across the entire\nparameter space by significantly reducing both the number of points with large\ninterpolation errors and the maximum interpolation error compared to previously\navailable schemes. For most cases, evolutionary tracks computed employing MLink\nand classical schemes lead to comparable planetary parameters at\nGyr-timescales. However, particularly for planets close to the top edge of the\nradius gap, the difference between the predicted planetary radii at a given age\nof tracks obtained employing MLink and classical interpolation schemes can\nexceed the typical observational uncertainties. Machine learning can be\nsuccessfully used to estimate atmospheric mass-loss rates from model grids\npaving the way to explore future larger and more complex grids of models\ncomputed accounting for more physical processes.",
        "The equation of state (EoS) of neutron matter plays a decisive role to\nunderstand the neutron star properties and the gravitational waves from neutron\nstar mergers. At sufficient densities, the appearance of hyperons generally\nsoftens the EoS, leading to a reduction in the maximum mass of neutron stars\nwell below the observed values of about 2 solar masses. Even though repulsive\nthree-body forces are known to solve this so-called ``hyperon puzzle'', so far\nperforming \\textit{ab initio} calculations with a substantial number of\nhyperons for neutron star properties has remained elusive. Starting from the\nnewly developed auxiliary field quantum Monte Carlo algorithm to simulate\nhyper-neutron matter (HNM) without any sign oscillations, we derive three\ndistinct EoSs by employing the state-of-the-art Nuclear Lattice Effective Field\nTheory. We include $N\\Lambda$, $\\Lambda\\Lambda$ two-body forces, $NN\\Lambda$,\nand $N\\Lambda\\Lambda$ three-body forces. Consequently, we determine essential\nastrophysical quantities such as the neutron star mass, radius, tidal\ndeformability, and the universal $I$-Love-$Q$ relation. The maximum mass,\nradius and tidal deformability of a $1.4M_\\odot$ neutron star are predicted to\nbe $2.17(1)(1)~M_\\odot$, $R_{1.4M\\odot}=13.10(1)(7)~$km, and\n$\\Lambda_{1.4M_\\odot}=597(5)(18)$, respectively, based on our most realistic\nEoS. These predictions are in good agreement with the latest astrophysical\nconstraints derived from observations of massive neutron stars, gravitational\nwaves, and joint mass-radius measurements. Also, for the first time in\n\\textit{ab initio} calculations, we investigate both non-rotating and rotating\nneutron star configurations. The results indicate that the impact of rotational\ndynamics on the maximum mass is small, regardless of whether hyperons are\npresent in the EoS or not.",
        "In this work we generalize ${\\cal M}_{2}$-extension that has been introduced\nrecently. For illustration we use the KdV equation. We present five different\n${\\cal M}_{3}$-extensions of the KdV equation and their recursion operators. We\ngive a compact form of ${\\cal M}_{n}$-extension of the KdV equation and\nrecursion operator of the coupled KdV system. The method of ${\\cal\nM}_{n}$-extension can be applied to any integrable scalar equation to obtain\nintegrable multi-field system of equations. We also present unshifted and\nshifted nonlocal reductions of an example of ${\\cal M}_{3}$-extension of KdV.",
        "Primordial black holes (PBHs) have been explored as potential dark matter\ncandidates, with various astrophysical observations placing upper limits on the\nfraction $f_\\mathrm{PBH}$ of dark matter in the form of PBHs. However, a\nlargely underutilized probe of PBH abundance is the temperature of the\nintergalactic medium (IGM), inferred from the thermal broadening of absorption\nlines in the Lyman-$\\alpha$ forest of quasar spectra. PBHs inject energy into\nthe IGM via Hawking radiation, altering its thermal evolution. In this work, we\nconstrain this energy injection by self-consistently modeling its interplay\nwith the cosmological ultraviolet background from galaxies and supermassive\nblack holes. Leveraging IGM temperature measurements spanning the past twelve\nbillion years ($z \\sim 0$ to $6$), we derive one of the most stringent\nconstraints on PBH-induced heating from light PBHs within the mass range\n$10^{15}\\unicode{x2013}10^{17}$ g. Specifically, for $M_\\mathrm{PBH} = 10^{16}$\ng, we find $f_\\mathrm{PBH} < 5 \\times 10^{-5}$ at 95% confidence, with the\nbound scaling approximately as $M_\\mathrm{PBH}^{4}$ at other masses. Our\ninclusion of helium reionization and low-redshift temperature measurements\nstrengthens previous IGM-based PBH constraints by an order of magnitude or\nmore. Compared to other existing limits, our result is among the strongest,\nsecond only to the constraints from the 511 keV line from the Galactic Centre,\nbut with distinct systematics. More broadly, this study highlights the IGM\nthermal history as a powerful and independent probe of beyond-standard-model\nphysics.",
        "We study the response of mono-energetic stellar populations with initially\nisotropic kinematics to impulsive and adiabatic changes to an underlying dark\nmatter potential. Half-light radii expand and velocity dispersions decrease as\nenclosed dark matter is removed. The details of this expansion and cooling\ndepend on the time scale on which the underlying potential changes. In the\nadiabatic regime, the product of half-light radius and average velocity\ndispersion is conserved. We show that the stellar populations maintain\ncentrally isotropic kinematics throughout their adiabatic evolution, and their\ndensities can be approximated by a family of analytical radial profiles.\nMetallicity gradients within the galaxy flatten as dark matter is slowly\nremoved. In the case of strong impulsive perturbations, stellar populations\ndevelop power-law-like density tails with radially biased kinematics. We show\nthat the distribution of stellar binding energies within the dark matter halo\nsubstantially widens after an impulsive perturbation, no matter the sign of the\nperturbation. This allows initially energetically separated stellar populations\nto mix, to the extent that previously chemo-dynamically distinct populations\nmay masquerade as a single population with large metallicity and energy spread.\nFinally, we show that in response to an impulsive perturbation, stellar\npopulations that are deeply embedded in cored dark matter halos undergo a\nseries of damped oscillations before reaching a virialised equilibrium state,\ndriven by inefficient phase mixing in the harmonic potentials of cored halos.\nThis slow return to equilibrium adds substantial systematic uncertainty to\ndynamical masses estimated from Jeans modeling or the virial theorem.",
        "Developing a dark matter detector with wide mass tunability is an immensely\ndesirable property, yet it is challenging due to maintaining strong\nsensitivity. Resonant cavities for dark matter detection have traditionally\nemployed mechanical tuning, moving parts around to change electromagnetic\nboundary conditions. However, these cavities have proven challenging to operate\nin sub-Kelvin cryogenic environments due to differential thermal contraction,\nlow heat capacities, and low thermal conductivities. Instead, we develop an\nelectronically tunable cavity architecture by coupling a superconducting 3D\nmicrowave cavity with a DC flux tunable SQUID. With a flux delivery system\nengineered to maintain high coherence in the cavity, we perform a hidden-photon\ndark matter search below the quantum-limited threshold. A microwave photon\ncounting technique is employed through repeated quantum non-demolition\nmeasurements using a transmon qubit. With this device, we perform a\nhidden-photon search with a dark count rate of around 64 counts\/s and constrain\nthe kinetic mixing angle to ${\\varepsilon}< 4\\times 10^{-13}$ in a tunable band\nfrom 5.672 GHz to 5.694 GHz. By coupling multimode tunable cavities to the\ntransmon, wider hidden-photon searching ranges are possible.",
        "Teleportation, introduced in science fiction literature, is an instantaneous\nchange of the position of a microscopic object. Two teleportation-like\nphenomena were predicted by quantum mechanics: quantum teleportation and,\nrecently, quantum particle teleportation. The former is investigated\nexperimentally and has applications in quantum communication and computing.\n  Here, we introduced the third teleportation-like phenomenon - an apparent\nteleportation. It seems to be a natural consequence of elementary particles and\nantiparticles of the Standard Model being indistinguishable. We give an example\nof a process leading to the apparent teleportation within a toy model of\nboson-like particles. It utilizes the local transport of particles and\nantiparticles and the local creation and annihilation of particle-antiparticle\npairs. Furthermore, we suggest a method to observe the apparent teleportation\nin nucleus-nucleus collisions at properly selected collision energy. The method\nrequires the measurement of correlations between momenta of charm and anticharm\nhadrons in collisions with a single $c\\bar{c}$ pair being produced. The\nultimate prediction following the apparent teleportation hypothesis is the\nuncorrelated emission of charm and anticharm hadrons. It can be tested by\ncontemporary experiments.\n  Observing the apparent teleportation would uncover the basic transport\nproperties of indistinguishable particles. In particular, the apparent\nteleportation may explain the rapid thermalisation of the system created in\ncollisions of two atomic nuclei. Theoretical and experimental efforts are\nneeded to observe the apparent teleportation processes and study their\nproperties.",
        "Recently, black hole models in a nonlinear modification of the Maxwell\nelectrodynamics were suggested, possessing simultaneously properties of an\nextreme charge and regularity (Bronnikov K. A., Phys. Rev. D, 110 (2024)\n024021). We study quasinormal modes of a massive scalar field around such black\nholes and show that they are characterized by a comparatively small damping\nrate, indicating the possible existence of arbitrarily long-lived quasinormal\nmodes, called quasi-resonances.",
        "One of the most important open questions in planet formation is how dust\ngrains in a protoplanetary disk manage to overcome growth barriers and form the\n$\\sim$100km planet building blocks that we call planetesimals. There appears to\nbe a gap between the largest grains that can be produce by coagulation, and the\nsmallest grains that are needed for the streaming instability (SI) to form\nplanetesimals. Here we explore a novel hypothesis: That dust coagulation and\nthe SI work in tandem. That they form a feedback loop where each one boosts the\naction of the other to bridge the gap between dust grains and planetesimals. We\ndevelop a semi-analytical model of dust concentration due to the SI, and an\nanalytic model of how the SI affects the fragmentation and radial drift\nbarriers. We then combine those to model our proposed feedback loop. In the\nfragmentation-limited regime, we find a powerful synergy between the SI and\ndust growth that drastically increases both grain sizes and densities. We find\nthat a midplane dust-to-gas ratio of $\\epsilon \\ge 0.3$ is a sufficient\ncondition for the feedback loop to reach the planetesimal-forming region for\nturbulence values $10^{-4} \\le \\alpha \\le 10^{-3}$ and grain sizes $0.01 \\le\n{\\rm St} \\le 0.1$. In contrast, the drift-limited regime only shows grain\ngrowth, without significant dust accumulation. Planet formation in the\ndrift-limited portion of the disk may require other processes (particle traps)\nto halt radial drift.",
        "We study finite-volume (FV) corrections to determinations of $g_A$ via\nlattice quantum chromodynamics (QCD) using analytic results and numerical\nanalysis. We observe that $SU(2)$ Heavy Baryon Chiral Perturbation Theory does\nnot provide an unambiguous prediction for the sign of the FV correction, which\nis not surprising when one also considers large-$N_c$ constraints on the axial\ncouplings. We further show that non-monotonic FV corrections are naturally\nallowed when one considers either including explicit $\\Delta$-resonance degrees\nof freedom or one works to higher orders in the chiral expansion. We\ninvestigate the potential impact of these FV corrections with a precision study\nof $g_A$ using models of FV corrections that are monotonic and non-monotonic.\nUsing lattice QCD data that is approximately at the 1% level of precision, we\ndo not see significant evidence of non-monotonic corrections. Looking forward\nto the next phase of lattice QCD calculations, we estimate that calculations\nthat are between the 0.1%-1%-level of precision may be sensitive to these FV\nartifacts. Finally, we present an update of the CalLat prediction of $g_A$ in\nthe isospin limit with sub-percent precision, $g_A^{\\rm QCD} = 1.2674(96)$.",
        "Learning to change shape is a fundamental strategy of adaptation and\nevolution of living organisms, from bacteria and cells to tissues and animals.\nHuman-made materials can also exhibit advanced shape morphing capabilities, but\nlack the ability to learn. Here, we build metamaterials that can learn complex\nshape-changing responses using a contrastive learning scheme. By being shown\nexamples of the target shape changes, our metamaterials are able to learn those\nshape changes by progressively updating internal learning degrees of freedom --\nthe local stiffnesses. Unlike traditional materials that are designed once and\nfor all, our metamaterials have the ability to forget and learn new shape\nchanges in sequence, to learn multiple shape changes that break reciprocity,\nand to learn multistable shape changes, which in turn allows them to perform\nreflex gripping actions and locomotion. Our findings establish metamaterials as\nan exciting platform for physical learning, which in turn opens avenues for the\nuse of physical learning to design adaptive materials and robots.",
        "A new theoretical framework for the nonequilibrium Green's function (NEGF)\nscheme is presented to account for the discrete nature of impurities doped in\nsemiconductor nanostructures. The short-range part of impurity potential is\nincluded as scattering potential in the self-energy due to spatially localized\nimpurity scattering, and the long-range part of impurity potential is treated\nas the self-consistent Hartree potential by coupling with the Poisson equation.\nThe position-dependent impurity scattering rate under inhomogeneous impurity\nprofiles is systematically derived so that its physical meaning is clarified.\nThe position dependence of the scattering rate turns out to be represented by\nthe `center of mass' coordinates in the Wigner coordinates, rather than the\nreal-space coordinates. Consequently, impurity scattering is intrinsically\nnonlocal in space. The proposed framework is applied to cylindrical thin wires\nunder the quasi-one-dimensional (quasi-1D) approximation. We show explicitly\nhow the discrete nature of impurities affects the transport properties such as\nelectrostatic potential, local density of states, carrier density, scattering\nrates, and mobility.",
        "The coupling between the dark matter (DM) halo and the stellar disc is a key\nfactor in galactic evolution. While the interaction between structures like the\nGalactic bar and DM halos has been explored (e.g. slowing down of the bar due\nto dynamical friction), the effect of spiral arms on the DM halo distribution\nhas received limited attention. We analyze a suite of simulations featuring\nstrong stellar spiral arms, ranging in complexity from test-particle models to\nfully cosmological hydrodynamical simulations. Using Fourier transforms, we\ncharacterize the phase and amplitude of the stellar spirals at different times\nand radii. We then apply the same methodology to DM particles near the stellar\ndisc and compare trends in Fourier coefficients and phases between the two\ncomponents. We detect a clear spiral arm signal in the DM distribution,\ncorrelated with the stellar spirals, confirming the reaction of the halo. The\nstrength of the DM spirals consistently measures around 10\\% of that of the\nstellar spiral arms. In the $N$-body simulation, the DM spiral persistently\ntrails the stellar spiral arm by approximately $10^\\circ$. A strong spiral\nsignal of a few km\\,s$^{-1}$ appears in the radial, azimuthal, and vertical\nvelocities of halo particles, distinct from the stellar kinematic signature. In\na test-particle simulation with an analytical spiral potential (omitting\nself-gravity), we reproduce a similar density and kinematic response, showing\nthat the test-particle halo responds in the same way as the $N$-body halo.\nFinally, we also find the rest of the simulations, indicating that the\ndynamical signatures of the forced response in the DM halo are independent of\nthe dynamical origin of the stellar spiral arms. We reveal the ubiquitous\npresence of DM spiral arms in Milky Way-like galaxies, driven by a forced\nresponse to the stellar spiral potential. (ABR)",
        "The development of the Willow quantum chip by Google has sparked significant\ninterest in quantum computing, ushering in a new wave of advancements in the\nfield. As quantum computing technology continues to mature, secure quantum\ncommunication has garnered increasing attention. To establish secure\ncommunication, several quantum key distribution (QKD) protocols have been\nproposed, such as the BB84 protocol, which leverages the principles of quantum\nsuperposition and other quantum properties to ensure secure transmission.\nHowever, existing QKD protocols may face vulnerabilities under certain\nconditions. This study proposes two types of man-in-the-middle (MITM) attack\ntechniques and demonstrates their potential to compromise quantum cryptography\nthrough practical case studies. Furthermore, this study proposes strategies to\ncounteract these MITM attacks and proposes methods to enhance the security of\nquantum cryptographic systems. The findings offer valuable insights for the\nfuture implementation and deployment of secure quantum communication systems.",
        "We consider the subcritical nonlinear Schr\\\"odinger equation on quantum\ngraphs with an attractive potential supported in the compact core, and\ninvestigate the existence and the nonexistence of Ground States, defined as\nminimizers of the energy at fixed $L^2$-norm, or mass. We finally reach the\nfollowing picture: for small and large mass there are Ground States. Moreover,\naccording to the metric features of the compact core of the graph and to the\nstrength of the potential, there may be a region of intermediate masses for\nwhich there are no Ground States. The study was originally inspired by the\nresearch on quantum waveguides, in which the curvature of a thin tube induces\nan effective attractive potential.",
        "We study a class of non-local functionals that was introduced by\nBrezis--Seeger--Van Schaftingen--Yung, and can be used to characterize the\ntotal variation of functions. We establish the $\\Gamma$-limit of these\nfunctionals with respect to area-strict convergence.",
        "The objective of this paper is to investigate the connection between penalty\nfunctions from stochastic optimal control, convex semigroups from analysis and\nconvex expectations from probability theory. Our main result provides a\none-to-one relation between these objects. As an application, we use the\nrepresentation via penality functions and duality arguments to show that convex\nexpectations are determined by their finite dimensional distributions. To\nillustrate this structural result, we show that Hu and Peng's axiomatic\ndescription of $G$-L\\'evy processes in terms of finite dimensional\ndistributions extends uniquely to the control approach introduced by Neufeld\nand Nutz. Finally, we show that convex expectations with a Markovian structure\nare fully determined by their one-dimensional distributions, which give rise to\na classical semigroup on the state space.",
        "We investigate the interrelations between the metric properties, order\nproperties and combinatorial properties of the set of balls in totally bounded\nultrametric space. In particular, the Gurvich-Vyalyi representation of finite,\nultrametric spaces by monotone rooted trees is generalized to the case of\ntotally bounded ultrametric spaces. It is shown that such spaces have isometric\ncompletions if and only if their labeled representing trees are isomorphic. We\ncharacterize up to isomorphism the representing trees of these spaces and, up\nto order isomorphism, the posets of open balls in such spaces.",
        "The transition to a cleaner energy mix, essential for achieving net-zero\ngreenhouse gas emissions by 2050, will significantly increase demand for metals\ncritical to renewable energy technologies. Energy Transition Metals (ETMs),\nincluding copper, lithium, nickel, cobalt, and rare earth elements, are\nindispensable for renewable energy generation and the electrification of global\neconomies. However, their markets are characterized by high price volatility\ndue to supply concentration, low substitutability, and limited price\nelasticity. This paper provides a comprehensive analysis of the price\nvolatility of ETMs, a subset of Critical Raw Materials (CRMs). Using a\ncombination of exploratory data analysis, data reduction, and visualization\nmethods, we identify key features for accurate point and density forecasts. We\nevaluate various volatility models, including Generalized Autoregressive\nConditional Heteroskedasticity (GARCH) and Stochastic Volatility (SV) models,\nto determine their forecasting performance. Our findings reveal significant\nheterogeneity in ETM volatility patterns, which challenge standard groupings by\ndata providers and geological classifications. The results contribute to the\nliterature on CRM economics and commodity volatility, offering novel insights\ninto the complex dynamics of ETM markets and the modeling of their returns and\nvolatilities.",
        "Searches for continuous gravitational waves from isolated compact objects and\nthose in binary systems aim to detect non-axisymmetric, deformed neutron stars\nat particular locations in the Galaxy or all-sky. However, a large fraction of\nknown pulsars have rotational frequencies that lie outside the audio frequency\nband, rendering current detectors insensitive to these pulsars. In this work,\nwe show that DECIGO, a future space-based deci-hertz gravitational-wave\ninterferometer, will be sensitive to severely deformed compact objects, e.g.\nhybrid stars, neutron stars, or magnetars. We estimate the number of possible\ncompact objects that could be detected with such high deformations, both via\ntheir individual continuous gravitational-wave emission and the stochastic\ngravitational-wave background created by a superposition of gravitational waves\nfrom the $\\sim 10^8$ compact objects in the Galaxy. Furthermore, we show that\nthe existence of such compact objects could be probed across a wide parameter\nspace at a fraction of the computational cost of current searches for isolated\ncompact objects and those in binary systems. For known pulsars, we will be able\nto both beat the spin-down limit and probe the Brans-Dicke modified theory of\ngravity parameter $\\zeta<1$ for approximately 85% of known pulsars with $f_{\\rm\ngw}<10$ Hz, the latter of which is currently only possible for $O(10)$ pulsars.\nDECIGO will thus open a new window to probe highly deformed compact objects and\nover half of the known pulsars, both of which are currently inaccessible to\nground-based detectors.",
        "What is the maximum number of points that can be selected from an $n \\times\nn$ square lattice such that no $k+1$ of them are in a line? This has been asked\nmore than $100$ years ago for $k=2$ and it remained wide open ever since. In\nthis paper, we prove the precise answer is $kn$, provided that\n$k>C\\sqrt{n\\log{n}}$ for an absolute constant $C$. The proof relies on\ncarefully constructed bi-uniform random bipartite graphs and concentration\ninequalities.",
        "This article presents a novel network protocol that incorporates a quantum\nphotonic channel for symmetric key distribution, a Dilithium signature to\nreplace factor-based public key cryptography for enhanced authentication,\nsecurity, and privacy. The protocol uses strong hash functions to hash original\nmessages and verify heightened data integrity at the destination. This Quantum\nGood Authentication Protocol (QGP) provides high-level security provided by the\ntheory of quantum mechanics. QGP also has the advantage of quantum-resistant\ndata protection that prevents current digital computer and future quantum\ncomputer attacks.\n  QGP transforms the Transmission Control Protocol\/Internet Protocol (TCP\/IP)\nby adding a quantum layer at the bottom of Open Systems Interconnection (OSI)\nmodel (layer 0) and modifying the top layer (layer 7) with Dilithium\nsignatures, thus improving the security of the original OSI model. In addition,\nQGP incorporates strong encryption, hardware-based quantum channels,\npost-quantum signatures, and secure hash algorithms over a platform of\ndecryptors, switches, routers, and network controllers to form a testbed of the\nnext-generation, secure quantum internet. The experiments presented here show\nthat QGP provides secure authentication and improved security and privacy and\ncan be adopted as a new protocol for the next-generation quantum Internet.",
        "The discovery of neutrino oscillations and masses provides strong motivation\nto extend the Standard Model by including right-handed neutrinos, which lead to\nheavy neutrino states that could exist at the electroweak scale. These states\nmay also be influenced by new high-scale, weakly interacting physics.\nIncorporating right-handed neutrinos into an effective field theory framework\n-- the $\\nu$SMEFT -- offers a systematic approach to study the phenomenology of\nheavy neutrinos in current and upcoming experiments. In this work, we present\nthe first prospective 95\\% exclusion plots achievable at a future lepton\ncollider operating at a center-of-mass energy of $\\sqrt{s}=0.5 ~\\rm{TeV}$ for\nwhat we term the agnostic $\\nu$SMEFT scenario. This study focuses on the\nhigh-mass regime where the heavy neutrino $N$ decays promptly into leptons and\njets. Specifically, we analyze the processes $e^+e^- \\to \\nu N \\to \\nu \\mu^{-}\n\\mu^{+} \\nu$ and $e^+e^- \\to \\nu N \\to \\nu \\mu^{-} \\mathrm{j} \\mathrm{j}$,\nderiving the exclusion regions in the $\\frac{\\alpha}{\\Lambda^2}$ vs. $m_N$\nparameter space. When compared to prospective limits for the LHeC, we find that\nthe semi-leptonic process with final jets in a lepton collider offers the\ngreatest sensitivity, even with a straightforward cut-based analysis. The\nexpected bounds are as stringent as those considered in recent studies for the\nlow-mass regime where the $N$ may be long-lived and detectable via displaced\ndecay searches, both at the LHC and future colliders.",
        "We prove that for any simply connected isotropic reductive group G over a\nDedekind domain D, any Zariski-locally trivial principal G-bundle over D is\ntrivial. The corresponding result for quasi-split groups was proved in 1967 by\nG. Harder."
      ]
    }
  },
  {
    "id":2411.12897,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Mapping Tree Species Using Advanced Remote Sensing Technologies: A State-of-the-Art Review and Perspective",
    "start_abstract":"Timely and accurate information on tree species (TS) is crucial for developing strategies sustainable management conservation of artificial natural forests. Over the last four decades, advances in remote sensing technologies have made TS classification possible. Since many studies topic been conducted their comprehensive results novel findings published literature, it necessary to conduct an updated review status, trends, potentials, challenges recommend future directions. The will provide overview various optical light detection ranging (LiDAR) sensors; present assess current techniques\/methods for, a general trend method development in, classification; identify limitations In this review, several concluding remarks were made. They include following: (1) A large group using high-resolution satellite, airborne multi-\/hyperspectral imagery, LiDAR data. (2) \u201cmultiple\u201d was observed. (3) Machine learning methods including deep models demonstrated be significant improving accuracy. (4) Recently, unmanned aerial vehicle- (UAV-) based sensors caught interest researchers practitioners topic-related research applications. addition, three directions recommended, refining categories methods, data fusion algorithms or processing chains, exploring new spectral unmixing automatically extract map from satellite hyperspectral",
    "start_categories":[
      "Remote Sensing Technologies"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Morphological transformation and spatial-logical aggregation for tree species classification using hyperspectral imagery"
      ],
      "abstract":[
        "Hyperspectral image (HSI) consists of abundant spectral and spatial characteristics, which contribute to a more accurate identification of materials and land covers. However, most existing methods of hyperspectral image analysis primarily focus on spectral knowledge or coarse-grained spatial information while neglecting the fine-grained morphological structures. In the classification task of complex objects, spatial morphological differences can help to search for the boundary of fine-grained classes, e.g., forestry tree species. Focusing on subtle traits extraction, a spatial-logical aggregation network (SLA-NET) is proposed with morphological transformation for tree species classification. The morphological operators are effectively embedded with the trainable structuring elements, which contributes to distinctive morphological representations. We evaluate the classification performance of the proposed method on two tree species datasets, and the results demonstrate that the proposed SLA-NET significantly outperforms the other state-of-the-art classifiers."
      ],
      "categories":[
        "Evolutionary Biology"
      ]
    },
    "list":{
      "title":[
        "Runge type approximation results for spaces of smooth Whitney jets",
        "Design Considerations in Offline Preference-based RL",
        "Increasing the p-Selmer rank by twisting",
        "Copilot Arena: A Platform for Code LLM Evaluation in the Wild",
        "Demystifying integrable QFTs in AdS: No-go theorems for higher-spin\n  charges",
        "4bit-Quantization in Vector-Embedding for RAG",
        "ViT-2SPN: Vision Transformer-based Dual-Stream Self-Supervised\n  Pretraining Networks for Retinal OCT Classification",
        "Impact of Electric Spatially Discordant Alternans on Cardiac Magnetic\n  Field",
        "Supersymmetric Higher-Spin Gauge Theories in any $d$ and their Coupling\n  Constants within BRST Formalism",
        "How Collective Intelligence Emerges in a Crowd of People Through Learned\n  Division of Labor: A Case Study",
        "A real-time battle situation intelligent awareness system based on\n  Meta-learning & RNN",
        "Nonlinear optical metasurfaces empowered by bound-states in the\n  continuum",
        "Parametric Hypersensitivity and Transport in the Steady-State\n  Open-System Holstein Model",
        "GeoWarp: Warped spatial processes for inferring subsea sediment\n  properties",
        "On Fairness of Unified Multimodal Large Language Model for Image\n  Generation",
        "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances\n  Rare Disease Diagnosis from Clinical Notes",
        "Transparent Decompilation for Timing Side-Channel Analyses",
        "GraspCorrect: Robotic Grasp Correction via Vision-Language Model-Guided\n  Feedback",
        "Improved estimates of statistical properties in some non-uniformly\n  hyperbolic dynamical systems",
        "Diffusion Restoration Adapter for Real-World Image Restoration",
        "Grokking Explained: A Statistical Phenomenon",
        "Power Ramp-Rate Control via Power Regulation for Storageless\n  Grid-Connected Photovoltaic Systems",
        "Adaptive Grasping of Moving Objects in Dense Clutter via Global-to-Local\n  Detection and Static-to-Dynamic Planning",
        "On 1-regular and 1-uniform metric measure spaces",
        "Measuring ultrafast laser pulses using a single-shot amplitude swing\n  implementation",
        "Provable Zero-Shot Generalization in Offline Reinforcement Learning",
        "Plan-over-Graph: Towards Parallelable LLM Agent Schedule",
        "LET measurements and simulation modelling of the charged particle field\n  for the Clatterbridge ocular proton therapy beamline",
        "Configuration of Single Giant Planet Systems Generating `Oumuamua-Like\n  Interstellar Asteroids"
      ],
      "abstract":[
        "We prove Runge type approximation results for linear partial differential\noperators with constant coefficients on spaces of smooth Whitney jets. Among\nothers, we characterize when for a constant coefficient linear partial\ndifferential operator $P(D)$ and for closed subsets $F_1\\subset F_2$ of\n$\\mathbb{R}^d$ the restrictions to $F_1$ of smooth Whitney jets $f$ on $F_2$\nsatisfying $P(D)f=0$ on $F_2$ are dense in the space of smooth Whitney jets on\n$F_1$ satisfying the same partial differential equation on $F_1$. For elliptic\noperators we give a geometric evaluation of this characterization.\nAdditionally, for differential operators with a single characteristic\ndirection, like parabolic operators, we give a sufficient geometric condition\nfor the above density to hold. Under mild additional assumptions on $\\partial\nF_1$ and for $F_2=\\mathbb{R}^d$ this sufficient conditions is also necessary.\nAs an application of our work, we characterize those open subsets $\\Omega$ of\nthe complex plane satisfying $\\Omega=\\operatorname{int}\\overline{\\Omega}$ for\nwhich the set of holomorphic polynomials are dense in $A^\\infty(\\Omega)$, under\nthe mild additional hypothesis that $\\overline{\\Omega}$ satisfies the strong\nregularity condition. Furthermore, for the wave operator in one spatial\nvariable, a simple sufficient geometric condition on $F_1,\nF_2\\subset\\mathbb{R}^2$ is given for the above density to hold. For the special\ncase of $F_2=\\mathbb{R}^2$ this sufficient condition is also necessary under\nmild additional hypotheses on $F_1$.",
        "Offline algorithms for Reinforcement Learning from Human Preferences (RLHF),\nwhich use only a fixed dataset of sampled responses given an input, and\npreference feedback among these responses, have gained increasing prominence in\nthe literature on aligning language models. In this paper, we study how the\ndifferent design choices made in methods such as DPO, IPO, SLiC and many\nvariants influence the quality of the learned policy, from a theoretical\nperspective. Our treatment yields insights into the choices of loss function,\nthe policy which is used to normalize log-likelihoods, and also the role of the\ndata sampling policy. Notably, our results do not rely on the standard\nreparameterization-style arguments used to motivate some of the algorithms in\nthis family, which allows us to give a unified treatment to a broad class of\nmethods. We also conduct a small empirical study to verify some of the\ntheoretical findings on a standard summarization benchmark.",
        "In this paper, we study the $p$-Selmer groups in the family of $p$-twists of\nan elliptic curve $E$ over a number field $K$. We prove that if $E\/K$ is an\nelliptic curve over a number field $K$, and if $d$ is congruent to the\ndimension of the Selmer group of $E\/K$ modulo $2$ and is greater than that\ndimension, then there exist infinitely many characters $\\chi \\in\n\\text{Hom}(G_K, \\mu_p)$ such that $\\text{dim}_{\\mathbb{F}_p}(\\text{Sel}_p(E\/K,\n\\chi)) = d$ under certain conditions.",
        "Evaluating in-the-wild coding capabilities of large language models (LLMs) is\na challenging endeavor with no clear solution. We introduce Copilot Arena, a\nplatform to collect user preferences for code generation through native\nintegration into a developer's working environment. Copilot Arena comprises a\nnovel interface for comparing pairs of model outputs, a sampling strategy\noptimized to reduce latency, and a prompting scheme to enable code completion\nfunctionality. Copilot Arena has served over 4.5 million suggestions from 10\nmodels and collected over 11k pairwise judgements. Our results highlight the\nimportance of model evaluations in integrated settings. We find that model\nrankings from Copilot Arena differ from those of existing evaluations, which we\nattribute to the more realistic distribution of data and tasks contained in\nCopilot Arena. We also identify novel insights into human preferences on code\nsuch as an observed consistency in user preference across programming languages\nyet significant variation in preference due to task category. We open-source\nCopilot Arena and release data to enable human-centric evaluations and improve\nunderstanding of coding assistants.",
        "Higher-spin conserved currents and charges feature prominently in integrable\n2d QFTs in flat space. Motivated by the question of integrable field theories\nin AdS space, we consider the consequences of higher-spin currents for QFTs in\nAdS$_2$, and find that their effect is much more constraining than in flat\nspace. Specifically, it is impossible to preserve: (a)~any higher-spin charges\nwhen deforming a massive free field by interactions, or (b)~any spin-4 charges\nwhen deforming a CFT by a Virasoro primary. Therefore, in these settings, there\nare no integrable theories in AdS with higher-spin conserved charges. Along the\nway, we explain how higher-spin charges lead to integer spacing in the spectrum\nof primaries, sum rules on the OPE data, and constraints on correlation\nfunctions. We also explain a key difference between AdS and flat space: in AdS\none cannot `partially' conserve a higher-spin current along particular\ndirections, since the AdS isometries imply full conservation.",
        "Retrieval-augmented generation (RAG) is a promising technique that has shown\ngreat potential in addressing some of the limitations of large language models\n(LLMs). LLMs have two major limitations: they can contain outdated information\ndue to their training data, and they can generate factually inaccurate\nresponses, a phenomenon known as hallucinations. RAG aims to mitigate these\nissues by leveraging a database of relevant documents, which are stored as\nembedding vectors in a high-dimensional space. However, one of the challenges\nof using high-dimensional embeddings is that they require a significant amount\nof memory to store. This can be a major issue, especially when dealing with\nlarge databases of documents. To alleviate this problem, we propose the use of\n4-bit quantization to store the embedding vectors. This involves reducing the\nprecision of the vectors from 32-bit floating-point numbers to 4-bit integers,\nwhich can significantly reduce the memory requirements. Our approach has\nseveral benefits. Firstly, it significantly reduces the memory storage\nrequirements of the high-dimensional vector database, making it more feasible\nto deploy RAG systems in resource-constrained environments. Secondly, it speeds\nup the searching process, as the reduced precision of the vectors allows for\nfaster computation. Our code is available at\nhttps:\/\/github.com\/taeheej\/4bit-Quantization-in-Vector-Embedding-for-RAG",
        "Optical Coherence Tomography (OCT) is a non-invasive imaging modality\nessential for diagnosing various eye diseases. Despite its clinical\nsignificance, developing OCT-based diagnostic tools faces challenges, such as\nlimited public datasets, sparse annotations, and privacy concerns. Although\ndeep learning has made progress in automating OCT analysis, these challenges\nremain unresolved. To address these limitations, we introduce the Vision\nTransformer-based Dual-Stream Self-Supervised Pretraining Network (ViT-2SPN), a\nnovel framework designed to enhance feature extraction and improve diagnostic\naccuracy. ViT-2SPN employs a three-stage workflow: Supervised Pretraining,\nSelf-Supervised Pretraining (SSP), and Supervised Fine-Tuning. The pretraining\nphase leverages the OCTMNIST dataset (97,477 unlabeled images across four\ndisease classes) with data augmentation to create dual-augmented views. A\nVision Transformer (ViT-Base) backbone extracts features, while a negative\ncosine similarity loss aligns feature representations. Pretraining is conducted\nover 50 epochs with a learning rate of 0.0001 and momentum of 0.999.\nFine-tuning is performed on a stratified 5.129% subset of OCTMNIST using\n10-fold cross-validation. ViT-2SPN achieves a mean AUC of 0.93, accuracy of\n0.77, precision of 0.81, recall of 0.75, and an F1 score of 0.76, outperforming\nexisting SSP-based methods.",
        "Spatially discordant alternans (SDA) play a crucial role in cardiac\narrhythmogenesis by creating steep repolarization gradients facilitating\nconduction block and reentry. While traditionally studied using electrical\nindicators, this work provides a novel perspective by characterizing SDA\nthrough their magnetic field signatures. Using a one-dimensional cardiac fiber\nmodel, we demonstrate that magnetic field measurements effectively detect SDA\nand temperature dependent changes in cardiac action potentials, offering a\nnon-invasive alternative to conventional electrophysiological metrics. Our\nresults reveal that the spatial organization of SDA is mirrored in the magnetic\nfield distribution, with SDA nodes clearly identifiable via spatial mapping.\nNotably, magnetic restitution curves exhibit a distinct pattern from APD-based\nindicators, closely following the dynamics of the action potential upstroke.\nThese findings establish the cardiac magnetic field as a powerful diagnostic\ntool for detecting SDA, opening new avenues for biomagnetic monitoring of\narrhythmic risk.",
        "Nonlinear field equations for the supersymmetric higher-spin gauge theory\ndescribing totally symmetric bosonic and fermionic massless fields along with\nhook-type bosonic fields of all spins in any space-time dimension are\npresented. One of the novel features of the proposed formalism is that the\n$osp(1,2)$ invariance and factorisation conditions are formulated within the\nBRST formalism, that greatly simplifies the form of nonlinear HS equations. To\nmatch the list of vertices found by Metsaev, higher-spin gauge theory is\nanticipated to possess an infinite number of independent coupling constants. A\nconjecture that these coupling constants result from the locality restrictions\non the elements of the factorisation ideal is put forward.",
        "This paper investigates the factors fostering collective intelligence (CI)\nthrough a case study of *LinYi's Experiment, where over 2000 human players\ncollectively controll an avatar car. By conducting theoretical analysis and\nreplicating observed behaviors through numerical simulations, we demonstrate\nhow self-organized division of labor (DOL) among individuals fosters the\nemergence of CI and identify two essential conditions fostering CI by\nformulating this problem into a stability problem of a Markov Jump Linear\nSystem (MJLS). These conditions, independent of external stimulus, emphasize\nthe importance of both elite and common players in fostering CI. Additionally,\nwe propose an index for emergence of CI and a distributed method for estimating\njoint actions, enabling individuals to learn their optimal social roles without\nglobal action information of the whole crowd.",
        "In modern warfare, real-time and accurate battle situation analysis is\ncrucial for making strategic and tactical decisions. The proposed real-time\nbattle situation intelligent awareness system (BSIAS) aims at meta-learning\nanalysis and stepwise RNN (recurrent neural network) modeling, where the former\ncarries out the basic processing and analysis of battlefield data, which\nincludes multi-steps such as data cleansing, data fusion, data mining and\ncontinuously updates, and the latter optimizes the battlefield modeling by\nstepwise capturing the temporal dependencies of data set. BSIAS can predict the\npossible movement from any side of the fence and attack routes by taking a\nsimulated battle as an example, which can be an intelligent support platform\nfor commanders to make scientific decisions during wartime. This work delivers\nthe potential application of integrated BSIAS in the field of battlefield\ncommand & analysis engineering.",
        "Optical bound-states in the continuum (BICs) have greatly enriched the field\nof nonlinear optics with novel ways to control and manipulate light-matter\ninteraction at the nanoscale. This has been made possible by their unique\nphysical properties, including effective confinement of light, non-trivial\ntopological features, and robustness upon the propagation of the optical field\nboth in the real and momentum space. Regarding the exploration of nonlinear\noptical response in various photonic nanostructures supporting BICs, particular\nattention has been paid to optical metasurfaces, chiefly due to their ability\nto control the light flow at subwavelength scale, design and fabrication\nflexibility, and convenient phase-matching conditions. In this review, we\noutline and discuss recent advances in metasurface-based frequency conversion\nprocesses utilizing the versatile physics of BICs, with a particular emphasis\non the main physics background pertaining to nonlinear optical phenomena and\noptics of BICs, as well as state-of-the-art functionalities enabled by\nBIC-driven nonlinear metasurfaces. These applications include harmonic\ngeneration, harmonic chiroptical effects, generation of complex quantum states,\nand broadband terahertz generation. In addition, several emerging research\nfields and the existing challenges of photonic nanodevices relying on BICs are\ndiscussed.",
        "We demonstrate that the nonequilibrium steady state (NESS) of an open-system\nHolstein model with linear bias displays extreme sensitivity to the closed\nsystem parameters. This sensitivity is shown to correspond to avoided crossings\nin the closed system spectrum, as previously demonstrated in the Rabi model. We\nthen develop a kinetic model to analyze the effects of environmental parameters\non NESS hypersensitivity. This reveals that hypersensitivity only exists in\nintermediate environmental parameter regimes, a prediction that is verified\nnumerically. The inherent spatial character of the Holstein model offers a\nnatural connection to transport, revealing that transport properties in the\nsteady-state regime can be optimized by simultaneously coordinating the closed-\nand open-system parameters.",
        "For offshore structures like wind turbines, subsea infrastructure, pipelines,\nand cables, it is crucial to quantify the properties of the seabed sediments at\na proposed site. However, data collection offshore is costly, so analysis of\nthe seabed sediments must be made from measurements that are spatially sparse.\nAdding to this challenge, the structure of the seabed sediments exhibits both\nnonstationarity and anisotropy. To address these issues, we propose GeoWarp, a\nhierarchical spatial statistical modeling framework for inferring the 3-D\ngeotechnical properties of subsea sediments. GeoWarp decomposes the seabed\nproperties into a region-wide vertical mean profile (modeled using B-splines),\nand a nonstationary 3-D spatial Gaussian process. Process nonstationarity and\nanisotropy are accommodated by warping space in three dimensions and by\nallowing the process variance to change with depth. We apply GeoWarp to\nmeasurements of the seabed made using cone penetrometer tests (CPTs) at six\nsites on the North West Shelf of Australia. We show that GeoWarp captures the\ncomplex spatial distribution of the sediment properties, and produces realistic\n3-D simulations suitable for downstream engineering analyses. Through\ncross-validation, we show that GeoWarp has predictive performance superior to\nother state-of-the-art methods, demonstrating its value as a tool in offshore\ngeotechnical engineering.",
        "Unified multimodal large language models (U-MLLMs) have demonstrated\nimpressive performance in visual understanding and generation in an end-to-end\npipeline. Compared with generation-only models (e.g., Stable Diffusion),\nU-MLLMs may raise new questions about bias in their outputs, which can be\naffected by their unified capabilities. This gap is particularly concerning\ngiven the under-explored risk of propagating harmful stereotypes. In this\npaper, we benchmark the latest U-MLLMs and find that most exhibit significant\ndemographic biases, such as gender and race bias. To better understand and\nmitigate this issue, we propose a locate-then-fix strategy, where we audit and\nshow how the individual model component is affected by bias. Our analysis shows\nthat bias originates primarily from the language model. More interestingly, we\nobserve a \"partial alignment\" phenomenon in U-MLLMs, where understanding bias\nappears minimal, but generation bias remains substantial. Thus, we propose a\nnovel balanced preference model to balance the demographic distribution with\nsynthetic data. Experiments demonstrate that our approach reduces demographic\nbias while preserving semantic fidelity. We hope our findings underscore the\nneed for more holistic interpretation and debiasing strategies of U-MLLMs in\nthe future.",
        "Background: Several studies show that large language models (LLMs) struggle\nwith phenotype-driven gene prioritization for rare diseases. These studies\ntypically use Human Phenotype Ontology (HPO) terms to prompt foundation models\nlike GPT and LLaMA to predict candidate genes. However, in real-world settings,\nfoundation models are not optimized for domain-specific tasks like clinical\ndiagnosis, yet inputs are unstructured clinical notes rather than standardized\nterms. How LLMs can be instructed to predict candidate genes or disease\ndiagnosis from unstructured clinical notes remains a major challenge. Methods:\nWe introduce RAG-driven CoT and CoT-driven RAG, two methods that combine\nChain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze\nclinical notes. A five-question CoT protocol mimics expert reasoning, while RAG\nretrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in\nMan). We evaluated these approaches on rare disease datasets, including 5,980\nPhenopacket-derived notes, 255 literature-based narratives, and 220 in-house\nclinical notes from Childrens Hospital of Philadelphia. Results: We found that\nrecent foundations models, including Llama 3.3-70B-Instruct and\nDeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2\nand GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both\noutperform foundation models in candidate gene prioritization from clinical\nnotes; in particular, both methods with DeepSeek backbone resulted in a top-10\ngene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT\nworks better for high-quality notes, where early retrieval can anchor the\nsubsequent reasoning steps in domain-specific evidence, while CoT-driven RAG\nhas advantage when processing lengthy and noisy notes.",
        "This paper considers the problem of analyzing the timing side-channel\nsecurity of binary programs through decompilation and source-level analysis. We\nfocus on two popular policies, namely constant-time and speculative\nconstant-time, (S)CT for short, used to protect cryptographic libraries.\n  First, we observe that popular decompilers remove (S)CT violations, i.e.,\ntransform non-(S)CT programs into (S)CT programs; it follows that analyzing\ndecompiled programs is not sound. Second, we develop techniques to prove that\ndecompilers are transparent, i.e., neither introduce nor remove (S)CT\nviolations. Third, we apply our techniques to \\refleCT{}, a core but\nnon-trivial decompiler. As a contribution of independent interest, we find that\nconstant-time verification tools may not be sound, due to their use of\npreprocessors (e.g.\\, binary lifters or IR converters) that eliminate CT\nviolations.",
        "Despite significant advancements in robotic manipulation, achieving\nconsistent and stable grasping remains a fundamental challenge, often limiting\nthe successful execution of complex tasks. Our analysis reveals that even\nstate-of-the-art policy models frequently exhibit unstable grasping behaviors,\nleading to failure cases that create bottlenecks in real-world robotic\napplications. To address these challenges, we introduce GraspCorrect, a\nplug-and-play module designed to enhance grasp performance through\nvision-language model-guided feedback. GraspCorrect employs an iterative visual\nquestion-answering framework with two key components: grasp-guided prompting,\nwhich incorporates task-specific constraints, and object-aware sampling, which\nensures the selection of physically feasible grasp candidates. By iteratively\ngenerating intermediate visual goals and translating them into joint-level\nactions, GraspCorrect significantly improves grasp stability and consistently\nenhances task success rates across existing policy models in the RLBench and\nCALVIN datasets.",
        "Building upon previous works by Young, Chernov-Zhang and\nBruin-Melbourne-Terhesiu, we present a general scheme to improve bounds on the\nstatistical properties (in particular, decay of correlations, and rates in the\nalmost sure invariant principle) for a class of non-uniformly hyperbolic\ndynamical systems. Specifically, for systems with polynomial, yet summable\nmixing rates, our method removes logarithmic factors of earlier arguments,\nresulting in essentially optimal bounds. Applications include Wojtkowski's\nsystem of two falling balls, dispersing billiards with flat points and\nBunimovich's flower-shaped billiard tables.",
        "Diffusion models have demonstrated their powerful image generation\ncapabilities, effectively fitting highly complex image distributions. These\nmodels can serve as strong priors for image restoration. Existing methods often\nutilize techniques like ControlNet to sample high quality images with low\nquality images from these priors. However, ControlNet typically involves\ncopying a large part of the original network, resulting in a significantly\nlarge number of parameters as the prior scales up. In this paper, we propose a\nrelatively lightweight Adapter that leverages the powerful generative\ncapabilities of pretrained priors to achieve photo-realistic image restoration.\nThe Adapters can be adapt to both denoising UNet and DiT, and performs\nexcellent.",
        "Grokking, or delayed generalization, is an intriguing learning phenomenon\nwhere test set loss decreases sharply only after a model's training set loss\nhas converged. This challenges conventional understanding of the training\ndynamics in deep learning networks. In this paper, we formalize and investigate\ngrokking, highlighting that a key factor in its emergence is a distribution\nshift between training and test data. We introduce two synthetic datasets\nspecifically designed to analyze grokking. One dataset examines the impact of\nlimited sampling, and the other investigates transfer learning's role in\ngrokking. By inducing distribution shifts through controlled imbalanced\nsampling of sub-categories, we systematically reproduce the phenomenon,\ndemonstrating that while small-sampling is strongly associated with grokking,\nit is not its cause. Instead, small-sampling serves as a convenient mechanism\nfor achieving the necessary distribution shift. We also show that when classes\nform an equivariant map, grokking can be explained by the model's ability to\nlearn from similar classes or sub-categories. Unlike earlier work suggesting\nthat grokking primarily arises from high regularization and sparse data, we\ndemonstrate that it can also occur with dense data and minimal hyper-parameter\ntuning. Our findings deepen the understanding of grokking and pave the way for\ndeveloping better stopping criteria in future training processes.",
        "Photovoltaic Power Ramp-Rate Control (PRRC) constitutes a key ancillary\nservice for future power systems. Although its implementation through the\ninstallation of storage systems or irradiance sensors has been widely\ninvestigated, fewer studies have explored the power curtailment approach. The\nlatter lacks efficiency, as it voluntarily produces power discharges, yet it is\na cost-effective solution in terms of capital expenditures. This paper proposes\na novel storageless and sensorless photovoltaic PRRC for grid-connected\napplications in which the photovoltaic power, rather than the voltage, is the\ncontrolled magnitude. The aforementioned contribution makes the effective\ntracking of the power ramp-rate limit possible compared to the existing methods\nin the literature. The method is assisted by a real-time curve-fitting\nalgorithm that estimates the Maximum Power Point while operating suboptimally.\nThus, no direct temperature or irradiance measurement systems are needed. The\nvalidation of the proposed PRRC strategy has been tested by simulation and\ncompared to another approach available in the literature, considering\nreal-field highly variable irradiance data. Experimental validation of the\nproposed strategy has been performed in real time via Controller\nHardware-in-the-Loop.",
        "Robotic grasping is facing a variety of real-world uncertainties caused by\nnon-static object states, unknown object properties, and cluttered object\narrangements. The difficulty of grasping increases with the presence of more\nuncertainties, where commonly used learning-based approaches struggle to\nperform consistently across varying conditions. In this study, we integrate the\nidea of similarity matching to tackle the challenge of grasping novel objects\nthat are simultaneously in motion and densely cluttered using a single RGBD\ncamera, where multiple uncertainties coexist. We achieve this by shifting\nvisual detection from global to local states and operating grasp planning from\nstatic to dynamic scenes. Notably, we introduce optimization methods to enhance\nplanning efficiency for this time-sensitive task. Our proposed system can adapt\nto various object types, arrangements and movement speeds without the need for\nextensive training, as demonstrated by real-world experiments. Videos are\navailable at https:\/\/youtu.be\/sdC50dx-xp8?si=27oVr4dhG0rqN_tT.",
        "A metric measure space $(X,\\mu)$ is 1-regular if \\[0< \\lim_{r\\to 0}\n\\frac{\\mu(B(x,r))}{r}<\\infty\\] for $\\mu$-a.e $x\\in X$. We give a complete\ngeometric characterisation of the rectifiable and purely unrectifiable part of\na 1-regular measure in terms of its tangent spaces.\n  A special instance of a 1-regular metric measure space is a 1-uniform space\n$(Y,\\nu)$, which satisfies $\\nu(B(y,r))=r$ for all $y\\in Y$ and $r>0$. We prove\nthat there are exactly three 1-uniform metric measure spaces.",
        "Single-shot characterization techniques are crucial when dealing with\nshot-to-shot pulse-shape fluctuations (e.g., unstable laser systems,\nhigh-power, or with low repetition rate) since the scanning configurations\ncannot measure single pulses. The demand for simple setups that can be easily\nadapted to a wide variety of experimental conditions is continuously rising. In\nthis work, we propose a single-shot implementation of amplitude swing,\nmaintaining the compactness, versatility, and robustness of the scanning\nversions of this technique. First, we theoretically study the proposed\nimplementation, based on a pair of uniaxial wedges. Then, we present the\nretrieval ptychographic algorithm. Finally, we experimentally demonstrate the\nsetup by comparing the single-shot and scanning traces and their retrieved\npulses. In sum, we provide the ultrafast science community with a simple and\nversatile setup capable of measuring single laser pulses, which is necessary\nfor characterizing fluctuating pulse trains, meeting the current increasing\ndemand.",
        "In this work, we study offline reinforcement learning (RL) with zero-shot\ngeneralization property (ZSG), where the agent has access to an offline dataset\nincluding experiences from different environments, and the goal of the agent is\nto train a policy over the training environments which performs well on test\nenvironments without further interaction. Existing work showed that classical\noffline RL fails to generalize to new, unseen environments. We propose\npessimistic empirical risk minimization (PERM) and pessimistic proximal policy\noptimization (PPPO), which leverage pessimistic policy evaluation to guide\npolicy learning and enhance generalization. We show that both PERM and PPPO are\ncapable of finding a near-optimal policy with ZSG. Our result serves as a first\nstep in understanding the foundation of the generalization phenomenon in\noffline reinforcement learning.",
        "Large Language Models (LLMs) have demonstrated exceptional abilities in\nreasoning for task planning. However, challenges remain under-explored for\nparallel schedules. This paper introduces a novel paradigm, plan-over-graph, in\nwhich the model first decomposes a real-life textual task into executable\nsubtasks and constructs an abstract task graph. The model then understands this\ntask graph as input and generates a plan for parallel execution. To enhance the\nplanning capability of complex, scalable graphs, we design an automated and\ncontrollable pipeline to generate synthetic graphs and propose a two-stage\ntraining scheme. Experimental results show that our plan-over-graph method\nsignificantly improves task performance on both API-based LLMs and trainable\nopen-sourced LLMs. By normalizing complex tasks as graphs, our method naturally\nsupports parallel execution, demonstrating global efficiency. The code and data\nare available at https:\/\/github.com\/zsq259\/Plan-over-Graph.",
        "Proton therapy can achieve a highly targeted treatment by utilising the\nadvantageous dosimetric characteristics of the Bragg Peak. Protons traversing\nthrough a material will deposit their maximum energy at the Bragg Peak through\nionisation and other interactions, transferring minimal excess dose to\nsurrounding tissue and organs. This rate of energy loss is also quantified by\nthe linear energy transfer (LET), which is indicative of radiation quality and\nradiobiological effects. However it is a challenging physical quantity to\nmeasure, as characterisation of radiation fields and the impact of LET on\ntreatment requires advanced tools and technology. The MiniPIX-Timepix is a\nminiaturised, hybrid semiconductor pixel detector capable of high resolution\nspectrometric tracking, enabling wide-range detection of the deposited energy,\nposition and direction of single particles. Experimental measurements were\nperformed at a clinical facility, the Clatterbridge Cancer Centre which houses\na 60 MeV ocular proton therapy beamline. A realistic end-to-end model of the\nfacility was developed in the Monte Carlo code TOPAS (TOol for PArticle\nSimulation) and was used to simulate the experimental conditions. The detector\nwas held at 45$^{\\circ}$ and 60$^{\\circ}$ perpendicular to the beam, and placed\ndownstream of various thickness Polymethyl methacrylate (PMMA) blocks to\nacquire data along the dose deposition depth. Empirical cluster data providing\ntrack length and the energy deposition distributions were used to obtain the\nLET spectra. The determined values for the LET in silicon and dose averaged LET\nacross the BP show general agreement with simulated results, supporting the\napplicability of the TOPAS CCC model. This work explores the capability of the\nMiniPIX detector to measure physical quantities to resolve the LET, and\ndiscusses experimental considerations and further possibilities.",
        "The first discovered interstellar small object, `Oumuamua (1I\/2017 U1),\npresents unique physical properties of extremely elongated geometric shape and\ndual characteristics of an asteroid and a comet. These properties suggest a\npossible origin through tidal fragmentation, which posits that `Oumuamua was\nproduced through intensive tidal fragmentation during a close encounter with a\nstar or a white dwarf, resulting in its shape and ejection from its natal\nsystem. According to this mechanism, a high initial orbit eccentricity and a\nsmall pericentre of the parent body are necessary to produce `Oumuamua-like\nobjects. To verify whether this mechanism can occur in single giant planet\nsystems, we conduct long-term numerical simulations of systems with a low-mass\n($0.5M_\\odot$) host star and a giant planet in this study. We determine that an\neccentric orbit ($e_\\mathrm{p}\\sim0.2$) and a Jupiter-mass ($M_\\mathrm{p}\\sim\nM_\\mathrm{J}$) of the planet appears to be optimal to generate sufficient\nperturbations for the production of `Oumuamua-like objects. When the planetary\nsemi-major axis $a_\\mathrm{p}$ increases, the proportion of planetesimals\nejected beyond the system $P(\\mathrm{ej})$ increases accordingly, while the\npossibilities of ejected planetesimals undergoing stellar tidal fragmentation\n$P(\\mathrm{tidal}|\\mathrm{ej})$ remains relatively constant at $\\sim0.6\\%$.\nFocusing on stellar tidal fragmentation alone, the ratio of extremely elongated\ninterstellar objects to all interstellar objects is $P_\\mathrm{e}\\sim3\\%$."
      ]
    }
  },
  {
    "id":2411.12897,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Morphological transformation and spatial-logical aggregation for tree species classification using hyperspectral imagery",
    "start_abstract":"Hyperspectral image (HSI) consists of abundant spectral and spatial characteristics, which contribute to a more accurate identification of materials and land covers. However, most existing methods of hyperspectral image analysis primarily focus on spectral knowledge or coarse-grained spatial information while neglecting the fine-grained morphological structures. In the classification task of complex objects, spatial morphological differences can help to search for the boundary of fine-grained classes, e.g., forestry tree species. Focusing on subtle traits extraction, a spatial-logical aggregation network (SLA-NET) is proposed with morphological transformation for tree species classification. The morphological operators are effectively embedded with the trainable structuring elements, which contributes to distinctive morphological representations. We evaluate the classification performance of the proposed method on two tree species datasets, and the results demonstrate that the proposed SLA-NET significantly outperforms the other state-of-the-art classifiers.",
    "start_categories":[
      "Evolutionary Biology"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Mapping Tree Species Using Advanced Remote Sensing Technologies: A State-of-the-Art Review and Perspective"
      ],
      "abstract":[
        "Timely and accurate information on tree species (TS) is crucial for developing strategies sustainable management conservation of artificial natural forests. Over the last four decades, advances in remote sensing technologies have made TS classification possible. Since many studies topic been conducted their comprehensive results novel findings published literature, it necessary to conduct an updated review status, trends, potentials, challenges recommend future directions. The will provide overview various optical light detection ranging (LiDAR) sensors; present assess current techniques\/methods for, a general trend method development in, classification; identify limitations In this review, several concluding remarks were made. They include following: (1) A large group using high-resolution satellite, airborne multi-\/hyperspectral imagery, LiDAR data. (2) \u201cmultiple\u201d was observed. (3) Machine learning methods including deep models demonstrated be significant improving accuracy. (4) Recently, unmanned aerial vehicle- (UAV-) based sensors caught interest researchers practitioners topic-related research applications. addition, three directions recommended, refining categories methods, data fusion algorithms or processing chains, exploring new spectral unmixing automatically extract map from satellite hyperspectral"
      ],
      "categories":[
        "Remote Sensing Technologies"
      ]
    },
    "list":{
      "title":[
        "Runge type approximation results for spaces of smooth Whitney jets",
        "Design Considerations in Offline Preference-based RL",
        "Increasing the p-Selmer rank by twisting",
        "Copilot Arena: A Platform for Code LLM Evaluation in the Wild",
        "Demystifying integrable QFTs in AdS: No-go theorems for higher-spin\n  charges",
        "4bit-Quantization in Vector-Embedding for RAG",
        "ViT-2SPN: Vision Transformer-based Dual-Stream Self-Supervised\n  Pretraining Networks for Retinal OCT Classification",
        "Impact of Electric Spatially Discordant Alternans on Cardiac Magnetic\n  Field",
        "Supersymmetric Higher-Spin Gauge Theories in any $d$ and their Coupling\n  Constants within BRST Formalism",
        "How Collective Intelligence Emerges in a Crowd of People Through Learned\n  Division of Labor: A Case Study",
        "A real-time battle situation intelligent awareness system based on\n  Meta-learning & RNN",
        "Nonlinear optical metasurfaces empowered by bound-states in the\n  continuum",
        "Parametric Hypersensitivity and Transport in the Steady-State\n  Open-System Holstein Model",
        "GeoWarp: Warped spatial processes for inferring subsea sediment\n  properties",
        "On Fairness of Unified Multimodal Large Language Model for Image\n  Generation",
        "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances\n  Rare Disease Diagnosis from Clinical Notes",
        "Transparent Decompilation for Timing Side-Channel Analyses",
        "GraspCorrect: Robotic Grasp Correction via Vision-Language Model-Guided\n  Feedback",
        "Improved estimates of statistical properties in some non-uniformly\n  hyperbolic dynamical systems",
        "Diffusion Restoration Adapter for Real-World Image Restoration",
        "Grokking Explained: A Statistical Phenomenon",
        "Power Ramp-Rate Control via Power Regulation for Storageless\n  Grid-Connected Photovoltaic Systems",
        "Adaptive Grasping of Moving Objects in Dense Clutter via Global-to-Local\n  Detection and Static-to-Dynamic Planning",
        "On 1-regular and 1-uniform metric measure spaces",
        "Measuring ultrafast laser pulses using a single-shot amplitude swing\n  implementation",
        "Provable Zero-Shot Generalization in Offline Reinforcement Learning",
        "Plan-over-Graph: Towards Parallelable LLM Agent Schedule",
        "LET measurements and simulation modelling of the charged particle field\n  for the Clatterbridge ocular proton therapy beamline",
        "Configuration of Single Giant Planet Systems Generating `Oumuamua-Like\n  Interstellar Asteroids"
      ],
      "abstract":[
        "We prove Runge type approximation results for linear partial differential\noperators with constant coefficients on spaces of smooth Whitney jets. Among\nothers, we characterize when for a constant coefficient linear partial\ndifferential operator $P(D)$ and for closed subsets $F_1\\subset F_2$ of\n$\\mathbb{R}^d$ the restrictions to $F_1$ of smooth Whitney jets $f$ on $F_2$\nsatisfying $P(D)f=0$ on $F_2$ are dense in the space of smooth Whitney jets on\n$F_1$ satisfying the same partial differential equation on $F_1$. For elliptic\noperators we give a geometric evaluation of this characterization.\nAdditionally, for differential operators with a single characteristic\ndirection, like parabolic operators, we give a sufficient geometric condition\nfor the above density to hold. Under mild additional assumptions on $\\partial\nF_1$ and for $F_2=\\mathbb{R}^d$ this sufficient conditions is also necessary.\nAs an application of our work, we characterize those open subsets $\\Omega$ of\nthe complex plane satisfying $\\Omega=\\operatorname{int}\\overline{\\Omega}$ for\nwhich the set of holomorphic polynomials are dense in $A^\\infty(\\Omega)$, under\nthe mild additional hypothesis that $\\overline{\\Omega}$ satisfies the strong\nregularity condition. Furthermore, for the wave operator in one spatial\nvariable, a simple sufficient geometric condition on $F_1,\nF_2\\subset\\mathbb{R}^2$ is given for the above density to hold. For the special\ncase of $F_2=\\mathbb{R}^2$ this sufficient condition is also necessary under\nmild additional hypotheses on $F_1$.",
        "Offline algorithms for Reinforcement Learning from Human Preferences (RLHF),\nwhich use only a fixed dataset of sampled responses given an input, and\npreference feedback among these responses, have gained increasing prominence in\nthe literature on aligning language models. In this paper, we study how the\ndifferent design choices made in methods such as DPO, IPO, SLiC and many\nvariants influence the quality of the learned policy, from a theoretical\nperspective. Our treatment yields insights into the choices of loss function,\nthe policy which is used to normalize log-likelihoods, and also the role of the\ndata sampling policy. Notably, our results do not rely on the standard\nreparameterization-style arguments used to motivate some of the algorithms in\nthis family, which allows us to give a unified treatment to a broad class of\nmethods. We also conduct a small empirical study to verify some of the\ntheoretical findings on a standard summarization benchmark.",
        "In this paper, we study the $p$-Selmer groups in the family of $p$-twists of\nan elliptic curve $E$ over a number field $K$. We prove that if $E\/K$ is an\nelliptic curve over a number field $K$, and if $d$ is congruent to the\ndimension of the Selmer group of $E\/K$ modulo $2$ and is greater than that\ndimension, then there exist infinitely many characters $\\chi \\in\n\\text{Hom}(G_K, \\mu_p)$ such that $\\text{dim}_{\\mathbb{F}_p}(\\text{Sel}_p(E\/K,\n\\chi)) = d$ under certain conditions.",
        "Evaluating in-the-wild coding capabilities of large language models (LLMs) is\na challenging endeavor with no clear solution. We introduce Copilot Arena, a\nplatform to collect user preferences for code generation through native\nintegration into a developer's working environment. Copilot Arena comprises a\nnovel interface for comparing pairs of model outputs, a sampling strategy\noptimized to reduce latency, and a prompting scheme to enable code completion\nfunctionality. Copilot Arena has served over 4.5 million suggestions from 10\nmodels and collected over 11k pairwise judgements. Our results highlight the\nimportance of model evaluations in integrated settings. We find that model\nrankings from Copilot Arena differ from those of existing evaluations, which we\nattribute to the more realistic distribution of data and tasks contained in\nCopilot Arena. We also identify novel insights into human preferences on code\nsuch as an observed consistency in user preference across programming languages\nyet significant variation in preference due to task category. We open-source\nCopilot Arena and release data to enable human-centric evaluations and improve\nunderstanding of coding assistants.",
        "Higher-spin conserved currents and charges feature prominently in integrable\n2d QFTs in flat space. Motivated by the question of integrable field theories\nin AdS space, we consider the consequences of higher-spin currents for QFTs in\nAdS$_2$, and find that their effect is much more constraining than in flat\nspace. Specifically, it is impossible to preserve: (a)~any higher-spin charges\nwhen deforming a massive free field by interactions, or (b)~any spin-4 charges\nwhen deforming a CFT by a Virasoro primary. Therefore, in these settings, there\nare no integrable theories in AdS with higher-spin conserved charges. Along the\nway, we explain how higher-spin charges lead to integer spacing in the spectrum\nof primaries, sum rules on the OPE data, and constraints on correlation\nfunctions. We also explain a key difference between AdS and flat space: in AdS\none cannot `partially' conserve a higher-spin current along particular\ndirections, since the AdS isometries imply full conservation.",
        "Retrieval-augmented generation (RAG) is a promising technique that has shown\ngreat potential in addressing some of the limitations of large language models\n(LLMs). LLMs have two major limitations: they can contain outdated information\ndue to their training data, and they can generate factually inaccurate\nresponses, a phenomenon known as hallucinations. RAG aims to mitigate these\nissues by leveraging a database of relevant documents, which are stored as\nembedding vectors in a high-dimensional space. However, one of the challenges\nof using high-dimensional embeddings is that they require a significant amount\nof memory to store. This can be a major issue, especially when dealing with\nlarge databases of documents. To alleviate this problem, we propose the use of\n4-bit quantization to store the embedding vectors. This involves reducing the\nprecision of the vectors from 32-bit floating-point numbers to 4-bit integers,\nwhich can significantly reduce the memory requirements. Our approach has\nseveral benefits. Firstly, it significantly reduces the memory storage\nrequirements of the high-dimensional vector database, making it more feasible\nto deploy RAG systems in resource-constrained environments. Secondly, it speeds\nup the searching process, as the reduced precision of the vectors allows for\nfaster computation. Our code is available at\nhttps:\/\/github.com\/taeheej\/4bit-Quantization-in-Vector-Embedding-for-RAG",
        "Optical Coherence Tomography (OCT) is a non-invasive imaging modality\nessential for diagnosing various eye diseases. Despite its clinical\nsignificance, developing OCT-based diagnostic tools faces challenges, such as\nlimited public datasets, sparse annotations, and privacy concerns. Although\ndeep learning has made progress in automating OCT analysis, these challenges\nremain unresolved. To address these limitations, we introduce the Vision\nTransformer-based Dual-Stream Self-Supervised Pretraining Network (ViT-2SPN), a\nnovel framework designed to enhance feature extraction and improve diagnostic\naccuracy. ViT-2SPN employs a three-stage workflow: Supervised Pretraining,\nSelf-Supervised Pretraining (SSP), and Supervised Fine-Tuning. The pretraining\nphase leverages the OCTMNIST dataset (97,477 unlabeled images across four\ndisease classes) with data augmentation to create dual-augmented views. A\nVision Transformer (ViT-Base) backbone extracts features, while a negative\ncosine similarity loss aligns feature representations. Pretraining is conducted\nover 50 epochs with a learning rate of 0.0001 and momentum of 0.999.\nFine-tuning is performed on a stratified 5.129% subset of OCTMNIST using\n10-fold cross-validation. ViT-2SPN achieves a mean AUC of 0.93, accuracy of\n0.77, precision of 0.81, recall of 0.75, and an F1 score of 0.76, outperforming\nexisting SSP-based methods.",
        "Spatially discordant alternans (SDA) play a crucial role in cardiac\narrhythmogenesis by creating steep repolarization gradients facilitating\nconduction block and reentry. While traditionally studied using electrical\nindicators, this work provides a novel perspective by characterizing SDA\nthrough their magnetic field signatures. Using a one-dimensional cardiac fiber\nmodel, we demonstrate that magnetic field measurements effectively detect SDA\nand temperature dependent changes in cardiac action potentials, offering a\nnon-invasive alternative to conventional electrophysiological metrics. Our\nresults reveal that the spatial organization of SDA is mirrored in the magnetic\nfield distribution, with SDA nodes clearly identifiable via spatial mapping.\nNotably, magnetic restitution curves exhibit a distinct pattern from APD-based\nindicators, closely following the dynamics of the action potential upstroke.\nThese findings establish the cardiac magnetic field as a powerful diagnostic\ntool for detecting SDA, opening new avenues for biomagnetic monitoring of\narrhythmic risk.",
        "Nonlinear field equations for the supersymmetric higher-spin gauge theory\ndescribing totally symmetric bosonic and fermionic massless fields along with\nhook-type bosonic fields of all spins in any space-time dimension are\npresented. One of the novel features of the proposed formalism is that the\n$osp(1,2)$ invariance and factorisation conditions are formulated within the\nBRST formalism, that greatly simplifies the form of nonlinear HS equations. To\nmatch the list of vertices found by Metsaev, higher-spin gauge theory is\nanticipated to possess an infinite number of independent coupling constants. A\nconjecture that these coupling constants result from the locality restrictions\non the elements of the factorisation ideal is put forward.",
        "This paper investigates the factors fostering collective intelligence (CI)\nthrough a case study of *LinYi's Experiment, where over 2000 human players\ncollectively controll an avatar car. By conducting theoretical analysis and\nreplicating observed behaviors through numerical simulations, we demonstrate\nhow self-organized division of labor (DOL) among individuals fosters the\nemergence of CI and identify two essential conditions fostering CI by\nformulating this problem into a stability problem of a Markov Jump Linear\nSystem (MJLS). These conditions, independent of external stimulus, emphasize\nthe importance of both elite and common players in fostering CI. Additionally,\nwe propose an index for emergence of CI and a distributed method for estimating\njoint actions, enabling individuals to learn their optimal social roles without\nglobal action information of the whole crowd.",
        "In modern warfare, real-time and accurate battle situation analysis is\ncrucial for making strategic and tactical decisions. The proposed real-time\nbattle situation intelligent awareness system (BSIAS) aims at meta-learning\nanalysis and stepwise RNN (recurrent neural network) modeling, where the former\ncarries out the basic processing and analysis of battlefield data, which\nincludes multi-steps such as data cleansing, data fusion, data mining and\ncontinuously updates, and the latter optimizes the battlefield modeling by\nstepwise capturing the temporal dependencies of data set. BSIAS can predict the\npossible movement from any side of the fence and attack routes by taking a\nsimulated battle as an example, which can be an intelligent support platform\nfor commanders to make scientific decisions during wartime. This work delivers\nthe potential application of integrated BSIAS in the field of battlefield\ncommand & analysis engineering.",
        "Optical bound-states in the continuum (BICs) have greatly enriched the field\nof nonlinear optics with novel ways to control and manipulate light-matter\ninteraction at the nanoscale. This has been made possible by their unique\nphysical properties, including effective confinement of light, non-trivial\ntopological features, and robustness upon the propagation of the optical field\nboth in the real and momentum space. Regarding the exploration of nonlinear\noptical response in various photonic nanostructures supporting BICs, particular\nattention has been paid to optical metasurfaces, chiefly due to their ability\nto control the light flow at subwavelength scale, design and fabrication\nflexibility, and convenient phase-matching conditions. In this review, we\noutline and discuss recent advances in metasurface-based frequency conversion\nprocesses utilizing the versatile physics of BICs, with a particular emphasis\non the main physics background pertaining to nonlinear optical phenomena and\noptics of BICs, as well as state-of-the-art functionalities enabled by\nBIC-driven nonlinear metasurfaces. These applications include harmonic\ngeneration, harmonic chiroptical effects, generation of complex quantum states,\nand broadband terahertz generation. In addition, several emerging research\nfields and the existing challenges of photonic nanodevices relying on BICs are\ndiscussed.",
        "We demonstrate that the nonequilibrium steady state (NESS) of an open-system\nHolstein model with linear bias displays extreme sensitivity to the closed\nsystem parameters. This sensitivity is shown to correspond to avoided crossings\nin the closed system spectrum, as previously demonstrated in the Rabi model. We\nthen develop a kinetic model to analyze the effects of environmental parameters\non NESS hypersensitivity. This reveals that hypersensitivity only exists in\nintermediate environmental parameter regimes, a prediction that is verified\nnumerically. The inherent spatial character of the Holstein model offers a\nnatural connection to transport, revealing that transport properties in the\nsteady-state regime can be optimized by simultaneously coordinating the closed-\nand open-system parameters.",
        "For offshore structures like wind turbines, subsea infrastructure, pipelines,\nand cables, it is crucial to quantify the properties of the seabed sediments at\na proposed site. However, data collection offshore is costly, so analysis of\nthe seabed sediments must be made from measurements that are spatially sparse.\nAdding to this challenge, the structure of the seabed sediments exhibits both\nnonstationarity and anisotropy. To address these issues, we propose GeoWarp, a\nhierarchical spatial statistical modeling framework for inferring the 3-D\ngeotechnical properties of subsea sediments. GeoWarp decomposes the seabed\nproperties into a region-wide vertical mean profile (modeled using B-splines),\nand a nonstationary 3-D spatial Gaussian process. Process nonstationarity and\nanisotropy are accommodated by warping space in three dimensions and by\nallowing the process variance to change with depth. We apply GeoWarp to\nmeasurements of the seabed made using cone penetrometer tests (CPTs) at six\nsites on the North West Shelf of Australia. We show that GeoWarp captures the\ncomplex spatial distribution of the sediment properties, and produces realistic\n3-D simulations suitable for downstream engineering analyses. Through\ncross-validation, we show that GeoWarp has predictive performance superior to\nother state-of-the-art methods, demonstrating its value as a tool in offshore\ngeotechnical engineering.",
        "Unified multimodal large language models (U-MLLMs) have demonstrated\nimpressive performance in visual understanding and generation in an end-to-end\npipeline. Compared with generation-only models (e.g., Stable Diffusion),\nU-MLLMs may raise new questions about bias in their outputs, which can be\naffected by their unified capabilities. This gap is particularly concerning\ngiven the under-explored risk of propagating harmful stereotypes. In this\npaper, we benchmark the latest U-MLLMs and find that most exhibit significant\ndemographic biases, such as gender and race bias. To better understand and\nmitigate this issue, we propose a locate-then-fix strategy, where we audit and\nshow how the individual model component is affected by bias. Our analysis shows\nthat bias originates primarily from the language model. More interestingly, we\nobserve a \"partial alignment\" phenomenon in U-MLLMs, where understanding bias\nappears minimal, but generation bias remains substantial. Thus, we propose a\nnovel balanced preference model to balance the demographic distribution with\nsynthetic data. Experiments demonstrate that our approach reduces demographic\nbias while preserving semantic fidelity. We hope our findings underscore the\nneed for more holistic interpretation and debiasing strategies of U-MLLMs in\nthe future.",
        "Background: Several studies show that large language models (LLMs) struggle\nwith phenotype-driven gene prioritization for rare diseases. These studies\ntypically use Human Phenotype Ontology (HPO) terms to prompt foundation models\nlike GPT and LLaMA to predict candidate genes. However, in real-world settings,\nfoundation models are not optimized for domain-specific tasks like clinical\ndiagnosis, yet inputs are unstructured clinical notes rather than standardized\nterms. How LLMs can be instructed to predict candidate genes or disease\ndiagnosis from unstructured clinical notes remains a major challenge. Methods:\nWe introduce RAG-driven CoT and CoT-driven RAG, two methods that combine\nChain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze\nclinical notes. A five-question CoT protocol mimics expert reasoning, while RAG\nretrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in\nMan). We evaluated these approaches on rare disease datasets, including 5,980\nPhenopacket-derived notes, 255 literature-based narratives, and 220 in-house\nclinical notes from Childrens Hospital of Philadelphia. Results: We found that\nrecent foundations models, including Llama 3.3-70B-Instruct and\nDeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2\nand GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both\noutperform foundation models in candidate gene prioritization from clinical\nnotes; in particular, both methods with DeepSeek backbone resulted in a top-10\ngene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT\nworks better for high-quality notes, where early retrieval can anchor the\nsubsequent reasoning steps in domain-specific evidence, while CoT-driven RAG\nhas advantage when processing lengthy and noisy notes.",
        "This paper considers the problem of analyzing the timing side-channel\nsecurity of binary programs through decompilation and source-level analysis. We\nfocus on two popular policies, namely constant-time and speculative\nconstant-time, (S)CT for short, used to protect cryptographic libraries.\n  First, we observe that popular decompilers remove (S)CT violations, i.e.,\ntransform non-(S)CT programs into (S)CT programs; it follows that analyzing\ndecompiled programs is not sound. Second, we develop techniques to prove that\ndecompilers are transparent, i.e., neither introduce nor remove (S)CT\nviolations. Third, we apply our techniques to \\refleCT{}, a core but\nnon-trivial decompiler. As a contribution of independent interest, we find that\nconstant-time verification tools may not be sound, due to their use of\npreprocessors (e.g.\\, binary lifters or IR converters) that eliminate CT\nviolations.",
        "Despite significant advancements in robotic manipulation, achieving\nconsistent and stable grasping remains a fundamental challenge, often limiting\nthe successful execution of complex tasks. Our analysis reveals that even\nstate-of-the-art policy models frequently exhibit unstable grasping behaviors,\nleading to failure cases that create bottlenecks in real-world robotic\napplications. To address these challenges, we introduce GraspCorrect, a\nplug-and-play module designed to enhance grasp performance through\nvision-language model-guided feedback. GraspCorrect employs an iterative visual\nquestion-answering framework with two key components: grasp-guided prompting,\nwhich incorporates task-specific constraints, and object-aware sampling, which\nensures the selection of physically feasible grasp candidates. By iteratively\ngenerating intermediate visual goals and translating them into joint-level\nactions, GraspCorrect significantly improves grasp stability and consistently\nenhances task success rates across existing policy models in the RLBench and\nCALVIN datasets.",
        "Building upon previous works by Young, Chernov-Zhang and\nBruin-Melbourne-Terhesiu, we present a general scheme to improve bounds on the\nstatistical properties (in particular, decay of correlations, and rates in the\nalmost sure invariant principle) for a class of non-uniformly hyperbolic\ndynamical systems. Specifically, for systems with polynomial, yet summable\nmixing rates, our method removes logarithmic factors of earlier arguments,\nresulting in essentially optimal bounds. Applications include Wojtkowski's\nsystem of two falling balls, dispersing billiards with flat points and\nBunimovich's flower-shaped billiard tables.",
        "Diffusion models have demonstrated their powerful image generation\ncapabilities, effectively fitting highly complex image distributions. These\nmodels can serve as strong priors for image restoration. Existing methods often\nutilize techniques like ControlNet to sample high quality images with low\nquality images from these priors. However, ControlNet typically involves\ncopying a large part of the original network, resulting in a significantly\nlarge number of parameters as the prior scales up. In this paper, we propose a\nrelatively lightweight Adapter that leverages the powerful generative\ncapabilities of pretrained priors to achieve photo-realistic image restoration.\nThe Adapters can be adapt to both denoising UNet and DiT, and performs\nexcellent.",
        "Grokking, or delayed generalization, is an intriguing learning phenomenon\nwhere test set loss decreases sharply only after a model's training set loss\nhas converged. This challenges conventional understanding of the training\ndynamics in deep learning networks. In this paper, we formalize and investigate\ngrokking, highlighting that a key factor in its emergence is a distribution\nshift between training and test data. We introduce two synthetic datasets\nspecifically designed to analyze grokking. One dataset examines the impact of\nlimited sampling, and the other investigates transfer learning's role in\ngrokking. By inducing distribution shifts through controlled imbalanced\nsampling of sub-categories, we systematically reproduce the phenomenon,\ndemonstrating that while small-sampling is strongly associated with grokking,\nit is not its cause. Instead, small-sampling serves as a convenient mechanism\nfor achieving the necessary distribution shift. We also show that when classes\nform an equivariant map, grokking can be explained by the model's ability to\nlearn from similar classes or sub-categories. Unlike earlier work suggesting\nthat grokking primarily arises from high regularization and sparse data, we\ndemonstrate that it can also occur with dense data and minimal hyper-parameter\ntuning. Our findings deepen the understanding of grokking and pave the way for\ndeveloping better stopping criteria in future training processes.",
        "Photovoltaic Power Ramp-Rate Control (PRRC) constitutes a key ancillary\nservice for future power systems. Although its implementation through the\ninstallation of storage systems or irradiance sensors has been widely\ninvestigated, fewer studies have explored the power curtailment approach. The\nlatter lacks efficiency, as it voluntarily produces power discharges, yet it is\na cost-effective solution in terms of capital expenditures. This paper proposes\na novel storageless and sensorless photovoltaic PRRC for grid-connected\napplications in which the photovoltaic power, rather than the voltage, is the\ncontrolled magnitude. The aforementioned contribution makes the effective\ntracking of the power ramp-rate limit possible compared to the existing methods\nin the literature. The method is assisted by a real-time curve-fitting\nalgorithm that estimates the Maximum Power Point while operating suboptimally.\nThus, no direct temperature or irradiance measurement systems are needed. The\nvalidation of the proposed PRRC strategy has been tested by simulation and\ncompared to another approach available in the literature, considering\nreal-field highly variable irradiance data. Experimental validation of the\nproposed strategy has been performed in real time via Controller\nHardware-in-the-Loop.",
        "Robotic grasping is facing a variety of real-world uncertainties caused by\nnon-static object states, unknown object properties, and cluttered object\narrangements. The difficulty of grasping increases with the presence of more\nuncertainties, where commonly used learning-based approaches struggle to\nperform consistently across varying conditions. In this study, we integrate the\nidea of similarity matching to tackle the challenge of grasping novel objects\nthat are simultaneously in motion and densely cluttered using a single RGBD\ncamera, where multiple uncertainties coexist. We achieve this by shifting\nvisual detection from global to local states and operating grasp planning from\nstatic to dynamic scenes. Notably, we introduce optimization methods to enhance\nplanning efficiency for this time-sensitive task. Our proposed system can adapt\nto various object types, arrangements and movement speeds without the need for\nextensive training, as demonstrated by real-world experiments. Videos are\navailable at https:\/\/youtu.be\/sdC50dx-xp8?si=27oVr4dhG0rqN_tT.",
        "A metric measure space $(X,\\mu)$ is 1-regular if \\[0< \\lim_{r\\to 0}\n\\frac{\\mu(B(x,r))}{r}<\\infty\\] for $\\mu$-a.e $x\\in X$. We give a complete\ngeometric characterisation of the rectifiable and purely unrectifiable part of\na 1-regular measure in terms of its tangent spaces.\n  A special instance of a 1-regular metric measure space is a 1-uniform space\n$(Y,\\nu)$, which satisfies $\\nu(B(y,r))=r$ for all $y\\in Y$ and $r>0$. We prove\nthat there are exactly three 1-uniform metric measure spaces.",
        "Single-shot characterization techniques are crucial when dealing with\nshot-to-shot pulse-shape fluctuations (e.g., unstable laser systems,\nhigh-power, or with low repetition rate) since the scanning configurations\ncannot measure single pulses. The demand for simple setups that can be easily\nadapted to a wide variety of experimental conditions is continuously rising. In\nthis work, we propose a single-shot implementation of amplitude swing,\nmaintaining the compactness, versatility, and robustness of the scanning\nversions of this technique. First, we theoretically study the proposed\nimplementation, based on a pair of uniaxial wedges. Then, we present the\nretrieval ptychographic algorithm. Finally, we experimentally demonstrate the\nsetup by comparing the single-shot and scanning traces and their retrieved\npulses. In sum, we provide the ultrafast science community with a simple and\nversatile setup capable of measuring single laser pulses, which is necessary\nfor characterizing fluctuating pulse trains, meeting the current increasing\ndemand.",
        "In this work, we study offline reinforcement learning (RL) with zero-shot\ngeneralization property (ZSG), where the agent has access to an offline dataset\nincluding experiences from different environments, and the goal of the agent is\nto train a policy over the training environments which performs well on test\nenvironments without further interaction. Existing work showed that classical\noffline RL fails to generalize to new, unseen environments. We propose\npessimistic empirical risk minimization (PERM) and pessimistic proximal policy\noptimization (PPPO), which leverage pessimistic policy evaluation to guide\npolicy learning and enhance generalization. We show that both PERM and PPPO are\ncapable of finding a near-optimal policy with ZSG. Our result serves as a first\nstep in understanding the foundation of the generalization phenomenon in\noffline reinforcement learning.",
        "Large Language Models (LLMs) have demonstrated exceptional abilities in\nreasoning for task planning. However, challenges remain under-explored for\nparallel schedules. This paper introduces a novel paradigm, plan-over-graph, in\nwhich the model first decomposes a real-life textual task into executable\nsubtasks and constructs an abstract task graph. The model then understands this\ntask graph as input and generates a plan for parallel execution. To enhance the\nplanning capability of complex, scalable graphs, we design an automated and\ncontrollable pipeline to generate synthetic graphs and propose a two-stage\ntraining scheme. Experimental results show that our plan-over-graph method\nsignificantly improves task performance on both API-based LLMs and trainable\nopen-sourced LLMs. By normalizing complex tasks as graphs, our method naturally\nsupports parallel execution, demonstrating global efficiency. The code and data\nare available at https:\/\/github.com\/zsq259\/Plan-over-Graph.",
        "Proton therapy can achieve a highly targeted treatment by utilising the\nadvantageous dosimetric characteristics of the Bragg Peak. Protons traversing\nthrough a material will deposit their maximum energy at the Bragg Peak through\nionisation and other interactions, transferring minimal excess dose to\nsurrounding tissue and organs. This rate of energy loss is also quantified by\nthe linear energy transfer (LET), which is indicative of radiation quality and\nradiobiological effects. However it is a challenging physical quantity to\nmeasure, as characterisation of radiation fields and the impact of LET on\ntreatment requires advanced tools and technology. The MiniPIX-Timepix is a\nminiaturised, hybrid semiconductor pixel detector capable of high resolution\nspectrometric tracking, enabling wide-range detection of the deposited energy,\nposition and direction of single particles. Experimental measurements were\nperformed at a clinical facility, the Clatterbridge Cancer Centre which houses\na 60 MeV ocular proton therapy beamline. A realistic end-to-end model of the\nfacility was developed in the Monte Carlo code TOPAS (TOol for PArticle\nSimulation) and was used to simulate the experimental conditions. The detector\nwas held at 45$^{\\circ}$ and 60$^{\\circ}$ perpendicular to the beam, and placed\ndownstream of various thickness Polymethyl methacrylate (PMMA) blocks to\nacquire data along the dose deposition depth. Empirical cluster data providing\ntrack length and the energy deposition distributions were used to obtain the\nLET spectra. The determined values for the LET in silicon and dose averaged LET\nacross the BP show general agreement with simulated results, supporting the\napplicability of the TOPAS CCC model. This work explores the capability of the\nMiniPIX detector to measure physical quantities to resolve the LET, and\ndiscusses experimental considerations and further possibilities.",
        "The first discovered interstellar small object, `Oumuamua (1I\/2017 U1),\npresents unique physical properties of extremely elongated geometric shape and\ndual characteristics of an asteroid and a comet. These properties suggest a\npossible origin through tidal fragmentation, which posits that `Oumuamua was\nproduced through intensive tidal fragmentation during a close encounter with a\nstar or a white dwarf, resulting in its shape and ejection from its natal\nsystem. According to this mechanism, a high initial orbit eccentricity and a\nsmall pericentre of the parent body are necessary to produce `Oumuamua-like\nobjects. To verify whether this mechanism can occur in single giant planet\nsystems, we conduct long-term numerical simulations of systems with a low-mass\n($0.5M_\\odot$) host star and a giant planet in this study. We determine that an\neccentric orbit ($e_\\mathrm{p}\\sim0.2$) and a Jupiter-mass ($M_\\mathrm{p}\\sim\nM_\\mathrm{J}$) of the planet appears to be optimal to generate sufficient\nperturbations for the production of `Oumuamua-like objects. When the planetary\nsemi-major axis $a_\\mathrm{p}$ increases, the proportion of planetesimals\nejected beyond the system $P(\\mathrm{ej})$ increases accordingly, while the\npossibilities of ejected planetesimals undergoing stellar tidal fragmentation\n$P(\\mathrm{tidal}|\\mathrm{ej})$ remains relatively constant at $\\sim0.6\\%$.\nFocusing on stellar tidal fragmentation alone, the ratio of extremely elongated\ninterstellar objects to all interstellar objects is $P_\\mathrm{e}\\sim3\\%$."
      ]
    }
  },
  {
    "id":2411.07018,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Field Emission in Superconducting Accelerators: Instrumented Measurements for Its Understanding and Mitigation",
    "start_abstract":"Several new accelerator projects are adopting superconducting RF (SRF) technology. When accelerating SRF cavities maintain high RF gradients, field emission, the emission of electrons from cavity walls, can occur and may impact operational cavity gradient, radiological environment via activated components, and reliability. In this talk, we will discuss instrumented measurements of field emission from the two 1.1 GeV superconducting continuous wave (CW) linacs in CEBAF. The goal is to improve the understanding of field emission sources originating from cryomodule production, installation and operation. Such basic knowledge is needed in guiding field emission control, mitigation, and reduction toward high gradient and reliable operation of superconducting accelerators.",
    "start_categories":[
      "physics.acc-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Accelerating cavity fault prediction using deep learning at Jefferson laboratory"
      ],
      "abstract":[
        "Abstract Accelerating cavities are an integral part of the continuous electron beam accelerator facility (CEBAF) at Jefferson Laboratory. When any over 400 in CEBAF experiences a fault, it disrupts delivery to experimental user halls. In this study, we propose use deep learning model predict slowly developing cavity faults. By utilizing pre-fault signals, train long short-term memory-convolutional neural network binary classifier distinguish between radio-frequency (RF) signals during normal operation and RF indicative impending We optimize by adjusting fault confidence threshold implementing multiple consecutive window criterion identify events, ensuring low false positive rate. Results obtained from analysis real dataset collected accelerating simulating deployed scenario demonstrate model\u2019s ability with 99.99% accuracy correctly 80% Notably, these achievements were achieved context highly imbalanced dataset, predictions made several hundred milliseconds before onset fault. Anticipating faults enables preemptive measures improve operational efficiency preventing or mitigating their occurrence."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense\n  Retrievers",
        "LexPro-1.0 Technical Report",
        "Observer-Based Data-Driven Consensus Control for Nonlinear Multi-Agent\n  Systems against DoS and FDI attacks",
        "Automated Extraction of Spatio-Semantic Graphs for Identifying Cognitive\n  Impairment",
        "Generative AI & Changing Work: Systematic Review of Practitioner-led\n  Work Transformations through the Lens of Job Crafting",
        "Learning Code-Edit Embedding to Model Student Debugging Behavior",
        "Monochromatic graph decompositions and monochromatic piercing inspired\n  by anti-Ramsey colorings",
        "Select2Drive: Pragmatic Communications for Real-Time Collaborative\n  Autonomous Driving",
        "Determination of the density in the linear elastic wave equation",
        "Semicustom Frontend VLSI Design and Analysis of a 32-bit Brent-Kung\n  Adder in Cadence Suite",
        "DiffCLIP: Differential Attention Meets CLIP",
        "Splitting algorithms for paraxial and It\\^o-Schr\\\"odinger models of wave\n  propagation in random media",
        "Numerical homological regularities over positively graded algebras",
        "Provable Benefits of Task-Specific Prompts for In-context Learning",
        "Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of\n  Frozen Language Models",
        "Bidirectionalization For The Common People",
        "Ambiguity Function Analysis and Optimization of Frequency-Hopping MIMO\n  Radar with Movable Antennas",
        "vS-Graphs: Integrating Visual SLAM and Situational Graphs through\n  Multi-level Scene Understanding",
        "Dynamic Noise Preference Optimization for LLM Self-Improvement via\n  Synthetic Data",
        "A Review on Geometry and Surface Inspection in 3D Concrete Printing",
        "Closing a Source Complexity Gap between Chapel and HPX",
        "Super-Linear Speedup by Generalizing Runtime Repeated Recursion\n  Unfolding in Prolog",
        "Notes on Khovanov homology",
        "ArtInsight: Enabling AI-Powered Artwork Engagement for Mixed\n  Visual-Ability Families",
        "Orbit recovery from invariants of low degree in representations of\n  finite groups",
        "A low-PAPR Pilot Design and Optimization for OTFS Modulation",
        "Guided SAM: Label-Efficient Part Segmentation",
        "The Building Blocks of Classical Nonparametric Two-Sample Testing\n  Procedures: Statistically Equivalent Blocks",
        "Anderson localized states for the nonlinear Maryland model on\n  $\\mathbb{Z}^d$"
      ],
      "abstract":[
        "Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization.",
        "In this report, we introduce our first-generation reasoning model,\nLexPro-1.0, a large language model designed for the highly specialized Chinese\nlegal domain, offering comprehensive capabilities to meet diverse realistic\nneeds. Existing legal LLMs face two primary challenges. Firstly, their design\nand evaluation are predominantly driven by computer science perspectives,\nleading to insufficient incorporation of legal expertise and logic, which is\ncrucial for high-precision legal applications, such as handling complex\nprosecutorial tasks. Secondly, these models often underperform due to a lack of\ncomprehensive training data from the legal domain, limiting their ability to\neffectively address real-world legal scenarios. To address this, we first\ncompile millions of legal documents covering over 20 types of crimes from 31\nprovinces in China for model training. From the extensive dataset, we further\nselect high-quality for supervised fine-tuning, ensuring enhanced relevance and\nprecision. The model further undergoes large-scale reinforcement learning\nwithout additional supervision, emphasizing the enhancement of its reasoning\ncapabilities and explainability. To validate its effectiveness in complex legal\napplications, we also conduct human evaluations with legal experts. We develop\nfine-tuned models based on DeepSeek-R1-Distilled versions, available in three\ndense configurations: 14B, 32B, and 70B.",
        "Existing data-driven control methods generally do not address False Data\nInjection (FDI) and Denial-of-Service (DoS) attacks simultaneously. This letter\nintroduces a distributed data-driven attack-resilient consensus problem under\nboth FDI and DoS attacks and proposes a data-driven consensus control\nframework, consisting of a group of comprehensive attack-resilient observers.\nThe proposed group of observers is designed to estimate FDI attacks, external\ndisturbances, and lumped disturbances, combined with a DoS attack compensation\nmechanism. A rigorous stability analysis of the approach is provided to ensure\nthe boundedness of the distributed neighborhood estimation consensus error. The\neffectiveness of the approach is validated through numerical examples involving\nboth leaderless consensus and leader-follower consensus, demonstrating\nsignificantly improved resilient performance compared to existing data-driven\ncontrol approaches.",
        "Existing methods for analyzing linguistic content from picture descriptions\nfor assessment of cognitive-linguistic impairment often overlook the\nparticipant's visual narrative path, which typically requires eye tracking to\nassess. Spatio-semantic graphs are a useful tool for analyzing this narrative\npath from transcripts alone, however they are limited by the need for manual\ntagging of content information units (CIUs). In this paper, we propose an\nautomated approach for estimation of spatio-semantic graphs (via automated\nextraction of CIUs) from the Cookie Theft picture commonly used in\ncognitive-linguistic analyses. The method enables the automatic\ncharacterization of the visual semantic path during picture description.\nExperiments demonstrate that the automatic spatio-semantic graphs effectively\ndifferentiate between cognitively impaired and unimpaired speakers. Statistical\nanalyses reveal that the features derived by the automated method produce\ncomparable results to the manual method, with even greater group differences\nbetween clinical groups of interest. These results highlight the potential of\nthe automated approach for extracting spatio-semantic features in developing\nclinical speech models for cognitive impairment assessment.",
        "Widespread integration of Generative AI tools is transforming white-collar\nwork, reshaping how workers define their roles, manage their tasks, and\ncollaborate with peers. This has created a need to develop an overarching\nunderstanding of common worker-driven patterns around these transformations. To\nfill this gap, we conducted a systematic literature review of 23 studies from\nthe ACM Digital Library that focused on workers' lived-experiences and\npractitioners with GenAI. Our findings reveal that while many professionals\nhave delegated routine tasks to GenAI to focus on core responsibilities, they\nhave also taken on new forms of AI managerial labor to monitor and refine GenAI\noutputs. Additionally, practitioners have restructured collaborations,\nsometimes bypassing traditional peer and subordinate interactions in favor of\nGenAI assistance. These shifts have fragmented cohesive tasks into piecework\ncreating tensions around role boundaries and professional identity. Our\nanalysis suggests that current frameworks, like job crafting, need to evolve to\naddress the complexities of GenAI-driven transformations.",
        "Providing effective feedback for programming assignments in computer science\neducation can be challenging: students solve problems by iteratively submitting\ncode, executing it, and using limited feedback from the compiler or the\nauto-grader to debug. Analyzing student debugging behavior in this process may\nreveal important insights into their knowledge and inform better personalized\nsupport tools. In this work, we propose an encoder-decoder-based model that\nlearns meaningful code-edit embeddings between consecutive student code\nsubmissions, to capture their debugging behavior. Our model leverages\ninformation on whether a student code submission passes each test case to\nfine-tune large language models (LLMs) to learn code editing representations.\nIt enables personalized next-step code suggestions that maintain the student's\ncoding style while improving test case correctness. Our model also enables us\nto analyze student code-editing patterns to uncover common student errors and\ndebugging behaviors, using clustering techniques. Experimental results on a\nreal-world student code submission dataset demonstrate that our model excels at\ncode reconstruction and personalized code suggestion while revealing\ninteresting patterns in student debugging behavior.",
        "Anti-Ramsey theory was initiated in 1975 by Erd\\H{o}s, Simonovits and S\\'os,\ninspiring hundreds of publications since then. The present work is the third\nand last piece of our trilogy in which we introduce a far-reaching\ngeneralization via the following two functions for any graph $G$ and family\n${\\cal F}$ of graphs:\n  If $K_2 \\in {\\cal F}$, let $f(n,G|{\\cal F})$ be the smallest integer $k$ such\nthat every edge coloring of $K_n$ with at least $k$ colors forces a copy of $G$\nin which all color classes are members of ${\\cal F}$.\n  If $K_2 \\notin {\\cal F}$, let $g(n,G|{\\cal F})$ be the largest integer $k$\nfor which there exists an edge coloring of $K_n$ using exactly $k$ colors, such\nthat every copy of $G$ contains an induced color class which is a member of\n${\\cal F}$.\n  We develop methods suitable for deriving asymptotically tight results for the\n$f$-function and the $g$-function for many combinations of $G$ and ${\\cal F}$.\n  The preceding parts of the trilogy are arXiv: 2405.19812 and 2408.04257,\npublished in Discrete Applied Math. Vol. 363 and Mathematics Vol. 12:23,\nrespectively.",
        "Vehicle-to-Everything communications-assisted Autonomous Driving (V2X-AD) has\nwitnessed remarkable advancements in recent years, with pragmatic\ncommunications (PragComm) emerging as a promising paradigm for real-time\ncollaboration among vehicles and other agents.Simultaneously, extensive\nresearch has explored the interplay between collaborative perception and\ndecision-making in end-to-end driving frameworks.In this work, we revisit the\ncollaborative driving problem and propose the Select2Drive framework to\noptimize the utilization of limited computational and communication\nresources.Particularly, to mitigate cumulative latency in perception and\ndecision-making, Select2Drive introduces Distributed Predictive Perception\n(DPP) by formulating an active prediction paradigm and simplifies\nhigh-dimensional semantic feature prediction into computation cost-efficient,\nmotion-aware reconstruction. Given the \"less is more\" principle that a\nbroadened perceptual horizon possibly confuses the decision module rather than\ncontributing to it, Select2Drive utilizes Area-of-Importance-based PragComm\n(APC) to prioritize the communications of critical regions, thus boosting both\ncommunication efficiency and decision-making efficacy. Empirical evaluations on\nthe V2Xverse dataset and CARLA driving simulator demonstrate that Select2Drive\nachieves a 11.31% (resp. 7.69%) improvement in offline perception tasks under\nlimited bandwidth (resp. pose error conditions). Moreover, it delivers at most\n14.68% and 31.76% enhancement in closed-loop driving scores and route\ncompletion rates, particularly in scenarios characterized by dense traffic and\nhigh-speed dynamics.",
        "We study the inverse boundary value problem for the linear elastic wave\nequation in three-dimensional isotropic medium. We show that both the Lam\\'e\nparameters and the density can be uniquely recovered from the boundary\nmeasurements under the strictly convex foliation condition.",
        "Adders are fundamental components in digital circuits, playing a crucial role\nin arithmetic operations within computing systems and many other applications.\nThis paper focuses on the design and simulation of a 32-bit Brent-Kung parallel\nprefix adder, which is recognized for its efficient carry propagation and\nlogarithmic delay characteristics. The Brent-Kung architecture balances\ncomputational speed and hardware complexity, making it suitable for high-speed\ndigital applications. The design is implemented using Verilog HDL and simulated\nusing Cadence Design Suite tools, including NCLaunch and Genus, to evaluate its\nperformance in terms of scalability, speed, and functional working. Comparative\nanalysis with traditional adder architectures highlights the advantages of the\nBrent-Kung adder for modern digital systems.",
        "We propose DiffCLIP, a novel vision-language model that extends the\ndifferential attention mechanism to CLIP architectures. Differential attention\nwas originally developed for large language models to amplify relevant context\nwhile canceling out noisy information. In this work, we integrate this\nmechanism into CLIP's dual encoder (image and text) framework. With minimal\nadditional parameters, DiffCLIP achieves superior performance on image-text\nunderstanding tasks. Across zero-shot classification, retrieval, and robustness\nbenchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably,\nthese gains come with negligible computational overhead, demonstrating that\ndifferential attention can significantly enhance multi-modal representations\nwithout sacrificing efficiency. Code can be found at\nhttps:\/\/github.com\/hammoudhasan\/DiffCLIP.",
        "This paper introduces a full discretization procedure to solve wave beam\npropagation in random media modeled by a paraxial wave equation or an\nIt\\^o-Schr\\\"odinger stochastic partial differential equation. This method bears\nsimilarities with the phase screen method used routinely to solve such\nproblems. The main axis of propagation is discretized by a centered splitting\nscheme with step $\\Delta z$ while the transverse variables are treated by a\nspectral method after appropriate spatial truncation. The originality of our\napproach is its theoretical validity even when the typical wavelength $\\theta$\nof the propagating signal satisfies $\\theta\\ll\\Delta z$. More precisely, we\nobtain a convergence of order $\\Delta z$ in mean-square sense while the errors\non statistical moments are of order $(\\Delta z)^2$ as expected for standard\ncentered splitting schemes. This is a surprising result as splitting schemes\ntypically do not converge when $\\Delta z$ is not the smallest scale of the\nproblem. The analysis is based on equations satisfied by statistical moments in\nthe It\\^o-Schr\\\"odinger case and on integral (Duhamel) expansions for the\nparaxial model. Several numerical simulations illustrate and confirm the\ntheoretical findings.",
        "We study numerical regularities for complexes over noncommutative noetherian\nlocally finite $\\mathbb{N}$-graded algebras $A$ such as CM (cm)-regularity, Tor\n(tor)-regularity (Ext (ext)-regularity) and Ex (ex)-regularity, which are the\nsupremum or infimum degrees of some associated canonical complexes. We show\nthat for any right bounded complex $X$ with finitely generated cohomologies,\nthe supremum degree of $R\\underline{\\text{Hom}}_A(X, A_0)$ coincides with the\nopposite of the infimum degree of $X$ if $A_0$ is semisimple. If $A$ has a\nbalanced dualizing complex and $A_0$ is semisimple, we prove that the\nCM-regularity of $X$ coincides with the supremum degree of\n$R\\underline{\\text{Hom}}_A(A_0,X)$ for any left bounded complex $X$ with\nfinitely generated cohomologies.\n  Several inequalities concerning the numerical regularities and the supremum\nor infimum degree of derived Hom or derived tensor complexes are given for\nnoncommutative noetherian locally finite $\\mathbb{N}$-graded algebras. Some of\nthese are generalizations of J\\o rgensen's results on the inequalities between\nthe CM-regularity and Tor-regularity, some are new even in the connected graded\ncase. Conditions are given under which the inequalities become equalities by\nestablishing two technical lemmas.\n  Following Kirkman, Won and Zhang, we also use the numerical AS-regularity\n(resp. little AS-regularity) to study Artin-Schelter regular property\n(finite-dimensional property) for noetherian $\\mathbb{N}$-graded algebras. We\nprove that the numerical AS-regularity of $A$ is zero if and only if that $A$\nis an $\\mathbb{N}$-graded AS-regular algebra under some mild conditions, which\ngeneralizes a result of Dong-Wu and a result of Kirkman-Won-Zhang. If $A$ has a\nbalanced dualizing complex and $A_0$ is semisimple, we prove that the little\nAS-regularity of $A$ is zero if and only if $A$ is finite-dimensional.",
        "The in-context learning capabilities of modern language models have motivated\na deeper mathematical understanding of sequence models. A line of recent work\nhas shown that linear attention models can emulate projected gradient descent\niterations to implicitly learn the task vector from the data provided in the\ncontext window. In this work, we consider a novel setting where the global task\ndistribution can be partitioned into a union of conditional task distributions.\nWe then examine the use of task-specific prompts and prediction heads for\nlearning the prior information associated with the conditional task\ndistribution using a one-layer attention model. Our results on loss landscape\nshow that task-specific prompts facilitate a covariance-mean decoupling where\nprompt-tuning explains the conditional mean of the distribution whereas the\nvariance is learned\/explained through in-context learning. Incorporating\ntask-specific head further aids this process by entirely decoupling estimation\nof mean and variance components. This covariance-mean perspective similarly\nexplains how jointly training prompt and attention weights can provably help\nover fine-tuning after pretraining.",
        "Tool learning can further broaden the usage scenarios of large language\nmodels (LLMs). However most of the existing methods either need to finetune\nthat the model can only use tools seen in the training data, or add tool\ndemonstrations into the prompt with lower efficiency. In this paper, we present\na new Tool Learning method Chain-of-Tools. It makes full use of the powerful\nsemantic representation capability of frozen LLMs to finish tool calling in CoT\nreasoning with a huge and flexible tool pool which may contain unseen tools.\nEspecially, to validate the effectiveness of our approach in the massive unseen\ntool scenario, we construct a new dataset SimpleToolQuestions. We conduct\nexperiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two\nknowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions).\nExperimental results show that our approach performs better than the baseline.\nWe also identify dimensions of the model output that are critical in tool\nselection, enhancing the model interpretability. Our code and data are\navailable at: https:\/\/github.com\/fairyshine\/Chain-of-Tools .",
        "This paper presents an innovative approach to applying bidirectional\ntransformations (BX) in practice. To introduce BX to a wider audience of\ntechnologists, engineers, and researchers, we have chosen to use C# to develop\nBifrons - a library of BX lenses that replaces domain-specific programming\nlanguages (DSL) in practical use. The proposed approach simplifies the\nimplementation effort for two-way transformations by using simple symmetric\nlenses as the initial design pattern. It ensures correctness within reason by\nproviding a simple lens-testing framework. We demonstrate the usability of BX\nlenses in a realistic scenario by using Bifrons to perform a case study\nexperiment synchronizing data from two structurally and technologically\nheterogeneous databases.",
        "In this paper, we propose a movable antenna (MA)-enabled frequency-hopping\n(FH) multiple-input multiple-output (MIMO) radar system and investigate its\nsensing resolution. Specifically, we derive the expression of the ambiguity\nfunction and analyze the relationship between its main lobe width and the\ntransmit antenna positions. In particular, the optimal antenna distribution to\nachieve the minimum main lobe width in the angular domain is characterized. We\ndiscover that this minimum width is related to the antenna size, the antenna\nnumber, and the target angle. Meanwhile, we present lower bounds of the\nambiguity function in the Doppler and delay domains, and show that the impact\nof the antenna size on the radar performance in these two domains is very\ndifferent from that in the angular domain. Moreover, the performance\nenhancement brought by MAs exhibits a certain trade-off between the main lobe\nwidth and the side lobe peak levels. Therefore, we propose to balance between\nminimizing the side lobe levels and narrowing the main lobe of the ambiguity\nfunction by optimizing the antenna positions. To achieve this goal, we propose\na low-complexity algorithm based on the Rosen's gradient projection method, and\nshow that its performance is very close to the baseline. Simulation results are\npresented to validate the theoretical analysis on the properties of the\nambiguity function, and demonstrate that MAs can reduce the main lobe width and\nsuppress the side lobe levels of the ambiguity function, thereby enhancing\nradar performance.",
        "Current Visual Simultaneous Localization and Mapping (VSLAM) systems often\nstruggle to create maps that are both semantically rich and easily\ninterpretable. While incorporating semantic scene knowledge aids in building\nricher maps with contextual associations among mapped objects, representing\nthem in structured formats like scene graphs has not been widely addressed,\nencountering complex map comprehension and limited scalability. This paper\nintroduces visual S-Graphs (vS-Graphs), a novel real-time VSLAM framework that\nintegrates vision-based scene understanding with map reconstruction and\ncomprehensible graph-based representation. The framework infers structural\nelements (i.e., rooms and corridors) from detected building components (i.e.,\nwalls and ground surfaces) and incorporates them into optimizable 3D scene\ngraphs. This solution enhances the reconstructed map's semantic richness,\ncomprehensibility, and localization accuracy. Extensive experiments on standard\nbenchmarks and real-world datasets demonstrate that vS-Graphs outperforms\nstate-of-the-art VSLAM methods, reducing trajectory error by an average of\n3.38% and up to 9.58% on real-world data. Furthermore, the proposed framework\nachieves environment-driven semantic entity detection accuracy comparable to\nprecise LiDAR-based frameworks using only visual features. A web page\ncontaining more media and evaluation outcomes is available on\nhttps:\/\/snt-arg.github.io\/vsgraphs-results\/.",
        "Although LLMs have achieved significant success, their reliance on large\nvolumes of human-annotated data has limited their potential for further\nscaling. In this situation, utilizing self-generated synthetic data has become\ncrucial for fine-tuning LLMs without extensive human annotation. However,\ncurrent methods often fail to ensure consistent improvements across iterations,\nwith performance stagnating after only minimal updates. To overcome these\nchallenges, we introduce Dynamic Noise Preference Optimization (DNPO). DNPO\nemploys a dynamic sample labeling mechanism to construct preference pairs for\ntraining and introduces controlled, trainable noise into the preference\noptimization process. Our approach effectively prevents stagnation and enables\ncontinuous improvement. In experiments with Zephyr-7B, DNPO consistently\noutperforms existing methods, showing an average performance boost of 2.6%\nacross multiple benchmarks. Additionally, DNPO shows a significant improvement\nin model-generated data quality, with a 29.4% win-loss rate gap compared to the\nbaseline in GPT-4 evaluations. This highlights its effectiveness in enhancing\nmodel performance through iterative refinement.",
        "Given the substantial growth in the use of additive manufacturing in\nconstruction (AMC), it is necessary to ensure the quality of printed specimens\nwhich can be much more complex than conventionally manufactured parts. This\nstudy explores the various aspects of geometry and surface quality control for\n3D concrete printing (3DCP), with a particular emphasis on deposition-based\nmethods, namely extrusion and shotcrete 3D printing (SC3DP). A comprehensive\noverview of existing quality control (QC) methods and strategies is provided\nand preceded by an in-depth discussion. Four categories of data capture\ntechnologies are investigated and their advantages and limitations in the\ncontext of AMC are discussed. Additionally, the effects of environmental\nconditions and objects' properties on data capture are also analyzed. The study\nextends to automated data capture planning methods for different sensors.\nFurthermore, various quality control strategies are explored across different\nstages of the fabrication cycle of the printed object including: (i) During\nprinting, (ii) Layer-wise, (iii) Preassembly, and (iv) Assembly. In addition to\nreviewing the methods already applied in AMC, we also address various research\ngaps and future trends and highlight potential methodologies from adjacent\ndomains that could be transferred to AMC.",
        "A previous case study measured performance vs source-code complexity across\nmultiple languages. The case study identified Chapel and HPX provide similar\nperformance and code complexity. This paper is the result of initial steps\ntoward closing the source-code complexity gap between Chapel and HPX by using a\nsource-to-source compiler. The investigation assesses the single-machine\nperformance of both Chapel and Chplx applications across Arm and x86.",
        "Runtime repeated recursion unfolding was recently introduced as a\njust-in-time program transformation strategy that can achieve super-linear\nspeedup. So far, the method was restricted to single linear direct recursive\nrules in the programming language Constraint Handling Rules (CHR). In this\ncompanion paper, we generalize the technique to multiple recursion and to\nmultiple recursive rules and provide an implementation of the generalized\nmethod in the logic programming language Prolog.\n  The basic idea of the approach is as follows: When a recursive call is\nencountered at runtime, the recursive rule is unfolded with itself and this\nprocess is repeated with each resulting unfolded rule as long as it is\napplicable to the current call. In this way, more and more recursive steps are\ncombined into one recursive step. Then an interpreter applies these rules to\nthe call starting from the most unfolded rule. For recursions which have\nsufficiently simplifyable unfoldings, a super-linear can be achieved, i.e. the\ntime complexity is reduced.\n  We implement an unfolder, a generalized meta-interpreter and a novel\nround-robin rule processor for our generalization of runtime repeated recursion\nunfolding with just ten clauses in Prolog. We illustrate the feasibility of our\ntechnique with worst-case time complexity estimates and benchmarks for some\nbasic classical algorithms that achieve a super-linear speedup.",
        "These are expository lecture notes from a graduate topics course taught by\nthe author on Khovanov homology and related invariants. Major topics include\nthe Jones polynomial, Khovanov homology, Bar-Natan's cobordism category,\napplications of Khovanov homology, some spectral sequences, Khovanov stable\nhomotopy type, and skein lasagna modules. Topological and algebraic exposition\nare sprinkled throughout as needed.",
        "We introduce ArtInsight, a novel AI-powered system to facilitate deeper\nengagement with child-created artwork in mixed visual-ability families.\nArtInsight leverages large language models (LLMs) to craft a respectful and\nthorough initial description of a child's artwork, and provides: creative\nAI-generated descriptions for a vivid overview, audio recording to capture the\nchild's own description of their artwork, and a set of AI-generated questions\nto facilitate discussion between blind or low-vision (BLV) family members and\ntheir children. Alongside ArtInsight, we also contribute a new rubric to score\nAI-generated descriptions of child-created artwork and an assessment of\nstate-of-the-art LLMs. We evaluated ArtInsight with five groups of BLV family\nmembers and their children, and as a case study with one BLV child therapist.\nOur findings highlight a preference for ArtInsight's longer,\nartistically-tailored descriptions over those generated by existing BLV AI\ntools. Participants highlighted the creative description and audio recording\ncomponents as most beneficial, with the former helping ``bring a picture to\nlife'' and the latter centering the child's narrative to generate context-aware\nAI responses. Our findings reveal different ways that AI can be used to support\nart engagement, including before, during, and after interaction with the child\nartist, as well as expectations that BLV adults and their sighted children have\nabout AI-powered tools.",
        "Motivated by applications to equivariant neural networks and cryo-electron\nmicroscopy we consider the problem of recovering the generic orbit in a\nrepresentation of a finite group from invariants of low degree. The main result\nproved here is that invariants of degree at most three separate generic orbits\nin the regular representation of a finite group defined over any infinite\nfield. This answers a question posed in a 2023 ACHA paper of Bandeira et. al.\nWe also discuss this problem for subregular representations of the dihedral and\nsymmetric groups.",
        "Orthogonal time frequency space (OTFS) modulation has been proposed recently\nas a new waveform in the context of doubly-selective multi-path channels. This\narticle proposes a novel pilot design that improves OTFS spectral efficiency\n(SE) while reducing its peak-to-average power ratio (PAPR). Instead of adopting\nan embedded data-orthogonal pilot for channel estimation, our scheme relies on\nChu sequences superimposed to data symbols. We optimize the construction by\ninvestigating the best energy split between pilot and data symbols. Two\nequalizers, and an iterative channel estimation and equalization procedure are\nconsidered. We present extensive numerical results of relevant performance\nmetrics, including the normalized mean squared error of the estimator, bit\nerror rate, PAPR and SE. Our results show that, while the embedded pilot scheme\nestimates the channel more accurately, our approach yields a better tradeoff by\nachieving much higher spectral efficiency and lower PAPR.",
        "Localizing object parts precisely is essential for tasks such as object\nrecognition and robotic manipulation. Recent part segmentation methods require\nextensive training data and labor-intensive annotations. Segment-Anything Model\n(SAM) has demonstrated good performance on a wide range of segmentation\nproblems, but requires (manual) positional prompts to guide it where to\nsegment. Furthermore, since it has been trained on full objects instead of\nobject parts, it is prone to over-segmentation of parts. To address this, we\npropose a novel approach that guides SAM towards the relevant object parts. Our\nmethod learns positional prompts from coarse patch annotations that are easier\nand cheaper to acquire. We train classifiers on image patches to identify part\nclasses and aggregate patches into regions of interest (ROIs) with positional\nprompts. SAM is conditioned on these ROIs and prompts. This approach, termed\n`Guided SAM', enhances efficiency and reduces manual effort, allowing effective\npart segmentation with minimal labeled data. We demonstrate the efficacy of\nGuided SAM on a dataset of car parts, improving the average IoU on state of the\nart models from 0.37 to 0.49 with annotations that are on average five times\nmore efficient to acquire.",
        "Statistically equivalent blocks are not frequently considered in the context\nof nonparametric two-sample hypothesis testing. Despite the limited exposure,\nthis paper shows that a number of classical nonparametric hypothesis tests can\nbe derived on the basis of statistically equivalent blocks and their\nfrequencies. Far from a moot historical point, this allows for a more unified\napproach in considering the many two-sample nonparametric tests based on ranks,\nsigns, placements, order statistics, and runs. Perhaps more importantly, this\napproach also allows for the easy extension of many univariate nonparametric\ntests into arbitrarily high dimensions that retain all null properties\nregardless of dimensionality and are invariant to the scaling of the\nobservations. These generalizations do not require depth functions or the\nexplicit use of spatial signs or ranks and may be of use in various areas such\nas life-testing and quality control. In the manuscript, an overview of\nstatistically equivalent blocks and tests based on these blocks are provided.\nThis is followed by reformulations of some popular univariate tests and\ngeneralizations to higher dimensions. Comments comparing proposed methods to\nthose based on spatial signs and ranks are offered along with some conclusions.",
        "In this paper, we investigate Anderson localization for a nonlinear\nperturbation of the Maryland model\n$H=\\varepsilon\\Delta+\\cot\\pi(\\theta+j\\cdot\\alpha)\\delta_{j,j'}$ on\n$\\mathbb{Z}^d$. Specifically, if $\\varepsilon,\\delta$ are sufficiently small,\nwe construct a large number of time quasi-periodic and space exponentially\ndecaying solutions (i.e., Anderson localized states) for the equation\n$i\\frac{\\partial u}{\\partial t}=Hu+\\delta|u|^{2p}u$ with a Diophantine\n$\\alpha$. Our proof combines eigenvalue estimates of the Maryland model with\nthe Craig-Wayne-Bourgain method, which originates from KAM theory for\nHamiltonian PDEs."
      ]
    }
  },
  {
    "id":2411.07018,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"Accelerating cavity fault prediction using deep learning at Jefferson laboratory",
    "start_abstract":"Abstract Accelerating cavities are an integral part of the continuous electron beam accelerator facility (CEBAF) at Jefferson Laboratory. When any over 400 in CEBAF experiences a fault, it disrupts delivery to experimental user halls. In this study, we propose use deep learning model predict slowly developing cavity faults. By utilizing pre-fault signals, train long short-term memory-convolutional neural network binary classifier distinguish between radio-frequency (RF) signals during normal operation and RF indicative impending We optimize by adjusting fault confidence threshold implementing multiple consecutive window criterion identify events, ensuring low false positive rate. Results obtained from analysis real dataset collected accelerating simulating deployed scenario demonstrate model\u2019s ability with 99.99% accuracy correctly 80% Notably, these achievements were achieved context highly imbalanced dataset, predictions made several hundred milliseconds before onset fault. Anticipating faults enables preemptive measures improve operational efficiency preventing or mitigating their occurrence.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Field Emission in Superconducting Accelerators: Instrumented Measurements for Its Understanding and Mitigation"
      ],
      "abstract":[
        "Several new accelerator projects are adopting superconducting RF (SRF) technology. When accelerating SRF cavities maintain high RF gradients, field emission, the emission of electrons from cavity walls, can occur and may impact operational cavity gradient, radiological environment via activated components, and reliability. In this talk, we will discuss instrumented measurements of field emission from the two 1.1 GeV superconducting continuous wave (CW) linacs in CEBAF. The goal is to improve the understanding of field emission sources originating from cryomodule production, installation and operation. Such basic knowledge is needed in guiding field emission control, mitigation, and reduction toward high gradient and reliable operation of superconducting accelerators."
      ],
      "categories":[
        "physics.acc-ph"
      ]
    },
    "list":{
      "title":[
        "An exposition on the supersimplicity of certain expansions of the\n  additive group of the integers",
        "Evaluating the resilience of ESG investments in European Markets during\n  turmoil periods",
        "A new local time-decoupled squared Wasserstein-2 method for training\n  stochastic neural networks to reconstruct uncertain parameters in dynamical\n  systems",
        "A Comprehensive Framework for Statistical Inference in Measurement\n  System Assessment Studies",
        "A Packaging Method for ALPIDE Integration Enabling Flexible and\n  Low-Material-Budget Designs",
        "FairUDT: Fairness-aware Uplift Decision Trees",
        "Generalization error bound for denoising score matching under relaxed\n  manifold assumption",
        "Kauffman bracket skein module of the $(3,3,3,3)$-pretzel link exterior",
        "Breakdown of broken-symmetry approach to exchange interaction",
        "The $D_{s1}(2460) \\to D_s \\pi^+ \\pi^- $ decay from a $D_{s1}$ molecular\n  perspective",
        "Gravitational waves and primordial black holes from the T-model\n  inflation with Gauss-Bonnet correction",
        "Inverse Gaussian Distribution, Introduction and\n  Applications:Comprehensive Analysis of Power Plant Performance: A Study of\n  Combined Cycle and Nuclear Power Plant",
        "Personalized Convolutional Dictionary Learning of Physiological Time\n  Series",
        "Liquidity Competition Between Brokers and an Informed Trader",
        "A Comparison of Strategies to Embed Physics-Informed Neural Networks in\n  Nonlinear Model Predictive Control Formulations Solved via Direct\n  Transcription",
        "Corona algebras and strongly self-absorbing $\\mathrm{C}^{\\ast}$-dynamics",
        "Monolithic On-Chip Phononic Chiral Anomalous Bulk States on LiNbO3\n  Thin-films",
        "Discovery of a large magnetic nonlinear Hall effect in an altermagnet",
        "A Variable Coefficient Free Boundary Problem for $L^p$-solvability of\n  Parabolic Dirichlet Problems in Graph Domains",
        "High temperature surface state in Kondo insulator U$_3$Bi$_4$Ni$_3$",
        "Measurable Improvement in Multi-Qubit Readout Using a Kinetic Inductance\n  Traveling Wave Parametric Amplifier",
        "Behaviour of Newton Polygon over polynomial composition",
        "LAMOST Reveals Long-lived Protoplanetary Disks",
        "Towards Spectral Convergence of Locally Linear Embedding on Manifolds\n  with Boundary",
        "Unveiling the kinematics of a central region in the triple AGN host NGC\n  7733-7734 interacting group",
        "Crossover from BKT to first-order transition induced by higher-order\n  terms in 2D XY models",
        "Species scale associated with Weinberg operator and bound on Majorana\n  neutrino mass",
        "Mechanical Torque Disruption of Dust Grains Induced by Supernova Shock\n  Waves",
        "Quantifying the Upper Limit of Backflash Attack in Quantum Key\n  Distribution"
      ],
      "abstract":[
        "In this short note, we present a self-contained exposition of the\nsupersimplicity of certain expansions of the additive group of the integers,\nsuch as adding a generic predicate (due to Chatzidakis and Pillay), a predicate\nfor the square-free integers (due to Bhardwaj and Tran) or a predicate for the\nprime integers (due to Kaplan and Shelah, assuming Dickson's conjecture).",
        "This study investigates the resilience of Environmental, Social, and\nGovernance (ESG) investments during periods of financial instability, comparing\nthem with traditional equity indices across major European markets-Germany,\nFrance, and Italy. Using daily returns from October 2021 to February 2024, the\nanalysis explores the effects of key global disruptions such as the Covid-19\npandemic and the Russia-Ukraine conflict on market performance. A mixture of\ntwo generalised normal distributions (MGND) and EGARCH-in-mean models are used\nto identify periods of market turmoil and assess volatility dynamics. The\nfindings indicate that during crises, ESG investments present higher volatility\nin Germany and Italy than in France. Despite some regional variations, ESG\nportfolios demonstrate greater resilience compared to traditional ones,\noffering potential risk mitigation during market shocks. These results\nunderscore the importance of integrating ESG factors into long-term investment\nstrategies, particularly in the face of unpredictable financial turmoil.",
        "In this work, we propose and analyze a new local time-decoupled squared\nWasserstein-2 method for reconstructing the distribution of unknown parameters\nin dynamical systems. Specifically, we show that a stochastic neural network\nmodel, which can be effectively trained by minimizing our proposed local\ntime-decoupled squared Wasserstein-2 loss function, is an effective model for\napproximating the distribution of uncertain model parameters in dynamical\nsystems. Through several numerical examples, we showcase the effectiveness of\nour proposed method in reconstructing the distribution of parameters in\ndifferent dynamical systems.",
        "Measurement system analysis aims to quantify the variability in data\nattributable to the measurement system and evaluate its contribution to overall\ndata variability. This paper conducts a rigorous theoretical investigation of\nthe statistical methods used in such analyses, focusing on variance components\nand other critical parameters. While established techniques exist for\nsingle-variable cases, a systematic theoretical exploration of their properties\nhas been largely overlooked. This study addresses this gap by examining\nestimators for variance components and other key parameters in measurement\nsystem assessment, analyzing their statistical properties, and providing new\ninsights into their reliability, performance, and applicability.",
        "This work presents a novel solution for the packaging of ALPIDE chips that\nfacilitates non-planar assembly with a minimal material budget. This solution\nrepresents a technological advancement based on methodologies developed for the\nALICE ITS1 and the STAR tracker two decades ago. The core of this approach\ninvolves the use of flexible cables composed of aluminum and polyimide, with\nthicknesses on the order of tens of micrometers. These cables are connected to\nthe sensors using single-point Tape Automated Bonding (spTAB), which replaces\nthe traditional wire bonding technique that is suboptimal for curved\nintegrations. The spTAB bonding is achieved by creating openings in the\npolyimide layer, allowing aluminum wires to remain free-standing, which are\nthen connected to the sensor using pressure and ultrasonic energy. Extending\nthis concept, we have applied this approach to entire printed circuit boards\n(PCBs), resulting in a fully flexible packaging solution maintaining an\nultra-low material budget. This work introduces a prototype utilizing this\nmethod to bond an ALPIDE chip, proposing it as a viable option for future\ndesigns necessitating flexible packaging for both the chip and associated\nelectronics. The overall workflow, comprising microfabrication and assembly, is\ncarried out at the Fondazione Bruno Kessler and INFN TIFPA laboratories and\nwill be detailed to elucidate our procedures and demonstrate the applicability\nof our solution in future experimental setups. The proposed packaging features\na flexible PCB constructed from three stacked layers, each containing 20 $\\mu$m\nthick aluminum features and a 25 $\\mu$m thick polyimide substrate. These layers\ninclude a ground layer, a signal layer (encompassing both digital and analog\nsignals), and a local bonding layer (which substitutes wire bonding).",
        "Training data used for developing machine learning classifiers can exhibit\nbiases against specific protected attributes. Such biases typically originate\nfrom historical discrimination or certain underlying patterns that\ndisproportionately under-represent minority groups, such as those identified by\ntheir gender, religion, or race. In this paper, we propose a novel approach,\nFairUDT, a fairness-aware Uplift-based Decision Tree for discrimination\nidentification. FairUDT demonstrates how the integration of uplift modeling\nwith decision trees can be adapted to include fair splitting criteria.\nAdditionally, we introduce a modified leaf relabeling approach for removing\ndiscrimination. We divide our dataset into favored and deprived groups based on\na binary sensitive attribute, with the favored dataset serving as the treatment\ngroup and the deprived dataset as the control group. By applying FairUDT and\nour leaf relabeling approach to preprocess three benchmark datasets, we achieve\nan acceptable accuracy-discrimination tradeoff. We also show that FairUDT is\ninherently interpretable and can be utilized in discrimination detection tasks.\nThe code for this project is available https:\/\/github.com\/ara-25\/FairUDT",
        "We examine theoretical properties of the denoising score matching estimate.\nWe model the density of observations with a nonparametric Gaussian mixture. We\nsignificantly relax the standard manifold assumption allowing the samples step\naway from the manifold. At the same time, we are still able to leverage a nice\ndistribution structure. We derive non-asymptotic bounds on the approximation\nand generalization errors of the denoising score matching estimate. The rates\nof convergence are determined by the intrinsic dimension. Furthermore, our\nbounds remain valid even if we allow the ambient dimension grow polynomially\nwith the sample size.",
        "We show that the Kauffman bracket skein module of the $(3,3,3,3)$-pretzel\nlink exterior over $\\mathbb{Q}(q^{\\frac{1}{2}})$ is not finitely generated as a\nmodule over $\\mathbb{Q}(q^{\\frac{1}{2}})[t_1,t_2]$, where $t_1,t_2$ are the\nmeridians of two components. This disproves a finiteness conjecture proposed in\n2021.",
        "Broken-symmetry (BS) approaches are widely employed to evaluate Heisenberg\nexchange parameters, primarily in combination with DFT calculations. For many\nmagnetic materials, BS-DFT calculations give reasonable estimations of exchange\nparameters although systematic failures have also been reported. While the\nlatter were attributed to deficiencies of approximate exchange-correlation\nfunctional, we prove here by treating a simple model system that the\nbroken-symmetry methodology has serious problems. Detailed analysis clarifies\nthe intrinsic issue with the broken-symmetry treatment of low-spin states. It\nshows, in particular, that the error in the BS calculation of exchange\nparameter scales with the degree of covalency between the magnetic and the\nbridging orbitals. As a possible tool to overcome this intrinsic drawback of\nsingle-determinant BS approaches, we propose their extension to a minimal\nmulticonfigurational version.",
        "We conduct a theoretical study of the $ D_{s1}(2460) \\to D_s \\pi^+ \\pi^- $\ndecay from the perspective that the $D_{s1}$ is a molecular state, built mostly\nfrom the $D^* K$ and $D_s^* \\eta$ components. The $D^*$ and $D_s^*$ mesons are\nallowed to decay into two pseudoscalars, with one of them merging with the\nother pseudoscalar that forms the $D_{s1}$ state, ultimately leading to the\n$\\pi^+ \\pi^- D_s$ final state. This results in a triangle diagram mechanism\nwhere all theoretical ingredients are well-known, leading to a free parameter\nframework. We evaluate the mass distributions of particle pairs and find good\nagreement with the experimental distributions of a recent LHCb experiment,\nproviding strong support to the molecular picture of the $D_{s1}(2460)$ state.\nWe also discuss the role played by the scalar mesons $f_0(500)$ and $f_0(980)$,\nat odds with the interpretation of the experimental analysis.",
        "Recently, the worldwide Pulsar Timing Array (PTA) collaborations detected a\nstochastic gravitational wave(GW) background in the nanohertz range, which may\noriginate from the early universe's inflationary phase. So in this work, we\ninvestigated induce GWs in the T-model inflation with Gauss-Bonnet coupling.\nConsider the scenario of traversing a domain wall in moduli space, we take the\ncoupling coefficient to be an approximately step function. Within suitable\nparameter regions, the model exhibits de Sitter fixed points, which allows\ninflation to undergo an ultra-slow-roll phase, which causes the power spectrum\nto exhibit a peak. Such a peak can induce nanohertz GWs, which provids an\nexplanation for the PTA observational data. Furthermore, we consider the case\nof multiple domain wall crossings, and adopting a double-step coupling\nfunction. In this case, the resulting GW spectrum has two peaks with\nfrequencies around \\(10^{-8} \\,\\text{Hz}\\) and \\(10^{-2}\\,\\text{Hz}\\),\nrespectively. Which can be observed by the PTA and the space GW detectors\nsimultaneously.Additionally, the reentry of the power spectrum peaks into the\nhorizon leads to the collapse into primordial black holes (PBHs). We calculate\nthe abundance of PBHs and found that the masses is in the range of \\(10^{-14}\n\\sim 10^{-13} M_\\odot\\) and around \\(10^{-2} M_\\odot\\) , which constitute\nsignificant components of the current dark matter.",
        "This paper presents a comprehensive analysis of power plant performance using\nthe inverse Gaussian (IG) distribution framework. We combine theoretical\nfoundations with practical applications, focusing on both combined cycle and\nnuclear power plant contexts. The study demonstrates the advantages of the IG\ndistribution in modeling right-skewed industrial data, particularly in power\ngeneration. Using the UCI Combined Cycle Power Plant Dataset, we establishthe\nsuperiority of IG-based models over traditional approaches through rigorous\nstatistical testing and model validation. The methodology developed here\nextends naturally to nuclear power plant applications, where similar\nstatistical patterns emerge in operational data. Our findings suggest that\nIG-based models provide more accurate predictions and better capture the\nunderlying physical processes in power generation systems.",
        "Human physiological signals tend to exhibit both global and local structures:\nthe former are shared across a population, while the latter reflect\ninter-individual variability. For instance, kinetic measurements of the gait\ncycle during locomotion present common characteristics, although idiosyncrasies\nmay be observed due to biomechanical disposition or pathology. To better\nrepresent datasets with local-global structure, this work extends Convolutional\nDictionary Learning (CDL), a popular method for learning interpretable\nrepresentations, or dictionaries, of time-series data. In particular, we\npropose Personalized CDL (PerCDL), in which a local dictionary models local\ninformation as a personalized spatiotemporal transformation of a global\ndictionary. The transformation is learnable and can combine operations such as\ntime warping and rotation. Formal computational and statistical guarantees for\nPerCDL are provided and its effectiveness on synthetic and real human\nlocomotion data is demonstrated.",
        "We study a multi-agent setting in which brokers transact with an informed\ntrader. Through a sequential Stackelberg-type game, brokers manage trading\ncosts and adverse selection with an informed trader. In particular, supplying\nliquidity to the informed traders allows the brokers to speculate based on the\nflow information. They simultaneously attempt to minimize inventory risk and\ntrading costs with the lit market based on the informed order flow, also known\nas the internalization-externalization strategy. We solve in closed form for\nthe trading strategy that the informed trader uses with each broker and propose\na system of equations which classify the equilibrium strategies of the brokers.\nBy solving these equations numerically we may study the resulting strategies in\nequilibrium. Finally, we formulate a competitive game between brokers in order\nto determine the liquidity prices subject to precommitment supplied to the\ninformed trader and provide a numerical example in which the resulting\nequilibrium is not Pareto efficient.",
        "This study aims to benchmark candidate strategies for embedding neural\nnetwork (NN) surrogates in nonlinear model predictive control (NMPC)\nformulations that are subject to systems described with partial differential\nequations and that are solved via direct transcription (i.e., simultaneous\nmethods). This study focuses on the use of physics-informed NNs and\nphysics-informed convolutional NNs as the internal (surrogate) models within\nthe NMPC formulation. One strategy embeds NN models as explicit algebraic\nconstraints, leveraging the automatic differentiation (AD) of an algebraic\nmodelling language (AML) to evaluate the derivatives. Alternatively, the solver\ncan be provided with derivatives computed external to the AML via the AD\nroutines of the machine learning environment the NN is trained in. The three\nnumerical experiments considered in this work reveal that replacing mechanistic\nmodels with NN surrogates may not always offer computational advantages when\nsmooth activation functions are used in conjunction with a local nonlinear\nsolver (e.g., Ipopt), even with highly nonlinear systems. Moreover, in this\ncontext, the external function evaluation of the NN surrogates often\noutperforms the embedding strategies that rely on explicit algebraic\nconstraints, likely due to the difficulty in initializing the auxiliary\nvariables and constraints introduced by explicit algebraic reformulations.",
        "This article concerns the structure of $\\mathrm{C}^{\\ast}$-algebraic group\nactions induced on corona algebras from a given $\\sigma$-unital\n$\\mathrm{C}^{\\ast}$-dynamical system over a locally compact group $G$. We prove\nthat such actions satisfy the so-called dynamical folding property, which\ngeneralizes a fundamental property observed for corona algebras in works of\nManuilov--Thomsen and Phillips--Weaver. We then focus on corona actions induced\nfrom $G$-$\\mathrm{C}^{\\ast}$-dynamics that are assumed to absorb a given\nstrongly self-absorbing and unitarily regular $G$-action $\\gamma$. It is proved\nthat these corona actions are $\\gamma$-saturated, which is a stronger property\nthan being separably $\\gamma$-stable. Conversely, if one assumes that the\nunderlying $\\mathrm{C}^{\\ast}$-dynamics absorbs the trivial action on the\ncompact operators, then $\\gamma$-saturation of the corona action is equivalent\nto the original action being $\\gamma$-absorbing. These results are a dynamical\nversion of recent work by Farah and the third-named author.",
        "Phononic materials are crucial for developing efficient, robust mechanical\nwaveguides with strong transport properties, enabling advances in sensing,\nsignal processing, energy harvesting, and microfluidics. A key motivation is\ntheir integration into monolithic systems for on-chip applications. While\ntopological phononic materials developed in the past decade offer\nunidirectional edge states immune to backscattering, their integration requires\nlarge volumes to control localized small volumes' transport properties,\nlimiting their efficiency and application in modern phononic circuits. The\nrecently introduced chiral anomalous bulk states (CABSs) combine the advantages\nof topological materials with innovative boundary designs, overcoming\ntransmission limitations and ensuring full material utilization for superior\nwave propagation. Here, we present the first on-chip monolithic CABS device\nintegrated on a suspended LiNbO3 thin film. This breakthrough enables the\ncreation of phononic waveguides with unmatched unidirectionality, low loss, and\nhigh transmission efficiency, seamlessly integrated with broadband\npiezoelectric transducers, and showcasing their potential for high-fidelity,\nbroad-bandwidth microwave signal transmission. Additionally, we exploit the\nslow-wave characteristics of CABSs for delay lines and high-density signal\nprocessing. Tailoring wave propagation through boundary engineering opens a new\nparadigm for phononic\/photonic device design, with implications across\nmicroelectronics, high-frequency communications, radar, and advanced sensing\ntechnologies. The work sets the stage for the future development of highly\nscalable, multifunctional, and robust phononic systems, unlocking new avenues\nfor integrated acoustic technologies.",
        "Since Edwin Halls groundbreaking discovery of the Hall effect in 1879,\nmagnetism, spin, and quantization have been expanding the scope of Hall\neffects, continuously driving transformative progress in science and\ntechnology. Among them, the latest nonlinear Hall effect (NLHE), where\nlongitudinal electric field tunes quantum geometry to generate nonlinear Hall\nvoltage, attracts wide attention as a sensitive probe of topological phases\nacross a wide range of materials. Here, we report a new Hall effect member: the\nmagnetic nonlinear Hall effect (MNLHE), characterized by a quadratic Hall\nconductivity dependence on magnetic field, rather than electric field as in\nNLHE. This finding relies on an altermagnet, Mn5Si3 thin film, whose\nalternating-sign Berry curvatures ensure higher-order MNLHE clearly\ndistinguishable from the first-order anomalous Hall effect. The observed\nquadratic dependence originates from chiral next-nearest-neighbor hopping\nprocesses that acquire magnetic-exchange-driven Zeeman energies and\nHaldane-like chiral flux phases. Remarkably, this MNLHE is non-analytic, as\nreversing the magnetic field flips the alternating spin-splitting bands and\nreverses the hopping chirality, which is absent in traditional NLHE. Beyond\noffering a distinctive transport fingerprint for altermagnet Mn5Si3 thin film,\nthis MNLHE is large and unsaturated up to 60 T, providing opportunities for\npulsed high-field sensing technologies in both fundamental researches and\nengineering applications.",
        "We investigate variable coefficient analogs of a recent work of Bortz,\nHofmann, Martell and Nystr\\\"om [BHMN25]. In particular, we show that if\n$\\Omega$ is the region above the graph of a Lip(1,1\/2) (parabolic Lipschitz)\nfunction and $L$ is a parabolic operator in divergence form \\[L = \\partial_t -\n\\text{div} A \\nabla\\] with $A$ satisfying an $L^1$ Carleson condition on its\nspatial and time derivatives, then the $L^p$-solvability of the Dirichlet\nproblem for $L$ and $L^*$ implies that the graph function has a half-order time\nderivative in BMO. Equivalently, the graph is parabolic uniformly rectifiable.\n  In the case of $A$ symmetric, we only require that the Dirichlet problem for\n$L$ is solvable, which requires us to adapt a clever integration by parts\nargument by Lewis and Nystr\\\"om. A feature of the present work is that we must\novercome the lack of translation invariance in our equation, which is a\nfundamental tool in similar works, including [BHMN25].",
        "The resurgence of interest in Kondo insulators has been driven by two major\nmysteries: the presence of metallic surface states and the observation of\nquantum oscillations. To further explore these mysteries, it is crucial to\ninvestigate another similar system beyond the two existing ones, SmB$_6$ and\nYbB$_{12}$. Here, we address this by reporting on a Kondo insulator,\nU$_3$Bi$_4$Ni$_3$. Our transport measurements reveal that a surface state\nemerges below 250 K and dominates transport properties below 150 K, which is\nwell above the temperature scale of SmB$_6$ and YbB$_{12}$. At low\ntemperatures, the surface conductivity is about one order of magnitude higher\nthan the bulk. The robustness of the surface state indicates that it is\ninherently protected. The similarities and differences between\nU$_3$Bi$_4$Ni$_3$ and the other two Kondo insulators will provide valuable\ninsights into the nature of metallic surface states in Kondo insulators and\ntheir interplay with strong electron correlations.",
        "Increasing the size and complexity of quantum information systems requires\nhighly-multiplexed readout architectures, as well as amplifier chains operating\nnear the quantum limit (QL) of added noise. While documented prior efforts in\nKITWPA integration in quantum systems are scarce, in this work we demonstrate\nintegration of a KI-TWPA with a multiplexed-qubit device. To quantify the\nsystem noise improvement we perform an ac Stark shift calibration to precisely\ndetermine noise power levels on-chip (at each cavity's reference plane) and the\ntotal system gain. We then characterize the qubit state measurement fidelity\nand the corresponding signal-to-noise ratio (SNR). To conduct the most faithful\nmeasurement of the benefits offered by the KI-TWPA we perform these\nmeasurements for readout chains where the high electron mobility transistor\n(HEMT) amplifier is the first-stage amplifier (FSA) - with none of the external\nhardware required to operate the KI-TWPA - and with the KI-TWPA as the FSA.\nWhile some readout cavities fall outside the KI-TWPA bandwidth, for those\ninside the bandwidth we demonstrate a maximum improvement in the state\nmeasurement SNR by a factor of 1.45, and increase the fidelity from 96.2% to\n97.8%. These measurements demonstrate a system noise below 5 quanta referenced\non-chip and we bound the KI-TWPA excess noise to be below 4 quanta for the six\ncavities inside its bandwidth. These results show a promising path forward for\nrealizing quantum-limited readout chains in large qubit systems using a single\nparametric amplifier.",
        "In this paper, we study the structure of Newton polygons for compositions of\npolynomials over the rationals. We establish sufficient conditions under which\nthe successive vertices of the Newton polygon of the composition $ g(f^n(x)) $\nwith respect to a prime $ p $ can be explicitly described in terms of the\nNewton polygon of the polynomial $ g(x) $. Our results provide deeper insights\ninto how the Newton polygon of a polynomial evolves under iteration and\ncomposition, with applications to the study of dynamical irreducibility,\neventual stability, non-monogenity of tower of number fields, etc.",
        "While both observations and theories demonstrate that protoplanetary disks\nare not expected to live much longer than $\\sim$10 Myr, several examples of\nprolonged disks have been observed in the past. In this work, we perform a\nsystematic search for aged YSOs still surrounded by protoplanetary disks in the\nM star catalog from the LAMOST archive. We identify 14 sources older than 10\nMyr, still surrounded by protoplanetary disks and with ongoing accretion\nactivities, significantly improving the census of the category known as the\nPeter Pan disks. The stellar parameters, variability and accretion properties\nof these objects, as well as their spatial distribution, are investigated.\nNearly all of these objects are distributed far away from nearby associations\nand star forming regions, but show evidence of being members of open clusters.\nInvestigating the correlation between mass accretion rates and stellar masses,\nwe find these long-lived disks accrete at systematically lower levels, compared\nto their younger counterparts with similar stellar masses. Studying the\nevolution of mass accretion rates with stellar ages, we find these aged disks\nfollow similar trend as young ones.",
        "We study the eigenvalues and eigenfunctions of a differential operator that\ngoverns the asymptotic behavior of the unsupervised learning algorithm known as\nLocally Linear Embedding when a large data set is sampled from an interval or\ndisc. In particular, the differential operator is of second order, mixed-type,\nand degenerates near the boundary. We show that a natural regularity condition\non the eigenfunctions imposes a consistent boundary condition and use the\nFrobenius method to estimate pointwise behavior. We then determine the limiting\nsequence of eigenvalues analytically and compare them to numerical predictions.\nFinally, we propose a variational framework for determining eigenvalues on\nother compact manifolds.",
        "We present a detailed study of the interacting triple active galactic nuclear\nsystem NGC 7733-34, focusing on stellar kinematics, ionised gas characteristics\nand star formation within the central region and stellar bars of both galaxies.\nWe performed a comprehensive analysis using archival data from MUSE, HST\/ACS,\nand DECaLS, complemented by observations from UVIT and IRSF. We identified a\ndisc-like bulge in both NGC 7733 and NGC 7734 through 2-D decomposition. A\ncentral nuclear structure, with a semi-major axis of $\\sim$1.113 kpc, was\ndetected in NGC 7733 via photometric and kinematic analysis, confirmed by the\nstrong anti-correlation between $V\/\\sigma$ and $h_{3}$, indicative of circular\norbits in the centre. NGC 7734 lacks a distinct nuclear structure. The presence\nof disc-like bulge results in an anti-correlation between $V\/\\sigma$ and\n$h_{3}$ along with diffuse light. However, it does show higher central velocity\ndispersion, possibly attributed to an interaction with a smaller clump, which\nis likely a fourth galaxy within the system. Both galaxies demonstrate ongoing\nstar formation, evidenced by $FUV$ and $H\\alpha$ observations. NGC 7734 shows\nrecent star formation along its bar, while NGC 7733 experiences bar quenching.\nThe star formation rate (SFR) analysis of NGC 7734 reveals that the bar\nregion's SFR dominates the galaxy's overall SFR. Conversely, in NGC 7733, the\nlack of star formation along the bar and the presence of a Seyfert 2 active\ngalactic nuclei at the galaxy centre leave the possibility of a connection\nbetween both facts. However, it does not affect the galaxy's overall star\nformation. Our findings provide valuable insights into the stellar and gas\nkinematics, star formation processes, and active galactic nuclear feedback\nmechanisms in interacting galaxies hosting stellar bars.",
        "We study phase transitions in $XY$ models, generalized by inclusion of $n$\nhigher-order pairwise interactions of equal strength, by Monte Carlo\nsimulation. It is found that by adding new terms the\nBerezinskii-Kosterlitz-Thouless (BKT) transition, observed in the standard $XY$\nmodel, gradually changes to the first-order phase transition. We determine the\ncritical number of terms for which the first-order transition appears as\n$n_c=6$. It is also found that for $n=5$ the transition is pseudo-first-order\nbut it becomes true first-order if the couplings are allowed to increase. In\ngeneral, a more rapid increase of the coupling intensity supports the\nfirst-order transition, however, a too fast increase may result in splitting of\nthe single transition to multiple transitions. Consequently, the minimal number\nof the terms required for the change of the BKT phase transition to first order\nin the present model with arbitrary couplings is estimated to be $2 < n_c \\leq\n5$.",
        "When states in a tower like the Kaluza-Klein or the string tower couple to\nanother state through the irrelevant operators of the same type, their\ncontributions to the loop corrections of the relevant or the marginal operators\nare not negligible, threatening the perturbativity. This can be avoided\nprovided the cutoff scale is lower than the species scale associated with the\nirrelevant operator. We apply this to towers of states associated with the\nneutrino which couple to the Higgs through the Weinberg operator, the\ndimension-5 irrelevant operator generating the Majorana neutrino mass.\nRequiring the `Majorana species scale', the species scale associated with the\nWeinberg operator, to be below the gravitational species scale, one finds the\nlower bound on the Majorana neutrino mass determined by the species number. The\nFestina-Lente bound also gives the lower bound on the Majorana neutrino mass,\nbut it is not so stringent. Meanwhile, even if the neutrino mass is of the\nDirac type at the renormalizable level, the Majorana mass term still can be\nwritten in the effective field theory action so far as the Weinberg operator is\nnot forbidden. Even if the Majorana neutrino mass is larger than the Dirac one,\nso far as there are sufficient degrees of freedom with mass smaller than the\nscale of the cosmological constant, the observation of the Majorana nature of\nthe neutrino may not contradict to quantum gravity constraints which rules out\nthe neutrino mass purely given by the Majorana type.",
        "The feedback from massive stars drives the evolution of interstellar dust\ngrains by altering their physical properties via a number of radiative and\nmechanical processes. Through these interactions, interstellar grains can\nachieve high rotational velocities due to unbalanced torques, potentially\nleading to their disruption. Mechanical torque disruption occurs when gas-grain\ncollisions, induced by the passage of shocks, spin grains to critical\nrotational velocities. This study aims to investigate the effects of stochastic\nmechanical torque disruption on both pre-existent and supernova-condensed dust\ngrains located within wind-blown bubbles. The impact of mechanical torque\ndisruption on supernova-condensed dust and dust grains in wind-blown bubbles is\ninvestigated through post-processing of three-dimensional hydrodynamical\nsimulation outputs. The associated timescale is then compared to those of\nkinetic sputtering and grain shattering. Before the supernova explosion, dust\ngrain disruption timescales within wind-driven bubbles are on the order of\nmillions of years due to the low-density environment. The timescales for\nmechanical torque disruption (METD) are longer than those for kinetic\nsputtering and comparable to those of grain shattering, primarily due the high\ngrain drift velocities typical of these regions.",
        "Quantum Key Distribution (QKD) theoretically provides information-theoretic\nsecurity based on physical laws. However, imperfections in practice lead to the\npossibility of quantum hacking on the QKD implementation, especially the\npassive attacks that are difficult to be detected. In this paper, we study\nexperimentally and theoretically the upper limit of a backflash attack, as one\nof the vital passive attacks, on a fiber-based QKD system. We experimentally\ndemonstrate the backflash attack on a full equipped fiber-based QKD receiver to\nshow its feasibility and limited distinguish ratio of decoding. More\nimportantly, we have developed a simulation model to analyze the maximum\ndistinguish ratio of decoding can be achieved considering the wide-spectrum\nfeature of backflash photons, which indicates that Eve can extract effective\nkey information from 95.7% of the backflash photons. Consequently, the secure\nkey rate of the decoy-state BB84 QKD system under backflash attack is\ncalculated. This work provides a general methodology to comprehensively\nevaluate the effect of the backflash attack on a QKD system."
      ]
    }
  },
  {
    "id":2411.19844,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"A New Kind of Science",
    "start_abstract":"3R3. A New Kind of Science. - S Wolfram (Wolfram Res Inc, 100 Trade Center Dr, Champaign IL 61820-7237). Media, Champaign, IL. 2002. 1197 pp. ISBN 1-57955-008-8. $44.95. Reviewed by M Gad-el-Hak (Eng Build, Rm 303, Virginia Commonwealth Univ, 601 W Main St, PO Box 843015, Richmond, VA 23284-3015). Reviewing Science is like stepping in a minefield. The danger lies going against the deluge praise, proving relevance to this audience, and arguing proposed new science that allegedly set replace science, as we know it. Those issues will be addressed turn, but first brief background. Stephen considered many have been child prodigy: journal paper particle physics at age 15; stint Oxford; PhD from Caltech 20; youngest recipient MacArthur Prize; faculty positions Caltech, Princeton, Illinois; significant contributions cellular automata complexity theory; developer popular software Mathematica; successful entrepreneur, becoming multi-millionaire 30. Running his company via e-mail videoconference, spent last 10 years virtual seclusion, relentlessly, tirelessly, secretly, nocturnally working on an idea possessed him: generating simple computations, algorithms only few lines. book, targeting both scientists non-scientists, partially about using rules generate complex patterns. In task, author has succeeded beyond reproach not showing can done brilliantly beautifully, also explaining it lucidly enough for all understand, appreciate, savor. opinion several reviewers, including one, aspect book tour de force clarity, elegance, simplicity. problem huge leap takes since nature computer-generated patterns look or behave similarly natural man-made things around us\u2014a snow flake, turbulent flow, lung, mollusk shell, traffic jam, outbreak starfish coral reef, entire universe\u2014therefore must way works. Nature runs its course same computer program. That essence science: yield secrets universe, solve our long-standing problems, provide theory everything. More flight fancy later. Deluge: was widely anticipated before actual publication. Published May 14, 2002, quickly became Amazon.com bestseller promptly reviewed scientific press. Heavyweights former included York Times, Chicago Tribune, Newsweek, Time, Daily Telegraph, Le Monde, Frankfurter Allgemeine Zeitung, Economist. Except last, press went gaga over touting author's claim stand existing head. Economist (p 79, June 1, 2002) more subdued even provocatively titling review \"The Emperor's Theory.\" press, reviews were somewhat less glorious skeptical. Physics Today 55, July 2002), Leo Kadanoff's once pointed, subtle polite, concluding he cannot support view any \"new kind science\" displayed Wolfram's book. Newsweek 59, 27, quoted famed physicist Freeman Dyson: \"There's tradition approaching senility come up with grand, improbable theories. unusual he's doing 40s.\" Kadanoff Dyson express minority opinion, however, majority reviewers being excited reason every human mystery currently depressed stock market, free will, quantum field theory, entropy. For present reviewer, lurks high particularly so months behind who already anointed Isaac Newton 21st century. Relevance: As aims replacing readers Applied Mechanics Reviews stake matter. Mechanics\u2014classical most part occasionally quantum\u2014is underlying branch upon which almost applied mechanics based. mathematics here often form partial differential equations, where space time are indefinitely divisible continuum. example, most, all, fluid flows described well-known, well-posed Navier\u2013Stokes equations. those first-principles equations solved agreement experiment reproach. It problem, such frustrated scores him. search simpler alternative is, therefore, quite alluring. mechanics, when they solved, powerful predictive tool explain mechanical world us well help design machines. When analytical solutions unattainable, discretized brute numerical integration used. But possible some situations, example realistic high-Reynolds-number other multi-scale problems required computational memory speed overwhelm today's supercomputers. impenetrable certain degree empiricism introduced relatively faster computations then proceed. Heuristic turbulence modeling compromise. Despite limitations, traditional works exceedingly well, mechanicians happily practice their craft. Readers should, care passionately if laws supplanted science. Argument: Cellular late 1940s John von Neumann Stanislaw Ulam, although claims independently discovered three decades discrete dynamical systems whose behavior completely specified terms repetitive local relation. continuum represented uniform one-, two-, three-dimensional grid, each cell containing single bit data, 0 red, white, blue, etc, bits states. advances steps. state cell, location, computed step algorithm priori defined close neighbors. Simple programs could, fact, result researched one-dimensional arranged line. data updated based value two nearest cells. methodically studied identified total 256 different rules. Space\u2013time diagrams generated show four distinct patterns: dull uniformity; periodic time-dependence; fractal behavior; truly non-repetitive says broken than 300 fix \"errors\" Darwin, Newton, great ones corrected all. proposes radical notion development world, uncover fundamental universe. pattern-generating capabilities supplant difficult-to-solve yet-to-be-found just because resemble does mean work way. Furthermore, believed represent reality used make predictions agree observations. This Galileo's paradigm underpinning modern explanatory power authority stem ability verifiable predictions; otherwise mere post-hoc speculation. exactly what is. games speculation possibly compete horsepower F=ma E=mc2.Wolfram's boasting, throughout 1200 pages, minimum excessive. He writes, \"I vastly I ever thought possible, fact now touches area besides.\" writes ideas originating him, credits belong elsewhere. Alan Turing conceptualized simplest universal computer, machine. Thinking universe vast digital brainchild Edward Fredkin. use machine environment physical detailed Tommaso Toffoli Norman Margolus. Other Per Bak, Charles Bennett, Hans Meinhardt percolate properly credited. Writing person, relegating notes 350 pages grudgingly dismissively mentioning names, restricting list references own publications, dispel important shortcoming. took approach bypassing peer process. self-published acting author, editor, publisher. opening paragraph mostly favorable Time's (May 20, worth reflecting on: \"Cranks occupational hazard scientist eventually faces. Fortunately, these characters usually easy spot. If someone grand overturns centuries knowledge\u2014especially spans unrelated fields biology economics\u2014the odds good she crank. publishes standard journals general readers, watch out. And issued rather conventional publisher, case pretty much airtight.\" extravagant cold fusion\u2014a` la Stanley Pons Martin Fleischman\u2014and deserve proportionally vigilant scrutiny. validated nor subjected process rest mortals expected do. contrast old anti-Newtonian model predict anything. emperor no clothes. offense play brick build edifice call Bottom Line: fun reading pictures, bad recommendation. inspiration, read Newton's Principia Mathematica, Latin. solving Newtonian framework still best bet, one's better books mechanics.",
    "start_categories":[
      "cs.FL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "The structure of musical harmony as an ordered phase of sound: A statistical mechanics approach to music theory"
      ],
      "abstract":[
        "Music, while allowing nearly unlimited creative expression, almost always conforms to a set of rigid rules at fundamental level. The description and study these rules, the ordered structures that arise from them, is basis field music theory. Here, I present theoretical formalism aims explain why basic patterns emerge in music, using same statistical mechanics framework describes emergent order across phase transitions physical systems. first apply mean approximation demonstrate occur this model disordered sound discrete sets pitches, including 12-fold octave division used Western music. Beyond model, use numerical simulation uncover musical harmony. These results provide new lens through which view discover ideas explore."
      ],
      "categories":[
        "cs.NA"
      ]
    },
    "list":{
      "title":[
        "A \"cubist\" decomposition of the Handel-Mosher axis bundle and the\n  conjugacy problem for $\\mathrm{Out}(F_r)$",
        "Design of Cavity Backed Slotted Antenna using Machine Learning\n  Regression Model",
        "A numerical scheme for a multi-scale model of thrombus in arteries",
        "A Novel P-bit-based Probabilistic Computing Approach for Solving the 3-D\n  Protein Folding Problem",
        "Discovering Dataset Nature through Algorithmic Clustering based on\n  String Compression",
        "Modeling Driver Behavior in Speed Advisory Systems: Koopman-based\n  Approach with Online Update",
        "The effect of thermal misbalance on magnetohydrodynamic modes in coronal\n  magnetic cylinders",
        "The steady inviscid compressible self-similar flows and the stability\n  analysis",
        "Tactical Asset Allocation with Macroeconomic Regime Detection",
        "Clustering by Nonparametric Smoothing",
        "Vacuum permittivity and gravitational refractive index revisited",
        "Deceptive Sequential Decision-Making via Regularized Policy Optimization",
        "Feedback control solves pseudoconvex optimal tracking problems in\n  nonlinear dynamical systems",
        "Axion Emission from Proton Cooper Pairs in Neutron Stars",
        "Some characterizations of weak left braces",
        "Data-driven continuation of patterns and their bifurcations",
        "Spaces of subgroups of toral groups",
        "Geometric properties for a certain subclass of normalized harmonic\n  mappings",
        "A new tail bound for the sum of bounded independent random variables",
        "SUSY transformation as the coupler of non-interacting systems",
        "Joint State-Parameter Estimation for the Reduced Fracture Model via the\n  United Filter",
        "Recent advances in high-dimensional quantum frequency combs",
        "Optimal Spectral Transitions in High-Dimensional Multi-Index Models",
        "Star Formation Rates, Metallicities, and Stellar Masses on kpc-scales in\n  TNG50",
        "Propagation of optical solitons in the dielectric medium of a liquid\n  csystal",
        "ContinuouSP: Generative Model for Crystal Structure Prediction with\n  Invariance and Continuity",
        "Phase space geometry of collective spin systems: Scaling and Fractality",
        "Flat band driven itinerant magnetism in the Co-pnictides\n  (La,Ca)Co$_2$(As,P)$_2$",
        "Semi-analytical Engineering of Strongly Driven Nonlinear Systems Beyond\n  Floquet and Perturbation Theory"
      ],
      "abstract":[
        "We show that the axis bundle of a nongeometric fully irreducible outer\nautomorphism admits a canonical \"cubist\" decomposition into branched cubes that\nfit together with special combinatorics. From this structure, we locate a\ncanonical finite collection of periodic fold lines in each axis bundle. This\ngives a solution to the conjugacy problem in $\\mathrm{Out}(F_r)$ for fully\nirreducible outer automorphisms. This can be considered as an analogue of\nresults of Hamenst\\\"adt and Agol from the surface setting, which state that the\nset of trivalent train tracks carrying the unstable lamination of a\npseudo-Anosov map can be given the structure of a CAT(0) cube complex, and that\nthere is a canonical periodic fold line in this cube complex.",
        "In this paper, a regression-based machine learning model is used for the\ndesign of cavity backed slotted antenna. This type of antenna is commonly used\nin military and aviation communication systems. Initial reflection coefficient\ndata of cavity backed slotted antenna is generated using electromagnetic\nsolver. These reflection coefficient data is then used as input for training\nregression-based machine learning model. The model is trained to predict the\ndimensions of cavity backed slotted antenna based on the input reflection\ncoefficient for a wide frequency band varying from 1 GHz to 8 GHz. This\napproach allows for rapid prediction of optimal antenna configurations,\nreducing the need for repeated physical testing and manual adjustments, may\nlead to significant amount of design and development cost saving. The proposed\nmodel also demonstrates its versatility in predicting multi frequency resonance\nacross 1 GHz to 8 GHz. Also, the proposed approach demonstrates the potential\nfor leveraging machine learning in advanced antenna design, enhancing\nefficiency and accuracy in practical applications such as radar, military\nidentification systems and secure communication networks.",
        "In this article, the time-discretization of the fluid structure interaction\nmodel in the three-dimensional boundary domain is taken into account, which\nexplains the mechanical interaction between the blood flow and the Hookean\nelasticity. The interface between the two phases is given by a soft transition\nlayer and spreads to the finite thickness. On the implicit Euler scheme for\nthis discretization, We derive a variety of priori estimates and then use the\nFaedo-Galerkin method to prove the local well-poseedness results.",
        "In the post-Moore era, the need for efficient solutions to non-deterministic\npolynomial-time (NP) problems is becoming more pressing. In this context, the\nIsing model implemented by the probabilistic computing systems with\nprobabilistic bits (p-bits) has attracted attention due to the widespread\navailability of p-bits and support for large-scale simulations. This study\nmarks the first work to apply probabilistic computing to tackle protein\nfolding, a significant NP-complete problem challenge in biology. We represent\nproteins as sequences of hydrophobic (H) and polar (P) beads within a\nthree-dimensional (3-D) grid and introduce a novel many-body interaction-based\nencoding method to map the problem onto an Ising model. Our simulations show\nthat this approach significantly simplifies the energy landscape for short\npeptide sequences of six amino acids, halving the number of energy levels.\nFurthermore, the proposed mapping method achieves approximately 100 times\nacceleration for sequences consisting of ten amino acids in identifying the\ncorrect folding configuration. We predicted the optimal folding configuration\nfor a peptide sequence of 36 amino acids by identifying the ground state. These\nfindings highlight the unique potential of the proposed encoding method for\nsolving protein folding and, importantly, provide new tools for solving similar\nNP-complete problems in biology by probabilistic computing approach.",
        "Text datasets can be represented using models that do not preserve text\nstructure, or using models that preserve text structure. Our hypothesis is that\ndepending on the dataset nature, there can be advantages using a model that\npreserves text structure over one that does not, and viceversa. The key is to\ndetermine the best way of representing a particular dataset, based on the\ndataset itself. In this work, we propose to investigate this problem by\ncombining text distortion and algorithmic clustering based on string\ncompression. Specifically, a distortion technique previously developed by the\nauthors is applied to destroy text structure progressively. Following this, a\nclustering algorithm based on string compression is used to analyze the effects\nof the distortion on the information contained in the texts. Several\nexperiments are carried out on text datasets and artificially-generated\ndatasets. The results show that in strongly structural datasets the clustering\nresults worsen as text structure is progressively destroyed. Besides, they show\nthat using a compressor which enables the choice of the size of the\nleft-context symbols helps to determine the nature of the datasets. Finally,\nthe results are contrasted with a method based on multidimensional projections\nand analogous conclusions are obtained.",
        "Accurate driver behavior modeling is essential for improving the interaction\nand cooperation of the human driver with the driver assistance system. This\npaper presents a novel approach for modeling the response of human drivers to\nvisual cues provided by a speed advisory system using a Koopman-based method\nwith online updates. The proposed method utilizes the Koopman operator to\ntransform the nonlinear dynamics of driver-speed advisory system interactions\ninto a linear framework, allowing for efficient real-time prediction. An online\nupdate mechanism based on Recursive Least Squares (RLS) is integrated into the\nKoopman-based model to ensure continuous adaptation to changes in driver\nbehavior over time. The model is validated using data collected from a\nhuman-in-the-loop driving simulator, capturing diverse driver-specific\ntrajectories. The results demonstrate that the offline learned Koopman-based\nmodel can closely predict driver behavior and its accuracy is further enhanced\nthrough an online update mechanism with the RLS method.",
        "This study investigates the dispersion of magnetohydrodynamic waves\ninfluenced by thermal misbalance in a cylindrical configuration with a finite\naxial magnetic field within solar coronal plasmas. Specifically, it examines\nhow thermal misbalance, characterized by two distinct timescales directly\nlinked to the cooling and heating functions, influences the dispersion\nrelation. This investigation is a key approach for understanding non-adiabatic\neffects on the behaviour of these waves. Our findings reveal that the effect of\nthermal misbalance on fast sausage and kink modes, consistent with previous\nstudies on slabs, is small but slightly more pronounced than previously\nthought. The impact is smaller at long-wavelength limits but increases at\nshorter wavelengths, leading to higher damping rates. This minor effect on fast\nmodes occurs despite the complex interaction of thermal misbalance terms within\nthe dispersion relation, even at low-frequency limits defined by the\ncharacteristic timescales. Additionally, a very small amplification is\nobserved, indicating a suppressed damping state for the long-wavelength\nfundamental fast kink mode. In contrast, slow magnetoacoustic modes are\nsignificantly affected by thermal misbalance, with the cusp frequency shifting\nslightly to lower values, which is significant for smaller longitudinal\nwavenumbers. This thermal misbalance likely accounts for the substantial\nattenuation observed in the propagation of slow magnetoacoustic waves within\nthe solar atmosphere. The long-wavelength limit leads to an analytical\nexpression that accurately describes the frequency shifts in slow modes due to\nmisbalance, closely aligning with both numerical and observational results.",
        "We investigate the steady inviscid compressible self-similar flows which\ndepends only on the polar angle in spherical coordinates. It is shown that\nbesides the purely supersonic and subsonic self-similar flows, there exists\npurely sonic flows, Beltrami flows with a nonconstant proportionnality factor\nand smooth transonic self-similar flows with large vorticity. For a constant\nsupersonic incoming flow past an infinitely long circular cone, a conic shock\nattached to the tip of the cone will form, provided the opening angle of the\ncone is less than a critical value. We introduce the shock polar for the radial\nand polar components of the velocity and show that there exists a monotonicity\nrelation between the shock angle and the radial velocity, which seems to be new\nand not been observed before. If a supersonic incoming flow is self-similar\nwith nonzero azimuthal velocity, a conic shock also form attached to the tip of\nthe cone. The state at the downstream may change smoothly from supersonic to\nsubsonic, thus the shock can be supersonic-supersonic, supersonic-subsonic and\neven supersonic-sonic where the shock front and the sonic front coincide. We\nfurther investigate the structural stability of smooth self-similar\nirrotational transonic flows and analyze the corresponding linear mixed type\nsecond order equation of Tricomi type. By exploring some key properties of the\nself-similar solutions, we find a multiplier and identify a class of admissible\nboundary conditions for the linearized mixed type second-order equation. We\nalso prove the existence and uniqueness of a class of smooth transonic flows\nwith nonzero vorticity which depends only on the polar and azimuthal angles in\nspherical coordinates.",
        "This paper extends the tactical asset allocation literature by incorporating\nregime modeling using techniques from machine learning. We propose a novel\nmodel that classifies current regimes, forecasts the distribution of future\nregimes, and integrates these forecasts with the historical performance of\nindividual assets to optimize portfolio allocations. Utilizing a macroeconomic\ndata set from the FRED-MD database, our approach employs a modified k-means\nalgorithm to ensure consistent regime classification over time. We then\nleverage these regime predictions to estimate expected returns and\nvolatilities, which are subsequently mapped into portfolio allocations using\nvarious sizing schemes. Our method outperforms traditional benchmarks such as\nequal-weight, buy-and-hold, and random regime models. Additionally, we are the\nfirst to apply a regime detection model from a large macroeconomic dataset to\ntactical asset allocation, demonstrating significant improvements in portfolio\nperformance. Our work presents several key contributions, including a novel\ndata-driven regime detection algorithm tailored for uncertainty in forecasted\nregimes and applying the FRED-MD data set for tactical asset allocation.",
        "A novel formulation of the clustering problem is introduced in which the task\nis expressed as an estimation problem, where the object to be estimated is a\nfunction which maps a point to its distribution of cluster membership. Unlike\nexisting approaches which implicitly estimate such a function, like Gaussian\nMixture Models (GMMs), the proposed approach bypasses any explicit modelling\nassumptions and exploits the flexible estimation potential of nonparametric\nsmoothing. An intuitive approach for selecting the tuning parameters governing\nestimation is provided, which allows the proposed method to automatically\ndetermine both an appropriate level of flexibility and also the number of\nclusters to extract from a given data set. Experiments on a large collection of\npublicly available data sets are used to document the strong performance of the\nproposed approach, in comparison with relevant benchmarks from the literature.\nR code to implement the proposed approach is available from\nhttps:\/\/github.com\/DavidHofmeyr\/ CNS",
        "The present paper reanalyzes the problem of the refractive properties of the\nphysical vacuum and their modification under the action of the gravitational\nfield and the electromagnetic field. This problem was studied in our previous\nworks and in the subsequent works of the researchers: Leuchs, Urban, Mainland\nand their collaborators. By modeling the physical vacuum as a\nparticle-antiparticle system, we can deduce with a certain approximation, in a\nsemiclassical theory, the properties of the free vacuum and the vacuum modified\nby the interaction with a gravitational field and an electromagnetic field.\nMore precise calculation of permittivities of free vacuum and near a particle\ncan lead to a non-point model of the particle. This modeling can follow both\nthe quantum and the general relativistic path as well as the phenomenological\npath, the results complementing each other.",
        "Autonomous systems are increasingly expected to operate in the presence of\nadversaries, though an adversary may infer sensitive information simply by\nobserving a system, without even needing to interact with it. Therefore, in\nthis work we present a deceptive decision-making framework that not only\nconceals sensitive information, but in fact actively misleads adversaries about\nit. We model autonomous systems as Markov decision processes, and we consider\nadversaries that attempt to infer their reward functions using inverse\nreinforcement learning. To counter such efforts, we present two regularization\nstrategies for policy synthesis problems that actively deceive an adversary\nabout a system's underlying rewards. The first form of deception is\n``diversionary'', and it leads an adversary to draw any false conclusion about\nwhat the system's reward function is. The second form of deception is\n``targeted'', and it leads an adversary to draw a specific false conclusion\nabout what the system's reward function is. We then show how each form of\ndeception can be implemented in policy optimization problems, and we\nanalytically bound the loss in total accumulated reward that is induced by\ndeception. Next, we evaluate these developments in a multi-agent sequential\ndecision-making problem with one real agent and multiple decoys. We show that\ndiversionary deception can cause the adversary to believe that the most\nimportant agent is the least important, while attaining a total accumulated\nreward that is $98.83\\%$ of its optimal, non-deceptive value. Similarly, we\nshow that targeted deception can make any decoy appear to be the most important\nagent, while still attaining a total accumulated reward that is $99.25\\%$ of\nits optimal, non-deceptive value.",
        "Achieving optimality in controlling physical systems is a profound challenge\nacross diverse scientific and engineering fields, spanning neuromechanics,\nbiochemistry, autonomous systems, economics, and beyond. Traditional solutions,\nrelying on time-consuming offline iterative algorithms, often yield limited\ninsights into fundamental natural processes. In this work, we introduce a\nnovel, causally deterministic approach, presenting the closed-form optimal\ntracking controller (OTC) that inherently solves pseudoconvex optimization\nproblems in various fields. Through rigorous analysis and comprehensive\nnumerical examples, we demonstrate OTC's capability of achieving both high\naccuracy and rapid response, even when facing high-dimensional and\nhigh-dynamical real-world problems. Notably, our OTC outperforms\nstate-of-the-art methods by, e.g., solving a 1304-dimensional neuromechanics\nproblem 1311 times faster or with 113 times higher accuracy. Most importantly,\nOTC embodies a causally deterministic system interpretation of optimality\nprinciples, providing a new and fundamental perspective of optimization in\nnatural and artificial processes. We anticipate our work to be an important\nstep towards establishing a general causally deterministic optimization theory\nfor a broader spectrum of system and problem classes, promising advances in\nunderstanding optimality principles in complex systems.",
        "We investigate axion emission from singlet proton Cooper pairs in neutron\nstars, a process that dominates axion emission in young neutron stars in the\nKSVZ model. By re-deriving its emissivity, we confirm consistency with most\nexisting literature, except for a recent study that exhibits a different\ndependence on the effective mass. This discrepancy results in more than an\norder-of-magnitude deviation in emissivity, significantly impacting constraints\non the KSVZ axion from the cooling observations of the Cassiopeia A neutron\nstar. Furthermore, we examine uncertainties arising from neutron-star equations\nof state and their role in the discrepancy, finding that the large deviation\npersists regardless of the choice of equations of state.",
        "As generalizations of skew left braces, weak left braces were introduced\nrecently by Catino, Mazzotta, Miccoli and Stefanelli to study ceratin special\ndegenerate set-theoretical solutions of the Yang-Baxter equation. In this note,\nas analogues of the notions of regular subgroups of holomorph of groups, Gamma\nfunctions on groups and affine and semi-affine structures on groups, we propose\nthe notions of good inverse subsemigroups and Gamma functions associated to\nClifford semigroups and affine structures on inverse semigroups, respectively,\nby which weak left braces are characterized. Moreover, symmetric,\n$\\lambda$-homomorphic and $\\lambda$-anti-homomorphic weak left braces are\nintroduced and the algebraic structures of these weak left braces are given.",
        "Patterns and nonlinear waves, such as spots, stripes, and rotating spirals,\narise prominently in many natural processes and in reaction-diffusion models.\nOur goal is to compute boundaries between parameter regions with different\nprevailing patterns and waves. We accomplish this by evolving randomized\ninitial data to full patterns and evaluate feature functions, such as the\nnumber of connected components or their area distribution, on their sublevel\nsets. The resulting probability measure on the feature space, which we refer to\nas pattern statistics, can then be compared at different parameter values using\nthe Wasserstein distance. We show that arclength predictor-corrector\ncontinuation can be used to trace out transition and bifurcation curves in\nparameter space by maximizing the distance of the pattern statistics. The\nutility of this approach is demonstrated through a range of examples involving\nhomogeneous states, spots, stripes, and spiral waves.",
        "We study the space of conjugacy classes of subgroups of a compact Lie group G\nwhose identity component is a torus, and consider how various invariants of\nsubgroups behave as sheaves over this space. This feeds in to the author's\nprogramme to give algebraic models of rational G-equivariant cohomology\ntheories.\n  The methods are illustrated by making the outcome explicit for all toral\nsubgroups of compact connected rank 2 groups.",
        "Let $\\mathcal{H}$ be the class of harmonic functions $f=h+\\overline{g}$ in\nthe unit disk $\\mathbb{D}:=\\{z\\in\\mathbb{C}:|z|<1\\}$, where $h$ and $g$ are\nanalytic in $\\mathbb{D}$ with the normalization $h(0)=g(0)=h'(0)-1=0$. Let\n$\\mathcal{P}_{\\mathcal{H}}^0(\\alpha,M)$ denote the subclass of $\\mathcal{H}$ in\n$\\mathbb{D}$ satisfying $\\text{Re}\\left((1-\\alpha)h'(z)+\\alpha\nzh''(z)\\right)>-M+\\left|(1-\\alpha)g'(z)+\\alpha zg''(z)\\right|$ with $g'(0)=0$,\n$M>0$ and $\\alpha\\in(0,1]$. In this paper, we investigate some fundamental\nproperties of functions belonging to the class\n$\\mathcal{P}_{\\mathcal{H}}^0(\\alpha,M)$, including coefficient bounds, growth\nestimates, convexity, starlikeness, convex combinations, and convolution.",
        "We construct a new tail bound for the sum of independent random variables for\nsituations in which the expected value of the sum is known and each random\nvariable lies within a specified interval, which may be different for each\nvariable. This new bound can be computed by solving a two-dimensional convex\noptimization problem. Simulations demonstrate that the new bound is often\nsubstantially tighter than Hoeffding's inequality for cases in which both\nbounds are applicable.",
        "Quasi-one-dimensional chains of atoms can be effectively described by\none-dimensional Dirac-type equation. Crystal structure of the chain is\nreflected by pseudo-spin of the quasi-particles. In the article, we present a\nsimple framework where supersymmetric transformation is utilized to generate an\ninteraction between two, initially non-interacting systems described by\npseudo-spin-one Dirac-type equation. In the presented example, the\ntransformation converts two asymptotically non-interacting atomic chains into a\nsaw chain locally. The model possesses a flat band whose energy can be\nfine-tuned deliberately.",
        "In this paper, we introduce an effective United Filter method for jointly\nestimating the solution state and physical parameters in flow and transport\nproblems within fractured porous media. Fluid flow and transport in fractured\nporous media are critical in subsurface hydrology, geophysics, and reservoir\ngeomechanics. Reduced fracture models, which represent fractures as\nlower-dimensional interfaces, enable efficient multi-scale simulations.\nHowever, reduced fracture models also face accuracy challenges due to modeling\nerrors and uncertainties in physical parameters such as permeability and\nfracture geometry. To address these challenges, we propose a United Filter\nmethod, which integrates the Ensemble Score Filter (EnSF) for state estimation\nwith the Direct Filter for parameter estimation. EnSF, based on a score-based\ndiffusion model framework, produces ensemble representations of the state\ndistribution without deep learning. Meanwhile, the Direct Filter, a recursive\nBayesian inference method, estimates parameters directly from state\nobservations. The United Filter combines these methods iteratively: EnSF\nestimates are used to refine parameter values, which are then fed back to\nimprove state estimation. Numerical experiments demonstrate that the United\nFilter method surpasses the state-of-the-art Augmented Ensemble Kalman Filter,\ndelivering more accurate state and parameter estimation for reduced fracture\nmodels. This framework also provides a robust and efficient solution for\nPDE-constrained inverse problems with uncertainties and sparse observations.",
        "High-dimensional entanglement in qudit states offers a promising pathway\ntowards the realization of practical, large-scale quantum systems that are\nhighly controllable. These systems can be leveraged for various applications,\nincluding advanced quantum information processing, secure communications,\ncomputation, and metrology. In this context, quantum frequency combs have a\ncrucial role as they inherently support multiple modes in both temporal and\nfrequency domains, while preserving a single spatial mode. The multiple\ntemporal and frequency modes of quantum frequency combs facilitate the\ngeneration, characterization, and control of high-dimensional time-frequency\nentanglement in extensive quantum systems. In this review article, we provide\nan overview of recent technological advancements in high-dimensional\nenergy-time entangled quantum frequency combs. We explore how these\ntime-frequency qudits, achieved using scalable telecommunications-wavelength\ncomponents, can empower the creation of large-scale quantum states. Advances in\nquantum frequency combs can unlock new capabilities and versatility for\npromising developments in quantum science and technology.",
        "We consider the problem of how many samples from a Gaussian multi-index model\nare required to weakly reconstruct the relevant index subspace. Despite its\nincreasing popularity as a testbed for investigating the computational\ncomplexity of neural networks, results beyond the single-index setting remain\nelusive. In this work, we introduce spectral algorithms based on the\nlinearization of a message passing scheme tailored to this problem. Our main\ncontribution is to show that the proposed methods achieve the optimal\nreconstruction threshold. Leveraging a high-dimensional characterization of the\nalgorithms, we show that above the critical threshold the leading eigenvector\ncorrelates with the relevant index subspace, a phenomenon reminiscent of the\nBaik-Ben Arous-Peche (BBP) transition in spiked models arising in random matrix\ntheory. Supported by numerical experiments and a rigorous theoretical\nframework, our work bridges critical gaps in the computational limits of weak\nlearnability in multi-index model.",
        "Integral field units (IFU) have extended our knowledge of galactic properties\nto kpc (or, sometimes, even smaller) patches of galaxies. These scales are\nwhere the physics driving galaxy evolution (feedback, chemical enrichment,\netc.) take place. Quantifying the spatially-resolved properties of galaxies,\nboth observationally and theoretically, is therefore critical to our\nunderstanding of galaxy evolution. To this end, we investigate\nspatially-resolved scaling relations within central galaxies\n($M_\\star>10^{9.0}$) at $z=0$ in IllustrisTNG. We examine both the resolved\nstar-forming main sequence (rSFMS) and the resolved mass-metallicity relation\n(rMZR) using $1~{\\rm kpc}\\times1~{\\rm kpc}$ maps of galaxies. We find that the\nrSFMS in IllustrisTNG is well-described by a power-law, but has some dependence\non the host galaxy's mass. Conversely, the rMZR for IllustrisTNG can be\ndescribed by a single power-law at low stellar mass surface density that\nflattens at high surface densities and is independent of host galaxy mass. We\nfind quantitative agreement in both the rSFMS and rMZR with recent IFU\nobservational campaigns. Furthermore, we argue that the rSFMS is an indirect\nresult of the Schmidt-Kennicutt (SK) law and local gas fraction relation, which\nare both independent of host galaxy properties. Finally, we expand upon a\nlocalized leaky-box model to study the evolution of idealized spaxels and find\nthat it provides a good description of these resolved relations. The degree of\nagreement, however, between idealized spaxels and simulated spaxels depends on\nthe `net' outflow rate for the spaxel, and the observed scaling relations\nindicate a preference for a low net outflow rate.",
        "Aim. Implement a stochastic representation of the wave function for a pair of\nentangled soliton functions in a liquid crystal. Show the applicability of a\nspecial soliton representation of quantum mechanics for modeling real entangled\nsystems. Methodology. The central place in the study is occupied by the method\nof mathematical modeling. As part of the calculation of stochastics by the\nmethod of abstraction and concretization, a detailed mathematical apparatus is\ngiven, adapted to the real physical case. A qualitative analysis of the\nbehavior of the material during the propagation of soliton pulses in it is\ncarried out. Results. The main value of the stochastic theory for a system of\nentangled solitons lies in the possibility of modeling the entangled states of\nreal systems - photons. In the framework of this work, the optical 1D envelopes\nof solitons in a nematic liquid crystal are considered in approximation to the\nconditions of a real physical problem. Research implications. The theoretical\nand\/or practical significance lies in the fundamental possibility of modeling\nreal entangled systems based on the constructed stochastic model of entangled\nsolitons and subsequent creation of special applications on its basis. In\nparticular, there will be a prospect of applying quantum teleportation to the\nproblem of propagation of quantum computing for use among the components of\nquantum computing networks.",
        "The discovery of new materials using crystal structure prediction (CSP) based\non generative machine learning models has become a significant research topic\nin recent years. In this paper, we study invariance and continuity in the\ngenerative machine learning for CSP. We propose a new model, called\nContinuouSP, which effectively handles symmetry and periodicity in crystals. We\nclearly formulate the invariance and the continuity, and construct a model\nbased on the energy-based model. Our preliminary evaluation demonstrates the\neffectiveness of this model with the CSP task.",
        "We examine the scaling of the inverse participation ratio of spin coherent\nstates in the energy basis of three collective spin systems: a bounded harmonic\noscillator, the Lipkin-Meshkov-Glick model, and the Quantum Kicked Top. The\nfinite-size quantum probing provides detailed insights into the structure of\nthe phase space, particularly the relationship between critical points in\nclassical dynamics and their quantum counterparts in collective spin systems.\nWe introduce a finite-size scaling mass exponent that makes it possible to\nidentify conditions under which a power-law behavior emerges, allowing to\nassign a fractal dimension to a coherent state. For the Quantum Kicked Top, the\nfractal dimension of coherent states -- when well-defined -- exhibits three\ngeneral behaviors: one related to the presence of critical points and two\nassociated with regular and chaotic dynamics. The finite-size scaling analysis\npaves the way toward exploring collective spin systems relevant to quantum\ntechnologies within the quantum-classical framework.",
        "Flat bands can induce strong electron correlation effects that help stabilize\nboth magnetic and superconducting states. Here, we carry out angle-resolved\nphotoemission spectroscopy and density functional theory calculations to study\nthe electronic structure of the Co-pnictides CaCo$_2$As$_2$ and LaCo$_2$P$_2$.\nWe find that, while the $k_z$ Fermi topology of ferromagnetic LaCo$_2$P$_2$ is\nmarkedly 2-dimensional, antiferromagnetic CaCo$_2$As$_2$ develops a 3D Fermi\nsurface described by a $zig-zag$-like band dispersion perpendicular to the\nCo-As plane. Furthermore, the magnetism is driven by the electronic\ncorrelations of the flat bands with $d_{xy}$ and $d_{z^2}$ orbital character at\nthe Fermi level. Our results link the electronic dimensionality and the\nmagnetic order, and emphasize the critical role of the As-As and P-P bond\nstrength along the $c$-direction to understand the electronic band structure\nand the rich phase diagram of transition metal pnictides.",
        "Strongly driven nonlinear systems are frequently encountered in physics, yet\ntheir accurate control is generally challenging due to the intricate dynamics.\nIn this work, we present a non-perturbative, semi-analytical framework for\ntailoring such systems. The key idea is heuristically extending the Floquet\ntheory to nonlinear differential equations using the Harmonic Balance method.\nAdditionally, we establish a novel constrained optimization technique inspired\nby the Lagrange multiplier method. This approach enables accurate engineering\nof effective potentials across a broader parameter space, surpassing the\nlimitations of perturbative methods. Our method offers practical\nimplementations in diverse experimental platforms, facilitating nonclassical\nstate generation, versatile bosonic quantum simulations, and solving complex\noptimization problems across quantum and classical applications."
      ]
    }
  },
  {
    "id":2411.19844,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"The structure of musical harmony as an ordered phase of sound: A statistical mechanics approach to music theory",
    "start_abstract":"Music, while allowing nearly unlimited creative expression, almost always conforms to a set of rigid rules at fundamental level. The description and study these rules, the ordered structures that arise from them, is basis field music theory. Here, I present theoretical formalism aims explain why basic patterns emerge in music, using same statistical mechanics framework describes emergent order across phase transitions physical systems. first apply mean approximation demonstrate occur this model disordered sound discrete sets pitches, including 12-fold octave division used Western music. Beyond model, use numerical simulation uncover musical harmony. These results provide new lens through which view discover ideas explore.",
    "start_categories":[
      "cs.NA"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "A New Kind of Science"
      ],
      "abstract":[
        "3R3. A New Kind of Science. - S Wolfram (Wolfram Res Inc, 100 Trade Center Dr, Champaign IL 61820-7237). Media, Champaign, IL. 2002. 1197 pp. ISBN 1-57955-008-8. $44.95. Reviewed by M Gad-el-Hak (Eng Build, Rm 303, Virginia Commonwealth Univ, 601 W Main St, PO Box 843015, Richmond, VA 23284-3015). Reviewing Science is like stepping in a minefield. The danger lies going against the deluge praise, proving relevance to this audience, and arguing proposed new science that allegedly set replace science, as we know it. Those issues will be addressed turn, but first brief background. Stephen considered many have been child prodigy: journal paper particle physics at age 15; stint Oxford; PhD from Caltech 20; youngest recipient MacArthur Prize; faculty positions Caltech, Princeton, Illinois; significant contributions cellular automata complexity theory; developer popular software Mathematica; successful entrepreneur, becoming multi-millionaire 30. Running his company via e-mail videoconference, spent last 10 years virtual seclusion, relentlessly, tirelessly, secretly, nocturnally working on an idea possessed him: generating simple computations, algorithms only few lines. book, targeting both scientists non-scientists, partially about using rules generate complex patterns. In task, author has succeeded beyond reproach not showing can done brilliantly beautifully, also explaining it lucidly enough for all understand, appreciate, savor. opinion several reviewers, including one, aspect book tour de force clarity, elegance, simplicity. problem huge leap takes since nature computer-generated patterns look or behave similarly natural man-made things around us\u2014a snow flake, turbulent flow, lung, mollusk shell, traffic jam, outbreak starfish coral reef, entire universe\u2014therefore must way works. Nature runs its course same computer program. That essence science: yield secrets universe, solve our long-standing problems, provide theory everything. More flight fancy later. Deluge: was widely anticipated before actual publication. Published May 14, 2002, quickly became Amazon.com bestseller promptly reviewed scientific press. Heavyweights former included York Times, Chicago Tribune, Newsweek, Time, Daily Telegraph, Le Monde, Frankfurter Allgemeine Zeitung, Economist. Except last, press went gaga over touting author's claim stand existing head. Economist (p 79, June 1, 2002) more subdued even provocatively titling review \"The Emperor's Theory.\" press, reviews were somewhat less glorious skeptical. Physics Today 55, July 2002), Leo Kadanoff's once pointed, subtle polite, concluding he cannot support view any \"new kind science\" displayed Wolfram's book. Newsweek 59, 27, quoted famed physicist Freeman Dyson: \"There's tradition approaching senility come up with grand, improbable theories. unusual he's doing 40s.\" Kadanoff Dyson express minority opinion, however, majority reviewers being excited reason every human mystery currently depressed stock market, free will, quantum field theory, entropy. For present reviewer, lurks high particularly so months behind who already anointed Isaac Newton 21st century. Relevance: As aims replacing readers Applied Mechanics Reviews stake matter. Mechanics\u2014classical most part occasionally quantum\u2014is underlying branch upon which almost applied mechanics based. mathematics here often form partial differential equations, where space time are indefinitely divisible continuum. example, most, all, fluid flows described well-known, well-posed Navier\u2013Stokes equations. those first-principles equations solved agreement experiment reproach. It problem, such frustrated scores him. search simpler alternative is, therefore, quite alluring. mechanics, when they solved, powerful predictive tool explain mechanical world us well help design machines. When analytical solutions unattainable, discretized brute numerical integration used. But possible some situations, example realistic high-Reynolds-number other multi-scale problems required computational memory speed overwhelm today's supercomputers. impenetrable certain degree empiricism introduced relatively faster computations then proceed. Heuristic turbulence modeling compromise. Despite limitations, traditional works exceedingly well, mechanicians happily practice their craft. Readers should, care passionately if laws supplanted science. Argument: Cellular late 1940s John von Neumann Stanislaw Ulam, although claims independently discovered three decades discrete dynamical systems whose behavior completely specified terms repetitive local relation. continuum represented uniform one-, two-, three-dimensional grid, each cell containing single bit data, 0 red, white, blue, etc, bits states. advances steps. state cell, location, computed step algorithm priori defined close neighbors. Simple programs could, fact, result researched one-dimensional arranged line. data updated based value two nearest cells. methodically studied identified total 256 different rules. Space\u2013time diagrams generated show four distinct patterns: dull uniformity; periodic time-dependence; fractal behavior; truly non-repetitive says broken than 300 fix \"errors\" Darwin, Newton, great ones corrected all. proposes radical notion development world, uncover fundamental universe. pattern-generating capabilities supplant difficult-to-solve yet-to-be-found just because resemble does mean work way. Furthermore, believed represent reality used make predictions agree observations. This Galileo's paradigm underpinning modern explanatory power authority stem ability verifiable predictions; otherwise mere post-hoc speculation. exactly what is. games speculation possibly compete horsepower F=ma E=mc2.Wolfram's boasting, throughout 1200 pages, minimum excessive. He writes, \"I vastly I ever thought possible, fact now touches area besides.\" writes ideas originating him, credits belong elsewhere. Alan Turing conceptualized simplest universal computer, machine. Thinking universe vast digital brainchild Edward Fredkin. use machine environment physical detailed Tommaso Toffoli Norman Margolus. Other Per Bak, Charles Bennett, Hans Meinhardt percolate properly credited. Writing person, relegating notes 350 pages grudgingly dismissively mentioning names, restricting list references own publications, dispel important shortcoming. took approach bypassing peer process. self-published acting author, editor, publisher. opening paragraph mostly favorable Time's (May 20, worth reflecting on: \"Cranks occupational hazard scientist eventually faces. Fortunately, these characters usually easy spot. If someone grand overturns centuries knowledge\u2014especially spans unrelated fields biology economics\u2014the odds good she crank. publishes standard journals general readers, watch out. And issued rather conventional publisher, case pretty much airtight.\" extravagant cold fusion\u2014a` la Stanley Pons Martin Fleischman\u2014and deserve proportionally vigilant scrutiny. validated nor subjected process rest mortals expected do. contrast old anti-Newtonian model predict anything. emperor no clothes. offense play brick build edifice call Bottom Line: fun reading pictures, bad recommendation. inspiration, read Newton's Principia Mathematica, Latin. solving Newtonian framework still best bet, one's better books mechanics."
      ],
      "categories":[
        "cs.FL"
      ]
    },
    "list":{
      "title":[
        "A \"cubist\" decomposition of the Handel-Mosher axis bundle and the\n  conjugacy problem for $\\mathrm{Out}(F_r)$",
        "Design of Cavity Backed Slotted Antenna using Machine Learning\n  Regression Model",
        "A numerical scheme for a multi-scale model of thrombus in arteries",
        "A Novel P-bit-based Probabilistic Computing Approach for Solving the 3-D\n  Protein Folding Problem",
        "Discovering Dataset Nature through Algorithmic Clustering based on\n  String Compression",
        "Modeling Driver Behavior in Speed Advisory Systems: Koopman-based\n  Approach with Online Update",
        "The effect of thermal misbalance on magnetohydrodynamic modes in coronal\n  magnetic cylinders",
        "The steady inviscid compressible self-similar flows and the stability\n  analysis",
        "Tactical Asset Allocation with Macroeconomic Regime Detection",
        "Clustering by Nonparametric Smoothing",
        "Vacuum permittivity and gravitational refractive index revisited",
        "Deceptive Sequential Decision-Making via Regularized Policy Optimization",
        "Feedback control solves pseudoconvex optimal tracking problems in\n  nonlinear dynamical systems",
        "Axion Emission from Proton Cooper Pairs in Neutron Stars",
        "Some characterizations of weak left braces",
        "Data-driven continuation of patterns and their bifurcations",
        "Spaces of subgroups of toral groups",
        "Geometric properties for a certain subclass of normalized harmonic\n  mappings",
        "A new tail bound for the sum of bounded independent random variables",
        "SUSY transformation as the coupler of non-interacting systems",
        "Joint State-Parameter Estimation for the Reduced Fracture Model via the\n  United Filter",
        "Recent advances in high-dimensional quantum frequency combs",
        "Optimal Spectral Transitions in High-Dimensional Multi-Index Models",
        "Star Formation Rates, Metallicities, and Stellar Masses on kpc-scales in\n  TNG50",
        "Propagation of optical solitons in the dielectric medium of a liquid\n  csystal",
        "ContinuouSP: Generative Model for Crystal Structure Prediction with\n  Invariance and Continuity",
        "Phase space geometry of collective spin systems: Scaling and Fractality",
        "Flat band driven itinerant magnetism in the Co-pnictides\n  (La,Ca)Co$_2$(As,P)$_2$",
        "Semi-analytical Engineering of Strongly Driven Nonlinear Systems Beyond\n  Floquet and Perturbation Theory"
      ],
      "abstract":[
        "We show that the axis bundle of a nongeometric fully irreducible outer\nautomorphism admits a canonical \"cubist\" decomposition into branched cubes that\nfit together with special combinatorics. From this structure, we locate a\ncanonical finite collection of periodic fold lines in each axis bundle. This\ngives a solution to the conjugacy problem in $\\mathrm{Out}(F_r)$ for fully\nirreducible outer automorphisms. This can be considered as an analogue of\nresults of Hamenst\\\"adt and Agol from the surface setting, which state that the\nset of trivalent train tracks carrying the unstable lamination of a\npseudo-Anosov map can be given the structure of a CAT(0) cube complex, and that\nthere is a canonical periodic fold line in this cube complex.",
        "In this paper, a regression-based machine learning model is used for the\ndesign of cavity backed slotted antenna. This type of antenna is commonly used\nin military and aviation communication systems. Initial reflection coefficient\ndata of cavity backed slotted antenna is generated using electromagnetic\nsolver. These reflection coefficient data is then used as input for training\nregression-based machine learning model. The model is trained to predict the\ndimensions of cavity backed slotted antenna based on the input reflection\ncoefficient for a wide frequency band varying from 1 GHz to 8 GHz. This\napproach allows for rapid prediction of optimal antenna configurations,\nreducing the need for repeated physical testing and manual adjustments, may\nlead to significant amount of design and development cost saving. The proposed\nmodel also demonstrates its versatility in predicting multi frequency resonance\nacross 1 GHz to 8 GHz. Also, the proposed approach demonstrates the potential\nfor leveraging machine learning in advanced antenna design, enhancing\nefficiency and accuracy in practical applications such as radar, military\nidentification systems and secure communication networks.",
        "In this article, the time-discretization of the fluid structure interaction\nmodel in the three-dimensional boundary domain is taken into account, which\nexplains the mechanical interaction between the blood flow and the Hookean\nelasticity. The interface between the two phases is given by a soft transition\nlayer and spreads to the finite thickness. On the implicit Euler scheme for\nthis discretization, We derive a variety of priori estimates and then use the\nFaedo-Galerkin method to prove the local well-poseedness results.",
        "In the post-Moore era, the need for efficient solutions to non-deterministic\npolynomial-time (NP) problems is becoming more pressing. In this context, the\nIsing model implemented by the probabilistic computing systems with\nprobabilistic bits (p-bits) has attracted attention due to the widespread\navailability of p-bits and support for large-scale simulations. This study\nmarks the first work to apply probabilistic computing to tackle protein\nfolding, a significant NP-complete problem challenge in biology. We represent\nproteins as sequences of hydrophobic (H) and polar (P) beads within a\nthree-dimensional (3-D) grid and introduce a novel many-body interaction-based\nencoding method to map the problem onto an Ising model. Our simulations show\nthat this approach significantly simplifies the energy landscape for short\npeptide sequences of six amino acids, halving the number of energy levels.\nFurthermore, the proposed mapping method achieves approximately 100 times\nacceleration for sequences consisting of ten amino acids in identifying the\ncorrect folding configuration. We predicted the optimal folding configuration\nfor a peptide sequence of 36 amino acids by identifying the ground state. These\nfindings highlight the unique potential of the proposed encoding method for\nsolving protein folding and, importantly, provide new tools for solving similar\nNP-complete problems in biology by probabilistic computing approach.",
        "Text datasets can be represented using models that do not preserve text\nstructure, or using models that preserve text structure. Our hypothesis is that\ndepending on the dataset nature, there can be advantages using a model that\npreserves text structure over one that does not, and viceversa. The key is to\ndetermine the best way of representing a particular dataset, based on the\ndataset itself. In this work, we propose to investigate this problem by\ncombining text distortion and algorithmic clustering based on string\ncompression. Specifically, a distortion technique previously developed by the\nauthors is applied to destroy text structure progressively. Following this, a\nclustering algorithm based on string compression is used to analyze the effects\nof the distortion on the information contained in the texts. Several\nexperiments are carried out on text datasets and artificially-generated\ndatasets. The results show that in strongly structural datasets the clustering\nresults worsen as text structure is progressively destroyed. Besides, they show\nthat using a compressor which enables the choice of the size of the\nleft-context symbols helps to determine the nature of the datasets. Finally,\nthe results are contrasted with a method based on multidimensional projections\nand analogous conclusions are obtained.",
        "Accurate driver behavior modeling is essential for improving the interaction\nand cooperation of the human driver with the driver assistance system. This\npaper presents a novel approach for modeling the response of human drivers to\nvisual cues provided by a speed advisory system using a Koopman-based method\nwith online updates. The proposed method utilizes the Koopman operator to\ntransform the nonlinear dynamics of driver-speed advisory system interactions\ninto a linear framework, allowing for efficient real-time prediction. An online\nupdate mechanism based on Recursive Least Squares (RLS) is integrated into the\nKoopman-based model to ensure continuous adaptation to changes in driver\nbehavior over time. The model is validated using data collected from a\nhuman-in-the-loop driving simulator, capturing diverse driver-specific\ntrajectories. The results demonstrate that the offline learned Koopman-based\nmodel can closely predict driver behavior and its accuracy is further enhanced\nthrough an online update mechanism with the RLS method.",
        "This study investigates the dispersion of magnetohydrodynamic waves\ninfluenced by thermal misbalance in a cylindrical configuration with a finite\naxial magnetic field within solar coronal plasmas. Specifically, it examines\nhow thermal misbalance, characterized by two distinct timescales directly\nlinked to the cooling and heating functions, influences the dispersion\nrelation. This investigation is a key approach for understanding non-adiabatic\neffects on the behaviour of these waves. Our findings reveal that the effect of\nthermal misbalance on fast sausage and kink modes, consistent with previous\nstudies on slabs, is small but slightly more pronounced than previously\nthought. The impact is smaller at long-wavelength limits but increases at\nshorter wavelengths, leading to higher damping rates. This minor effect on fast\nmodes occurs despite the complex interaction of thermal misbalance terms within\nthe dispersion relation, even at low-frequency limits defined by the\ncharacteristic timescales. Additionally, a very small amplification is\nobserved, indicating a suppressed damping state for the long-wavelength\nfundamental fast kink mode. In contrast, slow magnetoacoustic modes are\nsignificantly affected by thermal misbalance, with the cusp frequency shifting\nslightly to lower values, which is significant for smaller longitudinal\nwavenumbers. This thermal misbalance likely accounts for the substantial\nattenuation observed in the propagation of slow magnetoacoustic waves within\nthe solar atmosphere. The long-wavelength limit leads to an analytical\nexpression that accurately describes the frequency shifts in slow modes due to\nmisbalance, closely aligning with both numerical and observational results.",
        "We investigate the steady inviscid compressible self-similar flows which\ndepends only on the polar angle in spherical coordinates. It is shown that\nbesides the purely supersonic and subsonic self-similar flows, there exists\npurely sonic flows, Beltrami flows with a nonconstant proportionnality factor\nand smooth transonic self-similar flows with large vorticity. For a constant\nsupersonic incoming flow past an infinitely long circular cone, a conic shock\nattached to the tip of the cone will form, provided the opening angle of the\ncone is less than a critical value. We introduce the shock polar for the radial\nand polar components of the velocity and show that there exists a monotonicity\nrelation between the shock angle and the radial velocity, which seems to be new\nand not been observed before. If a supersonic incoming flow is self-similar\nwith nonzero azimuthal velocity, a conic shock also form attached to the tip of\nthe cone. The state at the downstream may change smoothly from supersonic to\nsubsonic, thus the shock can be supersonic-supersonic, supersonic-subsonic and\neven supersonic-sonic where the shock front and the sonic front coincide. We\nfurther investigate the structural stability of smooth self-similar\nirrotational transonic flows and analyze the corresponding linear mixed type\nsecond order equation of Tricomi type. By exploring some key properties of the\nself-similar solutions, we find a multiplier and identify a class of admissible\nboundary conditions for the linearized mixed type second-order equation. We\nalso prove the existence and uniqueness of a class of smooth transonic flows\nwith nonzero vorticity which depends only on the polar and azimuthal angles in\nspherical coordinates.",
        "This paper extends the tactical asset allocation literature by incorporating\nregime modeling using techniques from machine learning. We propose a novel\nmodel that classifies current regimes, forecasts the distribution of future\nregimes, and integrates these forecasts with the historical performance of\nindividual assets to optimize portfolio allocations. Utilizing a macroeconomic\ndata set from the FRED-MD database, our approach employs a modified k-means\nalgorithm to ensure consistent regime classification over time. We then\nleverage these regime predictions to estimate expected returns and\nvolatilities, which are subsequently mapped into portfolio allocations using\nvarious sizing schemes. Our method outperforms traditional benchmarks such as\nequal-weight, buy-and-hold, and random regime models. Additionally, we are the\nfirst to apply a regime detection model from a large macroeconomic dataset to\ntactical asset allocation, demonstrating significant improvements in portfolio\nperformance. Our work presents several key contributions, including a novel\ndata-driven regime detection algorithm tailored for uncertainty in forecasted\nregimes and applying the FRED-MD data set for tactical asset allocation.",
        "A novel formulation of the clustering problem is introduced in which the task\nis expressed as an estimation problem, where the object to be estimated is a\nfunction which maps a point to its distribution of cluster membership. Unlike\nexisting approaches which implicitly estimate such a function, like Gaussian\nMixture Models (GMMs), the proposed approach bypasses any explicit modelling\nassumptions and exploits the flexible estimation potential of nonparametric\nsmoothing. An intuitive approach for selecting the tuning parameters governing\nestimation is provided, which allows the proposed method to automatically\ndetermine both an appropriate level of flexibility and also the number of\nclusters to extract from a given data set. Experiments on a large collection of\npublicly available data sets are used to document the strong performance of the\nproposed approach, in comparison with relevant benchmarks from the literature.\nR code to implement the proposed approach is available from\nhttps:\/\/github.com\/DavidHofmeyr\/ CNS",
        "The present paper reanalyzes the problem of the refractive properties of the\nphysical vacuum and their modification under the action of the gravitational\nfield and the electromagnetic field. This problem was studied in our previous\nworks and in the subsequent works of the researchers: Leuchs, Urban, Mainland\nand their collaborators. By modeling the physical vacuum as a\nparticle-antiparticle system, we can deduce with a certain approximation, in a\nsemiclassical theory, the properties of the free vacuum and the vacuum modified\nby the interaction with a gravitational field and an electromagnetic field.\nMore precise calculation of permittivities of free vacuum and near a particle\ncan lead to a non-point model of the particle. This modeling can follow both\nthe quantum and the general relativistic path as well as the phenomenological\npath, the results complementing each other.",
        "Autonomous systems are increasingly expected to operate in the presence of\nadversaries, though an adversary may infer sensitive information simply by\nobserving a system, without even needing to interact with it. Therefore, in\nthis work we present a deceptive decision-making framework that not only\nconceals sensitive information, but in fact actively misleads adversaries about\nit. We model autonomous systems as Markov decision processes, and we consider\nadversaries that attempt to infer their reward functions using inverse\nreinforcement learning. To counter such efforts, we present two regularization\nstrategies for policy synthesis problems that actively deceive an adversary\nabout a system's underlying rewards. The first form of deception is\n``diversionary'', and it leads an adversary to draw any false conclusion about\nwhat the system's reward function is. The second form of deception is\n``targeted'', and it leads an adversary to draw a specific false conclusion\nabout what the system's reward function is. We then show how each form of\ndeception can be implemented in policy optimization problems, and we\nanalytically bound the loss in total accumulated reward that is induced by\ndeception. Next, we evaluate these developments in a multi-agent sequential\ndecision-making problem with one real agent and multiple decoys. We show that\ndiversionary deception can cause the adversary to believe that the most\nimportant agent is the least important, while attaining a total accumulated\nreward that is $98.83\\%$ of its optimal, non-deceptive value. Similarly, we\nshow that targeted deception can make any decoy appear to be the most important\nagent, while still attaining a total accumulated reward that is $99.25\\%$ of\nits optimal, non-deceptive value.",
        "Achieving optimality in controlling physical systems is a profound challenge\nacross diverse scientific and engineering fields, spanning neuromechanics,\nbiochemistry, autonomous systems, economics, and beyond. Traditional solutions,\nrelying on time-consuming offline iterative algorithms, often yield limited\ninsights into fundamental natural processes. In this work, we introduce a\nnovel, causally deterministic approach, presenting the closed-form optimal\ntracking controller (OTC) that inherently solves pseudoconvex optimization\nproblems in various fields. Through rigorous analysis and comprehensive\nnumerical examples, we demonstrate OTC's capability of achieving both high\naccuracy and rapid response, even when facing high-dimensional and\nhigh-dynamical real-world problems. Notably, our OTC outperforms\nstate-of-the-art methods by, e.g., solving a 1304-dimensional neuromechanics\nproblem 1311 times faster or with 113 times higher accuracy. Most importantly,\nOTC embodies a causally deterministic system interpretation of optimality\nprinciples, providing a new and fundamental perspective of optimization in\nnatural and artificial processes. We anticipate our work to be an important\nstep towards establishing a general causally deterministic optimization theory\nfor a broader spectrum of system and problem classes, promising advances in\nunderstanding optimality principles in complex systems.",
        "We investigate axion emission from singlet proton Cooper pairs in neutron\nstars, a process that dominates axion emission in young neutron stars in the\nKSVZ model. By re-deriving its emissivity, we confirm consistency with most\nexisting literature, except for a recent study that exhibits a different\ndependence on the effective mass. This discrepancy results in more than an\norder-of-magnitude deviation in emissivity, significantly impacting constraints\non the KSVZ axion from the cooling observations of the Cassiopeia A neutron\nstar. Furthermore, we examine uncertainties arising from neutron-star equations\nof state and their role in the discrepancy, finding that the large deviation\npersists regardless of the choice of equations of state.",
        "As generalizations of skew left braces, weak left braces were introduced\nrecently by Catino, Mazzotta, Miccoli and Stefanelli to study ceratin special\ndegenerate set-theoretical solutions of the Yang-Baxter equation. In this note,\nas analogues of the notions of regular subgroups of holomorph of groups, Gamma\nfunctions on groups and affine and semi-affine structures on groups, we propose\nthe notions of good inverse subsemigroups and Gamma functions associated to\nClifford semigroups and affine structures on inverse semigroups, respectively,\nby which weak left braces are characterized. Moreover, symmetric,\n$\\lambda$-homomorphic and $\\lambda$-anti-homomorphic weak left braces are\nintroduced and the algebraic structures of these weak left braces are given.",
        "Patterns and nonlinear waves, such as spots, stripes, and rotating spirals,\narise prominently in many natural processes and in reaction-diffusion models.\nOur goal is to compute boundaries between parameter regions with different\nprevailing patterns and waves. We accomplish this by evolving randomized\ninitial data to full patterns and evaluate feature functions, such as the\nnumber of connected components or their area distribution, on their sublevel\nsets. The resulting probability measure on the feature space, which we refer to\nas pattern statistics, can then be compared at different parameter values using\nthe Wasserstein distance. We show that arclength predictor-corrector\ncontinuation can be used to trace out transition and bifurcation curves in\nparameter space by maximizing the distance of the pattern statistics. The\nutility of this approach is demonstrated through a range of examples involving\nhomogeneous states, spots, stripes, and spiral waves.",
        "We study the space of conjugacy classes of subgroups of a compact Lie group G\nwhose identity component is a torus, and consider how various invariants of\nsubgroups behave as sheaves over this space. This feeds in to the author's\nprogramme to give algebraic models of rational G-equivariant cohomology\ntheories.\n  The methods are illustrated by making the outcome explicit for all toral\nsubgroups of compact connected rank 2 groups.",
        "Let $\\mathcal{H}$ be the class of harmonic functions $f=h+\\overline{g}$ in\nthe unit disk $\\mathbb{D}:=\\{z\\in\\mathbb{C}:|z|<1\\}$, where $h$ and $g$ are\nanalytic in $\\mathbb{D}$ with the normalization $h(0)=g(0)=h'(0)-1=0$. Let\n$\\mathcal{P}_{\\mathcal{H}}^0(\\alpha,M)$ denote the subclass of $\\mathcal{H}$ in\n$\\mathbb{D}$ satisfying $\\text{Re}\\left((1-\\alpha)h'(z)+\\alpha\nzh''(z)\\right)>-M+\\left|(1-\\alpha)g'(z)+\\alpha zg''(z)\\right|$ with $g'(0)=0$,\n$M>0$ and $\\alpha\\in(0,1]$. In this paper, we investigate some fundamental\nproperties of functions belonging to the class\n$\\mathcal{P}_{\\mathcal{H}}^0(\\alpha,M)$, including coefficient bounds, growth\nestimates, convexity, starlikeness, convex combinations, and convolution.",
        "We construct a new tail bound for the sum of independent random variables for\nsituations in which the expected value of the sum is known and each random\nvariable lies within a specified interval, which may be different for each\nvariable. This new bound can be computed by solving a two-dimensional convex\noptimization problem. Simulations demonstrate that the new bound is often\nsubstantially tighter than Hoeffding's inequality for cases in which both\nbounds are applicable.",
        "Quasi-one-dimensional chains of atoms can be effectively described by\none-dimensional Dirac-type equation. Crystal structure of the chain is\nreflected by pseudo-spin of the quasi-particles. In the article, we present a\nsimple framework where supersymmetric transformation is utilized to generate an\ninteraction between two, initially non-interacting systems described by\npseudo-spin-one Dirac-type equation. In the presented example, the\ntransformation converts two asymptotically non-interacting atomic chains into a\nsaw chain locally. The model possesses a flat band whose energy can be\nfine-tuned deliberately.",
        "In this paper, we introduce an effective United Filter method for jointly\nestimating the solution state and physical parameters in flow and transport\nproblems within fractured porous media. Fluid flow and transport in fractured\nporous media are critical in subsurface hydrology, geophysics, and reservoir\ngeomechanics. Reduced fracture models, which represent fractures as\nlower-dimensional interfaces, enable efficient multi-scale simulations.\nHowever, reduced fracture models also face accuracy challenges due to modeling\nerrors and uncertainties in physical parameters such as permeability and\nfracture geometry. To address these challenges, we propose a United Filter\nmethod, which integrates the Ensemble Score Filter (EnSF) for state estimation\nwith the Direct Filter for parameter estimation. EnSF, based on a score-based\ndiffusion model framework, produces ensemble representations of the state\ndistribution without deep learning. Meanwhile, the Direct Filter, a recursive\nBayesian inference method, estimates parameters directly from state\nobservations. The United Filter combines these methods iteratively: EnSF\nestimates are used to refine parameter values, which are then fed back to\nimprove state estimation. Numerical experiments demonstrate that the United\nFilter method surpasses the state-of-the-art Augmented Ensemble Kalman Filter,\ndelivering more accurate state and parameter estimation for reduced fracture\nmodels. This framework also provides a robust and efficient solution for\nPDE-constrained inverse problems with uncertainties and sparse observations.",
        "High-dimensional entanglement in qudit states offers a promising pathway\ntowards the realization of practical, large-scale quantum systems that are\nhighly controllable. These systems can be leveraged for various applications,\nincluding advanced quantum information processing, secure communications,\ncomputation, and metrology. In this context, quantum frequency combs have a\ncrucial role as they inherently support multiple modes in both temporal and\nfrequency domains, while preserving a single spatial mode. The multiple\ntemporal and frequency modes of quantum frequency combs facilitate the\ngeneration, characterization, and control of high-dimensional time-frequency\nentanglement in extensive quantum systems. In this review article, we provide\nan overview of recent technological advancements in high-dimensional\nenergy-time entangled quantum frequency combs. We explore how these\ntime-frequency qudits, achieved using scalable telecommunications-wavelength\ncomponents, can empower the creation of large-scale quantum states. Advances in\nquantum frequency combs can unlock new capabilities and versatility for\npromising developments in quantum science and technology.",
        "We consider the problem of how many samples from a Gaussian multi-index model\nare required to weakly reconstruct the relevant index subspace. Despite its\nincreasing popularity as a testbed for investigating the computational\ncomplexity of neural networks, results beyond the single-index setting remain\nelusive. In this work, we introduce spectral algorithms based on the\nlinearization of a message passing scheme tailored to this problem. Our main\ncontribution is to show that the proposed methods achieve the optimal\nreconstruction threshold. Leveraging a high-dimensional characterization of the\nalgorithms, we show that above the critical threshold the leading eigenvector\ncorrelates with the relevant index subspace, a phenomenon reminiscent of the\nBaik-Ben Arous-Peche (BBP) transition in spiked models arising in random matrix\ntheory. Supported by numerical experiments and a rigorous theoretical\nframework, our work bridges critical gaps in the computational limits of weak\nlearnability in multi-index model.",
        "Integral field units (IFU) have extended our knowledge of galactic properties\nto kpc (or, sometimes, even smaller) patches of galaxies. These scales are\nwhere the physics driving galaxy evolution (feedback, chemical enrichment,\netc.) take place. Quantifying the spatially-resolved properties of galaxies,\nboth observationally and theoretically, is therefore critical to our\nunderstanding of galaxy evolution. To this end, we investigate\nspatially-resolved scaling relations within central galaxies\n($M_\\star>10^{9.0}$) at $z=0$ in IllustrisTNG. We examine both the resolved\nstar-forming main sequence (rSFMS) and the resolved mass-metallicity relation\n(rMZR) using $1~{\\rm kpc}\\times1~{\\rm kpc}$ maps of galaxies. We find that the\nrSFMS in IllustrisTNG is well-described by a power-law, but has some dependence\non the host galaxy's mass. Conversely, the rMZR for IllustrisTNG can be\ndescribed by a single power-law at low stellar mass surface density that\nflattens at high surface densities and is independent of host galaxy mass. We\nfind quantitative agreement in both the rSFMS and rMZR with recent IFU\nobservational campaigns. Furthermore, we argue that the rSFMS is an indirect\nresult of the Schmidt-Kennicutt (SK) law and local gas fraction relation, which\nare both independent of host galaxy properties. Finally, we expand upon a\nlocalized leaky-box model to study the evolution of idealized spaxels and find\nthat it provides a good description of these resolved relations. The degree of\nagreement, however, between idealized spaxels and simulated spaxels depends on\nthe `net' outflow rate for the spaxel, and the observed scaling relations\nindicate a preference for a low net outflow rate.",
        "Aim. Implement a stochastic representation of the wave function for a pair of\nentangled soliton functions in a liquid crystal. Show the applicability of a\nspecial soliton representation of quantum mechanics for modeling real entangled\nsystems. Methodology. The central place in the study is occupied by the method\nof mathematical modeling. As part of the calculation of stochastics by the\nmethod of abstraction and concretization, a detailed mathematical apparatus is\ngiven, adapted to the real physical case. A qualitative analysis of the\nbehavior of the material during the propagation of soliton pulses in it is\ncarried out. Results. The main value of the stochastic theory for a system of\nentangled solitons lies in the possibility of modeling the entangled states of\nreal systems - photons. In the framework of this work, the optical 1D envelopes\nof solitons in a nematic liquid crystal are considered in approximation to the\nconditions of a real physical problem. Research implications. The theoretical\nand\/or practical significance lies in the fundamental possibility of modeling\nreal entangled systems based on the constructed stochastic model of entangled\nsolitons and subsequent creation of special applications on its basis. In\nparticular, there will be a prospect of applying quantum teleportation to the\nproblem of propagation of quantum computing for use among the components of\nquantum computing networks.",
        "The discovery of new materials using crystal structure prediction (CSP) based\non generative machine learning models has become a significant research topic\nin recent years. In this paper, we study invariance and continuity in the\ngenerative machine learning for CSP. We propose a new model, called\nContinuouSP, which effectively handles symmetry and periodicity in crystals. We\nclearly formulate the invariance and the continuity, and construct a model\nbased on the energy-based model. Our preliminary evaluation demonstrates the\neffectiveness of this model with the CSP task.",
        "We examine the scaling of the inverse participation ratio of spin coherent\nstates in the energy basis of three collective spin systems: a bounded harmonic\noscillator, the Lipkin-Meshkov-Glick model, and the Quantum Kicked Top. The\nfinite-size quantum probing provides detailed insights into the structure of\nthe phase space, particularly the relationship between critical points in\nclassical dynamics and their quantum counterparts in collective spin systems.\nWe introduce a finite-size scaling mass exponent that makes it possible to\nidentify conditions under which a power-law behavior emerges, allowing to\nassign a fractal dimension to a coherent state. For the Quantum Kicked Top, the\nfractal dimension of coherent states -- when well-defined -- exhibits three\ngeneral behaviors: one related to the presence of critical points and two\nassociated with regular and chaotic dynamics. The finite-size scaling analysis\npaves the way toward exploring collective spin systems relevant to quantum\ntechnologies within the quantum-classical framework.",
        "Flat bands can induce strong electron correlation effects that help stabilize\nboth magnetic and superconducting states. Here, we carry out angle-resolved\nphotoemission spectroscopy and density functional theory calculations to study\nthe electronic structure of the Co-pnictides CaCo$_2$As$_2$ and LaCo$_2$P$_2$.\nWe find that, while the $k_z$ Fermi topology of ferromagnetic LaCo$_2$P$_2$ is\nmarkedly 2-dimensional, antiferromagnetic CaCo$_2$As$_2$ develops a 3D Fermi\nsurface described by a $zig-zag$-like band dispersion perpendicular to the\nCo-As plane. Furthermore, the magnetism is driven by the electronic\ncorrelations of the flat bands with $d_{xy}$ and $d_{z^2}$ orbital character at\nthe Fermi level. Our results link the electronic dimensionality and the\nmagnetic order, and emphasize the critical role of the As-As and P-P bond\nstrength along the $c$-direction to understand the electronic band structure\nand the rich phase diagram of transition metal pnictides.",
        "Strongly driven nonlinear systems are frequently encountered in physics, yet\ntheir accurate control is generally challenging due to the intricate dynamics.\nIn this work, we present a non-perturbative, semi-analytical framework for\ntailoring such systems. The key idea is heuristically extending the Floquet\ntheory to nonlinear differential equations using the Harmonic Balance method.\nAdditionally, we establish a novel constrained optimization technique inspired\nby the Lagrange multiplier method. This approach enables accurate engineering\nof effective potentials across a broader parameter space, surpassing the\nlimitations of perturbative methods. Our method offers practical\nimplementations in diverse experimental platforms, facilitating nonclassical\nstate generation, versatile bosonic quantum simulations, and solving complex\noptimization problems across quantum and classical applications."
      ]
    }
  },
  {
    "id":2411.06414,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"An EEG-based brain-computer interface for real-time multi-task robotic control",
    "start_abstract":"The Brain Computer Interface (BCI) is the communication between human brain and computer. Electroencephalogram (EEG) one of biomedical signals which can be obtained by attaching electrodes to scalp. Some EEG related applications developed help disabled people, such as based wheelchair or robotic arm. A hybrid BCI real-time control system proposed a multi-tasks robot. In this system, sliding window online data segmentation strategy segment training data, enable learn dynamic features when subject's state transfer from rest task execution state. achieve ensure continuity executing actions. addition, Common Spatial Pattern (CSP) better extract spatial these continuous actions that multiple commands are accurately classified. experiment, three subjects' collected, trained tested performance reliability system. records robot's spending time, moving distance, number objects pushing down. Experimental results given show feasibility Compared remote controller, similar performance. Thus, able robot in environment used develop robot-aided arm methods on neurological rehabilitation principles for stroke injury patients.",
    "start_categories":[
      "cs.RO",
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Braincomputer interfaces for communication and control"
      ],
      "abstract":[
        "For many years people have speculated that electroencephalographic activity or other electrophysiological measures of brain function might provide a new non-muscular channel for sending messages and commands to the external world \u2013 a brain\u2013computer interface (BCI). Over the past 15 years, productive BCI research programs have arisen. Encouraged by new understanding of brain function, by the advent of powerful low-cost computer equipment, and by growing recognition of the needs and potentials of people with disabilities, these programs concentrate on developing new augmentative communication and control technology for those with severe neuromuscular disorders, such as amyotrophic lateral sclerosis, brainstem stroke, and spinal cord injury. The immediate goal is to provide these users, who may be completely paralyzed, or \u2018locked in\u2019, with basic communication capabilities so that they can express their wishes to caregivers or even operate word processing programs or neuroprostheses. Present-day BCIs determine the intent of the user from a variety of different electrophysiological signals. These signals include slow cortical potentials, P300 potentials, and mu or beta rhythms recorded from the scalp, and cortical neuronal activity recorded by implanted electrodes. They are translated in real-time into commands that operate a computer display or other device. Successful operation requires that the user encode commands in these signals and that the BCI derive the commands from the signals. Thus, the user and the BCI system need to adapt to each other both initially and continually so as to ensure stable performance. Current BCIs have maximum information transfer rates up to 10\u201325 bits\/min. This limited capacity can be valuable for people whose severe disabilities prevent them from using conventional augmentative communication methods. At the same time, many possible applications of BCI technology, such as neuroprosthesis control, may require higher information transfer rates. Future progress will depend on: recognition that BCI research and development is an interdisciplinary problem, involving neurobiology, psychology, engineering, mathematics, and computer science; identification of those signals, whether evoked potentials, spontaneous rhythms, or neuronal firing rates, that users are best able to control independent of activity in conventional motor output pathways; development of training methods for helping users to gain and maintain that control; delineation of the best algorithms for translating these signals into device commands; attention to the identification and elimination of artifacts such as electromyographic and electro-oculographic activity; adoption of precise and objective procedures for evaluating BCI performance; recognition of the need for long-term as well as short-term assessment of BCI performance; identification of appropriate BCI applications and appropriate matching of applications and users; and attention to factors that affect user acceptance of augmentative technology, including ease of use, cosmesis, and provision of those communication and control capacities that are most important to the user. Development of BCI technology will also benefit from greater emphasis on peer-reviewed research publications and avoidance of the hyperbolic and often misleading media attention that tends to generate unrealistic expectations in the public and skepticism in other researchers. With adequate recognition and effective engagement of all these issues, BCI systems could eventually provide an important new communication and control option for those with motor disabilities and might also give those without disabilities a supplementary control channel or a control channel useful in special circumstances."
      ],
      "categories":[
        "Neurophysiology"
      ]
    },
    "list":{
      "title":[
        "Calibration-free measurement of the phonon temperature around a single\n  emitter",
        "Stability, growth, and doping of In$_{2}$(Si, Ge)$_{2}$O$_{7}$ as\n  promising n-type wide-gap semiconductors",
        "Critical Dynamics of Spin Boson Model",
        "Modulation of the galactic cosmic ray spectrum in an anisotropic\n  diffusion approach",
        "Non(anti)Commutative Superspace, Baker-Campbell-Hausdorff Closed Forms,\n  and Dirac-K\\\"ahler Twisted Supersymmetry",
        "Quantifying the generation of negatively charged boron vacancies in\n  He-ion irradiated hexagonal boron nitride",
        "Quorum sensing and absorbing phase transitions in colloidal active\n  matter",
        "Detecting high-dimensional time-bin entanglement in fiber-loop systems",
        "An Unconventional Ultra-Sub-Wavelength Receiving Nano-Antenna Activated\n  by ac Spin Pumping and the ac Inverse Spin Hall Effect",
        "Structure-Preserving Neural Ordinary Differential Equations for Stiff\n  Systems",
        "Connectivity and matching extendability of optimal $1$-embedded graphs\n  on the torus",
        "Forcing, genericity and CBERS",
        "Human-Agent Interaction in Synthetic Social Networks: A Framework for\n  Studying Online Polarization",
        "(FAPP) Infinity Does Macroscopic Irreversibility From Microscopic\n  Reversibility",
        "SeqSee: A schema-based approach to spectral sequence visualization",
        "Multi-Modality Representation Learning for Antibody-Antigen Interactions\n  Prediction",
        "On the role of true and false chirality in producing parity violating\n  energy differences",
        "Miniaturized liquid metal composite circuits with energy harvesting\n  coils for battery-free bioelectronics and optogenetics",
        "Multideterminantal measures",
        "On the Mordell-Weil rank and $2$-Selmer group of a family of elliptic\n  curves",
        "Gradient-based Explanations for Deep Learning Survival Models",
        "Parameter Choices for Sparse Multi-Parameter Regularization with the\n  $\\ell_1$ Norm",
        "Rigorous expansions of modular forms at CM points, I: Denominators",
        "Influence of Chemistry and Topography on the Wettability of Copper",
        "Principal component analysis for 5\/2 fractional quantum Hall states",
        "Simulating inverse patchy colloid models",
        "Formalising the intentional stance 2: a coinductive approach",
        "Searching for axion dark matter gegenschein of the Vela supernova\n  remnant with FAST",
        "Generalized network autoregressive modelling of longitudinal networks\n  with application to presidential elections in the USA"
      ],
      "abstract":[
        "The emission properties of a localized solid-state emitter are strongly\ninfluenced by its environment. The coupling to acoustic phonons impacts the\ncoherence of the emitter and its temperature dependence, and also results in\nthe apparition of phonon sidebands besides the sharp zero-phonon line. Here, we\npresent a method for measuring the absolute temperature of a localized emitter\nby directly plotting the ratio of the Stokes and anti-Stokes components of the\nphonon sideband as a function of the shift from the zero-phonon line. This\napproach requires no calibration and knowledge of the system, making it\napplicable to a wide range of emitters and materials. We validate the method\nusing a CdSe quantum dot in a ZnSe nanowire. We thus show that the quantum dot\nis significantly heated under non-resonant excitation when increasing the\nincident power at low temperature and is ascribed to the drop in thermal\nconductivity at these temperatures.",
        "In this paper we investigate, computationally and experimentally, the phase\nstability, electronic structure properties, and the propensity for n-type\ndoping of In$_{2}$X$_{2}$O$_{7}$ (X=Si, Ge) ternary oxides. This family of\nmaterials contains promising novel wide-gap semiconductors based on their\nestimated high $n$-type Baliga figures of merit and acceptable thermal\nconductivity for power electronics applications. Here, we find that both\nIn$_{2}$Si$_{2}$O$_{7}$ and In$_{2}$Ge$_{2}$O$_{7}$ to be n-type dopable, with\nZr providing between 10$^{16}$ and above 10$^{21}$ cm$^{-3}$ net donor\nconcentrations under O-poor conditions, depending on the chemistry, structure\n(ground-state thorvetite or high-pressure pyrochlore) and synthesis\ntemperature. Initial thin-film growth and annealing leads to polycrystalline\nIn$_{2}$Ge$_{2}$O$_{7}$ thin films in thorvetite structure with band gap over 4\neV, and confirms Zr doping predictions by achieving electron concentrations at\n10$^{14}$-10$^{16}$ cm$^{-3}$ under O-rich condition. While future epitaxial\ngrowth development is still needed, this study establishes\nIn$_{2}$X$_{2}$O$_{7}$ as promising n-type wide-gap semiconductors for power\nelectronic applications.",
        "In this work, we study the low-energy properties of the spin-boson model\n(SBM), which describes the dynamics of a 1\/2 spin associated with a thermostat\ncharacterized by a power law spectral density, $f(\\omega)\\propto |\\omega|^s$.\nThe theoretical description is constructed in the Schwinger--Keldysh technique,\nbased on the representation of the 1\/2-spin by Majorana fermions. We study the\ncritical dynamics of the system near the quantum phase transition by\nconstructing and analyzing the system of renormalization group equations. Our\ntheoretical approach is more universal, contrary to the one based on quantum\nclassical mapping, since it is applicable for $0<s\\leq 1$. We show that in both\nthe ohmic case $s=1$, and subohmic case $0<s<1$, the second order quantum phase\ntransition is observed in the model considered, and the critical magnetization\nexponent agrees with the exact hyperscaling result, $1\/\\delta=(1-s)\/(1+s)$.\nFurthermore, we obtain the dependence of the critical value of the spin-boson\ncoupling constant on the temperature of the bosonic thermal bath.",
        "We introduce a novel diffusion model for the propagation of cosmic rays (CRs)\nthat incorporates an anisotropic diffusion tensor of a general form within a\nrealistically modeled large-scale Galactic magnetic field. The parameters of\nthe model are consistent with the contemporary understanding of the large-scale\nGalactic magnetic field structure and the dynamics of small-scale turbulent CR\npropagation. The paper demonstrates the modulation of spectra of Galactic\ncosmic rays (GCRs) in the magnetic rigidity range of 1 - 30 PV (the CR knee)\nand explores the spatial variation of this phenomenon. The observed modulation\nof the spectrum is explained by changes in the leakage mechanism.",
        "Starting from an elementary calculation of super Lie group elements\nassociating with non(anti)-commutative Grassmann parameters, we derive several\nclosed expressions of Baker-Campbell-Hausdorff (BCH) formula which represent\nmultiplication properties of super Lie group elements in the corresponding\nsuperspace. We then show that parametrization of superspace in general may\nbecome infinite dimensional due to the presence of non(anti)commutativity. We\nshow that a Dirac-K\\\"ahler Twisted SUSY Algebra (also referred to as Marcus\nB-type Twisted SUSY Algebra or Geometric Langlands Twisted SUSY Algebra) with a\ncertain type of deformation, which we call an exponential deformation, may\ncircumvent this problem. We also provide, in terms of gauge covariantization of\nthe SUSY algebra, a geometric understanding of the exponential deformation, and\nsee that the framework constructed in this paper may serve as a\nnon(anti)commutative superspace framework providing the gauge covariant link\nformulation of twisted super Yang-Mills on a lattice.",
        "Hexagonal boron nitride (hBN) hosts luminescent defects possessing spin\nqualities compatible with quantum sensing protocols at room temperature.\nVacancies, in particular, are readily obtained via exposure to high-energy ion\nbeams. While the defect creation mechanism via such irradiation is well\nunderstood, the occurrence rate of optically active negatively charged\nvacancies ($V_B^-$) is an open question. In this work, we exploit focused\nhelium ions to systematically generate optically active vacancy defects in hBN\nflakes at varying density. By comparing the density-dependent spin splitting\nmeasured by magnetic resonance to calculations based on a microscopic charge\nmodel, in which we introduce a correction term due to a constant background\ncharge, we are able to quantify the number of $V_B^-$ defects generated by the\nion irradiation. We find that only a small fraction (0.2%) of all vacancies is\nin the optically active, negatively charged state. Our results provide a\nprotocol for measuring the generation efficiency of $V_B^-$, which is necessary\nfor understanding and optimizing luminescent centers in hBN.",
        "Unlike biological active matter that constantly adapt to their environment,\nthe motors of synthetic active particles are typically agnostic to their\nsurroundings and merely operate at constant force. Here, we design colloidal\nactive rods capable of modulating their inner activity in response to crowding,\nthereby enforcing a primitive form of quorum sensing interactions. Through\nexperiments, simulations and theory we elucidate the impact of these\ninteractions on the phase behavior of isotropic active matter. We demonstrate\nthat, when conditioned to density, motility regulation can either lead to an\nabsorbing phase transition, where all particles freeze their dynamics, or to\natypical phase separation, where flat interfaces supporting a net pressure drop\nare in mechanical equilibrium. Fully active and fully arrested particles can\nthen form heterogeneous patterns ruled by the competition between quorum\nsensing and mechanical interactions. Beyond the specifics of motile colloids,\nwe expect our findings to apply broadly to adaptive active matter assembled\nfrom living or synthetic units.",
        "Many quantum communication protocols rely on the distribution of entanglement\nbetween the different participating parties. One example is quantum key\ndistribution (QKD), an application that has matured to commercial use in recent\nyears. However, difficulties remain, especially with noise resilience and\nchannel capacity in long-distance communication. One way to overcome these\nproblems is to use high-dimensional entanglement, which has been shown to be\nmore robust to noise and enables higher secret-key rates. It is therefore\nimportant to have access to certifiable high-dimensional entanglement sources\nto confidently implement these advanced QKD protocols. Here, we develop a\nmethod for certifying high-dimensional time-bin entanglement in fiber-loop\nsystems. In these systems, entanglement creation and detection can utilize the\nsame physical components, and the number of time bins, and thus the\nentanglement dimension, can be adapted without making physical changes to the\nsetup. Our certification method builds on previous proposals for the\ncertification of angular-momentum entanglement in photon pairs. In particular,\nmeasurements in only two experimentally accessible bases are sufficient to\nobtain a lower bound on the entanglement dimension for both two- and\nmultiphoton quantum states. Numerical simulations show that the method is\nrobust against typical experimental noise effects and works well even with\nlimited measurement statistics, thus establishing time-bin encoded photons as a\npromising platform for high-dimensional quantum-communication protocols.",
        "We report an extreme sub-wavelength unconventional receiving antenna. It\nconsists of an array of nanomagnets connected to heavy metal nanostrips.\nIncident electromagnetic (EM) radiation generates intrinsic and extrinsic spin\nwaves in the nanomagnets, which pump spin into the heavy metal nanostrips at\ntheir own frequencies giving rise to a polychromatic alternating voltage across\nthe latter owing to the ac inverse spin Hall effect. This implements a\nreceiving nano-antenna. We demonstrate its operation at two different EM wave\nfrequencies of 1.5 GHz and 2.4 GHz - the latter being the Bluetooth and Wi-Fi\nfrequency. We measure the receiving gain at 2.4 GHz to be approximately -9 db.\nThe free space radiated wavelength \"lambda\" at 2.4 GHz is 12.5 cm while the\nantenna area A is merely 160 micron^2, making the ratio A\/lambda^2 =\n0.97x10^-8. This antenna's receiving gain should be very poor because of the\ntiny size. Yet the measured gain is more than 4000 times larger than the\ntheoretical limit for a conventional antenna of this size at this wavelength\nbecause of the unconventional operating principle.",
        "Neural ordinary differential equations (NODEs) are an effective approach for\ndata-based modeling of dynamical systems arising from simulations and\nexperiments. One of the major shortcomings of NODEs, especially when coupled\nwith explicit integrators, is its long-term stability, which impedes their\nefficiency and robustness when encountering stiff problems. In this work we\npresent a structure-preserving NODE approach, which integrates with a linear\nand nonlinear split and an exponential integrator, latter of which is an\nexplicit integrator with stability properties comparable to implicit methods.\nWe demonstrate that our model has advantages in both learning and deployment\nover standard explicit or even implicit NODE methods. The long-time stability\nis further enhanced by the Hurwitz matrix decomposition that constrains the\nspectrum of the linear operator, therefore stabilizing the linearized dynamics.\nWhen combined with a Lipschitz-controlled neural network treatment for the\nnonlinear operator, we show the nonlinear dynamics of the NODE are provably\nstable in the sense of Lyapunov. For high-dimensional data, we further rely on\nan autoencoder performing dimension reduction and Higham's algorithm for the\nmatrix-free application of the matrix exponential on a vector. We demonstrate\nthe effectiveness of the proposed NODE approach in various examples, including\nthe Grad-13 moment equations and the Kuramoto-Sivashinky equation.",
        "In this paper, we discuss optimal $1$-toroidal graphs (abbreviated as O1TG),\nwhich are drawn on the torus so that every edge crosses another edge at most\nonce, and has $n$ vertices and exactly $4n$ edges. We first consider\nconnectivity of O1TGs, and give the characterization of O1TGs having\nconnectivity exactly $k$ for each $k\\in \\{4, 5, 6, 8\\}$. In our argument, we\nalso show that there exists no O1TG having connectivity exactly $7$.\nFurthermore, using the result above, we discuss extendability of matchings, and\ngive the characterization of $1$-, $2$- and $3$-extendable O1TGs in turn.",
        "In this paper we continue the study of equivalence of generics filters\nstarted by Smythe in [Smy22]. We fully characterize those forcing posets for\nwhich the corresponding equivalence of generics is smooth using the purely\ntopological property of condensation. Next we leverage our characterization to\nshow that there are non-homogeneous forcing for which equivalence of generics\nis not smooth. Then we prove hyperfiniteness in the case of Prikry forcing and\nsome additional results addressing the problem whether generic equivalence for\nCohen forcing is hyperfinite.",
        "Online social networks have dramatically altered the landscape of public\ndiscourse, creating both opportunities for enhanced civic participation and\nrisks of deepening social divisions. Prevalent approaches to studying online\npolarization have been limited by a methodological disconnect: mathematical\nmodels excel at formal analysis but lack linguistic realism, while language\nmodel-based simulations capture natural discourse but often sacrifice\nanalytical precision. This paper introduces an innovative computational\nframework that synthesizes these approaches by embedding formal opinion\ndynamics principles within LLM-based artificial agents, enabling both rigorous\nmathematical analysis and naturalistic social interactions. We validate our\nframework through comprehensive offline testing and experimental evaluation\nwith 122 human participants engaging in a controlled social network\nenvironment. The results demonstrate our ability to systematically investigate\npolarization mechanisms while preserving ecological validity. Our findings\nreveal how polarized environments shape user perceptions and behavior:\nparticipants exposed to polarized discussions showed markedly increased\nsensitivity to emotional content and group affiliations, while perceiving\nreduced uncertainty in the agents' positions. By combining mathematical\nprecision with natural language capabilities, our framework opens new avenues\nfor investigating social media phenomena through controlled experimentation.\nThis methodological advancement allows researchers to bridge the gap between\ntheoretical models and empirical observations, offering unprecedented\nopportunities to study the causal mechanisms underlying online opinion\ndynamics.",
        "Infinity is central to deriving macroscopic irreversibility from reversible\nmicroscopic laws across mathematics, theoretical computer science and physics.\nIn analysis, infinite processes -- such as Dedekind cuts and Cauchy sequences\n-- construct real numbers as equivalence classes of rational approximations,\nbridging discrete rationals to the continuous real line. In quantum mechanics,\ninfinite tensor products model nested measurements, where sectorization\npartitions the Hilbert space into equivalence classes, reconciling unitary\nevolution with wavefunction collapse. In statistical mechanics, macrostates\nemerge as equivalence classes of microstates sharing identical macroscopic\nproperties, providing the statistical basis for thermodynamic irreversibility\ndespite reversible dynamics. Equivalence relations formalize\nFor-All-Practical-Purposes (FAPP) indistinguishability, reflecting operational\nlimits on precision and observation. Together, these examples reveal a unified\nframework where infinity and equivalence underpin emergent macroscopic behavior\nfrom microscopic reversibility.",
        "We present SeqSee, a software system that addresses spectral sequence\nvisualization through a schema-based approach. By introducing a standardized\nJSON schema as an intermediate representation, SeqSee decouples the\nmathematical computations of spectral sequences from their visualizations. We\ndemonstrate the system through a case study of the classical and C-motivic\nAdams spectral sequences.",
        "While deep learning models play a crucial role in predicting antibody-antigen\ninteractions (AAI), the scarcity of publicly available sequence-structure\npairings constrains their generalization. Current AAI methods often focus on\nresidue-level static details, overlooking fine-grained structural\nrepresentations of antibodies and their inter-antibody similarities. To tackle\nthis challenge, we introduce a multi-modality representation approach that\nintegates 3D structural and 1D sequence data to unravel intricate\nintra-antibody hierarchical relationships. By harnessing these representations,\nwe present MuLAAIP, an AAI prediction framework that utilizes graph attention\nnetworks to illuminate graph-level structural features and normalized adaptive\ngraph convolution networks to capture inter-antibody sequence associations.\nFurthermore, we have curated an AAI benchmark dataset comprising both\nstructural and sequence information along with interaction labels. Through\nextensive experiments on this benchmark, our results demonstrate that MuLAAIP\noutperforms current state-of-the-art methods in terms of predictive\nperformance. The implementation code and dataset are publicly available at\nhttps:\/\/github.com\/trashTian\/MuLAAIP for reproducibility.",
        "In this work we tackle the problem of showing which type of influences can\nlift the degeneracy between truly and falsely chiral systems, showing that only\nwhen both systems and influences are both truly (falsely) chiral, a parity\nviolating energy difference between left- and right-handed systems can be\nproduced. In particular, after considering the enantiomers of a chiral molecule\nas paradigmatic truly chiral systems, we rigorously show, under a quantum field\ntheoretically approach, that only a truly chiral influence such as the\n$Z^{0}$-mediated electroweak interaction can lift the degeneracy between\nenantiomers. On the contrary, we explicitly show that a falsely chiral\ninfluence, such as an axion-mediated interaction in chiral molecules, can not\nlift the aforementioned degeneracy. These results extend Barron's seminal ideas\n[L. D. Barron, True and false chirality and parity violation, Chem. Phys. Lett\n{\\bf 123}, 423 (1986)] to a quantum field theory-based approach.",
        "Over the past years, rapid progress has been made on soft-matter electronics\nfor wearable and implantable devices, for bioelectronics and optogenetics.\nLiquid Metal (LM) based electronics were especially popular, due to their\nlong-term durability, when subject to repetitive strain cycles. However, one\nmajor limitation has been the need for tethering bioelectronics circuits to\nexternal power, or the use of rigid bulky batteries. This has motivated a\ngrowing interest in wireless energy transfer, which demands circuit\nminiaturization. However, miniaturization of LM circuits is challenging due to\nlow LM-substrate adhesion, LM smearing, and challenges on\nmicrochip-interfacing. In this article, we address these challenges by\nhigh-resolution laser-assisted micropatterning of biphasic LM composites and\nvapor-assisted LM microchip soldering. Through development of a search\nalgorithm for optimization of the biphasic ink coil performance, we designed\nand implemented micro coils with trace spacing of 50 {\\mu}m that can harvest a\nsignificant amount of energy (178 mW\/cm2) through near field inductive\ncoupling. We show miniaturized soft-matter circuits with integrated SMD chips\nsuch as NFC chips, capacitors, and LEDs that are implemented in a few minutes\nthrough laser patterning, and vaporassisted soldering. In the context of\noptogenetics, where lightweight, miniaturized systems are needed to provide\noptical stimulation, soft coils stand out in terms of their improved\nconformability and flexibility. Thus, this article explores the applications of\nsoft coils in wearable and implantable devices, with a specific focus on their\nuse in optogenetics.",
        "We define multideterminantal probability measures, a family of probability\nmeasures on $[k]^n$ where $[k]=\\{1,2,\\dots,k\\}$, generalizing determinantal\nmeasures (which correspond to the case $k=2$). We give examples coming from the\npositive Grassmannian, from the dimer model and from the spanning tree model.\n  We also define and completely characterize determinantal probability measures\non the permutation group $S_n$.",
        "We consider the parametric family of elliptic curves over $\\mathbb{Q}$ of the\nform $E_{m} : y^{2} = x(x - n_{1})(x - n_{2}) + t^{2}$, where $n_{1}$, $n_{2}$\nand $t$ are particular polynomial expressions in an integral variable $m$. In\nthis paper, we investigate the torsion group $E_{m}(\\mathbb{Q})_{\\rm{tors}}$, a\nlower bound for the Mordell-Weil rank $r({E_{m}})$ and the $2$-Selmer group\n${\\rm{Sel}}_{2}(E_{m})$ under certain conditions on $m$. This extends the\nprevious works done in this direction, which are mostly concerned with the\nMordell-Weil ranks of various parametric families of elliptic curves.",
        "Deep learning survival models often outperform classical methods in\ntime-to-event predictions, particularly in personalized medicine, but their\n\"black box\" nature hinders broader adoption. We propose a framework for\ngradient-based explanation methods tailored to survival neural networks,\nextending their use beyond regression and classification. We analyze the\nimplications of their theoretical assumptions for time-dependent explanations\nin the survival setting and propose effective visualizations incorporating the\ntemporal dimension. Experiments on synthetic data show that gradient-based\nmethods capture the magnitude and direction of local and global feature\neffects, including time dependencies. We introduce GradSHAP(t), a\ngradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and\nSurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply\nthese methods to medical data with multi-modal inputs, revealing relevant\ntabular features and visual patterns, as well as their temporal dynamics.",
        "This paper introduces a multi-parameter regularization approach using the\n$\\ell_1$ norm, designed to better adapt to complex data structures and problem\ncharacteristics while offering enhanced flexibility in promoting sparsity in\nregularized solutions. As data volumes grow, sparse representations of learned\nfunctions become critical for reducing computational costs during function\noperations. We investigate how the selection of multiple regularization\nparameters influences the sparsity of regularized solutions. Specifically, we\ncharacterize the relationship between these parameters and the sparsity of\nsolutions under transform matrices, enabling the development of an iterative\nscheme for selecting parameters that achieve prescribed sparsity levels.\nSpecial attention is given to scenarios where the fidelity term is\nnon-differentiable, and the transform matrix lacks full row rank. In such\ncases, the regularized solution, along with two auxiliary vectors arising in\nthe sparsity characterization, are essential components of the multi-parameter\nselection strategy. To address this, we propose a fixed-point proximity\nalgorithm that simultaneously determines these three vectors. This algorithm,\ncombined with our sparsity characterization, forms the basis of a practical\nmulti-parameter selection strategy. Numerical experiments demonstrate the\neffectiveness of the proposed approach, yielding regularized solutions with\nboth predetermined sparsity levels and satisfactory approximation accuracy.",
        "We describe an algorithm to rigorously compute the power series expansion at\na CM point of a weight $2$ cusp form of level coprime to $6$. Our algorithm\nworks by bounding the denominators that appear due to ramification, and without\nrecourse to computing an explicit model of the corresponding modular curve. Our\nresult is the first in a series of papers toward an eventual implementation of\nequationless Chabauty.",
        "To understand the complex interplay of topography and surface chemistry in\nwetting, fundamental studies investigating both parameters are needed. Due to\nthe sensitivity of wetting to miniscule changes in one of the parameters it is\nimperative to precisely control the experimental approach. A profound\nunderstanding of their influence on wetting facilitates a tailored design of\nsurfaces with unique functionality. We present a multi-step study: The\ninfluence of surface chemistry is analyzed by determining the adsorption of\nvolatile carbonous species (A) and by sputter deposition of metallic copper and\ncopper oxides on flat copper substrates (B). A precise surface topography is\ncreated by laser processing. Isotropic topography is created by ps laser\nprocessing (C), and hierarchical anisotropic line patterns are produced by\ndirect laser interference patterning (DLIP) with different pulse durations (D).\nOur results reveal that the long-term wetting response of polished copper\nsurfaces stabilizes with time despite ongoing accumulation of hydrocarbons and\nis dominated by this adsorption layer over the oxide state of the substrate\n(Cu, CuO, Cu2O). The surfaces' wetting response can be precisely tuned by\ntailoring the topography via laser processing. The sub-pattern morphology of\nprimary line-like patterns showed great impact on the static contact angle,\nwetting anisotropy, and water adhesion. An increased roughness inside the\npattern valleys combined with a minor roughness on the peaks favors\nair-inclusions, isotropic hydrophobicity, and low water adhesion. Increasing\nthe aspect ratio showed to enhance air-inclusions and hydrophobicity despite\nincreased peak roughness while time dependent wetting transitions were\nobserved.",
        "For the special single-layer fractional quantum Hall system with a filling\nfactor of 5\/2, which has an even denominator, this paper uses principal\ncomponent analysis (PCA) to study its behavior under the breaking of\nparticle-hole symmetry. By introducing a model three-body potential to\nrepresent the mechanism of particle-hole symmetry breaking, the paper finds\nthat the 5\/2 system evolves into two types of special topological quantum\nstates with non-Abelian statistics as the strength and direction of the\nthree-body potential vary. The transition points of these states correspond to\nthe particle-hole symmetric pure Coulomb interaction system. Our results\nvalidate the applicability of machine learning as a new research tool in\nfractional quantum Hall systems. Furthermore, machine learning directly\nanalyzes the raw wave functions, without relying on prior empirical theoretical\nassumptions and models, making it applicable to a broader range of fractional\nquantum Hall systems experiencing phase transitions due to particle-hole\nsymmetry breaking.",
        "Nano- to micro-sized particles with differently charged surface areas exhibit\ncomplex interaction patterns, characterized by both opposite-charge attraction\nand like-charge repulsion. While several successful models have been proposed\nin the literature to describe directional attraction, models accounting for\nboth directional attraction and directional repulsion are much less numerous\nand often tailored to specific microscopic systems. Here we present a simple\nand versatile patchy model, where the interaction energy of a pair of particles\nis a sum of interactions between sites of different types located within the\nparticle volume. We implement different formulations of this model in both a\nself-developed Monte Carlo code and the widely used LAMMPS Molecular Dynamics\nsimulation software, providing basic toolkits for both simulation methods and,\nin the latter case, for different algorithms. By comparing physical observables\nand code performances, we discuss the different models, methods, and\nalgorithms, offering insights into optimization strategies and tricks of trade.",
        "Given a stochastic process with inputs and outputs, how might its behaviour\nbe related to pursuit of a goal? We model this using 'transducers', objects\nthat capture only the external behaviour of a system and not its internal\nstate. A companion paper summarises our results for cognitive scientists; the\ncurrent paper gives formal definitions and proofs.\n  To formalise the concept of a system that behaves as if it were pursuing a\ngoal, we consider what happens when a transducer (a 'policy') is coupled to\nanother transducer that comes equipped with a success condition (a\n'teleo-environment'). An optimal policy is identified with a transducer that\nbehaves as if it were perfectly rational in the pursuit of a goal; our\nframework also allows us to model constrained rationality.\n  Optimal policies obey a version of Bellman's principle: a policy that's\noptimal in one time step will again be optimal in the next time step, but with\nrespect to a different teleo-environment (obtained from the original one by a\nmodified version of Bayesian filtering). This property sometimes also applies\nto the bounded-rational case; we give a sufficient condition.\n  A policy is deterministic if and only if there exists a teleo-environment for\nwhich it is uniquely optimal among the set of all policies; we relate this to\nclassical representation theorems from decision theory. This result need not\nhold in the bounded-rational case; we give an example related to the\nabsent-minded driver problem. The formalism is defined using coinduction,\nfollowing the style proposed by Czajka.",
        "Axions are one of the leading dark matter candidates. If we are embedded in a\nMilky Way dark matter halo comprised of axions, their stimulated decay would\nenable us to observe a counterimage (``axion gegenschein\") with a frequency\nequal to half the axion mass in the opposite direction of a bright radio\nsource. This spectral line emission will be broadened to $\\Delta \\nu\/\\nu \\sim\n\\sigma_d\/c \\sim 10^{-3}$ due to the velocity dispersion of dark matter,\n$\\sigma_d$. In this pilot study, we perform the first search for the expected\naxion gegenschein image of Vela supernova remnant (SNR) with 26.4 hours of\neffective ON-OFF data from the Five-hundred-meter Aperture Spherical radio\nTelescope (FAST) L-band (1.0 - 1.5~GHz) 19-beam receiver. Our null detection\nlimits the axion-photon coupling strength to be $g_{a\\gamma\\gamma} \\lesssim 2\n\\times 10^{-10} \\mathrm{GeV}^{-1}$ in the mass ranges of $8.7\\,\\mu\\mathrm{eV}\n\\leq m_a \\leq 9.44\\,\\mu\\mathrm{eV}$ and $10.85\\,\\mu\\mathrm{eV} \\leq m_a \\leq\n12.01\\,\\mu\\mathrm{eV} $. These results provide a stronger constraint on\n$g_{a\\gamma\\gamma}$ in this axion mass range than the current limits obtained\nby the direct search of axion decay signal from galaxy clusters which uses FAST\nobservations, but is a factor of $\\sim 3$ times weaker than the current CAST\nlimit.Based on our observation strategy, data processing methods, and results,\nthe expected sensitivity will reach $\\sim 10^{-11}\\mathrm{GeV}^{-1}$ with $\\sim\n2000$ hours of observation in the future.",
        "Longitudinal networks are becoming increasingly relevant in the study of\ndynamic processes characterised by known or inferred community structure.\nGeneralised Network Autoregressive (GNAR) models provide a parsimonious\nframework for exploiting the underlying network and multivariate time series.\nWe introduce the community-$\\alpha$ GNAR model with interactions that exploits\nprior knowledge or exogenous variables for analysing interactions within and\nbetween communities, and can describe serial correlation in longitudinal\nnetworks. We derive new explicit finite-sample error bounds that validate\nanalysing high-dimensional longitudinal network data with GNAR models, and\nprovide insights into their attractive properties. We further illustrate our\napproach by analysing the dynamics of $\\textit{Red, Blue}$ and $\\textit{Swing}$\nstates throughout presidential elections in the USA from 1976 to 2020, that is,\na time series of length twelve on 51 time series (US states and Washington DC).\nOur analysis connects network autocorrelation to eight-year long terms,\nhighlights a possible change in the system after the 2016 election, and a\ndifference in behaviour between $\\textit{Red}$ and $\\textit{Blue}$ states."
      ]
    }
  },
  {
    "id":2411.06414,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"QEEGNet: Quantum Machine Learning for Enhanced Electroencephalography\n  Encoding",
    "start_abstract":"Electroencephalography (EEG) is a critical tool in neuroscience and clinical practice for monitoring analyzing brain activity. Traditional neural network models, such as EEGNet, have achieved considerable success decoding EEG signals but often struggle with the complexity high dimensionality of data. Recent advances quantum computing present new opportunities to enhance machine learning models through (QML) techniques. In this paper, we introduce Quantum-EEGNet (QEEGNet), novel hybrid that integrates classical EEGNet architecture improve encoding analysis, forward-looking approach, acknowledging results might not always surpass traditional methods it shows its potential. QEEGNet incorporates layers within network, allowing capture more intricate patterns data potentially offering computational advantages. We evaluate on benchmark dataset, BCI Competition IV 2a, demonstrating consistently outperforms most subjects other robustness noise. Our highlight significant potential quantum-enhanced networks suggesting directions both research practical applications field.",
    "start_categories":[
      "cs.RO",
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Braincomputer interfaces for communication and control"
      ],
      "abstract":[
        "For many years people have speculated that electroencephalographic activity or other electrophysiological measures of brain function might provide a new non-muscular channel for sending messages and commands to the external world \u2013 a brain\u2013computer interface (BCI). Over the past 15 years, productive BCI research programs have arisen. Encouraged by new understanding of brain function, by the advent of powerful low-cost computer equipment, and by growing recognition of the needs and potentials of people with disabilities, these programs concentrate on developing new augmentative communication and control technology for those with severe neuromuscular disorders, such as amyotrophic lateral sclerosis, brainstem stroke, and spinal cord injury. The immediate goal is to provide these users, who may be completely paralyzed, or \u2018locked in\u2019, with basic communication capabilities so that they can express their wishes to caregivers or even operate word processing programs or neuroprostheses. Present-day BCIs determine the intent of the user from a variety of different electrophysiological signals. These signals include slow cortical potentials, P300 potentials, and mu or beta rhythms recorded from the scalp, and cortical neuronal activity recorded by implanted electrodes. They are translated in real-time into commands that operate a computer display or other device. Successful operation requires that the user encode commands in these signals and that the BCI derive the commands from the signals. Thus, the user and the BCI system need to adapt to each other both initially and continually so as to ensure stable performance. Current BCIs have maximum information transfer rates up to 10\u201325 bits\/min. This limited capacity can be valuable for people whose severe disabilities prevent them from using conventional augmentative communication methods. At the same time, many possible applications of BCI technology, such as neuroprosthesis control, may require higher information transfer rates. Future progress will depend on: recognition that BCI research and development is an interdisciplinary problem, involving neurobiology, psychology, engineering, mathematics, and computer science; identification of those signals, whether evoked potentials, spontaneous rhythms, or neuronal firing rates, that users are best able to control independent of activity in conventional motor output pathways; development of training methods for helping users to gain and maintain that control; delineation of the best algorithms for translating these signals into device commands; attention to the identification and elimination of artifacts such as electromyographic and electro-oculographic activity; adoption of precise and objective procedures for evaluating BCI performance; recognition of the need for long-term as well as short-term assessment of BCI performance; identification of appropriate BCI applications and appropriate matching of applications and users; and attention to factors that affect user acceptance of augmentative technology, including ease of use, cosmesis, and provision of those communication and control capacities that are most important to the user. Development of BCI technology will also benefit from greater emphasis on peer-reviewed research publications and avoidance of the hyperbolic and often misleading media attention that tends to generate unrealistic expectations in the public and skepticism in other researchers. With adequate recognition and effective engagement of all these issues, BCI systems could eventually provide an important new communication and control option for those with motor disabilities and might also give those without disabilities a supplementary control channel or a control channel useful in special circumstances."
      ],
      "categories":[
        "Neurophysiology"
      ]
    },
    "list":{
      "title":[
        "Calibration-free measurement of the phonon temperature around a single\n  emitter",
        "Stability, growth, and doping of In$_{2}$(Si, Ge)$_{2}$O$_{7}$ as\n  promising n-type wide-gap semiconductors",
        "Critical Dynamics of Spin Boson Model",
        "Modulation of the galactic cosmic ray spectrum in an anisotropic\n  diffusion approach",
        "Non(anti)Commutative Superspace, Baker-Campbell-Hausdorff Closed Forms,\n  and Dirac-K\\\"ahler Twisted Supersymmetry",
        "Quantifying the generation of negatively charged boron vacancies in\n  He-ion irradiated hexagonal boron nitride",
        "Quorum sensing and absorbing phase transitions in colloidal active\n  matter",
        "Detecting high-dimensional time-bin entanglement in fiber-loop systems",
        "An Unconventional Ultra-Sub-Wavelength Receiving Nano-Antenna Activated\n  by ac Spin Pumping and the ac Inverse Spin Hall Effect",
        "Structure-Preserving Neural Ordinary Differential Equations for Stiff\n  Systems",
        "Connectivity and matching extendability of optimal $1$-embedded graphs\n  on the torus",
        "Forcing, genericity and CBERS",
        "Human-Agent Interaction in Synthetic Social Networks: A Framework for\n  Studying Online Polarization",
        "(FAPP) Infinity Does Macroscopic Irreversibility From Microscopic\n  Reversibility",
        "SeqSee: A schema-based approach to spectral sequence visualization",
        "Multi-Modality Representation Learning for Antibody-Antigen Interactions\n  Prediction",
        "On the role of true and false chirality in producing parity violating\n  energy differences",
        "Miniaturized liquid metal composite circuits with energy harvesting\n  coils for battery-free bioelectronics and optogenetics",
        "Multideterminantal measures",
        "On the Mordell-Weil rank and $2$-Selmer group of a family of elliptic\n  curves",
        "Gradient-based Explanations for Deep Learning Survival Models",
        "Parameter Choices for Sparse Multi-Parameter Regularization with the\n  $\\ell_1$ Norm",
        "Rigorous expansions of modular forms at CM points, I: Denominators",
        "Influence of Chemistry and Topography on the Wettability of Copper",
        "Principal component analysis for 5\/2 fractional quantum Hall states",
        "Simulating inverse patchy colloid models",
        "Formalising the intentional stance 2: a coinductive approach",
        "Searching for axion dark matter gegenschein of the Vela supernova\n  remnant with FAST",
        "Generalized network autoregressive modelling of longitudinal networks\n  with application to presidential elections in the USA"
      ],
      "abstract":[
        "The emission properties of a localized solid-state emitter are strongly\ninfluenced by its environment. The coupling to acoustic phonons impacts the\ncoherence of the emitter and its temperature dependence, and also results in\nthe apparition of phonon sidebands besides the sharp zero-phonon line. Here, we\npresent a method for measuring the absolute temperature of a localized emitter\nby directly plotting the ratio of the Stokes and anti-Stokes components of the\nphonon sideband as a function of the shift from the zero-phonon line. This\napproach requires no calibration and knowledge of the system, making it\napplicable to a wide range of emitters and materials. We validate the method\nusing a CdSe quantum dot in a ZnSe nanowire. We thus show that the quantum dot\nis significantly heated under non-resonant excitation when increasing the\nincident power at low temperature and is ascribed to the drop in thermal\nconductivity at these temperatures.",
        "In this paper we investigate, computationally and experimentally, the phase\nstability, electronic structure properties, and the propensity for n-type\ndoping of In$_{2}$X$_{2}$O$_{7}$ (X=Si, Ge) ternary oxides. This family of\nmaterials contains promising novel wide-gap semiconductors based on their\nestimated high $n$-type Baliga figures of merit and acceptable thermal\nconductivity for power electronics applications. Here, we find that both\nIn$_{2}$Si$_{2}$O$_{7}$ and In$_{2}$Ge$_{2}$O$_{7}$ to be n-type dopable, with\nZr providing between 10$^{16}$ and above 10$^{21}$ cm$^{-3}$ net donor\nconcentrations under O-poor conditions, depending on the chemistry, structure\n(ground-state thorvetite or high-pressure pyrochlore) and synthesis\ntemperature. Initial thin-film growth and annealing leads to polycrystalline\nIn$_{2}$Ge$_{2}$O$_{7}$ thin films in thorvetite structure with band gap over 4\neV, and confirms Zr doping predictions by achieving electron concentrations at\n10$^{14}$-10$^{16}$ cm$^{-3}$ under O-rich condition. While future epitaxial\ngrowth development is still needed, this study establishes\nIn$_{2}$X$_{2}$O$_{7}$ as promising n-type wide-gap semiconductors for power\nelectronic applications.",
        "In this work, we study the low-energy properties of the spin-boson model\n(SBM), which describes the dynamics of a 1\/2 spin associated with a thermostat\ncharacterized by a power law spectral density, $f(\\omega)\\propto |\\omega|^s$.\nThe theoretical description is constructed in the Schwinger--Keldysh technique,\nbased on the representation of the 1\/2-spin by Majorana fermions. We study the\ncritical dynamics of the system near the quantum phase transition by\nconstructing and analyzing the system of renormalization group equations. Our\ntheoretical approach is more universal, contrary to the one based on quantum\nclassical mapping, since it is applicable for $0<s\\leq 1$. We show that in both\nthe ohmic case $s=1$, and subohmic case $0<s<1$, the second order quantum phase\ntransition is observed in the model considered, and the critical magnetization\nexponent agrees with the exact hyperscaling result, $1\/\\delta=(1-s)\/(1+s)$.\nFurthermore, we obtain the dependence of the critical value of the spin-boson\ncoupling constant on the temperature of the bosonic thermal bath.",
        "We introduce a novel diffusion model for the propagation of cosmic rays (CRs)\nthat incorporates an anisotropic diffusion tensor of a general form within a\nrealistically modeled large-scale Galactic magnetic field. The parameters of\nthe model are consistent with the contemporary understanding of the large-scale\nGalactic magnetic field structure and the dynamics of small-scale turbulent CR\npropagation. The paper demonstrates the modulation of spectra of Galactic\ncosmic rays (GCRs) in the magnetic rigidity range of 1 - 30 PV (the CR knee)\nand explores the spatial variation of this phenomenon. The observed modulation\nof the spectrum is explained by changes in the leakage mechanism.",
        "Starting from an elementary calculation of super Lie group elements\nassociating with non(anti)-commutative Grassmann parameters, we derive several\nclosed expressions of Baker-Campbell-Hausdorff (BCH) formula which represent\nmultiplication properties of super Lie group elements in the corresponding\nsuperspace. We then show that parametrization of superspace in general may\nbecome infinite dimensional due to the presence of non(anti)commutativity. We\nshow that a Dirac-K\\\"ahler Twisted SUSY Algebra (also referred to as Marcus\nB-type Twisted SUSY Algebra or Geometric Langlands Twisted SUSY Algebra) with a\ncertain type of deformation, which we call an exponential deformation, may\ncircumvent this problem. We also provide, in terms of gauge covariantization of\nthe SUSY algebra, a geometric understanding of the exponential deformation, and\nsee that the framework constructed in this paper may serve as a\nnon(anti)commutative superspace framework providing the gauge covariant link\nformulation of twisted super Yang-Mills on a lattice.",
        "Hexagonal boron nitride (hBN) hosts luminescent defects possessing spin\nqualities compatible with quantum sensing protocols at room temperature.\nVacancies, in particular, are readily obtained via exposure to high-energy ion\nbeams. While the defect creation mechanism via such irradiation is well\nunderstood, the occurrence rate of optically active negatively charged\nvacancies ($V_B^-$) is an open question. In this work, we exploit focused\nhelium ions to systematically generate optically active vacancy defects in hBN\nflakes at varying density. By comparing the density-dependent spin splitting\nmeasured by magnetic resonance to calculations based on a microscopic charge\nmodel, in which we introduce a correction term due to a constant background\ncharge, we are able to quantify the number of $V_B^-$ defects generated by the\nion irradiation. We find that only a small fraction (0.2%) of all vacancies is\nin the optically active, negatively charged state. Our results provide a\nprotocol for measuring the generation efficiency of $V_B^-$, which is necessary\nfor understanding and optimizing luminescent centers in hBN.",
        "Unlike biological active matter that constantly adapt to their environment,\nthe motors of synthetic active particles are typically agnostic to their\nsurroundings and merely operate at constant force. Here, we design colloidal\nactive rods capable of modulating their inner activity in response to crowding,\nthereby enforcing a primitive form of quorum sensing interactions. Through\nexperiments, simulations and theory we elucidate the impact of these\ninteractions on the phase behavior of isotropic active matter. We demonstrate\nthat, when conditioned to density, motility regulation can either lead to an\nabsorbing phase transition, where all particles freeze their dynamics, or to\natypical phase separation, where flat interfaces supporting a net pressure drop\nare in mechanical equilibrium. Fully active and fully arrested particles can\nthen form heterogeneous patterns ruled by the competition between quorum\nsensing and mechanical interactions. Beyond the specifics of motile colloids,\nwe expect our findings to apply broadly to adaptive active matter assembled\nfrom living or synthetic units.",
        "Many quantum communication protocols rely on the distribution of entanglement\nbetween the different participating parties. One example is quantum key\ndistribution (QKD), an application that has matured to commercial use in recent\nyears. However, difficulties remain, especially with noise resilience and\nchannel capacity in long-distance communication. One way to overcome these\nproblems is to use high-dimensional entanglement, which has been shown to be\nmore robust to noise and enables higher secret-key rates. It is therefore\nimportant to have access to certifiable high-dimensional entanglement sources\nto confidently implement these advanced QKD protocols. Here, we develop a\nmethod for certifying high-dimensional time-bin entanglement in fiber-loop\nsystems. In these systems, entanglement creation and detection can utilize the\nsame physical components, and the number of time bins, and thus the\nentanglement dimension, can be adapted without making physical changes to the\nsetup. Our certification method builds on previous proposals for the\ncertification of angular-momentum entanglement in photon pairs. In particular,\nmeasurements in only two experimentally accessible bases are sufficient to\nobtain a lower bound on the entanglement dimension for both two- and\nmultiphoton quantum states. Numerical simulations show that the method is\nrobust against typical experimental noise effects and works well even with\nlimited measurement statistics, thus establishing time-bin encoded photons as a\npromising platform for high-dimensional quantum-communication protocols.",
        "We report an extreme sub-wavelength unconventional receiving antenna. It\nconsists of an array of nanomagnets connected to heavy metal nanostrips.\nIncident electromagnetic (EM) radiation generates intrinsic and extrinsic spin\nwaves in the nanomagnets, which pump spin into the heavy metal nanostrips at\ntheir own frequencies giving rise to a polychromatic alternating voltage across\nthe latter owing to the ac inverse spin Hall effect. This implements a\nreceiving nano-antenna. We demonstrate its operation at two different EM wave\nfrequencies of 1.5 GHz and 2.4 GHz - the latter being the Bluetooth and Wi-Fi\nfrequency. We measure the receiving gain at 2.4 GHz to be approximately -9 db.\nThe free space radiated wavelength \"lambda\" at 2.4 GHz is 12.5 cm while the\nantenna area A is merely 160 micron^2, making the ratio A\/lambda^2 =\n0.97x10^-8. This antenna's receiving gain should be very poor because of the\ntiny size. Yet the measured gain is more than 4000 times larger than the\ntheoretical limit for a conventional antenna of this size at this wavelength\nbecause of the unconventional operating principle.",
        "Neural ordinary differential equations (NODEs) are an effective approach for\ndata-based modeling of dynamical systems arising from simulations and\nexperiments. One of the major shortcomings of NODEs, especially when coupled\nwith explicit integrators, is its long-term stability, which impedes their\nefficiency and robustness when encountering stiff problems. In this work we\npresent a structure-preserving NODE approach, which integrates with a linear\nand nonlinear split and an exponential integrator, latter of which is an\nexplicit integrator with stability properties comparable to implicit methods.\nWe demonstrate that our model has advantages in both learning and deployment\nover standard explicit or even implicit NODE methods. The long-time stability\nis further enhanced by the Hurwitz matrix decomposition that constrains the\nspectrum of the linear operator, therefore stabilizing the linearized dynamics.\nWhen combined with a Lipschitz-controlled neural network treatment for the\nnonlinear operator, we show the nonlinear dynamics of the NODE are provably\nstable in the sense of Lyapunov. For high-dimensional data, we further rely on\nan autoencoder performing dimension reduction and Higham's algorithm for the\nmatrix-free application of the matrix exponential on a vector. We demonstrate\nthe effectiveness of the proposed NODE approach in various examples, including\nthe Grad-13 moment equations and the Kuramoto-Sivashinky equation.",
        "In this paper, we discuss optimal $1$-toroidal graphs (abbreviated as O1TG),\nwhich are drawn on the torus so that every edge crosses another edge at most\nonce, and has $n$ vertices and exactly $4n$ edges. We first consider\nconnectivity of O1TGs, and give the characterization of O1TGs having\nconnectivity exactly $k$ for each $k\\in \\{4, 5, 6, 8\\}$. In our argument, we\nalso show that there exists no O1TG having connectivity exactly $7$.\nFurthermore, using the result above, we discuss extendability of matchings, and\ngive the characterization of $1$-, $2$- and $3$-extendable O1TGs in turn.",
        "In this paper we continue the study of equivalence of generics filters\nstarted by Smythe in [Smy22]. We fully characterize those forcing posets for\nwhich the corresponding equivalence of generics is smooth using the purely\ntopological property of condensation. Next we leverage our characterization to\nshow that there are non-homogeneous forcing for which equivalence of generics\nis not smooth. Then we prove hyperfiniteness in the case of Prikry forcing and\nsome additional results addressing the problem whether generic equivalence for\nCohen forcing is hyperfinite.",
        "Online social networks have dramatically altered the landscape of public\ndiscourse, creating both opportunities for enhanced civic participation and\nrisks of deepening social divisions. Prevalent approaches to studying online\npolarization have been limited by a methodological disconnect: mathematical\nmodels excel at formal analysis but lack linguistic realism, while language\nmodel-based simulations capture natural discourse but often sacrifice\nanalytical precision. This paper introduces an innovative computational\nframework that synthesizes these approaches by embedding formal opinion\ndynamics principles within LLM-based artificial agents, enabling both rigorous\nmathematical analysis and naturalistic social interactions. We validate our\nframework through comprehensive offline testing and experimental evaluation\nwith 122 human participants engaging in a controlled social network\nenvironment. The results demonstrate our ability to systematically investigate\npolarization mechanisms while preserving ecological validity. Our findings\nreveal how polarized environments shape user perceptions and behavior:\nparticipants exposed to polarized discussions showed markedly increased\nsensitivity to emotional content and group affiliations, while perceiving\nreduced uncertainty in the agents' positions. By combining mathematical\nprecision with natural language capabilities, our framework opens new avenues\nfor investigating social media phenomena through controlled experimentation.\nThis methodological advancement allows researchers to bridge the gap between\ntheoretical models and empirical observations, offering unprecedented\nopportunities to study the causal mechanisms underlying online opinion\ndynamics.",
        "Infinity is central to deriving macroscopic irreversibility from reversible\nmicroscopic laws across mathematics, theoretical computer science and physics.\nIn analysis, infinite processes -- such as Dedekind cuts and Cauchy sequences\n-- construct real numbers as equivalence classes of rational approximations,\nbridging discrete rationals to the continuous real line. In quantum mechanics,\ninfinite tensor products model nested measurements, where sectorization\npartitions the Hilbert space into equivalence classes, reconciling unitary\nevolution with wavefunction collapse. In statistical mechanics, macrostates\nemerge as equivalence classes of microstates sharing identical macroscopic\nproperties, providing the statistical basis for thermodynamic irreversibility\ndespite reversible dynamics. Equivalence relations formalize\nFor-All-Practical-Purposes (FAPP) indistinguishability, reflecting operational\nlimits on precision and observation. Together, these examples reveal a unified\nframework where infinity and equivalence underpin emergent macroscopic behavior\nfrom microscopic reversibility.",
        "We present SeqSee, a software system that addresses spectral sequence\nvisualization through a schema-based approach. By introducing a standardized\nJSON schema as an intermediate representation, SeqSee decouples the\nmathematical computations of spectral sequences from their visualizations. We\ndemonstrate the system through a case study of the classical and C-motivic\nAdams spectral sequences.",
        "While deep learning models play a crucial role in predicting antibody-antigen\ninteractions (AAI), the scarcity of publicly available sequence-structure\npairings constrains their generalization. Current AAI methods often focus on\nresidue-level static details, overlooking fine-grained structural\nrepresentations of antibodies and their inter-antibody similarities. To tackle\nthis challenge, we introduce a multi-modality representation approach that\nintegates 3D structural and 1D sequence data to unravel intricate\nintra-antibody hierarchical relationships. By harnessing these representations,\nwe present MuLAAIP, an AAI prediction framework that utilizes graph attention\nnetworks to illuminate graph-level structural features and normalized adaptive\ngraph convolution networks to capture inter-antibody sequence associations.\nFurthermore, we have curated an AAI benchmark dataset comprising both\nstructural and sequence information along with interaction labels. Through\nextensive experiments on this benchmark, our results demonstrate that MuLAAIP\noutperforms current state-of-the-art methods in terms of predictive\nperformance. The implementation code and dataset are publicly available at\nhttps:\/\/github.com\/trashTian\/MuLAAIP for reproducibility.",
        "In this work we tackle the problem of showing which type of influences can\nlift the degeneracy between truly and falsely chiral systems, showing that only\nwhen both systems and influences are both truly (falsely) chiral, a parity\nviolating energy difference between left- and right-handed systems can be\nproduced. In particular, after considering the enantiomers of a chiral molecule\nas paradigmatic truly chiral systems, we rigorously show, under a quantum field\ntheoretically approach, that only a truly chiral influence such as the\n$Z^{0}$-mediated electroweak interaction can lift the degeneracy between\nenantiomers. On the contrary, we explicitly show that a falsely chiral\ninfluence, such as an axion-mediated interaction in chiral molecules, can not\nlift the aforementioned degeneracy. These results extend Barron's seminal ideas\n[L. D. Barron, True and false chirality and parity violation, Chem. Phys. Lett\n{\\bf 123}, 423 (1986)] to a quantum field theory-based approach.",
        "Over the past years, rapid progress has been made on soft-matter electronics\nfor wearable and implantable devices, for bioelectronics and optogenetics.\nLiquid Metal (LM) based electronics were especially popular, due to their\nlong-term durability, when subject to repetitive strain cycles. However, one\nmajor limitation has been the need for tethering bioelectronics circuits to\nexternal power, or the use of rigid bulky batteries. This has motivated a\ngrowing interest in wireless energy transfer, which demands circuit\nminiaturization. However, miniaturization of LM circuits is challenging due to\nlow LM-substrate adhesion, LM smearing, and challenges on\nmicrochip-interfacing. In this article, we address these challenges by\nhigh-resolution laser-assisted micropatterning of biphasic LM composites and\nvapor-assisted LM microchip soldering. Through development of a search\nalgorithm for optimization of the biphasic ink coil performance, we designed\nand implemented micro coils with trace spacing of 50 {\\mu}m that can harvest a\nsignificant amount of energy (178 mW\/cm2) through near field inductive\ncoupling. We show miniaturized soft-matter circuits with integrated SMD chips\nsuch as NFC chips, capacitors, and LEDs that are implemented in a few minutes\nthrough laser patterning, and vaporassisted soldering. In the context of\noptogenetics, where lightweight, miniaturized systems are needed to provide\noptical stimulation, soft coils stand out in terms of their improved\nconformability and flexibility. Thus, this article explores the applications of\nsoft coils in wearable and implantable devices, with a specific focus on their\nuse in optogenetics.",
        "We define multideterminantal probability measures, a family of probability\nmeasures on $[k]^n$ where $[k]=\\{1,2,\\dots,k\\}$, generalizing determinantal\nmeasures (which correspond to the case $k=2$). We give examples coming from the\npositive Grassmannian, from the dimer model and from the spanning tree model.\n  We also define and completely characterize determinantal probability measures\non the permutation group $S_n$.",
        "We consider the parametric family of elliptic curves over $\\mathbb{Q}$ of the\nform $E_{m} : y^{2} = x(x - n_{1})(x - n_{2}) + t^{2}$, where $n_{1}$, $n_{2}$\nand $t$ are particular polynomial expressions in an integral variable $m$. In\nthis paper, we investigate the torsion group $E_{m}(\\mathbb{Q})_{\\rm{tors}}$, a\nlower bound for the Mordell-Weil rank $r({E_{m}})$ and the $2$-Selmer group\n${\\rm{Sel}}_{2}(E_{m})$ under certain conditions on $m$. This extends the\nprevious works done in this direction, which are mostly concerned with the\nMordell-Weil ranks of various parametric families of elliptic curves.",
        "Deep learning survival models often outperform classical methods in\ntime-to-event predictions, particularly in personalized medicine, but their\n\"black box\" nature hinders broader adoption. We propose a framework for\ngradient-based explanation methods tailored to survival neural networks,\nextending their use beyond regression and classification. We analyze the\nimplications of their theoretical assumptions for time-dependent explanations\nin the survival setting and propose effective visualizations incorporating the\ntemporal dimension. Experiments on synthetic data show that gradient-based\nmethods capture the magnitude and direction of local and global feature\neffects, including time dependencies. We introduce GradSHAP(t), a\ngradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and\nSurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply\nthese methods to medical data with multi-modal inputs, revealing relevant\ntabular features and visual patterns, as well as their temporal dynamics.",
        "This paper introduces a multi-parameter regularization approach using the\n$\\ell_1$ norm, designed to better adapt to complex data structures and problem\ncharacteristics while offering enhanced flexibility in promoting sparsity in\nregularized solutions. As data volumes grow, sparse representations of learned\nfunctions become critical for reducing computational costs during function\noperations. We investigate how the selection of multiple regularization\nparameters influences the sparsity of regularized solutions. Specifically, we\ncharacterize the relationship between these parameters and the sparsity of\nsolutions under transform matrices, enabling the development of an iterative\nscheme for selecting parameters that achieve prescribed sparsity levels.\nSpecial attention is given to scenarios where the fidelity term is\nnon-differentiable, and the transform matrix lacks full row rank. In such\ncases, the regularized solution, along with two auxiliary vectors arising in\nthe sparsity characterization, are essential components of the multi-parameter\nselection strategy. To address this, we propose a fixed-point proximity\nalgorithm that simultaneously determines these three vectors. This algorithm,\ncombined with our sparsity characterization, forms the basis of a practical\nmulti-parameter selection strategy. Numerical experiments demonstrate the\neffectiveness of the proposed approach, yielding regularized solutions with\nboth predetermined sparsity levels and satisfactory approximation accuracy.",
        "We describe an algorithm to rigorously compute the power series expansion at\na CM point of a weight $2$ cusp form of level coprime to $6$. Our algorithm\nworks by bounding the denominators that appear due to ramification, and without\nrecourse to computing an explicit model of the corresponding modular curve. Our\nresult is the first in a series of papers toward an eventual implementation of\nequationless Chabauty.",
        "To understand the complex interplay of topography and surface chemistry in\nwetting, fundamental studies investigating both parameters are needed. Due to\nthe sensitivity of wetting to miniscule changes in one of the parameters it is\nimperative to precisely control the experimental approach. A profound\nunderstanding of their influence on wetting facilitates a tailored design of\nsurfaces with unique functionality. We present a multi-step study: The\ninfluence of surface chemistry is analyzed by determining the adsorption of\nvolatile carbonous species (A) and by sputter deposition of metallic copper and\ncopper oxides on flat copper substrates (B). A precise surface topography is\ncreated by laser processing. Isotropic topography is created by ps laser\nprocessing (C), and hierarchical anisotropic line patterns are produced by\ndirect laser interference patterning (DLIP) with different pulse durations (D).\nOur results reveal that the long-term wetting response of polished copper\nsurfaces stabilizes with time despite ongoing accumulation of hydrocarbons and\nis dominated by this adsorption layer over the oxide state of the substrate\n(Cu, CuO, Cu2O). The surfaces' wetting response can be precisely tuned by\ntailoring the topography via laser processing. The sub-pattern morphology of\nprimary line-like patterns showed great impact on the static contact angle,\nwetting anisotropy, and water adhesion. An increased roughness inside the\npattern valleys combined with a minor roughness on the peaks favors\nair-inclusions, isotropic hydrophobicity, and low water adhesion. Increasing\nthe aspect ratio showed to enhance air-inclusions and hydrophobicity despite\nincreased peak roughness while time dependent wetting transitions were\nobserved.",
        "For the special single-layer fractional quantum Hall system with a filling\nfactor of 5\/2, which has an even denominator, this paper uses principal\ncomponent analysis (PCA) to study its behavior under the breaking of\nparticle-hole symmetry. By introducing a model three-body potential to\nrepresent the mechanism of particle-hole symmetry breaking, the paper finds\nthat the 5\/2 system evolves into two types of special topological quantum\nstates with non-Abelian statistics as the strength and direction of the\nthree-body potential vary. The transition points of these states correspond to\nthe particle-hole symmetric pure Coulomb interaction system. Our results\nvalidate the applicability of machine learning as a new research tool in\nfractional quantum Hall systems. Furthermore, machine learning directly\nanalyzes the raw wave functions, without relying on prior empirical theoretical\nassumptions and models, making it applicable to a broader range of fractional\nquantum Hall systems experiencing phase transitions due to particle-hole\nsymmetry breaking.",
        "Nano- to micro-sized particles with differently charged surface areas exhibit\ncomplex interaction patterns, characterized by both opposite-charge attraction\nand like-charge repulsion. While several successful models have been proposed\nin the literature to describe directional attraction, models accounting for\nboth directional attraction and directional repulsion are much less numerous\nand often tailored to specific microscopic systems. Here we present a simple\nand versatile patchy model, where the interaction energy of a pair of particles\nis a sum of interactions between sites of different types located within the\nparticle volume. We implement different formulations of this model in both a\nself-developed Monte Carlo code and the widely used LAMMPS Molecular Dynamics\nsimulation software, providing basic toolkits for both simulation methods and,\nin the latter case, for different algorithms. By comparing physical observables\nand code performances, we discuss the different models, methods, and\nalgorithms, offering insights into optimization strategies and tricks of trade.",
        "Given a stochastic process with inputs and outputs, how might its behaviour\nbe related to pursuit of a goal? We model this using 'transducers', objects\nthat capture only the external behaviour of a system and not its internal\nstate. A companion paper summarises our results for cognitive scientists; the\ncurrent paper gives formal definitions and proofs.\n  To formalise the concept of a system that behaves as if it were pursuing a\ngoal, we consider what happens when a transducer (a 'policy') is coupled to\nanother transducer that comes equipped with a success condition (a\n'teleo-environment'). An optimal policy is identified with a transducer that\nbehaves as if it were perfectly rational in the pursuit of a goal; our\nframework also allows us to model constrained rationality.\n  Optimal policies obey a version of Bellman's principle: a policy that's\noptimal in one time step will again be optimal in the next time step, but with\nrespect to a different teleo-environment (obtained from the original one by a\nmodified version of Bayesian filtering). This property sometimes also applies\nto the bounded-rational case; we give a sufficient condition.\n  A policy is deterministic if and only if there exists a teleo-environment for\nwhich it is uniquely optimal among the set of all policies; we relate this to\nclassical representation theorems from decision theory. This result need not\nhold in the bounded-rational case; we give an example related to the\nabsent-minded driver problem. The formalism is defined using coinduction,\nfollowing the style proposed by Czajka.",
        "Axions are one of the leading dark matter candidates. If we are embedded in a\nMilky Way dark matter halo comprised of axions, their stimulated decay would\nenable us to observe a counterimage (``axion gegenschein\") with a frequency\nequal to half the axion mass in the opposite direction of a bright radio\nsource. This spectral line emission will be broadened to $\\Delta \\nu\/\\nu \\sim\n\\sigma_d\/c \\sim 10^{-3}$ due to the velocity dispersion of dark matter,\n$\\sigma_d$. In this pilot study, we perform the first search for the expected\naxion gegenschein image of Vela supernova remnant (SNR) with 26.4 hours of\neffective ON-OFF data from the Five-hundred-meter Aperture Spherical radio\nTelescope (FAST) L-band (1.0 - 1.5~GHz) 19-beam receiver. Our null detection\nlimits the axion-photon coupling strength to be $g_{a\\gamma\\gamma} \\lesssim 2\n\\times 10^{-10} \\mathrm{GeV}^{-1}$ in the mass ranges of $8.7\\,\\mu\\mathrm{eV}\n\\leq m_a \\leq 9.44\\,\\mu\\mathrm{eV}$ and $10.85\\,\\mu\\mathrm{eV} \\leq m_a \\leq\n12.01\\,\\mu\\mathrm{eV} $. These results provide a stronger constraint on\n$g_{a\\gamma\\gamma}$ in this axion mass range than the current limits obtained\nby the direct search of axion decay signal from galaxy clusters which uses FAST\nobservations, but is a factor of $\\sim 3$ times weaker than the current CAST\nlimit.Based on our observation strategy, data processing methods, and results,\nthe expected sensitivity will reach $\\sim 10^{-11}\\mathrm{GeV}^{-1}$ with $\\sim\n2000$ hours of observation in the future.",
        "Longitudinal networks are becoming increasingly relevant in the study of\ndynamic processes characterised by known or inferred community structure.\nGeneralised Network Autoregressive (GNAR) models provide a parsimonious\nframework for exploiting the underlying network and multivariate time series.\nWe introduce the community-$\\alpha$ GNAR model with interactions that exploits\nprior knowledge or exogenous variables for analysing interactions within and\nbetween communities, and can describe serial correlation in longitudinal\nnetworks. We derive new explicit finite-sample error bounds that validate\nanalysing high-dimensional longitudinal network data with GNAR models, and\nprovide insights into their attractive properties. We further illustrate our\napproach by analysing the dynamics of $\\textit{Red, Blue}$ and $\\textit{Swing}$\nstates throughout presidential elections in the USA from 1976 to 2020, that is,\na time series of length twelve on 51 time series (US states and Washington DC).\nOur analysis connects network autocorrelation to eight-year long terms,\nhighlights a possible change in the system after the 2016 election, and a\ndifference in behaviour between $\\textit{Red}$ and $\\textit{Blue}$ states."
      ]
    }
  },
  {
    "id":2411.06414,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Braincomputer interfaces for communication and control",
    "start_abstract":"For many years people have speculated that electroencephalographic activity or other electrophysiological measures of brain function might provide a new non-muscular channel for sending messages and commands to the external world \u2013 a brain\u2013computer interface (BCI). Over the past 15 years, productive BCI research programs have arisen. Encouraged by new understanding of brain function, by the advent of powerful low-cost computer equipment, and by growing recognition of the needs and potentials of people with disabilities, these programs concentrate on developing new augmentative communication and control technology for those with severe neuromuscular disorders, such as amyotrophic lateral sclerosis, brainstem stroke, and spinal cord injury. The immediate goal is to provide these users, who may be completely paralyzed, or \u2018locked in\u2019, with basic communication capabilities so that they can express their wishes to caregivers or even operate word processing programs or neuroprostheses. Present-day BCIs determine the intent of the user from a variety of different electrophysiological signals. These signals include slow cortical potentials, P300 potentials, and mu or beta rhythms recorded from the scalp, and cortical neuronal activity recorded by implanted electrodes. They are translated in real-time into commands that operate a computer display or other device. Successful operation requires that the user encode commands in these signals and that the BCI derive the commands from the signals. Thus, the user and the BCI system need to adapt to each other both initially and continually so as to ensure stable performance. Current BCIs have maximum information transfer rates up to 10\u201325 bits\/min. This limited capacity can be valuable for people whose severe disabilities prevent them from using conventional augmentative communication methods. At the same time, many possible applications of BCI technology, such as neuroprosthesis control, may require higher information transfer rates. Future progress will depend on: recognition that BCI research and development is an interdisciplinary problem, involving neurobiology, psychology, engineering, mathematics, and computer science; identification of those signals, whether evoked potentials, spontaneous rhythms, or neuronal firing rates, that users are best able to control independent of activity in conventional motor output pathways; development of training methods for helping users to gain and maintain that control; delineation of the best algorithms for translating these signals into device commands; attention to the identification and elimination of artifacts such as electromyographic and electro-oculographic activity; adoption of precise and objective procedures for evaluating BCI performance; recognition of the need for long-term as well as short-term assessment of BCI performance; identification of appropriate BCI applications and appropriate matching of applications and users; and attention to factors that affect user acceptance of augmentative technology, including ease of use, cosmesis, and provision of those communication and control capacities that are most important to the user. Development of BCI technology will also benefit from greater emphasis on peer-reviewed research publications and avoidance of the hyperbolic and often misleading media attention that tends to generate unrealistic expectations in the public and skepticism in other researchers. With adequate recognition and effective engagement of all these issues, BCI systems could eventually provide an important new communication and control option for those with motor disabilities and might also give those without disabilities a supplementary control channel or a control channel useful in special circumstances.",
    "start_categories":[
      "Neurophysiology"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b1",
        "b9"
      ],
      "title":[
        "An EEG-based brain-computer interface for real-time multi-task robotic control",
        "QEEGNet: Quantum Machine Learning for Enhanced Electroencephalography\n  Encoding"
      ],
      "abstract":[
        "The Brain Computer Interface (BCI) is the communication between human brain and computer. Electroencephalogram (EEG) one of biomedical signals which can be obtained by attaching electrodes to scalp. Some EEG related applications developed help disabled people, such as based wheelchair or robotic arm. A hybrid BCI real-time control system proposed a multi-tasks robot. In this system, sliding window online data segmentation strategy segment training data, enable learn dynamic features when subject's state transfer from rest task execution state. achieve ensure continuity executing actions. addition, Common Spatial Pattern (CSP) better extract spatial these continuous actions that multiple commands are accurately classified. experiment, three subjects' collected, trained tested performance reliability system. records robot's spending time, moving distance, number objects pushing down. Experimental results given show feasibility Compared remote controller, similar performance. Thus, able robot in environment used develop robot-aided arm methods on neurological rehabilitation principles for stroke injury patients.",
        "Electroencephalography (EEG) is a critical tool in neuroscience and clinical practice for monitoring analyzing brain activity. Traditional neural network models, such as EEGNet, have achieved considerable success decoding EEG signals but often struggle with the complexity high dimensionality of data. Recent advances quantum computing present new opportunities to enhance machine learning models through (QML) techniques. In this paper, we introduce Quantum-EEGNet (QEEGNet), novel hybrid that integrates classical EEGNet architecture improve encoding analysis, forward-looking approach, acknowledging results might not always surpass traditional methods it shows its potential. QEEGNet incorporates layers within network, allowing capture more intricate patterns data potentially offering computational advantages. We evaluate on benchmark dataset, BCI Competition IV 2a, demonstrating consistently outperforms most subjects other robustness noise. Our highlight significant potential quantum-enhanced networks suggesting directions both research practical applications field."
      ],
      "categories":[
        "cs.RO",
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "A Scalable and Robust Compilation Framework for Emitter-Photonic Graph\n  State",
        "Environmental Influences on Collaboration Network Evolution: A\n  Historical Analysis",
        "Graphs of unbounded linear cliquewidth must transduce all trees",
        "Dynamic Bragg microcavities in collisions of unipolar light pulses of\n  unusual shape in two- and three-level medium",
        "Global Lipschitz and Sobolev estimates for the Monge-Amp\\`ere\n  eigenfunctions of general bounded convex domains",
        "Learning to Retrieve and Reason on Knowledge Graph through Active\n  Self-Reflection",
        "Stress energy momentum in terms of geodesic accelerations and\n  variational tensors including torsion",
        "Relations amongst the distances between $C^{*}$-subalgebras and some\n  canonically associated operator algebras",
        "A posteriori error control for a finite volume scheme for a\n  cross-diffusion model of ion transport",
        "MaSS13K: A Matting-level Semantic Segmentation Benchmark",
        "Nuclear Structure Properties and Stellar Weak Rates for 76Se: Unblocking\n  of the Gamow Teller Strength",
        "Active 6D Pose Estimation for Textureless Objects using Multi-View RGB\n  Frames",
        "Aspects of Artificial Intelligence: Transforming Machine Learning\n  Systems Naturally",
        "Micromotion compensation using dark and bright ions",
        "Connection points on double regular polygons",
        "Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video\n  Understanding",
        "Functional Reactive Programming with Effects, A More Permissive Approach",
        "White's conjecture for matroids and inner projections",
        "Incidence equivalence and the Bloch-Beilinson filtration",
        "Nonperturbative Open Quantum Dynamics Bypass Influence Functional",
        "Improving Medical Waste Classification with Hybrid Capsule Networks",
        "Athermal creep deformation of ultrastable amorphous solids",
        "Convergence analysis of linearized $\\ell_q$ penalty methods for\n  nonconvex optimization with nonlinear equality constraints",
        "OrbID: Identifying Orbcomm Satellite RF Fingerprints",
        "TEMPO: A Python Package for Time Evolution of Pulse Sequences in QuTiP",
        "PRISM: A Robust Framework for Skill-based Meta-Reinforcement Learning\n  with Noisy Demonstrations",
        "Ultra-Low-Latency Edge Intelligent Sensing: A Source-Channel Tradeoff\n  and Its Application to Coding Rate Adaptation",
        "Ramp Up NTT in Record Time using GPU-Accelerated Algorithms and\n  LLM-based Code Generation"
      ],
      "abstract":[
        "Quantum graph states are critical resources for various quantum algorithms,\nand also determine essential interconnections in distributed quantum computing.\nThere are two schemes for generating graph states probabilistic scheme and\ndeterministic scheme. While the all-photonic probabilistic scheme has garnered\nsignificant attention, the emitter-photonic deterministic scheme has been\nproved to be more scalable and feasible across several hardware platforms.\n  This paper studies the GraphState-to-Circuit compilation problem in the\ncontext of the deterministic scheme. Previous research has primarily focused on\noptimizing individual circuit parameters, often neglecting the characteristics\nof quantum hardware, which results in impractical implementations.\nAdditionally, existing algorithms lack scalability for larger graph sizes. To\nbridge these gaps, we propose a novel compilation framework that partitions the\ntarget graph state into subgraphs, compiles them individually, and subsequently\ncombines and schedules the circuits to maximize emitter resource utilization.\nFurthermore, we incorporate local complementation to transform graph states and\nminimize entanglement overhead. Evaluation of our framework on various graph\ntypes demonstrates significant reductions in CNOT gates and circuit duration,\nup to 52% and 56%. Moreover, it enhances the suppression of photon loss,\nachieving improvements of up to x1.9.",
        "We analysed two large collaboration networks -- the Microsoft Academic Graph\n(1800-2020) and Internet Movie Database (1900-2020) -- to quantify network\nresponses to major historical events. Our analysis revealed four properties of\nnetwork-environment interaction. First, historical events can influence network\nevolution, with effects persisting far longer than previously recognised; the\nacademic network showed 45\\% declines during World Wars and 90\\% growth during\nLa Belle Epoque. Second, node and edge processes exhibited different\nenvironmental sensitivities; while node addition\/removal tracked historical\nevents, edge formation maintained stable statistical properties even during\nmajor disruptions. Third, different collaboration networks showed distinct\nresponse patterns; academic networks displayed sharp disruptions and rapid\nrecoveries, while entertainment networks showed gradual changes and greater\nresilience. Fourth, both networks developed increasing resilience. Our results\nprovide new insights for modelling network evolution and managing collaborative\nsystems during periods of external disruption.",
        "The Pathwidth Theorem states that if a class of graphs has unbounded\npathwidth, then it contains all trees as graph minors. We prove a similar\nresult for dense graphs: if a class of graphs has unbounded linear cliquewidth,\nthen it can produce all trees via some fixed CMSO transduction.",
        "Unipolar light pulses with a non-zero electric area due to the unidirectional\naction on charged particles can be used for the ultrafast control of the\nproperties of quantum systems. To control atomic properties in an efficient\nway, it is necessary to vary the temporal shape of the pulses used. This has\nled to the problem of obtaining pulses of an unusual shape, such as a\nrectangular one. A number of new phenomena, not possible with conventional\nmulti-cycle pulses, were discovered by analyzing the interaction of such\nunipolar pulses with matter. These include the formation of dynamic\nmicrocavities at each resonant transition of a multilevel medium when such\npulses collide with the medium. In this work, we compare the behavior of\ndynamic microcavities in a two-level and a three-level medium when unipolar\npulses of unusual shape (rectangular) are collided with the medium. We do this\non the basis of the numerical solution of the system for the density matrix of\nthe medium and the wave equation for the electric field. Medium parameters\ncorrespond to atomic hydrogen. It is shown that for rectangular pulses in a\nthree-level medium, the dynamics of the cavities can be very different from the\ntwo-level model, as opposed to pulses of other shapes (e.g. Gaussian shape).\nWhen the third level of the medium is taken into account, the self-induced\ntransparency-like regime disappears. Differences in the dynamics of resonators\nin a three-level medium are revealed when the pulses behave like 2{\\pi} pulses\nof self-induced transparency.",
        "We show that the Monge-Amp\\`ere eigenfunctions of general bounded convex\ndomains are globally Lipschitz. The same result holds for convex solutions to\ndegenerate Monge-Amp\\`ere equations of the form $\\det D^2 u =M|u|^p$ with zero\nboundary condition on general bounded convex domains in ${\\mathbb R}^n$ within\nthe sharp threshold $p>n-2$. As a consequence, we obtain global $W^{2, 1}$\nestimates for these solutions.",
        "Extensive research has investigated the integration of large language models\n(LLMs) with knowledge graphs to enhance the reasoning process. However,\nunderstanding how models perform reasoning utilizing structured graph knowledge\nremains underexplored. Most existing approaches rely on LLMs or retrievers to\nmake binary judgments regarding the utilization of knowledge, which is too\ncoarse. Meanwhile, there is still a lack of feedback mechanisms for reflection\nand correction throughout the entire reasoning path. This paper proposes an\nActive self-Reflection framework for knowledge Graph reasoning ARG, introducing\nfor the first time an end-to-end training approach to achieve iterative\nreasoning grounded on structured graphs. Within the framework, the model\nleverages special tokens to \\textit{actively} determine whether knowledge\nretrieval is necessary, performs \\textit{reflective} critique based on the\nretrieved knowledge, and iteratively reasons over the knowledge graph. The\nreasoning paths generated by the model exhibit high interpretability, enabling\ndeeper exploration of the model's understanding of structured knowledge.\nUltimately, the proposed model achieves outstanding results compared to\nexisting baselines in knowledge graph reasoning tasks.",
        "General relativity and its extensions including torsion identify stress\nenergy momentum as being proportional to the Einstein tensor, thus ensuring\nboth symmetry and conservation. Here we visualize stress energy and momentum by\nidentifying the associated relative fractional accelerations of geodesics\nencoded in the Einstein tensor. This also provides an intuitive explanation for\nthe vanishing divergence of the Einstein tensor. In order to obtain this same\nenergy and momentum for other actions such as that of Dirac theory including\ntorsion, we then review the various stress energy momentum tensors resulting\nfrom the variation of different quantities derived from parallel transport, and\ndetail their interrelationships. This provides an opportunity to revisit some\nclassic material from a geometric point of view, including Einstein-Cartan\ntheory, the Sciama-Kibble formalism, and the Belinfante-Rosenfeld relation,\nwhose derivation in the mostly pluses signature would seem to not be otherwise\nreadily available.",
        "We prove that the Christensen distance (resp., the Kadison-Kastler distance)\nbetween two $C^*$-subalgebras $\\mathcal{A}$ and $\\mathcal{B}$ of a\n$C^*$-algebra $\\mathcal{C}$ is equal to that between their enveloping von\nNeumann algebras $\\mathcal{A}^{**}$ and $\\mathcal{B}^{**}$ (resp., the tensor\nproduct algebras $\\mathcal{A} \\otimes^{\\min} \\mathcal{D}$ and $\\mathcal{B}\n\\otimes^{\\min} \\mathcal{D}$, for any unital commutative $C^*$-algebra\n$\\mathcal{D}$).",
        "We derive a reliable a posteriori error estimate for a cell-centered finite\nvolume scheme approximating a cross-diffusion system modeling ion transport\nthrough nanopores. To this end we derive an abstract stability framework that\nis independent of the numerical scheme and introduce a suitable (conforming)\nreconstruction of the numerical solution. The stability framework relies on\nsome simplifying assumption that coincide with those made in weak uniqueness\nresults for this system. This is the first a posteriori error estimate for a\ncross-diffusion system. Along the way, we derive a pointwise a posteriori error\nestimate for a finite volume scheme approximating the diffusion equation. We\nconduct numerical experiments showing that the error estimator scales with the\nsame order as the true error.",
        "High-resolution semantic segmentation is essential for applications such as\nimage editing, bokeh imaging, AR\/VR, etc. Unfortunately, existing datasets\noften have limited resolution and lack precise mask details and boundaries. In\nthis work, we build a large-scale, matting-level semantic segmentation dataset,\nnamed MaSS13K, which consists of 13,348 real-world images, all at 4K\nresolution. MaSS13K provides high-quality mask annotations of a number of\nobjects, which are categorized into seven categories: human, vegetation,\nground, sky, water, building, and others. MaSS13K features precise masks, with\nan average mask complexity 20-50 times higher than existing semantic\nsegmentation datasets. We consequently present a method specifically designed\nfor high-resolution semantic segmentation, namely MaSSFormer, which employs an\nefficient pixel decoder that aggregates high-level semantic features and\nlow-level texture features across three stages, aiming to produce\nhigh-resolution masks with minimal computational cost. Finally, we propose a\nnew learning paradigm, which integrates the high-quality masks of the seven\ngiven categories with pseudo labels from new classes, enabling MaSSFormer to\ntransfer its accurate segmentation capability to other classes of objects. Our\nproposed MaSSFormer is comprehensively evaluated on the MaSS13K benchmark\ntogether with 14 representative segmentation models. We expect that our\nmeticulously annotated MaSS13K dataset and the MaSSFormer model can facilitate\nthe research of high-resolution and high-quality semantic segmentation.\nDatasets and codes can be found at https:\/\/github.com\/xiechenxi99\/MaSS13K.",
        "At finite temperatures ($\\geq 10^7$K), $^{76}$Se is abundant in the core of\nmassive stars and electron capture on $^{76}$Se has a consequential role to\nplay in the dynamics of core collapse. The present work may be classified into\ntwo main categories. In the first phase, we study the nuclear structure\nproperties of $^{76}$Se using the interacting boson model-1 (IBM-1). The IBM-1\ninvestigations include the energy levels, $B(E2)$ values, and the prediction of\nthe geometry. We performed the extended consistent-Q formalism (ECQF)\ncalculation and later the triaxial formalism calculation (constructed by adding\nthe cubic term to the ECQF). The geometry of $^{76}$Se can be envisioned within\nthe formalism of the potential energy surface based on the classical limit of\nthe IBM-1 model.\n  In the second phase, we reconfirm the unblocking of the Gamow-Teller (GT)\nstrength in $^{76}$Se (a test case for nuclei having $N > 40$ and $Z < 40$).\nUsing the deformed pn-QRPA model, we calculate GT transitions, stellar electron\ncapture cross section (within the limit of low momentum transfer), and stellar\nweak rates for $^{76}$Se. The distinguishing feature of our calculation is a\nstate-by-state evaluation of stellar weak rates in a fully microscopic fashion.\nResults are compared with experimental data and previous calculations. The\ncalculated GT distribution fulfills the Ikeda sum rule. Rates for\n$\\beta$-delayed neutrons and emission probabilities are also calculated. Our\nstudy suggests that at high stellar temperatures and low densities, the\n$\\beta^+$-decay on $^{76}$Se should not be neglected and needs to be taken into\nconsideration along with electron capture rates for simulation of presupernova\nevolution of massive stars.",
        "Estimating the 6D pose of textureless objects from RBG images is an important\nproblem in robotics. Due to appearance ambiguities, rotational symmetries, and\nsevere occlusions, single-view based 6D pose estimators are still unable to\nhandle a wide range of objects, motivating research towards multi-view pose\nestimation and next-best-view prediction that addresses these limitations. In\nthis work, we propose a comprehensive active perception framework for\nestimating the 6D poses of textureless objects using only RGB images. Our\napproach is built upon a key idea: decoupling the 6D pose estimation into a\nsequential two-step process can greatly improve both accuracy and efficiency.\nFirst, we estimate the 3D translation of each object, resolving scale and depth\nambiguities inherent to RGB images. These estimates are then used to simplify\nthe subsequent task of determining the 3D orientation, which we achieve through\ncanonical scale template matching. Building on this formulation, we then\nintroduce an active perception strategy that predicts the next best camera\nviewpoint to capture an RGB image, effectively reducing object pose uncertainty\nand enhancing pose accuracy. We evaluate our method on the public ROBI dataset\nas well as on a transparent object dataset that we created. When evaluated\nusing the same camera viewpoints, our multi-view pose estimation significantly\noutperforms state-of-the-art approaches. Furthermore, by leveraging our\nnext-best-view strategy, our method achieves high object pose accuracy with\nsubstantially fewer viewpoints than heuristic-based policies.",
        "In this paper, we study the machine learning elements which we are interested\nin together as a machine learning system, consisting of a collection of machine\nlearning elements and a collection of relations between the elements. The\nrelations we concern are algebraic operations, binary relations, and binary\nrelations with composition that can be reasoned categorically. A machine\nlearning system transformation between two systems is a map between the\nsystems, which preserves the relations we concern. The system transformations\ngiven by quotient or clustering, representable functor, and Yoneda embedding\nare highlighted and discussed by machine learning examples. An adjunction\nbetween machine learning systems, a special machine learning system\ntransformation loop, provides the optimal way of solving problems. Machine\nlearning system transformations are linked and compared by their maps at\n2-cell, natural transformations. New insights and structures can be obtained\nfrom universal properties and algebraic structures given by monads, which are\ngenerated from adjunctions.",
        "Stray electric fields induce excess micromotion in ion traps, limiting\nexperimental performance. We present a new micromotion-compensation technique\nthat utilizes a dark ion in a bright-dark-bright linear ion crystal. Stray\nelectric fields in the radial plane of the trap deform the crystal axially. We\nexploit the mode softening near the transition to the zig-zag configuration to\nincrease our sensitivity dramatically. We corroborate our results with a\nmodified ion-displacement compensation method using a single bright ion. Our\nmodification allows us to compensate stray fields on the 2D radial plane from a\n1D measurement of the ion position on the camera. Both methods require only a\nfixed imaging camera and continuous ion-fluorescence detection. As such, they\ncan be readily implemented in virtually any ion-trapping experiment without\nadditional hardware modifications.",
        "We study connection points on the double regular $n$-gon translation surface,\nfor $n \\geq 7$ odd and its staircase model. For $n \\neq 9$, we provide a large\nfamily of points with coordinates in the trace field that are not connection\npoints. This family includes the central points, and for $n=7$ we conjecture\nthat all the remaining points are connection points. Further, in the case where\n$n \\geq 7$ is a prime number, we provide a constructive proof by exhibiting an\nexplicit separatrix passing through a central point that does not extend to a\nsaddle connection.",
        "Despite advanced token compression techniques, existing multimodal large\nlanguage models (MLLMs) still struggle with hour-long video understanding. In\nthis work, we propose Video-XL-Pro, an efficient method for extremely long\nvideo understanding, built upon Reconstructive Compression of Tokens (ReCoT), a\nlearnable module that leverages self-supervised learning to generate\ncomprehensive and compact video tokens. ReCoT introduces two key components:\n(i) Dynamic Token Synthesizer (DTS): DTS generates pseudo-video tokens from\nstatic image tokens by learning intra-token relationships, which are then used\nin masked video modeling. (ii) Semantic-Guided Masking (SGM): SGM adaptively\nmasks redundant visual tokens to facilitate more effective reconstructive\nlearning. To improve training efficiency in MLLMs fine-tuning, we introduce a\nvideo-specific dataset pruning strategy and design a simple yet Query-aware\nSelector that enables the model to precisely locate query-relevant video\ntokens. With only 3B parameters, Video-XL-Pro outperforms most 7B models\ntrained on larger datasets across multiple long video understanding benchmarks.\nMoreover, it can process over 8K frames on a single A100 GPU while maintaining\nhigh-quality performance.",
        "We introduce a functional reactive programming language that extends\nWORMHOLES, an enhancement of YAMPA with support for effects. Our proposal\nrelaxes the constraint in WORMHOLES that restricts all resources to single-use.\nResources are categorized into two kinds: input\/output resources and internal\nresources. Input\/output resources model interactions with the environment and\nfollow constraints similar to those in WORMHOLES. Internal resources, on the\nother hand, enable communication between program components and can be used\nmultiple times. We demonstrate that programs written in our language can be\ntranslated into equivalent effect-free YAMPA programs, ensuring that our\napproach remains compatible with existing functional reactive paradigms.",
        "White's conjecture predicts quadratic generators for the ideal of any matroid\nbase polytope. We prove that White's conjecture for any matroid $M$ implies it\nalso for any matroid $M'$, where $M$ and $M'$ differ by one basis. Our study is\nmotivated by inner projections of algebraic varieties.",
        "Let $X$ be a smooth projective variety of dimension $d$ over an arbitrary\nbase field $k$ and $CH^n(X)_{\\mathbb Q}$ be the $\\mathbb Q$-vector space of\ncodimension $n$ algebraic cycles of $X$ modulo rational equivalence, $1\\leq n\n\\leq d$. Consider the $\\mathbb Q$-vector subspaces $CH^n(X)_{\\mathbb Q}\n\\supseteq CH^n_{\\mathrm{alg}}(X)_{\\mathbb Q} \\supseteq\nCH^n_{\\mathrm{inc}}(X)_{\\mathbb Q}$ of algebraic cycles which are,\nrespectively, algebraically and incident (in the sense of Griffiths) equivalent\nto zero.\n  Our main result computes $CH^d_{\\mathrm{inc}}(X)_{\\mathbb Q}$ (which\ncoincides with the Albanese kernel $T(X)_{\\mathbb Q}$ when $k$ is algebraically\nclosed) in terms of Voevodsky's triangulated category of motives $DM_k$,\nnamely, we show that $CH^d_{\\mathrm{inc}}(X)_{\\mathbb Q}$ is given by the\nsecond step of the orthogonal filtration $F^{\\bullet}$ on $CH^d(X)_{\\mathbb\nQ}$, i.e. $F^2 CH^d (X)_{\\mathbb Q}= CH^d_{\\mathrm{inc}}(X)_{\\mathbb Q}$. The\northogonal filtration $F^\\bullet$ on $CH^n(X)_{\\mathbb Q}$ was introduced by\nthe first author, and is an unconditionally finite filtration satisfying\nseveral of the properties of the still conjectural Bloch-Beilinson filtration.\n  We also prove that the exterior product and intersection product of algebraic\ncycles algebraically equivalent to zero is contained in the second step of the\northogonal filtration.\n  Furthermore, if we assume that the field $k$ is either finite or the\nalgebraic closure of a finite field, then the main result holds in any\ncodimension, i.e. $F^2 CH^n_{\\mathrm{alg}}(X)_{\\mathbb Q}=\nCH^n_{\\mathrm{inc}}(X)_{\\mathbb Q}$. We also compute in the whole Chow group,\n$CH^n(X)_{\\mathbb Q}$, the second step of the orthogonal filtration $F^2\nCH^n(X)_{\\mathbb Q}$ in terms of the vanishing of several intersection\npairings.",
        "An ordered moment approach to exact open quantum dynamics is presented, which\nbypasses the Feynmann-Vernon influence functional formalism. The hierarchical\nequations of motion are constructed using Wick's contraction, which follows\nspecific orderings of the bath's creation and annihilation operators. Our\napproach moves beyond the traditional influence functional formalism, offering\na more intuitive and direct framework, and extends the applicability of theory\nto nonlinear system--bath coupling scenarios.",
        "The improper disposal and mismanagement of medical waste pose severe\nenvironmental and public health risks, contributing to greenhouse gas emissions\nand the spread of infectious diseases. Efficient and accurate medical waste\nclassification is crucial for mitigating these risks. We explore the\nintegration of capsule networks with a pretrained DenseNet model to improve\nmedical waste classification. To the best of our knowledge, capsule networks\nhave not yet been applied to this task, making this study the first to assess\ntheir effectiveness.\n  A diverse dataset of medical waste images collected from multiple public\nsources, is used to evaluate three model configurations: (1) a pretrained\nDenseNet model as a baseline, (2) a pretrained DenseNet with frozen layers\ncombined with a capsule network, and (3) a pretrained DenseNet with unfrozen\nlayers combined with a capsule network. Experimental results demonstrate that\nincorporating capsule networks improves classification performance, with F1\nscores increasing from 0.89 (baseline) to 0.92 (hybrid model with unfrozen\nlayers). This highlights the potential of capsule networks to address the\nspatial limitations of traditional convolutional models and improve\nclassification robustness.\n  While the capsule-enhanced model demonstrated improved classification\nperformance, direct comparisons with prior studies were challenging due to\ndifferences in dataset size and diversity. Previous studies relied on smaller,\ndomain-specific datasets, which inherently yielded higher accuracy. In\ncontrast, our study employs a significantly larger and more diverse dataset,\nleading to better generalization but introducing additional classification\nchallenges. This highlights the trade-off between dataset complexity and model\nperformance.",
        "We numerically investigate the athermal creep deformation of amorphous\nmaterials having a wide range of stability. The imposed shear stress serves as\nthe control parameter, allowing us to examine the time-dependent transient\nresponse through both the macroscopic strain and microscopic observables. Least\nstable samples exhibit monotonicity in the transient strain rate versus time,\nwhile more stable samples display a pronounced non-monotonic S-shaped curve,\ncorresponding to failure by sharp shear band formation. We identify a diverging\ntimescale associated with the fluidization process and extract the\ncorresponding critical exponents. Our results are compared with predictions\nfrom existing scaling theories relevant to soft matter systems. The numerical\nfindings for stable, brittle-like materials represent a challenge for\ntheoretical descriptions. We monitor the microscopic initiation of shear bands\nduring creep responses. Our study encompasses creep deformation across a\nvariety of materials ranging from ductile soft matter to brittle metallic and\noxide glasses, all within the same numerical framework.",
        "In this paper, we consider nonconvex optimization problems with nonlinear\nequality constraints. We assume that the objective function and the functional\nconstraints are locally smooth. To solve this problem, we introduce a\nlinearized $\\ell_q$ penalty based method, where $q \\in (1,2]$ is the parameter\ndefining the norm used in the construction of the penalty function. Our method\ninvolves linearizing the objective function and functional constraints in a\nGauss-Newton fashion at the current iteration in the penalty formulation and\nintroduces a quadratic regularization. This approach yields an easily solvable\nsubproblem, whose solution becomes the next iterate. By using a novel dynamic\nrule for the choice of the regularization parameter, we establish that the\niterates of our method converge to an $\\epsilon$-first-order solution in\n$\\mathcal{O}(1\/{\\epsilon^{2+ (q-1)\/q}})$ outer iterations. Finally, we put\ntheory into practice and evaluate the performance of the proposed algorithm by\nmaking numerical comparisons with existing methods from literature.",
        "An increase in availability of Software Defined Radios (SDRs) has caused a\ndramatic shift in the threat landscape of legacy satellite systems, opening\nthem up to easy spoofing attacks by low-budget adversaries. Physical-layer\nauthentication methods can help improve the security of these systems by\nproviding additional validation without modifying the space segment. This paper\nextends previous research on Radio Frequency Fingerprinting (RFF) of satellite\ncommunication to the Orbcomm satellite formation. The GPS and Iridium\nconstellations are already well covered in prior research, but the feasibility\nof transferring techniques to other formations has not yet been examined, and\nraises previously undiscussed challenges.\n  In this paper, we collect a novel dataset containing 8992474 packets from the\nOrbcom satellite constellation using different SDRs and locations. We use this\ndataset to train RFF systems based on convolutional neural networks. We achieve\nan ROC AUC score of 0.53 when distinguishing different satellites within the\nconstellation, and 0.98 when distinguishing legitimate satellites from SDRs in\na spoofing scenario. We also demonstrate the possibility of mixing datasets\nusing different SDRs in different physical locations.",
        "TEMPO (Time-dependent Evolution of Multiple Pulse Operations) offers\naccessible and efficient simulations of pulse sequences in Python, using the\nsuite of master equation solvers available in the Quantum Toolbox in Python\n(QuTiP). It enables straightforward definition of pulse sequence structures,\nincluding any underlying time-dependent Hamiltonians and pulse timing\ninformation, and faster simulations of pulse sequence dynamics (compared to\nnaive implementations using QuTiP) while remaining compatible with the existing\ncollection of QuTiP subpackages. Given the ubiquitous use of pulse sequences\nthroughout quantum information\/computing sciences, magnetic resonance studies,\nand quantum metrology, this work has immediate relevance to a wide array of\nresearch applications.",
        "Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen\ntasks but faces challenges in long-horizon environments. Skill-based approaches\ntackle this by decomposing state-action sequences into reusable skills and\nemploying hierarchical decision-making. However, these methods are highly\nsusceptible to noisy offline demonstrations, resulting in unstable skill\nlearning and degraded performance. To overcome this, we propose Prioritized\nRefinement for Skill-Based Meta-RL (PRISM), a robust framework that integrates\nexploration near noisy data to generate online trajectories and combines them\nwith offline data. Through prioritization, PRISM extracts high-quality data to\nlearn task-relevant skills effectively. By addressing the impact of noise, our\nmethod ensures stable skill learning and achieves superior performance in\nlong-horizon tasks, even with noisy and sub-optimal data.",
        "The forthcoming sixth-generation (6G) mobile network is set to merge edge\nartificial intelligence (AI) and integrated sensing and communication (ISAC)\nextensively, giving rise to the new paradigm of edge intelligent sensing\n(EI-Sense). This paradigm leverages ubiquitous edge devices for environmental\nsensing and deploys AI algorithms at edge servers to interpret the observations\nvia remote inference on wirelessly uploaded features. A significant challenge\narises in designing EI-Sense systems for 6G mission-critical applications,\nwhich demand high performance under stringent latency constraints. To tackle\nthis challenge, we focus on the end-to-end (E2E) performance of EI-Sense and\ncharacterize a source-channel tradeoff that balances source distortion and\nchannel reliability. In this work, we establish a theoretical foundation for\nthe source-channel tradeoff by quantifying the effects of source coding on\nfeature discriminant gains and channel reliability on packet loss. Building on\nthis foundation, we design the coding rate control by optimizing the tradeoff\nto minimize the E2E sensing error probability, leading to a low-complexity\nalgorithm for ultra-low-latency EI-Sense. Finally, we validate our theoretical\nanalysis and proposed coding rate control algorithm through extensive\nexperiments on both synthetic and real datasets, demonstrating the sensing\nperformance gain of our approach with respect to traditional\nreliability-centric methods.",
        "Homomorphic encryption (HE) is a core building block in privacy-preserving\nmachine learning (PPML), but HE is also widely known as its efficiency\nbottleneck. Therefore, many GPU-accelerated cryptographic schemes have been\nproposed to improve the performance of HE. However, these methods often require\ncomplex modifications tailored to specific algorithms and are tightly coupled\nwith specific GPU and operating systems. It is interesting to ask how to\ngenerally offer more practical GPU-accelerated cryptographic algorithm\nimplementations. Given the powerful code generation capabilities of large\nlanguage models (LLMs), we aim to explore their potential to automatically\ngenerate practical GPU-friendly algorithm code using CPU-friendly code. In this\npaper, we focus on number theoretic transform (NTT) -- the core mechanism of\nHE. We first develop and optimize a GPU-friendly NTT (GNTT) family that\nexploits PyTorch's fast matrix computation and precomputation, achieving an\napproximately 62x speedup -- a significant boost over existing ones. Then we\nexplore GPU-friendly code generation using various LLMs, including DeepSeek-R1,\nOpenAI o1 and o3-mini. We discover many interesting findings throughout the\nprocess. For instance, somewhat surprisingly, our experiments demonstrate that\nDeepSeek-R1 significantly outperforms OpenAI o3-mini and o1, but still cannot\nbeat our optimized protocol. The findings provide valuable insights for\nturbocharging PPML and enhancing code generation capabilities of LLMs. Codes\nare available at: https:\/\/github.com\/LMPC-Lab\/GenGPUCrypto."
      ]
    }
  },
  {
    "id":2411.07453,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Review of Research on Condition Assessment of Nuclear Power Plant Equipment Based on Data-Driven",
    "start_abstract":"The condition assessment of the entire life cycle of nuclear power equipment has a significant impact on improving the safety and economy of nuclear power plants. In the past, operation and maintenance of systems, equipment, and structures of domestic nuclear power plants, mostly relied on the alarm mechanism of equipments, the simple threshold judgments of parameters, or the empirical judgments of engineers. With the implementation of online monitoring system in nuclear power plants, a large number of equipment operation data have been accumulated, and the use of data-driven technology to assess the health of equipment has become the focus of attention in the industry. In this paper, the current situation of the online monitoring system of nuclear power equipment was introduced and the common malfunction of nuclear power equipment was analyzed. The condition assessment of nuclear power equipment were categorized into three major problems (i.e., anomaly detection, life prediction, and fault diagnosis), the situation of research and application were summarized respectively, and the application potential of deep learning technology in this field was emphasized. Based on this, the challenges and possible solutions to the condition assessment of nuclear power plant equipment were further analyzed.",
    "start_categories":[
      "nucl-th"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      ],
      "abstract":[
        "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth\/width\/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 \/ 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Reward Models Identify Consistency, Not Causality",
        "On the Data-Driven Modeling of Price-Responsive Flexible Loads:\n  Formulation and Algorithm",
        "Using Large Language Models for Solving Thermodynamic Problems",
        "Advancing Precision Oncology Through Modeling of Longitudinal and\n  Multimodal Data",
        "Spherical Dense Text-to-Image Synthesis",
        "Expressive Music Data Processing and Generation",
        "Show Me Your Code! Kill Code Poisoning: A Lightweight Method Based on\n  Code Naturalness",
        "Language-agnostic, automated assessment of listeners' speech recall\n  using large language models",
        "Beyond Pairwise: Global Zero-shot Temporal Graph Generation",
        "Temporal Context Awareness: A Defense Framework Against Multi-turn\n  Manipulation Attacks on Large Language Models",
        "Dimension of diagonal self-affine measures with exponentially separated\n  projections",
        "Global linearization without hyperbolicity",
        "TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image\n  Diffusion Models",
        "Design of Bayesian Clinical Trials with Clustered Data and Multiple\n  Endpoints",
        "Asynchronous Cooperative Multi-Agent Reinforcement Learning with Limited\n  Communication",
        "A note on the existence of nontrivial zero modes on Riemannian manifolds",
        "Optimal Binary Variable-Length Codes with a Bounded Number of 1's per\n  Codeword: Design, Analysis, and Applications",
        "Improved high-index saddle dynamics for finding saddle points and\n  solution landscape",
        "Adaptive refinement for eigenvalue problems based on an associated\n  source problem",
        "One for All: A General Framework of LLMs-based Multi-Criteria Decision\n  Making on Human Expert Level",
        "High-frequency lead-lag relationships in the Chinese stock index futures\n  market: tick-by-tick dynamics of calendar spreads",
        "Pareto-undominated strategy-proof rules in economies with\n  multidimensional single-peaked preferences",
        "On Quantizing Neural Representation for Variable-Rate Video Coding",
        "Estimating Time Delays between Signals under Mixed Noise Influence with\n  Novel Cross- and Bispectral Methods",
        "Thresholds for the biased Maker-Breaker domination games",
        "Towards Safer Social Media Platforms: Scalable and Performant Few-Shot\n  Harmful Content Moderation Using Large Language Models",
        "ESPnet-SDS: Unified Toolkit and Demo for Spoken Dialogue Systems",
        "On $p$-adic Siegel--Eisenstein series II: How to avoid the regularity\n  condition for $p$",
        "Social hierarchy shapes foraging decisions"
      ],
      "abstract":[
        "Reward models (RMs) play a crucial role in aligning large language models\n(LLMs) with human preferences and enhancing reasoning quality. Traditionally,\nRMs are trained to rank candidate outputs based on their correctness and\ncoherence. However, in this work, we present several surprising findings that\nchallenge common assumptions about RM behavior. Our analysis reveals that\nstate-of-the-art reward models prioritize structural consistency over causal\ncorrectness. Specifically, removing the problem statement has minimal impact on\nreward scores, whereas altering numerical values or disrupting the reasoning\nflow significantly affects RM outputs. Furthermore, RMs exhibit a strong\ndependence on complete reasoning trajectories truncated or incomplete steps\nlead to significant variations in reward assignments, indicating that RMs\nprimarily rely on learned reasoning patterns rather than explicit problem\ncomprehension. These findings hold across multiple architectures, datasets, and\ntasks, leading to three key insights: (1) RMs primarily assess coherence rather\nthan true reasoning quality; (2) The role of explicit problem comprehension in\nreward assignment is overstated; (3) Current RMs may be more effective at\nranking responses than verifying logical validity. Our results suggest a\nfundamental limitation in existing reward modeling approaches, emphasizing the\nneed for a shift toward causality-aware reward models that go beyond\nconsistency-driven evaluation.",
        "The flexible loads in power systems, such as interruptible and transferable\nloads, are critical flexibility resources for mitigating power imbalances.\nDespite their potential, accurate modeling of these loads is a challenging work\nand has not received enough attention, limiting their integration into\noperational frameworks. To bridge this gap, this paper develops a data-driven\nidentification theory and algorithm for price-responsive flexible loads\n(PRFLs). First, we introduce PRFL models that capture both static and dynamic\ndecision mechanisms governing their response to electricity price variations.\nSecond, We develop a data-driven identification framework that explicitly\nincorporates forecast and measurement errors. Particularly, we give a\ntheoretical analysis to quantify the statistical impact of such noise on\nparameter estimation. Third, leveraging the bilevel structure of the\nidentification problem, we propose a Bayesian optimization-based algorithm that\nfeatures the scalability to large sample sizes and the ability to offer\nposterior differentiability certificates as byproducts. Numerical tests\ndemonstrate the effectiveness and superiority of the proposed approach.",
        "Large Language Models (LLMs) have made significant progress in reasoning,\ndemonstrating their capability to generate human-like responses. This study\nanalyzes the problem-solving capabilities of LLMs in the domain of\nthermodynamics. A benchmark of 22 thermodynamic problems to evaluate LLMs is\npresented that contains both simple and advanced problems. Five different LLMs\nare assessed: GPT-3.5, GPT-4, and GPT-4o from OpenAI, Llama 3.1 from Meta, and\nle Chat from MistralAI. The answers of these LLMs were evaluated by trained\nhuman experts, following a methodology akin to the grading of academic exam\nresponses. The scores and the consistency of the answers are discussed,\ntogether with the analytical skills of the LLMs. Both strengths and weaknesses\nof the LLMs become evident. They generally yield good results for the simple\nproblems, but also limitations become clear: The LLMs do not provide consistent\nresults, they often fail to fully comprehend the context and make wrong\nassumptions. Given the complexity and domain-specific nature of the problems,\nthe statistical language modeling approach of the LLMs struggles with the\naccurate interpretation and the required reasoning. The present results\nhighlight the need for more systematic integration of thermodynamic knowledge\nwith LLMs, for example, by using knowledge-based methods.",
        "Cancer evolves continuously over time through a complex interplay of genetic,\nepigenetic, microenvironmental, and phenotypic changes. This dynamic behavior\ndrives uncontrolled cell growth, metastasis, immune evasion, and therapy\nresistance, posing challenges for effective monitoring and treatment. However,\ntoday's data-driven research in oncology has primarily focused on\ncross-sectional analysis using data from a single modality, limiting the\nability to fully characterize and interpret the disease's dynamic\nheterogeneity. Advances in multiscale data collection and computational methods\nnow enable the discovery of longitudinal multimodal biomarkers for precision\noncology. Longitudinal data reveal patterns of disease progression and\ntreatment response that are not evident from single-timepoint data, enabling\ntimely abnormality detection and dynamic treatment adaptation. Multimodal data\nintegration offers complementary information from diverse sources for more\nprecise risk assessment and targeting of cancer therapy. In this review, we\nsurvey methods of longitudinal and multimodal modeling, highlighting their\nsynergy in providing multifaceted insights for personalized care tailored to\nthe unique characteristics of a patient's cancer. We summarize the current\nchallenges and future directions of longitudinal multimodal analysis in\nadvancing precision oncology.",
        "Recent advancements in text-to-image (T2I) have improved synthesis results,\nbut challenges remain in layout control and generating omnidirectional\npanoramic images. Dense T2I (DT2I) and spherical T2I (ST2I) models address\nthese issues, but so far no unified approach exists. Trivial approaches, like\nprompting a DT2I model to generate panoramas can not generate proper spherical\ndistortions and seamless transitions at the borders. Our work shows that\nspherical dense text-to-image (SDT2I) can be achieved by integrating\ntraining-free DT2I approaches into finetuned panorama models. Specifically, we\npropose MultiStitchDiffusion (MSTD) and MultiPanFusion (MPF) by integrating\nMultiDiffusion into StitchDiffusion and PanFusion, respectively. Since no\nbenchmark for SDT2I exists, we further construct Dense-Synthetic-View\n(DSynView), a new synthetic dataset containing spherical layouts to evaluate\nour models. Our results show that MSTD outperforms MPF across image quality as\nwell as prompt- and layout adherence. MultiPanFusion generates more diverse\nimages but struggles to synthesize flawless foreground objects. We propose\nbootstrap-coupling and turning off equirectangular perspective-projection\nattention in the foreground as an improvement of MPF. Link to code\nhttps:\/\/github.com\/sdt2i\/spherical-dense-text-to-image",
        "Musical expressivity and coherence are indispensable in music composition and\nperformance, while often neglected in modern AI generative models. In this\nwork, we introduce a listening-based data-processing technique that captures\nthe expressivity in musical performance. This technique derived from Weber's\nlaw reflects the human perceptual truth of listening and preserves musical\nsubtlety and expressivity in the training input. To facilitate musical\ncoherence, we model the output interdependencies among multiple arguments in\nthe music data such as pitch, duration, velocity, etc. in the neural networks\nbased on the probabilistic chain rule. In practice, we decompose the\nmulti-output sequential model into single-output submodels and condition\npreviously sampled outputs on the subsequent submodels to induce conditional\ndistributions. Finally, to select eligible sequences from all generations, a\ntentative measure based on the output entropy was proposed. The entropy\nsequence is set as a criterion to select predictable and stable generations,\nwhich is further studied under the context of informational aesthetic measures\nto quantify musical pleasure and information gain along the music tendency.",
        "Neural code models (NCMs) have demonstrated extraordinary capabilities in\ncode intelligence tasks. Meanwhile, the security of NCMs and NCMs-based systems\nhas garnered increasing attention. In particular, NCMs are often trained on\nlarge-scale data from potentially untrustworthy sources, providing attackers\nwith the opportunity to manipulate them by inserting crafted samples into the\ndata. This type of attack is called a code poisoning attack (also known as a\nbackdoor attack). It allows attackers to implant backdoors in NCMs and thus\ncontrol model behavior, which poses a significant security threat. However,\nthere is still a lack of effective techniques for detecting various complex\ncode poisoning attacks.\n  In this paper, we propose an innovative and lightweight technique for code\npoisoning detection named KillBadCode. KillBadCode is designed based on our\ninsight that code poisoning disrupts the naturalness of code. Specifically,\nKillBadCode first builds a code language model (CodeLM) on a lightweight\n$n$-gram language model. Then, given poisoned data, KillBadCode utilizes CodeLM\nto identify those tokens in (poisoned) code snippets that will make the code\nsnippets more natural after being deleted as trigger tokens. Considering that\nthe removal of some normal tokens in a single sample might also enhance code\nnaturalness, leading to a high false positive rate (FPR), we aggregate the\ncumulative improvement of each token across all samples. Finally, KillBadCode\npurifies the poisoned data by removing all poisoned samples containing the\nidentified trigger tokens. The experimental results on two code poisoning\nattacks and four code intelligence tasks demonstrate that KillBadCode\nsignificantly outperforms four baselines. More importantly, KillBadCode is very\nefficient, with a minimum time consumption of only 5 minutes, and is 25 times\nfaster than the best baseline on average.",
        "Speech-comprehension difficulties are common among older people. Standard\nspeech tests do not fully capture such difficulties because the tests poorly\nresemble the context-rich, story-like nature of ongoing conversation and are\ntypically available only in a country's dominant\/official language (e.g.,\nEnglish), leading to inaccurate scores for native speakers of other languages.\nAssessments for naturalistic, story speech in multiple languages require\naccurate, time-efficient scoring. The current research leverages modern large\nlanguage models (LLMs) in native English speakers and native speakers of 10\nother languages to automate the generation of high-quality, spoken stories and\nscoring of speech recall in different languages. Participants listened to and\nfreely recalled short stories (in quiet\/clear and in babble noise) in their\nnative language. LLM text-embeddings and LLM prompt engineering with semantic\nsimilarity analyses to score speech recall revealed sensitivity to known\neffects of temporal order, primacy\/recency, and background noise, and high\nsimilarity of recall scores across languages. The work overcomes limitations\nassociated with simple speech materials and testing of closed native-speaker\ngroups because recall data of varying length and details can be mapped across\nlanguages with high accuracy. The full automation of speech generation and\nrecall scoring provides an important step towards comprehension assessments of\nnaturalistic speech with clinical applicability.",
        "Temporal relation extraction (TRE) is a fundamental task in natural language\nprocessing (NLP) that involves identifying the temporal relationships between\nevents in a document. Despite the advances in large language models (LLMs),\ntheir application to TRE remains limited. Most existing approaches rely on\npairwise classification, in which event pairs are considered individually,\nleading to computational inefficiency and a lack of global consistency in the\nresulting temporal graph. In this work, we propose a novel zero-shot method for\nTRE that generates a document's complete temporal graph at once, then applies\ntransitive constraints optimization to refine predictions and enforce temporal\nconsistency across relations. Additionally, we introduce OmniTemp, a new\ndataset with complete annotations for all pairs of targeted events within a\ndocument. Through experiments and analyses, we demonstrate that our method\nsignificantly outperforms existing zero-shot approaches while achieving\ncompetitive performance with supervised models.",
        "Large Language Models (LLMs) are increasingly vulnerable to sophisticated\nmulti-turn manipulation attacks, where adversaries strategically build context\nthrough seemingly benign conversational turns to circumvent safety measures and\nelicit harmful or unauthorized responses. These attacks exploit the temporal\nnature of dialogue to evade single-turn detection methods, representing a\ncritical security vulnerability with significant implications for real-world\ndeployments.\n  This paper introduces the Temporal Context Awareness (TCA) framework, a novel\ndefense mechanism designed to address this challenge by continuously analyzing\nsemantic drift, cross-turn intention consistency and evolving conversational\npatterns. The TCA framework integrates dynamic context embedding analysis,\ncross-turn consistency verification, and progressive risk scoring to detect and\nmitigate manipulation attempts effectively. Preliminary evaluations on\nsimulated adversarial scenarios demonstrate the framework's potential to\nidentify subtle manipulation patterns often missed by traditional detection\ntechniques, offering a much-needed layer of security for conversational AI\nsystems. In addition to outlining the design of TCA , we analyze diverse attack\nvectors and their progression across multi-turn conversation, providing\nvaluable insights into adversarial tactics and their impact on LLM\nvulnerabilities. Our findings underscore the pressing need for robust,\ncontext-aware defenses in conversational AI systems and highlight TCA framework\nas a promising direction for securing LLMs while preserving their utility in\nlegitimate applications. We make our implementation available to support\nfurther research in this emerging area of AI security.",
        "Let $ \\mu $ be a self-affine measure associated with a diagonal affine\niterated function system (IFS) $ \\Phi = \\{ (x_{1}, \\ldots, x_{d}) \\mapsto (\nr_{i, 1}x_{1} + t_{i,1}, \\ldots, r_{i,d}x_{d} + t_{i,d}) \\}_{i\\in\\Lambda} $ on\n$ \\mathbb{R}^{d} $ and a probability vector $ p = (p_{i})_{i\\in\\Lambda}$. For $\n1 \\leq j \\leq d $, denote the $ j $-th the Lyapunov exponent by $ \\chi_{j} :=\n\\sum_{i\\in\\Lambda} - p_{i} \\log | r_{i,j} |$, and define the IFS induced by $\n\\Phi $ on the $j$-th coordinate as $ \\Phi_{j} := \\{ x \\mapsto r_{i,j}x +\nt_{i,j}\\}_{i\\in\\Lambda}$. We prove that if $ \\chi_{j_{1}} \\neq \\chi_{j_{2}} $\nfor $ 1 \\leq j_{1} < j_{2} \\leq d $, and $ \\Phi_{j}$ is exponentially separated\nfor $ 1 \\leq j \\leq d $, then the dimension of $ \\mu $ is the minimum of $ d $\nand its Lyapunov dimension. This confirms a conjecture of Rapaport by removing\nthe additional assumption that the linear parts of the maps in $ \\Phi $ are\ncontained in a 1-dimensional subgroup. One of the main ingredients of the proof\ninvolves disintegrating $ \\mu $ into random measures with convolution\nstructure. In the course of the proof, we establish new results on dimension\nand entropy increase for these random measures.",
        "We give a proof of an extension of the Hartman-Grobman theorem to\nnonhyperbolic but asymptotically stable equilibria of vector fields. Moreover,\nthe linearizing topological conjugacy is (i) defined on the entire basin of\nattraction if the vector field is complete, and (ii) a $C^{k\\geq 1}$\ndiffeomorphism on the complement of the equilibrium if the vector field is\n$C^k$ and the underlying space is not $5$-dimensional. We also show that the\n$C^k$ statement in the $5$-dimensional case is equivalent to the\n$4$-dimensional smooth Poincar\\'{e} conjecture.",
        "Recent advances in text-to-image diffusion models enable photorealistic image\ngeneration, but they also risk producing malicious content, such as NSFW\nimages. To mitigate risk, concept erasure methods are studied to facilitate the\nmodel to unlearn specific concepts. However, current studies struggle to fully\nerase malicious concepts implicitly embedded in prompts (e.g., metaphorical\nexpressions or adversarial prompts) while preserving the model's normal\ngeneration capability. To address this challenge, our study proposes TRCE,\nusing a two-stage concept erasure strategy to achieve an effective trade-off\nbetween reliable erasure and knowledge preservation. Firstly, TRCE starts by\nerasing the malicious semantics implicitly embedded in textual prompts. By\nidentifying a critical mapping objective(i.e., the [EoT] embedding), we\noptimize the cross-attention layers to map malicious prompts to contextually\nsimilar prompts but with safe concepts. This step prevents the model from being\noverly influenced by malicious semantics during the denoising process.\nFollowing this, considering the deterministic properties of the sampling\ntrajectory of the diffusion model, TRCE further steers the early denoising\nprediction toward the safe direction and away from the unsafe one through\ncontrastive learning, thus further avoiding the generation of malicious\ncontent. Finally, we conduct comprehensive evaluations of TRCE on multiple\nmalicious concept erasure benchmarks, and the results demonstrate its\neffectiveness in erasing malicious concepts while better preserving the model's\noriginal generation ability. The code is available at:\nhttp:\/\/github.com\/ddgoodgood\/TRCE. CAUTION: This paper includes model-generated\ncontent that may contain offensive material.",
        "In the design of clinical trials, it is essential to assess the design\noperating characteristics (i.e., the probabilities of making correct\ndecisions). Common practice for the evaluation of operating characteristics in\nBayesian clinical trials relies on estimating the sampling distribution of\nposterior summaries via Monte Carlo simulation. It is computationally intensive\nto repeat this estimation process for each design configuration considered,\nparticularly for clustered data that are analyzed using complex,\nhigh-dimensional models. In this paper, we propose an efficient method to\nassess operating characteristics and determine sample sizes for Bayesian trials\nwith clustered data and multiple endpoints. We prove theoretical results that\nenable posterior probabilities to be modelled as a function of the sample size.\nUsing these functions, we assess operating characteristics at a range of sample\nsizes given simulations conducted at only two sample sizes. These theoretical\nresults are also leveraged to quantify the impact of simulation variability on\nour sample size recommendations. The applicability of our methodology is\nillustrated using a current clinical trial with clustered data.",
        "We consider the problem setting in which multiple autonomous agents must\ncooperatively navigate and perform tasks in an unknown,\ncommunication-constrained environment. Traditional multi-agent reinforcement\nlearning (MARL) approaches assume synchronous communications and perform poorly\nin such environments. We propose AsynCoMARL, an asynchronous MARL approach that\nuses graph transformers to learn communication protocols from dynamic graphs.\nAsynCoMARL can accommodate infrequent and asynchronous communications between\nagents, with edges of the graph only forming when agents communicate with each\nother. We show that AsynCoMARL achieves similar success and collision rates as\nleading baselines, despite 26\\% fewer messages being passed between agents.",
        "We prove a necessary criterion for the (non-)existence of nontrivial\nsolutions to the Dirac equation $D\\psi=A \\cdot_{Cl} \\psi$ on Riemannian\nmanifolds that are either closed or of bounded geometry. This generalizes a\nresult of Rupert Frank and Michael Loss on $\\mathbb{R}^n$ where the criterion\nrelates the $L^n$-norm of $A$ to the Sobolev constant on $\\mathbb{R}^n$. On\nRiemannian manifolds the role of the Sobolev constant will be replaced by the\nYamabe invariant. If $n$ is odd, we show that our criterion is sharp on\n$\\mathbb{S}^n$.",
        "In this paper, we consider the problem of constructing optimal average-length\nbinary codes under the constraint that each codeword must contain at most $D$\nones, where $D$ is a given input parameter. We provide an $O(n^2D)$-time\ncomplexity algorithm for the construction of such codes, where $n$ is the\nnumber of codewords. We also describe several scenarios where the need to\ndesign these kinds of codes naturally arises. Our algorithms allow us to\nconstruct both optimal average-length prefix binary codes and optimal\naverage-length alphabetic binary codes. In the former case, our $O(n^2D)$-time\nalgorithm substantially improves on the previously known $O(n^{2+D})$-time\ncomplexity algorithm for the same problem. We also provide a Kraft-like\ninequality for the existence of (optimal) variable-length binary codes, subject\nto the above-described constraint on the number of 1's in each codeword.",
        "We present an improved high-index saddle dynamics (iHiSD) for finding saddle\npoints and constructing solution landscapes, which is a crossover dynamics from\ngradient flow to traditional HiSD such that the Morse theory for gradient flow\ncould be involved. We propose analysis for the reflection manifold in iHiSD,\nand then prove its stable and nonlocal convergence from outside of the region\nof attraction to the saddle point, which resolves the dependence of the\nconvergence of HiSD on the initial value. We then present and analyze a\ndiscretized iHiSD that inherits these convergence properties. Furthermore,\nbased on the Morse theory, we prove that any two saddle points could be\nconnected by a sequence of trajectories of iHiSD. Theoretically, this implies\nthat a solution landscape with a finite number of stationary points could be\ncompletely constructed by means of iHiSD, which partly answers the completeness\nissue of the solution landscape for the first time and indicates the necessity\nof integrating the gradient flow in HiSD. Different methods are compared by\nnumerical experiments to substantiate the effectiveness of the iHiSD method.",
        "We introduce an adaptive finite element scheme for the efficient\napproximation of a (large) collection of eigenpairs of selfadjoint elliptic\noperators in which the adaptive refinement is driven by the solution of a\nsingle source problem -- the so-called landscape problem for the operator --\ninstead of refining based on the computed eigenpairs. Some theoretical\njustification for the approach is provided, and extensive empirical results\nindicate that it can provide an attractive alternative to standard adaptive\nschemes, particularly in the hp-adaptive environment.",
        "Multi-Criteria Decision Making~(MCDM) is widely applied in various fields,\nusing quantitative and qualitative analyses of multiple levels and attributes\nto support decision makers in making scientific and rational decisions in\ncomplex scenarios. However, traditional MCDM methods face bottlenecks in\nhigh-dimensional problems. Given the fact that Large Language Models~(LLMs)\nachieve impressive performance in various complex tasks, but limited work\nevaluates LLMs in specific MCDM problems with the help of human domain experts,\nwe further explore the capability of LLMs by proposing an LLM-based evaluation\nframework to automatically deal with general complex MCDM problems. Within the\nframework, we assess the performance of various typical open-source models, as\nwell as commercial models such as Claude and ChatGPT, on 3 important\napplications, these models can only achieve around 60\\% accuracy rate compared\nto the evaluation ground truth. Upon incorporation of Chain-of-Thought or\nfew-shot prompting, the accuracy rates rise to around 70\\%, and highly depend\non the model. In order to further improve the performance, a LoRA-based\nfine-tuning technique is employed. The experimental results show that the\naccuracy rates for different applications improve significantly to around 95\\%,\nand the performance difference is trivial between different models, indicating\nthat LoRA-based fine-tuned LLMs exhibit significant and stable advantages in\naddressing MCDM tasks and can provide human-expert-level solutions to a wide\nrange of MCDM challenges.",
        "Lead-lag relationships, integral to market dynamics, offer valuable insights\ninto the trading behavior of high-frequency traders (HFTs) and the flow of\ninformation at a granular level. This paper investigates the lead-lag\nrelationships between stock index futures contracts of different maturities in\nthe Chinese financial futures market (CFFEX). Using high-frequency\n(tick-by-tick) data, we analyze how price movements in near-month futures\ncontracts influence those in longer-dated contracts, such as next-month,\nquarterly, and semi-annual contracts. Our findings reveal a consistent pattern\nof price discovery, with the near-month contract leading the others by one\ntick, driven primarily by liquidity. Additionally, we identify a negative\nfeedback effect of the \"lead-lag spread\" on the leading asset, which can\npredict returns of leading asset. Backtesting results demonstrate the\nprofitability of trading based on the lead-lag spread signal, even after\naccounting for transaction costs. Altogether, our analysis offers valuable\ninsights to understand and capitalize on the evolving dynamics of futures\nmarkets.",
        "In the problem of fully allocating a social endowment of perfectly divisible\ncommodities among a group of agents with multidimensional single-peaked\npreferences, we study strategy-proof rules that are not Pareto-dominated by\nother strategy-proof rules. Specifically, we: (i) establish a sufficient\ncondition for a rule to be Pareto-undominated strategy-proof; (ii) introduce a\nbroad class of rules satisfying this property by extending the family of\n\"sequential allotment rules\" to the multidimensional setting; and (iii) provide\na new characterization of the \"multidimensional uniform rule\" involving\nPareto-undominated strategy-proofness. Results (i) and (iii) generalize\nprevious findings that were only applicable to the two-agent case.",
        "This work introduces NeuroQuant, a novel post-training quantization (PTQ)\napproach tailored to non-generalized Implicit Neural Representations for\nvariable-rate Video Coding (INR-VC). Unlike existing methods that require\nextensive weight retraining for each target bitrate, we hypothesize that\nvariable-rate coding can be achieved by adjusting quantization parameters (QPs)\nof pre-trained weights. Our study reveals that traditional quantization\nmethods, which assume inter-layer independence, are ineffective for\nnon-generalized INR-VC models due to significant dependencies across layers. To\naddress this, we redefine variable-rate INR-VC as a mixed-precision\nquantization problem and establish a theoretical framework for sensitivity\ncriteria aimed at simplified, fine-grained rate control. Additionally, we\npropose network-wise calibration and channel-wise quantization strategies to\nminimize quantization-induced errors, arriving at a unified formula for\nrepresentation-oriented PTQ calibration. Our experimental evaluations\ndemonstrate that NeuroQuant significantly outperforms existing techniques in\nvarying bitwidth quantization and compression efficiency, accelerating encoding\nby up to eight times and enabling quantization down to INT2 with minimal\nreconstruction loss. This work introduces variable-rate INR-VC for the first\ntime and lays a theoretical foundation for future research in rate-distortion\noptimization, advancing the field of video coding technology. The materials\nwill be available at https:\/\/github.com\/Eric-qi\/NeuroQuant.",
        "A common problem to signal processing are biases introduced by correlated\nnoise. In Time-Delay Estimation (TDE), which quantifies a time lag between two\nsignals, noise mixing introduces a bias towards zero delay in conventional TDE\nprotocols based on the cross- or bispectrum. Here we propose two novel TDE\napproaches that address these shortcomings: (1) A cross-spectrum based TDE\nprotocol that relies on estimating the periodicity of the phase spectrum rather\nthan its slope, and (2) a bispectrum based TDE analysis, bispectral\nantisymmetrization, which removes contributions from not just Gaussian but all\nindependent sources. In a simulation study, we compare conventional and novel\nTDE protocols and resolve differences in performance with respect to noise\nGaussianity and auto-correlation structure. As a proof-of-concept, we also\nperform TDE analysis on a neural stimulation dataset (n=3). We find that\nantisymmetrization consistently outperforms conventional bispectral TDE methods\nat low signal-to-noise ratios (SNR) and removes spurious zero-delay estimates\nin all mixed-noise environments. TDE based on phase periodicity also improves\nsignal sensitivity compared to conventional cross-spectral methods. These\nobservations are stable with respect to the magnitude of the delay and the\nstatistical properties of the noise.",
        "In the $(a,b)$-biased Maker-Breaker domination game, two players alternately\nselect unplayed vertices in a graph $G$ such that Dominator selects $a$ and\nStaller selects $b$ vertices per move. Dominator wins if the vertices he\nselected during the game form a dominating set of $G$, while Staller wins if\nshe can prevent Dominator from achieving this goal. Given a positive integer\n$b$, Dominator's threshold, $\\textrm{a}_b$, is the minimum $a$ such that\nDominator wins the $(a,b)$-biased game on $G$ when he starts the game.\nSimilarly, $\\textrm{a}'_b$ denotes the minimum $a$ such that Dominator wins\nwhen Staller starts the $(a,b)$-biased game. Staller's thresholds,\n$\\textrm{b}_a$ and $\\textrm{b}'_a$, are defined analogously. It is proved that\nStaller wins the $(k-1,k)$-biased games in a graph $G$ if its order is\nsufficiently large with respect to a function of $k$ and the maximum degree of\n$G$. Along the way, the $\\ell$-local domination number of a graph is\nintroduced. This new parameter is proved to bound Dominator's thresholds\n$\\textrm{a}_\\ell$ and $\\textrm{a}_\\ell'$ from above. As a consequence,\n$\\textrm{a}_1'(G)\\le 2$ holds for every claw-free graph $G$. More specific\nresults are obtained for thresholds in line graphs and Cartesian grids. Based\non the concept of $[1,k]$-factor of a graph $G$, we introduce the star\npartition width $\\sigma(G)$ of $G$, and prove that $\\textrm{a}_1'(G)\\le\n\\sigma(G)$ holds for any nontrivial graph $G$, while\n$\\textrm{a}_1'(G)=\\sigma(G)$ if $G$ is a tree.",
        "The prevalence of harmful content on social media platforms poses significant\nrisks to users and society, necessitating more effective and scalable content\nmoderation strategies. Current approaches rely on human moderators, supervised\nclassifiers, and large volumes of training data, and often struggle with\nscalability, subjectivity, and the dynamic nature of harmful content (e.g.,\nviolent content, dangerous challenge trends, etc.). To bridge these gaps, we\nutilize Large Language Models (LLMs) to undertake few-shot dynamic content\nmoderation via in-context learning. Through extensive experiments on multiple\nLLMs, we demonstrate that our few-shot approaches can outperform existing\nproprietary baselines (Perspective and OpenAI Moderation) as well as prior\nstate-of-the-art few-shot learning methods, in identifying harm. We also\nincorporate visual information (video thumbnails) and assess if different\nmultimodal techniques improve model performance. Our results underscore the\nsignificant benefits of employing LLM based methods for scalable and dynamic\nharmful content moderation online.",
        "Advancements in audio foundation models (FMs) have fueled interest in\nend-to-end (E2E) spoken dialogue systems, but different web interfaces for each\nsystem makes it challenging to compare and contrast them effectively. Motivated\nby this, we introduce an open-source, user-friendly toolkit designed to build\nunified web interfaces for various cascaded and E2E spoken dialogue systems.\nOur demo further provides users with the option to get on-the-fly automated\nevaluation metrics such as (1) latency, (2) ability to understand user input,\n(3) coherence, diversity, and relevance of system response, and (4)\nintelligibility and audio quality of system output. Using the evaluation\nmetrics, we compare various cascaded and E2E spoken dialogue systems with a\nhuman-human conversation dataset as a proxy. Our analysis demonstrates that the\ntoolkit allows researchers to effortlessly compare and contrast different\ntechnologies, providing valuable insights such as current E2E systems having\npoorer audio quality and less diverse responses. An example demo produced using\nour toolkit is publicly available here:\nhttps:\/\/huggingface.co\/spaces\/Siddhant\/Voice_Assistant_Demo.",
        "In a previous paper, the authors showed that two kinds of $p$-adic\nSiegel--Eisenstein series of degree $n$ coincide with classical modular forms\nof weight $k$ for $\\Gamma _0(p)$, under the assumption that $p$ is a regular\nprime. The purpose of this paper is to show that this condition on $p$ can be\nremoved if the degree $n$ is low compared with $k$, namely, $n\\le 2k+1$.",
        "Social foraging is a widespread form of animal foraging in which groups of\nindividuals coordinate their decisions to exploit resources in the environment.\nAnimals show a variety of social structures from egalitarian to hierarchical.\nIn this study, we examine how different forms of social hierarchy shape\nforaging decisions. We developed a mechanistic analytically tractable model to\nstudy the underlying processes of social foraging, tying the microscopic\nindividual to the macroscopic group levels. Based on a stochastic evidence\naccumulation framework, we developed a model of patch-leaving decisions in a\nlarge hierarchical group with leading and following individuals. Across a\nvariety of information sharing mechanisms, we were able to analytically\nquantify emergent collective dynamics. We found that follower-leader dynamics\nthrough observations of leader movements or through counting the number of\nindividuals in a patch confers, for most conditions, a benefit for the\nfollowing individuals by increasing their accuracy in inferring patch richness.\nOn the other hand, misinformation, through the communication of false beliefs\nabout food rewards or patch quality, shows to be detrimental to following\nindividuals, but paradoxically may lead to increased group cohesion. In an era\nwhere there is a huge amount of animal foraging data collected, our model\nprovides a systematic way to conceptualize and understand those data by\nuncovering hidden mechanisms underlying social foraging decisions."
      ]
    }
  },
  {
    "id":2411.07453,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
    "start_abstract":"Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth\/width\/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 \/ 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Review of Research on Condition Assessment of Nuclear Power Plant Equipment Based on Data-Driven"
      ],
      "abstract":[
        "The condition assessment of the entire life cycle of nuclear power equipment has a significant impact on improving the safety and economy of nuclear power plants. In the past, operation and maintenance of systems, equipment, and structures of domestic nuclear power plants, mostly relied on the alarm mechanism of equipments, the simple threshold judgments of parameters, or the empirical judgments of engineers. With the implementation of online monitoring system in nuclear power plants, a large number of equipment operation data have been accumulated, and the use of data-driven technology to assess the health of equipment has become the focus of attention in the industry. In this paper, the current situation of the online monitoring system of nuclear power equipment was introduced and the common malfunction of nuclear power equipment was analyzed. The condition assessment of nuclear power equipment were categorized into three major problems (i.e., anomaly detection, life prediction, and fault diagnosis), the situation of research and application were summarized respectively, and the application potential of deep learning technology in this field was emphasized. Based on this, the challenges and possible solutions to the condition assessment of nuclear power plant equipment were further analyzed."
      ],
      "categories":[
        "nucl-th"
      ]
    },
    "list":{
      "title":[
        "Nano-topographical changes in latent fingerprint due to degradation over\n  time studied by Atomic force microscopy -- option to set a timeline?",
        "Nonlinear Temperature Sensitivity of Residential Electricity Demand:\n  Evidence from a Distributional Regression Approach",
        "Conformal Prediction and Human Decision Making",
        "On monotone alternating inverse monoids",
        "Oblique rotational axis detection using elliptical optical vortex based\n  on rotational Doppler effect",
        "Sublinear Variational Optimization of Gaussian Mixture Models with\n  Millions to Billions of Parameters",
        "On the asymptotic validity of confidence sets for linear functionals of\n  solutions to integral equations",
        "Revisiting Frank-Wolfe for Structured Nonconvex Optimization",
        "Cayley unitary elements in group algebras under oriented involutions",
        "Tractable General Equilibrium",
        "Mesoscopic Collective Dynamics in Liquids and the Dual Model",
        "Comprehensive landscape and simple rules for transition-metal Heusler\n  semiconductors",
        "Efficient Truncations of SU($N_c$) Lattice Gauge Theory for Quantum\n  Simulation",
        "Attractors for Singular-Degenerate Porous Medium Type Equations Arising\n  in Models for Biofilm Growth",
        "Topology-preserving discretization for the magneto-frictional equations\n  arising in the Parker conjecture",
        "Deformation theory and Koszul duality for Rota-Baxter systems",
        "Higher-order chiral scalar from boundary reduction of 3d higher-spin\n  gravity",
        "Topological phase diagram and quantum magnetotransport effects in\n  (Pb,Sn)Se quantum wells with magnetic barriers (Pb,Eu)Se",
        "Non-Canonical Crosslinks Confound Evolutionary Protein Structure Models",
        "SymEFT for local tastes of staggered lattice QCD",
        "The Pell sequence and cyclotomic matrices involving squares over finite\n  fields",
        "Graduated orders over completed group rings and conductor formul\\ae",
        "Sterile-neutrino search based on 259 days of KATRIN data",
        "The canonical differential equations of the one-loop-like integrals",
        "Defining the mean turbulent boundary layer thickness based on streamwise\n  velocity skewness",
        "On limiting distributions of Graham, Knuth, Patashnik recurrences",
        "A Foundational Theory for Decentralized Sensory Learning",
        "Models of liquid samples confinement for nanoscale NMR",
        "Binary Bosonic Mixtures with Pair Hopping in Synthetic Dimension: Phase\n  Transitions and Demixing Effects"
      ],
      "abstract":[
        "Latent fingerprints, if present, are crucial in identifying the suspect who\nwas at the crime scene. If there are many latent fingerprints or the suspect is\nfrom the same household, crime investigators may have difficulty identifying\nwhose latent fingerprints are time-related to the crime. Here, we report\nchanges in the nanoscale topography of latent fingerprints, which may serve as\na timeline and could help estimate when the latent fingerprint was imprinted.\nOn the latent fingerprint of an adolescent, we observed a change in\nnano-topography over time, specifically the formation of nano-chain structures\nin space between the imprinted papillary ridges. We consequently compared this\nobservation with the decomposition of the latent fingerprints of a child and\nadult. We observed a significant difference in the time change in\nnano-topography of latent fingerprints of a child, adolescent, and young adult.\nThe nano-topographical changes of latent fingerprints were studied by atomic\nforce microscopy over 70 days. In the case of child's and adolescent's latent\nfingerprints, the first nano-chains were observed already 24 hours after\nimprinting of the latent fingerprint, and the number of nano-chains increased\nsteadily up to 21 days, then we observed that another organic material covered\nthe nano-chains, and they started slowly deteriorating; nevertheless, the\nnano-chains were still present on the 70th day.",
        "We estimate the temperature sensitivity of residential electricity demand\nduring extreme temperature events using the distribution-to-scalar regression\nmodel. Rather than relying on simple averages or individual quantile statistics\nof raw temperature data, we construct distributional summaries, such as\nprobability density, hazard rate, and quantile functions, to retain a more\ncomprehensive representation of temperature variation. This approach not only\nutilizes richer information from the underlying temperature distribution but\nalso enables the examination of extreme temperature effects that conventional\nmodels fail to capture. Additionally, recognizing that distribution functions\nare typically estimated from limited discrete observations and may be subject\nto measurement errors, our econometric framework explicitly addresses this\nissue. Empirical findings from the hazard-to-demand model indicate that\nresidential electricity demand exhibits a stronger nonlinear response to cold\nwaves than to heat waves, while heat wave shocks demonstrate a more pronounced\nincremental effect. Moreover, the temperature quantile-to-demand model produces\nlargely insignificant demand response estimates, attributed to the offsetting\ninfluence of two counteracting forces.",
        "Methods to quantify uncertainty in predictions from arbitrary models are in\ndemand in high-stakes domains like medicine and finance. Conformal prediction\nhas emerged as a popular method for producing a set of predictions with\nspecified average coverage, in place of a single prediction and confidence\nvalue. However, the value of conformal prediction sets to assist human\ndecisions remains elusive due to the murky relationship between coverage\nguarantees and decision makers' goals and strategies. How should we think about\nconformal prediction sets as a form of decision support? We outline a decision\ntheoretic framework for evaluating predictive uncertainty as informative\nsignals, then contrast what can be said within this framework about idealized\nuse of calibrated probabilities versus conformal prediction sets. Informed by\nprior empirical results and theories of human decisions under uncertainty, we\nformalize a set of possible strategies by which a decision maker might use a\nprediction set. We identify ways in which conformal prediction sets and posthoc\npredictive uncertainty quantification more broadly are in tension with common\ngoals and needs in human-AI decision making. We give recommendations for future\nresearch in predictive uncertainty quantification to support human decision\nmakers.",
        "In this paper, we consider the inverse submonoids $AM_n$ of monotone\ntransformations and $AO_n$ of order-preserving transformations of the\nalternating inverse monoid $AI_n$ on a chain with $n$ elements. We compute the\ncardinalities, describe the Green's structures and the congruences, and\ncalculate the ranks of these two submonoids of $AI_n$.",
        "The rotational Doppler effect (RDE) of structured light carrying orbital\nangular momentum (OAM) has attracted widespread attention for applications in\noptical sensors and OAM spectrum detection. These studies, however, based on\nRDE, are mostly focused on the motion parameters of rotating objects; other\nequally important attitude characteristics, e.g., the tilt angle of the axis of\nrotation, have rarely been considered. We observed an interesting phenomenon in\nthe experiments: the rotational Doppler spectral distribution varies with the\nellipticity of the elliptical optical vortex (EOV) and the tilt angle between\nthe rotational axis and optical axis, which inspired us to wonder if it is\npossible to detect oblique rotational axis or compensate the rotational Doppler\nbroadening effect induced by oblique incidence by utilizing the EOV. Here, we\nreveal the RDE quantitative relationship with tilt angle and ellipticity for\nthe first time and report a novel approach for tilt angle measurement. By\nemploying a series of EOV with periodically varying ellipticity to illuminate a\nrotating object and analyze the time-frequency spectral distribution of\nscattered light associated with ellipticity and tilt angle, the tilt angle can\nbe acquired accurately based on the specific relationship between the tilt\nangle and ellipticity of the EOV. Furthermore, the spectrum broadening effect\narising from oblique incidence in the actual scenario may be addressed through\nour scheme. The method may find applications in industrial manufacturing and\ntarget attitude measurement, and our results provide new insights for obtaining\nmore information about objects.",
        "Gaussian Mixture Models (GMMs) range among the most frequently used machine\nlearning models. However, training large, general GMMs becomes computationally\nprohibitive for datasets with many data points $N$ of high-dimensionality $D$.\nFor GMMs with arbitrary covariances, we here derive a highly efficient\nvariational approximation, which is integrated with mixtures of factor\nanalyzers (MFAs). For GMMs with $C$ components, our proposed algorithm\nsignificantly reduces runtime complexity per iteration from\n$\\mathcal{O}(NCD^2)$ to a complexity scaling linearly with $D$ and remaining\nconstant w.r.t. $C$. Numerical validation of this theoretical complexity\nreduction then shows the following: the distance evaluations required for the\nentire GMM optimization process scale sublinearly with $NC$. On large-scale\nbenchmarks, this sublinearity results in speed-ups of an order-of-magnitude\ncompared to the state-of-the-art. As a proof of concept, we train GMMs with\nover 10 billion parameters on about 100 million images, and observe training\ntimes of approximately nine hours on a single state-of-the-art CPU.",
        "This paper examines the construction of confidence sets for parameters\ndefined as a linear functional of the solution to an integral equation\ninvolving conditional expectations. We show that any confidence set uniformly\nvalid over a broad class of probability laws, allowing the integral equation to\nbe arbitrarily ill-posed must have, with high probability under some laws, a\ndiameter at least as large as the diameter of the parameter's range over the\nmodel. Additionally, we establish that uniformly consistent estimators of the\nparameter do not exist. We show that, consistent with the weak instruments\nliterature, Wald confidence intervals are not uniformly valid. Furthermore, we\nargue that inverting the score test, a successful approach in that literature,\ndoes not extend to the broader class of parameters considered here. We present\na method for constructing uniformly valid confidence sets in the special case\nwhere all variables are binary and discuss its limitations. Finally, we\nemphasize that developing uniformly valid confidence sets for the general class\nof parameters considered in this paper remains an open problem.",
        "We introduce a new projection-free (Frank-Wolfe) method for optimizing\nstructured nonconvex functions that are expressed as a difference of two convex\nfunctions. This problem class subsumes smooth nonconvex minimization,\npositioning our method as a promising alternative to the classical Frank-Wolfe\nalgorithm. DC decompositions are not unique; by carefully selecting a\ndecomposition, we can better exploit the problem structure, improve\ncomputational efficiency, and adapt to the underlying problem geometry to find\nbetter local solutions. We prove that the proposed method achieves a\nfirst-order stationary point in $O(1\/\\epsilon^2)$ iterations, matching the\ncomplexity of the standard Frank-Wolfe algorithm for smooth nonconvex\nminimization in general. Specific decompositions can, for instance, yield a\ngradient-efficient variant that requires only $O(1\/\\epsilon)$ calls to the\ngradient oracle. Finally, we present numerical experiments demonstrating the\neffectiveness of the proposed method compared to the standard Frank-Wolfe\nalgorithm.",
        "Let $\\mathbf{F}$ be a real extension of $\\mathbb{Q}$, $G$ a finite group and\n$\\mathbf{F}G$ its group algebra. Given both a group homomorphism\n$\\sigma:G\\rightarrow \\{\\pm1\\}$ (called an orientation) and a group involution\n$^\\ast:G \\rightarrow G$ such that $gg^\\ast\\in N=ker(\\sigma)$, an oriented group\ninvolution $\\circledast$ of $\\mathbf{F}G$ is defined by $\\alpha=\\sum_{g\\in\nG}\\alpha_{g}g \\mapsto \\alpha^\\circledast=\\sum_{g\\in\nG}\\alpha_{g}\\sigma(g)g^{\\ast}$. In this paper, in case the involution on $G$ is\nthe classical one, $x\\mapsto x^{-1}$, $\\beta=x+x^{-1}$ is a skew-symmetric\nelement in $\\mathbf{F}G$ such that $1+\\beta$ is invertible, for $x\\in G$ with\n$\\sigma(x)=-1$, we consider Cayley unitary elements built out of $\\beta$. We\nprove that the coefficients of $(1+\\beta)^{-1}$ involve an interesting sequence\nwhich is a Fibonacci-like sequence.",
        "We study Walrasian economies (or general equilibrium models) and their\nsolution concept, the Walrasian equilibrium. A key challenge in this domain is\nidentifying price-adjustment processes that converge to equilibrium. One such\nprocess, t\\^atonnement, is an auction-like algorithm first proposed in 1874 by\nL\\'eon Walras. While continuous-time variants of t\\^atonnement are known to\nconverge to equilibrium in economies satisfying the Weak Axiom of Revealed\nPreferences (WARP), the process fails to converge in a pathological Walrasian\neconomy known as the Scarf economy. To address these issues, we analyze\nWalrasian economies using variational inequalities (VIs), an optimization\nframework. We introduce the class of mirror extragradient algorithms, which,\nunder suitable Lipschitz-continuity-like assumptions, converge to a solution of\nany VI satisfying the Minty condition in polynomial time. We show that the set\nof Walrasian equilibria of any balanced economy-which includes among others\nArrow-Debreu economies-corresponds to the solution set of an associated VI that\nsatisfies the Minty condition but is generally discontinuous. Applying the\nmirror extragradient algorithm to this VI we obtain a class of\nt\\^atonnement-like processes, which we call the mirror extrat\\^atonnement\nprocess. While our VI formulation is generally discontinuous, it is\nLipschitz-continuous in variationally stable Walrasian economies with bounded\nelasticity-including those satisfying WARP and the Scarf economy-thus\nestablishing the polynomial-time convergence of mirror extrat\\^atonnement in\nthese economies. We validate our approach through experiments on large\nArrow-Debreu economies with Cobb-Douglas, Leontief, and CES consumers, as well\nas the Scarf economy, demonstrating fast convergence in all cases without\nfailure.",
        "A microscopic vision is presented of a Dual Model of Liquids from a solid\npicture. Among the novelties of this model is that it provides quantitative\nexpressions of various extensive thermophysical properties. The introduction of\nthe statistical number of excited degrees of freedom (DoF) allows bypassing the\nproblem of other dual models which are sometimes unable to correctly reproduce\nthe expressions for those thermophysical quantities showing deviations due to\nthe activation or deactivation of internal DoF. The interpretation of the\nrelaxation times is given, their Order of Magnitude calculated and the way in\nwhich these times are involved in the different phases of the collective\ndynamics of liquids is discussed. A comparison is provided with results\nobtained in the frame of another phononic model of liquids, as well as with the\npredictions for the viscoelastic transition regions and with systems exhibiting\nkgap. In the last part of the paper, theoretical insights and experiments are\nsuggested as potential directions for future research and development.",
        "Heusler alloys, renowned for their multifunctionality and capacity for vast\nelemental customization, are primarily classified into half-Heusler (XYZ) and\nfull-Heusler (X2YZ) structural types. Typically, the 18-electron half-Heusler\nand the 24-electron full-Heusler alloys are recognized as semiconductors,\nfollowing the Slater-Pauling rule. Semiconductors are desired for many\napplications, but they represent a minor portion compared to the predominantly\nmetallic and half-metallic members of the Heusler family. To broaden the scope\nof Heusler semiconductors, advancements have been made in developing variants\nsuch as double-half Heuslers XX'Y2Z2 and quaternary full Heuslers XX'YZ, which\nincorporate four constituent elements. Recently, vacancy-filling\noff-stoichiometric Heuslers of ternary X1+bYZ (0 <= b <= 1) and quaternary\nXaX'bYZ (1 <= a + b <= 2) have emerged as a more versatile strategy. However,\nthe flexibility associated with off-stoichiometry inevitably leads to\ncomplications, including issues with fractional filling ratios and complex site\noccupations. This work presents a comprehensive landscape of\ntransition-metal-containing Heusler semiconductors, focusing on the\noff-stoichiometric Heuslers but seamlessly encompassing the\ninteger-stoichiometric systems. The structural and electronic properties can be\ntheoretically understood through a few simple rules. Many systems have been\nexperimentally validated, showcasing their potential for applications such as\nthermoelectric converters.",
        "Quantum simulations of lattice gauge theories offer the potential to directly\nstudy the non-perturbative dynamics of quantum chromodynamics, but naive\nanalyses suggest that they require large computational resources. Large $N_c$\nexpansions are performed to order 1\/$N_c$ to simplify the Hamiltonian of pure\nSU($N_c$) lattice gauge theories. A reformulation of the electric basis is\nintroduced with a truncation strategy based on the construction of local Krylov\nsubspaces with plaquette operators. Numerical simulations show that these\ntruncated Hamiltonians are consistent with traditional lattice calculations at\nrelatively small couplings. It is shown that the computational resources\nrequired for quantum simulation of time evolution generated by these\nHamiltonians is 17-19 orders of magnitude smaller than previous approaches.",
        "We investigate the long-time behaviour of solutions of a class of\nsingular-degenerate porous medium type equations in bounded Lipschitz domains\nwith mixed Dirichlet-Neumann boundary conditions. The existence of global\nattractors is shown under very general assumptions. Assuming, in addition, that\nsolutions are globally H\\\"older continuous and the reaction terms satisfy a\nsuitable sign condition in the vicinity of the degeneracy, we also prove the\nexistence of an exponential attractor, which, in turn, yields the finite\nfractal dimension of the global attractor. Moreover, we extend the results for\nscalar equations to systems where the degenerate equation is coupled to a\nsemilinear reaction-diffusion equation. The study of such systems is motivated\nby models for biofilm growth.",
        "The Parker conjecture, which explores whether magnetic fields in perfectly\nconducting plasmas can develop tangential discontinuities during magnetic\nrelaxation, remains an open question in astrophysics. Helicity conservation\nprovides a topological barrier during relaxation, preventing topologically\nnontrivial initial data relaxing to trivial solutions; preserving this\nmechanism discretely over long time periods is therefore crucial for numerical\nsimulation. This work presents an energy- and helicity-preserving finite\nelement discretization for the magneto-frictional system, for investigating the\nParker conjecture. The algorithm preserves a discrete version of the\ntopological barrier and a discrete Arnold inequality. We also discuss\nextensions to domains with nontrivial topology.",
        "This paper investigates Rota-Baxter systems in the sense of Brzezi\\'nski from\nthe perspective of operad theory. The minimal model of the Rota-Baxter system\noperad is constructed, equivalently a concrete construction of its Koszul dual\nhomotopy cooperad is given. The concept of homotopy Rota-Baxter systems and the\n$L_\\infty$-algebra that governs deformations of a Rota-Baxter system are\nderived from the Koszul dual homotopy cooperad. The notion of\ninfinity-Yang-Baxter pairs is introduced, which is a higher-order\ngeneralization of the traditional Yang-Baxter pairs. It is shown that a\nhomotopy Rota-Baxter system structure on the endomorphism algebra of a graded\nspace is equivalent to an associative infinity-Yang-Baxter pair on this graded\nalgebra, thereby generalizing the classical correspondence between Yang-Baxter\npairs and Rota-Baxter systems.",
        "We use a recently proposed covariant procedure to reduce the Chern-Simons\naction of three-dimensional higher-spin gravity to the boundary, resulting in a\nLorentz covariant action for higher-order chiral scalars. After gauge-fixing,\nwe obtain a higher-derivative action generalizing the $s=1$ Floreanini-Jackiw\nand $s=2$ Alekseev-Shatashvili actions to arbitrary spin $s$. For simplicity,\nwe treat the case of general spin at the linearized level, while the full\nnon-linear asymptotic boundary conditions are presented in component form for\nthe $SL(3,\\mathbb R)$ case. Finally, we extend the spin-3 linearized analysis\nto a background with non-trivial higher-spin charge and show that it has a\nricher structure of zero modes.",
        "Despite several theoretical predictions regarding the physics and application\nof quantum wells (QWs) of topological crystalline insulators (TCI), no\nquantized charge transport via helical or chiral edge states has been\nexperimentally demonstrated for such a class of systems. In this study, we\nreport here on a successful growth by molecular beam epitaxy of high\ncrystalline quality Pb$_{1-x}$Sn$_{x}$Se:Bi\/Pb$_{1-y}$Eu$_{y}$Se QWs with $x =\n0.25$ and $y = 0.1$, and on their magnetotransport characterization as a\nfunction of the QW thickness between 10 and 50 nm, temperatures down to 300 mK,\nperpendicular and tilted magnetic fields up to 36 T. The character of weak\nantilocalization magnetoresistance and universal conductance fluctuations\npoints to a notably long phase coherence length. It is argued that a relatively\nlarge magnitude of the dielectric constant of IV-VI compounds suppresses the\ndecoherence by electron-electron scattering. The observation of\nShubnikov-de-Haas oscillations and the quantum Hall effect, together with\nmultiband $k\\cdot p$ modelling, have enabled us to assess valley degeneracies,\nthe magnitude of strain, subbands effective masses, the Berry phases, and the\ntopological phase diagram as a function of the QW thickness. Our results\ndemonstrate that further progress in controlling Sn content, carrier densities,\nand magnetism in Pb$_{1-x}$Sn$_{x}$Se\/Pb$_{1-y}$Eu$_{y}$Se QWs will allow for\nthe exploration of the topologically protected quantized edge transport even in\nthe absence of an external magnetic field. Furthermore, a reduced strength of\nelectron-electron interactions will result in the absence of unpaired localized\nspins in the topological gap and, thus, in a substantially longer topological\nprotection length compared to the quantum spin Hall materials explored so far.",
        "Evolution-based protein structure prediction models have achieved\nbreakthrough success in recent years. However, they struggle to generalize\nbeyond evolutionary priors and on sequences lacking rich homologous data. Here\nwe present a novel, out-of-domain benchmark based on sactipeptides, a rare\nclass of ribosomally synthesized and post-translationally modified peptides\n(RiPPs) characterized by sulfur-to-$\\alpha$-carbon thioether bridges creating\ncross-links between cysteine residues and backbone. We evaluate recent models\non predicting conformations compatible with these cross-links bridges for the\n10 known sactipeptides with elucidated post-translational modifications.\nCrucially, the structures of 5 of them have not yet been experimentally\nresolved. This makes the task a challenging problem for evolution-based models,\nwhich we find exhibit limited performance (0.0% to 19.2% GDT-TS on\nsulfur-to-$\\alpha$-carbon distance). Our results point at the need for\nphysics-informed models to sustain progress in biomolecular structure\nprediction.",
        "The applicability of Symanzik Effective Field Theory (SymEFT) for the\ndescription of lattice artifacts assumes a local formulation of the lattice\ntheory. We discuss the symmetries realised by tastes local in spacetime of\nunrooted staggered quarks, approaching mass-degenerate 4-flavour QCD in the\ncontinuum limit. An outlook on some implications for the asymptotic\nlattice-spacing dependence is given for spectral quantities as well as local\ncomposite fields.",
        "In this paper, by some arithmetic properties of the Pell sequence and some\n$p$-adic tools, we study certain cyclotomic matrices involving squares over\nfinite fields. For example, let $1=s_1,s_2,\\cdots,s_{(q-1)\/2}$ be all the\nnonzero squares over $\\mathbb{F}_{q}$, where $q=p^f$ is an odd prime power with\n$q\\ge7$. We prove that the matrix\n  $$B_q((q-3)\/2)=\\left[\\left(s_i+s_j\\right)^{(q-3)\/2}\\right]_{2\\le i,j\\le\n(q-1)\/2}$$\n  is a singular matrix whenever $f\\ge2$. Also, for the case $q=p$, we show that\n  $$\\det B_p((p-3)\/2)=0\\Leftrightarrow Q_p\\equiv 2\\pmod{p^2\\mathbb{Z}},$$\n  where $Q_p$ is the $p$-th term of the companion Pell sequence\n$\\{Q_i\\}_{i=0}^{\\infty}$ defined by $Q_0=Q_1=2$ and $Q_{i+1}=2Q_i+Q_{i-1}$.",
        "We study graduated orders over completed group rings of $1$-dimensional\nadmissible $p$-adic Lie groups, and verify the equivariant $p$-adic Artin\nconjecture for such orders. Following Jacobinski and Plesken, we obtain a\nformula for the conductor of a graduated order into a self-dual order. We also\nrefine Nickel's central conductor formula by determining a hitherto implicit\nexponent $r_\\chi$.",
        "Neutrinos are the most abundant fundamental matter particles in the Universe\nand play a crucial role in particle physics and cosmology. Neutrino\noscillation, discovered about 25 years ago, reveals that the three known\nspecies mix with each other. Anomalous results from reactor and\nradioactive-source experiments suggest a possible fourth neutrino state, the\nsterile neutrino, which does not interact via the weak force. The KATRIN\nexperiment, primarily designed to measure the neutrino mass via tritium\n$\\beta$-decay, also searches for sterile neutrinos suggested by these\nanomalies. A sterile-neutrino signal would appear as a distortion in the\n$\\beta$-decay energy spectrum, characterized by a discontinuity in curvature\n(kink) related to the sterile-neutrino mass. This signature, which depends only\non the shape of the spectrum rather than its absolute normalization, offers a\nrobust, complementary approach to reactor experiments. KATRIN examined the\nenergy spectrum of 36 million tritium $\\beta$-decay electrons recorded in 259\nmeasurement days within the last 40 electronvolt below the endpoint. The\nresults exclude a substantial part of the parameter space suggested by the\ngallium anomaly and challenge the Neutrino-4 claim. Together with other\nneutrino-disappearance experiments, KATRIN probes sterile-to-active mass\nsplittings from a fraction of an electron-volt squared to several hundred\nelectron-volts squared, excluding light sterile neutrinos with mixing angles\nabove a few percent.",
        "Recently, a new approach for high loop integrals has been proposed in\n\\cite{Huang:2024nij}, where the whole parameter integration has been divided\ninto two parts: a one-loop-like integration and the remaining parameter\nintegration. In this paper, we systematically study the one-loop-like\nintegrals. We establish the IBP relations for the integral family and show how\nto complete the reduction. We find the canonical master integrals and write\ndown the corresponding canonical differential equations.",
        "A new statistical definition for the mean turbulent boundary layer thickness\nis introduced, based on the identification of the point where streamwise\nvelocity skewness changes sign in the outermost region of the boundary layer.\nThis definition is motivated by the phenomenology of streamwise velocity\nfluctuations near the turbulent\/non-turbulent interface, whose local\ncharacteristics are shown to be universal for turbulent boundary layers under\nlow freestream turbulence conditions (e.g., with or without pressure gradients,\nsurface roughness, etc.). This approach provides a turbulent boundary layer\nthickness that is consistent with previous definitions, such as those based on\nReynolds shear stress or `composite' mean velocity profiles, while being\nindependent of arbitrary thresholds and applicable to past single-point\nmeasurements. Two methods are proposed for estimating the turbulent boundary\nlayer thickness using this definition: one based on simple linear interpolation\nand the other on fitting a generalised Fourier model to the outer skewness\nprofile. The robustness and limitations of these methods are demonstrated\nthrough analysis of several published experimental and numerical datasets,\nwhich cover a range of canonical and non-canonical turbulent boundary layers.\nThese datasets vary in wall-normal resolution and measurement noise,\nparticularly in the critical turbulent\/non-turbulent interface region.",
        "Graham, Knuth and Patashnik in their book Concrete Mathematics called for\ndevelopment of a general theory of the solutions of recurrences defined by\n$$\\left|{ n\\atop k}\\right|=(\\alpha n+\\beta k+\\gamma)\\left|{n-1\\atop\nk}\\right|+(\\alpha' n+\\beta' k+\\gamma')\\left|{n-1\\atop k-1}\\right|+I_{n=k=0}$$\nfor $0\\le k\\le n$ and six parameters\n$\\alpha,\\beta,\\gamma,\\alpha'\\beta',\\gamma'$. Since then, a number of authors\ninvestigated various properties of the solutions of these recurrences. In this\nnote we consider a probabilistic aspect, namely we consider the limiting\ndistributions of sequences of integer valued random variables naturally\nassociated with the solutions of such recurrences. We will give a complete\ndescription of the limiting behavior when $\\alpha'=0$ and the remaining five\nparameters are non--negative.",
        "In both neuroscience and artificial intelligence, popular functional\nframeworks and neural network formulations operate by making use of extrinsic\nerror measurements and global learning algorithms. Through a set of conjectures\nbased on evolutionary insights on the origin of cellular adaptive mechanisms,\nwe reinterpret the core meaning of sensory signals to allow the brain to be\ninterpreted as a negative feedback control system, and show how this could lead\nto local learning algorithms without the need for global error correction\nmetrics. Thereby, a sufficiently good minima in sensory activity can be the\ncomplete reward signal of the network, as well as being both necessary and\nsufficient for biological learning to arise. We show that this method of\nlearning was likely already present in the earliest unicellular life forms on\nearth. We show evidence that the same principle holds and scales to\nmulticellular organisms where it in addition can lead to division of labour\nbetween cells. Available evidence shows that the evolution of the nervous\nsystem likely was an adaptation to more effectively communicate intercellular\nsignals to support such division of labour. We therefore propose that the same\nlearning principle that evolved already in the earliest unicellular life forms,\ni.e. negative feedback control of externally and internally generated sensor\nsignals, has simply been scaled up to become a fundament of the learning we see\nin biological brains today. We illustrate diverse biological settings, from the\nearliest unicellular organisms to humans, where this operational principle\nappears to be a plausible interpretation of the meaning of sensor signals in\nbiology, and how this relates to current neuroscientific theories and findings.",
        "Diffusion is a prominent source of noise affecting nuclear magnetic resonance\nat the nanometer scale (nano-NMR), preventing high resolution studies of\nunpolarized liquid samples. Actively managing diffusion noise through, for\nexample, sample confinement, is likely to unveil alternative noise sources\nwhich so far have been disregarded, as they occur in longer time-scales and\nare, consequently, masked by diffusion. These secondary noise sources could\ndiminish the advantages provided by sample confinement but, on the other hand,\nthey can provide with valuable information about the behavior of the sample and\nits interactions. In this article, we study for the first time and in detail\ntwo noise models for confined nano-NMR, namely, surface interactions and\nporosity, and discuss their implications for typical nano-NMR experiments.",
        "We employ the cluster Gutzwiller mean-field method to investigate the\nground-state phase diagrams and demixing effects in binary boson mixtures with\npair hopping in synthetic dimensions. Our study reveals two novel interspecies\npaired superfluid phases: the paired super-counter-fluid (PSCF) phase,\nfeaturing pairs of two particles of one species and two holes of the other, and\nthe SCF* phase, which combines PSCF and super-counter-fluid (SCF) orders. These\nphases provide new insights into XY ferromagnet states from a pseudo-spin\nperspective, with SCF* and PSCF states corresponding to different XY\nferromagnet phases depending on particle filling. We also identify a quantum\nquadruple critical point in the interexchange asymmetric case. Importantly, we\ndemonstrate that the mixed-demixed critical point is phase-dependent due to\npairing hopping, differing from normal two-component bosonic systems.\nExperimental schemes to observe these novel phases are proposed."
      ]
    }
  }
]