[
  {
    "id":2411.01668,
    "research_type":"basic",
    "start_id":"b4",
    "start_title":"Mean\u2010field games with differing beliefs for algorithmic trading",
    "start_abstract":"Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity.",
    "start_categories":[
      "q-fin.MF"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Linear-quadratic mean field games"
      ],
      "abstract":[
        "In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "The Eigenfunctions of the Transfer Operator for the Dyson model in a\n  field",
        "Minding Fuzzy Regions: A Data-driven Alternating Learning Paradigm for\n  Stable Lesion Segmentation",
        "Learning Privacy from Visual Entities",
        "A proposal for removing $\\pi N$-state contamination from the nucleon\n  induced pseudoscalar form factor in lattice QCD",
        "Modular Units on $X_{1}( p)$ and Quotients of the Cuspidal Group",
        "Entente: Cross-silo Intrusion Detection on Network Log Graphs with\n  Federated Learning",
        "Twin-Space Representation of Classical Mapping Model in the Constraint\n  Phase Space Representation: Numerically Exact Approach to Open Quantum\n  Systems",
        "Structure and Dynamics of Deep Eutectic Systems from Cluster-Optimized\n  Energy Functions",
        "SPRIG: Stackelberg Perception-Reinforcement Learning with Internal Game\n  Dynamics",
        "Towards Heisenberg limit without critical slowing down via quantum\n  reinforcement learning",
        "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
        "Unreflected Use of Tabular Data Repositories Can Undermine Research\n  Quality",
        "Quantum stochastic communication via high-dimensional entanglement",
        "The closure of linear foliations",
        "Dynamics near a class of nonhyperbolic fixed points",
        "Deformation theory and Koszul duality for Rota-Baxter systems",
        "Characterising planetary material accreted by cool helium atmosphere\n  white dwarfs using an exponentially decaying disc model",
        "Multisymplectic structure of nonintegrable Henon-Heiles system",
        "Sphere Precoding for Robust Near-Field Communications",
        "A Framework for Supporting the Reproducibility of Computational\n  Experiments in Multiple Scientific Domains",
        "Choroidal image analysis for OCT image sequences with applications in\n  systemic health",
        "MetaDE: Evolving Differential Evolution by Differential Evolution",
        "Semantic Neural Radiance Fields for Multi-Date Satellite Data",
        "Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go\n  Beyond",
        "Variance Reduction via Resampling and Experience Replay",
        "Explainable AI for Clinical Outcome Prediction: A Survey of Clinician\n  Perceptions and Preferences",
        "SGNetPose+: Stepwise Goal-Driven Networks with Pose Information for\n  Trajectory Prediction in Autonomous Driving",
        "ACCEPT: Diagnostic Forecasting of Battery Degradation Through\n  Contrastive Learning",
        "On Target Pattern Formation in the CHNS system"
      ],
      "abstract":[
        "The recent works \\cite{EFMV2024} and \\cite{JOP2023} have studied the spectral\nproperties of the Dyson model in the absence of an external field. This paper\nis a continuation of \\cite{EFMV2024} and aims to bridge the gap in the\nliterature by investigating the Dyson model in a field.\\\\ In this paper, we\nprove that, for high temperatures or strong magnetic fields, there exists a\nnon-negative, integrable (with respect to the unique half-line Gibbs measure)\neigenfunction of the transfer operator for the Dyson model if $\\alpha\\in(\\frac\n3 2,2]$. However, unlike in the zero-magnetic-field case, this eigenfunction is\nnot continuous.",
        "Deep learning has achieved significant advancements in medical image\nsegmentation, but existing models still face challenges in accurately\nsegmenting lesion regions. The main reason is that some lesion regions in\nmedical images have unclear boundaries, irregular shapes, and small tissue\ndensity differences, leading to label ambiguity. However, the existing model\ntreats all data equally without taking quality differences into account in the\ntraining process, resulting in noisy labels negatively impacting model training\nand unstable feature representations. In this paper, a data-driven alternating\nlearning (DALE) paradigm is proposed to optimize the model's training process,\nachieving stable and high-precision segmentation. The paradigm focuses on two\nkey points: (1) reducing the impact of noisy labels, and (2) calibrating\nunstable representations. To mitigate the negative impact of noisy labels, a\nloss consistency-based collaborative optimization method is proposed, and its\neffectiveness is theoretically demonstrated. Specifically, the label confidence\nparameters are introduced to dynamically adjust the influence of labels of\ndifferent confidence levels during model training, thus reducing the influence\nof noise labels. To calibrate the learning bias of unstable representations, a\ndistribution alignment method is proposed. This method restores the underlying\ndistribution of unstable representations, thereby enhancing the discriminative\ncapability of fuzzy region representations. Extensive experiments on various\nbenchmarks and model backbones demonstrate the superiority of the DALE\nparadigm, achieving an average performance improvement of up to 7.16%.",
        "Subjective interpretation and content diversity make predicting whether an\nimage is private or public a challenging task. Graph neural networks combined\nwith convolutional neural networks (CNNs), which consist of 14,000 to 500\nmillions parameters, generate features for visual entities (e.g., scene and\nobject types) and identify the entities that contribute to the decision. In\nthis paper, we show that using a simpler combination of transfer learning and a\nCNN to relate privacy with scene types optimises only 732 parameters while\nachieving comparable performance to that of graph-based methods. On the\ncontrary, end-to-end training of graph-based methods can mask the contribution\nof individual components to the classification performance. Furthermore, we\nshow that a high-dimensional feature vector, extracted with CNNs for each\nvisual entity, is unnecessary and complexifies the model. The graph component\nhas also negligible impact on performance, which is driven by fine-tuning the\nCNN to optimise image features for privacy nodes.",
        "In the PACS10 project, the PACS collaboration has generated three sets of the\nPACS10 gauge configurations at the physical point with lattice volume larger\nthan $(10\\;{\\rm fm})^4$ and three different lattice spacings. The isovector\nnucleon form factors had been already calculated by using two sets of the\nPACS10 gauge configurations. In our strategy, the smearing parameters of the\nnucleon interpolation operator were highly optimized to eliminate as much as\npossible the contribution of excited states in the nucleon two-point function.\nThis strategy was quite successful in calculations of the electric ($G_E$),\nmagnetic ($G_M$) and axial-vector ($F_A$) form factors, while the induced\npseudoscalar ($F_P$) and pseudoscalar ($G_P$) form factors remained strongly\naffected by residual contamination of $\\pi N$-state contribution. In this work,\nwe propose a simple method to remove the $\\pi N$-state contamination from the\n$F_P$ form factor, and then evaluate the induced pseudoscalar charge $g_P^\\ast$\nand the pion-nucleon coupling $g_{\\pi NN}$ from existing data in a new\nanalysis. Applying this method to the $G_P$ form factor is also considered with\na help of the axial Ward-Takahashi identity.",
        "Modular units are functions on modular curves whose divisors are supported on\nthe cusps. They form a free abelian group of rank at most one less than the\nnumber of cusps. In this paper we study the group of modular units on $X_{1}( p\n)$, with prime level $p \\ge 5$. We give an explicit basis for this group and\nstudy certain rational subgroups of it. We use the basis to numerically\ninvestigate the structure of the cuspidal group of $X_{1}( p)$ and its rational\nsubgroup. In the later stages of this paper we use our basis to determine a\nspecific large quotient of the cuspidal group.",
        "Graph-based Network Intrusion Detection System (GNIDS) has gained significant\nmomentum in detecting sophisticated cyber-attacks, like Advanced Persistent\nThreat (APT), in an organization or across organizations. Though achieving\nsatisfying detection accuracy and adapting to ever-changing attacks and normal\npatterns, all prior GNIDSs assume the centralized data settings directly, but\nnon-trivial data collection is not always practical under privacy regulations\nnowadays. We argue that training a GNIDS model has to consider privacy\nregulations, and propose to leverage federated learning (FL) to address this\nprominent challenge.\n  Yet, directly applying FL to GNIDS is unlikely to succeed, due to issues like\nnon-IID (independent and identically distributed) graph data over clients and\nthe diverse design choices taken by different GNIDS. We address these issues\nwith a set of novel techniques tailored to the graph datasets, including\nreference graph synthesis, graph sketching and adaptive contribution scaling,\nand develop a new system Entente. We evaluate Entente on the large-scale LANL,\nOpTC and Pivoting datasets. The result shows Entente outperforms the other\nbaseline FL algorithms and sometimes even the non-FL GNIDS. We also evaluate\nEntente under FL poisoning attacks tailored to the GNIDS setting, and show\nEntente is able to bound the attack success rate to low values. Overall, our\nresult suggests building cross-silo GNIDS is feasible and we hope to encourage\nmore efforts in this direction.",
        "The constraint coordinate-momentum \\textit{phase space} (CPS) has recently\nbeen developed to study nonadiabatic dynamics in gas-phase and condensed-phase\nmolecular systems. Although the CPS formulation is exact for describing the\ndiscrete (electronic\/ vibrational\/spin) state degrees of freedom (DOFs), when\nsystem-bath models in condense phase are studied, previous works often employ\nthe discretization of environmental bath DOFs, which breaks the time\nirreversibility and may make it difficult to obtain numerically converged\nresults in the long-time limit. In this paper, we develop an exact\ntrajectory-based phase space approach by adopting the twin-space (TS)\nformulation of quantum statistical mechanics, in which the density operator of\nthe reduced system is transformed to the wavefunction of an expanded system\nwith twice the DOFs. The classical mapping model (CMM) is then used to map the\nHamiltonian of the expanded system to its equivalent classical counterpart on\nCPS. To demonstrate the applicability of the TS-CMM approach, we compare\nsimulated population dynamics and nonlinear spectra for a few benchmark\ncondensed phase system-bath models with those obtained from the hierarchical\nequations of motion method, which shows that our approach yields accurate\ndynamics of open quantum systems.",
        "Generating energy functions for heterogeneous systems suitable for\nquantitative and predictive atomistic simulations is a challenging undertaking.\nThe present work combines a cluster-based approach with electronic structure\ncalculations at the density functional theory level and machine learning-based\nenergy functions for a spectroscopic reporter for eutectic mixtures consisting\nof water, acetamide and KSCN. Two water models are considered: TIP3P which is\nconsistent with the CGenFF energy function and TIP4P which - as a water model -\nis superior to TIP4P. Both fitted models, {\\bf M2$^{\\rm TIP3P}$} and {\\bf\n  M2$^{\\rm TIP4P}$}, yield favourable thermodynamic, structural, spectroscopic\nand transport properties from extensive molecular dynamics simulations. In\nparticular, the slow and fast decay times from 2-dimensional infrared\nspectroscopy and the viscosity for water-rich mixtures are described\nrealistically and consistent with experiments. On the other hand, including the\nco-solvent (acetamide) in the present case is expected to further improve the\ncomputed viscosity for low-water content. It is concluded that such a\ncluster-based approach is a promising and generalizable route for routine\nparametrization of heterogeneous, electrostatically dominated systems.",
        "Deep reinforcement learning agents often face challenges to effectively\ncoordinate perception and decision-making components, particularly in\nenvironments with high-dimensional sensory inputs where feature relevance\nvaries. This work introduces SPRIG (Stackelberg Perception-Reinforcement\nlearning with Internal Game dynamics), a framework that models the internal\nperception-policy interaction within a single agent as a cooperative\nStackelberg game. In SPRIG, the perception module acts as a leader,\nstrategically processing raw sensory states, while the policy module follows,\nmaking decisions based on extracted features. SPRIG provides theoretical\nguarantees through a modified Bellman operator while preserving the benefits of\nmodern policy optimization. Experimental results on the Atari BeamRider\nenvironment demonstrate SPRIG's effectiveness, achieving around 30% higher\nreturns than standard PPO through its game-theoretical balance of feature\nextraction and decision-making.",
        "Critical ground states of quantum many-body systems have emerged as vital\nresources for quantum-enhanced sensing. Traditional methods to prepare these\nstates often rely on adiabatic evolution, which may diminish the quantum\nsensing advantage. In this work, we propose a quantum reinforcement learning\n(QRL)-enhanced critical sensing protocol for quantum many-body systems with\nexotic phase diagrams. Starting from product states and utilizing\nQRL-discovered gate sequences, we explore sensing accuracy in the presence of\nunknown external magnetic fields, covering both local and global regimes. Our\nresults demonstrate that QRL-learned sequences reach the finite quantum speed\nlimit and generalize effectively across systems of arbitrary size, ensuring\naccuracy regardless of preparation time. This method can robustly achieve\nHeisenberg and super-Heisenberg limits, even in noisy environments with\npractical Pauli measurements. Our study highlights the efficacy of QRL in\nenabling precise quantum state preparation, thereby advancing scalable,\nhigh-accuracy quantum critical sensing.",
        "Flow matching in the continuous simplex has emerged as a promising strategy\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\nand Score Matching, a generative framework on the simplex based on a novel\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\nparameterized velocity field that transports from smooth categorical\ndistributions to distributions concentrated at a single vertex of the simplex.\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\nthe gradient of the probability density. Our framework enables high-quality,\ndiverse generation and scales efficiently to higher-dimensional simplices. To\nenable training-free guidance, we propose Straight-Through Guided Flows\n(STGFlow), a classifier-based guidance method that leverages straight-through\nestimators to steer the unconditional velocity field toward optimal vertices of\nthe simplex. STGFlow enables efficient inference-time guidance using\nclassifiers pre-trained on clean sequences, and can be used with any discrete\nflow method. Together, these components form a robust framework for\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\nperformance in conditional DNA promoter design, sequence-only protein\ngeneration, and target-binding peptide design for rare disease treatment.",
        "Data repositories have accumulated a large number of tabular datasets from\nvarious domains. Machine Learning researchers are actively using these datasets\nto evaluate novel approaches. Consequently, data repositories have an important\nstanding in tabular data research. They not only host datasets but also provide\ninformation on how to use them in supervised learning tasks. In this paper, we\nargue that, despite great achievements in usability, the unreflected usage of\ndatasets from data repositories may have led to reduced research quality and\nscientific rigor. We present examples from prominent recent studies that\nillustrate the problematic use of datasets from OpenML, a large data repository\nfor tabular data. Our illustrations help users of data repositories avoid\nfalling into the traps of (1) using suboptimal model selection strategies, (2)\noverlooking strong baselines, and (3) inappropriate preprocessing. In response,\nwe discuss possible solutions for how data repositories can prevent the\ninappropriate use of datasets and become the cornerstones for improved overall\nquality of empirical research studies.",
        "Entanglement has the ability to enhance the transmission of classical\ninformation over a quantum channel. However, fully harvesting this advantage\ntypically requires complex entangling measurements, which are challenging to\nimplement and scale with the system's size. In this work, we consider a natural\nquantum information primitive in which the message to be communicated is\nselected stochastically. We introduce a protocol that leverages\nhigh-dimensional entanglement to perform this task perfectly, without requiring\nquantum interference between particles at the measurement station. We\nexperimentally demonstrate the protocol's scalability in an optical setup using\n8-dimensional entanglement and multi-outcome detection, providing a practical\nsolution for stochastic communication and a robust method for certifying the\ndimensionality of entanglement in communication experiments.",
        "This paper presents a simplified geometric proof of the\nMolino-Alexandrino-Radeschi (MAR) Theorem, which states that the closure of a\nsingular Riemannian foliation on a complete Riemannian manifold is itself a\nsmooth singular Riemannian foliation. Our approach circumvents several\ntechnical and analytical tools employed in the previous proof of the Theorem,\nresulting in a more direct geometric demonstration. We first establish\nconditions for a projectable foliation to be Riemannian, focusing on compatible\nconnections. We then apply these results to linear foliations on vector bundles\nand their lifts to frame bundles. Finally, we use these findings to the\nlinearization of singular Riemannian foliations around leaf closures. This\nmethod allows us to prove the smoothness of the closure directly for the linear\nsemi-local model, bypassing the need for intermediate results on orbit-like\nfoliations.",
        "In this paper, we investigate some dynamical properties near a nonhyperbolic\nfixed point. Under some conditions on the higher nonlinear terms, we establish\na stable manifold theorem and a degenerate Hartman theorem. Furthermore, the\nfinite shadowing property also be discussed.",
        "This paper investigates Rota-Baxter systems in the sense of Brzezi\\'nski from\nthe perspective of operad theory. The minimal model of the Rota-Baxter system\noperad is constructed, equivalently a concrete construction of its Koszul dual\nhomotopy cooperad is given. The concept of homotopy Rota-Baxter systems and the\n$L_\\infty$-algebra that governs deformations of a Rota-Baxter system are\nderived from the Koszul dual homotopy cooperad. The notion of\ninfinity-Yang-Baxter pairs is introduced, which is a higher-order\ngeneralization of the traditional Yang-Baxter pairs. It is shown that a\nhomotopy Rota-Baxter system structure on the endomorphism algebra of a graded\nspace is equivalent to an associative infinity-Yang-Baxter pair on this graded\nalgebra, thereby generalizing the classical correspondence between Yang-Baxter\npairs and Rota-Baxter systems.",
        "We present Keck High Resolution Echelle Spectrometer (HIRES) observations and\nmodel atmosphere analysis for two nearby, cool, helium-dominated atmosphere\nwhite dwarfs that have been polluted by accretion: WD J1927-0355 and WD\nJ2141-3300. Detected elements common to both white dwarfs are Mg, Ca, Ti, Cr,\nFe, and Ni, with additional detections of Na, Al, Si and Sr in WD J2141-3300.\nWe present an approach for inferring the composition of the accreted material,\nby adopting a physically motivated model in which the mass accretion rate\ndecays exponentially with time, which provides constraints on the time since\nthe start of the accretion event. The accretion events were most likely to have\nbegan at least 1 Myr ago, however the characteristic disc lifetime could not be\nconstrained due to degeneracies. Both white dwarfs were found to have accreted\nbulk planetary material with compositions similar to that of both bulk Earth\nand chondritic meteorites. The parent bodies causing pollution in both cases\nwere inferred to be the mass of a small moon or dwarf planet.",
        "Multi-symplectic integrators are typically regarded as a discretization of\nthe Hamiltonian partial differential equations. This is due to the fact that,\nfor generic finite-dimensional Hamiltonian systems, there exists only one\nindependent symplectic structure. In this note, the second invariant symplectic\nform is presented for the nonintegrable Henon-Heiles system, Kepler problem,\nintegrable and non-integrable Toda type systems. This approach facilitates the\nconstruction of a multi-symplectic integrator, which effectively preserves both\nsymplectic forms for these benchmark problems.",
        "Near-field communication with large antenna arrays promises significant\nbeamforming and multiplexing gains. These communication links, however, are\nvery sensitive to user mobility as any small change in the user position may\nsuddenly drop the signal power. This leads to critical challenges for the\nrobustness of these near-field communication systems. In this paper, we propose\n\\textit{sphere precoding}, which is a robust precoding design to address user\nmobility in near-field communications. To gain insights into the spatial\ncorrelation of near-field channels, we extend the one-ring channel model to\nwhat we call one-sphere channel model and derive the channel covariance\nconsidering user mobility. Based on the one-sphere channel model, a robust\nprecoding design problem is defined to optimize the minimum\nsignal-to-interference-plus-noise ratio (SINR) satisfaction probability among\nmobile users. By utilizing the eigen structure of channel covariance, we\nfurther design a relaxed convex problem to approximate the solution of the\noriginal non-convex problem. The low-complexity solution effectively shapes a\nsphere that maintains the signal power for the target user and also nulls its\ninterference within spheres around the other users. Simulation results\nhighlight the efficacy of the proposed solution in achieving robust precoding\nyet high achievable rates in near-field communication systems.",
        "In recent years, the research community, but also the general public, has\nraised serious questions about the reproducibility and replicability of\nscientific work. Since many studies include some kind of computational work,\nthese issues are also a technological challenge, not only in computer science,\nbut also in most research domains. Computational replicability and\nreproducibility are not easy to achieve due to the variety of computational\nenvironments that can be used. Indeed, it is challenging to recreate the same\nenvironment via the same frameworks, code, programming languages, dependencies,\nand so on. We propose a framework, known as SciRep, that supports the\nconfiguration, execution, and packaging of computational experiments by\ndefining their code, data, programming languages, dependencies, databases, and\ncommands to be executed. After the initial configuration, the experiments can\nbe executed any number of times, always producing exactly the same results. Our\napproach allows the creation of a reproducibility package for experiments from\nmultiple scientific fields, from medicine to computer science, which can be\nre-executed on any computer. The produced package acts as a capsule, holding\nabsolutely everything necessary to re-execute the experiment. To evaluate our\nframework, we compare it with three state-of-the-art tools and use it to\nreproduce 18 experiments extracted from published scientific articles. With our\napproach, we were able to execute 16 (89%) of those experiments, while the\nothers reached only 61%, thus showing that our approach is effective. Moreover,\nall the experiments that were executed produced the results presented in the\noriginal publication. Thus, SciRep was able to reproduce 100% of the\nexperiments it could run.",
        "The choroid, a highly vascular layer behind the retina, is an extension of\nthe central nervous system and has parallels with the renal cortex, with blood\nflow far exceeding that of the brain and kidney. Thus, there has been growing\ninterest of choroidal blood flow reflecting physiological status of systemic\ndisease. Optical coherence tomography (OCT) enables high-resolution imaging of\nthe choroid, but conventional analysis methods remain manual or semi-automatic,\nlimiting reproducibility, standardisation and clinical utility. In this thesis,\nI develop several new methods to analyse the choroid in OCT image sequences,\nwith each successive method improving on its predecessors. I first develop two\nsemi-automatic approaches for choroid region (Gaussian Process Edge Tracing,\nGPET) and vessel (Multi-scale Median Cut Quantisation, MMCQ) analysis, which\nimprove on manual approaches but remain user-dependent. To address this, I\nintroduce DeepGPET, a deep learning-based region segmentation method which\nimproves on execution time, reproducibility, and end-user accessibility, but\nlacks choroid vessel analysis and automatic feature measurement. Improving on\nthis, I developed Choroidalyzer, a deep learning-based pipeline to segment the\nchoroidal space and vessels and generate fully automatic, clinically meaningful\nand reproducible choroidal features. I provide rigorous evaluation of these\nfour approaches and consider their potential clinical value in three\napplications into systemic health: OCTANE, assessing choroidal changes in renal\ntransplant recipients and donors; PREVENT, exploring choroidal associations\nwith Alzheimer's risk factors at mid-life; D-RISCii, assessing choroidal\nvariation and feasibility of OCT in critical care. In short, this thesis\ncontributes many open-source tools for standardised choroidal measurement and\nhighlights the choroid's potential as a biomarker in systemic health.",
        "As a cornerstone in the Evolutionary Computation (EC) domain, Differential\nEvolution (DE) is known for its simplicity and effectiveness in handling\nchallenging black-box optimization problems. While the advantages of DE are\nwell-recognized, achieving peak performance heavily depends on its\nhyperparameters such as the mutation factor, crossover probability, and the\nselection of specific DE strategies. Traditional approaches to this\nhyperparameter dilemma have leaned towards parameter tuning or adaptive\nmechanisms. However, identifying the optimal settings tailored for specific\nproblems remains a persistent challenge. In response, we introduce MetaDE, an\napproach that evolves DE's intrinsic hyperparameters and strategies using DE\nitself at a meta-level. A pivotal aspect of MetaDE is a specialized\nparameterization technique, which endows it with the capability to dynamically\nmodify DE's parameters and strategies throughout the evolutionary process. To\naugment computational efficiency, MetaDE incorporates a design that leverages\nparallel processing through a GPU-accelerated computing framework. Within such\na framework, DE is not just a solver but also an optimizer for its own\nconfigurations, thus streamlining the process of hyperparameter optimization\nand problem-solving into a cohesive and automated workflow. Extensive\nevaluations on the CEC2022 benchmark suite demonstrate MetaDE's promising\nperformance. Moreover, when applied to robot control via evolutionary\nreinforcement learning, MetaDE also demonstrates promising performance. The\nsource code of MetaDE is publicly accessible at:\nhttps:\/\/github.com\/EMI-Group\/metade.",
        "In this work we propose a satellite specific Neural Radiance Fields (NeRF)\nmodel capable to obtain a three-dimensional semantic representation (neural\nsemantic field) of the scene. The model derives the output from a set of\nmulti-date satellite images with corresponding pixel-wise semantic labels. We\ndemonstrate the robustness of our approach and its capability to improve noisy\ninput labels. We enhance the color prediction by utilizing the semantic\ninformation to address temporal image inconsistencies caused by non-stationary\ncategories such as vehicles. To facilitate further research in this domain, we\npresent a dataset comprising manually generated labels for popular multi-view\nsatellite images. Our code and dataset are available at\nhttps:\/\/github.com\/wagnva\/semantic-nerf-for-satellite-data.",
        "Large language models (LLMs) should undergo rigorous audits to identify\npotential risks, such as copyright and privacy infringements. Once these risks\nemerge, timely updates are crucial to remove undesirable responses, ensuring\nlegal and safe model usage. It has spurred recent research into LLM unlearning,\nfocusing on erasing targeted undesirable knowledge without compromising the\nintegrity of other, non-targeted responses. Existing studies have introduced\nvarious unlearning objectives to pursue LLM unlearning without necessitating\ncomplete retraining. However, each of these objectives has unique properties,\nand no unified framework is currently available to comprehend them thoroughly.\nTo fill the gap, we propose a toolkit of the gradient effect (G-effect),\nquantifying the impacts of unlearning objectives on model performance from a\ngradient perspective. A notable advantage is its broad ability to detail the\nunlearning impacts from various aspects across instances, updating steps, and\nLLM layers. Accordingly, the G-effect offers new insights into identifying\ndrawbacks of existing unlearning objectives, further motivating us to explore a\nseries of new solutions for their mitigation and improvements. Finally, we\noutline promising directions that merit further studies, aiming at contributing\nto the community to advance this important field.",
        "Experience replay is a foundational technique in reinforcement learning that\nenhances learning stability by storing past experiences in a replay buffer and\nreusing them during training. Despite its practical success, its theoretical\nproperties remain underexplored. In this paper, we present a theoretical\nframework that models experience replay using resampled $U$- and\n$V$-statistics, providing rigorous variance reduction guarantees. We apply this\nframework to policy evaluation tasks using the Least-Squares Temporal\nDifference (LSTD) algorithm and a Partial Differential Equation (PDE)-based\nmodel-free algorithm, demonstrating significant improvements in stability and\nefficiency, particularly in data-scarce scenarios. Beyond policy evaluation, we\nextend the framework to kernel ridge regression, showing that the experience\nreplay-based method reduces the computational cost from the traditional\n$O(n^3)$ in time to as low as $O(n^2)$ in time while simultaneously reducing\nvariance. Extensive numerical experiments validate our theoretical findings,\ndemonstrating the broad applicability and effectiveness of experience replay in\ndiverse machine learning tasks.",
        "Explainable AI (XAI) techniques are necessary to help clinicians make sense\nof AI predictions and integrate predictions into their decision-making\nworkflow. In this work, we conduct a survey study to understand clinician\npreference among different XAI techniques when they are used to interpret model\npredictions over text-based EHR data. We implement four XAI techniques (LIME,\nAttention-based span highlights, exemplar patient retrieval, and free-text\nrationales generated by LLMs) on an outcome prediction model that uses ICU\nadmission notes to predict a patient's likelihood of experiencing in-hospital\nmortality. Using these XAI implementations, we design and conduct a survey\nstudy of 32 practicing clinicians, collecting their feedback and preferences on\nthe four techniques. We synthesize our findings into a set of recommendations\ndescribing when each of the XAI techniques may be more appropriate, their\npotential limitations, as well as recommendations for improvement.",
        "Predicting pedestrian trajectories is essential for autonomous driving\nsystems, as it significantly enhances safety and supports informed\ndecision-making. Accurate predictions enable the prevention of collisions,\nanticipation of crossing intent, and improved overall system efficiency. In\nthis study, we present SGNetPose+, an enhancement of the SGNet architecture\ndesigned to integrate skeleton information or body segment angles with bounding\nboxes to predict pedestrian trajectories from video data to avoid hazards in\nautonomous driving. Skeleton information was extracted using a pose estimation\nmodel, and joint angles were computed based on the extracted joint data. We\nalso apply temporal data augmentation by horizontally flipping video frames to\nincrease the dataset size and improve performance. Our approach achieves\nstate-of-the-art results on the JAAD and PIE datasets using pose data with the\nbounding boxes, outperforming the SGNet model. Code is available on Github:\nSGNetPose+.",
        "Modeling lithium-ion battery (LIB) degradation offers significant cost\nsavings and enhances the safety and reliability of electric vehicles (EVs) and\nbattery energy storage systems (BESS). Whilst data-driven methods have received\ngreat attention for forecasting degradation, they often demonstrate limited\ngeneralization ability and tend to underperform particularly in critical\nscenarios involving accelerated degradation, which are crucial to predict\naccurately. These methods also fail to elucidate the underlying causes of\ndegradation. Alternatively, physical models provide a deeper understanding, but\ntheir complex parameters and inherent uncertainties limit their applicability\nin real-world settings. To this end, we propose a new model - ACCEPT. Our novel\nframework uses contrastive learning to map the relationship between the\nunderlying physical degradation parameters and observable operational\nquantities, combining the benefits of both approaches. Furthermore, due to the\nsimilarity of degradation paths between LIBs with the same chemistry, this\nmodel transfers non-trivially to most downstream tasks, allowing for zero-shot\ninference. Additionally, since categorical features can be included in the\nmodel, it can generalize to other LIB chemistries. This work establishes a\nfoundational battery degradation model, providing reliable forecasts across a\nrange of battery types and operating conditions.",
        "We study the concentration field in a prescribed 2D Cahn-Hilliard\nNavier-Stokes (CHNS) system. We formulate a description for the target pattern\nformation and pattern merging processes, and compare this description with\nsimulation results. Shear-augmented diffusion along streamlines causes a\nseparation of time scales, thus 2D CHNS system can be simplified to a 1D\nsystem. In this 1D system, target pattern formation is induced by linear\ninstability. The waveform of patterns are described by Jacobi Elliptic\nFunctions. The interface (of pattern) migration or coarsening velocity is\ndetermined by the derivative of interface curvature. The anomalous migration of\ninner pattern can be explained by the singularity at the origin and therefore\nthe boundary motion in the quasi-one-dimension system. Finally we derive a\nsimple criterion for when CHNS system becomes dynamic by following similar\ncases in MHD."
      ]
    }
  },
  {
    "id":2411.01668,
    "research_type":"basic",
    "start_id":"b3",
    "start_title":"Linear-quadratic mean field games",
    "start_abstract":"In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Mean\u2010field games with differing beliefs for algorithmic trading"
      ],
      "abstract":[
        "Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity."
      ],
      "categories":[
        "q-fin.MF"
      ]
    },
    "list":{
      "title":[
        "GM-MoE: Low-Light Enhancement with Gated-Mechanism Mixture-of-Experts",
        "Large Language Models with Human-In-The-Loop Validation for Systematic\n  Review Data Extraction",
        "MODRIC: A Cost Effective MODular Data Center Network Architecture with\n  Rich InterConnections",
        "Investigating Human-Aligned Large Language Model Uncertainty",
        "Synthesizing Consistent Novel Views via 3D Epipolar Attention without\n  Re-Training",
        "An X-ray view of the Cataclysmic Variable V902 Mon: Discovery of an\n  X-ray eclipse",
        "Superconducting LaPtH$_{ 6 }$ with triatomic hydrogen units",
        "Bridging Structural Dynamics and Biomechanics: Human Motion Estimation\n  through Footstep-Induced Floor Vibrations",
        "Connecting the dots: Tracing the evolutionary pathway of Polar Ring\n  Galaxies in the cases of NGC 3718, NGC 2685, and NGC 4262",
        "TRADES: Generating Realistic Market Simulations with Diffusion Models",
        "Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go\n  Beyond",
        "Optimizing CNN Architectures for Advanced Thoracic Disease\n  Classification",
        "Evolution of Spots and Stripes in Cellular Automata",
        "Formation of super-Earths and mini-Neptunes from rings of planetesimals",
        "Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and\n  efficient reconstruction of dynamic scene",
        "Practical programming research of Linear DML model based on the simplest\n  Python code: From the standpoint of novice researchers",
        "The NANOGrav 15-year Data Set: Search for Gravitational Wave Memory",
        "Online Nonstochastic Control with Convex Safety Constraints",
        "Curiosity-Diffuser: Curiosity Guide Diffusion Models for Reliability",
        "Toward Responsible Federated Large Language Models: Leveraging a Safety\n  Filter and Constitutional AI",
        "Formation of ultracold triatomic molecules by electric microwave\n  association",
        "Provable Benefits of Unsupervised Pre-training and Transfer Learning via\n  Single-Index Models",
        "Sliding Window Attention Training for Efficient Large Language Models",
        "Federated Learning Strategies for Coordinated Beamforming in Multicell\n  ISAC",
        "Spatial locking of chimera states to frequency heterogeneity in\n  nonlocally coupled oscillators",
        "Finding the ultra-narrow $^3\\!P_2 \\rightarrow \\, ^3\\!P_0$ electric\n  quadrupole transition in Ni$^{12+}$ ion for an optical clock",
        "Endogenous Persistence at the Effective Lower Bound",
        "Perception-Guided EEG Analysis: A Deep Learning Approach Inspired by\n  Level of Detail (LOD) Theory",
        "R.I.P.: Better Models by Survival of the Fittest Prompts"
      ],
      "abstract":[
        "Low-light enhancement has wide applications in autonomous driving, 3D\nreconstruction, remote sensing, surveillance, and so on, which can\nsignificantly improve information utilization. However, most existing methods\nlack generalization and are limited to specific tasks such as image recovery.\nTo address these issues, we propose \\textbf{Gated-Mechanism Mixture-of-Experts\n(GM-MoE)}, the first framework to introduce a mixture-of-experts network for\nlow-light image enhancement. GM-MoE comprises a dynamic gated weight\nconditioning network and three sub-expert networks, each specializing in a\ndistinct enhancement task. Combining a self-designed gated mechanism that\ndynamically adjusts the weights of the sub-expert networks for different data\ndomains. Additionally, we integrate local and global feature fusion within\nsub-expert networks to enhance image quality by capturing multi-scale features.\nExperimental results demonstrate that the GM-MoE achieves superior\ngeneralization with respect to 25 compared approaches, reaching\nstate-of-the-art performance on PSNR on 5 benchmarks and SSIM on 4 benchmarks,\nrespectively.",
        "Systematic reviews are time-consuming endeavors. Historically speaking,\nknowledgeable humans have had to screen and extract data from studies before it\ncan be analyzed. However, large language models (LLMs) hold promise to greatly\naccelerate this process. After a pilot study which showed great promise, we\ninvestigated the use of freely available LLMs for extracting data for\nsystematic reviews. Using three different LLMs, we extracted 24 types of data,\n9 explicitly stated variables and 15 derived categorical variables, from 112\nstudies that were included in a published scoping review. Overall we found that\nGemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Large 2 performed reasonably\nwell, with 71.17%, 72.14%, and 62.43% of data extracted being consistent with\nhuman coding, respectively. While promising, these results highlight the dire\nneed for a human-in-the-loop (HIL) process for AI-assisted data extraction. As\na result, we present a free, open-source program we developed (AIDE) to\nfacilitate user-friendly, HIL data extraction with LLMs.",
        "Shipping container based modular architectures provide design flexibility in\ndata centers with building blocks to expand the network as and when needed. In\nthis paper, high capacity Modular Data Center (MDC) network architecture with\nRich Inter Connections named MODRIC is proposed. MODRIC is a cost-effective\nswitch-centric network design which allows building a flexible MDC network with\ncommodity switches. It uses an inter-container connectivity similar to the\nstructure of generalized hypercube in order to provide high inter-container\nbandwidth. Further, a hybrid Clos topology is used to build the container\nnetwork. MODRIC is highly suitable for cost effectively building mega data\ncenters requiring high throughput capacity and resilience against failures.\nThis paper presents the proposed architecture, discusses its relevant\nproperties, and proposes suitable addressing, routing and network construction\nschemes. The paper also presents comparative studies on its cost and\nperformance with existing network topologies.",
        "Recent work has sought to quantify large language model uncertainty to\nfacilitate model control and modulate user trust. Previous works focus on\nmeasures of uncertainty that are theoretically grounded or reflect the average\novert behavior of the model. In this work, we investigate a variety of\nuncertainty measures, in order to identify measures that correlate with human\ngroup-level uncertainty. We find that Bayesian measures and a variation on\nentropy measures, top-k entropy, tend to agree with human behavior as a\nfunction of model size. We find that some strong measures decrease in\nhuman-similarity with model size, but, by multiple linear regression, we find\nthat combining multiple uncertainty measures provide comparable human-alignment\nwith reduced size-dependency.",
        "Large diffusion models demonstrate remarkable zero-shot capabilities in novel\nview synthesis from a single image. However, these models often face challenges\nin maintaining consistency across novel and reference views. A crucial factor\nleading to this issue is the limited utilization of contextual information from\nreference views. Specifically, when there is an overlap in the viewing frustum\nbetween two views, it is essential to ensure that the corresponding regions\nmaintain consistency in both geometry and appearance. This observation leads to\na simple yet effective approach, where we propose to use epipolar geometry to\nlocate and retrieve overlapping information from the input view. This\ninformation is then incorporated into the generation of target views,\neliminating the need for training or fine-tuning, as the process requires no\nlearnable parameters. Furthermore, to enhance the overall consistency of\ngenerated views, we extend the utilization of epipolar attention to a\nmulti-view setting, allowing retrieval of overlapping information from the\ninput view and other target views. Qualitative and quantitative experimental\nresults demonstrate the effectiveness of our method in significantly improving\nthe consistency of synthesized views without the need for any fine-tuning.\nMoreover, This enhancement also boosts the performance of downstream\napplications such as 3D reconstruction. The code is available at\nhttps:\/\/github.com\/botaoye\/ConsisSyn.",
        "V902 Mon is one of a few eclipsing Intermediate Polars (IPs), and show deep\neclipses in the optical lightcurves. The presence of a strong Fe K$\\alpha$\nfluorescence line in its X-ray spectrum and its low X-ray flux compared to\nother IPs suggests significant absorption, most likely from an accretion disk.\nIn an observation carried out using the Nuclear Spectroscopic Telescope Array\n(NuSTAR), we confirm the presence of an X-ray eclipse in the energy resolved\nlightcurves, coincident with the optical AAVSO\/CV-band lightcurves. Broadband\nX-ray spectral analysis using NuSTAR and XMM-Newton observations confirm a\nstrong absorption N$_{H}$ $\\sim 10^{23}$ cm$^{-2}$ local to the source, along\nwith a high equivalent width of about 0.7 keV for a Fe K$\\alpha$ fluorescence\nline. We interpret this using a model similar to an Accretion Disk Corona\nsource, which have a very high inclination and the compact object is heavily\nobscured by the body of the accretion disk. We propose that the primary X-rays\nfrom the accretion column in V902 Mon is hidden from our direct view at all\ntimes by the accretion disk. In this scenario, the observed scattered X-rays\nindicate substantial absorption of direct X-rays by the accretion disk.\nAdditionally, a strong Fe fluorescence line suggests reprocessing of the\nradiation by a more extended region, such as the pre-shock region, which could\nbe located a few white dwarf radii above the orbital plane.",
        "To veryfy \"hot supreconductivity\" recently proposed in lanthanum\nhydride-based compounds, we explored thermodynamically stable and\nsuperconducting phases in the lanthanum (La)-platinum (Pt)-hydrogen (H) ternary\nsystem at 20 GPa using an evolutionary construction scheme of a\nformation-enthalpy convex hull, universal neural network potential\ncalculations, and density functional theory calculations. Although we found no\nevidence of the hot superconductivity in this ternary system, we predicted a\nunique compound, LaPtH$_{ 6 }$, which has equilateral triangular H$_{ 3 }$\nunits nearly forming a two-dimensional kagome lattice between La and Pt layers\nand shows the superconductivity at 18.67 K. This structure is dynamically\nstable from ambient pressure to at least 200 GPa and the superconducting\ncritical temperature increases from 13.51 to 40.63 K.",
        "Quantitative estimation of human joint motion in daily living spaces is\nessential for early detection and rehabilitation tracking of\nneuromusculoskeletal disorders (e.g., Parkinson's) and mitigating trip and fall\nrisks for older adults. Existing approaches involve monitoring devices such as\ncameras, wearables, and pressure mats, but have operational constraints such as\ndirect line-of-sight, carrying devices, and dense deployment. To overcome these\nlimitations, we leverage gait-induced floor vibration to estimate lower-limb\njoint motion (e.g., ankle, knee, and hip flexion angles), allowing\nnon-intrusive and contactless gait health monitoring in people's living spaces.\nTo overcome the high uncertainty in lower-limb movement given the limited\ninformation provided by the gait-induced floor vibrations, we formulate a\nphysics-informed graph to integrate domain knowledge of gait biomechanics and\nstructural dynamics into the model. Specifically, different types of nodes\nrepresent heterogeneous information from joint motions and floor vibrations;\nTheir connecting edges represent the physiological relationships between joints\nand forces governed by gait biomechanics, as well as the relationships between\nforces and floor responses governed by the structural dynamics. As a result,\nour model poses physical constraints to reduce uncertainty while allowing\ninformation sharing between the body and the floor to make more accurate\npredictions. We evaluate our approach with 20 participants through a real-world\nwalking experiment. We achieved an average of 3.7 degrees of mean absolute\nerror in estimating 12 joint flexion angles (38% error reduction from\nbaseline), which is comparable to the performance of cameras and wearables in\ncurrent medical practices.",
        "Polar Ring Galaxies (PRGs) are a unique class of galaxies characterised by a\nring of gas and stars orbiting nearly orthogonal to the main body. This study\ndelves into the evolutionary trajectory of PRGs using the exemplary trio of NGC\n3718, NGC 2685, and NGC 4262. We investigate the distinct features of PRGs by\nanalysing their ring and host components to reveal their unique characteristics\nthrough Spectral Energy Distribution (SED) fitting. Using CIGALE, we performed\nSED fitting to independently analyse the ring and host spatially resolved\nregions, marking the first decomposed SED analysis for PRGs, which examines\nstellar populations using high-resolution observations from AstroSat UVIT at a\nresolved scale. The UV-optical surface profiles provide an initial idea that\ndistinct patterns in the galaxies, with differences in FUV and NUV, suggest\nthree distinct stages of ring evolution in the selected galaxies. The study of\nresolved-scale stellar regions reveals that the ring regions are generally\nyounger than their host galaxies, with the age disparity progressively\ndecreasing along the evolutionary sequence from NGC 3718 to NGC 4262. Star\nformation rates (SFR) also exhibit a consistent pattern, with higher SFR in the\nring of NGC 3718 compared to the others, and a progressive decrease through NGC\n2685 and NGC 4262. Finally, the representation of the galaxies in the HI gas\nfraction versus the NUV- r plane supports the idea that they are in three\ndifferent evolutionary stages of PRG evolution, with NGC 3718 in the initial\nstage, NGC 2685 in the intermediate stage, and NGC 4262 representing the final\nstage. NGC 3718, NGC 2685, and NGC 4262 represent different stages of this\nevolution, highlighting the dynamic nature of PRGs and emphasising the\nimportance of studying their evolutionary processes to gain insights into\ngalactic formation and evolution.",
        "Financial markets are complex systems characterized by high statistical\nnoise, nonlinearity, and constant evolution. Thus, modeling them is extremely\nhard. We address the task of generating realistic and responsive Limit Order\nBook (LOB) market simulations, which are fundamental for calibrating and\ntesting trading strategies, performing market impact experiments, and\ngenerating synthetic market data. Previous works lack realism, usefulness, and\nresponsiveness of the generated simulations. To bridge this gap, we propose a\nnovel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB\nSimulations (TRADES). TRADES generates realistic order flows conditioned on the\nstate of the market, leveraging a transformer-based architecture that captures\nthe temporal and spatial characteristics of high-frequency market data. There\nis a notable absence of quantitative metrics for evaluating generative market\nsimulation models in the literature. To tackle this problem, we adapt the\npredictive score, a metric measured as an MAE, by training a stock price\npredictive model on synthetic data and testing it on real data. We compare\nTRADES with previous works on two stocks, reporting an x3.27 and x3.47\nimprovement over SoTA according to the predictive score, demonstrating that we\ngenerate useful synthetic market data for financial downstream tasks. We assess\nTRADES's market simulation realism and responsiveness, showing that it\neffectively learns the conditional data distribution and successfully reacts to\nan experimental agent, giving sprout to possible calibrations and evaluations\nof trading strategies and market impact experiments. We developed DeepMarket,\nthe first open-source Python framework for market simulation with deep\nlearning. Our repository includes a synthetic LOB dataset composed of TRADES's\ngenerates simulations. We release the code at\ngithub.com\/LeonardoBerti00\/DeepMarket.",
        "Large language models (LLMs) should undergo rigorous audits to identify\npotential risks, such as copyright and privacy infringements. Once these risks\nemerge, timely updates are crucial to remove undesirable responses, ensuring\nlegal and safe model usage. It has spurred recent research into LLM unlearning,\nfocusing on erasing targeted undesirable knowledge without compromising the\nintegrity of other, non-targeted responses. Existing studies have introduced\nvarious unlearning objectives to pursue LLM unlearning without necessitating\ncomplete retraining. However, each of these objectives has unique properties,\nand no unified framework is currently available to comprehend them thoroughly.\nTo fill the gap, we propose a toolkit of the gradient effect (G-effect),\nquantifying the impacts of unlearning objectives on model performance from a\ngradient perspective. A notable advantage is its broad ability to detail the\nunlearning impacts from various aspects across instances, updating steps, and\nLLM layers. Accordingly, the G-effect offers new insights into identifying\ndrawbacks of existing unlearning objectives, further motivating us to explore a\nseries of new solutions for their mitigation and improvements. Finally, we\noutline promising directions that merit further studies, aiming at contributing\nto the community to advance this important field.",
        "Machine learning, particularly convolutional neural networks (CNNs), has\nshown promise in medical image analysis, especially for thoracic disease\ndetection using chest X-ray images. In this study, we evaluate various CNN\narchitectures, including binary classification, multi-label classification, and\nResNet50 models, to address challenges like dataset imbalance, variations in\nimage quality, and hidden biases. We introduce advanced preprocessing\ntechniques such as principal component analysis (PCA) for image compression and\npropose a novel class-weighted loss function to mitigate imbalance issues. Our\nresults highlight the potential of CNNs in medical imaging but emphasize that\nissues like unbalanced datasets and variations in image acquisition methods\nmust be addressed for optimal model performance.",
        "Cellular automata are computers, similar to Turing machines. The main\ndifference is that Turing machines use a one-dimensional tape, whereas cellular\nautomata use a two-dimensional grid. The best-known cellular automaton is the\nGame of Life, which is a universal computer. It belongs to a family of cellular\nautomata with 262,144 members. Playing the Game of Life generally involves\nengineering; that is, assembling a device composed of various parts that are\ncombined to achieve a specific intended result. Instead of engineering cellular\nautomata, we propose evolving cellular automata. Evolution applies mutation and\nselection to a population of organisms. If a mutation increases the fitness of\nan organism, it may have many descendants, displacing the less fit organisms.\nUnlike engineering, evolution does not work towards an imagined goal. Evolution\nworks towards increasing fitness, with no expectations about the specific form\nof the final result. Mutation, selection, and fitness yield structures that\nappear to be more organic and life-like than engineered structures. In our\nexperiments, the patterns resulting from evolving cellular automata look much\nlike the spots on leopards and the stripes on tigers.",
        "The solar system planetary architecture has been proposed to be consistent\nwith the terrestrial and giant planets forming from material rings at ~1 au and\n~5 au, respectively. Here, we show that super-Earths and mini-Neptunes may\nshare a similar formation pathway. In our simulations conducted with a disk\nalpha-viscosity of 4e-3, super-Earths accrete from rings of rocky material in\nthe inner disk, growing predominantly via planetesimal accretion. Mini-Neptunes\nprimarily originate from rings located beyond the water snowline, forming via\npebble accretion. Our simulations broadly match the period-ratio distribution,\nthe intra-system size uniformity, and the planet multiplicity distribution of\nexoplanets. The radius valley constrains the typical total mass available for\nrocky planet formation to be less than 3-6 Earth masses. Our results predict\nthat planets at ~1 au in systems with close-in super-Earths and mini-Neptunes\nare predominantly water-rich. Though relatively uncommon, at ~1% level, such\nsystems might also host rocky Earth-sized planets in the habitable zone that\nunderwent late giant impacts, akin to the Moon-forming event.",
        "Novel view synthesis has long been a practical but challenging task, although\nthe introduction of numerous methods to solve this problem, even combining\nadvanced representations like 3D Gaussian Splatting, they still struggle to\nrecover high-quality results and often consume too much storage memory and\ntraining time. In this paper we propose Swift4D, a divide-and-conquer 3D\nGaussian Splatting method that can handle static and dynamic primitives\nseparately, achieving a good trade-off between rendering quality and\nefficiency, motivated by the fact that most of the scene is the static\nprimitive and does not require additional dynamic properties. Concretely, we\nfocus on modeling dynamic transformations only for the dynamic primitives which\nbenefits both efficiency and quality. We first employ a learnable decomposition\nstrategy to separate the primitives, which relies on an additional parameter to\nclassify primitives as static or dynamic. For the dynamic primitives, we employ\na compact multi-resolution 4D Hash mapper to transform these primitives from\ncanonical space into deformation space at each timestamp, and then mix the\nstatic and dynamic primitives to produce the final output. This\ndivide-and-conquer method facilitates efficient training and reduces storage\nredundancy. Our method not only achieves state-of-the-art rendering quality\nwhile being 20X faster in training than previous SOTA methods with a minimum\nstorage requirement of only 30MB on real-world datasets. Code is available at\nhttps:\/\/github.com\/WuJH2001\/swift4d.",
        "This paper presents linear DML models for causal inference using the simplest\nPython code on a Jupyter notebook based on an Anaconda platform and compares\nthe performance of different DML models. The results show that current Library\nAPI technology is not yet sufficient to enable novice Python users to build\nqualified and high-quality DML models with the simplest coding approach. Novice\nusers attempting to perform DML causal inference using Python still have to\nimprove their mathematical and computer knowledge to adapt to more flexible DML\nprogramming. Additionally, the issue of mismatched outcome variable dimensions\nis also widespread when building linear DML models in Jupyter notebook.",
        "We present the results of a search for nonlinear gravitational wave memory in\nthe NANOGrav 15-year data set. We find no significant evidence for memory\nsignals in the dataset, with a maximum Bayes factor of 3.1 in favor of a model\nincluding memory. We therefore place upper limits on the strain of potential\ngravitational wave memory events as a function of sky location and observing\nepoch. We find upper limits that are not always more constraining than previous\nNANOGrav results. We show that it is likely due to the increase in common red\nnoise between the 12.5-year and 15-year NANOGrav datasets.",
        "This paper considers the online nonstochastic control problem of a linear\ntime-invariant system under convex state and input constraints that need to be\nsatisfied at all times. We propose an algorithm called Online Gradient Descent\nwith Buffer Zone for Convex Constraints (OGD-BZC), designed to handle scenarios\nwhere the system operates within general convex safety constraints. We\ndemonstrate that OGD-BZC, with appropriate parameter selection, satisfies all\nthe safety constraints under bounded adversarial disturbances. Additionally, to\nevaluate the performance of OGD-BZC, we define the regret with respect to the\nbest safe linear policy in hindsight. We prove that OGD-BZC achieves $\\tilde{O}\n(\\sqrt{T})$ regret given proper parameter choices. Our numerical results\nhighlight the efficacy and robustness of the proposed algorithm.",
        "One of the bottlenecks in robotic intelligence is the instability of neural\nnetwork models, which, unlike control models, lack a well-defined convergence\ndomain and stability. This leads to risks when applying intelligence in the\nphysical world. Specifically, imitation policy based on neural network may\ngenerate hallucinations, leading to inaccurate behaviors that impact the safety\nof real-world applications. To address this issue, this paper proposes the\nCuriosity-Diffuser, aimed at guiding the conditional diffusion model to\ngenerate trajectories with lower curiosity, thereby improving the reliability\nof policy. The core idea is to use a Random Network Distillation (RND)\ncuriosity module to assess whether the model's behavior aligns with the\ntraining data, and then minimize curiosity by classifier guidance diffusion to\nreduce overgeneralization during inference. Additionally, we propose a\ncomputationally efficient metric for evaluating the reliability of the policy,\nmeasuring the similarity between the generated behaviors and the training\ndataset, to facilitate research about reliability learning. Finally, simulation\nverify the effectiveness and applicability of the proposed method to a variety\nof scenarios, showing that Curiosity-Diffuser significantly improves task\nperformance and produces behaviors that are more similar to the training data.\nThe code for this work is available at: github.com\/CarlDegio\/Curiosity-Diffuser",
        "Recent research has increasingly focused on training large language models\n(LLMs) using federated learning, known as FedLLM. However, responsible AI\n(RAI), which aims to ensure safe responses, remains underexplored in the\ncontext of FedLLM. In FedLLM, client data used for training may contain harmful\ncontent, leading to unsafe LLMs that generate harmful responses. Aggregating\nsuch unsafe LLMs into the global model and distributing them to clients may\nresult in the widespread deployment of unsafe LLMs. To address this issue, we\nincorporate two well-known RAI methods into FedLLM: the safety filter and\nconstitutional AI. Our experiments demonstrate that these methods significantly\nenhance the safety of the LLM, achieving over a 20% improvement on AdvBench, a\nbenchmark for evaluating safety performance.",
        "A theoretical model is proposed for the formation of ultracold ground-state\ntriatomic molecules in weakly bound energy levels. The process is driven by the\nelectric component of a microwave field, which induces the association of an\nultracold atom colliding with an ultracold diatomic molecule. This model is\nexemplified using $^{39}$K atoms and $^{23}$Na$^{39}$K molecules, both in their\nground states, a scenario of experimental relevance. The model assumes that the\ndynamics of the association are dominated by the long-range van der Waals\ninteraction between $^{39}$K and $^{23}$Na$^{39}$K. The electric microwave\nassociation mechanism relies on the intrinsic electric dipole moment of\n$^{23}$Na$^{39}$K, which drives transitions between its lowest rotational\nlevels ( $j$=0 and $j$=1). The energies of the uppermost triatomic energy\nlevels are computed by numerically solving coupled Schr\\\"odinger equations\nusing the Mapped Fourier Grid Hamiltonian method. Measurable association rates\nare derived within the framework of a perturbative approach. This method of\nelectric microwave association provides an alternative to atom-molecule\nassociation via magnetic Feshbach resonances for forming ultracold, deeply\nbound triatomic molecules, and is applicable to a wide range of polar diatomic\nmolecules.",
        "Unsupervised pre-training and transfer learning are commonly used techniques\nto initialize training algorithms for neural networks, particularly in settings\nwith limited labeled data. In this paper, we study the effects of unsupervised\npre-training and transfer learning on the sample complexity of high-dimensional\nsupervised learning. Specifically, we consider the problem of training a\nsingle-layer neural network via online stochastic gradient descent. We\nestablish that pre-training and transfer learning (under concept shift) reduce\nsample complexity by polynomial factors (in the dimension) under very general\nassumptions. We also uncover some surprising settings where pre-training grants\nexponential improvement over random initialization in terms of sample\ncomplexity.",
        "Recent advances in transformer-based Large Language Models (LLMs) have\ndemonstrated remarkable capabilities across various tasks. However, their\nquadratic computational complexity concerning sequence length remains a\nsignificant bottleneck for processing long documents. As a result, many efforts\nlike sparse attention and state space models have been proposed to improve the\nefficiency of LLMs over long sequences. Though effective, these approaches\ncompromise the performance or introduce structural complexity. This calls for a\nsimple yet efficient model that preserves the fundamental Transformer\narchitecture. To this end, we introduce SWAT, which enables efficient\nlong-context handling via Sliding Window Attention Training. This paper first\nattributes the inefficiency of Transformers to the attention sink phenomenon\nresulting from the high variance of softmax operation. Then, we replace softmax\nwith the sigmoid function and utilize a balanced ALiBi and Rotary Position\nEmbedding for efficient information compression and retention. Experiments\ndemonstrate that SWAT achieves SOTA performance compared with state-of-the-art\nlinear recurrent architectures on eight benchmarks. Code is available at\nhttps:\/\/anonymous.4open.science\/r\/SWAT-attention.",
        "We propose two cooperative beamforming frameworks based on federated learning\n(FL) for multi-cell integrated sensing and communications (ISAC) systems. Our\nobjective is to address the following dilemma in multicell ISAC: 1) Beamforming\nstrategies that rely solely on local channel information risk generating\nsignificant inter-cell interference (ICI), which degrades network performance\nfor both communication users and sensing receivers in neighboring cells; 2)\nconversely centralized beamforming strategies can mitigate ICI by leveraging\nglobal channel information, but they come with substantial transmission\noverhead and latency that can be prohibitive for latency-sensitive and\nsource-constrained applications. To tackle these challenges, we first propose a\npartially decentralized training framework motivated by the vertical federated\nlearning (VFL) paradigm. In this framework, the participating base stations\n(BSs) collaboratively design beamforming matrices under the guidance of a\ncentral server. The central server aggregates local information from the BSs\nand provides feedback, allowing BSs to implicitly manage ICI without accessing\nthe global channel information. To make the solution scalable for densely\ndeployed wireless networks, we take further steps to reduce communication\noverhead by presenting a fully decentralized design based on the horizontal\nfederated learning (HFL). Specifically, we develop a novel loss function to\ncontrol the interference leakage power, enabling a more efficient training\nprocess by entirely eliminating local channel information exchange. Numerical\nresults show that the proposed solutions can achieve significant performance\nimprovements comparable to the benchmarks in terms of both communication and\nradar information rates.",
        "Chimera states in systems of nonlocally coupled oscillators, i.e.,\nself-organized coexistence of coherent and incoherent oscillator populations,\nhave attracted much attention. In this study, we consider the effect of\nfrequency heterogeneities on the chimera state and reveal that it induces\nspatial locking of the chimera state, i.e., the coherent and incoherent domains\nalign with lower and higher frequency regions, respectively, in a self-adaptive\nmanner. Using an extended self-consistency approach, we show that such\nspatially locked chimera states can be reproduced as steady solutions of the\nsystem in the continuum limit. Furthermore, we develop a variational argument\nto explain the mechanism leading to spatial locking. Our analysis reveals how\nheterogeneity can affect the collective dynamics of the chimera states and\noffers insights into their control and applications.",
        "The Ni$^{12+}$ ion features an electronic transition with a natural width of\nonly 8 mHz, allowing for a highly stable optical clock. We predict that the\nenergy of this strongly forbidden $3s^2 3p^4\\, ^3\\!P_2 \\rightarrow 3s^2 3p^4 \\,\n^3\\!P_0$ electric quadrupole transition is 20081(10) cm$^{-1}$. For this, we\nuse both a hybrid approach combining configuration interaction (CI) with\ncoupled-cluster (CC) method and a pure CI calculation for the complete\n16-electron system, ensuring convergence. The resulting very small theoretical\nuncertainty of only 0.05\\% allowed us to find the transition experimentally in\na few hours, yielding an energy of 20078.984(10) cm$^{-1}$. This level of\nagreement for a 16-electron system is unprecedented and qualifies our method\nfor future calculations of many other complex atomic systems. While paving the\nway for a high-precision optical clock based on Ni$^{12+}$, our theory and code\ndevelopment will also enable better predictions for other highly charged ions\nand other complex atomic systems.",
        "We develop a perfect foresight method to solve models with an interest rate\nlower bound constraint that nests OccBin\/DynareOBC and \\cite{Eggertsson2010}'s\nas well as \\cite{Mertens2014}'s pen and paper solutions as special cases. Our\nmethod generalizes the pen-and-paper solutions by allowing for endogenous\npersistence while maintaining tractability and interpretability. We prove that\nour method necessarily gives stable multipliers. We use it to solve a New\nKeynesian model with habit formation and government spending, which we match to\nexpectations data from the Great Recession. We find an output multiplier of\ngovernment spending close to 1 for the US and Japan.",
        "Objective: This study explores a novel deep learning approach for EEG\nanalysis and perceptual state guidance, inspired by Level of Detail (LOD)\ntheory. The goal is to improve perceptual state identification accuracy and\nadvance personalized psychological therapy. Methods: Portable EEG devices and\nmusic rhythm signals were used for data collection. LOD theory was applied to\ndynamically adjust EEG signal processing, extracting core perceptual features.\nA Unity-based software system integrated EEG data with audio materials. The\ndeep learning model combined a CNN for feature extraction and classification,\nand a DQN for reinforcement learning to optimize rhythm adjustments. Results:\nThe CNN achieved 94.05% accuracy in perceptual state classification. The DQN\nguided subjects to target states with a 92.45% success rate, averaging 13.2\nrhythm cycles. However, only 50% of users reported psychological alignment with\nthe target state, indicating room for improvement. Discussion: The results\nvalidate the potential of LOD-based EEG biofeedback. Limitations include\ndataset source, label subjectivity, and reward function optimization. Future\nwork will expand to diverse subjects, incorporate varied musical elements, and\nrefine reward functions for better generalization and personalization.",
        "Training data quality is one of the most important drivers of final model\nquality. In this work, we introduce a method for evaluating data integrity\nbased on the assumption that low-quality input prompts result in high variance\nand low quality responses. This is achieved by measuring the rejected response\nquality and the reward gap between the chosen and rejected preference pair. Our\nmethod, Rejecting Instruction Preferences (RIP) can be used to filter prompts\nfrom existing training sets, or to make high quality synthetic datasets,\nyielding large performance gains across various benchmarks compared to\nunfiltered data. Using Llama 3.1-8B-Instruct, RIP improves AlpacaEval2 LC Win\nRate by 9.4%, Arena-Hard by 8.7%, and WildBench by 9.9%. Using Llama\n3.3-70B-Instruct, RIP improves Arena-Hard from 67.5 to 82.9, which is from 18th\nplace to 6th overall in the leaderboard."
      ]
    }
  },
  {
    "id":2411.00575,
    "research_type":"basic",
    "start_id":"b0",
    "start_title":"Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem",
    "start_abstract":"Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b22"
      ],
      "title":[
        "Vertical slice modelling of nonlinear Eady waves using a compatible finite element method"
      ],
      "abstract":[
        "A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "Decentralized RISE-based Control for Exponential Heterogeneous\n  Multi-Agent Target Tracking of Second-Order Nonlinear Systems",
        "On the Origin and Fate of Our Universe",
        "Trend-Aware Supervision: On Learning Invariance for Semi-Supervised\n  Facial Action Unit Intensity Estimation",
        "Data-Driven Decision Making for Enhancing Small-Signal Stability in\n  Hybrid AC\/DC Grids Through Converter Control Role Assignment",
        "Multi-Year-to-Decadal Temperature Prediction using a Machine Learning\n  Model-Analog Framework",
        "Decentralized Inference for Spatial Data Using Low-Rank Models",
        "Quadratic quasinormal modes at null infinity on a Schwarzschild\n  spacetime",
        "On the Adversarial Vulnerabilities of Transfer Learning in Remote\n  Sensing",
        "Stress field in the vicinity of a bubble\/sphere moving in a dilute\n  surfactant solution",
        "A Hybrid Swarm Intelligence Approach for Optimizing Multimodal Large\n  Language Models Deployment in Edge-Cloud-based Federated Learning\n  Environments",
        "Making Puzzle Pieces Fit or Reshaping MiMiC for Multiscale Simulations\n  with CP2K and More",
        "To Hedge or Not to Hedge: Optimal Strategies for Stochastic Trade Flow\n  Management",
        "Generalized Venn and Venn-Abers Calibration with Applications in\n  Conformal Prediction",
        "Keyword Search in the Deep Web",
        "Holistic Semantic Representation for Navigational Trajectory Generation",
        "Fast Two-photon Microscopy by Neuroimaging with Oblong Random\n  Acquisition (NORA)",
        "Diatomic and Polyatomic Heteronuclear Ultralong-Range Rydberg Molecules",
        "Probing Green's Function Zeros by Co-tunneling through Mott Insulators",
        "Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems",
        "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement\n  Learning",
        "Low RCS High-Gain Broadband Substrate Integrated Waveguide Antenna Based\n  on Elliptical Polarization Conversion Metasurface",
        "OverThink: Slowdown Attacks on Reasoning LLMs",
        "Scalable solution chemical synthesis and comprehensive analysis of\n  Bi2Te3 and Sb2Te3",
        "Model Selection for Off-policy Evaluation: New Algorithms and\n  Experimental Protocol",
        "Projecting Assumptions: The Duality Between Sparse Autoencoders and\n  Concept Geometry",
        "Homomorphic Encryption in Healthcare Industry Applications for\n  Protecting Data Privacy",
        "A space-resolved visible spectrometer system using compact endoscopic\n  optics for full vertical profile measurement of impurity line emissions in\n  superconducting EAST tokamak",
        "Small Binary Stabilizer Subsystem Codes",
        "A Corrugated All-Metal Vivaldi Antenna for 5G Phased Array Applications"
      ],
      "abstract":[
        "This work presents a decentralized implementation of a Robust Integral of the\nSign of the Error (RISE) controller for multi-agent target tracking problems\nwith exponential convergence guarantees. Previous RISE-based approaches for\nmulti-agent systems required 2-hop communication, limiting practical\napplicability. New insights from a Lyapunov-based design-analysis approach are\nused to eliminate the need for multi-hop communication required in previous\nliterature, while yielding exponential target tracking. The new insights\ninclude the development of a new P-function which is developed which works in\ntandem with the inclusion of the interaction matrix in the Lyapunov function.\nNonsmooth Lyapunov-based stability analysis methods are used to yield\nsemi-global exponential convergence to the target agent state despite the\npresence of bounded disturbances with bounded derivatives. The resulting\noutcome is a controller that achieves exponential target tracking with only\nlocal information exchange between neighboring agents.",
        "This brief review, intended for high energy and astrophysics researchers,\nexplores the implications of recent theoretical advances in string theory and\nthe Swampland program for understanding bounds on the structure of positive\npotentials allowed in quantum gravity. This has a bearing on both inflationary\nmodels for the early universe as well as the fate of our universe. The paper\nincludes a review of the dS conjecture as well as the TransPlanckian Censorship\nConjecture (TCC) and its relation to the species scale. We provide evidence for\nthese principles as well as what they may lead to in terms of phenomenological\npredictions. (Talk presented at Lemaitre Conference 2024)",
        "With the increasing need for facial behavior analysis, semi-supervised AU\nintensity estimation using only keyframe annotations has emerged as a practical\nand effective solution to relieve the burden of annotation. However, the lack\nof annotations makes the spurious correlation problem caused by AU\nco-occurrences and subject variation much more prominent, leading to non-robust\nintensity estimation that is entangled among AUs and biased among subjects. We\nobserve that trend information inherent in keyframe annotations could act as\nextra supervision and raising the awareness of AU-specific facial appearance\nchanging trends during training is the key to learning invariant AU-specific\nfeatures. To this end, we propose \\textbf{T}rend-\\textbf{A}ware\n\\textbf{S}upervision (TAS), which pursues three kinds of trend awareness,\nincluding intra-trend ranking awareness, intra-trend speed awareness, and\ninter-trend subject awareness. TAS alleviates the spurious correlation problem\nby raising trend awareness during training to learn AU-specific features that\nrepresent the corresponding facial appearance changes, to achieve intensity\nestimation invariance. Experiments conducted on two commonly used AU benchmark\ndatasets, BP4D and DISFA, show the effectiveness of each kind of awareness. And\nunder trend-aware supervision, the performance can be improved without extra\ncomputational or storage costs during inference.",
        "Hybrid AC\/DC transmission grids incorporate Modular Multilevel Converters\nfunctioning as Interconnecting Power Converters (IPCs). The control role\nassigned to each converter significantly influences grid dynamics.\nTraditionally, these converters operate with static control roles, but recent\nstudies have proposed scheduling their roles based on day-ahead forecasts to\nenhance stability performance. However, in systems with high renewable energy\npenetration, forecast deviations can render scheduled control assignments\nsuboptimal or even lead to instability. To address this challenge, this work\nproposes an online scheduling recalculation algorithm that dynamically adapts\nIPC control roles during system operation. The approach leverages a data-driven\nmulti-criteria decision-making framework, integrating surrogate models of\nconventional small-signal stability analysis tools to enable a fast computation\nof system stability and stability performance indicators.",
        "Multi-year-to-decadal climate prediction is a key tool in understanding the\nrange of potential regional and global climate futures. Here, we present a\nframework that combines machine learning and analog forecasting for predictions\non these timescales. A neural network is used to learn a mask, specific to a\nregion and lead time, with global weights based on relative importance as\nprecursors to the evolution of that prediction target. A library of\nmask-weighted model states, or potential analogs, are then compared to a single\nmask-weighted observational state. The known future of the best matching\npotential analogs serve as the prediction for the future of the observational\nstate. We match and predict 2-meter temperature using the Berkeley Earth\nSurface Temperature dataset for observations, and a set of CMIP6 models as the\nanalog library. We find improved performance over traditional analog methods\nand initialized decadal predictions.",
        "Advancements in information technology have enabled the creation of massive\nspatial datasets, driving the need for scalable and efficient computational\nmethodologies. While offering viable solutions, centralized frameworks are\nlimited by vulnerabilities such as single-point failures and communication\nbottlenecks. This paper presents a decentralized framework tailored for\nparameter inference in spatial low-rank models to address these challenges. A\nkey obstacle arises from the spatial dependence among observations, which\nprevents the log-likelihood from being expressed as a summation-a critical\nrequirement for decentralized optimization approaches. To overcome this\nchallenge, we propose a novel objective function leveraging the evidence lower\nbound, which facilitates the use of decentralized optimization techniques. Our\napproach employs a block descent method integrated with multi-consensus and\ndynamic consensus averaging for effective parameter optimization. We prove the\nconvexity of the new objective function in the vicinity of the true parameters,\nensuring the convergence of the proposed method. Additionally, we present the\nfirst theoretical results establishing the consistency and asymptotic normality\nof the estimator within the context of spatial low-rank models. Extensive\nsimulations and real-world data experiments corroborate these theoretical\nfindings, showcasing the robustness and scalability of the framework.",
        "The ringdown of perturbed black holes has been studied since the 1970s, but\nuntil recently, studies have focused on linear perturbations. There is now\nburgeoning interest in nonlinear perturbative effects during ringdown. Here,\nusing a hyperboloidal framework, we provide a complete treatment of linear and\nquadratic quasinormal modes (QNMs and QQNMs) in second-order perturbation\ntheory, in Schwarzschild spacetime. We include novel methods for extracting\nQNMs and QQNMs amplitudes using a Laplace transform treatment, allowing for the\ninclusion of arbitrary initial data. We produce both time- and frequency-domain\ncodes. From these codes, we present new results further exploring the\nunforeseen dependence of QQNMs amplitudes on the parity of the progenitor\nsystem, as demonstrated in our letter [Phys. Rev. Lett. 134, 061401 (2025)].\nOur numerical results are restricted to perturbations of a Schwarzschild black\nhole, but our methods extend straightforwardly to the astrophysically realistic\ncase of a Kerr black hole.",
        "The use of pretrained models from general computer vision tasks is widespread\nin remote sensing, significantly reducing training costs and improving\nperformance. However, this practice also introduces vulnerabilities to\ndownstream tasks, where publicly available pretrained models can be used as a\nproxy to compromise downstream models. This paper presents a novel Adversarial\nNeuron Manipulation method, which generates transferable perturbations by\nselectively manipulating single or multiple neurons in pretrained models.\nUnlike existing attacks, this method eliminates the need for domain-specific\ninformation, making it more broadly applicable and efficient. By targeting\nmultiple fragile neurons, the perturbations achieve superior attack\nperformance, revealing critical vulnerabilities in deep learning models.\nExperiments on diverse models and remote sensing datasets validate the\neffectiveness of the proposed method. This low-access adversarial neuron\nmanipulation technique highlights a significant security risk in transfer\nlearning models, emphasizing the urgent need for more robust defenses in their\ndesign when addressing the safety-critical remote sensing tasks.",
        "In this study, we experimentally investigate the stress field around a bubble\nrising in a dilute surfactant solution (20 < Re < 220, high Peclet numbers)\nwhose surface gradually becomes contaminated, and compare it with that around a\nsphere free from surface contamination. We employ a newly developed\npolarization measurement technique, highly sensitive to stress fields near\ninterfaces. First, we validate this method by measuring the flow around a solid\nsphere settling at Re = 120 and comparing results with numerical predictions,\nconfirming its accuracy. We then measure the stress field around a bubble whose\ndrag force transitions from that of a clean interface to that of a rigid\ninterface within the observation region. The stress near the bubble's front\nresembles that of a clean bubble, while the rear behaves like a solid sphere.\nBetween these regions, a discontinuous phase retardation near the cap angle\nindicates a transition from slip to no-slip boundary conditions. Axisymmetric\nstress reconstruction reveals localized stress spike at the cap angle, which\nshifts as surfactant accumulates and increases the drag. Remarkably, the\nmeasured cap angle versus normalized drag coefficient agrees well with\nnumerical simulations at Re = 100 (Cuenot et al. 1997) and shows only a slight\ndeviation from the creeping-flow stagnant cap model (Sadhal and Johnson 1983).\nThis work demonstrates that polarization-based stress field measurements\neffectively capture the interplay between surface contamination and\nhydrodynamics at intermediate Reynolds numbers.",
        "The combination of Federated Learning (FL), Multimodal Large Language Models\n(MLLMs), and edge-cloud computing enables distributed and real-time data\nprocessing while preserving privacy across edge devices and cloud\ninfrastructure. However, the deployment of MLLMs in FL environments with\nresource-constrained edge devices presents significant challenges, including\nresource management, communication overhead, and non-IID data. To address these\nchallenges, we propose a novel hybrid framework wherein MLLMs are deployed on\nedge devices equipped with sufficient resources and battery life, while the\nmajority of training occurs in the cloud. To identify suitable edge devices for\ndeployment, we employ Particle Swarm Optimization (PSO), and Ant Colony\nOptimization (ACO) is utilized to optimize the transmission of model updates\nbetween edge and cloud nodes. This proposed swarm intelligence-based framework\naims to enhance the efficiency of MLLM training by conducting extensive\ntraining in the cloud and fine-tuning at the edge, thereby reducing energy\nconsumption and communication costs. Our experimental results show that the\nproposed method significantly improves system performance, achieving an\naccuracy of 92%, reducing communication cost by 30%, and enhancing client\nparticipation compared to traditional FL methods. These results make the\nproposed approach highly suitable for large-scale edge-cloud computing systems.",
        "MiMiC is a framework for modeling large-scale chemical processes that require\ntreatment at multiple resolutions. It does not aim to implement single-handedly\nall methods required to treat individual subsystems, but instead, it relegates\nthis task to specialized computational chemistry software while it serves as an\nintermediary between these external programs, and computes the interactions\nbetween the subsystems. MiMiC minimizes issues typically associated with\nmolecular dynamics performed with multiple programs, by adopting a\nmultiple-program multiple-data paradigm combined with a loose-coupling model.\nIn this article, we present the addition of a new client program, CP2K, to the\nMiMiC ecosystem, which required a major refactoring of the entire framework and\nin the end allowed us to unlock its full flexibility. By thorough timing\nanalysis, we verify that the introduced changes do not affect the performance\nof MiMiC or CP2K, and neither are they a source of significant computational\noverheads that would be detrimental to simulation efficiency. Moreover, we\ndemonstrate the benefits of the framework's modular design, by performing a\nQM\/MM MD simulation combining CP2K with previously interfaced OpenMM, with no\nadditional implementation effort required.",
        "This paper addresses the trade-off between internalisation and\nexternalisation in the management of stochastic trade flows. We consider agents\nwho must absorb flows and manage risk by deciding whether to warehouse it or\nhedge in the market, thereby incurring transaction costs and market impact.\nUnlike market makers, these agents cannot skew their quotes to attract\noffsetting flows and deter risk-increasing ones, leading to a fundamentally\ndifferent problem. Within the Almgren-Chriss framework, we derive\nalmost-closed-form solutions in the case of quadratic execution costs, while\nmore general cases require numerical methods. In particular, we discuss the\nchallenges posed by artificial boundary conditions when using classical\ngrid-based numerical PDE techniques and propose reinforcement learning methods\nas an alternative.",
        "Ensuring model calibration is critical for reliable predictions, yet popular\ndistribution-free methods, such as histogram binning and isotonic regression,\nprovide only asymptotic guarantees. We introduce a unified framework for Venn\nand Venn-Abers calibration, generalizing Vovk's binary classification approach\nto arbitrary prediction tasks and loss functions. Venn calibration leverages\nbinning calibrators to construct prediction sets that contain at least one\nmarginally perfectly calibrated point prediction in finite samples, capturing\nepistemic uncertainty in the calibration process. The width of these sets\nshrinks asymptotically to zero, converging to a conditionally calibrated point\nprediction. Furthermore, we propose Venn multicalibration, a novel methodology\nfor finite-sample calibration across subpopulations. For quantile loss,\ngroup-conditional and multicalibrated conformal prediction arise as special\ncases of Venn multicalibration, and Venn calibration produces novel conformal\nprediction intervals that achieve quantile-conditional coverage. As a separate\ncontribution, we extend distribution-free conditional calibration guarantees of\nhistogram binning and isotonic calibration to general losses.",
        "The Deep Web is constituted by data that are accessible through Web pages,\nbut not readily indexable by search engines as they are returned in dynamic\npages. In this paper we propose a conceptual framework for answering keyword\nqueries on Deep Web sources represented as relational tables with so-called\naccess limitations. We formalize the notion of optimal answer, characterize\nqueries for which an answer can be found, and present a method for query\nprocessing based on the construction of a query plan that minimizes the\naccesses to the data sources.",
        "Trajectory generation has garnered significant attention from researchers in\nthe field of spatio-temporal analysis, as it can generate substantial\nsynthesized human mobility trajectories that enhance user privacy and alleviate\ndata scarcity. However, existing trajectory generation methods often focus on\nimproving trajectory generation quality from a singular perspective, lacking a\ncomprehensive semantic understanding across various scales. Consequently, we\nare inspired to develop a HOlistic SEmantic Representation (HOSER) framework\nfor navigational trajectory generation. Given an origin-and-destination (OD)\npair and the starting time point of a latent trajectory, we first propose a\nRoad Network Encoder to expand the receptive field of road- and zone-level\nsemantics. Second, we design a Multi-Granularity Trajectory Encoder to\nintegrate the spatio-temporal semantics of the generated trajectory at both the\npoint and trajectory levels. Finally, we employ a Destination-Oriented\nNavigator to seamlessly integrate destination-oriented guidance. Extensive\nexperiments on three real-world datasets demonstrate that HOSER outperforms\nstate-of-the-art baselines by a significant margin. Moreover, the model's\nperformance in few-shot learning and zero-shot learning scenarios further\nverifies the effectiveness of our holistic semantic representation.",
        "Advances in neural imaging have enabled neuroscience to study how the joint\nactivity of large neural populations conspire to produce perception, behavior\nand cognition. Despite many advances in optical methods, there exists a\nfundamental tradeoff between imaging speed, field of view, and resolution that\nlimits the scope of neural imaging, especially for the raster-scanning\nmulti-photon imaging needed for imaging deeper into the brain. One approach to\novercoming this trade-off is in computational imaging: the co-development of\noptics and algorithms where the optics are designed to encode the target images\ninto fewer measurements that are faster to acquire, and the algorithms\ncompensate by inverting the optical image coding process to recover a larger or\nhigher resolution image. We present here one such approach for raster-scanning\ntwo-photon imaging: Neuroimaging with Oblong Random Acquisition (NORA). NORA\nquickly acquires each frame in a microscopic video by subsampling only a\nfraction of the fast scanning lines, ignoring large portions of each frame.\nNORA mitigates the information loss by extending the point-spread function in\nthe slow-scan direction to integrate the fluorescence of neighboring lines into\na single set of measurements. By imaging different, randomly selected, lines at\neach frame, NORA diversifies the information collected across frames and\nenables video-level reconstruction. Rather than reconstruct the video\nframe-by-frame using image-level recovery, NORA recovers full video sequences\nthrough a nuclear-norm minimization (i.e., matrix completion) on the\npixels-by-time matrix. We simulated NORA imaging using the Neural Anatomy and\nOptical Microscopy (NAOMi) biophysical simulation suite. Using these\nsimulations we demonstrate that NORA imaging can accurately recover 400 um X\n400 um fields of view at subsampling rates up to 20X, despite realistic noise\nand motion conditions.",
        "Ultra-long-range Rydberg molecules (ULRMs) have attracted significant\ninterest due to their unique electronic properties and potential applications\nin quantum technologies. We theoretically investigate the formation and\ncharacteristics of heteronuclear ULRMs, focusing on Rb-Cs systems. We explore\nthe vibrational energy levels of heteronuclear nD ULRMs and compare them with\nhomonuclear counterparts. We also predict the formation of polyatomic\nheteronuclear ULRMs, discussing how the binding energy and spectral features\nevolve as the number of ground-state atoms increases. Our theoretical\npredictions are presented in terms of molecular spectra and provide insight\ninto the formation dynamics of these systems. The study further explores the\npotential applications of heteronuclear ULRMs in quantum information\nprocessing, quantum simulation, and precision measurements, offering new\navenues for future research in many-body physics and quantum technologies.",
        "Quantum tunneling experiments have provided deep insights into basic\nexcitations occurring as Green's function poles in the realm of complex quantum\nmatter. However, strongly correlated quantum materials also allow for Green's\nfunctions zeros (GFZ) that may be seen as an antidote to the familiar poles,\nand have so far largely eluded direct experimental study. Here, we propose and\ninvestigate theoretically how co-tunneling through Mott insulators enables\ndirect access to the shadow band structure of GFZ. In particular, we derive an\neffective Hamiltonian for the GFZ that is shown to govern the co-tunneling\namplitude and reveal fingerprints of many-body correlations clearly\ndistinguishing the GFZ structure from the underlying free Bloch band structure\nof the system. Our perturbative analytical results are corroborated by\nnumerical data both in the framework of exact diagonalization and matrix\nproduct state simulations for a one-dimensional model system consisting of a\nSu-Schrieffer-Heeger-Hubbard model coupled to two single level quantum dots.",
        "Recent research in language modeling reveals two scaling effects: the\nwell-known improvement from increased training compute, and a lesser-known\nboost from applying more sophisticated or computationally intensive inference\nmethods. Inspired by recent findings on the fractal geometry of language, we\nintroduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe\nfor scaling inference time. For a given fixed model architecture and training\ncompute budget, RINS substantially improves language modeling performance. It\nalso generalizes beyond pure language tasks, delivering gains in multimodal\nsystems, including a +2% improvement in 0-shot ImageNet accuracy for\nSigLIP-B\/16. Additionally, by deriving data scaling laws, we show that RINS\nimproves both the asymptotic performance limits and the scaling exponents.\nThese advantages are maintained even when compared to state-of-the-art\nrecursive techniques like the \"repeat-all-over\" (RAO) strategy in Mobile LLM.\nFinally, stochastic RINS not only can enhance performance further but also\nprovides the flexibility to optionally forgo increased inference computation at\ntest time with minimal performance degradation.",
        "Reasoning language models have shown an uncanny ability to improve\nperformance at test-time by ``thinking longer''-that is, by generating longer\nchain-of-thought sequences and hence using more compute. However, the length of\ntheir chain-of-thought reasoning is not controllable, making it impossible to\nallocate test-time compute to achieve a desired level of performance. We\nintroduce Length Controlled Policy Optimization (LCPO), a simple reinforcement\nlearning method that optimizes for accuracy and adherence to user-specified\nlength constraints. We use LCPO to train L1, a reasoning language model that\nproduces outputs satisfying a length constraint given in its prompt. L1's\nlength control allows for smoothly trading off computational cost and accuracy\non a wide range of tasks, and outperforms the state-of-the-art S1 method for\nlength control. Furthermore, we uncover an unexpected short chain-of-thought\ncapability in models trained with LCPO. For instance, our 1.5B L1 model\nsurpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise\ncontrol over reasoning length, allowing for fine-grained allocation of\ntest-time compute and accuracy. We release code and models at\nhttps:\/\/www.cmu-l3.github.io\/l1",
        "Designed an elliptical polarization conversion metasurface (PCM) for Ka-band\napplications, alongside a high-gain substrate integrated waveguide (SIW)\nantenna. The PCM elements are integrated into the antenna design in a\nchessboard array configuration, with the goal of achieving effective reduction\nin the antenna's radar cross section (RCS). Both the PCM elements and antenna\nstructure exhibit a simple design. The top layer of the metasurface (MS)\nelements employs an elliptical pattern symmetric along the diagonal, enabling\nefficient conversion of linearly polarized waves. The antenna component, on the\nother hand, consists of a broadband dipole antenna fed by SIW slot coupling.\nVerified through simulations, the polarization conversion bandwidth of this PCM\nunit reaches 80.38% where polarization conversion ratio (PCR) exceeds 90%\n(25.3-59.3GHz), demonstrating exceptional conversion performance. When the\ndipole antenna is combined with the PCM, its -10dB impedance bandwidth reaches\nto 15.09% (33.7-39.2GHz), with a maximum realized gain of 9.1dBi. Notably, the\nantenna loaded with the chessboard PCM structure effectively disperses the\nenergy of scattered echoes around, significantly reducing the concentration of\nscattered energy in the direction of the incident wave, thereby achieving an\neffective reduction in RCS.",
        "We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 18x slowdown on FreshQA dataset and 46x\nslowdown on SQuAD dataset. The attack also shows high transferability across\nmodels. To protect applications, we discuss and implement defenses leveraging\nLLM-based and system design approaches. Finally, we discuss societal,\nfinancial, and energy impacts of OVERTHINK attack which could amplify the costs\nfor third-party applications operating reasoning models.",
        "Thermoelectric (TE) materials can directly convert heat into electrical\nenergy. However, they sustain costly production procedures and batch-to-batch\nperformance variations. Therefore, developing scalable synthetic techniques for\nlarge-scale and reproducible quality TE materials is critical for advancing TE\ntechnology. This study developed a facile, high throughput, solution-chemical\nsynthetic technique. Microwave-assisted thermolysis process, providing\nenergy-efficient volumetric heating, was used for the synthesis of bismuth and\nantimony telluride (Bi2Te3, Sb2Te3). As-made materials were characterized using\nvarious techniques, including XRPD, SEM, TEM, XAS, and XPS. Detailed\ninvestigation of the local atomic structure of the synthesized Bi2Te3 and\nSb2Te3 powder samples was conducted through synchrotron radiation XAS\nexperiments. The sintered TE materials exhibited low thermal conductivity,\nachieving the highest TE figure-of-merit values of 0.7 (573 K) and 0.9 (523 K)\nfor n-type Bi2Te3 and p-type Sb2Te3, respectively, shifted significantly to the\nhigh-temperature region when compared to earlier reports, highlighting their\npotential for power generation applications. The scalable, energyand\ntime-efficient synthetic method developed, along with the demonstration of its\npotential for TE materials, opens the door for a wider application of these\nmaterials with minimal environmental impact.",
        "Holdout validation and hyperparameter tuning from data is a long-standing\nproblem in offline reinforcement learning (RL). A standard framework is to use\noff-policy evaluation (OPE) methods to evaluate and select the policies, but\nOPE either incurs exponential variance (e.g., importance sampling) or has\nhyperparameters on their own (e.g., FQE and model-based). In this work we focus\non hyperparameter tuning for OPE itself, which is even more under-investigated.\nConcretely, we select among candidate value functions (\"model-free\") or\ndynamics (\"model-based\") to best assess the performance of a target policy. Our\ncontributions are two fold. We develop: (1) new model-free and model-based\nselectors with theoretical guarantees, and (2) a new experimental protocol for\nempirically evaluating them. Compared to the model-free protocol in prior\nworks, our new protocol allows for more stable generation of candidate value\nfunctions, better control of misspecification, and evaluation of model-free and\nmodel-based methods alike. We exemplify the protocol on a Gym environment, and\nfind that our new model-free selector, LSTD-Tournament, demonstrates promising\nempirical performance.",
        "Sparse Autoencoders (SAEs) are widely used to interpret neural networks by\nidentifying meaningful concepts from their representations. However, do SAEs\ntruly uncover all concepts a model relies on, or are they inherently biased\ntoward certain kinds of concepts? We introduce a unified framework that recasts\nSAEs as solutions to a bilevel optimization problem, revealing a fundamental\nchallenge: each SAE imposes structural assumptions about how concepts are\nencoded in model representations, which in turn shapes what it can and cannot\ndetect. This means different SAEs are not interchangeable -- switching\narchitectures can expose entirely new concepts or obscure existing ones. To\nsystematically probe this effect, we evaluate SAEs across a spectrum of\nsettings: from controlled toy models that isolate key variables, to\nsemi-synthetic experiments on real model activations and finally to\nlarge-scale, naturalistic datasets. Across this progression, we examine two\nfundamental properties that real-world concepts often exhibit: heterogeneity in\nintrinsic dimensionality (some concepts are inherently low-dimensional, others\nare not) and nonlinear separability. We show that SAEs fail to recover concepts\nwhen these properties are ignored, and we design a new SAE that explicitly\nincorporates both, enabling the discovery of previously hidden concepts and\nreinforcing our theoretical insights. Our findings challenge the idea of a\nuniversal SAE and underscores the need for architecture-specific choices in\nmodel interpretability. Overall, we argue an SAE does not just reveal concepts\n-- it determines what can be seen at all.",
        "Focussing on two different use cases-Quality Control methods in industrial\ncontexts and Neural Network algorithms for healthcare diagnostics-this research\ninvestigates the inclusion of Fully Homomorphic Encryption into real-world\napplications in the healthcare sector. We evaluate the performance, resource\nrequirements, and viability of deploying FHE in these settings through\nextensive testing and analysis, highlighting the progress made in FHE tooling\nand the obstacles still facing addressing the gap between conceptual research\nand practical applications. We start our research by describing the specific\ncase study and trust model were working with. Choosing the two FHE frameworks\nmost appropriate for industry development, we assess the resources and\nperformance requirements for implementing each of the two FHE frameworks in the\nfirst scenario, Quality Control algorithms. In conclusion, our findings\ndemonstrate the effectiveness and resource consumption of the two use\ncases-complex NN models and simple QC algorithms-when implemented in an FHE\nsetting.",
        "In Experimental Advanced Superconducting Tokamak (EAST tokamak) with tungsten\ndivertors and molybdenum first wall, lithiumization and boronization have been\nfrequently carried out to improve the plasma performance, in particular, in\nlong pulse discharges. A study on impurity behaviors of lithium, boron and\ntungsten atoms\/ions in the edge plasma is then crucially important. For the\npurpose, a space-resolved visible spectrometer system has been newly developed\nto observe full vertical profiles over a length of 1.7m of impurity line\nemissions in wavelength range of 320-800nm. For the full vertical profile\nmeasurement compact endoscopic optics is employed with an optical fiber bundle\nfor the system, which can be inserted into a 1.5m long extension tube called\n'long nose', because the distance between the diagnostic port and plasma center\nis considerably long. Therefore, a quartz glass window mounted from the vacuum\nvessel side is designed to withstand the reverse pressure. A mechanical shutter\nis also designed to open at a large angle of 235 degree so that the viewing\nangle of nearby ports is not blocked. Two sets of the fiber bundle, 60-channel\nlinear array and 11*10 channel planar array , with a length of 30m are attached\nto two sets of Czerny-Turner visible spectrometers for one-dimensional (1D)\nvertical profile measurement of core plasma and two-dimensional (2D)\nspectroscopy of divertor plasma, respectively. A complementary metal oxide\nsemiconductor (CMOS) detector with 2048*2048 pixels is used for the visible\nspectrometers. A preliminary result on the full vertical profile is obtained\nfor BII line emission at 703.19nm in the 1D system",
        "We establish a database consisting of a representative of every binary\nquantum stabilizer code under local Clifford permutation equivalence for $n\\leq\n9$.",
        "In this paper, a corrugated Vivaldi phased array antenna in the 28 GHz\nfrequency band is proposed for 5G communication applications. The presented\nconfiguration features an all-metal antipodal antenna structure with a broad\nbandwidth ranging from 26 to 30 GHz and beam steering capabilities from -30 to\n+30 degrees. The proposed antenna consists of a 4x4 array configuration, where\neach element has dimensions of 6.46x6.46x14.25 mm, resulting in an overall\nantenna structure with dimensions of 25.84x25.84x14.25 mm. The corrugation\nmethod is applied to minimize surface currents, resulting in a reduction in\ninterelement mutual couplings. Therefore, the return loss in the array\nstructure for central elements is decreased, and the antenna gain and radiation\nefficiency are improved. Moreover, the improved radiation efficiency allows for\nhigher power transmission and reception from an antenna, resulting in\npotentially higher data rates and better performance."
      ]
    }
  },
  {
    "id":2411.00575,
    "research_type":"basic",
    "start_id":"b22",
    "start_title":"Vertical slice modelling of nonlinear Eady waves using a compatible finite element method",
    "start_abstract":"A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution.",
    "start_categories":[
      "math.MP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem"
      ],
      "abstract":[
        "Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Decentralized RISE-based Control for Exponential Heterogeneous\n  Multi-Agent Target Tracking of Second-Order Nonlinear Systems",
        "On the Origin and Fate of Our Universe",
        "Trend-Aware Supervision: On Learning Invariance for Semi-Supervised\n  Facial Action Unit Intensity Estimation",
        "Data-Driven Decision Making for Enhancing Small-Signal Stability in\n  Hybrid AC\/DC Grids Through Converter Control Role Assignment",
        "Multi-Year-to-Decadal Temperature Prediction using a Machine Learning\n  Model-Analog Framework",
        "Decentralized Inference for Spatial Data Using Low-Rank Models",
        "Quadratic quasinormal modes at null infinity on a Schwarzschild\n  spacetime",
        "On the Adversarial Vulnerabilities of Transfer Learning in Remote\n  Sensing",
        "Stress field in the vicinity of a bubble\/sphere moving in a dilute\n  surfactant solution",
        "A Hybrid Swarm Intelligence Approach for Optimizing Multimodal Large\n  Language Models Deployment in Edge-Cloud-based Federated Learning\n  Environments",
        "Making Puzzle Pieces Fit or Reshaping MiMiC for Multiscale Simulations\n  with CP2K and More",
        "To Hedge or Not to Hedge: Optimal Strategies for Stochastic Trade Flow\n  Management",
        "Generalized Venn and Venn-Abers Calibration with Applications in\n  Conformal Prediction",
        "Keyword Search in the Deep Web",
        "Holistic Semantic Representation for Navigational Trajectory Generation",
        "Fast Two-photon Microscopy by Neuroimaging with Oblong Random\n  Acquisition (NORA)",
        "Diatomic and Polyatomic Heteronuclear Ultralong-Range Rydberg Molecules",
        "Probing Green's Function Zeros by Co-tunneling through Mott Insulators",
        "Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems",
        "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement\n  Learning",
        "Low RCS High-Gain Broadband Substrate Integrated Waveguide Antenna Based\n  on Elliptical Polarization Conversion Metasurface",
        "OverThink: Slowdown Attacks on Reasoning LLMs",
        "Scalable solution chemical synthesis and comprehensive analysis of\n  Bi2Te3 and Sb2Te3",
        "Model Selection for Off-policy Evaluation: New Algorithms and\n  Experimental Protocol",
        "Projecting Assumptions: The Duality Between Sparse Autoencoders and\n  Concept Geometry",
        "Homomorphic Encryption in Healthcare Industry Applications for\n  Protecting Data Privacy",
        "A space-resolved visible spectrometer system using compact endoscopic\n  optics for full vertical profile measurement of impurity line emissions in\n  superconducting EAST tokamak",
        "Small Binary Stabilizer Subsystem Codes",
        "A Corrugated All-Metal Vivaldi Antenna for 5G Phased Array Applications"
      ],
      "abstract":[
        "This work presents a decentralized implementation of a Robust Integral of the\nSign of the Error (RISE) controller for multi-agent target tracking problems\nwith exponential convergence guarantees. Previous RISE-based approaches for\nmulti-agent systems required 2-hop communication, limiting practical\napplicability. New insights from a Lyapunov-based design-analysis approach are\nused to eliminate the need for multi-hop communication required in previous\nliterature, while yielding exponential target tracking. The new insights\ninclude the development of a new P-function which is developed which works in\ntandem with the inclusion of the interaction matrix in the Lyapunov function.\nNonsmooth Lyapunov-based stability analysis methods are used to yield\nsemi-global exponential convergence to the target agent state despite the\npresence of bounded disturbances with bounded derivatives. The resulting\noutcome is a controller that achieves exponential target tracking with only\nlocal information exchange between neighboring agents.",
        "This brief review, intended for high energy and astrophysics researchers,\nexplores the implications of recent theoretical advances in string theory and\nthe Swampland program for understanding bounds on the structure of positive\npotentials allowed in quantum gravity. This has a bearing on both inflationary\nmodels for the early universe as well as the fate of our universe. The paper\nincludes a review of the dS conjecture as well as the TransPlanckian Censorship\nConjecture (TCC) and its relation to the species scale. We provide evidence for\nthese principles as well as what they may lead to in terms of phenomenological\npredictions. (Talk presented at Lemaitre Conference 2024)",
        "With the increasing need for facial behavior analysis, semi-supervised AU\nintensity estimation using only keyframe annotations has emerged as a practical\nand effective solution to relieve the burden of annotation. However, the lack\nof annotations makes the spurious correlation problem caused by AU\nco-occurrences and subject variation much more prominent, leading to non-robust\nintensity estimation that is entangled among AUs and biased among subjects. We\nobserve that trend information inherent in keyframe annotations could act as\nextra supervision and raising the awareness of AU-specific facial appearance\nchanging trends during training is the key to learning invariant AU-specific\nfeatures. To this end, we propose \\textbf{T}rend-\\textbf{A}ware\n\\textbf{S}upervision (TAS), which pursues three kinds of trend awareness,\nincluding intra-trend ranking awareness, intra-trend speed awareness, and\ninter-trend subject awareness. TAS alleviates the spurious correlation problem\nby raising trend awareness during training to learn AU-specific features that\nrepresent the corresponding facial appearance changes, to achieve intensity\nestimation invariance. Experiments conducted on two commonly used AU benchmark\ndatasets, BP4D and DISFA, show the effectiveness of each kind of awareness. And\nunder trend-aware supervision, the performance can be improved without extra\ncomputational or storage costs during inference.",
        "Hybrid AC\/DC transmission grids incorporate Modular Multilevel Converters\nfunctioning as Interconnecting Power Converters (IPCs). The control role\nassigned to each converter significantly influences grid dynamics.\nTraditionally, these converters operate with static control roles, but recent\nstudies have proposed scheduling their roles based on day-ahead forecasts to\nenhance stability performance. However, in systems with high renewable energy\npenetration, forecast deviations can render scheduled control assignments\nsuboptimal or even lead to instability. To address this challenge, this work\nproposes an online scheduling recalculation algorithm that dynamically adapts\nIPC control roles during system operation. The approach leverages a data-driven\nmulti-criteria decision-making framework, integrating surrogate models of\nconventional small-signal stability analysis tools to enable a fast computation\nof system stability and stability performance indicators.",
        "Multi-year-to-decadal climate prediction is a key tool in understanding the\nrange of potential regional and global climate futures. Here, we present a\nframework that combines machine learning and analog forecasting for predictions\non these timescales. A neural network is used to learn a mask, specific to a\nregion and lead time, with global weights based on relative importance as\nprecursors to the evolution of that prediction target. A library of\nmask-weighted model states, or potential analogs, are then compared to a single\nmask-weighted observational state. The known future of the best matching\npotential analogs serve as the prediction for the future of the observational\nstate. We match and predict 2-meter temperature using the Berkeley Earth\nSurface Temperature dataset for observations, and a set of CMIP6 models as the\nanalog library. We find improved performance over traditional analog methods\nand initialized decadal predictions.",
        "Advancements in information technology have enabled the creation of massive\nspatial datasets, driving the need for scalable and efficient computational\nmethodologies. While offering viable solutions, centralized frameworks are\nlimited by vulnerabilities such as single-point failures and communication\nbottlenecks. This paper presents a decentralized framework tailored for\nparameter inference in spatial low-rank models to address these challenges. A\nkey obstacle arises from the spatial dependence among observations, which\nprevents the log-likelihood from being expressed as a summation-a critical\nrequirement for decentralized optimization approaches. To overcome this\nchallenge, we propose a novel objective function leveraging the evidence lower\nbound, which facilitates the use of decentralized optimization techniques. Our\napproach employs a block descent method integrated with multi-consensus and\ndynamic consensus averaging for effective parameter optimization. We prove the\nconvexity of the new objective function in the vicinity of the true parameters,\nensuring the convergence of the proposed method. Additionally, we present the\nfirst theoretical results establishing the consistency and asymptotic normality\nof the estimator within the context of spatial low-rank models. Extensive\nsimulations and real-world data experiments corroborate these theoretical\nfindings, showcasing the robustness and scalability of the framework.",
        "The ringdown of perturbed black holes has been studied since the 1970s, but\nuntil recently, studies have focused on linear perturbations. There is now\nburgeoning interest in nonlinear perturbative effects during ringdown. Here,\nusing a hyperboloidal framework, we provide a complete treatment of linear and\nquadratic quasinormal modes (QNMs and QQNMs) in second-order perturbation\ntheory, in Schwarzschild spacetime. We include novel methods for extracting\nQNMs and QQNMs amplitudes using a Laplace transform treatment, allowing for the\ninclusion of arbitrary initial data. We produce both time- and frequency-domain\ncodes. From these codes, we present new results further exploring the\nunforeseen dependence of QQNMs amplitudes on the parity of the progenitor\nsystem, as demonstrated in our letter [Phys. Rev. Lett. 134, 061401 (2025)].\nOur numerical results are restricted to perturbations of a Schwarzschild black\nhole, but our methods extend straightforwardly to the astrophysically realistic\ncase of a Kerr black hole.",
        "The use of pretrained models from general computer vision tasks is widespread\nin remote sensing, significantly reducing training costs and improving\nperformance. However, this practice also introduces vulnerabilities to\ndownstream tasks, where publicly available pretrained models can be used as a\nproxy to compromise downstream models. This paper presents a novel Adversarial\nNeuron Manipulation method, which generates transferable perturbations by\nselectively manipulating single or multiple neurons in pretrained models.\nUnlike existing attacks, this method eliminates the need for domain-specific\ninformation, making it more broadly applicable and efficient. By targeting\nmultiple fragile neurons, the perturbations achieve superior attack\nperformance, revealing critical vulnerabilities in deep learning models.\nExperiments on diverse models and remote sensing datasets validate the\neffectiveness of the proposed method. This low-access adversarial neuron\nmanipulation technique highlights a significant security risk in transfer\nlearning models, emphasizing the urgent need for more robust defenses in their\ndesign when addressing the safety-critical remote sensing tasks.",
        "In this study, we experimentally investigate the stress field around a bubble\nrising in a dilute surfactant solution (20 < Re < 220, high Peclet numbers)\nwhose surface gradually becomes contaminated, and compare it with that around a\nsphere free from surface contamination. We employ a newly developed\npolarization measurement technique, highly sensitive to stress fields near\ninterfaces. First, we validate this method by measuring the flow around a solid\nsphere settling at Re = 120 and comparing results with numerical predictions,\nconfirming its accuracy. We then measure the stress field around a bubble whose\ndrag force transitions from that of a clean interface to that of a rigid\ninterface within the observation region. The stress near the bubble's front\nresembles that of a clean bubble, while the rear behaves like a solid sphere.\nBetween these regions, a discontinuous phase retardation near the cap angle\nindicates a transition from slip to no-slip boundary conditions. Axisymmetric\nstress reconstruction reveals localized stress spike at the cap angle, which\nshifts as surfactant accumulates and increases the drag. Remarkably, the\nmeasured cap angle versus normalized drag coefficient agrees well with\nnumerical simulations at Re = 100 (Cuenot et al. 1997) and shows only a slight\ndeviation from the creeping-flow stagnant cap model (Sadhal and Johnson 1983).\nThis work demonstrates that polarization-based stress field measurements\neffectively capture the interplay between surface contamination and\nhydrodynamics at intermediate Reynolds numbers.",
        "The combination of Federated Learning (FL), Multimodal Large Language Models\n(MLLMs), and edge-cloud computing enables distributed and real-time data\nprocessing while preserving privacy across edge devices and cloud\ninfrastructure. However, the deployment of MLLMs in FL environments with\nresource-constrained edge devices presents significant challenges, including\nresource management, communication overhead, and non-IID data. To address these\nchallenges, we propose a novel hybrid framework wherein MLLMs are deployed on\nedge devices equipped with sufficient resources and battery life, while the\nmajority of training occurs in the cloud. To identify suitable edge devices for\ndeployment, we employ Particle Swarm Optimization (PSO), and Ant Colony\nOptimization (ACO) is utilized to optimize the transmission of model updates\nbetween edge and cloud nodes. This proposed swarm intelligence-based framework\naims to enhance the efficiency of MLLM training by conducting extensive\ntraining in the cloud and fine-tuning at the edge, thereby reducing energy\nconsumption and communication costs. Our experimental results show that the\nproposed method significantly improves system performance, achieving an\naccuracy of 92%, reducing communication cost by 30%, and enhancing client\nparticipation compared to traditional FL methods. These results make the\nproposed approach highly suitable for large-scale edge-cloud computing systems.",
        "MiMiC is a framework for modeling large-scale chemical processes that require\ntreatment at multiple resolutions. It does not aim to implement single-handedly\nall methods required to treat individual subsystems, but instead, it relegates\nthis task to specialized computational chemistry software while it serves as an\nintermediary between these external programs, and computes the interactions\nbetween the subsystems. MiMiC minimizes issues typically associated with\nmolecular dynamics performed with multiple programs, by adopting a\nmultiple-program multiple-data paradigm combined with a loose-coupling model.\nIn this article, we present the addition of a new client program, CP2K, to the\nMiMiC ecosystem, which required a major refactoring of the entire framework and\nin the end allowed us to unlock its full flexibility. By thorough timing\nanalysis, we verify that the introduced changes do not affect the performance\nof MiMiC or CP2K, and neither are they a source of significant computational\noverheads that would be detrimental to simulation efficiency. Moreover, we\ndemonstrate the benefits of the framework's modular design, by performing a\nQM\/MM MD simulation combining CP2K with previously interfaced OpenMM, with no\nadditional implementation effort required.",
        "This paper addresses the trade-off between internalisation and\nexternalisation in the management of stochastic trade flows. We consider agents\nwho must absorb flows and manage risk by deciding whether to warehouse it or\nhedge in the market, thereby incurring transaction costs and market impact.\nUnlike market makers, these agents cannot skew their quotes to attract\noffsetting flows and deter risk-increasing ones, leading to a fundamentally\ndifferent problem. Within the Almgren-Chriss framework, we derive\nalmost-closed-form solutions in the case of quadratic execution costs, while\nmore general cases require numerical methods. In particular, we discuss the\nchallenges posed by artificial boundary conditions when using classical\ngrid-based numerical PDE techniques and propose reinforcement learning methods\nas an alternative.",
        "Ensuring model calibration is critical for reliable predictions, yet popular\ndistribution-free methods, such as histogram binning and isotonic regression,\nprovide only asymptotic guarantees. We introduce a unified framework for Venn\nand Venn-Abers calibration, generalizing Vovk's binary classification approach\nto arbitrary prediction tasks and loss functions. Venn calibration leverages\nbinning calibrators to construct prediction sets that contain at least one\nmarginally perfectly calibrated point prediction in finite samples, capturing\nepistemic uncertainty in the calibration process. The width of these sets\nshrinks asymptotically to zero, converging to a conditionally calibrated point\nprediction. Furthermore, we propose Venn multicalibration, a novel methodology\nfor finite-sample calibration across subpopulations. For quantile loss,\ngroup-conditional and multicalibrated conformal prediction arise as special\ncases of Venn multicalibration, and Venn calibration produces novel conformal\nprediction intervals that achieve quantile-conditional coverage. As a separate\ncontribution, we extend distribution-free conditional calibration guarantees of\nhistogram binning and isotonic calibration to general losses.",
        "The Deep Web is constituted by data that are accessible through Web pages,\nbut not readily indexable by search engines as they are returned in dynamic\npages. In this paper we propose a conceptual framework for answering keyword\nqueries on Deep Web sources represented as relational tables with so-called\naccess limitations. We formalize the notion of optimal answer, characterize\nqueries for which an answer can be found, and present a method for query\nprocessing based on the construction of a query plan that minimizes the\naccesses to the data sources.",
        "Trajectory generation has garnered significant attention from researchers in\nthe field of spatio-temporal analysis, as it can generate substantial\nsynthesized human mobility trajectories that enhance user privacy and alleviate\ndata scarcity. However, existing trajectory generation methods often focus on\nimproving trajectory generation quality from a singular perspective, lacking a\ncomprehensive semantic understanding across various scales. Consequently, we\nare inspired to develop a HOlistic SEmantic Representation (HOSER) framework\nfor navigational trajectory generation. Given an origin-and-destination (OD)\npair and the starting time point of a latent trajectory, we first propose a\nRoad Network Encoder to expand the receptive field of road- and zone-level\nsemantics. Second, we design a Multi-Granularity Trajectory Encoder to\nintegrate the spatio-temporal semantics of the generated trajectory at both the\npoint and trajectory levels. Finally, we employ a Destination-Oriented\nNavigator to seamlessly integrate destination-oriented guidance. Extensive\nexperiments on three real-world datasets demonstrate that HOSER outperforms\nstate-of-the-art baselines by a significant margin. Moreover, the model's\nperformance in few-shot learning and zero-shot learning scenarios further\nverifies the effectiveness of our holistic semantic representation.",
        "Advances in neural imaging have enabled neuroscience to study how the joint\nactivity of large neural populations conspire to produce perception, behavior\nand cognition. Despite many advances in optical methods, there exists a\nfundamental tradeoff between imaging speed, field of view, and resolution that\nlimits the scope of neural imaging, especially for the raster-scanning\nmulti-photon imaging needed for imaging deeper into the brain. One approach to\novercoming this trade-off is in computational imaging: the co-development of\noptics and algorithms where the optics are designed to encode the target images\ninto fewer measurements that are faster to acquire, and the algorithms\ncompensate by inverting the optical image coding process to recover a larger or\nhigher resolution image. We present here one such approach for raster-scanning\ntwo-photon imaging: Neuroimaging with Oblong Random Acquisition (NORA). NORA\nquickly acquires each frame in a microscopic video by subsampling only a\nfraction of the fast scanning lines, ignoring large portions of each frame.\nNORA mitigates the information loss by extending the point-spread function in\nthe slow-scan direction to integrate the fluorescence of neighboring lines into\na single set of measurements. By imaging different, randomly selected, lines at\neach frame, NORA diversifies the information collected across frames and\nenables video-level reconstruction. Rather than reconstruct the video\nframe-by-frame using image-level recovery, NORA recovers full video sequences\nthrough a nuclear-norm minimization (i.e., matrix completion) on the\npixels-by-time matrix. We simulated NORA imaging using the Neural Anatomy and\nOptical Microscopy (NAOMi) biophysical simulation suite. Using these\nsimulations we demonstrate that NORA imaging can accurately recover 400 um X\n400 um fields of view at subsampling rates up to 20X, despite realistic noise\nand motion conditions.",
        "Ultra-long-range Rydberg molecules (ULRMs) have attracted significant\ninterest due to their unique electronic properties and potential applications\nin quantum technologies. We theoretically investigate the formation and\ncharacteristics of heteronuclear ULRMs, focusing on Rb-Cs systems. We explore\nthe vibrational energy levels of heteronuclear nD ULRMs and compare them with\nhomonuclear counterparts. We also predict the formation of polyatomic\nheteronuclear ULRMs, discussing how the binding energy and spectral features\nevolve as the number of ground-state atoms increases. Our theoretical\npredictions are presented in terms of molecular spectra and provide insight\ninto the formation dynamics of these systems. The study further explores the\npotential applications of heteronuclear ULRMs in quantum information\nprocessing, quantum simulation, and precision measurements, offering new\navenues for future research in many-body physics and quantum technologies.",
        "Quantum tunneling experiments have provided deep insights into basic\nexcitations occurring as Green's function poles in the realm of complex quantum\nmatter. However, strongly correlated quantum materials also allow for Green's\nfunctions zeros (GFZ) that may be seen as an antidote to the familiar poles,\nand have so far largely eluded direct experimental study. Here, we propose and\ninvestigate theoretically how co-tunneling through Mott insulators enables\ndirect access to the shadow band structure of GFZ. In particular, we derive an\neffective Hamiltonian for the GFZ that is shown to govern the co-tunneling\namplitude and reveal fingerprints of many-body correlations clearly\ndistinguishing the GFZ structure from the underlying free Bloch band structure\nof the system. Our perturbative analytical results are corroborated by\nnumerical data both in the framework of exact diagonalization and matrix\nproduct state simulations for a one-dimensional model system consisting of a\nSu-Schrieffer-Heeger-Hubbard model coupled to two single level quantum dots.",
        "Recent research in language modeling reveals two scaling effects: the\nwell-known improvement from increased training compute, and a lesser-known\nboost from applying more sophisticated or computationally intensive inference\nmethods. Inspired by recent findings on the fractal geometry of language, we\nintroduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe\nfor scaling inference time. For a given fixed model architecture and training\ncompute budget, RINS substantially improves language modeling performance. It\nalso generalizes beyond pure language tasks, delivering gains in multimodal\nsystems, including a +2% improvement in 0-shot ImageNet accuracy for\nSigLIP-B\/16. Additionally, by deriving data scaling laws, we show that RINS\nimproves both the asymptotic performance limits and the scaling exponents.\nThese advantages are maintained even when compared to state-of-the-art\nrecursive techniques like the \"repeat-all-over\" (RAO) strategy in Mobile LLM.\nFinally, stochastic RINS not only can enhance performance further but also\nprovides the flexibility to optionally forgo increased inference computation at\ntest time with minimal performance degradation.",
        "Reasoning language models have shown an uncanny ability to improve\nperformance at test-time by ``thinking longer''-that is, by generating longer\nchain-of-thought sequences and hence using more compute. However, the length of\ntheir chain-of-thought reasoning is not controllable, making it impossible to\nallocate test-time compute to achieve a desired level of performance. We\nintroduce Length Controlled Policy Optimization (LCPO), a simple reinforcement\nlearning method that optimizes for accuracy and adherence to user-specified\nlength constraints. We use LCPO to train L1, a reasoning language model that\nproduces outputs satisfying a length constraint given in its prompt. L1's\nlength control allows for smoothly trading off computational cost and accuracy\non a wide range of tasks, and outperforms the state-of-the-art S1 method for\nlength control. Furthermore, we uncover an unexpected short chain-of-thought\ncapability in models trained with LCPO. For instance, our 1.5B L1 model\nsurpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise\ncontrol over reasoning length, allowing for fine-grained allocation of\ntest-time compute and accuracy. We release code and models at\nhttps:\/\/www.cmu-l3.github.io\/l1",
        "Designed an elliptical polarization conversion metasurface (PCM) for Ka-band\napplications, alongside a high-gain substrate integrated waveguide (SIW)\nantenna. The PCM elements are integrated into the antenna design in a\nchessboard array configuration, with the goal of achieving effective reduction\nin the antenna's radar cross section (RCS). Both the PCM elements and antenna\nstructure exhibit a simple design. The top layer of the metasurface (MS)\nelements employs an elliptical pattern symmetric along the diagonal, enabling\nefficient conversion of linearly polarized waves. The antenna component, on the\nother hand, consists of a broadband dipole antenna fed by SIW slot coupling.\nVerified through simulations, the polarization conversion bandwidth of this PCM\nunit reaches 80.38% where polarization conversion ratio (PCR) exceeds 90%\n(25.3-59.3GHz), demonstrating exceptional conversion performance. When the\ndipole antenna is combined with the PCM, its -10dB impedance bandwidth reaches\nto 15.09% (33.7-39.2GHz), with a maximum realized gain of 9.1dBi. Notably, the\nantenna loaded with the chessboard PCM structure effectively disperses the\nenergy of scattered echoes around, significantly reducing the concentration of\nscattered energy in the direction of the incident wave, thereby achieving an\neffective reduction in RCS.",
        "We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 18x slowdown on FreshQA dataset and 46x\nslowdown on SQuAD dataset. The attack also shows high transferability across\nmodels. To protect applications, we discuss and implement defenses leveraging\nLLM-based and system design approaches. Finally, we discuss societal,\nfinancial, and energy impacts of OVERTHINK attack which could amplify the costs\nfor third-party applications operating reasoning models.",
        "Thermoelectric (TE) materials can directly convert heat into electrical\nenergy. However, they sustain costly production procedures and batch-to-batch\nperformance variations. Therefore, developing scalable synthetic techniques for\nlarge-scale and reproducible quality TE materials is critical for advancing TE\ntechnology. This study developed a facile, high throughput, solution-chemical\nsynthetic technique. Microwave-assisted thermolysis process, providing\nenergy-efficient volumetric heating, was used for the synthesis of bismuth and\nantimony telluride (Bi2Te3, Sb2Te3). As-made materials were characterized using\nvarious techniques, including XRPD, SEM, TEM, XAS, and XPS. Detailed\ninvestigation of the local atomic structure of the synthesized Bi2Te3 and\nSb2Te3 powder samples was conducted through synchrotron radiation XAS\nexperiments. The sintered TE materials exhibited low thermal conductivity,\nachieving the highest TE figure-of-merit values of 0.7 (573 K) and 0.9 (523 K)\nfor n-type Bi2Te3 and p-type Sb2Te3, respectively, shifted significantly to the\nhigh-temperature region when compared to earlier reports, highlighting their\npotential for power generation applications. The scalable, energyand\ntime-efficient synthetic method developed, along with the demonstration of its\npotential for TE materials, opens the door for a wider application of these\nmaterials with minimal environmental impact.",
        "Holdout validation and hyperparameter tuning from data is a long-standing\nproblem in offline reinforcement learning (RL). A standard framework is to use\noff-policy evaluation (OPE) methods to evaluate and select the policies, but\nOPE either incurs exponential variance (e.g., importance sampling) or has\nhyperparameters on their own (e.g., FQE and model-based). In this work we focus\non hyperparameter tuning for OPE itself, which is even more under-investigated.\nConcretely, we select among candidate value functions (\"model-free\") or\ndynamics (\"model-based\") to best assess the performance of a target policy. Our\ncontributions are two fold. We develop: (1) new model-free and model-based\nselectors with theoretical guarantees, and (2) a new experimental protocol for\nempirically evaluating them. Compared to the model-free protocol in prior\nworks, our new protocol allows for more stable generation of candidate value\nfunctions, better control of misspecification, and evaluation of model-free and\nmodel-based methods alike. We exemplify the protocol on a Gym environment, and\nfind that our new model-free selector, LSTD-Tournament, demonstrates promising\nempirical performance.",
        "Sparse Autoencoders (SAEs) are widely used to interpret neural networks by\nidentifying meaningful concepts from their representations. However, do SAEs\ntruly uncover all concepts a model relies on, or are they inherently biased\ntoward certain kinds of concepts? We introduce a unified framework that recasts\nSAEs as solutions to a bilevel optimization problem, revealing a fundamental\nchallenge: each SAE imposes structural assumptions about how concepts are\nencoded in model representations, which in turn shapes what it can and cannot\ndetect. This means different SAEs are not interchangeable -- switching\narchitectures can expose entirely new concepts or obscure existing ones. To\nsystematically probe this effect, we evaluate SAEs across a spectrum of\nsettings: from controlled toy models that isolate key variables, to\nsemi-synthetic experiments on real model activations and finally to\nlarge-scale, naturalistic datasets. Across this progression, we examine two\nfundamental properties that real-world concepts often exhibit: heterogeneity in\nintrinsic dimensionality (some concepts are inherently low-dimensional, others\nare not) and nonlinear separability. We show that SAEs fail to recover concepts\nwhen these properties are ignored, and we design a new SAE that explicitly\nincorporates both, enabling the discovery of previously hidden concepts and\nreinforcing our theoretical insights. Our findings challenge the idea of a\nuniversal SAE and underscores the need for architecture-specific choices in\nmodel interpretability. Overall, we argue an SAE does not just reveal concepts\n-- it determines what can be seen at all.",
        "Focussing on two different use cases-Quality Control methods in industrial\ncontexts and Neural Network algorithms for healthcare diagnostics-this research\ninvestigates the inclusion of Fully Homomorphic Encryption into real-world\napplications in the healthcare sector. We evaluate the performance, resource\nrequirements, and viability of deploying FHE in these settings through\nextensive testing and analysis, highlighting the progress made in FHE tooling\nand the obstacles still facing addressing the gap between conceptual research\nand practical applications. We start our research by describing the specific\ncase study and trust model were working with. Choosing the two FHE frameworks\nmost appropriate for industry development, we assess the resources and\nperformance requirements for implementing each of the two FHE frameworks in the\nfirst scenario, Quality Control algorithms. In conclusion, our findings\ndemonstrate the effectiveness and resource consumption of the two use\ncases-complex NN models and simple QC algorithms-when implemented in an FHE\nsetting.",
        "In Experimental Advanced Superconducting Tokamak (EAST tokamak) with tungsten\ndivertors and molybdenum first wall, lithiumization and boronization have been\nfrequently carried out to improve the plasma performance, in particular, in\nlong pulse discharges. A study on impurity behaviors of lithium, boron and\ntungsten atoms\/ions in the edge plasma is then crucially important. For the\npurpose, a space-resolved visible spectrometer system has been newly developed\nto observe full vertical profiles over a length of 1.7m of impurity line\nemissions in wavelength range of 320-800nm. For the full vertical profile\nmeasurement compact endoscopic optics is employed with an optical fiber bundle\nfor the system, which can be inserted into a 1.5m long extension tube called\n'long nose', because the distance between the diagnostic port and plasma center\nis considerably long. Therefore, a quartz glass window mounted from the vacuum\nvessel side is designed to withstand the reverse pressure. A mechanical shutter\nis also designed to open at a large angle of 235 degree so that the viewing\nangle of nearby ports is not blocked. Two sets of the fiber bundle, 60-channel\nlinear array and 11*10 channel planar array , with a length of 30m are attached\nto two sets of Czerny-Turner visible spectrometers for one-dimensional (1D)\nvertical profile measurement of core plasma and two-dimensional (2D)\nspectroscopy of divertor plasma, respectively. A complementary metal oxide\nsemiconductor (CMOS) detector with 2048*2048 pixels is used for the visible\nspectrometers. A preliminary result on the full vertical profile is obtained\nfor BII line emission at 703.19nm in the 1D system",
        "We establish a database consisting of a representative of every binary\nquantum stabilizer code under local Clifford permutation equivalence for $n\\leq\n9$.",
        "In this paper, a corrugated Vivaldi phased array antenna in the 28 GHz\nfrequency band is proposed for 5G communication applications. The presented\nconfiguration features an all-metal antipodal antenna structure with a broad\nbandwidth ranging from 26 to 30 GHz and beam steering capabilities from -30 to\n+30 degrees. The proposed antenna consists of a 4x4 array configuration, where\neach element has dimensions of 6.46x6.46x14.25 mm, resulting in an overall\nantenna structure with dimensions of 25.84x25.84x14.25 mm. The corrugation\nmethod is applied to minimize surface currents, resulting in a reduction in\ninterelement mutual couplings. Therefore, the return loss in the array\nstructure for central elements is decreased, and the antenna gain and radiation\nefficiency are improved. Moreover, the improved radiation efficiency allows for\nhigher power transmission and reception from an antenna, resulting in\npotentially higher data rates and better performance."
      ]
    }
  },
  {
    "id":2411.00578,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association",
    "start_abstract":"Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "Voxel Scene Graph for Intracranial Hemorrhage"
      ],
      "abstract":[
        "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference",
        "Mobile Application Threats and Security",
        "Water Flow Detection Device Based on Sound Data Analysis and Machine\n  Learning to Detect Water Leakage",
        "Recent open heavy flavor studies for the Electron-Ion Collider",
        "A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention\n  Logit Interpolation (GALI)",
        "ReasonGraph: Visualisation of Reasoning Paths",
        "Foundations of block-parallel automata networks",
        "Enhancing Quantum-ready QUBO-based Suppression for Object Detection with\n  Appearance and Confidence Features",
        "Uncertainty-Aware Decoding with Minimum Bayes Risk",
        "StructVPR++: Distill Structural and Semantic Knowledge with Weighting\n  Samples for Visual Place Recognition",
        "Infinitely many solutions for elliptic system with Hamiltonian type",
        "Stochastic resonance in Schmitt trigger and its application towards weak\n  signal detection",
        "mmDEAR: mmWave Point Cloud Density Enhancement for Accurate Human Body\n  Reconstruction",
        "Dubrovin duality and mirror symmetry for ADE resolutions",
        "Geometric deformations of cuspidal $S_1$ singularities",
        "The thermal index of neutron-star matter in the virial approximation",
        "T CrA has a companion: First direct detection of T CrA B with\n  VLTI\/MATISSE",
        "MARS: Mesh AutoRegressive Model for 3D Shape Detailization",
        "Strengthening the Internal Adversarial Robustness in Lifted Neural\n  Networks",
        "Exchange Rate Sensitivity in Free Zone Trade: An Empirical Study of the\n  Istanbul Ataturk Airport Free Zone",
        "Mean and Variance Estimation Complexity in Arbitrary Distributions via\n  Wasserstein Minimization",
        "A dataset-free approach for self-supervised learning of 3D reflectional\n  symmetries",
        "Label-Efficient LiDAR Panoptic Segmentation",
        "ALLVB: All-in-One Long Video Understanding Benchmark",
        "Optimal power procurement for green cellular wireless networks under\n  uncertainty and chance constraints",
        "Efficient dynamic modal load reconstruction using physics-informed\n  Gaussian processes based on frequency-sparse Fourier basis functions",
        "Flexible Bayesian Tensor Decomposition for Verbal Autopsy Data",
        "Perceptual Visual Quality Assessment: Principles, Methods, and Future\n  Directions",
        "Analyzing public sentiment to gauge key stock events and determine\n  volatility in conjunction with time and options premiums"
      ],
      "abstract":[
        "Large language models (LLMs) achieve impressive performance by scaling model\nparameters, but this comes with significant inference overhead. Feed-forward\nnetworks (FFNs), which dominate LLM parameters, exhibit high activation\nsparsity in hidden neurons. To exploit this, researchers have proposed using a\nmixture-of-experts (MoE) architecture, where only a subset of parameters is\nactivated. However, existing approaches often require extensive training data\nand resources, limiting their practicality. We propose CMoE (Carved MoE), a\nnovel framework to efficiently carve MoE models from dense models. CMoE\nachieves remarkable performance through efficient expert grouping and\nlightweight adaptation. First, neurons are grouped into shared and routed\nexperts based on activation rates. Next, we construct a routing mechanism\nwithout training from scratch, incorporating a differentiable routing process\nand load balancing. Using modest data, CMoE produces a well-designed, usable\nMoE from a 7B dense model within five minutes. With lightweight fine-tuning, it\nachieves high-performance recovery in under an hour. We make our code publicly\navailable at https:\/\/github.com\/JarvisPei\/CMoE.",
        "The movement to mobile computing solutions provides flexibility to different\nusers whether it is a business user, a student, or even providing entertainment\nto children and adults of all ages. Due to these emerging technologies mobile\nusers are unable to safeguard private information in a very effective way and\ncybercrimes are increasing day by day. This manuscript will focus on security\nvulnerabilities in the mobile computing industry, especially focusing on\ntablets and smart phones. This study will dive into current security threats\nfor the Android & Apple iOS market, exposing security risks and threats that\nthe novice or average user may not be aware of. The purpose of this study is to\nanalyze current security risks and threats, and provide solutions that may be\ndeployed to protect against such threats.",
        "In this paper, we introduce a novel mechanism that uses machine learning\ntechniques to detect water leaks in pipes. The proposed simple and low-cost\nmechanism is designed that can be easily installed on building pipes with\nvarious sizes. The system works based on gathering and amplifying water flow\nsignals using a mechanical sound amplifier. Then sounds are recorded and\nconverted to digital signals in order to be analyzed. After feature extraction\nand selection, deep neural networks are used to discriminate between with and\nwithout leak pipes. The experimental results show that this device can detect\nat least 100 milliliters per minute (mL\/min) of water flow in a pipe so that it\ncan be used as a core of a water leakage detection system.",
        "The future Electron-Ion Collider (EIC) will operate a series of\nhigh-luminosity high-energy electron+proton ($e+p$) and electron+nucleus\n($\\textit{e + A}$) collisions to study several fundamental questions in the\nhigh energy and nuclear physics field. Heavy flavor hadron and jet production\nat the EIC plays an important role in exploring both potential modification on\nthe initial-state nuclear parton distribution functions (nPDFs) and final-state\nparton propagation and hadronization processes under different nuclear medium\nconditions. The current design of the EIC ePIC detector has good performance of\nvertex and track reconstruction, particle identification and energy\ndetermination in the pseudorapidity region of $-3.5<\\eta<3.5$, which will\nenable a series of high precision heavy flavor hadron and jet measurements.\nLatest simulation studies of the projected nuclear modification factor $R_{eA}$\nof heavy flavor jets and heavy flavor hadron inside jets in $e+p$ and\n$\\textit{e + Au}$ collisions at $\\sqrt{s} =$ 28.6 GeV and 63.2 GeV as well as\nthe projected statistical accuracy of inclusive and differential charm baryon\nover meson ratio measurements in $e+p$ collisions will be presented. The\nimpacts of these proposed EIC measurements on constraining the heavy quark\npropagation properties in cold nuclear medium and exploring the heavy quark\nhadronization process will be discussed.",
        "Transformer-based Large Language Models (LLMs) struggle to process inputs\nexceeding their training context window, with performance degrading due to\npositional out-of-distribution (O.O.D.) that disrupt attention computations.\nExisting solutions, fine-tuning and training-free methods, are limited by\ncomputational inefficiency, attention logit outliers or loss of local\npositional information. To address this, we propose Greedy Attention Logit\nInterpolation (GALI), a training-free length extrapolation method that\nmaximizes the utilization of pretrained positional intervals while avoiding\nattention logit outliers through attention logit interpolation. The result\ndemonstrates that GALI consistently outperforms state-of-the-art training-free\nmethods. Our findings reveal that LLMs interpret positional intervals unevenly\nwithin their training context window, suggesting that extrapolating within a\nsmaller positional interval range yields superior results-even for\nshort-context tasks. GALI represents a significant step toward resolving the\npositional O.O.D. challenge, enabling more reliable long-text understanding in\nLLMs. Our implementation of GALI, along with the experiments from our paper, is\nopen-sourced at https:\/\/github.com\/AcademyCityL\/GALI.",
        "Large Language Models (LLMs) reasoning processes are challenging to analyze\ndue to their complexity and the lack of organized visualization tools. We\npresent ReasonGraph, a web-based platform for visualizing and analyzing LLM\nreasoning processes. It supports both sequential and tree-based reasoning\nmethods while integrating with major LLM providers and over fifty\nstate-of-the-art models. ReasonGraph incorporates an intuitive UI with meta\nreasoning method selection, configurable visualization parameters, and a\nmodular framework that facilitates efficient extension. Our evaluation shows\nhigh parsing reliability, efficient processing, and strong usability across\nvarious downstream applications. By providing a unified visualization\nframework, ReasonGraph reduces cognitive load in analyzing complex reasoning\npaths, improves error detection in logical processes, and enables more\neffective development of LLM-based applications. The platform is open-source,\npromoting accessibility and reproducibility in LLM reasoning analysis.",
        "We settle the theoretical ground for the study of automata networks under\nblock-parallel update schedules, which are somehow dual to the block-sequential\nones, but allow for repetitions of automaton updates. This gain in expressivity\nbrings new challenges, and we analyse natural equivalence classes of update\nschedules: those leading to the same dynamics, and to the same limit dynamics,\nfor any automata network. Countings and enumeration algorithms are provided,\nfor their numerical study. We also prove computational complexity bounds for\nmany classical problems, involving fixed points, limit cycles, the recognition\nof subdynamics, reachability, etc. The PSPACE-completeness of computing the\nimage of a single configuration lifts the complexity of most problems, but the\nlandscape keeps some relief, in particular for reversible computations.",
        "Quadratic Unconstrained Binary Optimization (QUBO)-based suppression in\nobject detection is known to have superiority to conventional Non-Maximum\nSuppression (NMS), especially for crowded scenes where NMS possibly suppresses\nthe (partially-) occluded true positives with low confidence scores. Whereas\nexisting QUBO formulations are less likely to miss occluded objects than NMS,\nthere is room for improvement because existing QUBO formulations naively\nconsider confidence scores and pairwise scores based on spatial overlap between\npredictions. This study proposes new QUBO formulations that aim to distinguish\nwhether the overlap between predictions is due to the occlusion of objects or\ndue to redundancy in prediction, i.e., multiple predictions for a single\nobject. The proposed QUBO formulation integrates two features into the pairwise\nscore of the existing QUBO formulation: i) the appearance feature calculated by\nthe image similarity metric and ii) the product of confidence scores. These\nfeatures are derived from the hypothesis that redundant predictions share a\nsimilar appearance feature and (partially-) occluded objects have low\nconfidence scores, respectively. The proposed methods demonstrate significant\nadvancement over state-of-the-art QUBO-based suppression without a notable\nincrease in runtime, achieving up to 4.54 points improvement in mAP and 9.89\npoints gain in mAR.",
        "Despite their outstanding performance in the majority of scenarios,\ncontemporary language models still occasionally generate undesirable outputs,\nfor example, hallucinated text. While such behaviors have previously been\nlinked to uncertainty, there is a notable lack of methods that actively\nconsider uncertainty during text generation. In this work, we show how Minimum\nBayes Risk (MBR) decoding, which selects model generations according to an\nexpected risk, can be generalized into a principled uncertainty-aware decoding\nmethod. In short, we account for model uncertainty during decoding by\nincorporating a posterior over model parameters into MBR's computation of\nexpected risk. We show that this modified expected risk is useful for both\nchoosing outputs and deciding when to abstain from generation and can provide\nimprovements without incurring overhead. We benchmark different methods for\nlearning posteriors and show that performance improves with prediction\ndiversity. We release our code publicly.",
        "Visual place recognition is a challenging task for autonomous driving and\nrobotics, which is usually considered as an image retrieval problem. A commonly\nused two-stage strategy involves global retrieval followed by re-ranking using\npatch-level descriptors. Most deep learning-based methods in an end-to-end\nmanner cannot extract global features with sufficient semantic information from\nRGB images. In contrast, re-ranking can utilize more explicit structural and\nsemantic information in one-to-one matching process, but it is time-consuming.\nTo bridge the gap between global retrieval and re-ranking and achieve a good\ntrade-off between accuracy and efficiency, we propose StructVPR++, a framework\nthat embeds structural and semantic knowledge into RGB global representations\nvia segmentation-guided distillation. Our key innovation lies in decoupling\nlabel-specific features from global descriptors, enabling explicit semantic\nalignment between image pairs without requiring segmentation during deployment.\nFurthermore, we introduce a sample-wise weighted distillation strategy that\nprioritizes reliable training pairs while suppressing noisy ones. Experiments\non four benchmarks demonstrate that StructVPR++ surpasses state-of-the-art\nglobal methods by 5-23% in Recall@1 and even outperforms many two-stage\napproaches, achieving real-time efficiency with a single RGB input.",
        "In this paper, we use Legendre-Fenchel transform and a space decomposition to\ncarry out Fountain theorem and dual Fountain theorem for the following elliptic\nsystem of Hamiltonian type: \\[ \\begin{cases} \\begin{aligned} -\\Delta u&=H_v(u,\nv) \\,\\quad&&\\text{in}~\\Omega,\\\\ -\\Delta v&=H_u(u, v)\n\\,\\quad&&\\text{in}~\\Omega,\\\\ u,\\,v&=0~~&&\\text{on} ~ \\partial\\Omega,\\\\\n\\end{aligned} \\end{cases} \\] where $N\\ge 1$, $\\Omega \\subset \\mathbb{R}^N$ is a\nbounded domain and $H\\in C^1( \\mathbb{R}^2)$ is strictly convex, even and\nsubcritical. We mainly present two results: (i) When $H$ is superlinear, the\nsystem has infinitely many solutions, whose energies tend to infinity. (ii)\nWhen $H$ is sublinear, the system has infinitely many solutions, whose energies\nare negative and tend to 0. As a byproduct, the Lane-Emden system under\nsubcritical growth has infinitely many solutions.",
        "This study explores stochastic resonance (SR) in a Schmitt trigger circuit\nand its application to weak signal detection. SR, a phenomenon where noise\nsynchronizes with weak signals to enhance detectability, was demonstrated using\na custom-designed bi-stable Schmitt trigger system. The circuit's bi-stability\nwas validated through hysteresis curve analysis, confirming its suitability for\nSR studies. Experimental results revealed SR behavior by analyzing\nsignal-to-noise ratio (SNR) responses to noise amplitude variations. Detection\nexperiments were conducted to determine frequency and amplitude of damping\nsinusoidal pulses. Frequency detection proved effective, albeit with\nlimitations at low frequencies, while amplitude detection faced challenges due\nto mathematical complexities. Nonetheless, the study highlights SR's potential\nfor weak signal detection, with proposed enhancements to improve detection\naccuracy. This work underscores the adaptability of classical SR principles to\npractical detection systems and suggests future applications in advanced\ndetection technologies, including quantum systems.",
        "Millimeter-wave (mmWave) radar offers robust sensing capabilities in diverse\nenvironments, making it a highly promising solution for human body\nreconstruction due to its privacy-friendly and non-intrusive nature. However,\nthe significant sparsity of mmWave point clouds limits the estimation accuracy.\nTo overcome this challenge, we propose a two-stage deep learning framework that\nenhances mmWave point clouds and improves human body reconstruction accuracy.\nOur method includes a mmWave point cloud enhancement module that densifies the\nraw data by leveraging temporal features and a multi-stage completion network,\nfollowed by a 2D-3D fusion module that extracts both 2D and 3D motion features\nto refine SMPL parameters. The mmWave point cloud enhancement module learns the\ndetailed shape and posture information from 2D human masks in single-view\nimages. However, image-based supervision is involved only during the training\nphase, and the inference relies solely on sparse point clouds to maintain\nprivacy. Experiments on multiple datasets demonstrate that our approach\noutperforms state-of-the-art methods, with the enhanced point clouds further\nimproving performance when integrated into existing models.",
        "We show that, under Dubrovin's notion of ''almost'' duality, the Frobenius\nmanifold structure on the orbit spaces of the extended affine Weyl groups of\ntype $\\mathrm{ADE}$ is dual, for suitable choices of weight markings, to the\nequivariant quantum cohomology of the minimal resolution of the du Val\nsingularity of the same Dynkin type. We also provide a uniform Lie-theoretic\nconstruction of Landau-Ginzburg mirrors for the quantum cohomology of\n$\\mathrm{ADE}$ resolutions. The mirror B-model is described by a\none-dimensional LG superpotential associated to the spectral curve of the\n$\\widehat{\\mathrm{ADE}}$ affine relativistic Toda chain.",
        "To study a deformation of a singularity taking into consideration their\ndifferential geometric properties, a form representing the deformation using\nonly diffeomorphisms on the source space and isometries of the target space\nplays a crucial role. Such a form for an $S_1$ singularity is obtained by the\nauthor's previous work. On this form, we give a necessary and sufficient\ncondition for such a map is being a frontal. The form for an $S_1$ singularity\nwith the frontal condition can be considered such a form for a cuspidal $S_1$\nsingularity. Using this form, we investigate geometric properties of cuspidal\n$S_1$ singularities and the cuspidal cross caps appearing in the deformation.",
        "Motivated by gravitational wave observations of binary neutron-star mergers,\nwe study the thermal index of low-density, high-temperature dense matter. We\nuse the virial expansion to account for nuclear interaction effects. We focus\non the region of validity of the expansion, which reaches $10^{-3}$ fm$^{-3}$\nat $T=5$ MeV up to almost saturation density at $T=50$ MeV. In pure neutron\nmatter, we find an analytical expression for the thermal index, and show that\nit is nearly density- and temperature-independent, within a fraction of a\npercent of the non-interacting, non-relativistic value of $\\Gamma_\\text{th}\n\\approx 5\/3$. When we incorporate protons, electrons and photons, we find that\nthe density and temperature dependence of the thermal index changes\nsignificantly. We predict a smooth transition between an electron-dominated\nregime with $\\Gamma_\\text{th} \\approx 4\/3$ at low densities to a\nneutron-dominated region with $\\Gamma_\\text{th} \\approx 5\/3$ at high densities.\nThis behavior is by and large independent of proton fraction and is not\naffected by nuclear interactions in the region where the virial expansion\nconverges. We model this smooth transition analytically and provide a simple\nbut accurate parametrization of the inflection point between these regimes.\nWhen compared to tabulated realistic models of the thermal index, we find an\noverall agreement at high temperatures that weakens for colder matter. The\ndiscrepancies can be attributed to the missing contributions of nuclear\nclusters. The virial approximation provides a clear and physically intuitive\nframework for understanding the thermal properties of dense matter, offering a\ncomputationally efficient solution that makes it particularly well-suited for\nthe regimes relevant to neutron star binary remnants.",
        "T CrA is a Herbig Ae-type young star in a complex circumstellar environment;\nit includes a circumstellar disk, accretion streamers, jets, and outflows. It\nhas long been suspected to be a binary. However, until now, there has been no\ndirect detection of a companion. Here we present new VLTI\/MATISSE L- and N-band\nobservations of T CrA taken between 2023 May and 2024 August with the aim of\ntesting the binary nature of the system. We modeled the data with a geometric\nmodel using the Python tool oimodeler. We detected a companion (T CrA B) with a\nprojected separation of $\\Delta r = 153.2 \\pm 1.2$ mas ($\\approx 23$ au) toward\nthe west direction at a position angle of $275.4 \\pm 0.1^\\circ$, in 2024\nMay-August. Our results support that the companion has a nearly edge-on orbit\nthat is highly misaligned with respect to the circumprimary disk. Such a\nconfiguration could cause warping and tearing of the disk around the primary,\nwhich has been proposed by recent studies. In the L band the companion is\nextended, with a full width at half maximum (FWHM) size of $\\sim 1$ au,\nsuggesting that the emission comes from a disk around the secondary star. The\ncompanion flux is 0.2-0.3 Jy in the L band, and 0.2-0.7 Jy in the N band,\naccounting for 4-20% of the total emission at those wavelengths. The SED of the\ncompanion is compatible with thermal radiation of warm dust (600-800 K).",
        "State-of-the-art methods for mesh detailization predominantly utilize\nGenerative Adversarial Networks (GANs) to generate detailed meshes from coarse\nones. These methods typically learn a specific style code for each category or\nsimilar categories without enforcing geometry supervision across different\nLevels of Detail (LODs). Consequently, such methods often fail to generalize\nacross a broader range of categories and cannot ensure shape consistency\nthroughout the detailization process. In this paper, we introduce MARS, a novel\napproach for 3D shape detailization. Our method capitalizes on a novel\nmulti-LOD, multi-category mesh representation to learn shape-consistent mesh\nrepresentations in latent space across different LODs. We further propose a\nmesh autoregressive model capable of generating such latent representations\nthrough next-LOD token prediction. This approach significantly enhances the\nrealism of the generated shapes. Extensive experiments conducted on the\nchallenging 3D Shape Detailization benchmark demonstrate that our proposed MARS\nmodel achieves state-of-the-art performance, surpassing existing methods in\nboth qualitative and quantitative assessments. Notably, the model's capability\nto generate fine-grained details while preserving the overall shape integrity\nis particularly commendable.",
        "Lifted neural networks (i.e. neural architectures explicitly optimizing over\nrespective network potentials to determine the neural activities) can be\ncombined with a type of adversarial training to gain robustness for internal as\nwell as input layers, in addition to improved generalization performance. In\nthis work we first investigate how adversarial robustness in this framework can\nbe further strengthened by solely modifying the training loss. In a second step\nwe fix some remaining limitations and arrive at a novel training loss for\nlifted neural networks, that combines targeted and untargeted adversarial\nperturbations.",
        "This study as part of an ongoing research effort, empirically examines the\nrelationship between foreign trade in the Istanbul Ataturk Airport Free Zone\nand exchange rate movements. Monthly data from 2003 to 2016 were analyzed\nthrough stationarity tests (Unit Root), followed by the Vector Autoregressive\n(VAR) model, Cointegration Analysis, and the Toda-Yamamoto Causality Test. The\nfindings indicate that the exchange rate does not significantly affect imports\nand exports in the free zone. This result suggests that free zones, due to\ntheir structural characteristics and operational framework, may be relatively\ninsulated from exchange rate fluctuations. The study contributes to the\nliterature by providing a focused analysis of a specific free zone in Turkiye,\nhighlighting the potential independence of free zone trade from exchange rate\nvolatility.",
        "Parameter estimation is a fundamental challenge in machine learning, crucial\nfor tasks such as neural network weight fitting and Bayesian inference. This\npaper focuses on the complexity of estimating translation $\\boldsymbol{\\mu} \\in\n\\mathbb{R}^l$ and shrinkage $\\sigma \\in \\mathbb{R}_{++}$ parameters for a\ndistribution of the form $\\frac{1}{\\sigma^l} f_0 \\left( \\frac{\\boldsymbol{x} -\n\\boldsymbol{\\mu}}{\\sigma} \\right)$, where $f_0$ is a known density in\n$\\mathbb{R}^l$ given $n$ samples. We highlight that while the problem is\nNP-hard for Maximum Likelihood Estimation (MLE), it is possible to obtain\n$\\varepsilon$-approximations for arbitrary $\\varepsilon > 0$ within\n$\\text{poly} \\left( \\frac{1}{\\varepsilon} \\right)$ time using the Wasserstein\ndistance.",
        "In this paper, we explore a self-supervised model that learns to detect the\nsymmetry of a single object without requiring a dataset-relying solely on the\ninput object itself. We hypothesize that the symmetry of an object can be\ndetermined by its intrinsic features, eliminating the need for large datasets\nduring training. Additionally, we design a self-supervised learning strategy\nthat removes the necessity of ground truth labels. These two key elements make\nour approach both effective and efficient, addressing the prohibitive costs\nassociated with constructing large, labeled datasets for this task. The novelty\nof our method lies in computing features for each point on the object based on\nthe idea that symmetric points should exhibit similar visual appearances. To\nachieve this, we leverage features extracted from a foundational image model to\ncompute a visual descriptor for the points. This approach equips the point\ncloud with visual features that facilitate the optimization of our\nself-supervised model. Experimental results demonstrate that our method\nsurpasses the state-of-the-art models trained on large datasets. Furthermore,\nour model is more efficient, effective, and operates with minimal computational\nand data resources.",
        "A main bottleneck of learning-based robotic scene understanding methods is\nthe heavy reliance on extensive annotated training data, which often limits\ntheir generalization ability. In LiDAR panoptic segmentation, this challenge\nbecomes even more pronounced due to the need to simultaneously address both\nsemantic and instance segmentation from complex, high-dimensional point cloud\ndata. In this work, we address the challenge of LiDAR panoptic segmentation\nwith very few labeled samples by leveraging recent advances in label-efficient\nvision panoptic segmentation. To this end, we propose a novel method,\nLimited-Label LiDAR Panoptic Segmentation (L3PS), which requires only a minimal\namount of labeled data. Our approach first utilizes a label-efficient 2D\nnetwork to generate panoptic pseudo-labels from a small set of annotated\nimages, which are subsequently projected onto point clouds. We then introduce a\nnovel 3D refinement module that capitalizes on the geometric properties of\npoint clouds. By incorporating clustering techniques, sequential scan\naccumulation, and ground point separation, this module significantly enhances\nthe accuracy of the pseudo-labels, improving segmentation quality by up to\n+10.6 PQ and +7.9 mIoU. We demonstrate that these refined pseudo-labels can be\nused to effectively train off-the-shelf LiDAR segmentation networks. Through\nextensive experiments, we show that L3PS not only outperforms existing methods\nbut also substantially reduces the annotation burden. We release the code of\nour work at https:\/\/l3ps.cs.uni-freiburg.de.",
        "From image to video understanding, the capabilities of Multi-modal LLMs\n(MLLMs) are increasingly powerful. However, most existing video understanding\nbenchmarks are relatively short, which makes them inadequate for effectively\nevaluating the long-sequence modeling capabilities of MLLMs. This highlights\nthe urgent need for a comprehensive and integrated long video understanding\nbenchmark to assess the ability of MLLMs thoroughly. To this end, we propose\nALLVB (ALL-in-One Long Video Understanding Benchmark). ALLVB's main\ncontributions include: 1) It integrates 9 major video understanding tasks.\nThese tasks are converted into video QA formats, allowing a single benchmark to\nevaluate 9 different video understanding capabilities of MLLMs, highlighting\nthe versatility, comprehensiveness, and challenging nature of ALLVB. 2) A fully\nautomated annotation pipeline using GPT-4o is designed, requiring only human\nquality control, which facilitates the maintenance and expansion of the\nbenchmark. 3) It contains 1,376 videos across 16 categories, averaging nearly 2\nhours each, with a total of 252k QAs. To the best of our knowledge, it is the\nlargest long video understanding benchmark in terms of the number of videos,\naverage duration, and number of QAs. We have tested various mainstream MLLMs on\nALLVB, and the results indicate that even the most advanced commercial models\nhave significant room for improvement. This reflects the benchmark's\nchallenging nature and demonstrates the substantial potential for development\nin long video understanding.",
        "Given the increasing global emphasis on sustainable energy usage and the\nrising energy demands of cellular wireless networks, this work seeks an optimal\nshort-term, continuous-time power procurement schedule to minimize operating\nexpenditure and the carbon footprint of cellular wireless networks equipped\nwith energy storage capacity, and hybrid energy systems comprising uncertain\nrenewable energy sources. Despite the stochastic nature of wireless fading\nchannels, the network operator must ensure a certain quality-of-service (QoS)\nconstraint with high probability. This probabilistic constraint prevents using\nthe dynamic programming principle to solve the stochastic optimal control\nproblem. This work introduces a novel time-continuous Lagrangian relaxation\napproach tailored for real-time, near-optimal energy procurement in cellular\nnetworks, overcoming tractability problems associated with the probabilistic\nQoS constraint. The numerical solution procedure includes an efficient upwind\nfinite-difference solver for the Hamilton--Jacobi--Bellman equation\ncorresponding to the relaxed problem, and an effective combination of the\nlimited memory bundle method (LMBM) for handling nonsmooth optimization and the\nstochastic subgradient method (SSM) to navigate the stochasticity of the dual\nproblem. Numerical results, based on the German power system and daily cellular\ntraffic data, demonstrate the computational efficiency of the proposed\nnumerical approach, providing a near-optimal policy in a practical timeframe.",
        "Knowledge of the force time history of a structure is essential to assess its\nbehaviour, ensure safety and maintain reliability. However, direct measurement\nof external forces is often challenging due to sensor limitations, unknown\nforce characteristics, or inaccessible load points. This paper presents an\nefficient dynamic load reconstruction method using physics-informed Gaussian\nprocesses (GP) based on frequency-sparse Fourier basis functions. The GP's\ncovariance matrices are built using the description of the system dynamics, and\nthe model is trained using structural response measurements. This provides\nsupport and interpretability to the machine learning model, in contrast to\npurely data-driven methods. In addition, the model filters out irrelevant\ncomponents in the Fourier basis function by leveraging the sparsity of\nstructural responses in the frequency domain, thereby reducing computational\ncomplexity during optimization. The trained model for structural responses is\nthen integrated with the differential equation for a harmonic oscillator,\ncreating a probabilistic dynamic load model that predicts load patterns without\nrequiring force data during training. The model's effectiveness is validated\nthrough two case studies: a numerical model of a wind-excited 76-story building\nand an experiment using a physical scale model of the Lilleb{\\ae}lt Bridge in\nDenmark, excited by a servo motor. For both cases, validation of the\nreconstructed forces is provided using comparison metrics for several signal\nproperties. The developed model holds potential for applications in structural\nhealth monitoring, damage prognosis, and load model validation.",
        "Cause-of-death data is fundamental for understanding population health trends\nand inequalities as well as designing and evaluating public health\ninterventions. A significant proportion of global deaths, particularly in low-\nand middle-income countries (LMICs), do not have medically certified causes\nassigned. In such settings, verbal autopsy (VA) is a widely adopted approach to\nestimate disease burdens by interviewing caregivers of the deceased. Recently,\nlatent class models have been developed to model the joint distribution of\nsymptoms and perform probabilistic cause-of-death assignment. A large number of\nlatent classes are usually needed in order to characterize the complex\ndependence among symptoms, making the estimated symptom profiles challenging to\nsummarize and interpret. In this paper, we propose a flexible Bayesian tensor\ndecomposition framework that balances the predictive accuracy of the\ncause-of-death assignment task and the interpretability of the latent\nstructures. The key to our approach is to partition symptoms into groups and\nmodel the joint distributions of group-level symptom sub-profiles. The proposed\nmethods achieve better predictive accuracy than existing VA methods and provide\na more parsimonious representation of the symptom distributions. We show our\nmethods provide new insights into the clustering patterns of both symptoms and\ncauses using the PHMRC gold-standard VA dataset.",
        "As multimedia services such as video streaming, video conferencing, virtual\nreality (VR), and online gaming continue to expand, ensuring high perceptual\nvisual quality becomes a priority to maintain user satisfaction and\ncompetitiveness. However, multimedia content undergoes various distortions\nduring acquisition, compression, transmission, and storage, resulting in the\ndegradation of experienced quality. Thus, perceptual visual quality assessment\n(PVQA), which focuses on evaluating the quality of multimedia content based on\nhuman perception, is essential for optimizing user experiences in advanced\ncommunication systems. Several challenges are involved in the PVQA process,\nincluding diverse characteristics of multimedia content such as image, video,\nVR, point cloud, mesh, multimodality, etc., and complex distortion scenarios as\nwell as viewing conditions. In this paper, we first present an overview of PVQA\nprinciples and methods. This includes both subjective methods, where users\ndirectly rate their experiences, and objective methods, where algorithms\npredict human perception based on measurable factors such as bitrate, frame\nrate, and compression levels. Based on the basics of PVQA, quality predictors\nfor different multimedia data are then introduced. In addition to traditional\nimages and videos, immersive multimedia and generative artificial intelligence\n(GenAI) content are also discussed. Finally, the paper concludes with a\ndiscussion on the future directions of PVQA research.",
        "Analyzing stocks and making higher accurate predictions on where the price is\nheading continues to become more and more challenging therefore, we designed a\nnew financial algorithm that leverages social media sentiment analysis to\nenhance the prediction of key stock earnings and associated volatility. Our\nmodel integrates sentiment analysis and data retrieval techniques to extract\ncritical information from social media, analyze company financials, and compare\nsentiments between Wall Street and the general public. This approach aims to\nprovide investors with timely data to execute trades based on key events,\nrather than relying on long-term stock holding strategies. The stock market is\ncharacterized by rapid data flow and fluctuating community sentiments, which\ncan significantly impact trading outcomes. Stock forecasting is complex given\nits stochastic dynamic. Standard traditional prediction methods often overlook\nkey events and media engagement, focusing its practice into long-term\ninvestment options. Our research seeks to change the stochastic dynamic to a\nmore predictable environment by examining the impact of media on stock\nvolatility, understanding and identifying sentiment differences between Wall\nStreet and retail investors, and evaluating the impact of various media\nnetworks in predicting earning reports."
      ]
    }
  },
  {
    "id":2411.00578,
    "research_type":"applied",
    "start_id":"b25",
    "start_title":"Voxel Scene Graph for Intracranial Hemorrhage",
    "start_abstract":"Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association"
      ],
      "abstract":[
        "Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "An intriguing coincidence between the majority of vast polar structure\n  dwarfs and a recent major merger at the M31 position",
        "Fault-Resilience of Dissipative Processes for Quantum Computing",
        "Does a Large Language Model Really Speak in Human-Like Language?",
        "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs",
        "Time Evolution of the Symmetry Resolved Entanglement Entropy after a\n  Mass Quench",
        "Quantum metric induced magneto-optical effects in\n  $\\mathcal{PT}$-symmetric antiferromagnets",
        "Systematic Search for Long-Term Trends in Fermi-LAT Jetted Active\n  Galactic Nuclei",
        "Approximate Evaluation Method for the Probability of the Union of\n  Independent Events",
        "Unveiling stellar spin: Determining inclination angles in Be stars",
        "FEASTS Combined with Interferometry (IV): Mapping HI Emission to a limit\n  of $N_{\\text{HI}}=10^{17.7} \\text{cm}^{-2}$ in Seven Edge-on Galaxies",
        "Randomized Spectral Clustering for Large-Scale Multi-Layer Networks",
        "Liquidity provision of utility indifference type in decentralized\n  exchanges",
        "How good is PAC-Bayes at explaining generalisation?",
        "Sliding ferroelectric control of unconventional magnetism in stacked\n  bilayers",
        "The centimeter emission from planet-forming disks in Taurus",
        "The{N\/D}-Conjecture for Nonresonant Hyperplane Arrangements",
        "Fine structure of phase diagram for social impact theory",
        "Spherical accretion in the Schwarzschild spacetime in the Newtonian\n  analogous construct",
        "Finite groups in which some particular non-nilpotent maximal invariant\n  subgroups have indices a prime-power",
        "Functional Linear Projection and Impulse Response Analysis",
        "Are there minimal exceptional aGUTs from stable 5D orbifolds?",
        "An $\\epsilon$-regularity theorem for Perelman's reduced volume",
        "Interpreting the HI 21-cm cosmology maps through Largest Cluster\n  Statistics. Part II. Impact of the realistic foreground and instrumental\n  noise on synthetic SKA1-Low observations",
        "Distribution Transformers: Fast Approximate Bayesian Inference With\n  On-The-Fly Prior Adaptation",
        "Thin Spectra for Periodic and Ergodic Word Models",
        "The generalized Lelong numbers and intersection theory",
        "Bell nonlocality in quantum networks with unreliable sources:\n  Loophole-free postelection via self-testing",
        "Origin of the Zeroth Law of Thermodynamics and its Role in Statistical\n  Mechanics",
        "Divisibility rules for integers presented as permutations"
      ],
      "abstract":[
        "A significant part of the Milky Way (MW) dwarf galaxies orbit within a Vast\nPOlar Structure (VPOS), which is perpendicular to the Galactic disc and whose\norigin has not yet been identified. It includes the Large Magellanic Cloud\n(LMC) and its six dynamically associated dwarf galaxies. Andromeda Galaxy (M31)\nexperienced a major merger two to three billion years ago, and its accurate\nmodelling predicts that an associated tidal tail is pointing towards the\nGalaxy. Here, we tested a possible association between M31 tidal tail particles\nand MW dwarf galaxies, focusing first on the LMC and its associated dwarfs\nsince they are less affected by ram pressure. We traced back these dwarf galaxy\norbits by one billion years and calculated their association with the tidal\ntail particles in the 6D phase space, based on their proper motion from\n\\textit{Gaia} DR3. We find that for low-mass MW models (total mass less than 5\n$\\times 10^{11} M_{\\odot}$), the separation in the 6D space can be less than\n1$\\sigma$ for most of the M31 modelling, albeit with a significant degree of\nfreedom due to the still unknown proper motion of M31. We further discover that\nmany other dwarfs could also be associated with the M31 tidal tails if their\nmotions had been radially slowed, as expected from the ram pressure exerted by\nthe MW corona. This intriguing coincidence could explain the origin of the\nVPOS, which resulted from a matter exchange between M31 and MW.",
        "Dissipative processes have long been proposed as a means of performing\ncomputational tasks on quantum computers that may be intrinsically more robust\nto noise. In this work, we prove two main results concerning the\nerror-resilience capabilities of two types of dissipative algorithms:\ndissipative ground state preparation in the form of the dissipative quantum\neigensolver (DQE), and dissipative quantum computation (DQC). The first result\nis that under circuit-level depolarizing noise, a version of the DQE algorithm\napplied to the geometrically local, stabilizer-encoded Hamiltonians that arise\nnaturally when fermionic Hamiltonians are represented in qubits, can suppress\nthe additive error in the ground space overlap of the final output state\nexponentially in the code distance. This enables us to get closer to\nfault-tolerance for this task without the associated overhead. In contrast, for\ncomputation as opposed to ground state preparation, the second result proves\nthat DQC is no more robust to noise than the standard quantum circuit model.",
        "Large Language Models (LLMs) have recently emerged, attracting considerable\nattention due to their ability to generate highly natural, human-like text.\nThis study compares the latent community structures of LLM-generated text and\nhuman-written text within a hypothesis testing procedure. Specifically, we\nanalyze three text sets: original human-written texts ($\\mathcal{O}$), their\nLLM-paraphrased versions ($\\mathcal{G}$), and a twice-paraphrased set\n($\\mathcal{S}$) derived from $\\mathcal{G}$. Our analysis addresses two key\nquestions: (1) Is the difference in latent community structures between\n$\\mathcal{O}$ and $\\mathcal{G}$ the same as that between $\\mathcal{G}$ and\n$\\mathcal{S}$? (2) Does $\\mathcal{G}$ become more similar to $\\mathcal{O}$ as\nthe LLM parameter controlling text variability is adjusted? The first question\nis based on the assumption that if LLM-generated text truly resembles human\nlanguage, then the gap between the pair ($\\mathcal{O}$, $\\mathcal{G}$) should\nbe similar to that between the pair ($\\mathcal{G}$, $\\mathcal{S}$), as both\npairs consist of an original text and its paraphrase. The second question\nexamines whether the degree of similarity between LLM-generated and human text\nvaries with changes in the breadth of text generation. To address these\nquestions, we propose a statistical hypothesis testing framework that leverages\nthe fact that each text has corresponding parts across all datasets due to\ntheir paraphrasing relationship. This relationship enables the mapping of one\ndataset's relative position to another, allowing two datasets to be mapped to a\nthird dataset. As a result, both mapped datasets can be quantified with respect\nto the space characterized by the third dataset, facilitating a direct\ncomparison between them. Our results indicate that GPT-generated text remains\ndistinct from human-authored text.",
        "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x.",
        "In this paper we investigate the properties of the symmetry resolved\nentanglement entropy after a mass quench in the Ising field theory. Since the\ntheory is free and the post-quench state known explicitly, the one-point\nfunction of the relevant (composite) branch point twist field can be computed\nusing form factor techniques, similar to previous work on the branch point\ntwist field and the magnetisation, respectively. We find that the symmetry\nresolved entropy grows linearly in time at the same rate as the total entropy,\nand that there are sub-leading oscillatory corrections. This result provides\nthe first explicit computation of the out-of-equilibrium dynamics of the\nsymmetry resolved entropy employing twist fields in quantum field theory and is\nconsistent with existing results based on the quasiparticle picture.",
        "The magneto-optical effects (MOEs), as a fundamental physical phenomenon, can\nreveal the electronic structures of materials. The related probing methods are\nwidely used in the study of magnetic materials. However, space-time inversion\n($\\mathcal{PT}$) symmetric antiferromagnets were previously believed to be\nmagneto-optically inactive. Here, we point out that this traditional\nunderstanding is incorrect. Based on our generic formulas and symmetry\nanalysis, we find that in $\\mathcal{PT}$-symmetric antiferromagnets, it is the\nquantum metric, i.e., the real part of the quantum geometry, that induces MOEs.\nCombining a tight-binding model and first-principles calculations, we confirm\nthis observation by showing MOEs in the $\\mathcal{PT}$-symmetric\nantiferromagnet. Our work demonstrates that $\\mathcal{PT}$-symmetric\nantiferromagnets previously thought to lack MOEs can indeed exhibit MOEs and\ngreatly broaden the research on MOEs.",
        "Jetted Active Galactic Nuclei (AGN) exhibit variability across a wide range\nof time scales. Traditionally, this variability can often be modeled well as a\nstochastic process. However, in certain cases, jetted AGN variability displays\nregular patterns, enabling us to conduct investigations aimed at understanding\nits origins. Additionally, a novel type of variability has emerged in jetted\nAGN lightcurves, specifically, the observation of a long-term trend\ncharacterized by a linear increase of the flux with time in blazars such as PG\n1553+113, which is among the objects most likely to display periodic behavior.\nIn this paper, we present the results of a systematic search for long-term\ntrends, spanning $\\approx$10\\, years, utilizing 12 years of Fermi-LAT\nobservations. The study is focused on detecting the presence of linear or\nquadratic long-term trends in a sample of 3308 jetted AGN. Our analysis has\nidentified 40 jetted AGN that exhibit long-term trends, each with distinct\nproperties, which we also characterize in this study. These long-term trends\nmay originate from the dynamics of a supermassive black hole binary system, or\nthey could be the result of intrinsic phenomena within the jet itself. Our\nfindings can help in addressing questions pertaining to the astrophysical\norigins of variability and periodicity within jetted AGN.",
        "The evaluation of the probability of union of a large number of independent\nevents requires several combinations involving the factorial and the use of\nhigh performance computers with several hours of processing. Bounds and\nsimplifications on the probability of the union are useful in the analysis of\nstochastic problems across various areas including (but not limited to) systems\nreliability, biological systems, real-time fault-tolerant systems, probability\ntheory, information theory and communications. We propose an approximation to\nevaluate the probability of the union of several independent events that uses\nthe arithmetic mean of the probability of all of them. The approximate results\nare very close to, but larger than the exact values. The method allows a much\nsmaller number of operations with a similar result and more simplicity.",
        "The physical properties of stellar atmospheres in rapidly rotating massive\nstars, such as Be stars, are critical to understanding their evolution and\ntheir role as progenitors of supernovae. These stars, which often have\nnear-critical rotation, exhibit equatorial stretching and gravity darkening,\nwhich significantly complicates the determination of parameters such as the\ninclination angle. Be stars, characterized by their extreme rotational\nvelocities, serve as excellent candidates for exploring these phenomena.\nHowever, fundamental quantities such as polar and equatorial radii and\ninclination angles are typically derived from interferometry, which applies\nonly to a limited number of stars. This study aims to enhance the determination\nof inclination angles for Be stars using the ZPEKTR spectral synthesis code. By\nincorporating advanced models of gravity darkening and stellar deformation, we\nevaluated the effectiveness of this method with a sample of ten Be stars from\nthe BeSOS database, comparing results with established interferometric data.\nMethods. We used the ZPEKTR code to model the effects of stellar oblateness and\ngravity darkening on spectral lines, focusing on the HeI 4471 line. We applied\na chi-squared test minimization approach to identify the best-fitting models,\nand we evaluated the inclination angles derived against interferometric\nmeasurements. Our analysis reveals a robust linear correlation between the\ninclination angles derived from ZPEKTR and using interferometric techniques,\nwhich demonstrates an excellent agreement. The ZPEKTR code effectively models\nhigh rotational velocity effects, providing precise stellar parameter\ndeterminations. The results underscore the potential of advanced spectroscopic\ntechniques to yield inclination measurements comparable to interferometry,\nwhich offers a pathway to studying distant massive stars.",
        "We present a statistical study of the neutral atomic hydrogen (HI) gas\nextending into the circumgalactic medium perpendicular to the disk for 7\nedge-on galaxies with inclinations above $85^{\\circ}$ from the FEASTS program\nwith a $3\\sigma$ ($20\\,\\text{km}\\,\\text{s}^{-1}$) column density\n($N_{\\text{HI}}$) depth of $5\\times10^{17} \\text{cm}^{-2}$. We develop two\nphotometric methods to separate the extraplanar HI from the disk component,\nbased on existing interferometric data and parametric modeling of the disk flux\ndistribution respectively. With both methods, the FEASTS data exhibit clear\nextended wings beyond the disk along the minor axis. The extraplanar HI\naccounts for 5% to 20% of the total HI mass and extends to $20\\text{-}50$ kpc\nat $N_{\\text{HI}}=10^{18} \\text{cm}^{-2}$. We find a tight positive correlation\nbetween vertical extensions of the extraplanar HI and total HI mass\n$M_\\text{HI}$. The iso-density shape of HI at $N_{\\text{HI}}=10^{18}\n\\text{cm}^{-2}$ has an average axis ratio of $0.56\\pm0.11$. The off-disk\n$N_{\\text{HI}}$ profiles of these edge-on galaxies well represent the lower\nenvelop of previous Lyman-$\\alpha$ absorption measurements at low-redshift. Our\nresults suggest that at $N_{\\text{HI}}=5\\times10^{17} \\text{cm}^{-2}$, the HI\nextends considerably further than the known thin and thick disks in the\nvertical direction, but still remains much flattener than a spherical\ndistribution, consistent with theoretical expectations that outflow,\ncirculation, and accretion should have different impacts in these two\ndirections. We show the tension of our results with Illustris and TNG\npredictions, highlighting the constraining power of our results for future\nsimulations.",
        "Large-scale multi-layer networks with large numbers of nodes, edges, and\nlayers arise across various domains, which poses a great computational\nchallenge for the downstream analysis. In this paper, we develop an efficient\nrandomized spectral clustering algorithm for community detection of multi-layer\nnetworks. We first utilize the random sampling strategy to sparsify the\nadjacency matrix of each layer. Then we use the random projection strategy to\naccelerate the eigen-decomposition of the sum-of-squared sparsified adjacency\nmatrices of all layers. The communities are finally obtained via the k-means of\nthe eigenvectors. The algorithm not only has low time complexity but also saves\nthe storage space. Theoretically, we study the misclassification error rate of\nthe proposed algorithm under the multi-layer stochastic block models, which\nshows that the randomization does not deteriorate the error bound under certain\nconditions. Numerical studies on multi-layer networks with millions of nodes\nshow the superior efficiency of the proposed algorithm, which achieves\nclustering results rapidly. A new R package called MLRclust is developed and\nmade available to the public.",
        "We present a mathematical formulation of liquidity provision in decentralized\nexchanges. We focus on constant function market makers of utility indifference\ntype, which include constant product market makers with concentrated liquidity\nas a special case. First, we examine no-arbitrage conditions for a liquidity\npool and compute an optimal arbitrage strategy when there is an external liquid\nmarket. Second, we show that liquidity provision suffers from impermanent loss\nunless a transaction fee is levied under the general framework with\nconcentrated liquidity. Third, we establish the well-definedness of\narbitrage-free reserve processes of a liquidity pool in continuous-time and\nshow that there is no loss-versus-rebalancing under a nonzero fee if the\nexternal market price is continuous. We then argue that liquidity provision by\nmultiple liquidity providers can be understood as liquidity provision by a\nrepresentative liquidity provider, meaning that the analysis boils down to that\nfor a single liquidity provider. Last, but not least, we give an answer to the\nfundamental question in which sense the very construction of constant function\nmarket makers with concentrated liquidity in the popular platform Uniswap v3 is\noptimal.",
        "We discuss necessary conditions for a PAC-Bayes bound to provide a meaningful\ngeneralisation guarantee. Our analysis reveals that the optimal generalisation\nguarantee depends solely on the distribution of the risk induced by the prior\ndistribution. In particular, achieving a target generalisation level is only\nachievable if the prior places sufficient mass on high-performing predictors.\nWe relate these requirements to the prevalent practice of using data-dependent\npriors in deep learning PAC-Bayes applications, and discuss the implications\nfor the claim that PAC-Bayes ``explains'' generalisation.",
        "The control of unconventional magnetism, which displays an antiferromagnetic\nconfiguration with ferromagnetism-like properties, has drawn intense attention\nfor advancing antiferromagnetic spintronics. Here, through symmetry analysis,\nwe propose a general stacking rule, characterized by a connection operator\nlinking two stacked bilayers, for controlling unconventional magnetism via\nsliding ferroelectricity. Such rule enables the simultaneous switching of both\nelectric polarization and nonrelativistic spin splitting or anomalous Hall\neffect in altermagnets, a class of collinear unconventional magnets. By\ncomprehensively surveying the 80 layer groups, we identify all the stacking\norders that allow for such two types of simultaneous switching. Combined with\nfirst-principles calculations, we demonstrate the sliding ferroelectric control\nof spin polarization and anomalous Hall effect in the altermagnetic AgF2\nbilayer. Our work provides a symmetry strategy for achieving ferroelectric\ncontrol of unconventional magnetism in bilayer systems and opens avenues for\nexploring new types of magnetoelectric coupling.",
        "The last decade has witnessed remarkable advances in the characterization of\nthe (sub-)millimeter emission from planet-forming disks. Instead, the study of\nthe (sub-)centimeter emission has made more limited progress, to the point that\nonly a few exceptional disk-bearing objects have been characterized in the\ncentimeter regime. This work takes a broad view of the centimeter emission from\na large sample with VLA observations that is selected from previous ALMA\nsurveys of more representative disks in brightness and extent. We report on the\ndetection and characterization of flux at centimeter wavelengths from 21\nsources in the Taurus star-forming region. Complemented by literature and\narchival data, the entire photometry from 0.85 mm to 6 cm is fitted by a\ntwo-component model that determines the ubiquitous presence of free-free\nemission entangled with the dust emission. The flux density of the free-free\nemission is found to scale with the accretion rate but is independent of the\nouter disk morphology depicted by ALMA. The dust emission at 2 cm is still\nappreciable, and offers the possibility to extract an unprecedented large set\nof dust spectral indices in the centimeter regime. A pronounced change between\nthe median millimeter indices (2.3) and centimeter indices (2.8) suggests that\na large portion of the disk emission is optically thick up to 3 mm. The\ncomparison of both indices and fluxes with the ALMA disk extent indicates that\nthis portion can be as large as 40 au, and suggests that the grain population\nwithin this disk region that emits the observed centimeter emission is similar\nin disks with different size and morphology. All these results await\nconfirmation and dedicated dust modeling once facilities like ngVLA or SKA-mid\nare able to resolve the centimeter emission from planet-forming disks and\ndisentangle the various components.",
        "This paper studies Bernstein--Sato polynomials $b_{f,0}$ for homogeneous\npolynomials $f$ of degree $d$ with $n$ variables. It is open to know when\n$-{n\\over d}$ is a root of $b_{f,0}$. For essential indecomposable hyperplane\narrangements, this is a conjecture by Budur, Musta\\c{t}\\u{a} and Teitler and\nimplies the strong topological monodromy conjecture for arrangements. U.\nWalther gave a sufficient condition that a certain differential form does not\nvanish in the top cohomology group of Milnor fiber. We use Walther's result to\nverify the $n\\over d$-conjecture for weighted hyperplane arrangements\nsatisfying the nonresonant condition. We also give some essential\nindecomposable homogeneous polynomials $f$ such that $-{n\\over d}$ is not a\nroot of $b_{f,0}$. This leads to a conjectural sufficient condition for\n$b_{f,0}(-{n\\over d})=0$.",
        "In this paper, the social impact theory introduced by Latan\\'e is\nreconsidered. A fully differentiated society is considered; that is, initially\nevery actor has their own opinion. The equivalent of Muller's ratchet guards\nthat -- even for the non-deterministic case (with a positive social\ntemperature) -- any opinion once removed from the opinion space does not appear\nagain. With computer simulation, we construct the phase diagram for Latan\\'e\nmodel based on the number of surviving opinions after various evolution times.\nThe phase diagram is constructed on the two-dimensional plane of model control\nparameters responsible for the effective range of interaction among actors and\nthe social temperature. Introducing the Muller's ratchet-like mechanism gives a\nnon-zero chance for any opinion to be removed from the system. We believe that\nin such a case, for any positive temperature, ultimately a consensus is\nreached. However, even for a moderate system size, the time to reach consensus\nis very long. In contrast, for the deterministic case (without social\ntemperature), the system may be frozen with clusters of actors having several\ndifferent opinions, or even reach the cycle limit (with blinking structures).",
        "The velocity-dependent Newtonian analogous potentials (NAPs) corresponding to\ngeneral relativistic (GR) spacetimes accurately capture most of the\nrelativistic features, including all classical tests of GR, effectively\nrepresenting spacetime geometries in Newtonian terms. The NAP formulated by\nTejeda \\& Rosswog (TR13) for Schwarzschild spacetime has been applied to the\nstandard thin accretion disk around a black hole (BH) as well as in the context\nof streamlines of noninteracting particles accreting onto a Schwarzschild BH,\nshowing good agreement with the exact relativistic solutions. As a further\napplication, here we explore the extent to which TR13 NAP could describe a\ntransonic hydrodynamical spherical accretion flow in Schwarzschild spacetime\nwithin the framework of standard Newtonian hydrodynamics. Instead of obtaining\na typical single \"saddle-type\" sonic transition, a \"saddle-spiral pair\" is\nproduced, with the inner sonic point being an (unphysical) \"spiral type\" and\nthe outer being a usual \"saddle type.\" The Bondi accretion rate at outer sonic\nradii, however, remains consistent with that of the GR case. The primary reason\nfor the deviation of our findings from the classical Bondi solution is likely\ndue to the inconsistency between the Euler-type equation in the presence of\nvelocity-dependent TR13 NAP within the standard Newtonian hydrodynamics\nframework, and the corresponding GR Euler equation, regardless of the fluid's\nenergy. Our study suggests that a (modified) hydrodynamical formalism is needed\nto effectively implement such potentials in transonic accretion studies that\nalign with the spirit of TR13-like NAP, while remaining consistent with the GR\nhydrodynamics. This could then essentially circumvent GR hydrodynamics or GR\nmagnetohydrodynamics equations.",
        "Let $A$ and $G$ be finite groups such that $A$ acts coprimely on $G$ by\nautomorphisms, assume that $G$ has a maximal $A$-invariant subgroup $M$ that is\na direct product of some isomorphic simple groups, we prove that if $G$ has a\nnon-trivial $A$-invariant normal subgroup $N$ such that $N\\leq M$ and every\nnon-nilpotent maximal $A$-invariant subgroup $K$ of $G$ not containing $N$ has\nindex a prime-power and the projective special linear group $PSL_2(7)$ is not a\ncomposition factor of $G$, then $G$ is solvable.",
        "This paper proposes econometric methods for studying how economic variables\nrespond to function-valued shocks. Our methods are developed based on linear\nprojection estimation of predictive regression models with a function-valued\npredictor and other control variables. We show that the linear projection\ncoefficient associated with the functional variable allows for the impulse\nresponse interpretation in a functional structural vector autoregressive model\nunder a certain identification scheme, similar to well-known Sims' (1972)\ncausal chain, but with nontrivial complications in our functional setup. A\nnovel estimator based on an operator Schur complement is proposed and its\nasymptotic properties are studied. We illustrate its empirical applicability\nwith two examples involving functional variables: economy sentiment\ndistributions and functional monetary policy shocks.",
        "In analysing five dimensional orbifolds with exceptional gauge groups, we\nseek to find stable vacua configurations which satisfy the minimal requirements\nfor asymptotic grand unified models. In this respect we show that no minimal\nasymptotic grand unified theory can be built. Our results point towards\nnon-minimal models based on $E_6$: one featuring supersymmetry, and the other\nneeding a modification of the Coleman-Weinberg potential to stabilise the\nbreaking of $E_6$ to the standard model gauge group.",
        "In this article, we prove an $\\epsilon$-regularity theorem for Perelman's\nreduced volume. We show that on a Ricci flow, if Perelman's reduced volume is\nclose to $1$, then the curvature radius at the base point cannot be too small.",
        "The Largest Cluster Statistics\\,(LCS) analysis of the redshifted 21\\,cm maps\nhas been demonstrated to be an efficient and robust method for following the\ntime evolution of the largest ionized regions\\,(LIRs) during the Epoch of\nReionization\\,(EoR). The LCS can, in principle, constrain the reionization\nmodel and history by quantifying the morphology of neutral hydrogen\\,(\\HI)\ndistribution during the different stages of the EoR. Specifically, the\npercolation transition of ionized regions, quantified and constrained via LCS,\nprovides a crucial insight about the underlying reionization model. The\nprevious LCS analysis of EoR 21\\,cm maps demonstrates that the convolution of\nthe synthesized beam of the radio interferometric arrays, e.g. SKA1-Low with\nthe target signal, shifts the apparent percolation transition of ionized\nregions towards the lower redshifts. In this study, we present an optimal\nthresholding strategy to reduce this bias in the recovered percolation\ntransition. We assess the robustness of LCS analysis of the 21\\,cm maps in the\npresence of antenna-based gain calibration errors and instrumental noise for\nSKA1-Low. This analysis is performed using synthetic observations simulated by\nthe \\textsc{21cmE2E} pipeline, considering SKA1-Low AA4 configuration within a\nradius of 2\\,km from the array centre. Our findings suggest that a minimum of\n$1500$\\,hours of observation (SNR $\\gtrapprox 3$) are required for the LCS\nanalysis to credibly suppress the confusion introduced by thermal noise.\nFurther, we also demonstrate that for a maximum antenna-based calibration error\ntolerance of $\\sim 0.05\\%$ (post calibration), the reionization history can be\nrecovered in a robust and relatively unbiased manner using the LCS.",
        "While Bayesian inference provides a principled framework for reasoning under\nuncertainty, its widespread adoption is limited by the intractability of exact\nposterior computation, necessitating the use of approximate inference. However,\nexisting methods are often computationally expensive, or demand costly\nretraining when priors change, limiting their utility, particularly in\nsequential inference problems such as real-time sensor fusion. To address these\nchallenges, we introduce the Distribution Transformer -- a novel architecture\nthat can learn arbitrary distribution-to-distribution mappings. Our method can\nbe trained to map a prior to the corresponding posterior, conditioned on some\ndataset -- thus performing approximate Bayesian inference. Our novel\narchitecture represents a prior distribution as a (universally-approximating)\nGaussian Mixture Model (GMM), and transforms it into a GMM representation of\nthe posterior. The components of the GMM attend to each other via\nself-attention, and to the datapoints via cross-attention. We demonstrate that\nDistribution Transformers both maintain flexibility to vary the prior, and\nsignificantly reduces computation times-from minutes to milliseconds-while\nachieving log-likelihood performance on par with or superior to existing\napproximate inference methods across tasks such as sequential inference,\nquantum system parameter inference, and Gaussian Process predictive posterior\ninference with hyperpriors.",
        "We establish a new and simple criterion that suffices to generate many\nspectral gaps for periodic word models. This leads to new examples of ergodic\nSchr\\\"odinger operators with Cantor spectra having zero Hausdorff dimension\nthat simultaneously may have arbitrarily small supremum norm together with\narbitrarily long runs on which the potential vanishes.",
        "Let $X$ be a complex manifold of dimension $k,$ and $(V,\\omega)$ be a\nK\\\"ahler submanifold of dimension $l$ in $X,$ and $B\\Subset V$ be a domain with\n$\\mathcal{C}^2$-smooth boundary. Let $T$ be a positive plurisubharmonic current\non $X$ such that $T$ satisfies a reasonable approximation condition on $X$ and\nnear $\\partial B.$ In our previous work we introduce the concept of the\ngeneralized Lelong numbers $\\nu_j(T,B)\\in\\mathbb{R}$ of $T$ along $B$ for\n$0\\leq j\\leq l.$ When $l=0,$ $V=B$ is a single point $x,$ $\\nu_0(T,B)$ is none\nother than the classical Lelong number of $T$ at $x.$\n  This article has five purposes: Firstly, we formulate the notion of the\ngeneralized Lelong number of $T$ associated to every closed smooth $(j,j)$-form\non $V.$ This concept extends the previous notion of the generalized Lelong\nnumbers. We also establish their basic properties. Secondly, we define the\nhorizontal dimension $\\hbar$ of such a current $T$ along $B.$ Next, we\ncharacterize $\\hbar$ in terms of the generalized Lelong numbers. We also\nestablish a Siu's upper-semicontinuity type theorem for the generalized Lelong\nnumbers. In their above-mentioned context, Dinh and Sibony introduced some\ncohomology classes which may be regarded as their analogues of the classical\nLelong numbers. Our third objective is to generalize their notion to the\nbroader context where $T$ is (merely) positive pluriharmonic. Moreover, we also\nestablish a formula relating Dinh-Sibony classes and the generalized Lelong\nnumbers. Fourthly, we obtain an effective sufficient condition for defining the\nintersection of $m$ positive closed currents in the sense of Dinh-Sibony's\ntheory of tangent currents on a compact K\\\"ahler manifold. Finally, we\nestablish an effective sufficient condition for the continuity of the above\nintersection.",
        "We discuss Bell nonlocality in quantum networks with unreliable sources. Our\nmain result is a condition on the observed data which ensures that inconclusive\nevents can be safely discarded, without introducing any loophole. More\nformally, we characterize the fair-sampling property for measurements in a\nnetwork. When all measurements are fair-sampling, we show that the\npost-selection of conclusive outcomes does not compromise the assumption of\nsource independence, hence avoiding the detection loophole. Furthermore, we\nshow that in some cases, the fair-sampling property can in fact be guaranteed\nbased only on observed data. To show this, we prove that saturation of the\nFinner inequality provides a self-test of the underlying quantum model. We\nillustrate the relevance of our results by demonstrating an improvement in\ndevice-independent randomness generation for a photonic Bell test with a\nprobabilistic source and for the triangle network.",
        "In statistical mechanics the zeroth law of thermodynamics is taken as a\npostulate which, as its name indicates, logically precedes the first and second\nlaws. Treating it as a postulate has consequences for how temperature is\nintroduced into statistical mechanics and for the molecular interpretation of\ntemperature. One can, however, derive the zeroth law from first principles\nstarting from a classical Hamiltonian using basic mechanics and a geometric\nrepresentation of the phase space of kinetic energy configurations - the\nvelocity hypersphere. In this approach there is no difficulty in providing a\nmolecular interpretation of temperature, nor in deriving equality of\ntemperature as the condition of thermal equilibrium. The approach to the\nmacroscopic limit as a function of the number of atoms is easily determined.\nOne also obtains with little difficulty the Boltzmann probability distribution,\nthe statistical mechanical definition of entropy and the configuration\npartition function. These relations, along with the zeroth law, emerge as\nstraightforward consequences of atoms in random motion.",
        "In this note, we represent integers in a type of factoradic notation. Rather\nthan use the corresponding Lehmer code, we will view integers as permutations.\nGiven a pair of integers n and k, we give a formula for n mod k in terms of the\nfactoradic digits, and use this to deduce various divisibility rules."
      ]
    }
  },
  {
    "id":2411.00614,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion",
    "start_abstract":"Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Causal identification of single-cell experimental perturbation effects with CINEMA-OT"
      ],
      "abstract":[
        "Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "A Survey on Foundation-Model-Based Industrial Defect Detection",
        "Generalized $\\eta$-pairing theory and anomalous localization in\n  non-Hermitian systems",
        "Aerial Vision-and-Language Navigation with Grid-based View Selection and\n  Map Construction",
        "Real-Time Fast Marching Tree for Mobile Robot Motion Planning in Dynamic\n  Environments",
        "The Muddy Waters of Modeling Empathy in Language: The Practical Impacts\n  of Theoretical Constructs",
        "Theoretical study of the $\\Sigma N$ cusp in the\n  $K^-d\\rightarrow\\pi\\Lambda N$ reaction",
        "A modeling framework to support the electrification of private transport\n  in African cities: a case study of Addis Ababa",
        "Stacked Intelligent Metasurface Enabled Near-Field Multiuser\n  Beamfocusing in the Wave Domain",
        "Systematic Abductive Reasoning via Diverse Relation Representations in\n  Vector-symbolic Architecture",
        "A note On the existence of solutions to Hitchin's self-duality equations",
        "General relativistic quasi-spherical accretion in a dark matter halo",
        "General relativistic particle trajectories via quantum mechanical weak\n  values and the Schwarzschild-Alcubierre spacetime",
        "ProPINN: Demystifying Propagation Failures in Physics-Informed Neural\n  Networks",
        "RePanda: Pandas-powered Tabular Verification and Reasoning",
        "Comparing Native and Non-native English Speakers' Behaviors in\n  Collaborative Writing through Visual Analytics",
        "Multifractal Terrain Generation for Evaluating Autonomous Off-Road\n  Ground Vehicles",
        "Real-world actor-based image steganalysis via classifier inconsistency\n  detection",
        "OpenGERT: Open Source Automated Geometry Extraction with Geometric and\n  Electromagnetic Sensitivity Analyses for Ray-Tracing Propagation Models",
        "Improving TCM Question Answering through Tree-Organized Self-Reflective\n  Retrieval with LLMs",
        "Unsupervised Particle Tracking with Neuromorphic Computing",
        "Imagine to Hear: Auditory Knowledge Generation can be an Effective\n  Assistant for Language Models",
        "Efficient Point Clouds Upsampling via Flow Matching",
        "A classical proof of quantum knowledge for multi-prover interactive\n  proof systems",
        "Transient synchronization stability analysis and assessment of DFIG\n  system under severe faults",
        "Improving action segmentation via explicit similarity measurement",
        "VarDrop: Enhancing Training Efficiency by Reducing Variate Redundancy in\n  Periodic Time Series Forecasting",
        "Deep Understanding of Sign Language for Sign to Subtitle Alignment",
        "Efficient Hierarchical Contrastive Self-supervising Learning for Time\n  Series Classification via Importance-aware Resolution Selection",
        "Exploring Energy Landscapes for Minimal Counterfactual Explanations:\n  Applications in Cybersecurity and Beyond"
      ],
      "abstract":[
        "As industrial products become abundant and sophisticated, visual industrial\ndefect detection receives much attention, including two-dimensional and\nthree-dimensional visual feature modeling. Traditional methods use statistical\nanalysis, abnormal data synthesis modeling, and generation-based models to\nseparate product defect features and complete defect detection. Recently, the\nemergence of foundation models has brought visual and textual semantic prior\nknowledge. Many methods are based on foundation models (FM) to improve the\naccuracy of detection, but at the same time, increase model complexity and slow\ndown inference speed. Some FM-based methods have begun to explore lightweight\nmodeling ways, which have gradually attracted attention and deserve to be\nsystematically analyzed. In this paper, we conduct a systematic survey with\ncomparisons and discussions of foundation model methods from different aspects\nand briefly review non-foundation model (NFM) methods recently published.\nFurthermore, we discuss the differences between FM and NFM methods from\ntraining objectives, model structure and scale, model performance, and\npotential directions for future exploration. Through comparison, we find FM\nmethods are more suitable for few-shot and zero-shot learning, which are more\nin line with actual industrial application scenarios and worthy of in-depth\nresearch.",
        "By generalizing the eta-pairing theory to non-Hermitian Hubbard models on\narbitrary lattices, we obtain the sufficient and necessary condition for the\neta-pairing operator to be an eigenoperator of the Hamiltonian $H$, and find\nunique eta-pairing phenomena without Hermitian analogs. For instance, the\nHermitian conjugate of an eta-pairing eigenoperator may not be an\neigenoperator, eta-pairing eigenoperators can be spatially modulated, and the\n$SU(2)$ pseudospin symmetry may not be respected even if $H$ commutes with the\neta-pairing operators. Remarkably, these novel non-Hermitian phenomena are\nclosely related to each other by several theorems we establish and can lead to,\ne.g., the notion of non-Hermitian angular-momentum operators and the anomalous\nlocalization of eta-pairing eigenstates. Some issues on the $SO(4)$ and\nparticle-hole symmetries are clarified. Our general eta-pairing theory also\nreveals a previously unnoticed unification of these symmetries of the Hubbard\nmodel. To exemplify these findings, we propose the Hatano-Nelson-Hubbard model.\nIn this interacting non-Hermitian system without even the bulk translation\ninvariance, the right and left two-particle eta-pairing eigenstates are\nexponentially localized at opposite boundaries of the chain. We then generalize\nthis model to two dimensions and find that the eta-pairing eigenstate can\nexhibit the first- or second-order skin effect. Thus, eta-pairing may represent\na new mechanism for skin effects in interacting non-Hermitian systems, even in\nhigher dimensions and without the bulk translation symmetry. To realize all of\nthe non-Hermitian eta-pairing phenomena, we construct a general two-sublattice\nmodel defined on an arbitrary lattice, which can exhibit anomalous localization\nof eta-pairing eigenstates; besides, this model can reveal the eta-pairing\nstructure [e.g., the $SO(4)$ symmetry] in systems with Hermitian hoppings.",
        "Aerial Vision-and-Language Navigation (Aerial VLN) aims to obtain an unmanned\naerial vehicle agent to navigate aerial 3D environments following human\ninstruction. Compared to ground-based VLN, aerial VLN requires the agent to\ndecide the next action in both horizontal and vertical directions based on the\nfirst-person view observations. Previous methods struggle to perform well due\nto the longer navigation path, more complicated 3D scenes, and the neglect of\nthe interplay between vertical and horizontal actions. In this paper, we\npropose a novel grid-based view selection framework that formulates aerial VLN\naction prediction as a grid-based view selection task, incorporating vertical\naction prediction in a manner that accounts for the coupling with horizontal\nactions, thereby enabling effective altitude adjustments. We further introduce\na grid-based bird's eye view map for aerial space to fuse the visual\ninformation in the navigation history, provide contextual scene information,\nand mitigate the impact of obstacles. Finally, a cross-modal transformer is\nadopted to explicitly align the long navigation history with the instruction.\nWe demonstrate the superiority of our method in extensive experiments.",
        "This paper proposes the Real-Time Fast Marching Tree (RT-FMT), a real-time\nplanning algorithm that features local and global path generation,\nmultiple-query planning, and dynamic obstacle avoidance. During the search,\nRT-FMT quickly looks for the global solution and, in the meantime, generates\nlocal paths that can be used by the robot to start execution faster. In\naddition, our algorithm constantly rewires the tree to keep branches from\nforming inside the dynamic obstacles and to maintain the tree root near the\nrobot, which allows the tree to be reused multiple times for different goals.\nOur algorithm is based on the planners Fast Marching Tree (FMT*) and Real-time\nRapidly-Exploring Random Tree (RT-RRT*). We show via simulations that RT-FMT\noutperforms RT- RRT* in both execution cost and arrival time, in most cases.\nMoreover, we also demonstrate via simulation that it is worthwhile taking the\nlocal path before the global path is available in order to reduce arrival time,\neven though there is a small possibility of taking an inferior path.",
        "Conceptual operationalizations of empathy in NLP are varied, with some having\nspecific behaviors and properties, while others are more abstract. How these\nvariations relate to one another and capture properties of empathy observable\nin text remains unclear. To provide insight into this, we analyze the transfer\nperformance of empathy models adapted to empathy tasks with different\ntheoretical groundings. We study (1) the dimensionality of empathy definitions,\n(2) the correspondence between the defined dimensions and measured\/observed\nproperties, and (3) the conduciveness of the data to represent them, finding\nthey have a significant impact to performance compared to other transfer\nsetting features. Characterizing the theoretical grounding of empathy tasks as\ndirect, abstract, or adjacent further indicates that tasks that directly\npredict specified empathy components have higher transferability. Our work\nprovides empirical evidence for the need for precise and multidimensional\nempathy operationalizations.",
        "The $K^-d\\rightarrow\\pi\\Lambda N$ reaction is useful for exploring the\nhyperon-nucleon interaction through final state interactions. In particular,\nthe cusp structure of the $\\Lambda N$ invariant mass spectrum at the $\\Sigma N$\nthreshold contains information about the s-wave interaction of 1\/2-isospin\nhyperon-nucleon systems. The calculation of the spectrum is performed with the\naim of extracting the scattering length of the $\\Sigma N(I=1\/2)$ channel that\ncouples to the $\\Lambda N$ channel from this reaction, and the results are\ndiscussed in comparison with experimental data to highlight the factors that\nshould be considered.",
        "The electrification of road transport, as the predominant mode of\ntransportation in Africa, represents a great opportunity to reduce greenhouse\ngas emissions and dependence on costly fuel imports. However, it introduces\nmajor challenges for local energy infrastructures, including the deployment of\ncharging stations and the impact on often fragile electricity grids. Despite\nits importance, research on electric mobility planning in Africa remains\nlimited, while existing planning tools rely on detailed local mobility data\nthat is often unavailable, especially for privately owned passenger vehicles.\nIn this study, we introduce a novel framework designed to support private\nvehicle electrification in data-scarce regions and apply it to Addis Ababa,\nsimulating the mobility patterns and charging needs of 100,000 electric\nvehicles. Our analysis indicate that these vehicles generate a daily charging\ndemand of approximately 350 MWh and emphasize the significant influence of the\ncharging location on the spatial and temporal distribution of this demand.\nNotably, charging at public places can help smooth the charging demand\nthroughout the day, mitigating peak charging loads on the electricity grid. We\nalso estimate charging station requirements, finding that workplace charging\nrequires approximately one charging point per three electric vehicles, while\npublic charging requires only one per thirty. Finally, we demonstrate that\nphotovoltaic energy can cover a substantial share of the charging needs,\nemphasizing the potential for renewable energy integration. This study lays the\ngroundwork for electric mobility planning in Addis Ababa while offering a\ntransferable framework for other African cities.",
        "Intelligent surfaces represent a breakthrough technology capable of\ncustomizing the wireless channel cost-effectively. However, the existing works\ngenerally focus on planar wavefront, neglecting near-field spherical wavefront\ncharacteristics caused by large array aperture and high operation frequencies\nin the terahertz (THz). Additionally, the single-layer reconfigurable\nintelligent surface (RIS) lacks the signal processing ability to mitigate the\ncomputational complexity at the base station (BS). To address this issue, we\nintroduce a novel stacked intelligent metasurfaces (SIM) comprised of an array\nof programmable metasurface layers. The SIM aims to substitute conventional\ndigital baseband architecture to execute computing tasks with ultra-low\nprocessing delay, albeit with a reduced number of radio-frequency (RF) chains\nand low-resolution digital-to-analog converters. In this paper, we present a\nSIM-aided multiuser multiple-input single-output (MU-MISO) near-field system,\nwhere the SIM is integrated into the BS to perform beamfocusing in the wave\ndomain and customize an end-to-end channel with minimized inter-user\ninterference. Finally, the numerical results demonstrate that near-field\ncommunication achieves superior spatial gain over the far-field, and the SIM\neffectively suppresses inter-user interference as the wireless signals\npropagate through it.",
        "In abstract visual reasoning, monolithic deep learning models suffer from\nlimited interpretability and generalization, while existing neuro-symbolic\napproaches fall short in capturing the diversity and systematicity of\nattributes and relation representations. To address these challenges, we\npropose a Systematic Abductive Reasoning model with diverse relation\nrepresentations (Rel-SAR) in Vector-symbolic Architecture (VSA) to solve\nRaven's Progressive Matrices (RPM). To derive attribute representations with\nsymbolic reasoning potential, we introduce not only various types of atomic\nvectors that represent numeric, periodic and logical semantics, but also the\nstructured high-dimentional representation (SHDR) for the overall Grid\ncomponent. For systematic reasoning, we propose novel numerical and logical\nrelation functions and perform rule abduction and execution in a unified\nframework that integrates these relation representations. Experimental results\ndemonstrate that Rel-SAR achieves significant improvement on RPM tasks and\nexhibits robust out-of-distribution generalization. Rel-SAR leverages the\nsynergy between HD attribute representations and symbolic reasoning to achieve\nsystematic abductive reasoning with both interpretable and computable\nsemantics.",
        "In 1987, Hitchin introduced the self-duality equations on rank-2 complex\nvector bundles over compact Riemann surfaces with genus greater than one as a\nreduction of the Yang-Mills equation and established the existence of solutions\nto these equations starting from a Higgs stable bundle. In this paper, we fill\nin some technical details in Hitchin's original proof by the following three\nsteps. First, we reduce the existence of a solution of class $L_1^2$ to\nminimizing the energy functional within a Higgs stable orbit of the $L_2^2$\ncomplex gauge group action. Second, using this transformation, we obtain a\nsolution of class $L_1^2$ in this orbit. These two steps primarily follow\nHitchin's original approach. Finally, using the Coulomb gauge, we construct a\nsmooth solution by applying an $L_2^2$ unitary gauge transformation to the\n$L_1^2$ solution constructed previously. This last step provides additional\ntechnical details to Hitchin's original proof.",
        "Context. The Bondi spherical accretion solution has been used to model\naccretion onto compact objects in a variety of situations, from interpretation\nof observations to subgrid models in cosmological simulations. Aims. We aim to\ninvestigate how the presence of dark matter (DM) alters the dynamics and\nphysical properties of accretion onto supermassive black holes on scales\nranging from ~ 10 pc to the event horizon. Methods. In particular, we\ninvestigate Bondi-like accretion flows with zero and low specific angular\nmomentum around supermassive black holes surrounded by dark-matter halos by\nperforming 1D and 2.5D general relativistic hydrodynamics (GRHD) simulations\nusing the black hole accretion code (BHAC). Results. We find notable\ndifferences in the dynamics and structure of spherical accretion flows in the\npresence of DM. The most significant effects include increases in density,\ntemperature, and pressure, as well as variations in radial velocity both inside\nand outside the regions containing DM or even the production of outflow.\nConclusions. This investigation provides valuable insights into the role of\ncosmological effects, particularly DM, in shaping the behavior of accretion\nflows and black holes (BHs). Our simulations may be directly applicable to\nmodel systems with a large black hole-to-halo mass ratio, which are expected to\nbe found at very high redshifts.",
        "We show that the average trajectories of relativistic quantum particles in\nSchwarzschild spacetime, obtained via quantum mechanical weak measurements of\nmomentum and energy, are equivalent to the predicted flow lines of probability\ncurrent in curved spacetime quantum theory. We subsequently demonstrate that\nthese trajectories correspond exactly to classical null geodesics in a hybrid\nSchwarzschild-Alcubierre spacetime. This threefold equivalence demonstrates how\nquantum theory in curved spacetime can be formulated via operationally-defined\nmeasurements, and that such a theory may be interpreted deterministically, in\nthe spirit of hidden-variable models such as Bohmian mechanics, through the\nnovel connection to an underlying \"guiding metric.\"",
        "Physics-informed neural networks (PINNs) have earned high expectations in\nsolving partial differential equations (PDEs), but their optimization usually\nfaces thorny challenges due to the unique derivative-dependent loss function.\nBy analyzing the loss distribution, previous research observed the propagation\nfailure phenomenon of PINNs, intuitively described as the correct supervision\nfor model outputs cannot ``propagate'' from initial states or boundaries to the\ninterior domain. Going beyond intuitive understanding, this paper provides the\nfirst formal and in-depth study of propagation failure and its root cause.\nBased on a detailed comparison with classical finite element methods, we\nascribe the failure to the conventional single-point-processing architecture of\nPINNs and further prove that propagation failure is essentially caused by the\nlower gradient correlation of PINN models on nearby collocation points.\nCompared to superficial loss maps, this new perspective provides a more precise\nquantitative criterion to identify where and why PINN fails. The theoretical\nfinding also inspires us to present a new PINN architecture, named ProPINN,\nwhich can effectively unite the gradient of region points for better\npropagation. ProPINN can reliably resolve PINN failure modes and significantly\nsurpass advanced Transformer-based models with 46% relative promotion.",
        "Fact-checking tabular data is essential for ensuring the accuracy of\nstructured information. However, existing methods often rely on black-box\nmodels with opaque reasoning. We introduce RePanda, a structured fact\nverification approach that translates claims into executable pandas queries,\nenabling interpretable and verifiable reasoning.\n  To train RePanda, we construct PanTabFact, a structured dataset derived from\nthe TabFact train set, where claims are paired with executable queries\ngenerated using DeepSeek-Chat and refined through automated error correction.\nFine-tuning DeepSeek-coder-7B-instruct-v1.5 on PanTabFact, RePanda achieves\n84.09% accuracy on the TabFact test set.\n  To evaluate Out-of-Distribution (OOD) generalization, we interpret\nquestion-answer pairs from WikiTableQuestions as factual claims and refer to\nthis dataset as WikiFact. Without additional fine-tuning, RePanda achieves\n84.72% accuracy on WikiFact, significantly outperforming all other baselines\nand demonstrating strong OOD robustness. Notably, these results closely match\nthe zero-shot performance of DeepSeek-Chat (671B), indicating that our\nfine-tuning approach effectively distills structured reasoning from a much\nlarger model into a compact, locally executable 7B model.\n  Beyond fact verification, RePanda extends to tabular question answering by\ngenerating executable queries that retrieve precise answers. To support this,\nwe introduce PanWiki, a dataset mapping WikiTableQuestions to pandas queries.\nFine-tuning on PanWiki, RePanda achieves 75.1% accuracy in direct answer\nretrieval. These results highlight the effectiveness of structured\nexecution-based reasoning for tabular verification and question answering.\n  We have publicly released the dataset on Hugging Face at\ndatasets\/AtoosaChegini\/PanTabFact.",
        "Understanding collaborative writing dynamics between native speakers (NS) and\nnon-native speakers (NNS) is critical for enhancing collaboration quality and\nteam inclusivity. In this paper, we partnered with communication researchers to\ndevelop visual analytics solutions for comparing NS and NNS behaviors in 162\nwriting sessions across 27 teams. The primary challenges in analyzing writing\nbehaviors are data complexity and the uncertainties introduced by automated\nmethods. In response, we present \\textsc{COALA}, a novel visual analytics tool\nthat improves model interpretability by displaying uncertainties in author\nclusters, generating behavior summaries using large language models, and\nvisualizing writing-related actions at multiple granularities. We validated the\neffectiveness of \\textsc{COALA} through user studies with domain experts\n(N=2+2) and researchers with relevant experience (N=8). We present the insights\ndiscovered by participants using \\textsc{COALA}, suggest features for future\nAI-assisted collaborative writing tools, and discuss the broader implications\nfor analyzing collaborative processes beyond writing.",
        "We present a multifractal artificial terrain generation method that uses the\n3D Weierstrass-Mandelbrot function to control roughness. By varying the fractal\ndimension used in terrain generation across three different values, we generate\n60 unique off-road terrains. We use gradient maps to categorize the roughness\nof each terrain, consisting of low-, semi-, and high-roughness areas. To test\nhow the fractal dimension affects the difficulty of vehicle traversals, we\nmeasure the success rates, vertical accelerations, pitch and roll rates, and\ntraversal times of an autonomous ground vehicle traversing 20 randomized\nstraight-line paths in each terrain. As we increase the fractal dimension from\n2.3 to 2.45 and from 2.45 to 2.6, we find that the median area of low-roughness\nterrain decreases 13.8% and 7.16%, the median area of semi-rough terrain\nincreases 11.7% and 5.63%, and the median area of high-roughness terrain\nincreases 1.54% and 3.33%, all respectively. We find that the median success\nrate of the vehicle decreases 22.5% and 25% as the fractal dimension increases\nfrom 2.3 to 2.45 and from 2.45 to 2.6, respectively. Successful traversal\nresults show that the median root-mean-squared vertical accelerations, median\nroot-mean-squared pitch and roll rates, and median traversal times all increase\nwith the fractal dimension.",
        "In this paper, we propose a robust method for detecting guilty actors in\nimage steganography while effectively addressing the Cover Source Mismatch\n(CSM) problem, which arises when classifying images from one source using a\nclassifier trained on images from another source. Designed for an actor-based\nscenario, our method combines the use of Detection of Classifier\nInconsistencies (DCI) prediction with EfficientNet neural networks for feature\nextraction, and a Gradient Boosting Machine for the final classification. The\nproposed approach successfully determines whether an actor is innocent or\nguilty, or if they should be discarded due to excessive CSM. We show that the\nmethod remains reliable even in scenarios with high CSM, consistently achieving\naccuracy above 80% and outperforming the baseline method. This novel approach\ncontributes to the field of steganalysis by offering a practical and efficient\nsolution for handling CSM and detecting guilty actors in real-world\napplications.",
        "Accurate RF propagation modeling in urban environments is critical for\ndeveloping digital spectrum twins and optimizing wireless communication\nsystems. We introduce OpenGERT, an open-source automated Geometry Extraction\ntool for Ray Tracing, which collects and processes terrain and building data\nfrom OpenStreetMap, Microsoft Global ML Building Footprints, and USGS elevation\ndata. Using the Blender Python API, it creates detailed urban models for\nhigh-fidelity simulations with NVIDIA Sionna RT. We perform sensitivity\nanalyses to examine how variations in building height, position, and\nelectromagnetic material properties affect ray-tracing accuracy. Specifically,\nwe present pairwise dispersion plots of channel statistics (path gain, mean\nexcess delay, delay spread, link outage, and Rician K-factor) and investigate\nhow their sensitivities change with distance from transmitters. We also\nvisualize the variance of these statistics for selected transmitter locations\nto gain deeper insights. Our study covers Munich and Etoile scenes, each with\n10 transmitter locations. For each location, we apply five types of\nperturbations: material, position, height, height-position, and all combined,\nwith 50 perturbations each. Results show that small changes in permittivity and\nconductivity minimally affect channel statistics, whereas variations in\nbuilding height and position significantly alter all statistics, even with\nnoise standard deviations of 1 meter in height and 0.4 meters in position.\nThese findings highlight the importance of precise environmental modeling for\naccurate propagation predictions, essential for digital spectrum twins and\nadvanced communication networks. The code for geometry extraction and\nsensitivity analyses is available at github.com\/serhatadik\/OpenGERT\/.",
        "Objectives: Large language models (LLMs) can harness medical knowledge for\nintelligent question answering (Q&A), promising support for auxiliary diagnosis\nand medical talent cultivation. However, there is a deficiency of highly\nefficient retrieval-augmented generation (RAG) frameworks within the domain of\nTraditional Chinese Medicine (TCM). Our purpose is to observe the effect of the\nTree-Organized Self-Reflective Retrieval (TOSRR) framework on LLMs in TCM Q&A\ntasks.\n  Materials and Methods: We introduce the novel approach of knowledge\norganization, constructing a tree structure knowledge base with hierarchy. At\ninference time, our self-reflection framework retrieves from this knowledge\nbase, integrating information across chapters. Questions from the TCM Medical\nLicensing Examination (MLE) and the college Classics Course Exam (CCE) were\nrandomly selected as benchmark datasets.\n  Results: By coupling with GPT-4, the framework can improve the best\nperformance on the TCM MLE benchmark by 19.85% in absolute accuracy, and\nimprove recall accuracy from 27% to 38% on CCE datasets. In manual evaluation,\nthe framework improves a total of 18.52 points across dimensions of safety,\nconsistency, explainability, compliance, and coherence.\n  Conclusion: The TOSRR framework can effectively improve LLM's capability in\nQ&A tasks of TCM.",
        "We study the application of a neural network architecture for identifying\ncharged particle trajectories via unsupervised learning of delays and synaptic\nweights using a spike-time-dependent plasticity rule. In the considered model,\nthe neurons receive time-encoded information on the position of particle hits\nin a tracking detector for a particle collider, modeled according to the\ngeometry of the Compact Muon Solenoid Phase II detector. We show how a spiking\nneural network is capable of successfully identifying in a completely\nunsupervised way the signal left by charged particles in the presence of\nconspicuous noise from accidental or combinatorial hits. These results open the\nway to applications of neuromorphic computing to particle tracking, motivating\nfurther studies into its potential for real-time, low-power particle tracking\nin future high-energy physics experiments.",
        "Language models pretrained on text-only corpora often struggle with tasks\nthat require auditory commonsense knowledge. Previous work addresses this\nproblem by augmenting the language model to retrieve knowledge from external\naudio databases. This approach has several limitations, such as the potential\nlack of relevant audio in databases and the high costs associated with\nconstructing and querying the databases. To address these issues, we propose\nImagine to Hear, a novel approach that dynamically generates auditory knowledge\nusing generative models. Our framework detects multiple audio-related textual\nspans from the given prompt and generates corresponding auditory knowledge. We\ndevelop several mechanisms to efficiently process multiple auditory knowledge,\nincluding a CLAP-based rejection sampler and a language-audio fusion module.\nOur experiments show that our method achieves state-of-the-art performance on\nAuditoryBench without relying on external databases, highlighting the\neffectiveness of our generation-based approach.",
        "Diffusion models are a powerful framework for tackling ill-posed problems,\nwith recent advancements extending their use to point cloud upsampling. Despite\ntheir potential, existing diffusion models struggle with inefficiencies as they\nmap Gaussian noise to real point clouds, overlooking the geometric information\ninherent in sparse point clouds. To address these inefficiencies, we propose\nPUFM, a flow matching approach to directly map sparse point clouds to their\nhigh-fidelity dense counterparts. Our method first employs midpoint\ninterpolation to sparse point clouds, resolving the density mismatch between\nsparse and dense point clouds. Since point clouds are unordered\nrepresentations, we introduce a pre-alignment method based on Earth Mover's\nDistance (EMD) optimization to ensure coherent interpolation between sparse and\ndense point clouds, which enables a more stable learning path in flow matching.\nExperiments on synthetic datasets demonstrate that our method delivers superior\nupsampling quality but with fewer sampling steps. Further experiments on\nScanNet and KITTI also show that our approach generalizes well on RGB-D point\nclouds and LiDAR point clouds, making it more practical for real-world\napplications.",
        "In a proof of knowledge (PoK), a verifier becomes convinced that a prover\npossesses privileged information. In combination with zero-knowledge proof\nsystems, PoKs are an important part of secure protocols such as digital\nsignature schemes and authentication schemes as they enable a prover to\ndemonstrate possession of a certain piece of information (such as a private key\nor a credential), without revealing it. Formally, A PoK is defined via the\nexistence of an extractor, which is capable of reconstructing the key\ninformation that makes a verifier accept, given oracle access to the prover. We\nextend the concept of a PoK in the setting of a single classical verifier and\ntwo quantum provers, and exhibit the PoK property for a non-local game for the\nlocal Hamiltonian problem. More specifically, we construct an extractor which,\ngiven oracle access to a provers' strategy that leads to high acceptance\nprobability, is able to reconstruct the ground state of a local Hamiltonian.\nOur result can be seen as a new form of self-testing, where, in addition to\ncertifying a pre-shared entangled state and the prover's strategy, the verifier\nalso certifies a local quantum state. This technique thus provides a method to\nascertain that a prover has access to a quantum system, in particular, a ground\nstate, thus indicating a new level of verification for a proof of quantumness.",
        "In the transient stability analysis of renewable energy grid-tied systems,\nalthough a large amount of works have devoted to the detailed electromagnetic\ntransient simulation and the stability analyses of during-fault stage, the\nwhole low-voltage ride through (LVRT) process and relevant transient stability\nmechanism remain to be uncovered. Taking the doubly fed induction generator\nsystem as the objective, this paper divides the transient processes into four\ndifferent stages, including the pre-fault, during-fault, early post-fault, and\nlate post-fault ones, establishes the full mechanism models for each stage, and\nstudies the switching dynamics in detail. It is found that the during-fault\ndynamics can be determined by the phase-lock loop second-order equation within\nthe framework of the generalized swing equation (GSE). For the early post-fault\nstage, it can be treated as a series of quasi-steady states and its dominant\ndriving system dynamics can still be described by the GSE. Based on the local\ndynamics of unstable equilibrium point, the system transient stability can be\ncompletely determined by whether the initial state of the early post-fault\nstage is within or out of its basin of attraction (BOA). Based on these\nobservations, the BOA-based and equal area criterion (EAC)-based transient\nstability assessment methods are developed, which are supported by broad\nnumerical simulations and hardware-in-the-loop experiments. This work provides\na clear physical picture and perfectly solves the difficult stability analysis\nproblem when severe faults and LVRT have to be considered in most of DFIG\nengineering situations.",
        "Existing supervised action segmentation methods depend on the quality of\nframe-wise classification using attention mechanisms or temporal convolutions\nto capture temporal dependencies. Even boundary detection-based methods\nprimarily depend on the accuracy of an initial frame-wise classification, which\ncan overlook precise identification of segments and boundaries in case of\nlow-quality prediction. To address this problem, this paper proposes ASESM\n(Action Segmentation via Explicit Similarity Measurement) to enhance the\nsegmentation accuracy by incorporating explicit similarity evaluation across\nframes and predictions. Our supervised learning architecture uses frame-level\nmulti-resolution features as input to multiple Transformer encoders. The\nresulting multiple frame-wise predictions are used for similarity voting to\nobtain high quality initial prediction. We apply a newly proposed boundary\ncorrection algorithm that operates based on feature similarity between\nconsecutive frames to adjust the boundary locations iteratively through the\nlearning process. The corrected prediction is then further refined through\nmultiple stages of temporal convolutions. As post-processing, we optionally\napply boundary correction again followed by a segment smoothing method that\nremoves outlier classes within segments using similarity measurement between\nconsecutive predictions. Additionally, we propose a fully unsupervised boundary\ndetection-correction algorithm that identifies segment boundaries based solely\non feature similarity without any training. Experiments on 50Salads, GTEA, and\nBreakfast datasets show the effectiveness of both the supervised and\nunsupervised algorithms. Code and models are made available on Github.",
        "Variate tokenization, which independently embeds each variate as separate\ntokens, has achieved remarkable improvements in multivariate time series\nforecasting. However, employing self-attention with variate tokens incurs a\nquadratic computational cost with respect to the number of variates, thus\nlimiting its training efficiency for large-scale applications. To address this\nissue, we propose VarDrop, a simple yet efficient strategy that reduces the\ntoken usage by omitting redundant variate tokens during training. VarDrop\nadaptively excludes redundant tokens within a given batch, thereby reducing the\nnumber of tokens used for dot-product attention while preserving essential\ninformation. Specifically, we introduce k-dominant frequency hashing (k-DFH),\nwhich utilizes the ranked dominant frequencies in the frequency domain as a\nhash value to efficiently group variate tokens exhibiting similar periodic\nbehaviors. Then, only representative tokens in each group are sampled through\nstratified sampling. By performing sparse attention with these selected tokens,\nthe computational cost of scaled dot-product attention is significantly\nalleviated. Experiments conducted on public benchmark datasets demonstrate that\nVarDrop outperforms existing efficient baselines.",
        "The objective of this work is to align asynchronous subtitles in sign\nlanguage videos with limited labelled data. To achieve this goal, we propose a\nnovel framework with the following contributions: (1) we leverage fundamental\ngrammatical rules of British Sign Language (BSL) to pre-process the input\nsubtitles, (2) we design a selective alignment loss to optimise the model for\npredicting the temporal location of signs only when the queried sign actually\noccurs in a scene, and (3) we conduct self-training with refined pseudo-labels\nwhich are more accurate than the heuristic audio-aligned labels. From this, our\nmodel not only better understands the correlation between the text and the\nsigns, but also holds potential for application in the translation of sign\nlanguages, particularly in scenarios where manual labelling of large-scale sign\ndata is impractical or challenging. Extensive experimental results demonstrate\nthat our approach achieves state-of-the-art results, surpassing previous\nbaselines by substantial margins in terms of both frame-level accuracy and\nF1-score. This highlights the effectiveness and practicality of our framework\nin advancing the field of sign language video alignment and translation.",
        "Recently, there has been a significant advancement in designing\nSelf-Supervised Learning (SSL) frameworks for time series data to reduce the\ndependency on data labels. Among these works, hierarchical contrastive\nlearning-based SSL frameworks, which learn representations by contrasting data\nembeddings at multiple resolutions, have gained considerable attention. Due to\ntheir ability to gather more information, they exhibit better generalization in\nvarious downstream tasks. However, when the time series data length is\nsignificant long, the computational cost is often significantly higher than\nthat of other SSL frameworks. In this paper, to address this challenge, we\npropose an efficient way to train hierarchical contrastive learning models.\nInspired by the fact that each resolution's data embedding is highly dependent,\nwe introduce importance-aware resolution selection based training framework to\nreduce the computational cost. In the experiment, we demonstrate that the\nproposed method significantly improves training time while preserving the\noriginal model's integrity in extensive time series classification performance\nevaluations. Our code could be found here, https:\/\/github.com\/KEEBVIN\/IARS",
        "Counterfactual explanations have emerged as a prominent method in Explainable\nArtificial Intelligence (XAI), providing intuitive and actionable insights into\nMachine Learning model decisions. In contrast to other traditional feature\nattribution methods that assess the importance of input variables,\ncounterfactual explanations focus on identifying the minimal changes required\nto alter a model's prediction, offering a ``what-if'' analysis that is close to\nhuman reasoning. In the context of XAI, counterfactuals enhance transparency,\ntrustworthiness and fairness, offering explanations that are not just\ninterpretable but directly applicable in the decision-making processes.\n  In this paper, we present a novel framework that integrates perturbation\ntheory and statistical mechanics to generate minimal counterfactual\nexplanations in explainable AI. We employ a local Taylor expansion of a Machine\nLearning model's predictive function and reformulate the counterfactual search\nas an energy minimization problem over a complex landscape. In sequence, we\nmodel the probability of candidate perturbations leveraging the Boltzmann\ndistribution and use simulated annealing for iterative refinement. Our approach\nsystematically identifies the smallest modifications required to change a\nmodel's prediction while maintaining plausibility. Experimental results on\nbenchmark datasets for cybersecurity in Internet of Things environments,\ndemonstrate that our method provides actionable, interpretable counterfactuals\nand offers deeper insights into model sensitivity and decision boundaries in\nhigh-dimensional spaces."
      ]
    }
  },
  {
    "id":2411.00614,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"Causal identification of single-cell experimental perturbation effects with CINEMA-OT",
    "start_abstract":"Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion"
      ],
      "abstract":[
        "Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Nuclear matter in relativistic Brueckner-Hartree-Fock theory with local\n  and nonlocal covariant chiral interactions at leading order",
        "Nucleolus Credit Assignment for Effective Coalitions in Multi-agent\n  Reinforcement Learning",
        "Mechanics and Design of Metastructured Auxetic Patches with Bio-inspired\n  Materials",
        "Rethinking High-speed Image Reconstruction Framework with Spike Camera",
        "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
        "Optimal disk packing of chloroplasts in plant cells",
        "Are the Majority of Public Computational Notebooks Pathologically\n  Non-Executable?",
        "Native Three-Body Interactions in a Superconducting Lattice Gauge\n  Quantum Simulator",
        "Test Schedule Generation for Acceptance Testing of Mission-Critical\n  Satellite Systems",
        "A novel approach to data generation in generative model",
        "Towards Location-Specific Precipitation Projections Using Deep Neural\n  Networks",
        "Rotatable Antenna Enabled Wireless Communication: Modeling and\n  Optimization",
        "Adapting Automatic Speech Recognition for Accented Air Traffic Control\n  Communications",
        "A spatially varying differential equation for multi-patch pandemic\n  propagation",
        "Quantification of Uncertainties in Probabilistic Deep Neural Network by\n  Implementing Boosting of Variational Inference",
        "Loop Quantum Gravitational Signatures via Love Numbers",
        "Genetic algorithm enhanced Solovay-Kitaev algorithm for quantum\n  compiling",
        "Boosting MCSat Modulo Nonlinear Integer Arithmetic via Local Search",
        "You Can't Eat Your Cake and Have It Too: The Performance Degradation of\n  LLMs with Jailbreak Defense",
        "A Differentiable Rank-Based Objective For Better Feature Learning",
        "A framework for IoT-Enabled Smart Agriculture",
        "ClassInvGen: Class Invariant Synthesis using Large Language Models",
        "Language Models Can See Better: Visual Contrastive Decoding For LLM\n  Multimodal Reasoning",
        "Quasi-one-dimensional Supersolids in Luther-Emery Liquids",
        "Filament Mass Losses Forced by Magnetic Reconnection in the Solar Corona",
        "$\\phi$-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation",
        "Bayesian optimization of beam injection and storage in the PSI muEDM\n  Experiment",
        "Financial Fraud Detection with Entropy Computing",
        "Euclid Quick Data Release (Q1). First Euclid statistical study of the\n  active galactic nuclei contribution fraction"
      ],
      "abstract":[
        "The simultaneous description for nuclear matter and finite nuclei has been a\nlong-standing challenge in nuclear ab initio theory. With the success for\nnuclear matter, the relativistic Brueckner-Hartree-Fock (RBHF) theory with\ncovariant chiral interactions is a promising ab initio approach to describe\nboth nuclear matter and finite nuclei. In the description of the finite nuclei\nwith the current RBHF theory, the covariant chiral interactions have to be\nlocalized to make calculations feasible. In order to examine the reliability\nand validity, in this letter, the RBHF theory with local and nonlocal covariant\nchiral interactions at leading order are applied for nuclear matter. The\nlow-energy constants in the covariant chiral interactions determined with the\nlocal regularization are close to those with the nonlocal regularization.\nMoreover, the RBHF theory with local and nonlocal covariant chiral interactions\nprovide equally well description of the saturation properties of nuclear\nmatter. The present work paves the way for the implementation of covariant\nchiral interactions in RBHF theory for finite nuclei.",
        "In cooperative multi-agent reinforcement learning (MARL), agents typically\nform a single grand coalition based on credit assignment to tackle a composite\ntask, often resulting in suboptimal performance. This paper proposed a\nnucleolus-based credit assignment grounded in cooperative game theory, enabling\nthe autonomous partitioning of agents into multiple small coalitions that can\neffectively identify and complete subtasks within a larger composite task.\nSpecifically, our designed nucleolus Q-learning could assign fair credits to\neach agent, and the nucleolus Q-operator provides theoretical guarantees with\ninterpretability for both learning convergence and the stability of the formed\nsmall coalitions. Through experiments on Predator-Prey and StarCraft scenarios\nacross varying difficulty levels, our approach demonstrated the emergence of\nmultiple effective coalitions during MARL training, leading to faster learning\nand superior performance in terms of win rate and cumulative rewards especially\nin hard and super-hard environments, compared to four baseline methods. Our\nnucleolus-based credit assignment showed the promise for complex composite\ntasks requiring effective subteams of agents.",
        "Metastructured auxetic patches, characterized by negative Poisson's ratios,\noffer unique mechanical properties that closely resemble the behavior of human\ntissues and organs. As a result, these patches have gained significant\nattention for their potential applications in organ repair and tissue\nregeneration. This study focuses on neural networks-based computational\nmodeling of auxetic patches with a sinusoidal metastructure fabricated from\nsilk fibroin, a bio-inspired material known for its biocompatibility and\nstrength. The primary objective of this research is to introduce a novel,\ndata-driven framework for patch design. To achieve this, we conducted\nexperimental fabrication and mechanical testing to determine material\nproperties and validate the corresponding finite element models. Finite element\nsimulations were then employed to generate the necessary data, while greedy\nsampling, an active learning technique, was utilized to reduce the\ncomputational cost associated with data labeling. Two neural networks were\ntrained to accurately predict Poisson's ratios and stresses for strains up to\n15\\%, respectively. Both models achieved $R^2$ scores exceeding 0.995, which\nindicates highly reliable predictions. Building on this, we developed a neural\nnetwork-based design model capable of tailoring patch designs to achieve\nspecific mechanical properties. This model demonstrated superior performance\nwhen compared to traditional optimization methods, such as genetic algorithms,\nby providing more efficient and precise design solutions. The proposed\nframework represents a significant advancement in the design of bio-inspired\nmetastructures for medical applications, paving the way for future innovations\nin tissue engineering and regenerative medicine.",
        "Spike cameras, as innovative neuromorphic devices, generate continuous spike\nstreams to capture high-speed scenes with lower bandwidth and higher dynamic\nrange than traditional RGB cameras. However, reconstructing high-quality images\nfrom the spike input under low-light conditions remains challenging.\nConventional learning-based methods often rely on the synthetic dataset as the\nsupervision for training. Still, these approaches falter when dealing with\nnoisy spikes fired under the low-light environment, leading to further\nperformance degradation in the real-world dataset. This phenomenon is primarily\ndue to inadequate noise modelling and the domain gap between synthetic and real\ndatasets, resulting in recovered images with unclear textures, excessive noise,\nand diminished brightness. To address these challenges, we introduce a novel\nspike-to-image reconstruction framework SpikeCLIP that goes beyond traditional\ntraining paradigms. Leveraging the CLIP model's powerful capability to align\ntext and images, we incorporate the textual description of the captured scene\nand unpaired high-quality datasets as the supervision. Our experiments on\nreal-world low-light datasets U-CALTECH and U-CIFAR demonstrate that SpikeCLIP\nsignificantly enhances texture details and the luminance balance of recovered\nimages. Furthermore, the reconstructed images are well-aligned with the broader\nvisual features needed for downstream tasks, ensuring more robust and versatile\nperformance in challenging environments.",
        "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
        "Photosynthesis is vital for the survival of entire ecosystems on Earth. While\nlight is fundamental to this process, excessive exposure can be detrimental to\nplant cells. Chloroplasts, the photosynthetic organelles, actively move in\nresponse to light and self-organize within the cell to tune light absorption.\nThese disk-shaped motile organelles must balance dense packing for enhanced\nlight absorption under dim conditions with spatial rearrangements to avoid\ndamage from excessive light exposure. Here, we reveal that the packing\ncharacteristics of chloroplasts within plant cells show signatures of\noptimality. Combining measurements of chloroplast densities and\nthree-dimensional cell shape in the water plant Elodea densa, we construct an\nargument for optimal cell shape versus chloroplast size to achieve two targets:\ndense packing into a two-dimensional monolayer for optimal absorption under dim\nlight conditions and packing at the sidewalls for optimal light avoidance. We\nformalize these constraints using a model for random close packing matched with\npacking simulations of polydisperse hard disks confined within rectangular\nboxes. The optimal cell shape resulting from these models corresponds closely\nto that measured in the box-like plant cells, highlighting the importance of\nparticle packing in the light adaptation of plants. Understanding the interplay\nbetween structure and function sheds light on how plants achieve efficient\nphoto adaptation. It also highlights a broader principle: how cell shape\nrelates to the optimization of packing finite and relatively small numbers of\norganelles under confinement. This universal challenge in biological systems\nshares fundamental features with the mechanics of confined granular media and\nthe jamming transitions in dense active and passive systems across various\nscales and contexts.",
        "Computational notebooks are the de facto platforms for exploratory data\nscience, offering an interactive programming environment where users can\ncreate, modify, and execute code cells in any sequence. However, this\nflexibility often introduces code quality issues, with prior studies showing\nthat approximately 76% of public notebooks are non-executable, raising\nsignificant concerns about reusability. We argue that the traditional notion of\nexecutability - requiring a notebook to run fully and without error - is overly\nrigid, misclassifying many notebooks and overestimating their\nnon-executability. This paper investigates pathological executability issues in\npublic notebooks under varying notions and degrees of executability. Even\npartially improving executability can improve code comprehension and offer a\npathway for dynamic analyses. With this insight, we first categorize notebooks\ninto potentially restorable and pathological non-executable notebooks and then\nmeasure how removing misconfiguration and superficial execution issues in\nnotebooks can improve their executability (i.e., additional cells executed\nwithout error). In a dataset of 42,546 popular public notebooks containing\n34,659 non-executable notebooks, only 21.3% are truly pathologically\nnon-executable. For restorable notebooks, LLM-based methods fully restore 5.4%\nof previously non-executable notebooks. Among the partially restored, the\nexecutability of notebooks improves by 42.7% and 28% by installing the correct\nmodules and generating synthetic data. These findings challenge prior\nassumptions, suggesting that notebooks have higher executability than\npreviously reported, many of which offer valuable partial execution, and that\ntheir executability should be evaluated within the interactive notebook\nparadigm rather than through traditional software executability standards.",
        "While universal quantum computers remain under development, analog quantum\nsimulators offer a powerful alternative for understanding complex systems in\ncondensed matter, chemistry, and high-energy physics. One compelling\napplication is the characterization of real-time lattice gauge theories (LGTs).\nLGTs are nonperturbative tools, utilizing discretized spacetime to describe\ngauge-invariant models. They hold immense potential for understanding\nfundamental physics but require enforcing local constraints analogous to\nelectromagnetism's Gauss's Law. These constraints, which arise from gauge\nsymmetries and dictate the form of the interaction between matter and gauge\nfields, are a significant challenge for simulators to enforce. Implementing\nthese constraints at the hardware level in analog simulations is crucial. This\nrequires realizing multibody interactions between matter and gauge-field\nelements, enabling them to evolve together while suppressing unwanted two-body\ninteractions that violate the gauge symmetry. In this paper, we propose and\nimplement a novel parametrically activated three-qubit interaction within a\ncircuit quantum electrodynamics architecture. We experimentally demonstrate a\nminimal $U(1)$ spin-1\/2 model with a time evolution that intrinsically\nsatisfies Gauss's law in the system. This design serves as the foundational\nblock for simulating LGTs on a superconducting photonic lattice.",
        "Mission-critical system, such as satellite systems, healthcare systems, and\nnuclear power plant control systems, undergo rigorous testing to ensure they\nmeet specific operational requirements throughout their operation. This\nincludes Operational Acceptance Testing (OAT), which aims to ensure that the\nsystem functions correctly under real-world operational conditions. In\nsatellite development, In-Orbit Testing (IOT) is a crucial OAT activity\nperformed regularly and as needed after deployment in orbit to check the\nsatellite's performance and ensure that operational requirements are met. The\nscheduling of an IOT campaign, which executes multiple IOT procedures, is an\nimportant yet challenging problem, as it accounts for various factors,\nincluding satellite visibility, antenna usage costs, testing time periods, and\noperational constraints. To address the IOT scheduling problem, we propose a\nmulti-objective approach to generate near-optimal IOT schedules, accounting for\noperational costs, fragmentation (i.e., the splitting of tests), and resource\nefficiency, which align with practitioners' objectives for IOT scheduling. Our\nindustrial case study with SES Techcom shows significant improvements, as\nfollows: an average improvement of 49.4% in the cost objective, 60.4% in the\nfragmentation objective, and 30% in the resource usage objective, compared to\nour baselines. Additionally, our approach improves cost efficiency by 538% and\nresource usage efficiency by 39.42% compared to manually constructed schedules\nprovided by practitioners, while requiring only 12.5% of the time needed for\nmanual IOT scheduling.",
        "Variational Autoencoders (VAEs) and other generative models are widely\nemployed in artificial intelligence to synthesize new data. However, current\napproaches rely on Euclidean geometric assumptions and statistical\napproximations that fail to capture the structured and emergent nature of data\ngeneration. This paper introduces the Convergent Fusion Paradigm (CFP) theory,\na novel geometric framework that redefines data generation by integrating\ndimensional expansion accompanied by qualitative transformation. By modifying\nthe latent space geometry to interact with emergent high-dimensional\nstructures, CFP theory addresses key challenges such as identifiability issues\nand unintended artifacts like hallucinations in Large Language Models (LLMs).\nCFP theory is based on two key conceptual hypotheses that redefine how\ngenerative models structure relationships between data and algorithms. Through\nthe lens of CFP theory, we critically examine existing metric-learning\napproaches. CFP theory advances this perspective by introducing time-reversed\nmetric embeddings and structural convergence mechanisms, leading to a novel\ngeometric approach that better accounts for data generation as a structured\nepistemic process. Beyond its computational implications, CFP theory provides\nphilosophical insights into the ontological underpinnings of data generation.\nBy offering a systematic framework for high-dimensional learning dynamics, CFP\ntheory contributes to establishing a theoretical foundation for understanding\nthe data-relationship structures in AI. Finally, future research in CFP theory\nwill be led to its implications for fully realizing qualitative\ntransformations, introducing the potential of Hilbert space in generative\nmodeling.",
        "Accurate precipitation estimates at individual locations are crucial for\nweather forecasting and spatial analysis. This study presents a paradigm shift\nby leveraging Deep Neural Networks (DNNs) to surpass traditional methods like\nKriging for station-specific precipitation approximation. We propose two\ninnovative NN architectures: one utilizing precipitation, elevation, and\nlocation, and another incorporating additional meteorological parameters like\nhumidity, temperature, and wind speed. Trained on a vast dataset (1980-2019),\nthese models outperform Kriging across various evaluation metrics (correlation\ncoefficient, root mean square error, bias, and skill score) on a five-year\nvalidation set. This compelling evidence demonstrates the transformative power\nof deep learning for spatial prediction, offering a robust and precise\nalternative for station-specific precipitation estimation.",
        "Fluid antenna system (FAS) and movable antenna (MA) have recently emerged as\npromising technologies to exploit new spatial degrees of freedom (DoFs), which\nhave attracted growing attention in wireless communication. In this paper, we\npropose a new rotatable antenna (RA) model to improve the performance of\nwireless communication systems. Different from conventional fixed antennas, the\nproposed RA system can flexibly alter the three-dimensional (3D) boresight\ndirection of each antenna independently by adjusting its deflection angles to\nachieve a desired array directional gain pattern. Specifically, we investigate\nan RA-enabled uplink communication system, where the receive beamforming and\nthe deflection angles of all RAs at the base station (BS) are jointly optimized\nto maximize the minimum signal-to-interference-plus-noise ratio (SINR) among\nall the users. In the special single-user and free-space propagation setup, the\noptimal deflection angles of RAs are derived in closed form with the\nmaximum-ratio combining (MRC) beamformer applied at the BS. Moreover, we\nanalyze the asymptotic performance with an infinite number of antennas based on\nthis solution, which theoretically proves that the RA system can achieve a\nhigher array gain as compared to the fixed-antenna system. In the general\nmulti-user and multi-path channel setup, we first propose an alternating\noptimization (AO) algorithm to alternately optimize the receive beamforming and\nthe deflection angles of RAs in an iterative manner. Then, a two-stage\nalgorithm that solves the formulated problem without the need for iteration is\nfurther proposed to reduce computational complexity. Simulation results are\nprovided to validate our analytical results and demonstrate that the proposed\nRA system can significantly outperform other benchmark schemes.",
        "Effective communication in Air Traffic Control (ATC) is critical to\nmaintaining aviation safety, yet the challenges posed by accented English\nremain largely unaddressed in Automatic Speech Recognition (ASR) systems.\nExisting models struggle with transcription accuracy for Southeast\nAsian-accented (SEA-accented) speech, particularly in noisy ATC environments.\nThis study presents the development of ASR models fine-tuned specifically for\nSoutheast Asian accents using a newly created dataset. Our research achieves\nsignificant improvements, achieving a Word Error Rate (WER) of 0.0982 or 9.82%\non SEA-accented ATC speech. Additionally, the paper highlights the importance\nof region-specific datasets and accent-focused training, offering a pathway for\ndeploying ASR systems in resource-constrained military operations. The findings\nemphasize the need for noise-robust training techniques and region-specific\ndatasets to improve transcription accuracy for non-Western accents in ATC\ncommunications.",
        "We develop an extension of the Susceptible-Infected-Recovery (SIR) model to\naccount for spatial variations in population as well as infection and recovery\nparameters. The equations are derived by taking the continuum limit of discrete\ninteracting patches, and results in a diffusion equation with some nonlinear\nterms. The resulting population dynamics can be reinterpreted as a nonlinear\nheat flow equation where the temperature vector captures both infected and\nrecovered populations across multiple patches.",
        "Modern neural network architectures have achieved remarkable accuracies but\nremain highly dependent on their training data, often lacking interpretability\nin their learned mappings. While effective on large datasets, they tend to\noverfit on smaller ones. Probabilistic neural networks, such as those utilizing\nvariational inference, address this limitation by incorporating uncertainty\nestimation through weight distributions rather than point estimates. However,\nstandard variational inference often relies on a single-density approximation,\nwhich can lead to poor posterior estimates and hinder model performance. We\npropose Boosted Bayesian Neural Networks (BBNN), a novel approach that enhances\nneural network weight distribution approximations using Boosting Variational\nInference (BVI). By iteratively constructing a mixture of densities, BVI\nexpands the approximating family, enabling a more expressive posterior that\nleads to improved generalization and uncertainty estimation. While this\napproach increases computational complexity, it significantly enhances accuracy\nan essential tradeoff, particularly in high-stakes applications such as medical\ndiagnostics, where false negatives can have severe consequences. Our\nexperimental results demonstrate that BBNN achieves ~5% higher accuracy\ncompared to conventional neural networks while providing superior uncertainty\nquantification. This improvement highlights the effectiveness of leveraging a\nmixture-based variational family to better approximate the posterior\ndistribution, ultimately advancing probabilistic deep learning.",
        "Loop quantum gravitational effects can resolve the central singularity of\nblack holes while potentially leaving tiny traces of quantization in the\nexterior spacetime. We show the way these residues can, in principle, be\nexplored using tidal Love numbers (TLNs). We consider loop quantized\nSchwarzschild black hole, in particular the Ashtekar-Olmedo-Singh (AOS) model,\nand study the static response to external tidal fields of spin zero (scalar\nfield), spin one (vector field), and spin two (axial gravitational field)\ntypes. We find that, in contrast to the classical theory, where TLNs vanish,\nthey are non-vanishing and negative for all three responses and for all\nmultipoles. Besides, the magnitude of TLNs decreases as the black hole mass\nincreases, and TLNs, in response to the axial gravitational field, have the\nlargest magnitude among these three responses. Our results show that for black\nholes of mass $M \\gtrsim 4.3 \\times 10^{4} M_{\\textrm{Pl}}$, the AOS model is\nconsistent with current and next-generation detection limits for TLNs. Our\nfindings suggest that the quantum deformability of loop quantum black holes,\narising from the inherent fuzziness of spacetime geometry, reveals a\nfundamentally distinct internal structure compared to their classical\ncounterparts. This unique feature manifests as quantum hair, which, in\nprinciple, can be detected by future observations.",
        "Quantum compiling trying to approximate the target qubit gate by finding an\noptimal sequence (braid word) of basic braid operations is a fundamental\nproblem in quantum computing. We develop a genetic algorithm (GA) enhanced\nSolovay-Kitaev algorithm (SKA) to approximate single qubit gates with four\nbasic braid matrices of Fibonacci anyons. The GA-enhanced SKA demonstrates that\nthe algorithm performs strongly and can easily find the ideal braid word from\nan exponentially large space. The resulting precision of the approximate\nsingle-qubit quantum gate is superior to that of the Monte Carlo (MC) enhanced\nSKA, as well as comparable to that of the deep reinforcement learning (RL) for\nthe length of braid word greater than 25. The 2(3)-order approximation of\nGA-enhanced SKA for basic braiding length l0=50(30) leads to an optimal braid\nword at a distance of 5.9*10-7, which is sufficient for most cases of quantum\ncomputing. Our work provides an alternative approach to solving and optimizing\nquantum compilation of non-Abelian anyon quantum gates and is useful for\nrealizing topological quantum computation in the future.",
        "The Model Constructing Satisfiability (MCSat) approach to the SMT problem\nextends the ideas of CDCL from the SAT level to the theory level. Like SAT, its\nsearch is driven by incrementally constructing a model by assigning concrete\nvalues to theory variables and performing theory-level reasoning to learn\nlemmas when conflicts arise. Therefore, the selection of values can\nsignificantly impact the search process and the solver's performance. In this\nwork, we propose guiding the MCSat search by utilizing assignment values\ndiscovered through local search. First, we present a theory-agnostic framework\nto seamlessly integrate local search techniques within the MCSat framework.\nThen, we highlight how to use the framework to design a search procedure for\n(quantifier-free) Nonlinear Integer Arithmetic (NIA), utilizing accelerated\nhill-climbing and a new operation called feasible-sets jumping. We implement\nthe proposed approach in the MCSat engine of the Yices2 solver, and empirically\nevaluate its performance over the N IA benchmarks of SMT-LIB.",
        "With the rise of generative large language models (LLMs) like LLaMA and\nChatGPT, these models have significantly transformed daily life and work by\nproviding advanced insights. However, as jailbreak attacks continue to\ncircumvent built-in safety mechanisms, exploiting carefully crafted scenarios\nor tokens, the safety risks of LLMs have come into focus. While numerous\ndefense strategies--such as prompt detection, modification, and model\nfine-tuning--have been proposed to counter these attacks, a critical question\narises: do these defenses compromise the utility and usability of LLMs for\nlegitimate users? Existing research predominantly focuses on the effectiveness\nof defense strategies without thoroughly examining their impact on performance,\nleaving a gap in understanding the trade-offs between LLM safety and\nperformance. Our research addresses this gap by conducting a comprehensive\nstudy on the utility degradation, safety elevation, and exaggerated-safety\nescalation of LLMs with jailbreak defense strategies. We propose USEBench, a\nnovel benchmark designed to evaluate these aspects, along with USEIndex, a\ncomprehensive metric for assessing overall model performance. Through\nexperiments on seven state-of-the-art LLMs, we found that mainstream jailbreak\ndefenses fail to ensure both safety and performance simultaneously. Although\nmodel-finetuning performs the best overall, their effectiveness varies across\nLLMs. Furthermore, vertical comparisons reveal that developers commonly\nprioritize performance over safety when iterating or fine-tuning their LLMs.",
        "In this paper, we leverage existing statistical methods to better understand\nfeature learning from data. We tackle this by modifying the model-free variable\nselection method, Feature Ordering by Conditional Independence (FOCI), which is\nintroduced in \\cite{azadkia2021simple}. While FOCI is based on a non-parametric\ncoefficient of conditional dependence, we introduce its parametric,\ndifferentiable approximation. With this approximate coefficient of correlation,\nwe present a new algorithm called difFOCI, which is applicable to a wider range\nof machine learning problems thanks to its differentiable nature and learnable\nparameters. We present difFOCI in three contexts: (1) as a variable selection\nmethod with baseline comparisons to FOCI, (2) as a trainable model parametrized\nwith a neural network, and (3) as a generic, widely applicable neural network\nregularizer, one that improves feature learning with better management of\nspurious correlations. We evaluate difFOCI on increasingly complex problems\nranging from basic variable selection in toy examples to saliency map\ncomparisons in convolutional networks. We then show how difFOCI can be\nincorporated in the context of fairness to facilitate classifications without\nrelying on sensitive data.",
        "Unpredictable weather patterns and a lack of timely, accurate information\nsignificantly challenge farmers in Uganda, leading to poor crop management,\nreduced yields, and heightened vulnerability to environmental stress. This\nresearch presents a framework for IoT-enabled smart agriculture, leveraging\nRaspberry Pi-based technology to provide real-time monitoring of weather and\nenvironmental conditions. The framework integrates sensors for temperature,\nrainfall, soil moisture, and pressure, connected via an MCP3208\nanalog-to-digital converter. Data is displayed on an LCD for immediate feedback\nand transmitted to the ThingSpeak platform for centralized storage, analysis,\nand remote access through a mobile app or web interface. Farmers can leverage\nthis framework to optimize irrigation schedules and improve crop productivity\nthrough actionable insights derived from real-time and forecasted data on\nrainfall, temperature, pressure and soil moisture. Additionally, the system\nincorporates predictive weather forecasting to dynamically control sensor\nactivity, reducing energy consumption and extending sensor lifespan. Simulated\nusing Proteus, the proposed framework demonstrates significant potential to\nmitigate the impacts of unpredictable weather by reducing water consumption,\nimproving forecasting accuracy, and boosting productivity.",
        "Formal program specifications in the form of preconditions, postconditions,\nand class invariants have several benefits for the construction and maintenance\nof programs. They not only aid in program understanding due to their\nunambiguous semantics but can also be enforced dynamically (or even statically\nwhen the language supports a formal verifier). However, synthesizing\nhigh-quality specifications in an underlying programming language is limited by\nthe expressivity of the specifications or the need to express them in a\ndeclarative manner. Prior work has demonstrated the potential of large language\nmodels (LLMs) for synthesizing high-quality method pre\/postconditions for\nPython and Java, but does not consider class invariants.\n  In this work, we describe ClassInvGen, a method for co-generating executable\nclass invariants and test inputs to produce high-quality class invariants for a\nmainstream language such as C++, leveraging LLMs' ability to synthesize pure\nfunctions. We show that ClassInvGen outperforms a pure LLM-based technique to\ngenerate specifications (from code) as well as prior data-driven invariant\ninference techniques such as Daikon. We contribute a benchmark of standard C++\ndata structures along with a harness that can help measure both the correctness\nand completeness of generated specifications using tests and mutants. We also\ndemonstrate its applicability to real-world code by performing a case study on\nseveral classes within a widely used and high-integrity C++ codebase.",
        "Although Large Language Models (LLMs) excel in reasoning and generation for\nlanguage tasks, they are not specifically designed for multimodal challenges.\nTraining Multimodal Large Language Models (MLLMs), however, is\nresource-intensive and constrained by various training limitations. In this\npaper, we propose the Modular-based Visual Contrastive Decoding (MVCD)\nframework to move this obstacle. Our framework leverages LLMs' In-Context\nLearning (ICL) capability and the proposed visual contrastive-example decoding\n(CED), specifically tailored for this framework, without requiring any\nadditional training. By converting visual signals into text and focusing on\ncontrastive output distributions during decoding, we can highlight the new\ninformation introduced by contextual examples, explore their connections, and\navoid over-reliance on prior encoded knowledge. MVCD enhances LLMs' visual\nperception to make it see and reason over the input visuals. To demonstrate\nMVCD's effectiveness, we conduct experiments with four LLMs across five\nquestion answering datasets. Our results not only show consistent improvement\nin model accuracy but well explain the effective components inside our decoding\nstrategy. Our code will be available at https:\/\/github.com\/Pbhgit\/MVCD.",
        "The supersolid is a long-sought phase in condensed matter physics,\ncharacterized by the coexistence of density wave and superfluid orders. This\nphase is counterintuitive, as different symmetry-breaking orders typically\ncompete with one another. A deeper understanding of how such a state forms in\ncondensed matter systems remains an open question, especially in\nquasi-one-dimensional correlated systems. In this work, we investigate the\nemergence of supersolids in Luttinger-Emery liquids using a variational method.\nAs the system consists of coupled Luttinger-Emery liquid chains, we refer to\nthis phase as a quasi-one-dimensional supersolid. Notably, we demonstrate that\nthe quasi-one-dimensional supersolid phase is energetically favorable in chains\nwith finite size or short-range order. Furthermore, we investigate the\ncollective dynamics of these coexisting charge density waves and\nsuperconducting states, identifying a quasi-Goldstone mode. Our theory provides\nvaluable insights into both the ground state and the dynamic properties of\nsupersolids in strongly correlated systems.",
        "Recent observations of the solar atmosphere in cool extreme ultraviolet (EUV)\nlines have reported the prevalence of coronal rain falling from coronal cloud\nfilaments that are associated with the magnetic dips of coronal X-point\nstructures. These filaments mysteriously appear as clouds of mass in the corona\nthat subsequently shrink and disappear due to mass losses that drain as coronal\nrain along arced field lines. Using a two and a half dimensional,\nmagnetohydrodynamic model, we investigated evaporation-condensation as the\nformation mechanism of the subset of coronal cloud filaments that form above\ncoronal X-points. Our simulation included the effects of field-aligned thermal\nconduction and optically thin radiation and used the state-of-the-art\nTransition Region Adaptive Conduction (TRAC) method to model the formation,\nmaintenance, and mass loss of a filament above a coronal X-point. This paper\npresents a physical model that demonstrates magnetic reconnection as a filament\nloss mechanism, producing hybrid filament\/coronal rain via mass losses through\nthe X-point. A detailed analysis of how the mass of the filament forces the\nfield to reconnect is also presented, revealing three phases that characterize\nthe evolution of the reconnecting current sheet and associated mass losses. We\nconclude that the formation of certain coronal cloud filaments and subsequent\nmass losses via coronal rain can be explained by the evaporation-condensation\nmodel combined with filament mass losses forced by magnetic reconnection. We\nalso report that rebound shocks generated by the impact of coronal rain\ncondensations on the chromosphere together with retractive upflows can cause\nupward propagating condensations to form through a dynamic thermal runaway\nprocess.",
        "Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed $\\phi$-Decoding. To provide a precise and expressive estimation of step\nvalue, $\\phi$-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show $\\phi$-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https:\/\/github.com\/xufangzhi\/phi-Decoding, and the\nopen-source PyPI package is coming soon.",
        "The muEDM experiment at the Paul Scherrer Institute aims to measure the\nelectric dipole moment with an unprecedented sensitivity of $6 \\times\n10^{-23}\\,\\mathrm{e}\\cdot\\mathrm{cm}$. A key aspect of this experiment is the\ninjection and storage of the muon beam, which traverses a long, narrow\nsuperconducting channel before entering a solenoid magnet. The muon is then\nkicked by a pulsed magnetic field into a stable orbit within the solenoid's\ncentral region, where the electric dipole moment is measured. To study the beam\ninjection and storage process, we developed a G4beamline simulation to model\nthe dynamics of beam injection and storage, incorporating all relevant electric\nand magnetic fields. We subsequently employed a Bayesian optimization technique\nto improve the muon storage efficiency for Phase I of the muEDM experiment. The\noptimization is demonstrated using data simulated by G4beamline. We have\nobserved an enhancement in the beam injection and storage efficiency, which\nincreased to 0.556\\% through the utilization of Bayesian optimization with\nGaussian processes, compared to 0.324\\% when employing the polynomial chaos\nexpansion. This approach can be applied to adjust actual experimental\nparameters, aiding in achieving the desired performance for beam injection and\nstorage in the muEDM experiment.",
        "We introduce CVQBoost, a novel classification algorithm that leverages early\nhardware implementing Quantum Computing Inc's Entropy Quantum Computing (EQC)\nparadigm, Dirac-3 [Nguyen et. al. arXiv:2407.04512]. We apply CVQBoost to a\nfraud detection test case and benchmark its performance against XGBoost, a\nwidely utilized ML method. Running on Dirac-3, CVQBoost demonstrates a\nsignificant runtime advantage over XGBoost, which we evaluate on\nhigh-performance hardware comprising up to 48 CPUs and four NVIDIA L4 GPUs\nusing the RAPIDS AI framework. Our results show that CVQBoost maintains\ncompetitive accuracy (measured by AUC) while significantly reducing training\ntime, particularly as dataset size and feature complexity increase. To assess\nscalability, we extend our study to large synthetic datasets ranging from 1M to\n70M samples, demonstrating that CVQBoost on Dirac-3 is well-suited for\nlarge-scale classification tasks. These findings position CVQBoost as a\npromising alternative to gradient boosting methods, offering superior\nscalability and efficiency for high-dimensional ML applications such as fraud\ndetection.",
        "Active galactic nuclei (AGN) play a key role in galaxy evolution but are\nchallenging to identify due to their varied observational signatures.\nFurthermore, understanding their impact requires quantifying their strength\nrelative to their host galaxies. We developed a deep learning (DL) model for\nidentifying AGN in imaging data by deriving the contribution of the central\npoint source. Trained on Euclidised mock galaxy images with injected AGN\nlevels, in the form of varying contributions of the point-spread function\n(PSF), our model can precisely and accurately recover the injected AGN\ncontribution fraction $f_{\\rm PSF}$, with a mean difference between the\npredicted and true $f_{\\rm PSF}$ of $-0.0078$ and an overall root mean square\nerror (RMSE) of 0.051. This method moves beyond binary AGN classification,\nenabling precise AGN contribution measurements. Applying our model to a\nstellar-mass-limited sample ($M_{\\ast} \\ge 10^{9.8} M_{\\odot}$, $0.5 \\le z \\le\n2.0$) from the first \\Euclid quick data release (Q1), we identify $48,840 \\pm\n78$ AGN over 63.1 deg$^2$ ($7.8\\pm0.1$%) using a threshold of $f_{\\rm PSF} >\n0.2$. We compare our DL-selected AGN with those identified in X-ray,\nmid-infrared (MIR), and optical spectroscopy and investigate their overlapping\nfractions depending on different thresholds on the PSF contribution. We find\nthat the overlap increases with increasing X-ray or bolometric AGN luminosity.\nThe AGN luminosity in the $I_{\\rm E}$ filter correlates with host galaxy\nstellar mass, suggesting faster supermassive black hole (SMBH) growth in more\nmassive galaxies. Moreover, the mean relative contribution of the AGN is higher\nin quiescent galaxies than in star-forming ones. Starburst galaxies and the\nmost massive galaxies (across the star-formation main sequence) tend to host\nthe most luminous AGN, indicating concomitant assembly of the SMBH and the host\ngalaxy."
      ]
    }
  },
  {
    "id":2411.00714,
    "research_type":"basic",
    "start_id":"b0",
    "start_title":"Universality, criticality and complexity of information propagation in social media",
    "start_abstract":"Abstract Statistical laws of information avalanches in social media appear, at least according to existing empirical studies, not robust across systems. As a consequence, radically different processes may represent plausible driving mechanisms for propagation. Here, we analyze almost one billion time-stamped events collected from several online platforms \u2013 including Telegram, Twitter and Weibo over observation windows longer than ten years, show that the propagation is universal critical process. Universality arises identical macroscopic patterns platforms, irrespective details specific system hand. Critical behavior deduced power-law distributions, corresponding hyperscaling relations, characterizing size duration information. testing on our data indicates mixture simple complex contagion characterizes media. Data suggest complexity process correlated with semantic content propagated.",
    "start_categories":[
      "Human Behaviors"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Random graphs with arbitrary degree distributions and their applications"
      ],
      "abstract":[
        "Recent work on the structure of social networks and internet has focused attention graphs with distributions vertex degree that are significantly different from Poisson have been widely studied in past. In this paper we develop detail theory random arbitrary distributions. addition to simple undirected, unipartite graphs, examine properties directed bipartite graphs. Among other results, derive exact expressions for position phase transition at which a giant component first forms, mean size, size if there is one, number vertices certain distance away randomly chosen vertex, average vertex-vertex within graph. We apply our some real-world including world-wide web collaboration scientists Fortune 1000 company directors. demonstrate cases appropriate predict surprising accuracy behavior real world, while others measurable discrepancy between reality, perhaps indicating presence additional network not captured by"
      ],
      "categories":[
        "Modeling"
      ]
    },
    "list":{
      "title":[
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "Monetary-Fiscal Interaction and the Liquidity of Government Debt",
        "Efficient Long Speech Sequence Modelling for Time-Domain Depression\n  Level Estimation",
        "Revisiting Near-Far Field Boundary in Dual-Polarized XL-MIMO Systems",
        "Model Fusion via Neuron Transplantation",
        "Semantic Neural Radiance Fields for Multi-Date Satellite Data",
        "Exploring non-supersymmetric black holes with multiple bubbles in\n  five-dimensional minimal supergravity",
        "Computation of Magnetohydrodynamic Equilibria with Voigt Regularization",
        "MFSR-GAN: Multi-Frame Super-Resolution with Handheld Motion Modeling",
        "Enhanced Hybrid Deep Learning Approach for Botnet Attacks Detection in\n  IoT Environment",
        "Adopting Whisper for Confidence Estimation",
        "Rigidity of Higson coronas",
        "Photo-induced Dynamics and Momentum Distribution of Chiral Charge\n  Density Waves in 1T-TiSe$_{2}$",
        "Lyapunov exponents as probes for phase transitions of Kerr-AdS black\n  holes",
        "The Geometry of Optimal Gait Families for Steering Kinematic Locomoting\n  Systems",
        "Computation of whispering gallery modes for spherical symmetric\n  heterogeneous Helmholtz problems with piecewise smooth refractive index",
        "ResMoE: Space-efficient Compression of Mixture of Experts LLMs via\n  Residual Restoration",
        "A novel multi-agent dynamic portfolio optimization learning system based\n  on hierarchical deep reinforcement learning",
        "The number of smooth varieties in an MMP on a 3-fold of Fano type",
        "Using Read Promotion and Mixed Isolation Levels for Performant Yet\n  Serializable Execution of Transaction Programs",
        "Learning to Identify Conflicts in RPKI",
        "Long-Term Planning Around Humans in Domestic Environments with 3D Scene\n  Graphs",
        "Towards a definition of a meteor cluster: Detection of meteor clusters\n  from meteor orbit databases",
        "Optimizing Decomposition for Optimal Claim Verification",
        "Structural Embedding Projection for Contextual Large Language Model\n  Inference",
        "Toward Integrated Solutions: A Systematic Interdisciplinary Review of\n  Cybergrooming Research",
        "BERT-based model for Vietnamese Fact Verification Dataset",
        "xJailbreak: Representation Space Guided Reinforcement Learning for\n  Interpretable LLM Jailbreaking",
        "Are Cognitive Biases as Important as they Seem for Data Visualization?"
      ],
      "abstract":[
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "How does the monetary and fiscal policy mix alter households' saving\nincentives? To answer these questions, we build a heterogenous agents New\nKeynesian model where three different types of agents can save in assets with\ndifferent liquidity profiles to insure against idiosyncratic risk. Policy mixes\naffect saving incentives differently according to their effect on the liquidity\npremium -- the return difference between less liquid assets and public debt. We\nderive an intuitive analytical expression linking the liquidity premium with\nconsumption differentials amongst different types of agents. This underscores\nthe presence of a transmission mechanism through which the interaction of\nmonetary and fiscal policy shapes economic stability via its effect on the\nportfolio choice of private agents. We call it the 'self-insurance demand\nchannel', which moves the liquidity premium in the opposite direction to the\nstandard 'policy-driven supply channel'. Our analysis thus reveals the presence\nof two competing forces driving the liquidity premium. We show that the\nrelative strength of the two is tightly linked to the policy mix in place and\nthe type of business cycle shock hitting the economy. This implies that to\nstabilize the economy, monetary policy should consider the impact of the\n'self-insurance' on the liquidity premium.",
        "Depression significantly affects emotions, thoughts, and daily activities.\nRecent research indicates that speech signals contain vital cues about\ndepression, sparking interest in audio-based deep-learning methods for\nestimating its severity. However, most methods rely on time-frequency\nrepresentations of speech which have recently been criticized for their\nlimitations due to the loss of information when performing time-frequency\nprojections, e.g. Fourier transform, and Mel-scale transformation. Furthermore,\nsegmenting real-world speech into brief intervals risks losing critical\ninterconnections between recordings. Additionally, such an approach may not\nadequately reflect real-world scenarios, as individuals with depression often\npause and slow down in their conversations and interactions. Building on these\nobservations, we present an efficient method for depression level estimation\nusing long speech signals in the time domain. The proposed method leverages a\nstate space model coupled with the dual-path structure-based long sequence\nmodelling module and temporal external attention module to reconstruct and\nenhance the detection of depression-related cues hidden in the raw audio\nwaveforms. Experimental results on the AVEC2013 and AVEC2014 datasets show\npromising results in capturing consequential long-sequence depression cues and\ndemonstrate outstanding performance over the state-of-the-art.",
        "Extremely large-scale multiple-input multiple-output (XL-MIMO) is expected to\nbe an important technology in future sixth generation (6G) networks. Compared\nwith conventional single-polarized XL-MIMO, where signals are transmitted and\nreceived in only one polarization direction, dual-polarized XL-MIMO systems\nachieve higher data rate by improving multiplexing performances, and thus are\nthe focus of this paper. Due to enlarged aperture, near-field regions become\nnon-negligible in XL-MIMO communications, necessitating accurate near-far field\nboundary characterizations. However, existing boundaries developed for\nsingle-polarized systems only consider phase or power differences across array\nelements while irrespective of cross-polarization discrimination (XPD)\nvariances in dual-polarized XL-MIMO systems, deteriorating transmit covariance\noptimization performances. In this paper, we revisit near-far field boundaries\nfor dual-polarized XL-MIMO systems by taking XPD differences into account,\nwhich faces the following challenge. Unlike existing near-far field boundaries,\nwhich only need to consider co-polarized channel components, deriving\nboundaries for dual-polarized XL-MIMO systems requires modeling joint effects\nof co-polarized and cross-polarized components. To address this issue, we model\nXPD variations across antennas and introduce a non-uniform XPD distance to\ncomplement existing near-far field boundaries. Based on the new distance\ncriterion, we propose an efficient scheme to optimize transmit covariance.\nNumerical results validate our analysis and demonstrate the proposed\nalgorithm's effectiveness.",
        "Ensemble learning is a widespread technique to improve the prediction\nperformance of neural networks. However, it comes at the price of increased\nmemory and inference time. In this work we propose a novel model fusion\ntechnique called \\emph{Neuron Transplantation (NT)} in which we fuse an\nensemble of models by transplanting important neurons from all ensemble members\ninto the vacant space obtained by pruning insignificant neurons. An initial\nloss in performance post-transplantation can be quickly recovered via\nfine-tuning, consistently outperforming individual ensemble members of the same\nmodel capacity and architecture. Furthermore, NT enables all the ensemble\nmembers to be jointly pruned and jointly trained in a combined model. Comparing\nit to alignment-based averaging (like Optimal-Transport-fusion), it requires\nless fine-tuning than the corresponding OT-fused model, the fusion itself is\nfaster and requires less memory, while the resulting model performance is\ncomparable or better. The code is available under the following link:\nhttps:\/\/github.com\/masterbaer\/neuron-transplantation.",
        "In this work we propose a satellite specific Neural Radiance Fields (NeRF)\nmodel capable to obtain a three-dimensional semantic representation (neural\nsemantic field) of the scene. The model derives the output from a set of\nmulti-date satellite images with corresponding pixel-wise semantic labels. We\ndemonstrate the robustness of our approach and its capability to improve noisy\ninput labels. We enhance the color prediction by utilizing the semantic\ninformation to address temporal image inconsistencies caused by non-stationary\ncategories such as vehicles. To facilitate further research in this domain, we\npresent a dataset comprising manually generated labels for popular multi-view\nsatellite images. Our code and dataset are available at\nhttps:\/\/github.com\/wagnva\/semantic-nerf-for-satellite-data.",
        "The topological censorship theorem suggests that higher dimensional black\nholes can possess the domain of outer communication (DOC) of nontrivial\ntopology. In this paper, we seek for a black hole coexisting with two bubbles\nadjacent to the horizon in five-dimensional minimal supergravity, under the\nassumptions of stationarity and bi-axisymmetry. For simplicity, we also assume\nthat the spacetime is symmetric under the exchange of the two axisymmetric\nKilling vectors. To find the solution, we combine the inverse scattering method\nand the Harrison transformation, and we present the conditions for the absence\nof conical, orbifold and Dirac-Misner string singularities, respectively. As\nthe result, we find that the black hole with topology of $S^3$ or $S^2\\times\nS^1$ can be supported by two bubbles if we admit the conical singularities\n(deficits).",
        "This work presents the first numerical investigation of using Voigt\nregularization as a method for obtaining magnetohydrodynamic (MHD) equilibria\nwithout the assumption of nested magnetic flux surfaces. Voigt regularization\nmodifies the MHD dynamics by introducing additional terms that vanish in the\ninfinite-time limit, allowing for magnetic reconnection and the formation of\nmagnetic islands, which can overlap and produce field-line chaos. The utility\nof this approach is demonstrated through numerical solutions of two-dimensional\nideal and resistive test problems. Our results show that Voigt regularization\ncan significantly accelerate the convergence to solutions in resistive MHD\nproblems, while also highlighting challenges in applying the method to ideal\nMHD systems. This research opens up new possibilities for developing more\nefficient and robust MHD equilibrium solvers, which could contribute to the\ndesign and optimization of future fusion devices.",
        "Smartphone cameras have become ubiquitous imaging tools, yet their small\nsensors and compact optics often limit spatial resolution and introduce\ndistortions. Combining information from multiple low-resolution (LR) frames to\nproduce a high-resolution (HR) image has been explored to overcome the inherent\nlimitations of smartphone cameras. Despite the promise of multi-frame\nsuper-resolution (MFSR), current approaches are hindered by datasets that fail\nto capture the characteristic noise and motion patterns found in real-world\nhandheld burst images. In this work, we address this gap by introducing a novel\nsynthetic data engine that uses multi-exposure static images to synthesize\nLR-HR training pairs while preserving sensor-specific noise characteristics and\nimage motion found during handheld burst photography. We also propose MFSR-GAN:\na multi-scale RAW-to-RGB network for MFSR. Compared to prior approaches,\nMFSR-GAN emphasizes a \"base frame\" throughout its architecture to mitigate\nartifacts. Experimental results on both synthetic and real data demonstrates\nthat MFSR-GAN trained with our synthetic engine yields sharper, more realistic\nreconstructions than existing methods for real-world MFSR.",
        "Cyberattacks in an Internet of Things (IoT) environment can have significant\nimpacts because of the interconnected nature of devices and systems. An\nattacker uses a network of compromised IoT devices in a botnet attack to carry\nout various harmful activities. Detecting botnet attacks poses several\nchallenges because of the intricate and evolving nature of these threats.\nBotnet attacks erode trust in IoT devices and systems, undermining confidence\nin their security, reliability, and integrity. Deep learning techniques have\nsignificantly enhanced the detection of botnet attacks due to their ability to\nanalyze and learn from complex patterns in data. This research proposed the\nstacking of Deep convolutional neural networks, Bi-Directional Long Short-Term\nMemory (Bi-LSTM), Bi-Directional Gated Recurrent Unit (Bi-GRU), and Recurrent\nNeural Networks (RNN) for botnet attacks detection. The UNSW-NB15 dataset is\nutilized for botnet attacks detection. According to experimental results, the\nproposed model accurately provides for the intricate patterns and features of\nbotnet attacks, with a testing accuracy of 99.76%. The proposed model also\nidentifies botnets with a high ROC-AUC curve value of 99.18%. A performance\ncomparison of the proposed method with existing state-of-the-art models\nconfirms its higher performance. The outcomes of this research could strengthen\ncyber security procedures and safeguard against new attacks.",
        "Recent research on word-level confidence estimation for speech recognition\nsystems has primarily focused on lightweight models known as Confidence\nEstimation Modules (CEMs), which rely on hand-engineered features derived from\nAutomatic Speech Recognition (ASR) outputs. In contrast, we propose a novel\nend-to-end approach that leverages the ASR model itself (Whisper) to generate\nword-level confidence scores. Specifically, we introduce a method in which the\nWhisper model is fine-tuned to produce scalar confidence scores given an audio\ninput and its corresponding hypothesis transcript. Our experiments demonstrate\nthat the fine-tuned Whisper-tiny model, comparable in size to a strong CEM\nbaseline, achieves similar performance on the in-domain dataset and surpasses\nthe CEM baseline on eight out-of-domain datasets, whereas the fine-tuned\nWhisper-large model consistently outperforms the CEM baseline by a substantial\nmargin across all datasets.",
        "We show that under mild set theoretic hypotheses we have rigidity for\nalgebras of continuous functions over Higson coronas, topological spaces\narising in coarse geometry. In particular, we show that under $\\mathsf{OCA}$\nand $\\mathsf {MA}_{\\aleph_1}$, if two uniformly locally finite metric spaces\n$X$ and $Y$ have homeomorphic Higson coronas $\\nu X$ and $\\nu Y$, then $X$ and\n$Y$ are coarsely equivalent, a statement which provably does not follow from\n$\\mathsf{ZFC}$ alone.",
        "Exploring the photoinduced dynamics of chiral states offers promising avenues\nfor advanced control of condensed matter systems. Photoinduced or photoenhanced\nchirality in 1T-TiSe$_{2}$ has been suggested as a fascinating platform for\noptical manipulation of chiral states. However, the mechanisms underlying\nchirality training and its interplay with the charge density wave (CDW) phase\nremain elusive. Here, we use time-resolved X-ray diffraction (tr-XRD) with\ncircularly polarized pump lasers to probe the photoinduced dynamics of\nchirality in 1T-TiSe$_{2}$. We observe a notable ($\\sim$20%) difference in CDW\nintensity suppression between left- and right-circularly polarized pumps.\nAdditionally, we reveal momentum-resolved circular dichroism arising from\ndomains of different chirality, providing a direct link between CDW and\nchirality. An immediate increase in CDW correlation length upon laser pumping\nis detected, suggesting the photoinduced expansion of chiral domains. These\nresults both advance the potential of light-driven chirality by elucidating the\nmechanism driving chirality manipulation in TiSe$_2$, and they demonstrate that\ntr-XRD with circularly polarized pumps is an effective tool for chirality\ndetection in condensed matter systems.",
        "In this paper, we study proper time Lyapunov exponents and coordinate time\nLyapunov exponents of chaos for both massless and massive particles orbiting\nfour-dimensional and five-dimensional Kerr-AdS black holes, and explore their\nrelationships with phase transitions of these black holes. The results reveal\nthat these exponents can reflect the occurrence of phase transitions.\nSpecifically, when compared to the Lyapunov exponents of massive particles in\nchaotic states, the exponents corresponding to massless particles demonstrate a\nmore robust capability in describing the phase transitions. Furthermore, we\nconduct a study on critical exponents associated with the Lyapunov exponents in\nthese black holes, identifying a critical exponent value of 1\/2.",
        "Motion planning for locomotion systems typically requires translating\nhigh-level rigid-body tasks into low-level joint trajectories-a process that is\nstraightforward for car-like robots with fixed, unbounded actuation inputs but\nmore challenging for systems like snake robots, where the mapping depends on\nthe current configuration and is constrained by joint limits. In this paper, we\nfocus on generating continuous families of optimal gaits-collections of gaits\nparameterized by step size or steering rate-to enhance controllability and\nmaneuverability. We uncover the underlying geometric structure of these optimal\ngait families and propose methods for constructing them using both global and\nlocal search strategies, where the local method and the global method\ncompensate each other. The global search approach is robust to nonsmooth\nbehavior, albeit yielding reduced-order solutions, while the local search\nprovides higher accuracy but can be unstable near nonsmooth regions. To\ndemonstrate our framework, we generate optimal gait families for viscous and\nperfect-fluid three-link swimmers. This work lays a foundation for integrating\nlow-level joint controllers with higher-level motion planners in complex\nlocomotion systems.",
        "In this paper, we develop a numerical method for the computation of\n(quasi-)resonances in spherical symmetric heterogeneous Helmholtz problems with\npiecewise smooth refractive index. Our focus lies in resonances very close to\nthe real axis, which characterize the so-called whispering gallery modes. Our\nmethod involves a modal equation incorporating fundamental solutions to\ndecoupled problems, extending the known modal equation to the case of piecewise\nsmooth coefficients. We first establish the well-posedeness of the fundamental\nsystem, then we formulate the problem of resonances as a nonlinear eigenvalue\nproblem, whose determinant will be the modal equation in the piecewise smooth\ncase. In combination with the numerical approximation of the fundamental\nsolutions using a spectral method, we derive a Newton method to solve the\nnonlinear modal equation with a proper scaling. We show the local convergence\nof the algorithm in the piecewise constant case by proving the simplicity of\nthe roots. We confirm our approach through a series of numerical experiments in\nthe piecewise constant and variable case.",
        "Mixture-of-Experts (MoE) Transformer, the backbone architecture of multiple\nphenomenal language models, leverages sparsity by activating only a fraction of\nmodel parameters for each input token. The sparse structure, while allowing\nconstant time costs, results in space inefficiency: we still need to load all\nthe model parameters during inference. We introduce ResMoE, an innovative MoE\napproximation framework that utilizes Wasserstein barycenter to extract a\ncommon expert (barycenter expert) and approximate the residuals between this\nbarycenter expert and the original ones. ResMoE enhances the space efficiency\nfor inference of large-scale MoE Transformers in a one-shot and data-agnostic\nmanner without retraining while maintaining minimal accuracy loss, thereby\npaving the way for broader accessibility to large language models. We\ndemonstrate the effectiveness of ResMoE through extensive experiments on Switch\nTransformer, Mixtral, and DeepSeekMoE models. The results show that ResMoE can\nreduce the number of parameters in an expert by up to 75% while maintaining\ncomparable performance. The code is available at\nhttps:\/\/github.com\/iDEA-iSAIL-Lab-UIUC\/ResMoE.",
        "Deep Reinforcement Learning (DRL) has been extensively used to address\nportfolio optimization problems. The DRL agents acquire knowledge and make\ndecisions through unsupervised interactions with their environment without\nrequiring explicit knowledge of the joint dynamics of portfolio assets. Among\nthese DRL algorithms, the combination of actor-critic algorithms and deep\nfunction approximators is the most widely used DRL algorithm. Here, we find\nthat training the DRL agent using the actor-critic algorithm and deep function\napproximators may lead to scenarios where the improvement in the DRL agent's\nrisk-adjusted profitability is not significant. We propose that such situations\nprimarily arise from the following two problems: sparsity in positive reward\nand the curse of dimensionality. These limitations prevent DRL agents from\ncomprehensively learning asset price change patterns in the training\nenvironment. As a result, the DRL agents cannot explore the dynamic portfolio\noptimization policy to improve the risk-adjusted profitability in the training\nprocess. To address these problems, we propose a novel multi-agent Hierarchical\nDeep Reinforcement Learning (HDRL) algorithmic framework in this research.\nUnder this framework, the agents work together as a learning system for\nportfolio optimization. Specifically, by designing an auxiliary agent that\nworks together with the executive agent for optimal policy exploration, the\nlearning system can focus on exploring the policy with higher risk-adjusted\nreturn in the action space with positive return and low variance. In this way,\nwe can overcome the issue of the curse of dimensionality and improve the\ntraining efficiency in the positive reward sparse environment.",
        "In this paper, we prove that for a threefold of Fano type $X$ and a movable\n$\\mathbb{Q}$-Cartier Weil divisor $D$ on $X$, the number of smooth varieties\nthat arise during the running of a $D$-MMP is bounded by $1 + h^1(X, 2D)$.\nAdditionally, we prove a partial converse to the Kodaira vanishing theorem for\na movable divisor on a threefold of Fano type.",
        "We propose a theory that can determine the lowest isolation level that can be\nallocated to each transaction program in an application in a\nmixed-isolation-level setting, to guarantee that all executions will be\nserializable and thus preserve all integrity constraints, even those that are\nnot explicitly declared. This extends prior work applied to completely known\ntransactions, to deal with the realistic situation where transactions are\ngenerated by running programs with parameters that are not known in advance.\nUsing our theory, we propose an optimization method that allows for high\nthroughput while ensuring that all executions are serializable. Our method is\nbased on searching for application code modifications that are\nsemantics-preserving while improving the isolation level allocation. We\nillustrate our approach to the SmallBank benchmark.",
        "The long history of misconfigurations and errors in RPKI indicates that they\ncannot be easily avoided and will most probably persist also in the future.\nThese errors create conflicts between BGP announcements and their covering\nROAs, causing the RPKI validation to result in status invalid. Networks that\nenforce RPKI filtering with Route Origin Validation (ROV) would block such\nconflicting BGP announcements and as a result lose traffic from the\ncorresponding origins. Since the business incentives of networks are tightly\ncoupled with the traffic they relay, filtering legitimate traffic leads to a\nloss of revenue, reducing the motivation to filter invalid announcements with\nROV.\n  In this work, we introduce a new mechanism, LOV, designed for whitelisting\nbenign conflicts on an Internet scale. The resulting whitelist is made\navailable to RPKI supporting ASes to avoid filtering RPKI-invalid but benign\nroutes. Saving legitimate traffic resolves one main obstacle towards RPKI\ndeployment. We measure live BGP updates using LOV during a period of half a\nyear and whitelist 52,846 routes with benign origin errors.",
        "Long-term planning for robots operating in domestic environments poses unique\nchallenges due to the interactions between humans, objects, and spaces. Recent\nadvancements in trajectory planning have leveraged vision-language models\n(VLMs) to extract contextual information for robots operating in real-world\nenvironments. While these methods achieve satisfying performance, they do not\nexplicitly model human activities. Such activities influence surrounding\nobjects and reshape spatial constraints. This paper presents a novel approach\nto trajectory planning that integrates human preferences, activities, and\nspatial context through an enriched 3D scene graph (3DSG) representation. By\nincorporating activity-based relationships, our method captures the spatial\nimpact of human actions, leading to more context-sensitive trajectory\nadaptation. Preliminary results demonstrate that our approach effectively\nassigns costs to spaces influenced by human activities, ensuring that the robot\ntrajectory remains contextually appropriate and sensitive to the ongoing\nenvironment. This balance between task efficiency and social appropriateness\nenhances context-aware human-robot interactions in domestic settings. Future\nwork includes implementing a full planning pipeline and conducting user studies\nto evaluate trajectory acceptability.",
        "As of today, there is no official definition of a meteor cluster. It is\nusually identified as a large number of meteors sharing a similar radiant and\nvelocity, all occurring within a few seconds. Only eight clusters have been\nreported so far, from single-camera or camera network observations. We aim to\nprovide an overview of meteor clusters to help define what constitutes a\ncluster by potentially adding more to the already identified ones and\ndetermining their common parameters. A search for new clusters is performed in\npublicly available International Astronomical Union meteor databases with the\nDBSCAN algorithm. Then, a statistical significance method is applied to derive\nthe most promising cluster candidates. However, the method still lacks a way to\ndebias the atmospheric area surveyed by the cameras due to a lack of publicly\navailable data. A set of 16 statistically significant potential clusters is\nidentified, involving 4 to 7 fragments. The 90th percentile includes a duration\nof 8 seconds, a velocity difference of 2.2 km\/s, and a radiant spread of nearly\n4 degrees. The velocity difference may arise from the method used for orbit\ncomputation. Meteor clusters might be more frequent than currently reported.\nHowever, we recommend that future meteor orbit databases also include a way to\nestimate the surveyed area by the cameras involved in the detection. This would\nstrengthen the veracity of the 16 identified cluster candidates and ultimately\nallow scientists to fully debias the number of clusters, and hence derive the\nphysical lifetime expectancy of meteoroids, which is often overlooked due to\nthe focus on collisional lifetime estimates only. We also recommend that any\nfuture cluster observation report includes the expected number of random\noccurrences and consider the event to be real if this value is below 0.1.",
        "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.",
        "Structured embedding transformations offer a promising approach for enhancing\nthe efficiency and coherence of language model inference. The introduction of\nStructural Embedding Projection (SEP) provides a mechanism for refining token\nrepresentations through projection matrices that integrate hierarchical and\nrelational dependencies. The mathematical formulation of SEP enables embedding\nspaces to capture structured contextual relationships, thereby improving\nsemantic fidelity without significantly increasing computational overhead.\nExperimental evaluations conducted on a range of linguistic datasets revealed\nthat SEP contributed to reductions in perplexity and enhanced contextual\ncoherence, demonstrating its potential to refine language model outputs.\nComputational efficiency assessments highlighted variations across different\ndatasets, suggesting that the integration of structured embeddings introduced\ndataset-dependent trade-offs between inference speed and representational\nrichness. The qualitative analysis of generated responses indicated that SEP\nenhanced narrative consistency and topic alignment, leading to improved fluency\nin multi-sentence text generation. The modifications to embedding layers\nrequired precise optimization to ensure stable training dynamics, as the\nintroduction of structured transformations altered the traditional\nrepresentation-learning process. The architectural adjustments necessary for\nSEP implementation influenced inference latency and memory consumption,\nrequiring a balance between efficiency gains and additional processing demands.\nThe impact of SEP on lexical diversity suggested that embedding modifications\ninfluenced the model's vocabulary usage, reflecting a more context-aware\nselection of generated tokens.",
        "Cybergrooming exploits minors through online trust-building, yet research\nremains fragmented, limiting holistic prevention. Social sciences focus on\nbehavioral insights, while computational methods emphasize detection, but their\nintegration remains insufficient. This review systematically synthesizes both\nfields using the PRISMA framework to enhance clarity, reproducibility, and\ncross-disciplinary collaboration. Findings show that qualitative methods offer\ndeep insights but are resource-intensive, machine learning models depend on\ndata quality, and standard metrics struggle with imbalance and cultural\nnuances. By bridging these gaps, this review advances interdisciplinary\ncybergrooming research, guiding future efforts toward more effective prevention\nand detection strategies.",
        "The rapid advancement of information and communication technology has\nfacilitated easier access to information. However, this progress has also\nnecessitated more stringent verification measures to ensure the accuracy of\ninformation, particularly within the context of Vietnam. This paper introduces\nan approach to address the challenges of Fact Verification using the Vietnamese\ndataset by integrating both sentence selection and classification modules into\na unified network architecture. The proposed approach leverages the power of\nlarge language models by utilizing pre-trained PhoBERT and XLM-RoBERTa as the\nbackbone of the network. The proposed model was trained on a Vietnamese\ndataset, named ISE-DSC01, and demonstrated superior performance compared to the\nbaseline model across all three metrics. Notably, we achieved a Strict Accuracy\nlevel of 75.11\\%, indicating a remarkable 28.83\\% improvement over the baseline\nmodel.",
        "Safety alignment mechanism are essential for preventing large language models\n(LLMs) from generating harmful information or unethical content. However,\ncleverly crafted prompts can bypass these safety measures without accessing the\nmodel's internal parameters, a phenomenon known as black-box jailbreak.\nExisting heuristic black-box attack methods, such as genetic algorithms, suffer\nfrom limited effectiveness due to their inherent randomness, while recent\nreinforcement learning (RL) based methods often lack robust and informative\nreward signals. To address these challenges, we propose a novel black-box\njailbreak method leveraging RL, which optimizes prompt generation by analyzing\nthe embedding proximity between benign and malicious prompts. This approach\nensures that the rewritten prompts closely align with the intent of the\noriginal prompts while enhancing the attack's effectiveness. Furthermore, we\nintroduce a comprehensive jailbreak evaluation framework incorporating\nkeywords, intent matching, and answer validation to provide a more rigorous and\nholistic assessment of jailbreak success. Experimental results show the\nsuperiority of our approach, achieving state-of-the-art (SOTA) performance on\nseveral prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,\nLlama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in\njailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.\nThe codebase for this work is available at\nhttps:\/\/github.com\/Aegis1863\/xJailbreak.",
        "Research on cognitive biases and heuristics has become increasingly popular\nin the visualization literature in recent years. Researchers have studied the\neffects of biases on visualization interpretation and subsequent\ndecision-making. While this work is important, we contend that the view on\nbiases has presented human cognitive abilities in an unbalanced manner, placing\ntoo much emphasis on the flaws and limitations of human decision-making, and\npotentially suggesting that it should not be trusted. Several decision\nresearchers have argued that the flip side of biases -- i.e., mental shortcuts\nor heuristics -- demonstrate human ingenuity and serve as core markers of\nadaptive expertise. In this paper, we review the perspectives and sentiments of\nthe visualization community on biases and describe literature arguing for more\nbalanced views of biases and heuristics. We hope this paper will encourage\nvisualization researchers to consider a fuller picture of human cognitive\nlimitations and strategies for making decisions in complex environments."
      ]
    }
  },
  {
    "id":2411.00714,
    "research_type":"basic",
    "start_id":"b14",
    "start_title":"Random graphs with arbitrary degree distributions and their applications",
    "start_abstract":"Recent work on the structure of social networks and internet has focused attention graphs with distributions vertex degree that are significantly different from Poisson have been widely studied in past. In this paper we develop detail theory random arbitrary distributions. addition to simple undirected, unipartite graphs, examine properties directed bipartite graphs. Among other results, derive exact expressions for position phase transition at which a giant component first forms, mean size, size if there is one, number vertices certain distance away randomly chosen vertex, average vertex-vertex within graph. We apply our some real-world including world-wide web collaboration scientists Fortune 1000 company directors. demonstrate cases appropriate predict surprising accuracy behavior real world, while others measurable discrepancy between reality, perhaps indicating presence additional network not captured by",
    "start_categories":[
      "Modeling"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Universality, criticality and complexity of information propagation in social media"
      ],
      "abstract":[
        "Abstract Statistical laws of information avalanches in social media appear, at least according to existing empirical studies, not robust across systems. As a consequence, radically different processes may represent plausible driving mechanisms for propagation. Here, we analyze almost one billion time-stamped events collected from several online platforms \u2013 including Telegram, Twitter and Weibo over observation windows longer than ten years, show that the propagation is universal critical process. Universality arises identical macroscopic patterns platforms, irrespective details specific system hand. Critical behavior deduced power-law distributions, corresponding hyperscaling relations, characterizing size duration information. testing on our data indicates mixture simple complex contagion characterizes media. Data suggest complexity process correlated with semantic content propagated."
      ],
      "categories":[
        "Human Behaviors"
      ]
    },
    "list":{
      "title":[
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "Monetary-Fiscal Interaction and the Liquidity of Government Debt",
        "Efficient Long Speech Sequence Modelling for Time-Domain Depression\n  Level Estimation",
        "Revisiting Near-Far Field Boundary in Dual-Polarized XL-MIMO Systems",
        "Model Fusion via Neuron Transplantation",
        "Semantic Neural Radiance Fields for Multi-Date Satellite Data",
        "Exploring non-supersymmetric black holes with multiple bubbles in\n  five-dimensional minimal supergravity",
        "Computation of Magnetohydrodynamic Equilibria with Voigt Regularization",
        "MFSR-GAN: Multi-Frame Super-Resolution with Handheld Motion Modeling",
        "Enhanced Hybrid Deep Learning Approach for Botnet Attacks Detection in\n  IoT Environment",
        "Adopting Whisper for Confidence Estimation",
        "Rigidity of Higson coronas",
        "Photo-induced Dynamics and Momentum Distribution of Chiral Charge\n  Density Waves in 1T-TiSe$_{2}$",
        "Lyapunov exponents as probes for phase transitions of Kerr-AdS black\n  holes",
        "The Geometry of Optimal Gait Families for Steering Kinematic Locomoting\n  Systems",
        "Computation of whispering gallery modes for spherical symmetric\n  heterogeneous Helmholtz problems with piecewise smooth refractive index",
        "ResMoE: Space-efficient Compression of Mixture of Experts LLMs via\n  Residual Restoration",
        "A novel multi-agent dynamic portfolio optimization learning system based\n  on hierarchical deep reinforcement learning",
        "The number of smooth varieties in an MMP on a 3-fold of Fano type",
        "Using Read Promotion and Mixed Isolation Levels for Performant Yet\n  Serializable Execution of Transaction Programs",
        "Learning to Identify Conflicts in RPKI",
        "Long-Term Planning Around Humans in Domestic Environments with 3D Scene\n  Graphs",
        "Towards a definition of a meteor cluster: Detection of meteor clusters\n  from meteor orbit databases",
        "Optimizing Decomposition for Optimal Claim Verification",
        "Structural Embedding Projection for Contextual Large Language Model\n  Inference",
        "Toward Integrated Solutions: A Systematic Interdisciplinary Review of\n  Cybergrooming Research",
        "BERT-based model for Vietnamese Fact Verification Dataset",
        "xJailbreak: Representation Space Guided Reinforcement Learning for\n  Interpretable LLM Jailbreaking",
        "Are Cognitive Biases as Important as they Seem for Data Visualization?"
      ],
      "abstract":[
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "How does the monetary and fiscal policy mix alter households' saving\nincentives? To answer these questions, we build a heterogenous agents New\nKeynesian model where three different types of agents can save in assets with\ndifferent liquidity profiles to insure against idiosyncratic risk. Policy mixes\naffect saving incentives differently according to their effect on the liquidity\npremium -- the return difference between less liquid assets and public debt. We\nderive an intuitive analytical expression linking the liquidity premium with\nconsumption differentials amongst different types of agents. This underscores\nthe presence of a transmission mechanism through which the interaction of\nmonetary and fiscal policy shapes economic stability via its effect on the\nportfolio choice of private agents. We call it the 'self-insurance demand\nchannel', which moves the liquidity premium in the opposite direction to the\nstandard 'policy-driven supply channel'. Our analysis thus reveals the presence\nof two competing forces driving the liquidity premium. We show that the\nrelative strength of the two is tightly linked to the policy mix in place and\nthe type of business cycle shock hitting the economy. This implies that to\nstabilize the economy, monetary policy should consider the impact of the\n'self-insurance' on the liquidity premium.",
        "Depression significantly affects emotions, thoughts, and daily activities.\nRecent research indicates that speech signals contain vital cues about\ndepression, sparking interest in audio-based deep-learning methods for\nestimating its severity. However, most methods rely on time-frequency\nrepresentations of speech which have recently been criticized for their\nlimitations due to the loss of information when performing time-frequency\nprojections, e.g. Fourier transform, and Mel-scale transformation. Furthermore,\nsegmenting real-world speech into brief intervals risks losing critical\ninterconnections between recordings. Additionally, such an approach may not\nadequately reflect real-world scenarios, as individuals with depression often\npause and slow down in their conversations and interactions. Building on these\nobservations, we present an efficient method for depression level estimation\nusing long speech signals in the time domain. The proposed method leverages a\nstate space model coupled with the dual-path structure-based long sequence\nmodelling module and temporal external attention module to reconstruct and\nenhance the detection of depression-related cues hidden in the raw audio\nwaveforms. Experimental results on the AVEC2013 and AVEC2014 datasets show\npromising results in capturing consequential long-sequence depression cues and\ndemonstrate outstanding performance over the state-of-the-art.",
        "Extremely large-scale multiple-input multiple-output (XL-MIMO) is expected to\nbe an important technology in future sixth generation (6G) networks. Compared\nwith conventional single-polarized XL-MIMO, where signals are transmitted and\nreceived in only one polarization direction, dual-polarized XL-MIMO systems\nachieve higher data rate by improving multiplexing performances, and thus are\nthe focus of this paper. Due to enlarged aperture, near-field regions become\nnon-negligible in XL-MIMO communications, necessitating accurate near-far field\nboundary characterizations. However, existing boundaries developed for\nsingle-polarized systems only consider phase or power differences across array\nelements while irrespective of cross-polarization discrimination (XPD)\nvariances in dual-polarized XL-MIMO systems, deteriorating transmit covariance\noptimization performances. In this paper, we revisit near-far field boundaries\nfor dual-polarized XL-MIMO systems by taking XPD differences into account,\nwhich faces the following challenge. Unlike existing near-far field boundaries,\nwhich only need to consider co-polarized channel components, deriving\nboundaries for dual-polarized XL-MIMO systems requires modeling joint effects\nof co-polarized and cross-polarized components. To address this issue, we model\nXPD variations across antennas and introduce a non-uniform XPD distance to\ncomplement existing near-far field boundaries. Based on the new distance\ncriterion, we propose an efficient scheme to optimize transmit covariance.\nNumerical results validate our analysis and demonstrate the proposed\nalgorithm's effectiveness.",
        "Ensemble learning is a widespread technique to improve the prediction\nperformance of neural networks. However, it comes at the price of increased\nmemory and inference time. In this work we propose a novel model fusion\ntechnique called \\emph{Neuron Transplantation (NT)} in which we fuse an\nensemble of models by transplanting important neurons from all ensemble members\ninto the vacant space obtained by pruning insignificant neurons. An initial\nloss in performance post-transplantation can be quickly recovered via\nfine-tuning, consistently outperforming individual ensemble members of the same\nmodel capacity and architecture. Furthermore, NT enables all the ensemble\nmembers to be jointly pruned and jointly trained in a combined model. Comparing\nit to alignment-based averaging (like Optimal-Transport-fusion), it requires\nless fine-tuning than the corresponding OT-fused model, the fusion itself is\nfaster and requires less memory, while the resulting model performance is\ncomparable or better. The code is available under the following link:\nhttps:\/\/github.com\/masterbaer\/neuron-transplantation.",
        "In this work we propose a satellite specific Neural Radiance Fields (NeRF)\nmodel capable to obtain a three-dimensional semantic representation (neural\nsemantic field) of the scene. The model derives the output from a set of\nmulti-date satellite images with corresponding pixel-wise semantic labels. We\ndemonstrate the robustness of our approach and its capability to improve noisy\ninput labels. We enhance the color prediction by utilizing the semantic\ninformation to address temporal image inconsistencies caused by non-stationary\ncategories such as vehicles. To facilitate further research in this domain, we\npresent a dataset comprising manually generated labels for popular multi-view\nsatellite images. Our code and dataset are available at\nhttps:\/\/github.com\/wagnva\/semantic-nerf-for-satellite-data.",
        "The topological censorship theorem suggests that higher dimensional black\nholes can possess the domain of outer communication (DOC) of nontrivial\ntopology. In this paper, we seek for a black hole coexisting with two bubbles\nadjacent to the horizon in five-dimensional minimal supergravity, under the\nassumptions of stationarity and bi-axisymmetry. For simplicity, we also assume\nthat the spacetime is symmetric under the exchange of the two axisymmetric\nKilling vectors. To find the solution, we combine the inverse scattering method\nand the Harrison transformation, and we present the conditions for the absence\nof conical, orbifold and Dirac-Misner string singularities, respectively. As\nthe result, we find that the black hole with topology of $S^3$ or $S^2\\times\nS^1$ can be supported by two bubbles if we admit the conical singularities\n(deficits).",
        "This work presents the first numerical investigation of using Voigt\nregularization as a method for obtaining magnetohydrodynamic (MHD) equilibria\nwithout the assumption of nested magnetic flux surfaces. Voigt regularization\nmodifies the MHD dynamics by introducing additional terms that vanish in the\ninfinite-time limit, allowing for magnetic reconnection and the formation of\nmagnetic islands, which can overlap and produce field-line chaos. The utility\nof this approach is demonstrated through numerical solutions of two-dimensional\nideal and resistive test problems. Our results show that Voigt regularization\ncan significantly accelerate the convergence to solutions in resistive MHD\nproblems, while also highlighting challenges in applying the method to ideal\nMHD systems. This research opens up new possibilities for developing more\nefficient and robust MHD equilibrium solvers, which could contribute to the\ndesign and optimization of future fusion devices.",
        "Smartphone cameras have become ubiquitous imaging tools, yet their small\nsensors and compact optics often limit spatial resolution and introduce\ndistortions. Combining information from multiple low-resolution (LR) frames to\nproduce a high-resolution (HR) image has been explored to overcome the inherent\nlimitations of smartphone cameras. Despite the promise of multi-frame\nsuper-resolution (MFSR), current approaches are hindered by datasets that fail\nto capture the characteristic noise and motion patterns found in real-world\nhandheld burst images. In this work, we address this gap by introducing a novel\nsynthetic data engine that uses multi-exposure static images to synthesize\nLR-HR training pairs while preserving sensor-specific noise characteristics and\nimage motion found during handheld burst photography. We also propose MFSR-GAN:\na multi-scale RAW-to-RGB network for MFSR. Compared to prior approaches,\nMFSR-GAN emphasizes a \"base frame\" throughout its architecture to mitigate\nartifacts. Experimental results on both synthetic and real data demonstrates\nthat MFSR-GAN trained with our synthetic engine yields sharper, more realistic\nreconstructions than existing methods for real-world MFSR.",
        "Cyberattacks in an Internet of Things (IoT) environment can have significant\nimpacts because of the interconnected nature of devices and systems. An\nattacker uses a network of compromised IoT devices in a botnet attack to carry\nout various harmful activities. Detecting botnet attacks poses several\nchallenges because of the intricate and evolving nature of these threats.\nBotnet attacks erode trust in IoT devices and systems, undermining confidence\nin their security, reliability, and integrity. Deep learning techniques have\nsignificantly enhanced the detection of botnet attacks due to their ability to\nanalyze and learn from complex patterns in data. This research proposed the\nstacking of Deep convolutional neural networks, Bi-Directional Long Short-Term\nMemory (Bi-LSTM), Bi-Directional Gated Recurrent Unit (Bi-GRU), and Recurrent\nNeural Networks (RNN) for botnet attacks detection. The UNSW-NB15 dataset is\nutilized for botnet attacks detection. According to experimental results, the\nproposed model accurately provides for the intricate patterns and features of\nbotnet attacks, with a testing accuracy of 99.76%. The proposed model also\nidentifies botnets with a high ROC-AUC curve value of 99.18%. A performance\ncomparison of the proposed method with existing state-of-the-art models\nconfirms its higher performance. The outcomes of this research could strengthen\ncyber security procedures and safeguard against new attacks.",
        "Recent research on word-level confidence estimation for speech recognition\nsystems has primarily focused on lightweight models known as Confidence\nEstimation Modules (CEMs), which rely on hand-engineered features derived from\nAutomatic Speech Recognition (ASR) outputs. In contrast, we propose a novel\nend-to-end approach that leverages the ASR model itself (Whisper) to generate\nword-level confidence scores. Specifically, we introduce a method in which the\nWhisper model is fine-tuned to produce scalar confidence scores given an audio\ninput and its corresponding hypothesis transcript. Our experiments demonstrate\nthat the fine-tuned Whisper-tiny model, comparable in size to a strong CEM\nbaseline, achieves similar performance on the in-domain dataset and surpasses\nthe CEM baseline on eight out-of-domain datasets, whereas the fine-tuned\nWhisper-large model consistently outperforms the CEM baseline by a substantial\nmargin across all datasets.",
        "We show that under mild set theoretic hypotheses we have rigidity for\nalgebras of continuous functions over Higson coronas, topological spaces\narising in coarse geometry. In particular, we show that under $\\mathsf{OCA}$\nand $\\mathsf {MA}_{\\aleph_1}$, if two uniformly locally finite metric spaces\n$X$ and $Y$ have homeomorphic Higson coronas $\\nu X$ and $\\nu Y$, then $X$ and\n$Y$ are coarsely equivalent, a statement which provably does not follow from\n$\\mathsf{ZFC}$ alone.",
        "Exploring the photoinduced dynamics of chiral states offers promising avenues\nfor advanced control of condensed matter systems. Photoinduced or photoenhanced\nchirality in 1T-TiSe$_{2}$ has been suggested as a fascinating platform for\noptical manipulation of chiral states. However, the mechanisms underlying\nchirality training and its interplay with the charge density wave (CDW) phase\nremain elusive. Here, we use time-resolved X-ray diffraction (tr-XRD) with\ncircularly polarized pump lasers to probe the photoinduced dynamics of\nchirality in 1T-TiSe$_{2}$. We observe a notable ($\\sim$20%) difference in CDW\nintensity suppression between left- and right-circularly polarized pumps.\nAdditionally, we reveal momentum-resolved circular dichroism arising from\ndomains of different chirality, providing a direct link between CDW and\nchirality. An immediate increase in CDW correlation length upon laser pumping\nis detected, suggesting the photoinduced expansion of chiral domains. These\nresults both advance the potential of light-driven chirality by elucidating the\nmechanism driving chirality manipulation in TiSe$_2$, and they demonstrate that\ntr-XRD with circularly polarized pumps is an effective tool for chirality\ndetection in condensed matter systems.",
        "In this paper, we study proper time Lyapunov exponents and coordinate time\nLyapunov exponents of chaos for both massless and massive particles orbiting\nfour-dimensional and five-dimensional Kerr-AdS black holes, and explore their\nrelationships with phase transitions of these black holes. The results reveal\nthat these exponents can reflect the occurrence of phase transitions.\nSpecifically, when compared to the Lyapunov exponents of massive particles in\nchaotic states, the exponents corresponding to massless particles demonstrate a\nmore robust capability in describing the phase transitions. Furthermore, we\nconduct a study on critical exponents associated with the Lyapunov exponents in\nthese black holes, identifying a critical exponent value of 1\/2.",
        "Motion planning for locomotion systems typically requires translating\nhigh-level rigid-body tasks into low-level joint trajectories-a process that is\nstraightforward for car-like robots with fixed, unbounded actuation inputs but\nmore challenging for systems like snake robots, where the mapping depends on\nthe current configuration and is constrained by joint limits. In this paper, we\nfocus on generating continuous families of optimal gaits-collections of gaits\nparameterized by step size or steering rate-to enhance controllability and\nmaneuverability. We uncover the underlying geometric structure of these optimal\ngait families and propose methods for constructing them using both global and\nlocal search strategies, where the local method and the global method\ncompensate each other. The global search approach is robust to nonsmooth\nbehavior, albeit yielding reduced-order solutions, while the local search\nprovides higher accuracy but can be unstable near nonsmooth regions. To\ndemonstrate our framework, we generate optimal gait families for viscous and\nperfect-fluid three-link swimmers. This work lays a foundation for integrating\nlow-level joint controllers with higher-level motion planners in complex\nlocomotion systems.",
        "In this paper, we develop a numerical method for the computation of\n(quasi-)resonances in spherical symmetric heterogeneous Helmholtz problems with\npiecewise smooth refractive index. Our focus lies in resonances very close to\nthe real axis, which characterize the so-called whispering gallery modes. Our\nmethod involves a modal equation incorporating fundamental solutions to\ndecoupled problems, extending the known modal equation to the case of piecewise\nsmooth coefficients. We first establish the well-posedeness of the fundamental\nsystem, then we formulate the problem of resonances as a nonlinear eigenvalue\nproblem, whose determinant will be the modal equation in the piecewise smooth\ncase. In combination with the numerical approximation of the fundamental\nsolutions using a spectral method, we derive a Newton method to solve the\nnonlinear modal equation with a proper scaling. We show the local convergence\nof the algorithm in the piecewise constant case by proving the simplicity of\nthe roots. We confirm our approach through a series of numerical experiments in\nthe piecewise constant and variable case.",
        "Mixture-of-Experts (MoE) Transformer, the backbone architecture of multiple\nphenomenal language models, leverages sparsity by activating only a fraction of\nmodel parameters for each input token. The sparse structure, while allowing\nconstant time costs, results in space inefficiency: we still need to load all\nthe model parameters during inference. We introduce ResMoE, an innovative MoE\napproximation framework that utilizes Wasserstein barycenter to extract a\ncommon expert (barycenter expert) and approximate the residuals between this\nbarycenter expert and the original ones. ResMoE enhances the space efficiency\nfor inference of large-scale MoE Transformers in a one-shot and data-agnostic\nmanner without retraining while maintaining minimal accuracy loss, thereby\npaving the way for broader accessibility to large language models. We\ndemonstrate the effectiveness of ResMoE through extensive experiments on Switch\nTransformer, Mixtral, and DeepSeekMoE models. The results show that ResMoE can\nreduce the number of parameters in an expert by up to 75% while maintaining\ncomparable performance. The code is available at\nhttps:\/\/github.com\/iDEA-iSAIL-Lab-UIUC\/ResMoE.",
        "Deep Reinforcement Learning (DRL) has been extensively used to address\nportfolio optimization problems. The DRL agents acquire knowledge and make\ndecisions through unsupervised interactions with their environment without\nrequiring explicit knowledge of the joint dynamics of portfolio assets. Among\nthese DRL algorithms, the combination of actor-critic algorithms and deep\nfunction approximators is the most widely used DRL algorithm. Here, we find\nthat training the DRL agent using the actor-critic algorithm and deep function\napproximators may lead to scenarios where the improvement in the DRL agent's\nrisk-adjusted profitability is not significant. We propose that such situations\nprimarily arise from the following two problems: sparsity in positive reward\nand the curse of dimensionality. These limitations prevent DRL agents from\ncomprehensively learning asset price change patterns in the training\nenvironment. As a result, the DRL agents cannot explore the dynamic portfolio\noptimization policy to improve the risk-adjusted profitability in the training\nprocess. To address these problems, we propose a novel multi-agent Hierarchical\nDeep Reinforcement Learning (HDRL) algorithmic framework in this research.\nUnder this framework, the agents work together as a learning system for\nportfolio optimization. Specifically, by designing an auxiliary agent that\nworks together with the executive agent for optimal policy exploration, the\nlearning system can focus on exploring the policy with higher risk-adjusted\nreturn in the action space with positive return and low variance. In this way,\nwe can overcome the issue of the curse of dimensionality and improve the\ntraining efficiency in the positive reward sparse environment.",
        "In this paper, we prove that for a threefold of Fano type $X$ and a movable\n$\\mathbb{Q}$-Cartier Weil divisor $D$ on $X$, the number of smooth varieties\nthat arise during the running of a $D$-MMP is bounded by $1 + h^1(X, 2D)$.\nAdditionally, we prove a partial converse to the Kodaira vanishing theorem for\na movable divisor on a threefold of Fano type.",
        "We propose a theory that can determine the lowest isolation level that can be\nallocated to each transaction program in an application in a\nmixed-isolation-level setting, to guarantee that all executions will be\nserializable and thus preserve all integrity constraints, even those that are\nnot explicitly declared. This extends prior work applied to completely known\ntransactions, to deal with the realistic situation where transactions are\ngenerated by running programs with parameters that are not known in advance.\nUsing our theory, we propose an optimization method that allows for high\nthroughput while ensuring that all executions are serializable. Our method is\nbased on searching for application code modifications that are\nsemantics-preserving while improving the isolation level allocation. We\nillustrate our approach to the SmallBank benchmark.",
        "The long history of misconfigurations and errors in RPKI indicates that they\ncannot be easily avoided and will most probably persist also in the future.\nThese errors create conflicts between BGP announcements and their covering\nROAs, causing the RPKI validation to result in status invalid. Networks that\nenforce RPKI filtering with Route Origin Validation (ROV) would block such\nconflicting BGP announcements and as a result lose traffic from the\ncorresponding origins. Since the business incentives of networks are tightly\ncoupled with the traffic they relay, filtering legitimate traffic leads to a\nloss of revenue, reducing the motivation to filter invalid announcements with\nROV.\n  In this work, we introduce a new mechanism, LOV, designed for whitelisting\nbenign conflicts on an Internet scale. The resulting whitelist is made\navailable to RPKI supporting ASes to avoid filtering RPKI-invalid but benign\nroutes. Saving legitimate traffic resolves one main obstacle towards RPKI\ndeployment. We measure live BGP updates using LOV during a period of half a\nyear and whitelist 52,846 routes with benign origin errors.",
        "Long-term planning for robots operating in domestic environments poses unique\nchallenges due to the interactions between humans, objects, and spaces. Recent\nadvancements in trajectory planning have leveraged vision-language models\n(VLMs) to extract contextual information for robots operating in real-world\nenvironments. While these methods achieve satisfying performance, they do not\nexplicitly model human activities. Such activities influence surrounding\nobjects and reshape spatial constraints. This paper presents a novel approach\nto trajectory planning that integrates human preferences, activities, and\nspatial context through an enriched 3D scene graph (3DSG) representation. By\nincorporating activity-based relationships, our method captures the spatial\nimpact of human actions, leading to more context-sensitive trajectory\nadaptation. Preliminary results demonstrate that our approach effectively\nassigns costs to spaces influenced by human activities, ensuring that the robot\ntrajectory remains contextually appropriate and sensitive to the ongoing\nenvironment. This balance between task efficiency and social appropriateness\nenhances context-aware human-robot interactions in domestic settings. Future\nwork includes implementing a full planning pipeline and conducting user studies\nto evaluate trajectory acceptability.",
        "As of today, there is no official definition of a meteor cluster. It is\nusually identified as a large number of meteors sharing a similar radiant and\nvelocity, all occurring within a few seconds. Only eight clusters have been\nreported so far, from single-camera or camera network observations. We aim to\nprovide an overview of meteor clusters to help define what constitutes a\ncluster by potentially adding more to the already identified ones and\ndetermining their common parameters. A search for new clusters is performed in\npublicly available International Astronomical Union meteor databases with the\nDBSCAN algorithm. Then, a statistical significance method is applied to derive\nthe most promising cluster candidates. However, the method still lacks a way to\ndebias the atmospheric area surveyed by the cameras due to a lack of publicly\navailable data. A set of 16 statistically significant potential clusters is\nidentified, involving 4 to 7 fragments. The 90th percentile includes a duration\nof 8 seconds, a velocity difference of 2.2 km\/s, and a radiant spread of nearly\n4 degrees. The velocity difference may arise from the method used for orbit\ncomputation. Meteor clusters might be more frequent than currently reported.\nHowever, we recommend that future meteor orbit databases also include a way to\nestimate the surveyed area by the cameras involved in the detection. This would\nstrengthen the veracity of the 16 identified cluster candidates and ultimately\nallow scientists to fully debias the number of clusters, and hence derive the\nphysical lifetime expectancy of meteoroids, which is often overlooked due to\nthe focus on collisional lifetime estimates only. We also recommend that any\nfuture cluster observation report includes the expected number of random\noccurrences and consider the event to be real if this value is below 0.1.",
        "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.",
        "Structured embedding transformations offer a promising approach for enhancing\nthe efficiency and coherence of language model inference. The introduction of\nStructural Embedding Projection (SEP) provides a mechanism for refining token\nrepresentations through projection matrices that integrate hierarchical and\nrelational dependencies. The mathematical formulation of SEP enables embedding\nspaces to capture structured contextual relationships, thereby improving\nsemantic fidelity without significantly increasing computational overhead.\nExperimental evaluations conducted on a range of linguistic datasets revealed\nthat SEP contributed to reductions in perplexity and enhanced contextual\ncoherence, demonstrating its potential to refine language model outputs.\nComputational efficiency assessments highlighted variations across different\ndatasets, suggesting that the integration of structured embeddings introduced\ndataset-dependent trade-offs between inference speed and representational\nrichness. The qualitative analysis of generated responses indicated that SEP\nenhanced narrative consistency and topic alignment, leading to improved fluency\nin multi-sentence text generation. The modifications to embedding layers\nrequired precise optimization to ensure stable training dynamics, as the\nintroduction of structured transformations altered the traditional\nrepresentation-learning process. The architectural adjustments necessary for\nSEP implementation influenced inference latency and memory consumption,\nrequiring a balance between efficiency gains and additional processing demands.\nThe impact of SEP on lexical diversity suggested that embedding modifications\ninfluenced the model's vocabulary usage, reflecting a more context-aware\nselection of generated tokens.",
        "Cybergrooming exploits minors through online trust-building, yet research\nremains fragmented, limiting holistic prevention. Social sciences focus on\nbehavioral insights, while computational methods emphasize detection, but their\nintegration remains insufficient. This review systematically synthesizes both\nfields using the PRISMA framework to enhance clarity, reproducibility, and\ncross-disciplinary collaboration. Findings show that qualitative methods offer\ndeep insights but are resource-intensive, machine learning models depend on\ndata quality, and standard metrics struggle with imbalance and cultural\nnuances. By bridging these gaps, this review advances interdisciplinary\ncybergrooming research, guiding future efforts toward more effective prevention\nand detection strategies.",
        "The rapid advancement of information and communication technology has\nfacilitated easier access to information. However, this progress has also\nnecessitated more stringent verification measures to ensure the accuracy of\ninformation, particularly within the context of Vietnam. This paper introduces\nan approach to address the challenges of Fact Verification using the Vietnamese\ndataset by integrating both sentence selection and classification modules into\na unified network architecture. The proposed approach leverages the power of\nlarge language models by utilizing pre-trained PhoBERT and XLM-RoBERTa as the\nbackbone of the network. The proposed model was trained on a Vietnamese\ndataset, named ISE-DSC01, and demonstrated superior performance compared to the\nbaseline model across all three metrics. Notably, we achieved a Strict Accuracy\nlevel of 75.11\\%, indicating a remarkable 28.83\\% improvement over the baseline\nmodel.",
        "Safety alignment mechanism are essential for preventing large language models\n(LLMs) from generating harmful information or unethical content. However,\ncleverly crafted prompts can bypass these safety measures without accessing the\nmodel's internal parameters, a phenomenon known as black-box jailbreak.\nExisting heuristic black-box attack methods, such as genetic algorithms, suffer\nfrom limited effectiveness due to their inherent randomness, while recent\nreinforcement learning (RL) based methods often lack robust and informative\nreward signals. To address these challenges, we propose a novel black-box\njailbreak method leveraging RL, which optimizes prompt generation by analyzing\nthe embedding proximity between benign and malicious prompts. This approach\nensures that the rewritten prompts closely align with the intent of the\noriginal prompts while enhancing the attack's effectiveness. Furthermore, we\nintroduce a comprehensive jailbreak evaluation framework incorporating\nkeywords, intent matching, and answer validation to provide a more rigorous and\nholistic assessment of jailbreak success. Experimental results show the\nsuperiority of our approach, achieving state-of-the-art (SOTA) performance on\nseveral prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,\nLlama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in\njailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.\nThe codebase for this work is available at\nhttps:\/\/github.com\/Aegis1863\/xJailbreak.",
        "Research on cognitive biases and heuristics has become increasingly popular\nin the visualization literature in recent years. Researchers have studied the\neffects of biases on visualization interpretation and subsequent\ndecision-making. While this work is important, we contend that the view on\nbiases has presented human cognitive abilities in an unbalanced manner, placing\ntoo much emphasis on the flaws and limitations of human decision-making, and\npotentially suggesting that it should not be trusted. Several decision\nresearchers have argued that the flip side of biases -- i.e., mental shortcuts\nor heuristics -- demonstrate human ingenuity and serve as core markers of\nadaptive expertise. In this paper, we review the perspectives and sentiments of\nthe visualization community on biases and describe literature arguing for more\nbalanced views of biases and heuristics. We hope this paper will encourage\nvisualization researchers to consider a fuller picture of human cognitive\nlimitations and strategies for making decisions in complex environments."
      ]
    }
  },
  {
    "id":2411.00749,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"A 2021 update on cancer image analytics with deep learning",
    "start_abstract":"Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification"
      ],
      "abstract":[
        "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "SparAMX: Accelerating Compressed LLMs Token Generation on AMX-powered\n  CPUs",
        "VarGes: Improving Variation in Co-Speech 3D Gesture Generation via\n  StyleCLIPS",
        "Thermodynamics of driven systems with explicitly broken detailed balance",
        "How to Upscale Neural Networks with Scaling Law? A Survey and Practical\n  Guidelines",
        "Charge symmetry breaking in hypernuclei within RMF model",
        "The nonlinear limit of Babinet's Principle",
        "Structural and optical properties of in situ Eu-doped ZnCdO\/ZnMgO\n  superlattices grown by plasma-assisted molecular beam epitaxy",
        "Accenture-NVS1: A Novel View Synthesis Dataset",
        "CANUCS\/Technicolor: JWST Medium Band Photometry Finds Half of the Star\n  Formation at $z>7.5$ is Obscured",
        "Sparse wavefield reconstruction and denoising with boostlets",
        "Theoretical study of the Spectroscopic measurements of Kerr non-linear\n  resonators with four-body interaction",
        "Energy dynamics in a class of local random matrix Hamiltonians",
        "Attention Reallocation: Towards Zero-cost and Controllable Hallucination\n  Mitigation of MLLMs",
        "Pointwise estimates for the fundamental solutions of higher order\n  schr\\\"{o}dinger equations with finite rank perturbations",
        "3D ReX: Causal Explanations in 3D Neuroimaging Classification",
        "Spectroscopy of Supernova Remnants and Candidates in M31",
        "Learning to Unlearn while Retaining: Combating Gradient Conflicts in\n  Machine Unlearning",
        "Masking Countermeasures Against Side-Channel Attacks on Quantum\n  Computers",
        "Nonstatic Reissner-Nordstr$\\phi$m metric in the perturbative $f(R)$\n  theory: Embedding in the background of the FLRW cosmology, uniqueness of\n  solutions, the TOV equation",
        "Data Overvaluation Attack and Truthful Data Valuation",
        "Spatial Transcriptomics Analysis of Spatially Dense Gene Expression\n  Prediction",
        "Shadowing for Infinite Dimensional Dynamical Systems",
        "Quantum-Centric Algorithm for Sample-Based Krylov Diagonalization",
        "OmniParser V2: Structured-Points-of-Thought for Unified Visual Text\n  Parsing and Its Generality to Multimodal Large Language Models",
        "Characterising the Atacama segment of the Chile subduction margin\n  (24{\\deg}S-31{\\deg}S) with >165,000 earthquakes",
        "Milliwatt-level UV generation using sidewall poled lithium niobate",
        "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches",
        "Strategies for political-statement segmentation and labelling in\n  unstructured text",
        "Evidence of Galactic Interaction in the Small Magellanic Cloud Probed by\n  Gaia Selected Massive Star Candidates"
      ],
      "abstract":[
        "Large language models have high compute, latency, and memory requirements.\nWhile specialized accelerators such as GPUs and TPUs typically run these\nworkloads, CPUs are more widely available and consume less energy. Accelerating\nLLMs with CPUs enables broader AI access at a lower cost and power consumption.\nThis acceleration potential for CPUs is especially relevant during the\nmemory-bound decoding stage of LLM inference, which processes one token at a\ntime and is becoming increasingly utilized with reasoning models. We utilize\nAdvanced Matrix Extensions (AMX) support on the latest Intel CPUs together with\nunstructured sparsity to achieve a $1.42 \\times$ reduction in end-to-end\nlatency compared to the current PyTorch implementation by applying our\ntechnique in linear layers. We provide a set of open-source customized sparse\nkernels that can speed up any PyTorch model by automatically replacing all\nlinear layers with our custom sparse implementation. Furthermore, we\ndemonstrate for the first time the use of unstructured sparsity in the\nattention computation achieving a $1.14 \\times$ speedup over the current\nsystems without compromising accuracy. Code:\nhttps:\/\/github.com\/IntelLabs\/Hardware-Aware-Automated-Machine-Learning\/tree\/main\/SparAMX",
        "Generating expressive and diverse human gestures from audio is crucial in\nfields like human-computer interaction, virtual reality, and animation. Though\nexisting methods have achieved remarkable performance, they often exhibit\nlimitations due to constrained dataset diversity and the restricted amount of\ninformation derived from audio inputs. To address these challenges, we present\nVarGes, a novel variation-driven framework designed to enhance co-speech\ngesture generation by integrating visual stylistic cues while maintaining\nnaturalness. Our approach begins with the Variation-Enhanced Feature Extraction\n(VEFE) module, which seamlessly incorporates \\textcolor{blue}{style-reference}\nvideo data into a 3D human pose estimation network to extract StyleCLIPS,\nthereby enriching the input with stylistic information. Subsequently, we employ\nthe Variation-Compensation Style Encoder (VCSE), a transformer-style encoder\nequipped with an additive attention mechanism pooling layer, to robustly encode\ndiverse StyleCLIPS representations and effectively manage stylistic variations.\nFinally, the Variation-Driven Gesture Predictor (VDGP) module fuses MFCC audio\nfeatures with StyleCLIPS encodings via cross-attention, injecting this fused\ndata into a cross-conditional autoregressive model to modulate 3D human gesture\ngeneration based on audio input and stylistic clues. The efficacy of our\napproach is validated on benchmark datasets, where it outperforms existing\nmethods in terms of gesture diversity and naturalness. The code and video\nresults will be made publicly available upon\nacceptance:https:\/\/github.com\/mookerr\/VarGES\/ .",
        "In systems with detailed balance, the stationary distribution and the\nequilibrium distribution are identical, creating a clear connection between\nenergetic and entropic quantities. Many driven systems violate detailed balance\nand still pose a challenge for a consistent thermodynamic interpretation. Even\nsteady-state potentials like entropy or free energy are no longer state\nvariables. Here, we use a framework for systems with broken detailed balance,\nwhere Boltzmann entropy can be computed while properly taking constraints on\nstate transitions into account. As an illustration, we establish the\nthermodynamic relations for arbitrarily driven sample space-reducing processes\nthat are non-equilibrium but show steady states. We demonstrate that, despite\nexplicitly broken detailed balance, it remains feasible to define and\nunambiguously interpret the effective thermodynamic potentials.",
        "Neural scaling laws have revolutionized the design and optimization of\nlarge-scale AI models by revealing predictable relationships between model\nsize, dataset volume, and computational resources. Early research established\npower-law relationships in model performance, leading to compute-optimal\nscaling strategies. However, recent studies highlighted their limitations\nacross architectures, modalities, and deployment contexts. Sparse models,\nmixture-of-experts, retrieval-augmented learning, and multimodal models often\ndeviate from traditional scaling patterns. Moreover, scaling behaviors vary\nacross domains such as vision, reinforcement learning, and fine-tuning,\nunderscoring the need for more nuanced approaches. In this survey, we\nsynthesize insights from over 50 studies, examining the theoretical\nfoundations, empirical findings, and practical implications of scaling laws. We\nalso explore key challenges, including data efficiency, inference scaling, and\narchitecture-specific constraints, advocating for adaptive scaling strategies\ntailored to real-world applications. We suggest that while scaling laws provide\na useful guide, they do not always generalize across all architectures and\ntraining strategies.",
        "We study the charge symmetry breaking (CSB) effect in the binding energy of\nmirror hypernuclei in the mass region $A=7\\sim 48$ in relativistic mean field\n(RMF) models introducing $NN$ and $\\Lambda N$ interactions. The\nphenomenological $\\Lambda N$ CSB interaction is introduced and the strength\nparameter is fitted to reproduce the experimental binding energy difference\nbetween the mirror hypernuclei $^{12}_\\Lambda$B and $^{12}_\\Lambda$C. This\nmodel is applied to calculate the CSB energy anomaly in mirror hypernuclei with\nthe mass $A=7\\sim48$. The model is further applied to predict the binding\nenergy difference of mirror hypernuclei of $A$=40 with the isospin $T=1\/2$,\n$3\/2$ and $5\/2$ nuclei together with various hyper Ca isotopes and their mirror\nhypernuclei. Finally the binding energy systematics of $A=$48 hypernuclei are\npredicted with\/without the CSB effect by the PK1 and TM2 energy density\nfunctionals (EDFs).",
        "Babinet's principle is a powerful tool for predicting the scattering behavior\nof planar structures where the solution for the complementary structure is\nalready known. This makes it ubiquitous in the design of aperture antennas or\nmetamaterials. Even for plasmonic nanostructures, a qualitative match of the\nbehavior for complementary structures has been reported. Here, we discuss\nwhether Babinet's principle can be extended to nonlinear scattering. We compare\nthe third harmonic emission of plasmonic nanorods and complementary nanoslits\nby far field imaging and simulation. We find significantly different far field\nimages, in agreement between experiment and simulation. We explain these\ndifferences by the higher spatial resolution at the third harmonic wavelength\nand by additional eddy currents in slits that are not present in rods. Within\nthese limits, Babinet's principle can guide the design of inverted nonlinear\nplasmonic resonators, which promise to be more stable at high excitation power\ndue to better thermal conductivity.",
        "In situ Eu-doped ZnCdO-ZnMgO superlattices with varying ZnCdO:Eu and ZnMgO\nsublayers thicknesses were deposited by plasma assisted molecular beam epitaxy.",
        "This paper introduces ACC-NVS1, a specialized dataset designed for research\non Novel View Synthesis specifically for airborne and ground imagery. Data for\nACC-NVS1 was collected in Austin, TX and Pittsburgh, PA in 2023 and 2024. The\ncollection encompasses six diverse real-world scenes captured from both\nairborne and ground cameras, resulting in a total of 148,000 images. ACC-NVS1\naddresses challenges such as varying altitudes and transient objects. This\ndataset is intended to supplement existing datasets, providing additional\nresources for comprehensive research, rather than serving as a benchmark.",
        "We present a sample of 146 high-redshift ($z>7.5$) galaxies from the\nCANUCS\/Technicolor surveys, showcasing photometry in every wide- and\nmedium-band NIRCam filter in addition to ancillary HST data sampling $0.4-5 \\mu\nm$ (22 JWST bands out of 29 bands total). Additionally, 48 ($33\\%$) galaxies in\nour sample meet criteria to be classified as extreme emission line galaxies, 15\n($10\\%$) of which are completely missed by typical dropout selections due to\nfaint UV emission. By fitting the SEDs covering the rest-frame UV to optical at\n$z > 7.5$, we investigate the dust obscuration properties, giving an unbiased\nview of dust buildup in high-redshift galaxies free from spectroscopic\nfollow-up selection effects. Dust attenuation correlates with stellar mass, but\nmore strongly with star formation rate. We find typical galaxies at $z>7.5$\nhave $\\sim 25 \\%$ of their star formation obscured. However, since galaxies\nwith higher star formation rates suffer more attenuation, $\\sim 50 \\%$ of the\ntotal star formation rate density at $7.5<z<9$ is obscured. The obscured\nfraction drops to $\\sim 35 \\%$ in our $9<z<12$ bin, possibly due to substantial\ndust grain growth in the interstellar medium not having time to occur.\nExtrapolating the decline in dust obscuration of galaxies to higher redshifts,\nwe infer that dust obscuration should approach zero at $z > 15$, implying that\nepoch as when dust first forms in bright galaxies.",
        "Boostlets are spatiotemporal functions that decompose nondispersive\nwavefields into a collection of localized waveforms parametrized by dilations,\nhyperbolic rotations, and translations. We study the sparsity properties of\nboostlets and find that the resulting decompositions are significantly sparser\nthan those of other state-of-the-art representation systems, such as wavelets\nand shearlets. This translates into improved denoising performance when\nhard-thresholding the boostlet coefficients. The results suggest that boostlets\noffer a natural framework for sparsely decomposing wavefields in unified\nspace-time.",
        "Quantum annealing provides a promising way to solve combinational\noptimization problems where the solutions correspond to the ground state of the\nIsing Hamiltonian. We can implement quantum annealing using the Kerr non-linear\nresonators, with bifurcation phenomena emerging when subjected to a parametric\ndrive. These bifurcated states can function as bases of qubits. Moreover,\nintegrating four-body interactions between physical qubits enables the\nestablishment of effective all-to-all long-range interactions between logical\nqubits, which is essential for practical quantum annealing. While theoretical\nproposals exist for creating four-body interactions within Kerr non-linear\nresonators, there has not been experimental verification through their\nspectroscopic signatures. In this paper, we theoretically investigate the\nspectroscopic measurements of Kerr non-linear resonators featuring four-body\ninteraction. We identify six distinct frequencies exhibiting population changes\nby employing resonant driving on one resonator and weak driving on another.\nAnalytical and numerical calculations validate these findings. Our study\ndemonstrates the potential of spectroscopy in characterizing systems with\nfour-body interactions, offering insights for realizing quantum annealing with\nKerr parametric oscillators.",
        "Random matrix theory yields valuable insights into the universal features of\nquantum many-body chaotic systems. Although all-to-all interactions are\ntraditionally studied, many interesting dynamical questions, such as transport\nof a conserved density, require a notion of spatially local interactions. We\nstudy the transport of the energy, the most basic conserved density, in\nfew-body and 1D chains of nearest-neighbor random matrix terms that square to\none. In the few-body but large local Hilbert space dimension case, we develop a\nmapping for the energy dynamics to a single-particle hopping picture. This\nallows for the computation of the energy density autocorrelators and an\nout-of-time-ordered correlator of the energy density. In the 1D chain, we\nnumerically study the energy transport for a small local Hilbert space\ndimension. We also discuss the density of states throughout and touch upon the\nrelation to free probability theory.",
        "Multi-Modal Large Language Models (MLLMs) stand out in various tasks but\nstill struggle with hallucinations. While recent training-free mitigation\nmethods mostly introduce additional inference overhead via retrospection\nstrategy and contrastive decoding, we propose attention reallocation (AttnReal)\nto mitigate hallucinations with nearly zero extra cost. Our approach is\nmotivated by the key observations that, MLLM's unreasonable attention\ndistribution causes features to be dominated by historical output tokens, which\nfurther contributes to hallucinated responses because of the distribution gap\nbetween different token types. Based on the observations, AttnReal recycles\nexcessive attention from output tokens and reallocates it to visual tokens,\nwhich reduces MLLM's reliance on language priors and ensures the decoding\nprocess depends more on the visual inputs. More interestingly, we find that, by\ncontrolling the intensity of AttnReal, we can achieve a wide-range trade-off\nbetween the response faithfulness and overall performance. Comprehensive\nresults from different benchmarks validate the effectiveness of AttnReal across\nsix open-source MLLMs and three decoding strategies.",
        "This paper is dedicated to studying pointwise estimates of the fundamental\nsolution for the higher order Schr\\\"{o}dinger equation: % we investigate the\nfundamental solution of the higher order Schr\\\"{o}dinger equation\n$$i{\\partial}_{t}u(x,t)=Hu(x,t),\\ \\ \\ t\\in \\mathbb{R},\\ x\\in\n{\\mathbb{R}}^{n},$$ where the Hamiltonian $H$ is defined as\n$$H={(-\\Delta)}^{m}+\\displaystyle\\sum_{j=1}^{N} \\langle\\cdotp ,{\\varphi }_{j}\n\\rangle{\\varphi }_{j},$$ with each $\\varphi_j$ ($1\\le j\\le N$) satisfying\ncertain smoothness and decay conditions. %Let ${P}_{ac}(H)$ denote the\nprojection onto the absolutely continuous space of $H$. We show that for any\npositive integer $m>1$ and spatial dimension $n\\ge 1$, %under a spectral\nassumption, the operator is sharp in the sense that it\n  ${e}^{-i tH}P_{ac}(H)$ has an integral kernel $K(t,x,y)$ satisfying the\nfollowing pointwise estimate: $$\\left |K(t,x,y)\\right |\\lesssim\n|t|^{-\\frac{n}{2m}}(1+|t|^{-\\frac{1}{2m}}\\left | x-y\\right\n|)^{-\\frac{n(m-1)}{2m-1}} ,\\ \\ t\\ne 0,\\ x,y\\in {\\mathbb{R}}^{n}.$$ This\nestimate is consistent with the upper bounds in the free case. As an\napplication, we derive $L^p-L^q$ decay estimates for the propagator ${e}^{-\\i\ntH}P_{ac}(H)$, where the pairs $(1\/p, 1\/q)$ lie within a quadrilateral region\nin the plane.",
        "Explainability remains a significant problem for AI models in medical\nimaging, making it challenging for clinicians to trust AI-driven predictions.\nWe introduce 3D ReX, the first causality-based post-hoc explainability tool for\n3D models. 3D ReX uses the theory of actual causality to generate\nresponsibility maps which highlight the regions most crucial to the model's\ndecision. We test 3D ReX on a stroke detection model, providing insight into\nthe spatial distribution of features relevant to stroke.",
        "With a star formation rate of order 0.4 M$_\\odot $ yr$^{-1}$, M31 should have\nsignificant population of supernova remnants (SNRs), and, in fact, 156 SNR and\nSNR candidates have been suggested by Lee et al. (2014) by searching for\nnebulae with elevated [SII]\/H${\\alpha}$ ratios in narrow band images. Here we\nuse a combination of low and high resolution optical spectroscopy obtained with\nHectospec on the MMT to characterize 152 of these nebulae. Of these candidates,\nwe find 93 nebulae that have [SII]\/H${\\alpha}$ ratios that exceed 0.4, the\ntraditional ratio used to separate SNRs from HII regions, strongly suggesting\nthat at least these objects are SNRs. Our high resolution spectroscopy reveals\n108 nebulae that have velocity widths in H${\\alpha} $ (full-width at 20% peak\nflux) that exceed 50 km s$^{-1}$, significantly larger than found in HII\nregions. There are 72 objects that satisfy both tests. Here we discuss the\nspectroscopic characteristics of all of the objects in our sample, and the\nlikelihood that other objects in the sample of Lee et al. are also SNRs, and we\nbriefly consider confirmation by X-ray, radio and UV observations. We also\ndiscuss several new candidates that have been identified serendipitously in the\ncourse of examining a large amount of archival Hectospec data.",
        "Machine Unlearning has recently garnered significant attention, aiming to\nselectively remove knowledge associated with specific data while preserving the\nmodel's performance on the remaining data. A fundamental challenge in this\nprocess is balancing effective unlearning with knowledge retention, as naive\noptimization of these competing objectives can lead to conflicting gradients,\nhindering convergence and degrading overall performance. To address this issue,\nwe propose Learning to Unlearn while Retaining, aimed to mitigate gradient\nconflicts between unlearning and retention objectives. Our approach\nstrategically avoids conflicts through an implicit gradient regularization\nmechanism that emerges naturally within the proposed framework. This prevents\nconflicting gradients between unlearning and retention, leading to effective\nunlearning while preserving the model's utility. We validate our approach\nacross both discriminative and generative tasks, demonstrating its\neffectiveness in achieving unlearning without compromising performance on\nremaining data. Our results highlight the advantages of avoiding such gradient\nconflicts, outperforming existing methods that fail to account for these\ninteractions.",
        "We propose a modification to the transpiler of a quantum computer to\nsafeguard against side-channel attacks aimed at learning information about a\nquantum circuit. We demonstrate that if it is feasible to shield a specific\nsubset of gates from side-channel attacks, then it is possible to conceal all\ninformation in a quantum circuit by transpiling it into a new circuit whose\ndepth grows linearly, depending on the quantum computer's architecture. We\nprovide concrete examples of implementing this protection on IBM's quantum\ncomputers, utilizing their virtual gates and editing their transpiler.",
        "This article introduces a nonstatic Reissner-Nordstr$\\phi$m metric, a metric\nthat does not emit electromagnetic waves but can emit gravitational waves. We\nfirst use the GR theory to study a charged spherically symmetric gravitational\nsource (CSSGS), the obtained results are further improved in comparison with\nthe previous studies. In particular, this article considers that the field is\nnot necessarily static. The metric tensors $ g_{\\mu\\nu} $ are considered both\noutside and inside the gravitational source (the results show that in the first\ncase $ g_{\\mu\\nu} $ are time independent, in the latter case they are time\ndependent). The gravitational acceleration and the event horizon of a charged\nblack hole are investigated. The results prove that the gravitational field is\nalways attractive. We then use the perturbative $ f(R) $ theory to consider\nCSSGS. The obtained results not only correct the solution of Einstein's\nequation in magnitude (this will describe astronomical and cosmological\nquantities more accurately than Einstein's equation), but also reveal new\neffects. Outside the gravitational source, the metric tensors can depend on\ntime, this makes it possible for a spherically symmetric gravitational source\nto emit gravitational waves (Einstein's equation cannot give this effect).\nHowever, a spherically symmetric field still does not emit electromagnetic\nwaves. Next we present a new method for embedding the spherically symmetric\nmetrics of a star (or a black hole) in the background of the FLRW cosmological.\nFinally, we discuss the uniqueness of the solutions of the f(R) theory. The\nperturbative TOV equation is also found.",
        "In collaborative machine learning, data valuation, i.e., evaluating the\ncontribution of each client' data to the machine learning model, has become a\ncritical task for incentivizing and selecting positive data contributions.\nHowever, existing studies often assume that clients engage in data valuation\ntruthfully, overlooking the practical motivation for clients to exaggerate\ntheir contributions. To unlock this threat, this paper introduces the first\ndata overvaluation attack, enabling strategic clients to have their data\nsignificantly overvalued. Furthermore, we propose a truthful data valuation\nmetric, named Truth-Shapley. Truth-Shapley is the unique metric that guarantees\nsome promising axioms for data valuation while ensuring that clients' optimal\nstrategy is to perform truthful data valuation. Our experiments demonstrate the\nvulnerability of existing data valuation metrics to the data overvaluation\nattack and validate the robustness and effectiveness of Truth-Shapley.",
        "Spatial transcriptomics (ST) measures gene expression at fine-grained spatial\nresolution, offering insights into tissue molecular landscapes. Previous\nmethods for spatial gene expression prediction usually crop spots of interest\nfrom pathology tissue slide images, and learn a model that maps each spot to a\nsingle gene expression profile. However, it fundamentally loses spatial\nresolution of gene expression: 1) each spot often contains multiple cells with\ndistinct gene expression; 2) spots are cropped at fixed resolutions, limiting\nthe ability to predict gene expression at varying spatial scales. To address\nthese limitations, this paper presents PixNet, a dense prediction network\ncapable of predicting spatially resolved gene expression across spots of\nvarying sizes and scales directly from pathology images. Different from\nprevious methods that map individual spots to gene expression values, we\ngenerate a dense continuous gene expression map from the pathology image, and\naggregate values within spots of interest to predict the gene expression. Our\nPixNet outperforms state-of-the-art methods on 3 common ST datasets, while\nshowing superior performance in predicting gene expression across multiple\nspatial scales. The source code will be publicly available.",
        "In this paper we extend some results about Shadowing Lemma there are known on\nfinite dimensional compact manifolds without border and $\\mathbb{R}^n$, to an\ninfinite dimensional space. In fact, we prove that if $\\{\\mathcal{T}(t):t\\ge\n0\\}$ is a Morse-Smale semigroup defined in a Hilbert space with global\nattractor $\\mathcal{A}$, then $\\mathcal{T}(1)|_{\\mathcal{A}}:\\mathcal{A}\\to\n\\mathcal{A} $ admits the Lipschitz Shadowing property. Moreover, for any\npositively invariant bounded neighborhood $\\mathcal{U}\\supset\\mathcal{A}$ of\nthe global attractor, the map $\\mathcal{T}(1)|_{\\mathcal{U}}:\\mathcal{U}\\to\n\\mathcal{U}$ has the H\\\"{o}lder-Shadowing property. As applications, we obtain\nnew results related to the structural stability of Morse-Smale semigroups\ndefined in Hilbert spaces and continuity of global attractors.",
        "Approximating the ground state of many-body systems is a key computational\nbottleneck underlying important applications in physics and chemistry. It has\nlong been viewed as a promising application for quantum computers. The most\nwidely known quantum algorithm for ground state approximation, quantum phase\nestimation, is out of reach of current quantum processors due to its high\ncircuit-depths. Quantum diagonalization algorithms based on subspaces represent\nalternatives to phase estimation, which are feasible for pre-fault-tolerant and\nearly-fault-tolerant quantum computers. Here, we introduce a quantum\ndiagonalization algorithm which combines two key ideas on quantum subspaces: a\nclassical diagonalization based on quantum samples, and subspaces constructed\nwith quantum Krylov states. We prove that our algorithm converges in polynomial\ntime under the working assumptions of Krylov quantum diagonalization and\nsparseness of the ground state. We then show numerical investigations of\nlattice Hamiltonians, which indicate that our method can outperform existing\nKrylov quantum diagonalization in the presence of shot noise, making our\napproach well-suited for near-term quantum devices. Finally, we carry out the\nlargest ground-state quantum simulation of the single-impurity Anderson model\non a system with $41$ bath sites, using $85$ qubits and up to $6 \\cdot 10^3$\ntwo-qubit gates on a Heron quantum processor, showing excellent agreement with\ndensity matrix renormalization group calculations.",
        "Visually-situated text parsing (VsTP) has recently seen notable advancements,\ndriven by the growing demand for automated document understanding and the\nemergence of large language models capable of processing document-based\nquestions. While various methods have been proposed to tackle the complexities\nof VsTP, existing solutions often rely on task-specific architectures and\nobjectives for individual tasks. This leads to modal isolation and complex\nworkflows due to the diversified targets and heterogeneous schemas. In this\npaper, we introduce OmniParser V2, a universal model that unifies VsTP typical\ntasks, including text spotting, key information extraction, table recognition,\nand layout analysis, into a unified framework. Central to our approach is the\nproposed Structured-Points-of-Thought (SPOT) prompting schemas, which improves\nmodel performance across diverse scenarios by leveraging a unified\nencoder-decoder architecture, objective, and input\\&output representation. SPOT\neliminates the need for task-specific architectures and loss functions,\nsignificantly simplifying the processing pipeline. Our extensive evaluations\nacross four tasks on eight different datasets show that OmniParser V2 achieves\nstate-of-the-art or competitive results in VsTP. Additionally, we explore the\nintegration of SPOT within a multimodal large language model structure, further\nenhancing text localization and recognition capabilities, thereby confirming\nthe generality of SPOT prompting technique. The code is available at\n\\href{https:\/\/github.com\/AlibabaResearch\/AdvancedLiterateMachinery}{AdvancedLiterateMachinery}.",
        "The Atacama segment in Northern Chile (24{\\deg}S to 31{\\deg}S) is a mature\nseismic gap with no major event (Mw>8) since 1922. In addition to regular\nseismicity, around the subducting Copiap\\'o ridge, the region hosts seismic\nswarms, and shallow and deep slow slip events. To characterize the fine\nstructure of this seismic gap and its seismic-aseismic interplay, we\ninstrumented the region with almost 200 seismic and geodetic stations. Using\nmachine learning, we derived a dense, high-resolution seismicity catalog,\nencompassing over 165,000 events with double-difference relocated hypocenters.\nOur catalog details the outer rise, interface, intraslab, crustal and mantle\nwedge seismicity. We infer a detailed slab geometry, showing that the flat slab\nis dipping towards the south with a narrower extent along dip. The slab\ngeometry controls the intraslab seismicity, with cross-cutting activity in the\nregion of highest bending and a downdip limit around 105 km slab depth. Our\ncatalogue exhibits significant seismicity in the mantle wedge upper corner\nbetween 28{\\deg}S and 31{\\deg}S, highlighting the brittle behavior of the cold\nnose. On the subduction interface, interplate locking controls the updip end of\nthe seismicity, with seismicity extending closer to the trench in low-locking\nareas. On fine scales, resolved by relative uncertainties below 50 m, the\nsubduction interface has a complex 3D structure, showing a fractal distribution\nof seismic patches down to a scale of tens of meters. Our results provide a\nholistic view of this complex subduction zone, while at the same time giving\ninsights into fine-scale structures and processes.",
        "Integrated coherent sources of ultra-violet (UV) light are essential for a\nwide range of applications, from ion-based quantum computing and optical clocks\nto gas sensing and microscopy. Conventional approaches that rely on UV gain\nmaterials face limitations in terms of wavelength versatility; in response\nfrequency upconversion approaches that leverage various optical nonlinearities\nhave received considerable attention. Among these, the integrated thin-film\nlithium niobate (TFLN) photonic platform shows particular promise owing to\nlithium niobate's transparency into the UV range, its strong second order\nnonlinearity, and high optical confinement. However, to date, the high\npropagation losses and lack of reliable techniques for consistent poling of\ncm-long waveguides with small poling periods have severely limited the utility\nof this platform. Here we present a sidewall poled lithium niobate (SPLN)\nwaveguide approach that overcomes these obstacles and results in a more than\ntwo orders of magnitude increase in generated UV power compared to the\nstate-of-the-art. Our UV SPLN waveguides feature record-low propagation losses\nof 2.3 dB\/cm, complete domain inversion of the waveguide cross-section, and an\noptimum 50% duty cycle, resulting in a record-high normalized conversion\nefficiency of 5050 %W$^{-1}$cm$^{-2}$, and 4.2 mW of generated on-chip power at\n390 nm wavelength. This advancement makes the TFLN photonic platform a viable\noption for high-quality on-chip UV generation, benefiting emerging\napplications.",
        "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers.",
        "Analysis of parliamentary speeches and political-party manifestos has become\nan integral area of computational study of political texts. While speeches have\nbeen overwhelmingly analysed using unsupervised methods, a large corpus of\nmanifestos with by-statement political-stance labels has been created by the\nparticipants of the MARPOR project. It has been recently shown that these\nlabels can be predicted by a neural model; however, the current approach relies\non provided statement boundaries, limiting out-of-domain applicability. In this\nwork, we propose and test a range of unified split-and-label frameworks --\nbased on linear-chain CRFs, fine-tuned text-to-text models, and the combination\nof in-context learning with constrained decoding -- that can be used to jointly\nsegment and classify statements from raw textual data. We show that our\napproaches achieve competitive accuracy when applied to raw text of political\nmanifestos, and then demonstrate the research potential of our method by\napplying it to the records of the UK House of Commons and tracing the political\ntrajectories of four major parties in the last three decades.",
        "We present identifications and kinematic analysis of 7,426 massive\n($\\mathrm{\\geq}8M_{\\odot}$) stars in the Small Magellanic Cloud (SMC), using\nGaia DR3 data. We used Gaia ($G_\\mathrm{BP}-G_\\mathrm{RP}$, $G$)\ncolor-magnitude diagram to select the population of massive stars, and parallax\nto omit foreground objects. The spatial distribution of the 7,426 massive star\ncandidates is generally consistent with the spatial distribution of the\ninterstellar medium, such as H$\\alpha$ and H i emission. The identified massive\nstars show inhomogeneous distributions over the galaxy, showing several\nsuperstructures formed by massive stars with several hundred parsecs scale. The\nstellar superstructures defined by the surface density have opposite mean\nproper motions in the east and west, moving away from each other. Similarly,\nthe mean line-of-sight velocities of the superstructures are larger to the\nsoutheast and smaller to the northwest. The different east-west properties of\nthe superstructures' proper motion, line-of-sight velocity indicate that the\nSMC is being stretched by tidal forces and\/or ram pressure from the Large\nMagellanic Cloud to the southeast, thereby rejecting the presence of galaxy\nrotation in the SMC."
      ]
    }
  },
  {
    "id":2411.00749,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification",
    "start_abstract":"Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "A 2021 update on cancer image analytics with deep learning"
      ],
      "abstract":[
        "Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Weight Distribution of the Weighted Coordinates Poset Block Space and\n  Singleton Bound",
        "Feedback-augmented Non-homogeneous Hidden Markov Models for Longitudinal\n  Causal Inference",
        "Nonlocal characteristics of two-qubit gates and their argand diagrams",
        "Stability of the Euclidean 3-ball under L2-curvature pinching",
        "Observational and Theoretical Constraints on First-Order Phase\n  Transitions in Neutron Stars",
        "Breather interactions in the integrable discrete Manakov system and\n  trigonometric Yang-Baxter maps",
        "Structure evolution with cosmic backgrounds from radio to far infrared",
        "A Family of Semi-norms in $C^*$-algebras",
        "An Iterative Block Matrix Inversion (IBMI) Algorithm for Symmetric\n  Positive Definite Matrices with Applications to Covariance Matrices",
        "Mean-Field Analysis of Latent Variable Process Models on Dynamically\n  Evolving Graphs with Feedback Effects",
        "Dirichlet problem for diffusions with jumps",
        "Remote preparation of motional Schr\\\"{o}dinger cat states via\n  dissipatively-driven non-Gaussian mechanical entanglement",
        "Frequency-resolved time lags due to X-ray disk reprocessing in AGN",
        "The rise of the galactic empire: luminosity functions at $z\\sim17$ and\n  $z\\sim25$ estimated with the MIDIS$+$NGDEEP ultra-deep JWST\/NIRCam dataset",
        "Subdomains of Post-COVID-Syndrome (PCS) -- A Population-Based Study",
        "Function-Coherent Gambles",
        "Generators of the algebraic symplectic bordism ring",
        "Improvements to monoscopic analysis for imaging atmospheric Cherenkov\n  telescopes: Application to H.E.S.S",
        "Theory of Irreversibility in Quantum Many-Body Systems",
        "Strictly Metrizable Graphs are Minor-Closed",
        "Supersymmetry in nonlinear and linear Quantum Optics: the Kerr-like and\n  multiphoton Jaynes-Cummings models",
        "Electron scale magnetic holes generation driven by Whistler-to-Bernstein\n  mode conversion in fully kinetic plasma turbulence",
        "Gradient-free Importance Sampling Scheme for Efficient Reliability\n  Estimation",
        "Ensemble-averaged mean-field many-body level density: an indicator of\n  integrable versus chaotic single-particle dynamics",
        "Deep Learning-Based Diffusion MRI Tractography: Integrating Spatial and\n  Anatomical Information",
        "Geometric Properties of Periodic Lattices in Function Fields",
        "Consonance in music -- the Pythagorean approach revisited",
        "Perturbing finite temperature multicomponent DFT 1D Kohn-Sham systems:\n  Peierls Gap & Kohn Anomaly",
        "Rough estimates of solar system gravitomagnetic effects in\n  post-Newtonian gravity"
      ],
      "abstract":[
        "In this paper, we determine the complete weight distribution of the space $\n\\mathbb{F}_q^N $ endowed by the weighted coordinates poset block metric\n($(P,w,\\pi)$-metric), also known as the $(P,w,\\pi)$-space, thereby obtaining it\nfor $(P,w)$-space, $(P,\\pi)$-space, $\\pi$-space, and $P$-space as special\ncases. Further, when $P$ is a chain, the resulting space is called as\nNiederreiter-Rosenbloom-Tsfasman (NRT) weighted block space and when $P$ is\nhierarchical, the resulting space is called as weighted coordinates\nhierarchical poset block space. The complete weight distribution of both the\nspaces are deduced from the main result. Moreover, we define an $I$-ball for an\nideal $I$ in $P$ and study the characteristics of it in $(P,w,\\pi)$-space.\n  We investigate the relationship between the $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Given an ideal $I$, we investigate how the maximum\ndistance separability (MDS) is related with $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Duality theorem is derived for an MDS\n$(P,w,\\pi)$-code when all the blocks are of same length. Finally, the\ndistribution of codewords among $r$-balls is analyzed in the case of chain\nposet, when all the blocks are of same length.",
        "Hidden Markov models are widely used for modeling sequential data but\ntypically have limited applicability in observational causal inference due to\ntheir strong conditional independence assumptions. I introduce\nfeedback-augmented non-homogeneous hidden Markov model (FAN-HMM), which\nincorporate time-varying covariates and feedback mechanisms from past\nobservations to latent states and future responses. Integrating these models\nwith the structural causal model framework allows flexible causal inference in\nlongitudinal data with time-varying unobserved heterogeneity and multiple\ncausal pathways. I show how, in a common case of categorical response\nvariables, long-term causal effects can be estimated efficiently without the\nneed for simulating counterfactual trajectories. Using simulation experiments,\nI study the performance of FAN-HMM under the common misspecification of the\nnumber of latent states, and finally apply the proposed approach to estimate\nthe effect of the 2013 parental leave reform on fathers' paternal leave uptake\nin Finnish workplaces.",
        "In this paper, we show the usefulness of the chords present in the argand\ndiagram of squared eigenvalues of nonlocal part of two-qubit gates to study\ntheir nonlocal characteristics. We discuss the criteria for perfect entanglers\nto transform a pair of orthonormal product states into a pair of orthonormal\nmaximally entangled states. Perfect entanglers with a chord passing through\norigin can do such a transformation. In the Weyl chamber, we identify the\nregions of perfect entanglers with at least one chord passing through origin.\nWe also provide the conditions for a perfect entangler without any chord\npassing through origin to transform a pair of orthonormal product states into\northonormal maximally entangled states. Finally, we show that similar to\nentangling power, gate typicality can also be described using the chords\npresent in the argand diagram. For each chord describing the entangling power,\nthere exists a chord describing the gate typicality. We show the geometrical\nrelation between the two sets of chords.",
        "In this article, we consider compact Riemannian 3-manifolds with boundary. We\nprove that if the $L^2$-norm of the curvature is small and if the\n$H^{1\/2}$-norm of the difference of the fundamental forms of the boundary is\nsmall, then the manifold is diffeomorphic to the Euclidean ball. Moreover, we\nobtain that the manifold and the ball are metrically close (uniformly and in\n$H^2$-norm), with a quantitative, optimal bound. The required smallness\nassumption only depends on the volumes of the manifold and its boundary and on\na trace and Sobolev constant of the manifold. The proof only relies on\nelementary computations based on the Bochner formula for harmonic functions and\ntensors, and on the 2-spheres effective uniformisation result of\nKlainerman-Szeftel.",
        "Understanding the equation of state (EOS) of neutron stars (NSs) is a\nfundamental challenge in astrophysics and nuclear physics. A first-order phase\ntransition (FOPT) at high densities could lead to the formation of a quark\ncore, significantly affecting NS properties. This review explores observational\nand theoretical constraints on such transitions using multi-messenger\nastrophysics. X-ray observations, including mass-radius measurements from NICER\nand spectral features like quasi-periodic oscillations (QPOs) and cyclotron\nresonance scattering features (CRSFs), provide indirect evidence of EOS\nmodifications. Gravitational wave detections, particularly from binary NS\nmergers such as GW170817, constrain tidal deformability and post-merger\noscillations, which may carry signatures of phase transitions. Pulsar timing\noffers additional constraints through measurements of mass, spin evolution, and\nglitches, with millisecond pulsars exceeding twice the solar mass posing\nchallenges to purely hadronic EOSs. Theoretical models and numerical\nsimulations predict that an FOPT could impact gravitational wave signals,\ntwin-star configurations, and NS cooling. Future advancements, including\nnext-generation gravitational wave detectors, high-precision X-ray telescopes,\nand improved theoretical modeling, will enhance our ability to probe phase\ntransitions in NSs. A combination of these approaches will provide crucial\ninsights into the existence and properties of deconfined quark matter in NS\ninteriors.",
        "The goal of this work is to obtain a complete characterization of soliton and\nbreather interactions in the integrable discrete Manakov (IDM) system, a vector\ngeneralization of the Ablowitz-Ladik model. The IDM system, which in the\ncontinuous limit reduces to the Manakov system (i.e., a 2-component vector\nnonlinear Schrodinger equation), was shown to admit a variety of discrete\nvector soliton solutions: fundamental solitons, fundamental breathers, and\ncomposite breathers. While the interaction of fundamental solitons was studied\nearly on, no results are presently available for other types of\nsoliton-breather and breather-breather interactions. Our study reveals that\nupon interacting with a fundamental breather, a fundamental soliton becomes a\nfundamental breather. Conversely, the interaction of two fundamental breathers\ngenerically yields two fundamental breathers with polarization shifts, but may\nalso result in a fundamental soliton and a fundamental breather. Composite\nbreathers interact trivially both with each other and with a fundamental\nsoliton or breather. Explicit formulas for the scattering coefficients that\ncharacterize fundamental and composite breathers are given. This allows us to\ninterpret the interactions in terms of a refactorization problem and derive the\nassociated Yang-Baxter maps describing the effect of interactions on the\npolarizations. These give the first examples of parametric Yang-Baxter maps of\ntrigonometric type.",
        "Cosmic background radiation, both diffuse and discrete in nature, produced at\ndifferent cosmic epochs before and after recombination, provides key\ninformation on the evolution of cosmic structures. We discuss the main classes\nof sources that contribute to the extragalactic background light from radio to\nsub-millimetre wavelenghs and the currently open question on the level of the\ncosmic radio background spectrum. The redshifted 21cm line signal from\ncosmological neutral Hydrogen during the primeval phases of cosmic structures\nas a probe of the cosmological reionisation process is presented, along with\nthe route for confident detection of this signal. We then describe the basic\nformalism and the feasibility to study via a differential approach, based\nmainly on dipole analysis, the tiny imprints in the CB spectrum expected from a\nvariety of cosmological and astrophysical processes at work during the early\nphases of cosmic perturbation and structure evolution. Finally, we discuss the\nidentification of high-redshift sub-millimetre lensed galaxies with extreme\nmagnifications in the Planck maps and their use for the comprehension of\nfundamental processes in early galaxy formation and evolution.",
        "We introduce a new family of non-negative real-valued functions on a\n$C^*$-algebra $\\mathcal{A}$, i.e., for $0\\leq \\mu \\leq 1,$\n$$\\|a\\|_{\\sigma_{\\mu}}= \\text{sup}\\left\\lbrace \\sqrt{|f(a)|^2 \\sigma_{\\mu}\nf(a^*a)}: f\\in \\mathcal{A}', \\, f(1)=\\|f\\|=1 \\right\\rbrace, \\quad $$ where\n$a\\in \\mathcal{A}$ and $\\sigma_{\\mu}$ is an interpolation path of the symmetric\nmean $\\sigma$. These functions are semi-norms as they satisfy the norm axioms,\nexcept for the triangle inequality. Special cases satisfying triangle\ninequality, and a complete equality characterization is also discussed. Various\nbounds and relationships will be established for this new family, with a\nconnection to the existing literature in the algebra of all bounded linear\noperators on a Hilbert space.",
        "Obtaining the inverse of a large symmetric positive definite matrix\n$\\mathcal{A}\\in\\mathbb{R}^{p\\times p}$ is a continual challenge across many\nmathematical disciplines. The computational complexity associated with direct\nmethods can be prohibitively expensive, making it infeasible to compute the\ninverse. In this paper, we present a novel iterative algorithm (IBMI), which is\ndesigned to approximate the inverse of a large, dense, symmetric positive\ndefinite matrix. The matrix is first partitioned into blocks, and an iterative\nprocess using block matrix inversion is repeated until the matrix approximation\nreaches a satisfactory level of accuracy. We demonstrate that the two-block,\nnon-overlapping approach converges for any positive definite matrix, while\nnumerical results provide strong evidence that the multi-block, overlapping\napproach also converges for such matrices.",
        "In this paper, we study the asymptotic behavior of a class of dynamic\nco-evolving latent space networks. The model we study is subject to\nbi-directional feedback effects, meaning that at any given time, the latent\nprocess depends on its own value and the graph structure at the previous time\nstep, and the graph structure at the current time depends on the value of the\nlatent processes at the current time but also on the graph structure at the\nprevious time instance (sometimes called a persistence effect). We construct\nthe mean-field limit of this model, which we use to characterize the limiting\nbehavior of a random sample taken from the latent space network in the limit as\nthe number of nodes in the network diverges. From this limiting model, we can\nderive the limiting behavior of the empirical measure of the latent process and\nestablish the related graphon limit of the latent particle network process. We\nalso provide a description of the rich conditional probabilistic structure of\nthe limiting model. The inherent dependence structure complicates the\nmathematical analysis significantly. In the process of proving our main\nresults, we derive a general conditional propagation of chaos result, which is\nof independent interest. In addition, our novel approach of studying the\nlimiting behavior of random samples proves to be a very useful methodology for\nfully grasping the asymptotic behavior of co-evolving particle systems.\nNumerical results are included to illustrate the theoretical findings.",
        "In this paper, we study Dirichlet problem for non-local operator on bounded\ndomains in ${\\mathbb R}^d$\n  $$\n  {\\cal L}u = {\\rm div}(A(x) \\nabla (x)) + b(x) \\cdot \\nabla u(x)\n  + \\int_{{\\mathbb R}^d} (u(y)-u(x) ) J(x, dy) , $$ where\n$A(x)=(a_{ij}(x))_{1\\leq i,j\\leq d}$ is a measurable $d\\times d$ matrix-valued\nfunction on ${\\mathbb R}^d$ that is uniformly elliptic and bounded, $b$ is an\n${\\mathbb R}^d$-valued function so that $|b|^2$ is in some Kato class ${\\mathbb\nK}_d$, for each $x\\in {\\mathbb R}^d$, $J(x, dy)$ is a finite measure on\n${\\mathbb R}^d$ so that $x\\mapsto J(x, {\\mathbb R}^d)$ is in the Kato class\n${\\mathbb K}_d$. We show there is a unique Feller process $X$ having strong\nFeller property associated with ${\\cal L}$, which can be obtained from the\ndiffusion process having generator $ {\\rm div}(A(x) \\nabla (x)) + b(x) \\cdot\n\\nabla u(x) $ through redistribution. We further show that for any bounded\nconnected open subset $D\\subset{\\mathbb R}^d$ that is regular with respect to\nthe Laplace operator $\\Delta$ and for any bounded continuous function $\\varphi\n$ on $D^c$, the Dirichlet problem ${\\cal L} u=0$ in $D$ with $u=\\varphi$ on\n$D^c$ has a unique bounded continuous weak solution on ${\\mathbb R}^d$. This\nunique weak solution can be represented in terms of the Feller process\nassociated with ${\\cal L}$.",
        "In this paper, we propose a driven-dissipative scheme for generating\nnon-Gaussian mechanical entangled states and remotely preparing mechanical\nSchr\\\"{o}dinger cat states via the entanglement. The system under study\nconsists of a cavity optomechanical setup with two frequency-mismatched\nmechanical oscillators coupled to a cavity field driven by a bichromatic pump.\nWe show that under proper conditions, an effective Hamiltonian for\nnondegenerate parametric downconversion involving the two mechanical\noscillators and the cavity field can be engineered. We demonstrate analytically\nand numerically that the cavity dissipation drives the mechanical oscillators\ninto a steady-state pair-coherent state. The no-Gaussianity and nonclassical\nproperties, including Winger negativity, entanglement and quantum steering, of\nthe achieved non-Gaussian mechanical state are investigated in detail. We\nfurther show that homodyne detection on one mechanical oscillator enables the\nremote generation of Schr\\\"{o}dinger cat states in the other oscillator through\nthe non-Gaussian mechanical entanglement. As we show, this detection can be\nimplemented by transferring the mechanical state to the output field of an\nauxiliary probe cavity coupled to the target oscillator, followed by homodyne\ndetection on the output field. We also discuss the robustness of the mechanical\nentangled states and cat states against thermal fluctuations. Our findings\nestablish a feasible approach for the dissipative and remote preparation of\nmechanical nonclassical states.",
        "Over the last years, a number of broadband reverberation mapping campaigns\nhave been conducted to explore the short-term UV and optical variability of\nnearby AGN. Despite the extensive data collected, the origin of the observed\nvariability is still debated in the literature. Frequency-resolved time lags\noffer a promising approach to distinguish between different scenarios, as they\nprobe variability on different time scales. In this study, we present the\nexpected frequency-resolved lags resulting from X-ray reprocessing in the\naccretion disk. The predicted lags are found to feature a general shape that\nresembles that of observational measurements, while exhibiting strong\ndependence on various physical parameters. Additionally, we compare our model\npredictions to observational data for the case of NGC 5548, concluding that the\nX-ray illumination of the disk can effectively account for the observed\nfrequency-resolved lags and power spectra in a self-consistent way. To date,\nX-ray disk reprocessing is the only physical model that has successfully\nreproduced the observed multi-wavelength variability, in both amplitude and\ntime delays, across a range of temporal frequencies.",
        "We present a sample of six F200W and three F277W dropout sources identified\nas $16<z<25$ galaxy candidates based on the deepest JWST\/NIRCam data to date,\nprovided by the MIRI Deep Imaging Survey (MIDIS) and the Next Generation Deep\nExtragalactic Exploratory Public survey (NGDEEP), reaching 5$\\sigma$ depths of\n$\\sim31.5$ mag (AB) at $\\geq2$ $\\mu$m. We estimate ultraviolet (UV) luminosity\nfunctions and densities at $z\\sim17$ and $z\\sim25$. We find that the number\ndensity of galaxies with absolute magnitudes $-19<M_\\mathrm{UV}<-18$ (AB) at\n$z\\sim17$ ($z\\sim25$) is a factor of 4 (25) smaller than at $z\\sim12$; a\nsimilar evolution is observed for the luminosity density. Compared to\nstate-of-the-art galaxy simulations, we find the need for an enhanced UV-photon\nproduction at $z=17-25$ in $\\mathrm{M}_\\mathrm{DM}=10^{8.5-9.5}$ M$_\\odot$ dark\nmatter halos, maybe provided by an increase in the star formation efficiency at\nearly times and\/or by intense bursts fed by very low metallicity or primordial\ngas. There are few robust theoretical predictions for the evolution of galaxies\nabove $z\\sim20$ in the literature, however, the continuing rapid drop in the\nhalo mass function suggests more rapid evolution than we observe if photon\nproduction efficiencies remained constant. Our $z>16$ galaxy candidates present\nmass-weighted ages around 30 Myr, and attenuations $\\mathrm{A(V)}<0.1$ mag.\nTheir average stellar mass is\n$\\mathrm{M}_\\bigstar\\sim10^{7}\\,\\mathrm{M}_\\odot$, implying a star formation\nefficiency (stellar-to-baryon mass fraction) around 10%. We find three galaxies\nwith very blue UV spectral slopes ($\\beta\\sim-3$) compatible with low\nmetallicity or Pop III and young ($\\lesssim10$ Myr) stars and\/or high escape\nfractions of ionizing photons, the rest presenting slopes $\\beta\\sim-2.5$\nsimilar to $z=10-12$ samples.",
        "Post-COVID Syndrome (PCS), encompassing the multifaceted sequelae of\nCOVID-19, can be severity-graded using a score comprising 12 different\nlong-term symptom complexes. Acute COVID-19 severity and individual resilience\nwere previously identified as key predictors of this score. This study\nvalidated these predictors and examined their relationship to PCS symptom\ncomplexes, using an expanded dataset (n=3,372) from the COVIDOM cohort study.\nClassification and Regression Tree (CART) analysis resolved the detailed\nrelationship between the predictors and the constituting symptom complexes of\nthe PCS score. Among newly recruited COVIDOM participants (n=1,930), the PCS\nscore was again found to be associated with both its putative predictors. Of\nthe score-constituting symptom complexes, neurological symptoms, sleep\ndisturbance, and fatigue were predicted by individual resilience, whereas acute\ndisease severity predicted exercise intolerance, chemosensory deficits, joint\nor muscle pain, signs of infection, and fatigue. These associations inspired\nthe definition of two novel PCS scores that included the above-mentioned\nsubsets of symptom complexes only. Both novel scores were inversely correlated\nwith quality of life, measured by the EQ-5D-5L index. The newly defined scores\nmay enhance the assessment of PCS severity, both in a research context and to\ndelineate distinct PCS subdomains with different therapeutic and interventional\nneeds in clinical practise.",
        "The desirable gambles framework provides a foundational approach to imprecise\nprobability theory but relies heavily on linear utility assumptions. This paper\nintroduces {\\em function-coherent gambles}, a generalization that accommodates\nnon-linear utility while preserving essential rationality properties. We\nestablish core axioms for function-coherence and prove a representation theorem\nthat characterizes acceptable gambles through continuous linear functionals.\nThe framework is then applied to analyze various forms of discounting in\nintertemporal choice, including hyperbolic, quasi-hyperbolic, scale-dependent,\nand state-dependent discounting. We demonstrate how these alternatives to\nconstant-rate exponential discounting can be integrated within the\nfunction-coherent framework. This unified treatment provides theoretical\nfoundations for modeling sophisticated patterns of time preference within the\ndesirability paradigm, bridging a gap between normative theory and observed\nbehavior in intertemporal decision-making under genuine uncertainty.",
        "Algebraic symplectic cobordism is the universal symplectically oriented\ncohomology theory for schemes, represented by the motivic commutative ring\nspectrum $\\text{MSp}$ constructed by Panin and Walter. The graded algebraic\ndiagonal $\\text{MSp}^*$ of the coefficient ring of $\\text{MSp}$ is unknown.\nThrough a symplectic version of the Pontryagin-Thom construction, one can\nassociate any symplectic variety $X$ with a symplectic class $[X]_\\text{MSp}$\nin $\\text{MSp}^{-\\text{dim} X}$. Still, the problem in using these classes to\nstudy the ring $\\text{MSp}^*$ is the paucity of non-trivial examples of\nsymplectic varieties. We modify this construction to obtain elements in\n$\\text{MSp}^*$ from a large family of varieties that are not symplectic but\ncarry a certain \"symplectic twist\". Then, using a strategy relying on the Adams\nspectral sequence for $\\text{MSp}$, we find a criterion to select generators\namong these classes, after taking a completion along the motivic Hopf map\n$\\eta$.",
        "Imaging atmospheric Cherenkov telescopes (IACTs) detect gamma rays by\nmeasuring the Cherenkov light emitted by secondary particles in the air shower\nwhen the gamma rays hit the atmosphere. At low energies, the limited amount of\nCherenkov light produced typically implies that the event is registered by one\nIACT only. Such events are called monoscopic events, and their analysis is\nparticularly difficult. Challenges include the reconstruction of the event's\narrival direction, energy, and the rejection of background events. Here, we\npresent a set of improvements, including a machine-learning algorithm to\ndetermine the correct orientation of the image, an intensity-dependent\nselection cut that ensures optimal performance, and a collection of new image\nparameters. To quantify these improvements, we use the central telescope of the\nH.E.S.S. IACT array. Knowing the correct image orientation, which corresponds\nto the arrival direction of the photon in the camera frame, is especially\nimportant for the angular reconstruction, which could be improved in resolution\nby 57% at 100 GeV. The event selection cut, which now depends on the total\nmeasured intensity of the events, leads to a reduction of the low-energy\nthreshold for source analyses by ~50%. The new image parameters characterize\nthe intensity and time distribution within the recorded images and complement\nthe traditionally used Hillas parameters in the machine learning algorithms. We\nevaluate their importance to the algorithms in a systematic approach and\ncarefully evaluate associated systematic uncertainties. We find that including\nsubsets of the new variables in machine-learning algorithms improves the\nreconstruction and background rejection, resulting in a sensitivity improved by\n41% at the low-energy threshold.",
        "We address the longstanding challenge in quantum many-body theory of\nreconciling unitary dynamics with irreversible relaxation. In classical chaos,\nthe unitary evolution operator develops Ruelle-Pollicott (RP) resonances inside\nthe unit circle in the continuum limit, leading to mixing. In the semiclassical\nlimit, chaotic single-particle quantum systems relax with the same RP\nresonances. In contrast, the theory of quantum many-body RP resonances and\ntheir link to irreversibility remain underdeveloped. Here, we relate the\nspectral form factor to the sum of autocorrelation functions and, in generic\nmany-body lattice systems without conservation laws, argue that all quantum\nmany-body RP resonances converge inside the unit disk, highlighting the role of\nnonunitary and the thermodynamic limit. While we conjecture this picture to be\ngeneral, we analytically prove the emergence of irreversibility in the random\nphase model (RPM), a paradigmatic Floquet quantum circuit model, in the limit\nof large local Hilbert space dimension. To this end, we couple it to local\nenvironments and compute the exact time evolution of autocorrelation functions,\nthe dissipative form factor, and out-of-time-order correlation functions\n(OTOCs). Although valid for any dissipation strength, we then focus on weak\ndissipation to clarify the origin of irreversibility in unitary systems. When\nthe dissipationless limit is taken after the thermodynamic limit, the unitary\nquantum map develops an infinite tower of decaying RP resonances -- chaotic\nsystems display so-called anomalous relaxation. We also show that the OTOC in\nthe RPM can undergo a two-stage relaxation and that during the second stage,\nthe approach to the stationary value is again controlled by the leading RP\nresonance.\n  [See the paper for the full abstract.]",
        "A consistent path system in a graph $G$ is an collection of paths, with\nexactly one path between any two vertices in $G$. A path system is said to be\nconsistent if it is intersection-closed. We say that $G$ is strictly metrizable\nif every consistent path system in $G$ can be realized as the system of unique\ngeodesics with respect to some assignment of positive edge weight. In this\npaper, we show that the family of strictly metrizable graphs is minor-closed.",
        "A novel approach is proposed to analyze a rather vast counter-rotating\nHamiltonian interaction in the context of cavity quantum electrodynamics. The\nmethod relies upon the supersymmetric mapping of the corresponding rotating\ninteraction and allows the analysis of the dynamics in the counter-rotating\nsystem in a fully general and exact analytical manner. Intriguing features of\nthe counter-rotating system are revealed through the simple supersymmetric\ntransformation. In turn, such interesting attributes have an important range of\npotential technological applications. In this way, supersymmetry emerges as a\nuseful tool to both connect and construct exactly solvable photonic systems in\ncavity quantum electrodynamics, and more generally in quantum optics, as well\nas to analyze the corresponding physical consequences and their possible\ntechnology implementations.",
        "Magnetic holes (MHs) are coherent structures characterized by a strong and\nlocalized magnetic field amplitude dip, commonly observed in the solar wind and\nplanetary magnetosheaths. These structures come in different sizes, from\nmagnetohydrodynamic to kinetic scales. Magnetospheric Multiscale (MMS)\nobservations have revealed electron scale MHs to be ubiquitous in the turbulent\nEarth's magnetosheath, potentially playing an important role in the energy\ncascade and dissipation. Despite abundant observations, the origin of electron\nscale MHs is still unclear and debated. In this work, we use fully kinetic\nsimulations to investigate the role of plasma turbulence in generating electron\nscale MHs. We perform a fully kinetic simulation of freely decaying plasma\nturbulence, initialized with typical Earth's magnetosheath parameters. We find\nthat electron scale MHs can be generated by turbulence via the following\nmechanism: first, large-scale turbulent velocity shears produce regions with\nhigh electron temperature anisotropy; these localized regions become unstable,\ngenerating oblique electron scale whistler waves; as they propagate over the\ninhomogeneous turbulent background, whistler fluctuations develop an\nelectrostatic component, turning into Bernstein-like modes; the strong\nelectrostatic fluctuations produce current filaments that merge into an\nelectron scale current vortex; the resulting electron vortex locally reduces\nthe magnetic field amplitude, finally evolving into an electron scale MH. We\nshow that MHs generated by this mechanism have properties consistent with MMS\nobservations and nontrivial kinetic features. We provide numerical evidence of\na new electron scale MH generation mechanism, driven by turbulence. Our results\nhave potential implications for understanding the formation and occurrence of\nelectron scale MHs in turbulent environments, such as the Earth's\nmagnetosheath.",
        "This work presents a novel gradient-free importance sampling-based framework\nfor precisely and efficiently estimating rare event probabilities, often\nencountered in reliability analyses of engineering systems. The approach is\nformulated around our foundational Approximate Sampling Target with\nPost-processing Adjustment (ASTPA) methodology. ASTPA uniquely constructs and\ndirectly samples an unnormalized target distribution, relaxing the optimal\nimportance sampling distribution (ISD). The target's normalizing constant is\nthen estimated using our inverse importance sampling (IIS) scheme, employing an\nISD fitted based on the obtained samples. In this work, a gradient-free\nsampling method within ASTPA is developed through a guided dimension-robust\npreconditioned Crank-Nicolson (pCN) algorithm, particularly suitable for\nblack-box computational models where analytical gradient information is not\navailable. To boost the sampling efficiency of pCN in our context, a\ncomputationally effective, general discovery stage for the rare event domain is\ndevised, providing (multi-modal) rare event samples used in initializing the\npCN chains. A series of diverse test functions and engineering problems\ninvolving high dimensionality and strong nonlinearity is presented,\ndemonstrating the advantages of the proposed framework compared to several\nstate-of-the-art sampling methods.",
        "According to the quantum chaos paradigm, the nature of a system's classical\ndynamics, whether integrable or chaotic, is universally reflected in the\nfluctuations of its quantum spectrum. However, since many-body spectra in the\nmean field limit are composed of independent single-particle energy levels,\ntheir spectral fluctuations always display Poissonian behavior and hence cannot\nbe used to distinguish underlying chaotic from integrable single-particle\ndynamics. We demonstrate that this distinction can, instead, be revealed from\nthe mean many-body level density (at fixed energy) and its variance after\naveraging over ensembles representing different types of single-particle\ndynamics. This is in strong contrast to the energy-averaged mean level density\n(of a given system) that is assumed not to carry such information and is\nroutinely removed to focus on universal signatures. To support our claim we\nsystematically analyze the role of single-particle level correlations, that\nenter through Poisson and random matrix statistics (of various symmetry\nclasses) into the ensemble-averaged density of states and its variance,\ncontrasting bosonic and fermionic many-body systems. Our analytical study,\ntogether with extensive numerical simulations for systems with $N \\ge 5$\nparticles consistently reveal significant differences (up to an order of\nmagnitude for fermions and even larger for bosons) in the mean many-body level\ndensities, depending on the nature of the underlying dynamics. Notably, in the\nfermionic case Poisson-type single-particle level fluctuations precisely cancel\ncontributions from indistinguishability, such that the average many-body\nspectral density equals the (Thomas-Fermi) volume term. We further highlight\nthe difference between the mean level density and its variance as functions of\nthe total energy $E$ and the excitation energy $Q$.",
        "Diffusion MRI tractography technique enables non-invasive visualization of\nthe white matter pathways in the brain. It plays a crucial role in neuroscience\nand clinical fields by facilitating the study of brain connectivity and\nneurological disorders. However, the accuracy of reconstructed tractograms has\nbeen a longstanding challenge. Recently, deep learning methods have been\napplied to improve tractograms for better white matter coverage, but often\ncomes at the expense of generating excessive false-positive connections. This\nis largely due to their reliance on local information to predict long range\nstreamlines. To improve the accuracy of streamline propagation predictions, we\nintroduce a novel deep learning framework that integrates image-domain spatial\ninformation and anatomical information along tracts, with the former extracted\nthrough convolutional layers and the later modeled via a Transformer-decoder.\nAdditionally, we employ a weighted loss function to address fiber class\nimbalance encountered during training. We evaluate the proposed method on the\nsimulated ISMRM 2015 Tractography Challenge dataset, achieving a valid\nstreamline rate of 66.2%, white matter coverage of 63.8%, and successfully\nreconstructing 24 out of 25 bundles. Furthermore, on the multi-site\nTractoinferno dataset, the proposed method demonstrates its ability to handle\nvarious diffusion MRI acquisition schemes, achieving a 5.7% increase in white\nmatter coverage and a 4.1% decrease in overreach compared to RNN-based methods.",
        "Periodic lattices are natural generalizations of lattices, which arise\nnaturally in diophantine approximations with rationals of bounded denominators.\nIn this paper, we prove analogues of classical theorems in geometry of numbers\nfor periodic lattices in function fields. Moreover, we use special matrices to\ncompute the covering and packing radii of special periodic lattices.",
        "The Pythagorean school attributed consonance in music to simplicity of\nfrequency ratios between musical tones. In the last two centuries, the\nconsonance curves developed by Helmholtz, Plompt and Levelt shifted focus to\npsycho-acoustic considerations in perceiving consonances. The appearance of\npeaks of these curves at the ratios considered by the Pythagorean school, and\nwhich were a consequence of an attempt to understand the world by nice\nmathematical proportions, remained a curiosity. This paper addresses this\ncuriosity, by describing a mathematical model of musical sound, along with a\nmathematical definition of consonance. First, we define pure, complex and mixed\ntones as mathematical models of musical sound. By a sequence of numerical\nexperiments and analytic calculations, we show that continuous cosine\nsimilarity, abbreviated as cosim, applied to these models quantifies the\nelusive concept of consonance as a frequency ratio which gives a local maximum\nof the cosim function. We prove that these maxima occur at the ratios\nconsidered as consonant in classical music theory. Moreover, we provide a\nsimple explanation why the number of musical intervals considered as consonant\nby musicians is finite, but has been increasing over the centuries.\nSpecifically, our formulas show that the number of consonant intervals changes\nwith the depth of the tone (the number of harmonics present).",
        "One of the greatest challenges when designing new technologies that make use\nof non-trivial quantum materials is the difficulty associated with predicting\nmaterial-specific properties, such as critical temperature, gap parameter, etc.\nThere is naturally a great amount of interest in these types of condensed\nmatter systems because of their application to quantum sensing, quantum\nelectronics, and quantum computation; however, they are exceedingly difficult\nto address from first principles because of the famous many-body problem. For\nthis reason, a full electron-nuclear quantum calculation will likely remain\ncompletely out of reach for the foreseeable future. A practical alternative is\nprovided by finite temperature, multi component density functional theory\n(MCDFT), which is a formally exact method of computing the equilibrium state\nenergy of a many-body quantum system. In this work, we use this construction\nalongside a perturbative scheme to demonstrate that the phenomena Peierls\neffect and Kohn Anomaly are both natural features of the KS equations without\nadditional structure needed. We find the temperature dependent ionic density\nfor a simple 1D lattice which is then used to derive the ionic densities\ntemperature dependent affect on the electronic band structure. This is\naccomplished by Fourier transforming the ionic density term found within this\nKS electronic equation. Using the Peierls effect phonon distortion gap openings\nin relation to the Fermi level, we then perturb the KS ionic equation with a\nconduction electron density, deriving the Kohn Anomaly. This provides a\nworkable predictive strategy for interesting electro-phonon related material\nproperties which could be extended to 2D and 3D real materials while retaining\nthe otherwise complicated temperature dependence.",
        "In order to describe properly the gravity interactions including the mass\ncurrents, in the gravitomagnetism we construct four Maxwell type gravitational\nequations which are shown to be analogs of the Maxwell equations in the\nelectromagnetism. Next, exploiting the Maxwell type gravitational equations, we\nexplicitly predict the mass magnetic fields for both the isolated system of the\nspinning Moon orbiting the spinning Earth and that of the Sun and solar system\nplanets orbiting the spinning Sun, whose phenomenological values have not been\nevaluated in the precedented Newtonian gravity formalisms. In the\ngravitomagnetism we also phenomenologically investigate the mass magnetic\ngeneral relativity (GR) forces associated with the mass magnetic fields, to\nfind that they are extremely small but non-vanishing compared to the\ncorresponding mass electric Newtonian forces. Moreover, the directions of the\nmass magnetic GR forces for the solar system planets except Venus and Uranus\nare shown to be anti-parallel to those of their mass electric Newtonian forces.\nNext we investigate the mass magnetic dipole moment related with the B-ring of\nSaturn, to evaluate $\\vec{m}_{M}(Ring)=-1.141\\times 10^{4}~{\\rm\nm^{3}~sec^{-1}}~\\hat{\\omega}$ with $\\hat{\\omega}$ being the unit vector along\nthe axis direction of the spinning B-ring. The predicted value of\n$\\vec{m}_{M}(Ring)$ is shown to be directly related with the Cassini data on\nthe total mass of the rings of Saturn."
      ]
    }
  },
  {
    "id":2411.00758,
    "research_type":"basic",
    "start_id":"b6",
    "start_title":"Introduction to Nonimaging Optics, second edition",
    "start_abstract":"Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book.",
    "start_categories":[
      "physics.optics"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Inverse methods for illumination optics"
      ],
      "abstract":[
        "\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "Increasing the Energy-Efficiency of Wearables Using Low-Precision Posit\n  Arithmetic with PHEE",
        "Coherent Local Explanations for Mathematical Optimization",
        "Transformers with Joint Tokens and Local-Global Attention for Efficient\n  Human Pose Estimation",
        "Imperfect Knowledge Management (IKM) in GEFRED (GENeralized model for\n  Fuzzy RElational Databases)",
        "An Interpretable Neural Control Network with Adaptable Online Learning\n  for Sample Efficient Robot Locomotion Learning",
        "We Need to Effectively Integrate Computing Skills Across Discipline\n  Curricula",
        "Sweeping Orders for Simplicial Complex Reconstruction",
        "Rapid and Inexpensive Inertia Tensor Estimation from a Single Object\n  Throw",
        "Fractional Correspondence Framework in Detection Transformer",
        "Modelling Capillary Rise with a Slip Boundary Condition: Well-posedness\n  and Long-time Dynamics of Solutions to Washburn's Equation",
        "A comprehensive study of bound-states for the nonlinear Schr\\\"odinger\n  equation on single-knot metric graphs",
        "A precise asymptotic analysis of learning diffusion models: theory and\n  insights",
        "BoKDiff: Best-of-K Diffusion Alignment for Target-Specific 3D Molecule\n  Generation",
        "TMI-CLNet: Triple-Modal Interaction Network for Chronic Liver Disease\n  Prognosis From Imaging, Clinical, and Radiomic Data Fusion",
        "Constructing self-similar subsets within the fractal support of Lacunary\n  Wavelet Series for their multifractal analysis",
        "Fenchel-Young Variational Learning",
        "HyperNOs: Automated and Parallel Library for Neural Operators Research",
        "Understanding Generalization in Transformers: Error Bounds and Training\n  Dynamics Under Benign and Harmful Overfitting",
        "Linear extrapolation for the graph of function of single variable based\n  on walks",
        "RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow\n  Trajectories",
        "The Society of HiveMind: Multi-Agent Optimization of Foundation Model\n  Swarms to Unlock the Potential of Collective Intelligence",
        "Food Recommendation With Balancing Comfort and Curiosity",
        "Movable Antenna Aided Multiuser Communications: Antenna Position\n  Optimization Based on Statistical Channel Information",
        "ACCESS : A Benchmark for Abstract Causal Event Discovery and Reasoning",
        "A characterization of binomial Macaulay dual generators for complete\n  intersections",
        "VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large\n  Scenes",
        "Sheaf-Theoretic Causal Emergence for Resilience Analysis in Distributed\n  Systems",
        "Gradient Deconfliction via Orthogonal Projections onto Subspaces For\n  Multi-task Learning",
        "Coordinated Inauthentic Behavior and Information Spreading on Twitter"
      ],
      "abstract":[
        "Wearable biomedical devices are increasingly being used for continuous\npatient health monitoring, enabling real-time insights and extended data\ncollection without the need for prolonged hospital stays. These devices must be\nenergy efficient to minimize battery size, improve comfort, and reduce\nrecharging intervals. This paper investigates the use of specialized\nlow-precision arithmetic formats to enhance the energy efficiency of biomedical\nwearables. Specifically, we explore posit arithmetic, a floating-point-like\nrepresentation, in two key applications: cough detection for chronic cough\nmonitoring and R peak detection in ECG analysis. Simulations reveal that 16-bit\nposits can replace 32-bit IEEE 754 floating point numbers with minimal accuracy\nloss in cough detection. For R peak detection, posit arithmetic achieves\nsatisfactory accuracy with as few as 10 or 8 bits, compared to the 16-bit\nrequirement for floating-point formats. To further this exploration, we\nintroduce PHEE, a modular and extensible architecture that integrates the\nCoprosit posit coprocessor within a RISC-V-based system. Using the X-HEEP\nframework, PHEE seamlessly incorporates posit arithmetic, demonstrating reduced\nhardware area and power consumption compared to a floating-point counterpart\nsystem. Post-synthesis results targeting 16nm TSMC technology show that the\nposit hardware targeting these biomedical applications can be 38% smaller and\nconsume up to 54% less energy at the functional unit level, with no performance\ncompromise. These findings establish the potential of low-precision posit\narithmetic to significantly improve the energy efficiency of wearable\nbiomedical devices.",
        "The surge of explainable artificial intelligence methods seeks to enhance\ntransparency and explainability in machine learning models. At the same time,\nthere is a growing demand for explaining decisions taken through complex\nalgorithms used in mathematical optimization. However, current explanation\nmethods do not take into account the structure of the underlying optimization\nproblem, leading to unreliable outcomes. In response to this need, we introduce\nCoherent Local Explanations for Mathematical Optimization (CLEMO). CLEMO\nprovides explanations for multiple components of optimization models, the\nobjective value and decision variables, which are coherent with the underlying\nmodel structure. Our sampling-based procedure can provide explanations for the\nbehavior of exact and heuristic solution algorithms. The effectiveness of CLEMO\nis illustrated by experiments for the shortest path problem, the knapsack\nproblem, and the vehicle routing problem.",
        "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) have led\nto significant progress in 2D body pose estimation. However, achieving a good\nbalance between accuracy, efficiency, and robustness remains a challenge. For\ninstance, CNNs are computationally efficient but struggle with long-range\ndependencies, while ViTs excel in capturing such dependencies but suffer from\nquadratic computational complexity. This paper proposes two ViT-based models\nfor accurate, efficient, and robust 2D pose estimation. The first one,\nEViTPose, operates in a computationally efficient manner without sacrificing\naccuracy by utilizing learnable joint tokens to select and process a subset of\nthe most important body patches, enabling us to control the trade-off between\naccuracy and efficiency by changing the number of patches to be processed. The\nsecond one, UniTransPose, while not allowing for the same level of direct\ncontrol over the trade-off, efficiently handles multiple scales by combining\n(1) an efficient multi-scale transformer encoder that uses both local and\nglobal attention with (2) an efficient sub-pixel CNN decoder for better speed\nand accuracy. Moreover, by incorporating all joints from different benchmarks\ninto a unified skeletal representation, we train robust methods that learn from\nmultiple datasets simultaneously and perform well across a range of scenarios\n-- including pose variations, lighting conditions, and occlusions. Experiments\non six benchmarks demonstrate that the proposed methods significantly\noutperform state-of-the-art methods while improving computational efficiency.\nEViTPose exhibits a significant decrease in computational complexity (30% to\n44% less in GFLOPs) with a minimal drop of accuracy (0% to 3.5% less), and\nUniTransPose achieves accuracy improvements ranging from 0.9% to 43.8% across\nthese benchmarks.",
        "Imperfect Knowledge Management (IKM) aids in managing imprecise, uncertain,\nor incomplete aspects of meaning. IKM acknowledges that an enterprise's\nknowledge is often imperfect, characterized by varying degrees of imprecision,\nuncertainty, or incompleteness. In this context, knowledge is viewed as an\nobject described by attributes and values. Our focus is on the domain of\ncompetencies (know how) in the production of coated cardboard, particularly the\nprocess of converting finished products from the manufactured cardboard. This\nprocess involves both classic and fuzzy attributes that are used to assess the\nquality of the cardboard. This article introduces a set of protocols designed\nto model a fuzzy metaknowledge base using GEFRED (GENeralized model for Fuzzy\nRElational Databases) in an Oracle 8i relational database system.",
        "Robot locomotion learning using reinforcement learning suffers from training\nsample inefficiency and exhibits the non-understandable\/black-box nature. Thus,\nthis work presents a novel SME-AGOL to address such problems. Firstly,\nSequential Motion Executor (SME) is a three-layer interpretable neural network,\nwhere the first produces the sequentially propagating hidden states, the second\nconstructs the corresponding triangular bases with minor non-neighbor\ninterference, and the third maps the bases to the motor commands. Secondly, the\nAdaptable Gradient-weighting Online Learning (AGOL) algorithm prioritizes the\nupdate of the parameters with high relevance score, allowing the learning to\nfocus more on the highly relevant ones. Thus, these two components lead to an\nanalyzable framework, where each sequential hidden state\/basis represents the\nlearned key poses\/robot configuration. Compared to state-of-the-art methods,\nthe SME-AGOL requires 40% fewer samples and receives 150% higher final\nreward\/locomotion performance on a simulated hexapod robot, while taking merely\n10 minutes of learning time from scratch on a physical hexapod robot. Taken\ntogether, this work not only proposes the SME-AGOL for sample efficient and\nunderstandable locomotion learning but also emphasizes the potential\nexploitation of interpretability for improving sample efficiency and learning\nperformance.",
        "Computing is increasingly central to innovation across a wide range of\ndisciplinary and interdisciplinary problem domains. Students across\nnoncomputing disciplines need to apply sophisticated computational skills and\nmethods to fields as diverse as biology, linguistics, and art. Furthermore,\ncomputing plays a critical role in \"momentous geopolitical events\", such as\nelections in several countries including the US, and is changing how people\n\"work, collaborate, communicate, shop, eat, travel, get news and entertainment,\nand quite simply live\". Traditional computing courses, however, fail to equip\nnon-computing discipline students with the necessary computing skills - if they\ncan even get into classes packed with CS majors. A pressing question facing\nacademics today is: How do we effectively integrate computing skills that are\nuseful for the discipline into discipline curricula?\n  We advocate an approach where courses in discipline X include the computing\nrelevant to the learning outcomes of that course, as used by practitioners in\nX. We refer to the computing skills relevant to a course in discipline X as an\n\"ounce of computing skills\", to highlight our belief regarding the amount of\ncomputing to be integrated in that course. In this article, we outline our\ninsights regarding the development of an ounce of computing skills for a\ndiscipline course, and the evaluation of the developed ounce. The key takeaways\nare that the goal has to be to advance students in their disciplines, and only\nthe disciplinary experts can tell us how computing is used in that discipline.\nComputer scientists know how to teach computing, but the classes can't be about\nCS values. The disciplinary values are paramount.",
        "Simplicial complexes arising from real-world settings may not be directly\nobservable. Hence, for an unknown simplicial complex in Euclidean space, we\nwant to efficiently reconstruct it by querying local structure. In particular,\nwe are interested in queries for the indegree of a simplex $\\sigma$ in some\ndirection: the number of cofacets of $\\sigma$ contained in some halfspace\n\"below\" $\\sigma$. Fasy et al. proposed a method that, given the vertex set of a\nsimplicial complex, uses indegree queries to reconstruct the set of edges. In\nparticular, they use a sweep algorithm through the vertex set, identifying\nedges adjacent to and above each vertex in the sweeping order. The algorithm\nrelies on a natural but crucial property of the sweeping order: at a given\nvertex $v$, all edges adjacent to $v$ contained in the halfspace below $v$ have\nanother endpoint that appeared earlier in the order.\n  The edge reconstruction algorithm does not immediately extend to\nhigher-dimensional simplex reconstruction. In particular, it is not possible to\nsweep through a set of $i$-simplices in a fixed direction and maintain that all\n$(i+1)$-cofacets of a given simplex $\\sigma$ that come below $\\sigma$ are\nknown. We circumvent this by defining a sweeping order on a set of\n$i$-simplices, that additionally pairs each $i$-simplex $\\sigma$ with a\ndirection perpendicular to $\\sigma$. Analogous to Fasy et al., our order has\nthe crucial property that, at any $i$-simplex $\\sigma$ paired with direction\n$s$, each $(i+1)$-dimensional coface of $\\sigma$ that lies in the halfspace\nbelow $\\sigma$ with respect to the direction $s$ has an $i$-dimensional face\nthat appeared earlier in the order. We show how to compute such an order and\nuse it to extend the edge reconstruction algorithm of Fasy et al. to simplicial\ncomplex reconstruction. Our algorithm can reconstruct arbitrary embedded\nsimplicial complexes.",
        "The inertia tensor is an important parameter in many engineering fields, but\nmeasuring it can be cumbersome and involve multiple experiments or accurate and\nexpensive equipment. We propose a method to measure the moment of inertia\ntensor of a rigid body from a single spinning throw, by attaching a small and\ninexpensive stand-alone measurement device consisting of a gyroscope,\naccelerometer and a reaction wheel. The method includes a compensation for the\nincrease of moment of inertia due to adding the measurement device to the body,\nand additionally obtains the location of the centre of gravity of the body as\nan intermediate result. Experiments performed with known rigid bodies show that\nthe mean accuracy is around 2\\%.",
        "The Detection Transformer (DETR), by incorporating the Hungarian algorithm,\nhas significantly simplified the matching process in object detection tasks.\nThis algorithm facilitates optimal one-to-one matching of predicted bounding\nboxes to ground-truth annotations during training. While effective, this strict\nmatching process does not inherently account for the varying densities and\ndistributions of objects, leading to suboptimal correspondences such as failing\nto handle multiple detections of the same object or missing small objects. To\naddress this, we propose the Regularized Transport Plan (RTP). RTP introduces a\nflexible matching strategy that captures the cost of aligning predictions with\nground truths to find the most accurate correspondences between these sets. By\nutilizing the differentiable Sinkhorn algorithm, RTP allows for soft,\nfractional matching rather than strict one-to-one assignments. This approach\nenhances the model's capability to manage varying object densities and\ndistributions effectively. Our extensive evaluations on the MS-COCO and VOC\nbenchmarks demonstrate the effectiveness of our approach. RTP-DETR, surpassing\nthe performance of the Deform-DETR and the recently introduced DINO-DETR,\nachieving absolute gains in mAP of +3.8% and +1.7%, respectively.",
        "The aim of this paper is to extend Washburn's capillary rise equation by\nincorporating a slip condition at the pipe wall. The governing equation is\nderived using fundamental principles from continuum mechanics. A new scaling is\nintroduced, allowing for a systematic analysis of different flow regimes. We\nprove the global-in-time existence and uniqueness of a bounded positive\nsolution to Washburn's equation that includes the slip parameter, as well as\nthe continuous dependence of the solution in the maximum norm on the initial\ndata. Thus, the initial-value problem for Washburn's equation is shown to be\nwell-posed in the sense of Hadamard. Additionally, we show that the unique\nequilibrium solution may be reached either monotonically or in an oscillatory\nfashion, similarly to the no-slip case. Finally, we determine the basin of\nattraction for the system, ensuring that the equilibrium state will be reached\nfrom the initial data we impose. These results hold for any positive value of\nthe nondimensional slip parameter in the model, and for all values of the ratio\n$h_0\/h_e$ in the range $[0,3\/2]$, where $h_0$ is the initial height of the\nfluid column and $h_e$ is its equilibrium height.",
        "We study the existence and qualitative properties of action ground-states\n(that is, bound-states with minimal action) {of the nonlinear Schr\\\"odinger\nequation} over single-knot metric graphs -- which are made of half-lines, loops\nand pendants, all connected at a single vertex. First, we prove existence of\naction ground-state for generic single-knot graphs, even in the absence of an\nassociated variational problem. Second, for regular single-knot graphs of\nlength $\\ell$, we perform a complete analysis of positive monotone\nbound-states. Furthermore, we characterize all positive bound-states when\n$\\ell$ is small and prove some symmetry-breaking results for large $\\ell$.\nFinally, we apply the results to some particular graphs to illustrate the\ncomplex relation between action ground-states and the topological {and metric}\nfeatures of the underlying metric graph.\n  The proofs are nonvariational, using a careful phase-plane analysis, the\nstudy of sections of period functions, asymptotic estimates and blowup\narguments. We show, in particular, how nonvariational techniques are\ncomplementary to variational ones in order to deeply understand bound-states of\nthe nonlinear Schr\\\"odinger equation on metric graphs.",
        "In this manuscript, we consider the problem of learning a flow or\ndiffusion-based generative model parametrized by a two-layer auto-encoder,\ntrained with online stochastic gradient descent, on a high-dimensional target\ndensity with an underlying low-dimensional manifold structure. We derive a\ntight asymptotic characterization of low-dimensional projections of the\ndistribution of samples generated by the learned model, ascertaining in\nparticular its dependence on the number of training samples. Building on this\nanalysis, we discuss how mode collapse can arise, and lead to model collapse\nwhen the generative model is re-trained on generated synthetic data.",
        "Structure-based drug design (SBDD) leverages the 3D structure of biomolecular\ntargets to guide the creation of new therapeutic agents. Recent advances in\ngenerative models, including diffusion models and geometric deep learning, have\ndemonstrated promise in optimizing ligand generation. However, the scarcity of\nhigh-quality protein-ligand complex data and the inherent challenges in\naligning generated ligands with target proteins limit the effectiveness of\nthese methods. We propose BoKDiff, a novel framework that enhances ligand\ngeneration by combining multi-objective optimization and Best-of-K alignment\nmethodologies. Built upon the DecompDiff model, BoKDiff generates diverse\ncandidates and ranks them using a weighted evaluation of molecular properties\nsuch as QED, SA, and docking scores. To address alignment challenges, we\nintroduce a method that relocates the center of mass of generated ligands to\ntheir docking poses, enabling accurate sub-component extraction. Additionally,\nwe integrate a Best-of-N (BoN) sampling approach, which selects the optimal\nligand from multiple generated candidates without requiring fine-tuning. BoN\nachieves exceptional results, with QED values exceeding 0.6, SA scores above\n0.75, and a success rate surpassing 35%, demonstrating its efficiency and\npracticality. BoKDiff achieves state-of-the-art results on the CrossDocked2020\ndataset, including a -8.58 average Vina docking score and a 26% success rate in\nmolecule generation. This study is the first to apply Best-of-K alignment and\nBest-of-N sampling to SBDD, highlighting their potential to bridge generative\nmodeling with practical drug discovery requirements. The code is provided at\nhttps:\/\/github.com\/khodabandeh-ali\/BoKDiff.git.",
        "Chronic liver disease represents a significant health challenge worldwide and\naccurate prognostic evaluations are essential for personalized treatment plans.\nRecent evidence suggests that integrating multimodal data, such as computed\ntomography imaging, radiomic features, and clinical information, can provide\nmore comprehensive prognostic information. However, modalities have an inherent\nheterogeneity, and incorporating additional modalities may exacerbate the\nchallenges of heterogeneous data fusion. Moreover, existing multimodal fusion\nmethods often struggle to adapt to richer medical modalities, making it\ndifficult to capture inter-modal relationships. To overcome these limitations,\nWe present the Triple-Modal Interaction Chronic Liver Network (TMI-CLNet).\nSpecifically, we develop an Intra-Modality Aggregation module and a\nTriple-Modal Cross-Attention Fusion module, which are designed to eliminate\nintra-modality redundancy and extract cross-modal information, respectively.\nFurthermore, we design a Triple-Modal Feature Fusion loss function to align\nfeature representations across modalities. Extensive experiments on the liver\nprognosis dataset demonstrate that our approach significantly outperforms\nexisting state-of-the-art unimodal models and other multi-modal techniques. Our\ncode is available at https:\/\/github.com\/Mysterwll\/liver.git.",
        "Given a fractal $\\mathcal{I}$ whose Hausdorff dimension matches with the\nupper-box dimension, we propose a new method which consists in selecting inside\n$\\mathcal{I}$ some subsets (called quasi-Cantor sets) of almost same dimension\nand with controled properties of self-similarties at prescribed scales. It\nallows us to estimate below the Hausdorff dimension $\\mathcal{I}$ intersected\nto limsup sets of contracted balls selected according a Bernoulli law, in\ncontexts where classical Mass Transference Principles cannot be applied. We\napply this result to the computation of the increasing multifractal spectrum of\nlacunary wavelet series supported on $\\mathcal{I}$.",
        "From a variational perspective, many statistical learning criteria involve\nseeking a distribution that balances empirical risk and regularization. In this\npaper, we broaden this perspective by introducing a new general class of\nvariational methods based on Fenchel-Young (FY) losses, treated as divergences\nthat generalize (and encompass) the familiar Kullback-Leibler divergence at the\ncore of classical variational learning. Our proposed formulation -- FY\nvariational learning -- includes as key ingredients new notions of FY free\nenergy, FY evidence, FY evidence lower bound, and FY posterior. We derive\nalternating minimization and gradient backpropagation algorithms to compute (or\nlower bound) the FY evidence, which enables learning a wider class of models\nthan previous variational formulations. This leads to generalized FY variants\nof classical algorithms, such as an FY expectation-maximization (FYEM)\nalgorithm, and latent-variable models, such as an FY variational autoencoder\n(FYVAE). Our new methods are shown to be empirically competitive, often\noutperforming their classical counterparts, and most importantly, to have\nqualitatively novel features. For example, FYEM has an adaptively sparse\nE-step, while the FYVAE can support models with sparse observations and sparse\nposteriors.",
        "This paper introduces HyperNOs, a PyTorch library designed to streamline and\nautomate the process of exploring neural operators, with a special focus on\nhyperparameter optimization for comprehensive and exhaustive exploration.\nIndeed, HyperNOs takes advantage of state-of-the-art optimization algorithms\nand parallel computing implemented in the Ray-tune library to efficiently\nexplore the hyperparameter space of neural operators. We also implement many\nuseful functionalities for studying neural operators with a user-friendly\ninterface, such as the possibility to train the model with a fixed number of\nparameters or to train the model with multiple datasets and different\nresolutions. We integrate Fourier neural operators and convolutional neural\noperators in our library, achieving state of the art results on many\nrepresentative benchmarks, demonstrating the capabilities of HyperNOs to handle\nreal datasets and modern architectures. The library is designed to be easy to\nuse with the provided model and datasets, but also to be easily extended to use\nnew datasets and custom neural operator architectures.",
        "Transformers serve as the foundational architecture for many successful\nlarge-scale models, demonstrating the ability to overfit the training data\nwhile maintaining strong generalization on unseen data, a phenomenon known as\nbenign overfitting. However, research on how the training dynamics influence\nerror bounds within the context of benign overfitting has been limited. This\npaper addresses this gap by developing a generalization theory for a two-layer\ntransformer with labeled flip noise. Specifically, we present generalization\nerror bounds for both benign and harmful overfitting under varying\nsignal-to-noise ratios (SNR), where the training dynamics are categorized into\nthree distinct stages, each with its corresponding error bounds. Additionally,\nwe conduct extensive experiments to identify key factors that influence test\nerrors in transformers. Our experimental results align closely with the\ntheoretical predictions, validating our findings.",
        "The quantum walk was introduced as a quantum counterpart of the random walk\nand has been intensively studied since around 2000. Its applications include\ntopological insulators, radioactive waste reduction, and quantum search. The\nfirst author in 2019 defined a time-series model based on the measure of the\n``discrete-time\" and ``discrete-space\" quantum walk in one dimension. Inspired\nby his model, this paper proposes a new model for the graph of a function of a\nsingle variable determined by the measure which comes from the weak limit\nmeasure of a ``continuous-time or discrete-time\" and ``discrete-space\" walk.\nThe measure corresponds to a ``continuous-time\" and ``continuous-space\" walk in\none dimension. Moreover, we also presents a method of a linear extrapolation\nfor the graph by our model.",
        "Diffusion models have achieved remarkable success across various domains.\nHowever, their slow generation speed remains a critical challenge. Existing\nacceleration methods, while aiming to reduce steps, often compromise sample\nquality, controllability, or introduce training complexities. Therefore, we\npropose RayFlow, a novel diffusion framework that addresses these limitations.\nUnlike previous methods, RayFlow guides each sample along a unique path towards\nan instance-specific target distribution. This method minimizes sampling steps\nwhile preserving generation diversity and stability. Furthermore, we introduce\nTime Sampler, an importance sampling technique to enhance training efficiency\nby focusing on crucial timesteps. Extensive experiments demonstrate RayFlow's\nsuperiority in generating high-quality images with improved speed, control, and\ntraining efficiency compared to existing acceleration techniques.",
        "Multi-agent systems address issues of accessibility and scalability of\nartificial intelligence (AI) foundation models, which are often represented by\nlarge language models. We develop a framework - the \"Society of HiveMind\"\n(SOHM) - that orchestrates the interaction between multiple AI foundation\nmodels, imitating the observed behavior of animal swarms in nature by following\nmodern evolutionary theories. On the one hand, we find that the SOHM provides a\nnegligible benefit on tasks that mainly require real-world knowledge. On the\nother hand, we remark a significant improvement on tasks that require intensive\nlogical reasoning, indicating that multi-agent systems are capable of\nincreasing the reasoning capabilities of the collective compared to the\nindividual agents. Our findings demonstrate the potential of combining a\nmultitude of diverse AI foundation models to form an artificial swarm\nintelligence capable of self-improvement through interactions with a given\nenvironment.",
        "Food is a key pleasure of traveling, but travelers face a trade-off between\nexploring curious new local food and choosing comfortable, familiar options.\nThis creates demand for personalized recommendation systems that balance these\ncompeting factors. To the best of our knowledge, conventional recommendation\nmethods cannot provide recommendations that offer both curiosity and comfort\nfor food unknown to the user at a travel destination. In this study, we propose\nnew quantitative methods for estimating comfort and curiosity: Kernel Density\nScoring (KDS) and Mahalanobis Distance Scoring (MDS). KDS probabilistically\nestimates food history distribution using kernel density estimation, while MDS\nuses Mahalanobis distances between foods. These methods score food based on how\ntheir representation vectors fit the estimated distributions. We also propose a\nranking method measuring the balance between comfort and curiosity based on\ntaste and ingredients. This balance is defined as curiosity (return) gained per\nunit of comfort (risk) in choosing a food. For evaluation the proposed method,\nwe newly collected a dataset containing user surveys on Japanese food and\nassessments of foreign food regarding comfort and curiosity. Comparing our\nmethods against the existing method, the Wilcoxon signed-rank test showed that\nwhen estimating comfort from taste and curiosity from ingredients, the\nMDS-based method outperformed the Baseline, while the KDS-based method showed\nno significant differences. When estimating curiosity from taste and comfort\nfrom ingredients, both methods outperformed the Baseline. The MDS-based method\nconsistently outperformed KDS in ROC-AUC values.",
        "The movable antenna (MA) technology has attracted great attention recently\ndue to its promising capability in improving wireless channel conditions by\nflexibly adjusting antenna positions. To reap maximal performance gains of MA\nsystems, existing works mainly focus on MA position optimization to cater to\nthe instantaneous channel state information (CSI). However, the resulting\nreal-time antenna movement may face challenges in practical implementation due\nto the additional time overhead and energy consumption required, especially in\nfast time-varying channel scenarios. To address this issue, we propose in this\npaper a new approach to optimize the MA positions based on the users'\nstatistical CSI over a large timescale. In particular, we propose a general\nfield response based statistical channel model to characterize the random\nchannel variations caused by the local movement of users. Based on this model,\na two-timescale optimization problem is formulated to maximize the ergodic sum\nrate of multiple users, where the precoding matrix and the positions of MAs at\nthe base station (BS) are optimized based on the instantaneous and statistical\nCSI, respectively. To solve this non-convex optimization problem, a log-barrier\npenalized gradient ascent algorithm is developed to optimize the MA positions,\nwhere two methods are proposed to approximate the ergodic sum rate and its\ngradients with different complexities. Finally, we present simulation results\nto evaluate the performance of the proposed design and algorithms based on\npractical channels generated by ray-tracing. The results verify the performance\nadvantages of MA systems compared to their fixed-position antenna (FPA)\ncounterparts in terms of long-term rate improvement, especially for scenarios\nwith more diverse channel power distributions in the angular domain.",
        "Identifying cause-and-effect relationships is critical to understanding\nreal-world dynamics and ultimately causal reasoning. Existing methods for\nidentifying event causality in NLP, including those based on Large Language\nModels (LLMs), exhibit difficulties in out-of-distribution settings due to the\nlimited scale and heavy reliance on lexical cues within available benchmarks.\nModern benchmarks, inspired by probabilistic causal inference, have attempted\nto construct causal graphs of events as a robust representation of causal\nknowledge, where \\texttt{CRAB} \\citep{romanou2023crab} is one such recent\nbenchmark along this line. In this paper, we introduce \\texttt{ACCESS}, a\nbenchmark designed for discovery and reasoning over abstract causal events.\nUnlike existing resources, \\texttt{ACCESS} focuses on causality of everyday\nlife events on the abstraction level. We propose a pipeline for identifying\nabstractions for event generalizations from \\texttt{GLUCOSE}\n\\citep{mostafazadeh-etal-2020-glucose}, a large-scale dataset of implicit\ncommonsense causal knowledge, from which we subsequently extract $1,4$K causal\npairs. Our experiments highlight the ongoing challenges of using statistical\nmethods and\/or LLMs for automatic abstraction identification and causal\ndiscovery in NLP. Nonetheless, we demonstrate that the abstract causal\nknowledge provided in \\texttt{ACCESS} can be leveraged for enhancing QA\nreasoning performance in LLMs.",
        "We characterize a binomial such that the Artinian algebra whose Macaulay dual\ngenerator is the binomial is a complete intersection. As an application, we\nprove that the Artinian algebra with a binomial Macaulay dual generator has the\nstrong Lefschetz property in characteristic 0 if the Artinian algebra is a\ncomplete intersection.",
        "VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework\ndesigned for large scenes. The framework comprises four main components: VIO\nFront End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO\nFront End, RGB frames are processed through dense bundle adjustment and\nuncertainty estimation to extract scene geometry and poses. Based on this\noutput, the mapping module incrementally constructs and maintains a 2D Gaussian\nmap. Key components of the 2D Gaussian Map include a Sample-based Rasterizer,\nScore Manager, and Pose Refinement, which collectively improve mapping speed\nand localization accuracy. This enables the SLAM system to handle large-scale\nurban environments with up to 50 million Gaussian ellipsoids. To ensure global\nconsistency in large-scale scenes, we design a Loop Closure module, which\ninnovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian\nSplatting for loop closure detection and correction of the Gaussian map.\nAdditionally, we propose a Dynamic Eraser to address the inevitable presence of\ndynamic objects in real-world outdoor scenes. Extensive evaluations in indoor\nand outdoor environments demonstrate that our approach achieves localization\nperformance on par with Visual-Inertial Odometry while surpassing recent\nGS\/NeRF SLAM methods. It also significantly outperforms all existing methods in\nterms of mapping and rendering quality. Furthermore, we developed a mobile app\nand verified that our framework can generate high-quality Gaussian maps in real\ntime using only a smartphone camera and a low-frequency IMU sensor. To the best\nof our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method\ncapable of operating in outdoor environments and supporting kilometer-scale\nlarge scenes.",
        "Distributed systems often exhibit emergent behaviors that impact their\nresilience (Franz-Kaiser et al., 2020; Adilson E. Motter, 2002; Jianxi Gao,\n2016). This paper presents a theoretical framework combining attributed graph\nmodels, flow-on-graph simulation, and sheaf-theoretic causal emergence analysis\nto evaluate system resilience. We model a distributed system as a graph with\nattributes (capturing component state and connections) and use sheaf theory to\nformalize how local interactions compose into global states. A flow simulation\non this graph propagates functional loads and failures. To assess resilience,\nwe apply the concept of causal emergence, quantifying whether macro-level\ndynamics (coarse-grained groupings) exhibit stronger causal efficacy (via\neffective information) than micro-level dynamics. The novelty lies in uniting\nsheaf-based formalization with causal metrics to identify emergent resilient\nstructures. We discuss limitless potential applications (illustrated by\nmicroservices, neural networks, and power grids) and outline future steps\ntoward implementing this framework (Lake et al., 2015).",
        "Although multi-task learning (MTL) has been a preferred approach and\nsuccessfully applied in many real-world scenarios, MTL models are not\nguaranteed to outperform single-task models on all tasks mainly due to the\nnegative effects of conflicting gradients among the tasks. In this paper, we\nfully examine the influence of conflicting gradients and further emphasize the\nimportance and advantages of achieving non-conflicting gradients which allows\nsimple but effective trade-off strategies among the tasks with stable\nperformance. Based on our findings, we propose the Gradient Deconfliction via\nOrthogonal Projections onto Subspaces (GradOPS) spanned by other task-specific\ngradients. Our method not only solves all conflicts among the tasks, but can\nalso effectively search for diverse solutions towards different trade-off\npreferences among the tasks. Theoretical analysis on convergence is provided,\nand performance of our algorithm is fully testified on multiple benchmarks in\nvarious domains. Results demonstrate that our method can effectively find\nmultiple state-of-the-art solutions with different trade-off strategies among\nthe tasks on multiple datasets.",
        "We explore the effects of coordinated users (i.e., users characterized by an\nunexpected, suspicious, or exceptional similarity) in information spreading on\nTwitter by quantifying the efficacy of their tactics in deceiving feed\nalgorithms to maximize information outreach. In particular, we investigate the\nbehavior of coordinated accounts within a large set of retweet-based\ninformation cascades identifying key differences between coordinated and\nnon-coordinated accounts in terms of position within the cascade, action delay\nand outreach. On average, coordinated accounts occupy higher positions of the\ninformation cascade (i.e., closer to the root), spread messages faster and\ninvolve a slightly higher number of users. When considering cascade metrics\nsuch as size, number of edges and height, we observe clear differences among\ninformation cascades that are associated to a systematically larger proportion\nof coordinated accounts, as confirmed by comparisons with statistical null\nmodels. To further characterize the activity of coordinated accounts we\nintroduce two new measures capturing their infectivity within the information\ncascade (i.e., their ability to involve other users) and their interaction with\nnon-coordinated accounts. Finally, we find that the interaction pattern between\nthe two classes of users follows a saturation-like process. A larger-scale\ntargeting of non-coordinated users does not require a larger amount of\ncoordinated accounts after a threshold value approximately 50%, after which\ninvolving more coordinated accounts within a cascade yields a null marginal\neffect. Our results contribute to shed light on the role of coordinated\naccounts and their effect on information diffusion."
      ]
    }
  },
  {
    "id":2411.00758,
    "research_type":"basic",
    "start_id":"b18",
    "start_title":"Inverse methods for illumination optics",
    "start_abstract":"\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers.",
    "start_categories":[
      "math.MP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Introduction to Nonimaging Optics, second edition"
      ],
      "abstract":[
        "Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book."
      ],
      "categories":[
        "physics.optics"
      ]
    },
    "list":{
      "title":[
        "Opinion Dynamics with Multiple Adversaries",
        "Text to Band Gap: Pre-trained Language Models as Encoders for\n  Semiconductor Band Gap Prediction",
        "Dynamical analysis of an HIV infection model including quiescent cells\n  and immune response",
        "GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for\n  Remote Sensing Image Analysis",
        "Controlling spin currents with magnon interference in a canted\n  antiferromagnet",
        "Pre-trained Models Succeed in Medical Imaging with Representation\n  Similarity Degradation",
        "Mass and Metal Flows in Isolated IllustrisTNG Halos",
        "Resonance nuclear excitation of the $^{229}$Th nucleus via electronic\n  bridge process in Th~II",
        "Adaptive Prompt: Unlocking the Power of Visual Prompt Tuning",
        "SN1987A bounds on neutrino quantum decoherence",
        "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
        "Cryogenic operation of silicon photomultiplier arrays",
        "LexGenie: Automated Generation of Structured Reports for European Court\n  of Human Rights Case Law",
        "Low-Rank Adapters Meet Neural Architecture Search for LLM Compression",
        "Strong Solutions and Quantization-Based Numerical Schemes for a Class of\n  Non-Markovian Volatility Models",
        "Shining Yourself: High-Fidelity Ornaments Virtual Try-on with Diffusion\n  Model",
        "Bulk-edge correspondence at the spin-to-integer quantum Hall effect\n  crossover in topological superconductors",
        "Privacy Ripple Effects from Adding or Removing Personal Information in\n  Language Model Training",
        "Generalized reciprocal diffractive imaging for stand-alone,\n  reference-free, fast-measurable quantitative phase microscopy",
        "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
        "A Fluctuation Theory of Transport Properties in Liquid-Phase Solutions",
        "Evaluating unsupervised contrastive learning framework for MRI sequences\n  classification",
        "Paper Copilot: The Artificial Intelligence and Machine Learning\n  Community Should Adopt a More Transparent and Regulated Peer Review Process",
        "A pilot survey on globular clusters with the Wide Field Survey Telescope\n  (WFST)",
        "Flux-Driven Circular Current in a Non-Hermitian Dimerized Aharonov-Bohm\n  Ring: Impact of Physical Gain and Loss",
        "Parameter-robust Preconditioners for the Stokes-Darcy Coupled Problem\n  without Fractional Operators",
        "Sub-kHz single-frequency pulsed semiconductor laser based on NPRO\n  injection locking",
        "Direct Evidence for the $\\bar{D}D^*\/D\\bar{D}^*$ Molecular Nature of\n  $G(3900)$ Through Triangular Singularity Mechanisms",
        "Sidebands to mHz QPOs in 4U 1626$-$67 in the second spin-down state"
      ],
      "abstract":[
        "Opinion dynamics model how the publicly expressed opinions of users in a\nsocial network coevolve according to their neighbors as well as their own\nintrinsic opinion. Motivated by the real-world manipulation of social networks\nduring the 2016 US elections and the 2019 Hong Kong protests, a growing body of\nwork models the effects of a strategic actor who interferes with the network to\ninduce disagreement or polarization. We lift the assumption of a single\nstrategic actor by introducing a model in which any subset of network users can\nmanipulate network outcomes. They do so by acting according to a fictitious\nintrinsic opinion. Strategic actors can have conflicting goals, and push\ncompeting narratives. We characterize the Nash Equilibrium of the resulting\nmeta-game played by the strategic actors. Experiments on real-world social\nnetwork datasets from Twitter, Reddit, and Political Blogs show that strategic\nagents can significantly increase polarization and disagreement, as well as\nincrease the \"cost\" of the equilibrium. To this end, we give worst-case upper\nbounds on the Price of Misreporting (analogous to the Price of Anarchy).\nFinally, we give efficient learning algorithms for the platform to (i) detect\nwhether strategic manipulation has occurred, and (ii) learn who the strategic\nactors are. Our algorithms are accurate on the same real-world datasets,\nsuggesting how platforms can take steps to mitigate the effects of strategic\nbehavior.",
        "In this study, we explore the use of a transformer-based language model as an\nencoder to predict the band gaps of semiconductor materials directly from their\ntext descriptions. Quantum chemistry simulations, including Density Functional\nTheory (DFT), are computationally intensive and time-consuming, which limits\ntheir practicality for high-throughput material screening, particularly for\ncomplex systems. Shallow machine learning (ML) models, while effective, often\nrequire extensive data preprocessing to convert non-numerical material\nproperties into numerical inputs. In contrast, our approach leverages textual\ndata directly, bypassing the need for complex feature engineering. We generate\nmaterial descriptions in two formats: formatted strings combining features and\nnatural language text generated using the ChatGPT API. We demonstrate that the\nRoBERTa model, pre-trained on natural language processing tasks, performs\neffectively as an encoder for prediction tasks. With minimal fine-tuning, it\nachieves a mean absolute error (MAE) of approximately 0.33 eV, performing\nbetter than shallow machine learning models such as Support Vector Regression,\nRandom Forest, and XGBoost. Even when only the linear regression head is\ntrained while keeping the RoBERTa encoder layers frozen, the accuracy remains\nnearly identical to that of the fully trained model. This demonstrates that the\npre-trained RoBERTa encoder is highly adaptable for processing domain-specific\ntext related to material properties, such as the band gap, significantly\nreducing the need for extensive retraining. This study highlights the potential\nof transformer-based language models to serve as efficient and versatile\nencoders for semiconductor materials property prediction tasks.",
        "This research gives a thorough examination of an HIV infection model that\nincludes quiescent cells and immune response dynamics in the host. The model,\nrepresented by a system of ordinary differential equations, captures the\ncomplex interaction between the host's immune response and viral infection. The\nstudy focuses on the model's fundamental aspects, such as equilibrium analysis,\ncomputing the basic reproduction number $\\mathcal{R}_0$, stability analysis,\nbifurcation phenomena, numerical simulations, and sensitivity analysis.\n  The analysis reveals both an infection equilibrium, which indicates the\npersistence of the illness, and an infection-free equilibrium, which represents\ndisease control possibilities. Applying matrix-theoretical approaches,\nstability analysis proved that the infection-free equilibrium is both locally\nand globally stable for $\\mathcal{R}_0 < 1$. For the situation of\n$\\mathcal{R}_0 > 1$, the infection equilibrium is locally asymptotically stable\nvia the Routh--Hurwitz criterion. We also studied the uniform persistence of\nthe infection, demonstrating that the infection remains present above a\npositive threshold under certain conditions. The study also found a\ntranscritical forward-type bifurcation at $\\mathcal{R}_0 = 1$, indicating a\ncritical threshold that affects the system's behavior. The model's temporal\ndynamics are studied using numerical simulations, and sensitivity analysis\nidentifies the most significant variables by assessing the effects of parameter\nchanges on system behavior.",
        "The continuous operation of Earth-orbiting satellites generates vast and\never-growing archives of Remote Sensing (RS) images. Natural language presents\nan intuitive interface for accessing, querying, and interpreting the data from\nsuch archives. However, existing Vision-Language Models (VLMs) are\npredominantly trained on web-scraped, noisy image-text data, exhibiting limited\nexposure to the specialized domain of RS. This deficiency results in poor\nperformance on RS-specific tasks, as commonly used datasets often lack\ndetailed, scientifically accurate textual descriptions and instead emphasize\nsolely on attributes like date and location. To bridge this critical gap, we\nintroduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and\nmulti-modal RS image analysis. GAIA comprises of 205,150 meticulously curated\nRS image-text pairs, representing a diverse range of RS modalities associated\nto different spatial resolutions. Unlike existing vision-language datasets in\nRS, GAIA specifically focuses on capturing a diverse range of RS applications,\nproviding unique information about environmental changes, natural disasters,\nand various other dynamic phenomena. The dataset provides a spatially and\ntemporally balanced distribution, spanning across the globe, covering the last\n25 years with a balanced temporal distribution of observations. GAIA's\nconstruction involved a two-stage process: (1) targeted web-scraping of images\nand accompanying text from reputable RS-related sources, and (2) generation of\nfive high-quality, scientifically grounded synthetic captions for each image\nusing carefully crafted prompts that leverage the advanced vision-language\ncapabilities of GPT-4o. Our extensive experiments, including fine-tuning of\nCLIP and BLIP2 models, demonstrate that GAIA significantly improves performance\non RS image classification, cross-modal retrieval and image captioning tasks.",
        "Controlling spin current lies at the heart of spintronics and its\napplications. The sign of spin currents is monotonous in ferromagnets once the\ncurrent direction is determined. Spin currents in antiferromagnets can possess\nopposite polarization, but requires enormous magnetic fields to lift the\ndegeneracy. Controlling spin currents with different polarization is urgently\ndemanded but remains hitherto elusive. Here, we demonstrate the control of spin\ncurrents at room temperature by magnon interference in a canted\nantiferromagnet, hematite recently also classified as an altermagnet.\nMagneto-optical characterization by Brillouin light scattering revealed that\nthe spatial periodicity of the beating patterns was tunable via the microwave\nfrequency. The inverse spin-Hall voltage changed sign as the frequency was\nscanned, i.e., a frequency-controlled switching of polarization in pure spin\ncurrents was obtained. Our work marks the use of antiferromagnetic magnon\ninterference to control spin currents, which substantially extends the horizon\nfor the emerging field of coherent antiferromagnetic spintronics.",
        "This paper investigates the critical problem of representation similarity\nevolution during cross-domain transfer learning, with particular focus on\nunderstanding why pre-trained models maintain effectiveness when adapted to\nmedical imaging tasks despite significant domain gaps. The study establishes a\nrigorous problem definition centered on quantifying and analyzing\nrepresentation similarity trajectories throughout the fine-tuning process,\nwhile carefully delineating the scope to encompass both medical image analysis\nand broader cross-domain adaptation scenarios. Our empirical findings reveal\nthree critical discoveries: the potential existence of high-performance models\nthat preserve both task accuracy and representation similarity to their\npre-trained origins; a robust linear correlation between layer-wise similarity\nmetrics and representation quality indicators; and distinct adaptation patterns\nthat differentiate supervised versus self-supervised pre-training paradigms.\nThe proposed similarity space framework not only provides mechanistic insights\ninto knowledge transfer dynamics but also raises fundamental questions about\noptimal utilization of pre-trained models. These results advance our\nunderstanding of neural network adaptation processes while offering practical\nimplications for transfer learning strategies that extend beyond medical\nimaging applications. The code will be available once accepted.",
        "The cicumgalactic medium (CGM) is a reservoir of metals and star-forming\nfuel. Most baryons in the universe are in the circumgalactic medium (CGM) or\nintergalactic medium (IGM). The baryon cycle -- how mass and metals reach the\nCGM from the inner regions of the galaxy and how gas from the CGM replenishes\nstar-forming activity in the inner regions -- is an essential question in\ngalaxy evolution. In this paper, we study the flow of mass and metals in a\nstacked sample of 2770 isolated halos from the IllustrisTNG cosmological\nhydrodynamic simulation. The mean gas flow as a function of radius and angle is\nsimilar across a large galactic mass range when accounting for different\nfeedback modes. Although both star formation and black holes cause powerful\noutflows, the flows from star formation are more angularly restricted. Black\nhole feedback dominates massflow throughout the halo, while star-formation\nfeedback mainly affects the inner region. When scaling by virial radius\n($R_v$), large dynamical changes occur at $0.2R_v$ for most halos, suggesting a\ncharacteristic size for the inner galaxy. Despite radio mode feedback from\nblack holes being the primary quenching mechanism in IllustrisTNG, a small\npopulation of high mass radio mode disks are able to form stars.",
        "The 8.4 eV transition in the $^{229}$Th nucleus is the basis for a\nhigh-precision nuclear clock with exceptional sensitivity to new physics\neffects. We have identified several cases in the Th$^+$ ion where electronic\nexcitations closely resonate with the nuclear excitation, with the smallest\nenergy difference being $\\Delta = -0.09$ cm$^{-1}$. We investigate the\nelectronic bridge process, in which nuclear excitation is induced via\nelectronic transitions, and demonstrate that a proper selection of laser\nfrequencies can lead to a dramatic enhancement of this effect. Additionally, we\nshow that the interaction with electrons significantly shortens the lifetime of\nthe nuclear excited state.",
        "Visual Prompt Tuning (VPT) has recently emerged as a powerful method for\nadapting pre-trained vision models to downstream tasks. By introducing\nlearnable prompt tokens as task-specific instructions, VPT effectively guides\npre-trained transformer models with minimal overhead. Despite its empirical\nsuccess, a comprehensive theoretical understanding of VPT remains an active\narea of research. Building on recent insights into the connection between\nmixture of experts and prompt-based approaches, we identify a key limitation in\nVPT: the restricted functional expressiveness in prompt formulation. To address\nthis limitation, we propose Visual Adaptive Prompt Tuning (VAPT), a new\ngeneration of prompts that redefines prompts as adaptive functions of the\ninput. Our theoretical analysis shows that this simple yet intuitive approach\nachieves optimal sample efficiency. Empirical results on VTAB-1K and FGVC\nfurther demonstrate VAPT's effectiveness, with performance gains of 7.34% and\n1.04% over fully fine-tuning baselines, respectively. Notably, VAPT also\nsurpasses VPT by a substantial margin while using fewer parameters. These\nresults highlight both the effectiveness and efficiency of our method and pave\nthe way for future research to explore the potential of adaptive prompts.",
        "We obtain stringent bounds on neutrino quantum decoherence from the analysis\nof SN1987A data. We show that for the decoherence model considered here, which\nallows for neutrino-loss along the trajectory, the bounds are many orders of\nmagnitude stronger than the ones that can be obtained from the analysis of data\nfrom reactor neutrino oscillation experiments or neutrino telescopes.",
        "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
        "The LHCb experiment at CERN has been upgraded for the Run 3 operation of the\nLarge Hadron Collider (LHC). A new concept of tracking detector based on\nScintillating Fibres (SciFi) read out with multichannel silicon\nphotomultipliers (SiPMs) was installed during its upgrade. One of the main\nchallenges the SciFi tracker will face during the Run 4 operation of the LHC is\nthe higher radiation environment due to fast neutrons, where the SiPMs are\nlocated. To cope with the increase in radiation, cryogenic cooling with liquid\nnitrogen is being investigated as a possible solution to mitigate the\nperformance degradation of the SiPMs induced by radiation damage. Thus, a\ndetailed performance study of different layouts of SiPM arrays produced by\nFondazione Bruno Kessler (FBK) and Hamamatsu Photonics K.K. is being carried\nout. These SiPMs have been designed to operate at cryogenic temperatures.\nSeveral SiPMs have been tested in a dedicated cryogenic setup down to 100 K.\nKey performance parameters such as breakdown voltage, dark count rate, photon\ndetection efficiency, gain and direct cross-talk are characterized as a\nfunction of the temperature. The main results of this study are going to be\npresented here.",
        "Analyzing large volumes of case law to uncover evolving legal principles,\nacross multiple cases, on a given topic is a demanding task for legal\nprofessionals. Structured topical reports provide an effective solution by\nsummarizing key issues, principles, and judgments, enabling comprehensive legal\nanalysis on a particular topic. While prior works have advanced query-based\nindividual case summarization, none have extended to automatically generating\nmulti-case structured reports. To address this, we introduce LexGenie, an\nautomated LLM-based pipeline designed to create structured reports using the\nentire body of case law on user-specified topics within the European Court of\nHuman Rights jurisdiction. LexGenie retrieves, clusters, and organizes relevant\npassages by topic to generate a structured outline and cohesive content for\neach section. Expert evaluation confirms LexGenie's utility in producing\nstructured reports that enhance efficient, scalable legal analysis.",
        "The rapid expansion of Large Language Models (LLMs) has posed significant\nchallenges regarding the computational resources required for fine-tuning and\ndeployment. Recent advancements in low-rank adapters have demonstrated their\nefficacy in parameter-efficient fine-tuning (PEFT) of these models. This\nretrospective paper comprehensively discusses innovative approaches that\nsynergize low-rank representations with Neural Architecture Search (NAS)\ntechniques, particularly weight-sharing super-networks. Robust solutions for\ncompressing and fine-tuning large pre-trained models are developed by\nintegrating these methodologies. Our analysis highlights the potential of these\ncombined strategies to democratize the use of LLMs, making them more accessible\nfor deployment in resource-constrained environments. The resulting models\nexhibit reduced memory footprints and faster inference times, paving the way\nfor more practical and scalable applications of LLMs. Models and code are\navailable at\nhttps:\/\/github.com\/IntelLabs\/Hardware-Aware-Automated-Machine-Learning.",
        "We investigate a class of non-Markovian processes that hold particular\nrelevance in the realm of mathematical finance. This family encompasses\npath-dependent volatility models, including those pioneered by [Platen and\nRendek, 2018] and, more recently, by [Guyon and Lekeufack, 2023], as well as an\nextension of the framework proposed by [Blanc et al., 2017]. Our study unfolds\nin two principal phases. In the first phase, we introduce a functional\nquantization scheme based on an extended version of the Lamperti transformation\nthat we propose to handle the presence of a memory term incorporated into the\ndiffusion coefficient. For scenarios involving a Brownian integral in the\ndiffusion term, we propose alternative numerical schemes that leverage the\npower of marginal recursive quantization. In the second phase, we study the\nproblem of existence and uniqueness of a strong solution for the SDEs related\nto the examples that motivate our study, in order to provide a theoretical\nbasis to correctly apply the proposed numerical schemes.",
        "While virtual try-on for clothes and shoes with diffusion models has gained\nattraction, virtual try-on for ornaments, such as bracelets, rings, earrings,\nand necklaces, remains largely unexplored. Due to the intricate tiny patterns\nand repeated geometric sub-structures in most ornaments, it is much more\ndifficult to guarantee identity and appearance consistency under large pose and\nscale variances between ornaments and models. This paper proposes the task of\nvirtual try-on for ornaments and presents a method to improve the geometric and\nappearance preservation of ornament virtual try-ons. Specifically, we estimate\nan accurate wearing mask to improve the alignments between ornaments and models\nin an iterative scheme alongside the denoising process. To preserve structure\ndetails, we further regularize attention layers to map the reference ornament\nmask to the wearing mask in an implicit way. Experimental results demonstrate\nthat our method successfully wears ornaments from reference images onto target\nmodels, handling substantial differences in scale and pose while preserving\nidentity and achieving realistic visual effects.",
        "The spin and integer quantum Hall effects are two cousins of topological\nphase transitions in two-dimensional electronic systems. Their close\nrelationship makes it possible to transform spin to integer quantum Hall effect\nin two-dimensional topological superconductors by continuous increase in a\nsymmetry breaking Zeeman magnetic field. We study peculiarities of bulk-edge\ncorrespondence and a fate of massless edge and bulk topological (instantons)\nexcitations at such the crossover.",
        "Due to the sensitive nature of personally identifiable information (PII), its\nowners may have the authority to control its inclusion or request its removal\nfrom large-language model (LLM) training. Beyond this, PII may be added or\nremoved from training datasets due to evolving dataset curation techniques,\nbecause they were newly scraped for retraining, or because they were included\nin a new downstream fine-tuning stage. We find that the amount and ease of PII\nmemorization is a dynamic property of a model that evolves throughout training\npipelines and depends on commonly altered design choices. We characterize three\nsuch novel phenomena: (1) similar-appearing PII seen later in training can\nelicit memorization of earlier-seen sequences in what we call assisted\nmemorization, and this is a significant factor (in our settings, up to 1\/3);\n(2) adding PII can increase memorization of other PII significantly (in our\nsettings, as much as $\\approx\\!7.5\\times$); and (3) removing PII can lead to\nother PII being memorized. Model creators should consider these first- and\nsecond-order privacy risks when training models to avoid the risk of new PII\nregurgitation.",
        "Optical microscopy has been employed to derive salient characteristics of an\nobject in various fields, including cell biology, flow cytometry, biopsy, and\nneuroscience. In particular, measuring the phase of light scattered from an\nobject aroused great interest by allowing retrieving quantitative parameters\nsuch as refractive index, an intrinsic property of a material. Reciprocal\ndiffractive imaging (RDI) has succeeded in recovering the light field scattered\nfrom diffusive objects without special restrictions on illumination and sample\nsupport from a single-shot intensity in the reference-free regime. However, RDI\nis limited to imaging samples in the diffusive regime, making application to\nbiological samples difficult. Here, we extend RDI to biological applications by\nspatially filtering the transmitted fields in the pupil plane. The proposed\nmethod is demonstrated by imaging the objects with known structures and various\nbiological samples, showing its capability as a stand-alone optical microscope.\nWe believe that the presented advance could be at the forefront of quantitative\nphase imaging due to the unique advantages the technique possesses.",
        "Interactive digital agents (IDAs) leverage APIs of stateful digital\nenvironments to perform tasks in response to user requests. While IDAs powered\nby instruction-tuned large language models (LLMs) can react to feedback from\ninterface invocations in multi-step exchanges, they have not been trained in\ntheir respective digital environments. Prior methods accomplish less than half\nof tasks in sophisticated benchmarks such as AppWorld. We present a\nreinforcement learning (RL) approach that trains IDAs directly in their target\nenvironments. We formalize this training as a partially observable Markov\ndecision process and derive LOOP, a data- and memory-efficient variant of\nproximal policy optimization. LOOP uses no value network and maintains exactly\none copy of the underlying LLM in memory, making its implementation\nstraightforward and as memory-efficient as fine-tuning a single LLM. A\n32-billion-parameter agent trained with LOOP in the AppWorld environment\noutperforms the much larger OpenAI o1 agent by 9 percentage points (15%\nrelative). To our knowledge, this is the first reported application of RL to\nIDAs that interact with a stateful, multi-domain, multi-app environment via\ndirect API calls. Our analysis sheds light on the effectiveness of RL in this\narea, showing that the agent learns to consult the API documentation, avoid\nunwarranted assumptions, minimize confabulation, and recover from setbacks.",
        "The challenge of comprehensively describing liquids and their mixtures beyond\nequilibrium continues to be a main concern in modern chemical physics and\nphysical chemistry, particularly in the context of calculating the transport\nproperties of liquid-phase systems. This paper presents a step towards a\nphenomenological nonequilibrium theory tailored for multicomponent liquid-phase\nsolutions. This field-theoretical framework, grounded in the principles of\nnonequilibrium statistical mechanics, integrates quasi-stationary concentration\nfluctuations consistent with equilibrium liquid theory as described by\nclassical density functional theory. This approach, which is a phenomenological\nextension of the well-known Dean-Kawasaki stochastic density functional theory,\nallows for the computation of practically important transport properties such\nas mobility and shear viscosity. We apply our approach to the calculation of\nthe corresponding quantities for solutions with a single solute. We derive\ngeneral formulas for the mobility of solute molecules and the shear viscosity\nof a single-solute solution. Based on these findings, we present new results\nand reproduce previously established results for systems such as the\nGaussian-core model, one-component plasma, and near-critical solutions.",
        "The automatic identification of Magnetic Resonance Imaging (MRI) sequences\ncan streamline clinical workflows by reducing the time radiologists spend\nmanually sorting and identifying sequences, thereby enabling faster diagnosis\nand treatment planning for patients. However, the lack of standardization in\nthe parameters of MRI scans poses challenges for automated systems and\ncomplicates the generation and utilization of datasets for machine learning\nresearch. To address this issue, we propose a system for MRI sequence\nidentification using an unsupervised contrastive deep learning framework. By\ntraining a convolutional neural network based on the ResNet-18 architecture,\nour system classifies nine common MRI sequence types as a 9-class\nclassification problem. The network was trained using an in-house internal\ndataset and validated on several public datasets, including BraTS, ADNI, Fused\nRadiology-Pathology Prostate Dataset, the Breast Cancer Dataset (ACRIN), among\nothers, encompassing diverse acquisition protocols and requiring only 2D slices\nfor training. Our system achieves a classification accuracy of over 0.95 across\nthe nine most common MRI sequence types.",
        "The rapid growth of submissions to top-tier Artificial Intelligence (AI) and\nMachine Learning (ML) conferences has prompted many venues to transition from\nclosed to open review platforms. Some have fully embraced open peer reviews,\nallowing public visibility throughout the process, while others adopt hybrid\napproaches, such as releasing reviews only after final decisions or keeping\nreviews private despite using open peer review systems. In this work, we\nanalyze the strengths and limitations of these models, highlighting the growing\ncommunity interest in transparent peer review. To support this discussion, we\nexamine insights from Paper Copilot, a website launched two years ago to\naggregate and analyze AI \/ ML conference data while engaging a global audience.\nThe site has attracted over 200,000 early-career researchers, particularly\nthose aged 18-34 from 177 countries, many of whom are actively engaged in the\npeer review process. Drawing on our findings, this position paper advocates for\na more transparent, open, and well-regulated peer review aiming to foster\ngreater community involvement and propel advancements in the field.",
        "We carry out an imaging survey of six globular clusters (GCs) with a limit\nmagnitude to 22 mag at the 5 sigma level, down to the main sequence stars of\nthe respective cluster, as one of the pilot observing program of the Wide Field\nSurvey Telescope (WFST). This paper present the early results of this survey,\nwhere we investigate the tidal characters at the periphery of the clusters NGC\n4147, NGC 5024, NGC 5053, NGC 5272, NGC 5904 and NGC 6341. We present the\nestimated number density of cluster candidates and their spatial distribution.\nWe confirm the presence of tidal arms in NGC 4147 and NGC 5904 and identify\nseveral intriguing potential tidal structures in NGC 4147, NGC 5024, NGC 5272,\ncorroborated the elliptical morphology of the periphery of NGC 6341. Our\nfindings underscore the WFST's capability for probing faint structural features\nin GCs, paving the way for future in-depth studies, especially for the search\nof the large scale tidal streams associated with the clusters with the future\nwide field survey.",
        "In the present work, we explore magnetic response of a dimerized ring\nsubjected to Aharonov-Bohm (AB) flux and environmental interactions.\nSpecifically, we introduce an imaginary site potential on the odd lattice sites\nto represent physical gain and loss, while the even lattice sites remain\nunperturbed. We investigate the induced current resulting from the AB flux in\nboth real and imaginary eigenspaces, aiming to enhance this current\nsignificantly by adjusting the gain\/loss parameter ($d$). Our analysis focuses\non how exceptional points in the real and imaginary eigenenergy spaces\ncontribute to notable increases in current at specific $d$ values, and the\nemergence of purely real current when the imaginary current vanishes. We focus\non how the converging and diverging nature of the energy spectrum leads to\ngradual increases and decreases in the current. Additionally, we study the\ninterplay between the correlations of dimerized hopping integrals and the\ngain-loss parameter, which affects the current and highlights key features\nassociated with these physical parameters. Furthermore, we consider how system\nsize impacts our findings. These investigations may reveal unconventional\ncharacteristics in various loop configurations, potentially paving the way for\nnew research directions.",
        "We consider the Stokes-Darcy coupled problem, which models the interaction\nbetween free-flow and porous medium flow. By enforcing the normal flux\ncontinuity interface condition directly within the finite-element spaces, we\nestablish unified well-posedness results for the coupled system under various\nboundary condition scenarios. Using the operator preconditioning framework, we\ndevelop a parameter-robust preconditioner that avoids the use of fractional\noperators. Numerical experiments employing both\n$H(\\operatorname{div})$-conforming and nonconforming finite-element methods are\npresented to confirm the theoretical findings and demonstrate the robustness of\nthe proposed block preconditioners with respect to the physical parameters and\nmesh size.",
        "We report a single-frequency, narrow-linewidth semiconductor pulsed laser\nbased on pump current modulation and optical injection locking technique. A\nmonolithic non-planar ring oscillator laser is employed as the seed source to\nguarantee the single-frequency narrow-linewidth performance. Simultaneously,\npulse operation is achieved by directly modulating the pump current of the\nsemiconductor laser. The single-frequency pulsed laser (SFPL) has achieved a\npulse repetition rate of 50 kHz-1 MHz, a pulse duration ranging from 120 ns to\na quasi-continuous state, and a peak power of 160 mW. Moreover, the SFPL has\nreached a pulsed laser linewidth as narrow as 905 Hz, optical spectrum\nsignal-to-noise ratio of better than 65 dB at a center wavelength of 1064.45\nnm. Such extremely narrow-linewidth, repetition-rate and pulse-width tunable\nSFPL has great potential for applications in coherent LIDAR, metrology, remote\nsensing, and nonlinear frequency conversion.",
        "The exotic hadron $G(3900)$, initially observed in the process $e^+ e^- \\to\nD\\bar{D}$, has been further supported by analyses from the BESIII\nCollaboration, which classify it as a $P$-wave molecular state of\n$D^{+}D^{*-}\/D^{-}D^{*+}$. However, theoretical discussions raise concerns\nabout its status as a true particle, emphasizing the need for additional\nstudies. In this Letter, we employ the triangular singularity mechanism to\ninvestigate $G(3900)$ across various reaction channels, allowing us to produce\nsignificant peaks without relying on the existence of a real particle. We\nidentify $X(4020)$, $Y(4320)$, and the tentative $X(4014)$ as potential sources\nof these peaks via decay processes to $\\gamma G(3900)$ or $\\pi G(3900)$. We\nstress the importance of experimental explorations of $e^+ e^- \\to X\/Y \\to\n\\gamma(\\pi) G(3900)$, which are essential for confirming the molecular\ncomposition of $D^{+}D^{*-}\/D^{-}D^{*+}$ and refining the mass measurement of\n$G(3900)$.",
        "We report results from an $AstroSat$ Target-of-Opportunity (ToO) observation\nof 4U 1626$-$67, performed on 2023 May 18, soon after the discovery of torque\nreversal to spin-down in the source. The X-ray emission exhibited significant\ndependence on both energy and torque state. This work highlights the comparison\nof timing features of 4U 1626$-$67 with a previous $AstroSat$ observation from\n2018, when the neutron star was in the spin-up state. The power density\nspectrum (PDS) of the 2023 observation comprised a sharp peak corresponding to\n$\\nu_{\\rm NS}\\sim$130 mHz X-ray pulsations along with a prominent\nquasi-periodic oscillation (QPO) feature at $\\nu_{\\rm QPO}\\sim$46 mHz with\n$\\sim$20\\% rms amplitude, which was positively correlated with energy. We also\nreport the detection of sidebands to QPO occurring at a beat frequency\n($\\nu_{\\rm NS}-\\nu_{\\rm QPO}$) of $\\sim$83 mHz with $\\sim$8\\% rms amplitude,\nhaving $>3\\sigma$ detection significance. Additionally, we utilized $Nuclear\n~Spectroscopic ~Telescope ~ARray$ ($NuSTAR$) observations from the same torque\nstate (2023 May-July) to analogize the presence and energy dependence of\nsidebands. The source retains timing properties in this spin-down torque state,\nsimilar to those seen in the previous spin-down phase. In sharp contrast, PDS\nfrom the 2018 observation was dominated by red noise, an absence of QPOs and a\nbroadening in the wings of the pulse frequency peak, indicating a coupling\nbetween periodic and low-frequency aperiodic variability. Furthermore, we\ndetected the known cyclotron resonance scattering feature (CRSF) at 37 keV in\nthe Large Area X-ray Proportional Counter (LAXPC) spectrum. We explore various\nmechanisms that could possibly explain the presence of QPOs exclusively during\nthe spin-down state."
      ]
    }
  },
  {
    "id":2411.01291,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI",
    "start_abstract":"Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"
      ],
      "abstract":[
        "Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Developing and Evaluating an AI-Assisted Prediction Model for Unplanned\n  Intensive Care Admissions following Elective Neurosurgery using Natural\n  Language Processing within an Electronic Healthcare Record System",
        "A Vehicle-Infrastructure Multi-layer Cooperative Decision-making\n  Framework",
        "Three-body structures of low-lying nuclear states of $^8$Li",
        "Nonequilibrium Continuous Transition in a Fast Rotating Turbulence",
        "Training Allostery-Inspired Mechanical Response in Disordered Elastic\n  Networks",
        "Spin glass behavior in amorphous CrSiTe3 alloy",
        "Representative dietary behavior patterns and associations with\n  cardiometabolic outcomes in Puerto Rico using a Bayesian latent class\n  analysis for non-probability samples",
        "On the coefficients of Tutte polynomials with one variable at 1",
        "Enhancing, Refining, and Fusing: Towards Robust Multi-Scale and Dense\n  Ship Detection",
        "Fluctuations in the email size modeled by a log-normal-like distribution",
        "Heterogeneity Matters even More in Distributed Learning: Study from\n  Generalization Perspective",
        "From Dense to Dynamic: Token-Difficulty Driven MoEfication of\n  Pre-Trained LLMs",
        "Differentiable Information Enhanced Model-Based Reinforcement Learning",
        "Underwater Soft Fin Flapping Motion with Deep Neural Network Based\n  Surrogate Model",
        "Automatic detection of single-electron regime of quantum dots and\n  definition of virtual gates using U-Net and clustering",
        "Agricultural Field Boundary Detection through Integration of \"Simple\n  Non-Iterative Clustering (SNIC) Super Pixels\" and \"Canny Edge Detection\n  Method\"",
        "A rigid origami elliptic-hyperbolic vertex duality",
        "Automation of Electroweak Corrections",
        "Momentum-Resolved Signatures of Carrier Screening Effects on\n  Electron-Phonon Coupling in MoS$_2$",
        "K-Anonymous A\/B Testing",
        "CLIMB-3D: Continual Learning for Imbalanced 3D Instance Segmentation",
        "DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance\n  Prior",
        "Optimal Transport Barycenter via Nonconvex-Concave Minimax Optimization",
        "Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual\n  Descriptions Using Gaussian Splatting, ChatGPT\/Deepseek, and Google Maps\n  Platform",
        "Cyclotron Maser Cooling towards Coherent Particle Beams",
        "Wetting simulation of the porous structure of a heat pipe using an\n  eXtended Discontinuous Galerkin Method and a Parameterized Level-Set",
        "From Traditional to Deep Learning Approaches in Whole Slide Image\n  Registration: A Methodological Review",
        "Towards modeling the short-range interactions of hidden\/open charm\n  pentaquark molecular states",
        "On the Nowicki Conjecture for the free Lie algebra of rank 2"
      ],
      "abstract":[
        "Introduction: Timely care in a specialised neuro-intensive therapy unit (ITU)\nreduces mortality and hospital stays, with planned admissions being safer than\nunplanned ones. However, post-operative care decisions remain subjective. This\nstudy used artificial intelligence (AI), specifically natural language\nprocessing (NLP) to analyse electronic health records (EHRs) and predict ITU\nadmissions for elective surgery patients. Methods: This study analysed the EHRs\nof elective neurosurgery patients from University College London Hospital\n(UCLH) using NLP. Patients were categorised into planned high dependency unit\n(HDU) or ITU admission; unplanned HDU or ITU admission; or ward \/ overnight\nrecovery (ONR). The Medical Concept Annotation Tool (MedCAT) was used to\nidentify SNOMED-CT concepts within the clinical notes. We then explored the\nutility of these identified concepts for a range of AI algorithms trained to\npredict ITU admission. Results: The CogStack-MedCAT NLP model, initially\ntrained on hospital-wide EHRs, underwent two refinements: first with data from\npatients with Normal Pressure Hydrocephalus (NPH) and then with data from\nVestibular Schwannoma (VS) patients, achieving a concept detection F1-score of\n0.93. This refined model was then used to extract concepts from EHR notes of\n2,268 eligible neurosurgical patients. We integrated the extracted concepts\ninto AI models, including a decision tree model and a neural time-series model.\nUsing the simpler decision tree model, we achieved a recall of 0.87 (CI 0.82 -\n0.91) for ITU admissions, reducing the proportion of unplanned ITU cases missed\nby human experts from 36% to 4%. Conclusion: The NLP model, refined for\naccuracy, has proven its efficiency in extracting relevant concepts, providing\na reliable basis for predictive AI models to use in clinically valid\napplications.",
        "Autonomous driving has entered the testing phase, but due to the limited\ndecision-making capabilities of individual vehicle algorithms, safety and\nefficiency issues have become more apparent in complex scenarios. With the\nadvancement of connected communication technologies, autonomous vehicles\nequipped with connectivity can leverage vehicle-to-vehicle (V2V) and\nvehicle-to-infrastructure (V2I) communications, offering a potential solution\nto the decision-making challenges from individual vehicle's perspective. We\npropose a multi-level vehicle-infrastructure cooperative decision-making\nframework for complex conflict scenarios at unsignalized intersections. First,\nbased on vehicle states, we define a method for quantifying vehicle impacts and\ntheir propagation relationships, using accumulated impact to group vehicles\nthrough motif-based graph clustering. Next, within and between vehicle groups,\na pass order negotiation process based on Large Language Models (LLM) is\nemployed to determine the vehicle passage order, resulting in planned vehicle\nactions. Simulation results from ablation experiments show that our approach\nreduces negotiation complexity and ensures safer, more efficient vehicle\npassage at intersections, aligning with natural decision-making logic.",
        "The four nucleons in $^8$Li outside the $\\alpha$-particle ($\\alpha=^4$He) can\nbe divided into pairs of one neutron ($n$) and 3 nucleons in the triton\n($t=^3$H), or 2 in the deuteron ($d=^2$H) and two neutrons in a dineutron\n($^2n$). The corresponding three-body structures, $\\alpha$+$t$+$n$ or\n$\\alpha$+$d$+$^2n$, are suggested to describe the bulk part of the low-energy\n($<10$~MeV) states of $^8$Li. Several breakup thresholds influence the\nstructures and possible decays. We calculate the three-body structures of the\nvarious $J^{\\pi}$ states, where different clustering appear, e.g. $^7$Li*+$n$,\n$^6$Li*$+^2n$, $^6$He*$+d$. The experimental $^8$Li spectrum can be reproduced\nwith fine tuning by a three-body potential parameter. Three unobserved $0^+$\nand an excited 2$^+$ states are found. All states appear as bound states or\nresonances. The lowest or highest energies have cluster structures,\n$\\alpha$+$t$+$n$ or $\\alpha$+$d$+$^2n$, respectively. We give calculated energy\nand width (if possible), geometry, and partial wave decomposition for all\nstates.",
        "We study the saturation of three-dimensional unstable perturbations on a fast\nrotating turbulent flow using direct numerical simulations (DNSs). Under the\neffect of Kolmogorov forcing, a transition between states dominated by coherent\ntwo-dimensional modes to states with three-dimensional variations\n(quasi-two-dimensional) is observed as we change the global rotation rate. We\nfind this akin to a critical phenomenon, wherein the order parameter scales\nwith the distance to the critical point raised to an exponent. The exponent\nitself deviates from the predicted mean field value. Also, the nature of the\nfluctuations of the order parameter near the critical point indicate the\npresence of on-off intermittency. The critical rotation rate at which the\ntransition occurs exhibits a linear scaling behaviour with the forcing wave\nnumber. A reduced model based on linear stability analysis is used to find the\nlinear threshold estimates; we find these to be in good agreement with the 3D\nnonlinear DNS results.",
        "Disordered elastic networks are a model material system in which it is\npossible to achieve tunable and trainable functions. This work investigates the\nmodification of local mechanical properties in disordered networks inspired by\nallosteric interactions in proteins: applying strain locally to a set of source\nnodes triggers a strain response at a distant set of target nodes. This is\ndemonstrated first by using directed aging to modify the existing mechanical\ncoupling between pairs of distant source and target nodes, and later as a means\nfor inducing coupling between formerly isolated source-target pairs. The\nexperimental results are compared with those predicted by simulations.",
        "Owing to the intrinsically high crystallization temperatures, layered\nphase-change materials, such as CrGeTe3 and InGeTe3, are attracting attention\nfor embedded memory applications, In addition to the electrical contrast, a\nmajor change in magnetic properties is observed in CrGeTe3 upon switching from\nthe crystalline to the amorphous state. In this work, we report a combined ab\ninitio modeling and magnetic characterization study on the isostructural\nsilicon parent compound of CrGeTe3, namely, CrSiTe3. Amorphous CrSiTe3 has\nsimilar structural properties to amorphous CrGeTe3; however, it shows a smaller\nenergy difference between the ferromagnetic configuration and the random\nmagnetic configuration, indicating a high probability of spin glass formation.\nIndeed, direct-current and alternating-current magnetic measurements show that\nthe coercive force of amorphous CrSiTe3 is higher than that of amorphous\nCrGeTe3. Therefore, the pinning effect of spins is enhanced in amorphous\nCrSiTe3, leading to a more robust spin glass state with a higher freezing\ntemperature. The large magnetic contrast between the amorphous and crystalline\nphase could make CrSiTe3 a potential candidate for phase-change magnetic\nswitching applications.",
        "There is limited understanding of how dietary behaviors cluster together and\ninfluence cardiometabolic health at a population level in Puerto Rico. Data\navailability is scarce, particularly outside of urban areas, and is often\nlimited to non-probability sample (NPS) data where sample inclusion mechanisms\nare unknown. In order to generalize results to the broader Puerto Rican\npopulation, adjustments are necessary to account for selection bias but are\ndifficult to implement for NPS data. Although Bayesian latent class models\nenable summaries of dietary behavior variables through underlying patterns,\nthey have not yet been adapted to the NPS setting. We propose a novel Weighted\nOverfitted Latent Class Analysis for Non-probability samples (WOLCAN). WOLCAN\nutilizes a quasi-randomization framework to (1) model pseudo-weights for an NPS\nusing Bayesian additive regression trees (BART) and a reference probability\nsample, and (2) integrate the pseudo-weights within a weighted\npseudo-likelihood approach for Bayesian latent class analysis, while\npropagating pseudo-weight uncertainty into parameter estimation. A stacked\nsample approach is used to allow shared individuals between the NPS and the\nreference sample. We evaluate model performance through simulations and apply\nWOLCAN to data from the Puerto Rico Observational Study of Psychosocial,\nEnvironmental, and Chronic Disease Trends (PROSPECT). We identify dietary\nbehavior patterns for adults in Puerto Rico aged 30 to 75 and examine their\nassociations with type 2 diabetes, hypertension, and hypercholesterolemia. Our\nfindings suggest that an out-of-home eating pattern is associated with a higher\nlikelihood of these cardiometabolic outcomes compared to a nutrition-sensitive\npattern. WOLCAN effectively reveals generalizable dietary behavior patterns and\ndemonstrates relevant applications in studying diet-disease relationships.",
        "Denote the Tutte polynomial of a graph $G$ and a matroid $M$ by $T_G(x,y)$\nand $T_M(x,y)$ respectively. $T_G(x,1)$ and $T_G(1,y)$ were generalized to\nhypergraphs and further extended to integer polymatroids by K\\'{a}lm\\'{a}n\n\\cite{Kalman} in 2013, called interior and exterior polynomials respectively.\nLet $G$ be a $(k+1)$-edge connected graph of order $n$ and size $m$, and let\n$g=m-n+1$. Guan et al. (2023) \\cite{Guan} obtained the coefficients of\n$T_G(1,y)$: \\[[y^j]T_G(1,y)=\\binom{m-j-1}{n-2} \\text{ for } g-k\\leq j\\leq g,\\]\nwhich was deduced from coefficients of the exterior polynomial of polymatroids.\nRecently, Chen and Guo (2025) \\cite{Chen} further obtained\n\\[[y^j]T_G(1,y)=\\binom{m-j-1}{n-2}-\\sum_{i=k+1}^{g-j}\\binom{m-j-i-1}{n-2}|\\mathcal{EC}_i(G)|\\]\nfor $g-3(k+1)\/2< j\\leq g$, where $\\mathcal{EC}_i(G)$ denotes the set of all\nminimal edge cuts with $i$ edges. In this paper, for any matroid $M=(X,rk)$ we\nfirst obtain\n\\[[y^j]T_M(1,y)=\\sum_{t=j}^{|X|-r}(-1)^{t-j}\\binom{t}{j}\\sigma_{r+t}(M),\\]\nwhere $\\sigma_{r+t}(M)$ denotes the number of spanning sets with $r+t$ elements\nin $M$ and $r=rk(M)$. Moveover, the expression of $[x^i]T_M(x,1)$ is obtained\nimmediately from the duality of the Tutte polynomial. As applications of our\nresults, we generalize the two aforementioned results on graphs to the setting\nof matroids. This not only resolves two open problems posed by Chen and Guo in\n\\cite{Chen} but also provides a purely combinatorial proof that is\nsignificantly simpler than their original proofs.",
        "Synthetic aperture radar (SAR) imaging, celebrated for its high resolution,\nall-weather capability, and day-night operability, is indispensable for\nmaritime applications. However, ship detection in SAR imagery faces significant\nchallenges, including complex backgrounds, densely arranged targets, and large\nscale variations. To address these issues, we propose a novel framework,\nCenter-Aware SAR Ship Detector (CASS-Det), designed for robust multi-scale and\ndensely packed ship detection. CASS-Det integrates three key innovations: (1) a\ncenter enhancement module (CEM) that employs rotational convolution to\nemphasize ship centers, improving localization while suppressing background\ninterference; (2) a neighbor attention module (NAM) that leverages cross-layer\ndependencies to refine ship boundaries in densely populated scenes; and (3) a\ncross-connected feature pyramid network (CC-FPN) that enhances multi-scale\nfeature fusion by integrating shallow and deep features. Extensive experiments\non the SSDD, HRSID, and LS-SSDD-v1.0 datasets demonstrate the state-of-the-art\nperformance of CASS-Det, excelling at detecting multi-scale and densely\narranged ships.",
        "A previously established frequency distribution model combining a log-normal\ndistribution with a logarithmic equation describes fluctuations in the email\nsize during send requests. Although the frequency distribution fit was\nconsidered satisfactory, the underlying mechanism driving this distribution\nremains inadequately explained. To address this gap, this study introduced a\nnovel email-send model that characterizes the sending process as an exponential\nfunction modulated by noise from a normal distribution. This model is\nconsistent with both the observed frequency distribution and the previously\nproposed frequency distribution model.",
        "In this paper, we investigate the effect of data heterogeneity across clients\non the performance of distributed learning systems, i.e., one-round Federated\nLearning, as measured by the associated generalization error. Specifically,\n\\(K\\) clients have each \\(n\\) training samples generated independently\naccording to a possibly different data distribution and their individually\nchosen models are aggregated by a central server. We study the effect of the\ndiscrepancy between the clients' data distributions on the generalization error\nof the aggregated model. First, we establish in-expectation and tail upper\nbounds on the generalization error in terms of the distributions. In part, the\nbounds extend the popular Conditional Mutual Information (CMI) bound which was\ndeveloped for the centralized learning setting, i.e., \\(K=1\\), to the\ndistributed learning setting with arbitrary number of clients $K \\geq 1$. Then,\nwe use a connection with information theoretic rate-distortion theory to derive\npossibly tighter \\textit{lossy} versions of these bounds. Next, we apply our\nlossy bounds to study the effect of data heterogeneity across clients on the\ngeneralization error for distributed classification problem in which each\nclient uses Support Vector Machines (D-SVM). In this case, we establish\nexplicit generalization error bounds which depend explicitly on the data\nheterogeneity degree. It is shown that the bound gets smaller as the degree of\ndata heterogeneity across clients gets higher, thereby suggesting that D-SVM\ngeneralizes better when the dissimilarity between the clients' training samples\nis bigger. This finding, which goes beyond D-SVM, is validated experimentally\nthrough a number of experiments.",
        "Training large language models (LLMs) for different inference constraints is\ncomputationally expensive, limiting control over efficiency-accuracy\ntrade-offs. Moreover, once trained, these models typically process tokens\nuniformly, regardless of their complexity, leading to static and inflexible\nbehavior. In this paper, we introduce a post-training optimization framework,\nDynaMoE, that adapts a pre-trained dense LLM to a token-difficulty-driven\nMixture-of-Experts model with minimal fine-tuning cost. This adaptation makes\nthe model dynamic, with sensitivity control to customize the balance between\nefficiency and accuracy. DynaMoE features a token-difficulty-aware router that\npredicts the difficulty of tokens and directs them to the appropriate\nsub-networks or experts, enabling larger experts to handle more complex tokens\nand smaller experts to process simpler ones. Our experiments demonstrate that\nDynaMoE can generate a range of adaptive model variants of the existing trained\nLLM with a single fine-tuning step, utilizing only $10B$ tokens, a minimal cost\ncompared to the base model's training. Each variant offers distinct trade-offs\nbetween accuracy and performance. Compared to the baseline post-training\noptimization framework, Flextron, our method achieves similar aggregated\naccuracy across downstream tasks, despite using only $\\frac{1}{9}\\text{th}$ of\ntheir fine-tuning cost.",
        "Differentiable environments have heralded new possibilities for learning\ncontrol policies by offering rich differentiable information that facilitates\ngradient-based methods. In comparison to prevailing model-free reinforcement\nlearning approaches, model-based reinforcement learning (MBRL) methods exhibit\nthe potential to effectively harness the power of differentiable information\nfor recovering the underlying physical dynamics. However, this presents two\nprimary challenges: effectively utilizing differentiable information to 1)\nconstruct models with more accurate dynamic prediction and 2) enhance the\nstability of policy training. In this paper, we propose a Differentiable\nInformation Enhanced MBRL method, MB-MIX, to address both challenges. Firstly,\nwe adopt a Sobolev model training approach that penalizes incorrect model\ngradient outputs, enhancing prediction accuracy and yielding more precise\nmodels that faithfully capture system dynamics. Secondly, we introduce mixing\nlengths of truncated learning windows to reduce the variance in policy gradient\nestimation, resulting in improved stability during policy learning. To validate\nthe effectiveness of our approach in differentiable environments, we provide\ntheoretical analysis and empirical results. Notably, our approach outperforms\nprevious model-based and model-free methods, in multiple challenging tasks\ninvolving controllable rigid robots such as humanoid robots' motion control and\ndeformable object manipulation.",
        "This study presents a novel framework for precise force control of\nfin-actuated underwater robots by integrating a deep neural network (DNN)-based\nsurrogate model with reinforcement learning (RL). To address the complex\ninteractions with the underwater environment and the high experimental costs, a\nDNN surrogate model acts as a simulator for enabling efficient training for the\nRL agent. Additionally, grid-switching control is applied to select optimized\nmodels for specific force reference ranges, improving control accuracy and\nstability. Experimental results show that the RL agent, trained in the\nsurrogate simulation, generates complex thrust motions and achieves precise\ncontrol of a real soft fin actuator. This approach provides an efficient\ncontrol solution for fin-actuated robots in challenging underwater\nenvironments.",
        "To realize practical quantum computers, a large number of quantum bits\n(qubits) will be required. Semiconductor spin qubits offer advantages such as\nhigh scalability and compatibility with existing semiconductor technologies.\nHowever, as the number of qubits increases, manual qubit tuning becomes\ninfeasible, motivating automated tuning approaches. In this study, we use\nU-Net, a neural network method for object detection, to identify charge\ntransition lines in experimental charge stability diagrams. The extracted\ncharge transition lines are analyzed using the Hough transform to determine\ntheir positions and angles. Based on this analysis, we obtain the\ntransformation matrix to virtual gates. Furthermore, we identify the\nsingle-electron regime by clustering the Hough transform outputs. We also show\nthe single-electron regime within the virtual gate space. These sequential\nprocesses are performed automatically. This approach will advance automated\ncontrol technologies for large-scale quantum devices.",
        "Efficient use of cultivated areas is a necessary factor for sustainable\ndevelopment of agriculture and ensuring food security. Along with the rapid\ndevelopment of satellite technologies in developed countries, new methods are\nbeing searched for accurate and operational identification of cultivated areas.\nIn this context, identification of cropland boundaries based on spectral\nanalysis of data obtained from satellite images is considered one of the most\noptimal and accurate methods in modern agriculture. This article proposes a new\napproach to determine the suitability and green index of cultivated areas using\nsatellite data obtained through the \"Google Earth Engine\" (GEE) platform. In\nthis approach, two powerful algorithms, \"SNIC (Simple Non-Iterative Clustering)\nSuper Pixels\" and \"Canny Edge Detection Method\", are combined. The SNIC\nalgorithm combines pixels in a satellite image into larger regions (super\npixels) with similar characteristics, thereby providing better image analysis.\nThe Canny Edge Detection Method detects sharp changes (edges) in the image to\ndetermine the precise boundaries of agricultural fields. This study, carried\nout using high-resolution multispectral data from the Sentinel-2 satellite and\nthe Google Earth Engine JavaScript API, has shown that the proposed method is\neffective in accurately and reliably classifying randomly selected agricultural\nfields. The combined use of these two tools allows for more accurate\ndetermination of the boundaries of agricultural fields by minimizing the\neffects of outliers in satellite images. As a result, more accurate and\nreliable maps can be created for agricultural monitoring and resource\nmanagement over large areas based on the obtained data. By expanding the\napplication capabilities of cloud-based platforms and artificial intelligence\nmethods in the agricultural field.",
        "The field of rigid origami concerns the folding of stiff, inelastic plates of\nmaterial along crease lines that act like hinges and form a straight-line\nplanar graph, called the crease pattern of the origami. Crease pattern vertices\nin the interior of the folded material and that are adjacent to four crease\nlines, i.e. degree-4 vertices, have a single degree of freedom and can be\nchained together to make flexible polyhedral surfaces. Degree-4 vertices that\ncan fold to a completely flat state have folding kinematics that are very\nwell-understood, and thus they have been used in many engineering and physics\napplications. However, degree-4 vertices that are not flat-foldable or not\nfolded from flat paper so that the vertex forms either an elliptic or\nhyperbolic cone, have folding angles at the creases that follow more\ncomplicated kinematic equations. In this work we present a new duality between\ngeneral degree-4 rigid origami vertices, where dual vertices come in\nelliptic-hyperbolic pairs that have essentially equivalent kinematics. This\nreveals a mathematical structure in the space of degree-4 rigid origami\nvertices that can be leveraged in applications, for example in the construction\nof flexible 3D structures that possess metamaterial properties.",
        "This dissertation addresses a topic that I have worked on over the past\ndecade: the automation of next-to-leading order electroweak corrections in the\nStandard Model of particle physics. After introducing the basic concepts and\ntechniques of next-to-leading order QCD calculations that underpin the\nMadGraph5_aMC@NLO framework, I present a few key features relevant to the\nautomated next-to-leading order electroweak contributions to short-distance\ncross sections, with an emphasis on the mixed QCD and electroweak coupling\nexpansions. These include the FKS subtraction, the renormalization and\nelectroweak input parameter schemes, and the complex mass scheme for dealing\nwith unstable particles. Issues related to the initial or final photons and\nleptons are also discussed. Two remaining challenges are highlighted if one\nwishes to go beyond next-to-leading order computations. Some phenomenological\napplications at the LHC are given to demonstrate the relevance of electroweak\ncorrections at colliders. Finally, an outlook on future studies concludes the\ndissertation.",
        "Electron-phonon coupling is central to many condensed matter phenomena.\nHarnessing these effects for novel material functionality in materials always\ninvolves non-equilibrium electronic states, which in turn alter\nquasi-free-carrier density and screening. Thus, gaining a fundamental\nunderstanding of the interplay of carrier screening and electron-phonon\ncoupling is essential for advancing ultrafast science. Prior works have mainly\nfocused on the impact of carrier screening on electronic structure properties.\nHere we investigate the non-equilibrium lattice dynamics of MoS2 after a\nphotoinduced Mott transition. The experimental data are closely reproduced by\nab-initio ultrafast dynamics simulations. We find that the non-thermal diffuse\nscattering signals in the vicinity of the Bragg peaks, originating from\nlong-wavelength phonon emission, can only be reproduced upon explicitly\naccounting for the screening of electron-phonon interaction introduced by the\nMott transition. These results indicate the screening influences\nelectron-phonon coupling, leading to a suppression of intravalley\nphonon-assisted carrier relaxation. Overall, the combined experimental and\ncomputational approach introduced here offers new prospects for exploring the\ninfluence of screening of the electron-phonon interactions and relaxation\npathways in driven solids.",
        "A core principle of Privacy by Design (PbD) is minimizing the data that is\nstored or shared about each individual respondent. PbD principles are mandated\nby the GDPR (see Article 5c and Article 25), as well as informing aspects of\nCalifornia Privacy Rights Act (CPRA). This paper describes a simple and\neffective approach that can be used in many a\/b testing and similar contexts to\nhelp meet these PbD goals. Specifically, the method presented describes an\napproach to run OLS regression on k-anonymized data. To help illustrate the\ngeneral utility of this approach, descriptions of two important use cases are\noffered: 1) calculating partial f-tests as a simple way to both check for a\/b\ntest interactions and to test for heterogeneity of treatment effects; and 2)\nregression adjustment using an approach similar to the popular CUPED method, as\na variance reduction method for a\/b tests. Using this method has advantages for\nprivacy and compliance, as well as often reducing data storage and processing\ncosts, by storing, sharing, or analyzing only aggregate level rather than\nindividual level data.",
        "While 3D instance segmentation has made significant progress, current methods\nstruggle to address realistic scenarios where new categories emerge over time\nwith natural class imbalance. This limitation stems from existing datasets,\nwhich typically feature few well-balanced classes. Although few datasets\ninclude unbalanced class annotations, they lack the diverse incremental\nscenarios necessary for evaluating methods under incremental settings.\nAddressing these challenges requires frameworks that handle both incremental\nlearning and class imbalance. However, existing methods for 3D incremental\nsegmentation rely heavily on large exemplar replay, focusing only on\nincremental learning while neglecting class imbalance. Moreover,\nfrequency-based tuning for balanced learning is impractical in these setups due\nto the lack of prior class statistics. To overcome these limitations, we\npropose a framework to tackle both \\textbf{C}ontinual \\textbf{L}earning and\nclass \\textbf{Imb}alance for \\textbf{3D} instance segmentation\n(\\textbf{CLIMB-3D}). Our proposed approach combines Exemplar Replay (ER),\nKnowledge Distillation (KD), and a novel Imbalance Correction (IC) module.\nUnlike prior methods, our framework minimizes ER usage, with KD preventing\nforgetting and supporting the IC module in compiling past class statistics to\nbalance learning of rare classes during incremental updates. To evaluate our\nframework, we design three incremental scenarios based on class frequency,\nsemantic similarity, and random grouping that aim to mirror real-world dynamics\nin 3D environments. Experimental results show that our proposed framework\nachieves state-of-the-art performance, with an increase of up to 16.76\\% in mAP\ncompared to the baseline. Code will be available at:\n\\href{https:\/\/github.com\/vgthengane\/CLIMB3D}{https:\/\/github.com\/vgthengane\/CLIMB3D}",
        "Gaussian SLAM systems excel in real-time rendering and fine-grained\nreconstruction compared to NeRF-based systems. However, their reliance on\nextensive keyframes is impractical for deployment in real-world robotic\nsystems, which typically operate under sparse-view conditions that can result\nin substantial holes in the map. To address these challenges, we introduce\nDenseSplat, the first SLAM system that effectively combines the advantages of\nNeRF and 3DGS. DenseSplat utilizes sparse keyframes and NeRF priors for\ninitializing primitives that densely populate maps and seamlessly fill gaps. It\nalso implements geometry-aware primitive sampling and pruning strategies to\nmanage granularity and enhance rendering efficiency. Moreover, DenseSplat\nintegrates loop closure and bundle adjustment, significantly enhancing\nframe-to-frame tracking accuracy. Extensive experiments on multiple large-scale\ndatasets demonstrate that DenseSplat achieves superior performance in tracking\nand mapping compared to current state-of-the-art methods.",
        "The optimal transport barycenter (a.k.a. Wasserstein barycenter) is a\nfundamental notion of averaging that extends from the Euclidean space to the\nWasserstein space of probability distributions. Computation of the\nunregularized barycenter for discretized probability distributions on point\nclouds is a challenging task when the domain dimension $d > 1$. Most practical\nalgorithms for approximating the barycenter problem are based on entropic\nregularization. In this paper, we introduce a nearly linear time $O(m \\log{m})$\nand linear space complexity $O(m)$ primal-dual algorithm, the\nWasserstein-Descent $\\dot{\\mathbb{H}}^1$-Ascent (WDHA) algorithm, for computing\nthe exact barycenter when the input probability density functions are\ndiscretized on an $m$-point grid. The key success of the WDHA algorithm hinges\non alternating between two different yet closely related Wasserstein and\nSobolev optimization geometries for the primal barycenter and dual Kantorovich\npotential subproblems. Under reasonable assumptions, we establish the\nconvergence rate and iteration complexity of WDHA to its stationary point when\nthe step size is appropriately chosen. Superior computational efficacy,\nscalability, and accuracy over the existing Sinkhorn-type algorithms are\ndemonstrated on high-resolution (e.g., $1024 \\times 1024$ images) 2D synthetic\nand real data.",
        "Urban digital twins are virtual replicas of cities that use multi-source data\nand data analytics to optimize urban planning, infrastructure management, and\ndecision-making. Towards this, we propose a framework focused on the\nsingle-building scale. By connecting to cloud mapping platforms such as Google\nMap Platforms APIs, by leveraging state-of-the-art multi-agent Large Language\nModels data analysis using ChatGPT(4o) and Deepseek-V3\/R1, and by using our\nGaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings\nframework can retrieve a building's 3D model, visual descriptions, and achieve\ncloud-based mapping integration with large language model-based data analytics\nusing a building's address, postal code, or geographic coordinates.",
        "This article presents a new particle beam cooling scheme, namely cyclotron\nmaser cooling (CMC). Relativistic gyrating particles, forced by a solenoidal\nmagnetic field over some length of their trajectory, move in a helical path and\nundergo emission or absorption of radiations stimulated by a resonance RF\nfield. Theoretical and experimental investigations on electron beams indicate\nthat when the action of the RF field exceeds a critical value the beam jumps\nabruptly to a coherent radiative system undergoing CMC in which most electrons\nare accumulated to a discrete energy with the same gyration phase. The\nmechanism of CMC was proved to be an elementary cooling process that is common\nto dissipative systems consisting of a driving field and oscillators with a\nstable energy. It leads to the generation of a coherent beam of particles that\nprovides means to control miscellaneous particle induced reactions. For\nexample, CMC electrons would generate coherent X-ray and gamma-ray photons\nthrough coherent inverse Compton scattering of laser radiation.",
        "We perform high-order simulations of two-phase flows in capillaries, with and\nwithout evaporation. Since a sharp-interface model is used, singularities can\narise at the three-phase contact line, where the fluid-fluid interface\ninteracts with the capillary wall. These singularities are especially\nchallenging when a highly accurate, high-order method with very little\nnumerical diffusion is used for the flow solver. In this work, we employ the\neXtended Discontinuous Galerkin (XDG) method, which has a very high accuracy\nbut a severe limit regarding e.g., the time-step restriction.\n  To address this challenge and enhance the stability of our numerical method\nwe introduce a novel approach for representing a moving interface in the case\nof two-phase flows. We propose a global analytical representation of the\ninterface-describing level-set field, defined by a small set of time-dependent\nparameters. Noteworthy for its simplicity and efficiency, this method\neffectively addresses the inherent complexity of two-phase flow problems.\nFurthermore, it significantly improves numerical stability and enables the use\nof larger time steps, ensuring both reliability and computational efficiency in\nour simulations.\n  We compare different analytic expressions for level-set representation,\nincluding the elliptic function and the fourth-order polynomial, and validate\nthe method against established literature data for capillary rise, both with\nand without evaporation. These results highlight the effectiveness of our\napproach in resolving complex interfacial dynamics.",
        "Whole slide image (WSI) registration is an essential task for analysing the\ntumour microenvironment (TME) in histopathology. It involves the alignment of\nspatial information between WSIs of the same section or serial sections of a\ntissue sample. The tissue sections are usually stained with single or multiple\nbiomarkers before imaging, and the goal is to identify neighbouring nuclei\nalong the Z-axis for creating a 3D image or identifying subclasses of cells in\nthe TME. This task is considerably more challenging compared to radiology image\nregistration, such as magnetic resonance imaging or computed tomography, due to\nvarious factors. These include gigapixel size of images, variations in\nappearance between differently stained tissues, changes in structure and\nmorphology between non-consecutive sections, and the presence of artefacts,\ntears, and deformations. Currently, there is a noticeable gap in the literature\nregarding a review of the current approaches and their limitations, as well as\nthe challenges and opportunities they present. We aim to provide a\ncomprehensive understanding of the available approaches and their application\nfor various purposes. Furthermore, we investigate current deep learning methods\nused for WSI registration, emphasising their diverse methodologies. We examine\nthe available datasets and explore tools and software employed in the field.\nFinally, we identify open challenges and potential future trends in this area\nof research.",
        "The hadronic $\\Sigma_c^{(*)}\\bar{D}^{(*)}$ and $\\Sigma_c^{(*)}{D}^{(*)}$\ninteractions are revisited, with a focus on their short-range parts, motivated\nby a tension between the interpretations of $P_{c\\bar{c}}(4312)$,\n$P_{c\\bar{c}}(4440)$, and $P_{c\\bar{c}}(4457)$ in effective field theory (EFT)\nframeworks and the one-boson-exchange (OBE) model. While the three states can\nbe interpreted as $\\Sigma_c\\bar{D}^{(*)}$ molecular states within EFT\nframeworks, this is not feasible in the single-channel OBE model with\nconsistent cutoff. In this work, the possibility to reconcile OBE model with\nEFTs by refitting the $\\rho$-, $\\omega$- and $\\sigma$-exchange interaction is\nexplored and ruled out. It is pointed out that the problem in OBE arises from\nthe strong short-range spin-dependent one-pion-exchange (OPE) interaction and\nthe fixed signs of other short-range interactions in OBE model also prevent the\ncancellation. To address this issue, the short-range subtraction strategies\nwithin the OBE model are revisited. Two subtraction schemes are explored:\nremoving the delta-function from all interactions and eliminating it only from\nthe pseudoscalar-meson-exchange component. These schemes favor different spin\nassignments for $P_{c\\bar{c}}(4440)$ and $P_{c\\bar{c}}(4457)$. Though solving\nthe problem, there is no clear dynamical picture to support the subtraction\nschemes. We propose a new quark-exchange mechanism motivated by the Pauli\nprinciple. Different from the two subtraction schemes in OBE, the\nquark-exchange mechanism offers an explanation grounded in microscopic\ndynamics. It is shown that the spin-dependent quark-exchange interaction\ncancels those from OPE. The differences in the predictions for the spin,\nisospin, and open-charm partner states of the experimental $P_{c\\bar{c}}$\nstates offer a way to distinguish between the subtracted OBE model and the OBE\nmodel with quark-exchange contributions.",
        "Let K[X_n]=K[x_1,\\ldots,x_n] be the polynomial algebra in n variables over a\nfield K of characteristic zero. A locally nilpotent linear derivation \\delta of\nK[X_n] is called Weitzenb\\\"ock due to his well known result from 1932 stating\nthat the algebra \\text{\\rm ker}(\\delta)=K[X_n]^{\\delta} of constants of\n$\\delta$ is finitely generated. The explicit form of a generating set of\n$K[X_n,Y_n]^{\\delta}$ was conjectured by Nowicki in 1994 in the case \\delta was\nsuch that \\delta(y_{i})=x_{i}$, $\\delta(x_i)=0, i=1,\\ldots,n. Nowicki's\nconjecture turned out to be true and, recently, has been applied to several\nrelatively free associative algebras. In this paper, we consider the free Lie\nalgebra \\mathcal{L}(x,y) of rank 2 generated by x and y over K and we assume\nthe Weitzenb\\\"ock derivation \\delta sending y to x, and x to zero. We introduce\nthe idea of pseudodeterminants and we present a characterization of Hall\nmonomials that are constants showing they are not so far from being\npseudodeterminants. We also give a complete list of generators of the constants\nof degree less than 7 which are, of course, pseudodeterminants."
      ]
    }
  },
  {
    "id":2411.01291,
    "research_type":"applied",
    "start_id":"b19",
    "start_title":"vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems",
    "start_abstract":"Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI"
      ],
      "abstract":[
        "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "$\\tt GrayHawk$: A public code for calculating the Gray Body Factors of\n  massless fields around spherically symmetric Black Holes",
        "MinGRU-Based Encoder for Turbo Autoencoder Frameworks",
        "Type semigroups for twisted groupoids and a dichotomy for groupoid\n  C*-algebras",
        "Algebraic surfaces as Hadamard products of curves",
        "$C^{1}$-Stable-Manifolds for Periodic Heteroclinic Chains in Bianchi IX:\n  Symbolic Computations and Statistical Properties",
        "Slow Light Waveguides based on Bound States in the Continuum",
        "High-Accuracy Physical Property Prediction for Organics via Molecular\n  Representation Learning: Bridging Data to Discovery",
        "Identical Suppression of Spin and Charge Density Wave Transitions in\n  La$_4$Ni$_3$O$_{10}$ by Pressure",
        "A Relaxed Wasserstein Distance Formulation for Mixtures of Radially\n  Contoured Distributions",
        "Weight Distribution of the Weighted Coordinates Poset Block Space and\n  Singleton Bound",
        "Cold dark gas in Cygnus X: The first large-scale mapping of\n  low-frequency carbon recombination lines",
        "Properties of Turnpike Functions for Discounted Finite MDPs",
        "Raman Forbidden Layer-Breathing Modes in Layered Semiconductor Materials\n  Activated by Phonon and Optical Cavity Effects",
        "Anomalous temperature-dependent magnetization in the nearly collinear\n  antiferromagnet Y$_2$Co$_3$",
        "The nebular spectra of SN 2023ixf: A lower mass, partially stripped\n  progenitor may be the result of binary interaction",
        "Segregation in Nuclear Stellar Clusters: Rates and Mass Distributions of\n  TDEs, QPEs, Plunges, and EMRIs",
        "Normal-normal continuous symmetric stress approximation in\n  three-dimensional linear elasticity",
        "KM3NeT Constraint on Lorentz-Violating Superluminal Neutrino Velocity",
        "Cosmic voids and the kinetic analysis. IV. Hubble tension and the\n  cosmological constant",
        "Machine Learns Quantum Complexity?",
        "MCMC for multi-modal distributions",
        "Weighted Heights and GIT Heights",
        "Physics-Aware Decoding for Communication Channels Governed by Partial\n  Differential Equations",
        "Energy-Adaptive Riemannian Conjugate Gradient Method for Density\n  Functional Theory",
        "Unified Micromechanics Theory of Composites",
        "What Bohmian mechanic says about arrival times of 1D vacuum squeezed\n  states",
        "Central galaxy alignments. Dependence on the mass and the large-scale\n  environment",
        "Propagation Times and Energy Losses of Cosmic Protons and Antiprotons in\n  Interplanetary Space",
        "Using Statistical Precision Medicine to Identify Optimal Treatments in a\n  Heart Failure Setting"
      ],
      "abstract":[
        "We introduce and describe $\\tt GrayHawk$, a publicly available\nMathematica-based tool designed for the efficient computation of gray-body\nfactors for spherically symmetric and asymptotically flat black holes. This\nprogram provides users with a rapid and reliable means to compute gray-body\nfactors for massless fields with spin \\(s = 0, 1\/2, 1, 2\\) in modes specified\nby the angular quantum number \\(l\\), given a black hole metric and the\nassociated parameter values. $\\tt GrayHawk$ is preloaded with seven different\nblack hole metrics, offering immediate applicability to a variety of\ntheoretical models. Additionally, its modular structure allows users to extend\nits functionality easily by incorporating alternative metrics or\nconfigurations. This versatility makes $\\tt GrayHawk$ a powerful and adaptable\nresource for researchers studying black hole physics and Hawking radiation. The\ncodes described in this work are publicly available at\nhttps:\/\/github.com\/marcocalza89\/GrayHawk.",
        "Early neural channel coding approaches leveraged dense neural networks with\none-hot encodings to design adaptive encoder-decoder pairs, improving block\nerror rate (BLER) and automating the design process. However, these methods\nstruggled with scalability as the size of message sets and block lengths\nincreased. TurboAE addressed this challenge by focusing on bit-sequence inputs\nrather than symbol-level representations, transforming the scalability issue\nassociated with large message sets into a sequence modeling problem. While\nrecurrent neural networks (RNNs) were a natural fit for sequence processing,\ntheir reliance on sequential computations made them computationally expensive\nand inefficient for long sequences. As a result, TurboAE adopted convolutional\nnetwork blocks, which were faster to train and more scalable, but lacked the\nsequential modeling advantages of RNNs. Recent advances in efficient RNN\narchitectures, such as minGRU and minLSTM, and structured state space models\n(SSMs) like S4 and S6, overcome these limitations by significantly reducing\nmemory and computational overhead. These models enable scalable sequence\nprocessing, making RNNs competitive for long-sequence tasks. In this work, we\nrevisit RNNs for Turbo autoencoders by integrating the lightweight minGRU model\nwith a Mamba block from SSMs into a parallel Turbo autoencoder framework. Our\nresults demonstrate that this hybrid design matches the performance of\nconvolutional network-based Turbo autoencoder approaches for short sequences\nwhile significantly improving scalability and training efficiency for long\nblock lengths. This highlights the potential of efficient RNNs in advancing\nneural channel coding for long-sequence scenarios.",
        "We develop a theory of type semigroups for arbitrary twisted, not necessarily\nHausdorff \\'etale groupoids. The type semigroup is a dynamical version of the\nCuntz semigroup. We relate it to traces, ideals, pure infiniteness, and stable\nfiniteness of the reduced and essential C*-algebras. If the reduced C*-algebra\nof a twisted groupoid is simple and the type semigroup satisfies a weak version\nof almost unperforation, then the C*-algebra is either stably finite or purely\ninfinite. We apply our theory to Cartan inclusions. We calculate the type\nsemigroup for the possibly non-Hausdorff groupoids associated to self-similar\ngroup actions on graphs and deduce a dichotomy for the resulting Exel-Pardo\nalgebras.",
        "We study projective surfaces in $\\mathbb{P}^3$ which can be written as\nHadamard product of two curves. We show that quadratic surfaces which are\nHadamard product of two lines are smooth and tangent to all coordinate planes,\nand such tangency points uniquely identify the quadric. The variety of such\nquadratic surfaces corresponds to the Zariski closure of the space of symmetric\nmatrices whose inverse has null diagonal. For higher-degree surfaces which are\nHadamard product of a line and a curve we show that the intersection with the\ncoordinate planes is always non-transversal.",
        "In this paper we study oscillatory Bianchi models of class A and are able to\nshow that for admissible periodic heteroclinic chains in Bianchi IX there\nexisist $C^{1}$- stable - manifolds of orbits that follow these chains towards\nthe big bang. A detailed study of Takens Linearization Theorem and the\nNon-Resonance-Conditions leads us to this new result in Bianchi class A. More\nprecisely, we can show that there are no heteroclinic chains in Bianchi IX with\nconstant continued fraction development that allow Takens-Linearization at all\nof their base points. Geometrically speaking, this excludes \"symmetric\"\nheteroclinic chains with the same number of \"bounces\" near all of the 3 Taub\nPoints - the result shows that we have to require some \"asymmetry\" in the\nbounces in order to allow for Takens Linearization, e.g. by considering\nadmissible 2-periodic continued fraction developments. We conclude by\ndiscussing the statistical properties of those solutions, including their\ntopological and measure-theoretic genericity.",
        "The concept of bound states in the continuum (BIC) has been advancing light\nconfinement technology in leaky environments. In this letter, we propose and\nnumerically demonstrate a slow light waveguide based on a BIC mode. We\nconsidered a waveguide with a polymer core loaded on a plane slab, which\nsupports a leaky guided mode coupled to the radiation continuum in the slab. We\nfound that periodic modulation of the polymer core along the propagation\ndirection can result in a high group index mode with a low propagation loss due\nto BIC confinement. The introduction of one-dimensional photonic crystals into\nthe BIC waveguides will largely expand its functionality and applications in\nintegrated photonics.",
        "The ongoing energy crisis has underscored the urgent need for\nenergy-efficient materials with high energy utilization efficiency, prompting a\nsurge in research into organic compounds due to their environmental\ncompatibility, cost-effective processing, and versatile modifiability. To\naddress the high experimental costs and time-consuming nature of traditional\ntrial-and-error methods in the discovery of highly functional organic\ncompounds, we apply the 3D transformer-based molecular representation learning\nalgorithm to construct a pre-trained model using 60 million semi-empirically\noptimized structures of small organic molecules, namely, Org-Mol, which is then\nfine-tuned with public experimental data to obtain prediction models for\nvarious physical properties. Despite the pre-training process relying solely on\nsingle molecular coordinates, the fine-tuned models achieves high accuracy\n(with $R^2$ values for the test set exceeding 0.95). These fine-tuned models\nare applied in a high-throughput screening process to identify novel immersion\ncoolants among millions of automatically constructed ester molecules, resulting\nin the experimental validation of two promising candidates. This work not only\ndemonstrates the potential of Org-Mol in predicting bulk properties for organic\ncompounds but also paves the way for the rational and efficient development of\nideal candidates for energy-saving materials.",
        "Understanding the interplay between magnetism and superconductivity in\nnickelate systems is a key focus of condensed matter research. Microscopic\ninsights into magnetism, which emerges near superconductivity, require a\nsynergistic approach that combines complementary techniques with controlled\nparameter tuning. In this paper, we present a systematic investigation of the\nthree-layer Ruddlesden-Popper (RP) nickelate La$_4$Ni$_3$O$_{10}$ using\nmuon-spin rotation\/relaxation ($\\mu$SR), neutron powder diffraction (NPD),\nresistivity, and specific heat measurements. At ambient pressure, two\nincommensurate spin density wave (SDW) transitions were identified at $T_{\\rm\nSDW} \\simeq 132$ K and $T^\\ast \\simeq 90$ K. NPD experiments revealed that the\nmagnetic wave vector $(0, 0.574, 0)$ remains unchanged below 130 K, indicating\nthat the transition at $T^\\ast$ corresponds to a reorientation of the Ni\nmagnetic moments within a similar magnetic structure. Comparison of the\nobserved internal magnetic fields with dipole-field calculations reveals a\nmagnetic structure consistent with an antiferromagnetically coupled SDW on the\nouter two Ni layers, with smaller moments on the inner Ni layer. The internal\nfields at muon stopping sites appeared abruptly at $T_{\\rm SDW}$, suggesting a\nfirst-order-like nature of the SDW transition, which is closely linked to the\ncharge density wave (CDW) order occurring at the same temperature ($T_{\\rm SDW}\n= T_{\\rm CDW}$). Under applied pressure, all transition temperatures, including\n$T_{\\rm SDW}$, $T^\\ast$, and $T_{\\rm CDW}$, were suppressed at a nearly uniform\nrate of $\\simeq -13$ K\/GPa. This behavior contrasts with the double-layer RP\nnickelate La$_3$Ni$_2$O$_7$, where pressure enhances the separation of the\ndensity wave transitions.",
        "Recently, a Wasserstein-type distance for Gaussian mixture models has been\nproposed. However, that framework can only be generalized to identifiable\nmixtures of general elliptically contoured distributions whose components come\nfrom the same family and satisfy marginal consistency. In this paper, we\npropose a simple relaxed Wasserstein distance for identifiable mixtures of\nradially contoured distributions whose components can come from different\nfamilies. We show some properties of this distance and that its definition does\nnot require marginal consistency. We apply this distance in color transfer\ntasks and compare its performance with the Wasserstein-type distance for\nGaussian mixture models in an experiment. The error of our method is more\nstable and the color distribution of our output image is more desirable.",
        "In this paper, we determine the complete weight distribution of the space $\n\\mathbb{F}_q^N $ endowed by the weighted coordinates poset block metric\n($(P,w,\\pi)$-metric), also known as the $(P,w,\\pi)$-space, thereby obtaining it\nfor $(P,w)$-space, $(P,\\pi)$-space, $\\pi$-space, and $P$-space as special\ncases. Further, when $P$ is a chain, the resulting space is called as\nNiederreiter-Rosenbloom-Tsfasman (NRT) weighted block space and when $P$ is\nhierarchical, the resulting space is called as weighted coordinates\nhierarchical poset block space. The complete weight distribution of both the\nspaces are deduced from the main result. Moreover, we define an $I$-ball for an\nideal $I$ in $P$ and study the characteristics of it in $(P,w,\\pi)$-space.\n  We investigate the relationship between the $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Given an ideal $I$, we investigate how the maximum\ndistance separability (MDS) is related with $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Duality theorem is derived for an MDS\n$(P,w,\\pi)$-code when all the blocks are of same length. Finally, the\ndistribution of codewords among $r$-balls is analyzed in the case of chain\nposet, when all the blocks are of same length.",
        "Understanding the transition from atomic gas to molecular gas is critical to\nexplain the formation and evolution of molecular clouds. However, the gas\nphases involved, cold HI and CO-dark molecular gas, are challenging to directly\nobserve and physically characterize. We observed the Cygnus X star-forming\ncomplex in carbon radio recombination lines (CRRLs) at 274--399 MHz with the\nGreen Bank Telescope at 21 pc (48') resolution. Of the 30 deg^2 surveyed, we\ndetect line-synthesized C273alpha emission from 24 deg^2 and produce the first\nlarge-area maps of low-frequency CRRLs. The morphology of the C273alpha\nemission reveals arcs, ridges, and extended possibly sheet-like gas which are\noften on the outskirts of CO emission and likely transitioning from HI-to-H_2.\nThe typical angular separation of C273alpha and 13CO emission is 12 pc, and we\nestimate C273alpha gas densities of n_H ~ 40 - 400 cm^3. The C273alpha line\nprofiles are Gaussian and likely turbulent broadened, spanning a large range of\nFWHM from 2 to 20 km\/s with a median of 10.6 km\/s. Mach numbers fall within\n10--30. The turbulent timescale is relatively short, 2.6 Myr, and we deduce\nthat the turbulent pressure likely dominates the evolution of the C273alpha\ngas. Velocity offsets between C273alpha and 13CO components are apparent\nthroughout the region and have a typical value of 2.9 km\/s. Two regimes have\nemerged from the data: one regime in which C273alpha and 13CO are strongly\nrelated (at N_H ~ 4 x 10^21 cm^-2), and a second, in which C273alpha emits\nindependently of the 13CO intensity. In the former regime, C273alpha may arise\nfrom the the envelopes of massive clouds (filaments), and in the latter,\nC273alpha emits from cold clumps in a more-diffuse mix of HI and H_2 gas.",
        "This paper studies discounted Markov Decision Processes (MDPs) with finite\nsets of states and actions. Value iteration is one of the major methods for\nfinding optimal policies. For each discount factor, starting from a finite\nnumber of iterations, which is called the turnpike integer, value iteration\nalgorithms always generate decision rules, which are deterministic optimal\npolicies for the infinite-horizon problems. This fact justifies the rolling\nhorizon approach for computing infinite-horizon optimal policies by conducting\na finite number of value iterations. This paper describes properties of\nturnpike integers and provides their upper bounds.",
        "We report Raman forbidden layer-breathing modes (LBMs) in layered\nsemiconductor materials (LSMs). The intensity distribution of all observed LBMs\ndepends on layer number, incident light wavelength and refractive index\nmismatch between LSM and underlying substrate. These results are understood by\na Raman scattering theory via the proposed spatial interference model, where\nthe naturally occurring optical and phonon cavities in LSMs enable spatially\ncoherent photon-phonon coupling mediated by the corresponding one-dimensional\nperiodic electronic states. Our work reveals the spatial coherence of photon\nand phonon fields on the phonon excitation via photon\/phonon cavity\nengineering.",
        "Y$_2$Co$_3$ is a newly discovered antiferromagnetic (AFM) compound with\ndistorted kagome layers. Previous investigations via bulk magnetization\nmeasurements suggested a complex noncollinear magnetic behavior, with magnetic\nmoments primarily anti-aligned along the $b$ axis and some canting towards the\n$ac$ plane. In this study, we report the magnetic structure of Y$_2$Co$_3$ to\nbe an A-type AFM structure with ferromagnetic (FM) interactions within the\ndistorted kagome plane and an interplane antiferromagnetic interaction, as\ndetermined by single-crystal neutron diffraction. The magnetic moments align\nalong the $b$ axis, with minimal canting towards the $c$ axis, at odds with the\nprevious interpretation of bulk magnetization measurements. The magnetic\nmoments on the two distinct Co sites are [0, -0.68(3), 0] $\\mu_B$ and [0,\n1.25(4), 0.07(1)] $\\mu_B$. We attribute the previously reported \"noncollinear\"\nbehavior to the considerable temperature dependence of itinerant AFM exchange\ninteractions, induced by thermal contraction along the $b$ axis. Additionally,\nour examination of lattice constants through pressure studies reveals\ncompensating effects on FM and AFM interactions, resulting in negligible\npressure dependence of $T_\\textrm{N}$.",
        "SN 2023ixf is one of the brightest Core Collapse Supernovae of the 21st\ncentury and offers a rare opportunity to investigate the late stage of a\nSupernova through nebular phase spectroscopy. We present four nebular phase\nspectra from day +291 to +413 after explosion. This is supplemented with high\ncadence early phase spectroscopic observations and photometry covering the\nfirst 500 days to investigate explosion parameters. The narrow and blue-shifted\nnebular Oxygen emission lines are used to infer an ejected Oxygen mass of\n$<0.65M_\\odot$, consistent with models of a relatively low mass\n($M_{ZAMS}<15M_\\odot$) progenitor. An energy of 0.3 to $1.4 \\times10^{51}$ erg\nand a light curve powered by an initial $^{56}$Ni mass of $0.049 \\pm 0.005\nM_\\odot$ appear consistent with a relatively standard Type II explosion, while\nan incomplete $\\gamma$-ray trapping (with timescale of $240\\pm4$ days) suggests\na lower ejecta mass. Assuming a typical explosion, the broad Hydrogen and\nCalcium profiles suggest a common origin within a lower mass, partially\nstripped envelope. Hydrogen emission broadens with time, indicating\ncontribution from an additional power source at an extended distance; while the\nemergence of high velocity ($\\sim$6,000 km s$^{-1}$) Hydrogen emission features\n(beginning around day +200) may be explained by Shock Interaction with a dense\nHydrogen-rich region located at $\\sim1.5 \\times 10^{16}$cm. Such envelope mass\nloss for a low mass progenitor may be explained through theoretical models of\nBinary interaction.",
        "Supermassive black holes at the centers of galaxies occasionally disrupt\nstars or consume stellar-mass black holes that wander too close, producing\nobservable electromagnetic or gravitational wave signals. We examine how mass\nsegregation impacts the rates and distributions of such events. Assuming a\nrelaxed stellar cluster, composed of stars and stellar-mass black holes, we\nshow that the tidal disruption rate of massive stars ($m\\gtrsim M_\\odot$) is\nenhanced relative to their abundance in the stellar population. For stars up to\n$m\\approx3M_\\odot$, this enhancement is roughly $m\/M_\\odot$ and it is driven by\nsegregation within the sphere of influence. Stars with masses\n$m\\gtrsim3M_\\odot$, if relaxed, are predominantly scattered by more massive\nstellar-mass black holes, leading to a constant enhancement factor of $\\approx\n10$, independent of mass. This aligns with observational evidence suggesting an\nover-representation of massive stars in tidal disruption events. For\nstellar-mass black holes, we predict an enhancement factor scaling as\n$m_\\bullet^{1\/2}$ for plunges and $m_\\bullet^{3\/2}$ for extreme-mass-ratio\ninspirals (EMRIs). The power of one-half in both cases reflects the shorter\nrelaxation times of heavier black holes, allowing them to segregate into the\nsphere of influence from greater distances, thereby increasing their abundance.\nThe additional power in the EMRIs' rate arises from the tendency of heavier\nblack holes to circularize and sink inward more efficiently. Finally, we\nestimate the rate of main sequence star inspirals and find that it favors\nlow-mass stars ($m\\lesssim M_\\odot$). This seems compatible with the\nobservationally estimated rate of quasi-periodic eruptions.",
        "We present a conforming setting for a mixed formulation of linear elasticity\nwith symmetric stress that has normal-normal continuous components across faces\nof tetrahedral meshes. We provide a stress element for this formulation with 30\ndegrees of freedom that correspond to standard boundary conditions. The\nresulting scheme converges quasi-optimally and is locking free. Numerical\nexperiments illustrate the performance.",
        "Lorentz invariance is a fundamental symmetry of spacetime and foundational to\nmodern physics. One of its most important consequences is the constancy of the\nspeed of light. This invariance, together with the geometry of spacetime,\nimplies that no particle can move faster than the speed of light. In this\narticle, we present the most stringent neutrino-based test of this prediction,\nusing the highest energy neutrino ever detected to date, KM3-230213A. The\narrival of this event, with an energy of $220^{+570}_{-110}\\,\\text{PeV}$, sets\na constraint on $\\delta \\equiv c_\\nu^2-1 < 4\\times10^{-22}$.",
        "The formation of the cosmic structures in the late Universe is considered\nusing Vlasov kinetic approach. The crucial point is the use of the\ngravitational potential with repulsive term of the cosmological constant which\nprovides a solution to the Hubble tension, that is the Hubble parameter for the\nlate Universe has to differ from its global cosmological value. This also\nprovides a mechanism of formation of stationary semi-periodic gravitating\nstructures of voids and walls, so that the cosmological constant has a role of\nthe scaling and hence can be compared with the observational data for given\nregions. The considered mechanism of the structure formation in late\ncosmological epoch then is succeeding the epoch described by the evolution of\nprimordial density fluctuations.",
        "We study how a machine based on deep learning algorithms learns Krylov spread\ncomplexity in quantum systems with N x N random Hamiltonians drawn from the\nGaussian unitary ensemble. Using thermofield double states as initial\nconditions, we demonstrate that a convolutional neural network-based algorithm\nsuccessfully learns the Krylov spread complexity across all timescales,\nincluding the late-time plateaus where states appear nearly featureless and\nrandom. Performance strongly depends on the basis choice, performing well with\nthe energy eigenbasis or the Krylov basis but failing in the original basis of\nthe random Hamiltonian. The algorithm also effectively distinguishes\ntemperature-dependent features of thermofield double states. Furthermore, we\nshow that the system time variable of state predicted by deep learning is an\nirrelevant quantity, reinforcing that the Krylov spread complexity well\ncaptures the essential features of the quantum state, even at late times.",
        "We explain the fundamental challenges of sampling from multimodal\ndistributions, particularly for high-dimensional problems. We present the major\ntypes of MCMC algorithms that are designed for this purpose, including parallel\ntempering, mode jumping and Wang-Landau, as well as several state-of-the-art\napproaches that have recently been proposed. We demonstrate these methods using\nboth synthetic and real-world examples of multimodal distributions with\ndiscrete or continuous state spaces.",
        "This paper examines the relationship between GIT heights and weighted\nheights, exploring their definitions and applications to weighted projective\nspaces and binary forms. Drawing on prior weighted height frameworks, we relate\nthem to Zhang's GIT height via the Veronese map, showing that for a semistable\ncycle Z in a weighted projective space over the algebraic closure of Q, the GIT\nheight h(Z) equals L(Z) plus an Archimedean Chow metric term. For binary forms\nf in V_d, we define an invariant height H(f) with respect to the Chow metric\nand establish that the moduli weighted height L(xi(f)) of f's invariants equals\nH(f) plus the field degree times the Chow height h_Ch(f), linking arithmetic\nand moduli properties.",
        "Digital communication systems inherently operate through physical media\ngoverned by partial differential equations (PDEs). In this paper, we introduce\na physics-aware decoding framework that integrates gradient descent-based error\ncorrecting algorithms with PDE-based channel modeling using differentiable PDE\nsolvers. At the core of our approach is gradient flow decoding, which harnesses\ngradient information directly from the PDE solver to guide the decoding\nprocess. We validate our method through numerical experiments on both the heat\nequation and the nonlinear Schr\\\"odinger equation (NLSE), demonstrating\nsignificant improvements in decoding performance. The implications of this work\nextend beyond decoding applications, establishing a new paradigm for\nphysics-aware signal processing that shows promise for various signal detection\nand signal recovery tasks.",
        "This paper presents a novel Riemannian conjugate gradient method for the\nKohn-Sham energy minimization problem in density functional theory (DFT), with\na focus on non-metallic crystal systems. We introduce an energy-adaptive metric\nthat preconditions the Kohn-Sham model, significantly enhancing optimization\nefficiency. Additionally, a carefully designed shift strategy and several\nalgorithmic improvements make the implementation comparable in performance to\nhighly optimized self-consistent field iterations. The energy-adaptive\nRiemannian conjugate gradient method has a sound mathematical foundation,\nincluding stability and convergence, offering a reliable and efficient\nalternative for DFT-based electronic structure calculations in computational\nchemistry.",
        "We consider the matrix composite materials (CM) of either random\n(statistically homogeneous or inhomogeneous), periodic, or deterministic\n(neither random nor periodic) structures. CMs exhibit linear or nonlinear\nbehavior, coupled or uncoupled multi-physical phenomena, locally elastic,\nweakly nonlocal (strain gradient and stress gradient), or strongly nonlocal\n(strain-type and displacement-type, peridynamics) phase properties. A modified\nComputational Analytical Micromechanics (CAM) approach introduces an exact\nAdditive General Integral Equation (AGIE) for CMs of any structure and phase\nproperties mentioned above. The unified iteration solution of static AGIEs is\nadapted to the body force with compact support serving as a fundamentally new\nuniversal training parameter. The approach also establishes a critical\nthreshold for filtering out unsuitable sub-datasets of effective parameters\nthrough a novel Representative Volume Element (RVE) concept, which extends\nHill's classical framework. This RVE concept eliminates sample size, boundary\nlayer, and edge effects, making it applicable to CMs of any structure and phase\nproperties, regardless of local or nonlocal, linear or nonlinear. Incorporating\nthis new RVE concept into machine learning and neural network techniques\nenables the construction of any unpredefined surrogate nonlocal operators. The\nmethodology is structured as a modular, block-based framework, allowing\nindependent development and refinement of software components. This flexible,\nrobust AGIE-CAM framework integrates data-driven, multi-scale, and\nmulti-physics modeling, accelerating research in CM of any microtopology and\nphase properties considered. The AGIE-CAM framework represents a groundbreaking\nparadigm shift in the micromechanics of composites, redefining the very\nphilosophy that underpins our understanding of their behavior at the\nmicroscopic level.",
        "We calculate the time of arrival probability distribution of a quantum\nparticle using the Bohmian formalism. The pilot-wave is given by the wave\nfunction of the one dimensional vacuum squeezed state but written in the\nSchr\\\"odinger representation. We made use of the unitary representation of the\nsymplectic group in the Hilbert space $L^2(\\mathbb{R})$. The solution to the\nBohmian equations are analytical function thus allowing for a closed expression\nof the time of arrival distribution which differs from the counterparts in the\nstandard quantum mechanics formulation.",
        "Observations indicate that central galaxies show a significant alignment of\ntheir main shape axes with other galaxies in their group, as well as with the\nlarge-scale structure of the universe. Simulations have corroborated this\nfinding, providing further insights into how the shape of the stellar component\naligns with the surrounding dark matter halo. Recent studies have also\ninvestigated the evolution of this alignment in bright central galaxies,\nrevealing that the shapes of the dark matter halo and the stellar component can\ndiffer. In this work, we aim at gaining a deeper understanding of galaxy\nalignments by quantifying how this property is related to the mass of the\nhaloes hosting central galaxies and to the large-scale environment measured at\ndifferent scales. By studying different angles, we describe how the alignments\nof central galaxies depend on the mass of the haloes they inhabit. We explore\nhow the main axes of central galaxies align across different scales, both in\nthree-dimensional and two-dimensional projections. We examine how halo mass\ninfluences these alignments and how they vary in the surrounding large-scale\nenvironment. To conduct this study, we employ the TNG300 hydrodynamical\nsimulations and compare our results with the spectroscopic data from the SDSS\nDR18. Three types of alignment were analysed: between stellar and dark matter\ncomponents, between satellite galaxies and the central galaxy, and between the\ncentral galaxy and its host halo. The results show that the alignment increases\nwith halo mass and varies with the environment (clusters, filaments, cluster\nperiphery, and others). However, after controlling for local density, we found\nthat most of the observed trends disappear. The SDSS observations confirm a\nmass dependence similar to the simulations, although observational biases limit\nthe detection of differences between the different environments.",
        "In this paper, we investigate the heliospheric modulation of cosmic rays in\ninterplanetary space, focusing on their propagation times and energy losses\nover the solar cycle. To perform the calculations, we employed a data-driven\nmodel based on the stochastic method. Our model was calibrated using\ntime-resolved and energy-resolved data from several missions including AMS-02,\nPAMELA, EPHIN\/SOHO, BESS, and data from Voyager-1. This approach allows us to\ncalculate probability density functions for the propagation time and energy\nlosses of cosmic protons and antiprotons in the heliosphere. Furthermore, we\nexplore the temporal evolution of these probabilities spanning from 1993 to\n2018, covering a full 22-year cycle of magnetic polarity, which includes two\nsolar minima and two magnetic reversals. Our calculations were carried out for\ncosmic protons and antiprotons, enabling us to investigate the role of\ncharge-sign dependent effects in cosmic ray transport. These findings provide\nvaluable insights into the physical processes of cosmic-ray propagation in the\nheliosphere and contribute to a deeper understanding of the solar modulation\nphenomenon.",
        "Identifying optimal medical treatments to improve survival has long been a\ncritical goal of pharmacoepidemiology. Traditionally, we use an average\ntreatment effect measure to compare outcomes between treatment plans. However,\nnew methods leveraging advantages of machine learning combined with the\nfoundational tenets of causal inference are offering an alternative to the\naverage treatment effect. Here, we use three unique, precision medicine\nalgorithms (random forests, residual weighted learning, efficient augmentation\nrelaxed learning) to identify optimal treatment rules where patients receive\nthe optimal treatment as indicated by their clinical history. First, we present\na simple hypothetical example and a real-world application among heart failure\npatients using Medicare claims data. We next demonstrate how the optimal\ntreatment rule improves the absolute risk in a hypothetical, three-modifier\nsetting. Finally, we identify an optimal treatment rule that optimizes the time\nto outcome in a real-world heart failure setting. In both examples, we compare\nthe average time to death under the optimized, tailored treatment rule with the\naverage time to death under a universal treatment rule to show the benefit of\nprecision medicine methods. The improvement under the optimal treatment rule in\nthe real-world setting is greatest (additional ~9 days under the tailored rule)\nfor survival time free of heart failure readmission."
      ]
    }
  },
  {
    "id":2411.01758,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data",
    "start_abstract":"Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "A review on segmentation of positron emission tomography images"
      ],
      "abstract":[
        "Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "CoddLLM: Empowering Large Language Models for Data Analytics",
        "Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain\n  Generalization",
        "Towards a Study of Low Energy Antiproton Annihilations on Nuclei",
        "Knowledge Phenomenology Research of Future Industrial Iconic Product\n  Innovation",
        "The simplest solutions of cold plasma equations: change in properties\n  from a hydrodynamic to a kinetic model",
        "Position: Stop Acting Like Language Model Agents Are Normal Agents",
        "Optimizing Energy Efficiency in Subthreshold RISC-V Cores",
        "Non-archimedean integration on totally disconnected spaces",
        "Cohomology of classifying spaces of rank 3 Kac-Moody groups",
        "Optimization Methods for Joint Eigendecomposition",
        "Graph factors and powers of Hamilton cycles in the budget-constrained\n  random graph process",
        "Teaching Dense Retrieval Models to Specialize with Listwise Distillation\n  and LLM Data Augmentation",
        "Learning to be Smooth: An End-to-End Differentiable Particle Smoother",
        "Advanced Zero-Shot Text-to-Speech for Background Removal and\n  Preservation with Controllable Masked Speech Prediction",
        "A diagrammatic approach to the Rasmussen invariant via tangles and\n  cobordisms",
        "Autonomous Robotic Bone Micro-Milling System with Automatic Calibration\n  and 3D Surface Fitting",
        "Structured Preconditioners in Adaptive Optimization: A Unified Analysis",
        "Learning a Game by Paying the Agents",
        "Invariant Measures for Data-Driven Dynamical System Identification:\n  Analysis and Application",
        "Non-dispersive graded impedance acoustic lenses",
        "Uncertainty-Aware Label Refinement on Hypergraphs for Personalized\n  Federated Facial Expression Recognition",
        "Spin Squeezing of Macroscopic Nuclear Spin Ensembles",
        "Locally and Polar Harmonic Maass Forms for Orthogonal Groups of\n  Signature $(2, n)$",
        "The properties of supermassive stars in galaxy merger driven direct\n  collapse I: models without rotation",
        "HEP High Power Targetry Roadmap -- Workshop Report",
        "G-Refer: Graph Retrieval-Augmented Large Language Model for Explainable\n  Recommendation",
        "MATS: An Audio Language Model under Text-only Supervision",
        "Bubbles-induced transition to elasto-inertial turbulence",
        "SFLD: Reducing the content bias for AI-generated Image Detection"
      ],
      "abstract":[
        "Large Language Models (LLMs) have the potential to revolutionize data\nanalytics by simplifying tasks such as data discovery and SQL query synthesis\nthrough natural language interactions. This work serves as a pivotal first step\ntoward the development of foundation models explicitly designed for data\nanalytics applications. To propel this vision forward, we unveil a new data\nrecipe for post-training LLMs, enhancing their comprehension of data management\nand empowering them to tackle complex real-world analytics tasks. Specifically,\nour innovative approach includes a scalable synthetic data generation method\nthat enables the creation of a broad spectrum of topics centered on data\nrepresentation and manipulation. Furthermore, we introduce two new tasks that\nseamlessly bridge tables and text. We show that such tasks can enhance models'\nunderstanding of schema creation and the nuanced translation between natural\nlanguage and tabular data. Leveraging this data recipe, we post-train a new\nfoundation model, named CoddLLM, based on Mistral-NeMo-12B. To assess the\nlanguage understanding and reasoning capabilities of LLMs in the realm of data\nanalytics, we contribute AnalyticsMMLU, a benchmark containing thousands of\nmultiple-choice questions on databases, data analysis, and machine learning.\nOur focus on data discovery, has resulted in the contribution of three\ncomprehensive benchmarks that address both database and data lake scenarios.\nCoddLLM not only excels in performance but also sets a new standard, achieving\nthe highest average accuracy across eight datasets. It outperforms\nGPT-3.5-Turbo on AnalyticsMMLU, exceeding GPT-4o by 12.1% in table selection\nand showing an average improvement of 24.9% in Text-to-SQL compared to the base\nmodel.",
        "We address the problem of semi-supervised domain generalization (SSDG), where\nthe distributions of train and test data differ, and only a small amount of\nlabeled data along with a larger amount of unlabeled data are available during\ntraining. Existing SSDG methods that leverage only the unlabeled samples for\nwhich the model's predictions are highly confident (confident-unlabeled\nsamples), limit the full utilization of the available unlabeled data. To the\nbest of our knowledge, we are the first to explore a method for incorporating\nthe unconfident-unlabeled samples that were previously disregarded in SSDG\nsetting. To this end, we propose UPCSC to utilize these unconfident-unlabeled\nsamples in SSDG that consists of two modules: 1) Unlabeled Proxy-based\nContrastive learning (UPC) module, treating unconfident-unlabeled samples as\nadditional negative pairs and 2) Surrogate Class learning (SC) module,\ngenerating positive pairs for unconfident-unlabeled samples using their\nconfusing class set. These modules are plug-and-play and do not require any\ndomain labels, which can be easily integrated into existing approaches.\nExperiments on four widely used SSDG benchmarks demonstrate that our approach\nconsistently improves performance when attached to baselines and outperforms\ncompeting plug-and-play methods. We also analyze the role of our method in\nSSDG, showing that it enhances class-level discriminability and mitigates\ndomain gaps. The code is available at https:\/\/github.com\/dongkwani\/UPCSC.",
        "A study of antiproton annihilations at rest on thin solid targets is underway\nat the ASACUSA facility, which now features a dedicated beam line for slow\nextraction at 250 eV. The experiment will employ new technologies, such as the\nTimepix4 ASICs coupled to silicon sensors, to measure the total multiplicity,\nenergy, and angular distribution of various prongs produced in thin solid\ntargets. A detection system consisting of seven Timepix4, covering most of the\nsolid angle, is being constructed. A 3D annihilation vertex reconstruction\nalgorithm from particle tracks in the single-plane detectors has been developed\nusing Monte Carlo simulations. The measurements will enable a study of\npbar-nucleus interactions, their dependence on nucleus mass and branching\nratios. The results will be used to assess and potentially improve various\nsimulation models.",
        "Iconic products, as innovative carriers supporting the development of future\nindustries, are key breakthrough points for driving the transformation of new\nquality productive forces. This article is grounded in the philosophy of\ntechnology and examines the evolution of human civilization to accurately\nidentify the patterns of product innovation. By integrating theories from\nsystems science, it analyzes the intrinsic logical differences between\ntraditional products and iconic products. The study finds that iconic products\nare based on a comprehensive knowledge system that integrates explicit and\ntacit knowledge, enabling them to adapt to complex dynamic environments.\nTherefore, based on the method of phenomenological essence reduction and the\nprocess of specialized knowledge acquisition, this study establishes the first\nprinciple of knowledge phenomenology: \"knowledge generation-moving from the\ntacit to the explicit-moving from the explicit to the tacit-fusion of the\nexplicit and tacit.\" Grounded in knowledge phenomenology, it reconstructs the\nproduct design evolution process and establishes a forward innovative design\nframework for iconic products, consisting of \"design problem space-explicit\nknowledge space-tacit knowledge space-innovative solution space.\" Furthermore,\nbased on FBS design theory, it develops a disruptive technology innovation\nforecasting framework of \"technology problem space-knowledge base\nprediction-application scenario prediction-coupled technology prediction,\"\nwhich collectively advances the innovation systems engineering of iconic\nproducts. In light of the analysis of the global future industrial competitive\nlandscape, it proposes a strategy for enhancing embodied intelligence in iconic\nproducts.",
        "We consider the transition from the kinetic model of Landau cold plasma to\nthe hydrodynamic one by constructing a \"multi-speed\" moment chain in the case\nof one spatial variable. Closing this chain at the first step leads to the\nstandard hydrodynamic system of cold plasma. The change in the properties of\nthe solution when closing the chain at the second step is discussed using the\nexample of two classes of solutions - affine in space and traveling waves, and\nit is shown that their properties change significantly compared to the\nhydrodynamic model.",
        "Language Model Agents (LMAs) are increasingly treated as capable of\nautonomously navigating interactions with humans and tools. Their design and\ndeployment tends to presume they are normal agents capable of sustaining\ncoherent goals, adapting across contexts and acting with a measure of\nintentionality. These assumptions are critical to prospective use cases in\nindustrial, social and governmental settings. But LMAs are not normal agents.\nThey inherit the structural problems of the large language models (LLMs) around\nwhich they are built: hallucinations, jailbreaking, misalignment and\nunpredictability. In this Position paper we argue LMAs should not be treated as\nnormal agents, because doing so leads to problems that undermine their utility\nand trustworthiness. We enumerate pathologies of agency intrinsic to LMAs.\nDespite scaffolding such as external memory and tools, they remain\nontologically stateless, stochastic, semantically sensitive, and linguistically\nintermediated. These pathologies destabilise the ontological properties of LMAs\nincluding identifiability, continuity, persistence and and consistency,\nproblematising their claim to agency. In response, we argue LMA ontological\nproperties should be measured before, during and after deployment so that the\nnegative effects of pathologies can be mitigated.",
        "Our goal in this paper is to understand how to maximize energy efficiency\nwhen designing standard-ISA processor cores for subthreshold operation. We\nhence develop a custom subthreshold library and use it to synthesize the\nopen-source RISC-V cores SERV, QERV, PicoRV32, Ibex, Rocket, and two variants\nof Vex, targeting a supply voltage of 300 mV in a commercial 130 nm process.\nSERV, QERV, and PicoRV32 are multi-cycle architectures, while Ibex, Vex, and\nRocket are pipelined architectures.\n  We find that SERV, QERV, PicoRV32, and Vex are Pareto optimal in one or more\nof performance, power, and area. The 2-stage Vex (Vex-2) is the most energy\nefficient core overall, mainly because it uses fewer cycles per instruction\nthan multi-cycle SERV, QERV, and PicoRV32 while retaining similar power\nconsumption. Pipelining increases core area, and we observe that for\nsubthreshold operation, the longer wires of pipelined designs require adding\nbuffers to maintain a cycle time that is low enough to achieve high energy\nefficiency. These buffers limit the performance gains achievable by deeper\npipelining because they result in cycle time no longer scaling proportionally\nwith pipeline stages. The added buffers and the additional area required for\npipelining logic however increase power consumption, and Vex-2 therefore\nprovides similar performance and lower power consumption than the 5-stage cores\nVex-5 and Rocket. A key contribution of this paper is therefore to demonstrate\nthat limited-depth pipelined RISC-V designs hit the sweet spot in balancing\nperformance and power consumption when optimizing for energy efficiency in\nsubthreshold operation.",
        "We work in the category $\\mathcal{CLM}^u_k$ of [5] of separated complete\nbounded $k$-linearly topologized modules over a complete linearly topologized\nring $k$ and discuss duality on certain exact subcategories. We study\ntopological and uniform structures on locally compact paracompact\n$0$-dimensional topological spaces $X$, named $td$-spaces in [11] and [17], and\nthe corresponding algebras $\\mathscr{C}_?(X,k)$ of continuous $k$-valued\nfunctions, with a choice of support and uniformity conditions. We apply the\nprevious duality theory to define and study the dual coalgebras\n$\\mathscr{D}_?(X,k)$ of $k$-valued measures on $X$. We then complete the\npicture by providing a direct definition of the various types of measures. In\nthe case of $X$ a commutative $td$-group $G$ the integration pairing provides\nperfect dualities of Hopf $k$-algebras between $$\\mathscr{C}_{\\rm unif}(G,k)\n\\longrightarrow \\mathscr{C}(G,k) \\;\\;\\;\\mbox{and}\\;\\;\\; \\mathscr{D}_{\\rm\nacs}(G,k) \\longrightarrow \\mathscr{D}_{\\rm unif}(G,k) \\;.$$ We conclude the\npaper with the remarkable example of $G= \\mathbb{G}_a(\\mathbb{Q}_p)$ and $k =\n\\mathbb{Z}_p$, leading to the basic Fontaine ring $${\\bf A}_{\\rm inf} = {\\rm W}\n\\left(\\widehat{\\mathbb{F}_p[[t^{1\/p^\\infty}]]}\\right) = \\mathscr{D}_{\\rm\nunif}(\\mathbb{Q}_p,\\mathbb{Z}_p) \\;.$$ We discuss Fourier duality between ${\\bf\nA}_{\\rm inf}$ and $\\mathscr{C}_{\\rm unif}(\\mathbb{Q}_p,\\mathbb{Z}_p)$ and\nexhibit a remarkable Fr\\'echet basis of $\\mathscr{C}_{\\rm\nunif}(\\mathbb{Q}_p,\\mathbb{Z}_p)$ related to the classical binomial\ncoefficients.",
        "We represent the rational and mod $p$ cohomology groups of classifying spaces\nof rank 3 Kac-Moody groups by a direct sum of the invariants of Weyl groups and\ntheir quotients. As an application, the authors conclude that there is a\n$p$-torsion for each prime $p$ in the integral cohomology groups of classifying\nspaces of rank 3 Kac-Moody groups. We also determine the ring structure of the\nrational cohomology with one exception case.",
        "Joint diagonalization, the process of finding a shared set of approximate\neigenvectors for a collection of matrices, arises in diverse applications such\nas multidimensional harmonic analysis or quantum information theory. This task\nis typically framed as an optimization problem: minimizing a non-convex\nfunction that quantifies off-diagonal matrix elements across possible bases. In\nthis work, we introduce a suite of efficient algorithms designed to locate\nlocal minimizers of this functional. Our methods leverage the Hessian's\nstructure to bypass direct computation of second-order derivatives, evaluating\nit as either an operator or bilinear form - a strategy that remains\ncomputationally feasible even for large-scale applications. Additionally, we\ndemonstrate that this Hessian-based information enables precise estimation of\nparameters, such as step-size, in first-order optimization techniques like\nGradient Descent and Conjugate Gradient, and the design of second-order methods\nsuch as (Quasi-)Newton. The resulting algorithms for joint diagonalization\noutperform existing techniques, and we provide comprehensive numerical evidence\nof their superior performance.",
        "We consider the following budget-constrained random graph process introduced\nby Frieze, Krivelevich and Michaeli. A player, called Builder, is presented\nwith $t$ distinct edges of $K_n$ one by one, chosen uniformly at random.\nBuilder may purchase at most $b$ of these edges, and must (irrevocably) decide\nwhether to purchase each edge as soon as it is offered. Builder's goal is to\nconstruct a graph which satisfies a certain property; we investigate the\nproperties of containing different $F$-factors or powers of Hamilton cycles.\n  We obtain general lower bounds on the budget $b$, as a function of $t$,\nrequired for Builder to obtain partial $F$-factors, for arbitrary $F$. These\nimply lower bounds for many distinct spanning structures, such as powers of\nHamilton cycles. Notably, our results show that, if $t$ is close to the hitting\ntime for a partial $F$-factor, then the budget $b$ cannot be substantially\nlower than $t$. These results give negative answers to questions of Frieze,\nKrivelevich and Michaeli.\n  Conversely, we also exhibit a simple strategy for constructing (partial)\n$F$-factors, in particular showing that our general lower bound is tight up to\nconstant factors. The ideas from this strategy can be exploited for other\nproperties. As an example, we obtain an essentially optimal strategy for powers\nof Hamilton cycles. In order to formally prove that this strategy succeeds, we\ndevelop novel tools for analysing multi-stage strategies, which may be of\ngeneral interest for studying other properties.",
        "While the current state-of-the-art dense retrieval models exhibit strong\nout-of-domain generalization, they might fail to capture nuanced\ndomain-specific knowledge. In principle, fine-tuning these models for\nspecialized retrieval tasks should yield higher effectiveness than relying on a\none-size-fits-all model, but in practice, results can disappoint. We show that\nstandard fine-tuning methods using an InfoNCE loss can unexpectedly degrade\neffectiveness rather than improve it, even for domain-specific scenarios. This\nholds true even when applying widely adopted techniques such as hard-negative\nmining and negative de-noising. To address this, we explore a training strategy\nthat uses listwise distillation from a teacher cross-encoder, leveraging rich\nrelevance signals to fine-tune the retriever. We further explore synthetic\nquery generation using large language models. Through listwise distillation and\ntraining with a diverse set of queries ranging from natural user searches and\nfactual claims to keyword-based queries, we achieve consistent effectiveness\ngains across multiple datasets. Our results also reveal that synthetic queries\ncan rival human-written queries in training utility. However, we also identify\nlimitations, particularly in the effectiveness of cross-encoder teachers as a\nbottleneck. We release our code and scripts to encourage further research.",
        "For challenging state estimation problems arising in domains like vision and\nrobotics, particle-based representations attractively enable temporal reasoning\nabout multiple posterior modes. Particle smoothers offer the potential for more\naccurate offline data analysis by propagating information both forward and\nbackward in time, but have classically required human-engineered dynamics and\nobservation models. Extending recent advances in discriminative training of\nparticle filters, we develop a framework for low-variance propagation of\ngradients across long time sequences when training particle smoothers. Our\n\"two-filter'' smoother integrates particle streams that are propagated forward\nand backward in time, while incorporating stratification and importance weights\nin the resampling step to provide low-variance gradient estimates for neural\nnetwork dynamics and observation models. The resulting mixture density particle\nsmoother is substantially more accurate than state-of-the-art particle filters,\nas well as search-based baselines, for city-scale global vehicle localization\nfrom real-world videos and maps.",
        "The acoustic background plays a crucial role in natural conversation. It\nprovides context and helps listeners understand the environment, but a strong\nbackground makes it difficult for listeners to understand spoken words. The\nappropriate handling of these backgrounds is situation-dependent: Although it\nmay be necessary to remove background to ensure speech clarity, preserving the\nbackground is sometimes crucial to maintaining the contextual integrity of the\nspeech. Despite recent advancements in zero-shot Text-to-Speech technologies,\ncurrent systems often struggle with speech prompts containing backgrounds. To\naddress these challenges, we propose a Controllable Masked Speech Prediction\nstrategy coupled with a dual-speaker encoder, utilizing a task-related control\nsignal to guide the prediction of dual background removal and preservation\ntargets. Experimental results demonstrate that our approach enables precise\ncontrol over the removal or preservation of background across various acoustic\nconditions and exhibits strong generalization capabilities in unseen scenarios.",
        "We introduce a diagrammatic approach to Rasmussen's $s$-invariant via tangles\nand cobordisms, combining Bar-Natan's formulation of Khovanov homology for\ntangles and cobordisms with the characterization of $s$ via the divisibility of\nthe Lee class, as developed in the author's previous works. This framework\nenables a \"divide-and-conquer\" method for computing $s$ from a tangle\ndecomposition of a given knot diagram, making it suitable for both\npen-and-paper calculations and algorithmic implementations. As an application,\nwe determine the $s$-invariants of pretzel knots of the form $P(p_1, -p_2,\n\\ldots, -p_l)$, where $l \\geq 3$ is odd, all $p_i$ are positive and odd, and\n$p_1 < \\min\\{p_2, \\ldots, p_l\\}$.",
        "Automating bone micro-milling using a robotic system presents challenges due\nto the uncertainties in both the external and internal features of bone tissue.\nFor example, during a mouse cranial window creation, a circular path with a\nradius of 2 to 4 mm needs to be milled on the mouse skull using a microdrill.\nThe uneven surface and non-uniform thickness of the mouse skull make it\ndifficult to fully automate this process, requiring the system to possess\nadvanced perceptual and adaptive capabilities. In this study, we propose an\nautomatic calibration and 3D surface fitting method and integrate it into an\nautonomous robotic bone micro-milling system, enabling it to quickly, in\nreal-time, and accurately perceive and adapt to the uneven surface and\nnon-uniform thickness of the target without human assistance. Validation\nexperiments on euthanized mice demonstrate that the improved system achieves a\nsuccess rate of 85.7 % and an average milling time of 2.1 minutes, showing not\nonly significant performance improvements over the previous system but also\nexceptional accuracy, speed, and stability compared to human operators.",
        "We present a novel unified analysis for a broad class of adaptive\noptimization algorithms with structured (e.g., layerwise, diagonal, and\nkronecker-factored) preconditioners for both online regret minimization and\noffline convex optimization. Our analysis not only provides matching rate to\nseveral important structured preconditioned algorithms including diagonal\nAdaGrad, full-matrix AdaGrad, and AdaGrad-Norm, but also gives an improved\nconvergence rate for a one-sided variant of Shampoo over that of original\nShampoo. Interestingly, more structured preconditioners (e.g., diagonal\nAdagrad, AdaGrad-Norm which use less space and compute) are often presented as\ncomputationally efficient approximations to full-matrix Adagrad, aiming for\nimproved optimization performance through better approximations. Our unified\nanalysis challenges this prevailing view and reveals, perhaps surprisingly,\nthat more structured preconditioners, despite using less space and computation\nper step, can outperform their less structured counterparts. To demonstrate\nthis, we show that one-sided Shampoo, which is relatively much cheaper than\nfull-matrix AdaGrad could outperform it both theoretically and experimentally.",
        "We study the problem of learning the utility functions of agents in a\nnormal-form game by observing the agents play the game repeatedly. Differing\nfrom most prior literature, we introduce a principal with the power to observe\nthe agents playing the game, send the agents signals, and send the agents\npayments as a function of their actions. Under reasonable behavioral models for\nthe agents such as iterated dominated action removal or a no-regret assumption,\nwe show that the principal can, using a number of rounds polynomial in the size\nof the game, learn the utility functions of all agents to any desirable\nprecision $\\varepsilon > 0$. We also show lower bounds in both models, which\nnearly match the upper bounds in the former model and also strictly separate\nthe two models: the principal can learn strictly faster in the iterated\ndominance model. Finally, we discuss implications for the problem of steering\nagents to a desired equilibrium: in particular, we introduce, using our\nutility-learning algorithm as a subroutine, the first algorithm for steering\nlearning agents without prior knowledge of their utilities.",
        "We propose a novel approach for performing dynamical system identification,\nbased upon the comparison of simulated and observed physical invariant\nmeasures. While standard methods adopt a Lagrangian perspective by directly\ntreating time-trajectories as inference data, we take on an Eulerian\nperspective and instead seek models fitting the observed global time-invariant\nstatistics. With this change in perspective, we gain robustness against\npervasive challenges in system identification including noise, chaos, and slow\nsampling. In the first half of this paper, we pose the system identification\ntask as a partial differential equation (PDE) constrained optimization problem,\nin which synthetic stationary solutions of the Fokker-Planck equation, obtained\nas fixed points of a finite-volume discretization, are compared to physical\ninvariant measures extracted from observed trajectory data. In the latter half\nof the paper, we improve upon this approach in two crucial directions. First,\nwe develop a Galerkin-inspired modification to the finite-volume surrogate\nmodel, based on data-adaptive unstructured meshes and Monte-Carlo integration,\nenabling the approach to efficiently scale to high-dimensional problems.\nSecond, we leverage Takens' seminal time-delay embedding theory to introduce a\ncritical data-dependent coordinate transformation which can guarantee unique\nsystem identifiability from the invariant measure alone. This contribution\nresolves a major challenge of system identification through invariant measures,\nas systems exhibiting distinct transient behaviors may still share the same\ntime-invariant statistics in their state-coordinates. Throughout, we present\ncomprehensive numerical tests which highlight the effectiveness of our approach\non a variety of challenging system identification tasks.",
        "Lenses are typically based on refractive index profiles derived from the\ngeometric approximation of high-frequency waves, yet the critical issue of\nimpedance mismatch is often neglected. Mismatched devices suffer from unwanted\nreflections and dispersion, which can significantly degrade performance in\npractical applications. In this work, we propose impedance profiles for lenses\nto achieve efficient wave transmission while maintaining the desired refractive\nindex and minimizing dispersion effects. A family of impedance profiles is\nderived from the acoustic wave equation such that the phase velocity is\npreserved. First, the 1D setting is considered to explain how dispersion occurs\ninside a lens and at its interfaces. Then, the method is applied to 2D\naxisymmetric configurations where the impedance mismatch is radially\nredistributed. These profiles are demonstrated in the acoustic setting of a\nLuneburg lens, but can be easily extended to more general scenarios such as\nimaging or cloaking in air and water, where matching the impedance of the\nbackground poses significant challenges.",
        "Most facial expression recognition (FER) models are trained on large-scale\nexpression data with centralized learning. Unfortunately, collecting a large\namount of centralized expression data is difficult in practice due to privacy\nconcerns of facial images. In this paper, we investigate FER under the\nframework of personalized federated learning, which is a valuable and practical\ndecentralized setting for real-world applications. To this end, we develop a\nnovel uncertainty-Aware label refineMent on hYpergraphs (AMY) method. For local\ntraining, each local model consists of a backbone, an uncertainty estimation\n(UE) block, and an expression classification (EC) block. In the UE block, we\nleverage a hypergraph to model complex high-order relationships between\nexpression samples and incorporate these relationships into uncertainty\nfeatures. A personalized uncertainty estimator is then introduced to estimate\nreliable uncertainty weights of samples in the local client. In the EC block,\nwe perform label propagation on the hypergraph, obtaining high-quality refined\nlabels for retraining an expression classifier. Based on the above, we\neffectively alleviate heterogeneous sample uncertainty across clients and learn\na robust personalized FER model in each client. Experimental results on two\nchallenging real-world facial expression databases show that our proposed\nmethod consistently outperforms several state-of-the-art methods. This\nindicates the superiority of hypergraph modeling for uncertainty estimation and\nlabel refinement on the personalized federated FER task. The source code will\nbe released at https:\/\/github.com\/mobei1006\/AMY.",
        "Spin squeezing has been explored in atomic systems as a tool for quantum\nsensing, improving experimental sensitivity beyond the spin standard quantum\nlimit for certain measurements. To optimize absolute metrological sensitivity,\nit is beneficial to consider macroscopic spin ensembles, such as nuclear spins\nin solids and liquids. Coupling a macroscopic spin ensemble to a\nparametrically-modulated resonant circuit can create collective spin squeezing\nby generating spin correlations mediated by the circuit. We analyze the\nsqueezing dynamics in the presence of decoherence and finite spin polarization,\nshowing that achieving 7 dB spin squeezing is feasible in several nuclear spin\nsystems. The metrological benefit of squeezing a macroscopic spin ensemble lies\nin the suppression of technical noise sources in the spin detection system\nrelative to the spin projection noise. This expands the experimental\nsensitivity bandwidth when searching for signals of unknown frequency and can\nimprove the resonant signal-to-noise ratio. Squeezing macroscopic spin\nensembles may prove to be a useful technique for fundamental physics\nexperiments aimed at detecting spin interactions with oscillating background\nfields, such as ultralight dark matter.",
        "We generalize the notions of locally and polar harmonic Maass forms to\ngeneral orthogonal groups of signature $(2, n)$ with singularities along real\nanalytic and algebraic cycles. We prove a current equation for locally harmonic\nMaass forms and recover the Fourier expansion of the Oda lift involving cycle\nintegrals. Moreover, using the newly defined polar harmonic Maass forms, we\nprove that meromorphic modular forms with singularities along special divisors\nare orthogonal to cusp forms with respect to a regularized Petersson inner\nproduct. Using this machinery, we derive a duality theorem involving cycle\nintegrals of meromorphic modular forms along real analytic cycles and cycle\nintegrals of locally harmonic Maass forms along algebraic cycles.",
        "The formation of the most massive quasars observed at high redshifts requires\nextreme accretion rates ($>1$ M$_\\odot$ yr$^{-1}$). Inflows of $10-1000$\nM$_\\odot$ yr$^{-1}$ are found in hydrodynamical simulations of galaxy mergers,\nleading to the formation of supermassive discs (SMDs) with high metallicities\n($>$ Z$_\\odot$). Supermassive stars (SMSs) born in these SMDs could be the\nprogenitors of the most extreme quasars. Here, we study the properties of\nnon-rotating SMSs forming in high metallicity SMDs. Using the stellar evolution\ncode GENEC, we compute numerically the hydrostatic structures of non-rotating\nSMSs with metallicities $Z=1-10$ Z$_\\odot$ by following their evolution under\nconstant accretion at rates $10-1000$ M$_\\odot$ yr$^{-1}$. We determine the\nfinal mass of the SMSs, set by the general-relativistic (GR) instability, by\napplying the relativistic equation of adiabatic pulsations to the hydrostatic\nstructures. We find that non-rotating SMSs with metallicities $Z=1-10$\nZ$_\\odot$ accreting at rates $10-1000$ M$_\\odot$ yr$^{-1}$ evolve as red\nsupergiant protostars until their final collapse. All the models reach the GR\ninstability during H-burning. The final mass is $\\sim10^6$ M$_\\odot$, nearly\nindependently of the metallicity and the accretion rate.",
        "Designing a reliable target is already a challenge for MW-class facilities\ntoday and has led several major accelerator facilities to operate at lower than\ndesign power due to target concerns. With present plans to increase beam power\nfor next generation accelerator facilities in the next decade, timely R and D\nin support of robust high power targets is critical to secure the full physics\nbenefits of ambitious accelerator power upgrades. A comprehensive R and D\nprogram must be implemented to address the many complex challenges faced by\nmulti MW beam intercepting devices. This roadmap is envisioned to be helpful to\nthe DOE-OHEP office when planning and prioritizing future R and D activities as\nwell as leveraging synergies across the Office of Science. The roadmap will be\nextremely beneficial to the broader (external to DOE HEP) HPT community by\ncommunicating OHEP s high level strategy and objectives for HPT R and D and\nhighlighting possible opportunities for collaboration.",
        "Explainable recommendation has demonstrated significant advantages in\ninforming users about the logic behind recommendations, thereby increasing\nsystem transparency, effectiveness, and trustworthiness. To provide\npersonalized and interpretable explanations, existing works often combine the\ngeneration capabilities of large language models (LLMs) with collaborative\nfiltering (CF) information. CF information extracted from the user-item\ninteraction graph captures the user behaviors and preferences, which is crucial\nfor providing informative explanations. However, due to the complexity of graph\nstructure, effectively extracting the CF information from graphs still remains\na challenge. Moreover, existing methods often struggle with the integration of\nextracted CF information with LLMs due to its implicit representation and the\nmodality gap between graph structures and natural language explanations. To\naddress these challenges, we propose G-Refer, a framework using graph\nretrieval-augmented large language models (LLMs) for explainable\nrecommendation. Specifically, we first employ a hybrid graph retrieval\nmechanism to retrieve explicit CF signals from both structural and semantic\nperspectives. The retrieved CF information is explicitly formulated as\nhuman-understandable text by the proposed graph translation and accounts for\nthe explanations generated by LLMs. To bridge the modality gap, we introduce\nknowledge pruning and retrieval-augmented fine-tuning to enhance the ability of\nLLMs to process and utilize the retrieved CF information to generate\nexplanations. Extensive experiments show that G-Refer achieves superior\nperformance compared with existing methods in both explainability and\nstability. Codes and data are available at https:\/\/github.com\/Yuhan1i\/G-Refer.",
        "Large audio-language models (LALMs), built upon powerful Large Language\nModels (LLMs), have exhibited remarkable audio comprehension and reasoning\ncapabilities. However, the training of LALMs demands a large corpus of\naudio-language pairs, which requires substantial costs in both data collection\nand training resources. In this paper, we propose MATS, an audio-language\nmultimodal LLM designed to handle Multiple Audio task using solely Text-only\nSupervision. By leveraging pre-trained audio-language alignment models such as\nCLAP, we develop a text-only training strategy that projects the shared\naudio-language latent space into LLM latent space, endowing the LLM with audio\ncomprehension capabilities without relying on audio data during training. To\nfurther bridge the modality gap between audio and language embeddings within\nCLAP, we propose the Strongly-related noisy text with audio (Santa) mechanism.\nSanta maps audio embeddings into CLAP language embedding space while preserving\nessential information from the audio input. Extensive experiments demonstrate\nthat MATS, despite being trained exclusively on text data, achieves competitive\nperformance compared to recent LALMs trained on large-scale audio-language\npairs.",
        "Interface-resolved direct numerical simulations are performed to investigate\nbubble-induced transition from laminar to elasto-inertial turbulent (EIT) state\nin a pressure-driven viscoelastic square channel flow. The Giesekus model is\nused to account for the viscoelasticity of the continuous phase while the\ndispersed phase is Newtonian. Simulations are performed for both single and\ntwo-phase flows for a wide range of the Reynolds (Re) and the Weissenberg (Wi)\nnumbers. It is demonstrated that injection of bubbles into a laminar\nviscoelastic flow introduces streamline curvature that is sufficient to trigger\nan elastic instability leading to a transition to a fully EIT regime. The\ntemporal turbulent kinetic energy spectrum shows a scaling of -2 for this\nmultiphase EIT regime. It is shown that, once the flow is fully transitioned to\na turbulent state by the injection of bubbles, the drag increases for all the\ncases. It is also observed that bubbles move towards the channel centreline and\nform a string-shaped alignment pattern in the core region at the lower values\nof Re=10 and Wi=1. In this regime, the flow exhibits an intermittent behaviour,\ni.e., there are turbulent like fluctuations in the core region while it is\nessentially laminar near the wall. Unlike the solid particles, it is found that\nincreasing shear-thinning effect breaks up the alignment of bubbles.\nInterestingly, the drag remains slightly lower in this intermittent regime than\nthe corresponding laminar state.",
        "Identifying AI-generated content is critical for the safe and ethical use of\ngenerative AI. Recent research has focused on developing detectors that\ngeneralize to unknown generators, with popular methods relying either on\nhigh-level features or low-level fingerprints. However, these methods have\nclear limitations: biased towards unseen content, or vulnerable to common image\ndegradations, such as JPEG compression. To address these issues, we propose a\nnovel approach, SFLD, which incorporates PatchShuffle to integrate high-level\nsemantic and low-level textural information. SFLD applies PatchShuffle at\nmultiple levels, improving robustness and generalization across various\ngenerative models. Additionally, current benchmarks face challenges such as low\nimage quality, insufficient content preservation, and limited class diversity.\nIn response, we introduce TwinSynths, a new benchmark generation methodology\nthat constructs visually near-identical pairs of real and synthetic images to\nensure high quality and content preservation. Our extensive experiments and\nanalysis show that SFLD outperforms existing methods on detecting a wide\nvariety of fake images sourced from GANs, diffusion models, and TwinSynths,\ndemonstrating the state-of-the-art performance and generalization capabilities\nto novel generative models."
      ]
    }
  },
  {
    "id":2411.01758,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"A review on segmentation of positron emission tomography images",
    "start_abstract":"Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data"
      ],
      "abstract":[
        "Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Operational Feasibility Analysis of a Cryogenic Active Intake Device for\n  Atmosphere-Breathing Electric Propulsion",
        "Exponentially Better Bounds for Quantum Optimization via Dynamical\n  Simulation",
        "Scalar Field Kantowski--Sachs Solutions in Teleparallel $F(T)$ Gravity",
        "In the graphical Sierpinski gasket, the reverse Riesz transform is\n  unbounded on $L^p$, $p\\in (1,2)$",
        "On the origin of radio polarization in pulsar polar caps",
        "Heavy Axions Can Disrupt $\\gamma$-ray Bursts",
        "TOPCAT\/STILTS Integration",
        "Estimating Task-based Performance Bounds for Accelerated MRI Image\n  Reconstruction Methods by Use of Learned-Ideal Observers",
        "JT Gravity in de Sitter Space and Its Extensions",
        "Generalised Process Theories",
        "Observer-Based Output-Feedback Backstepping Stabilization of Continua of\n  Hyperbolic PDEs and Application to Large-Scale $n+m$ Coupled Hyperbolic PDEs",
        "Deep inference of simulated strong lenses in ground-based surveys",
        "A simple recursive representation of the Faulhaber series",
        "Fragmentation measurements with the FOOT experiment",
        "Galactic structure dependence of cloud-cloud collisions driven star\n  formation in the barred galaxy NGC 3627",
        "Fabrication and characterization of bimetallic silica-based and\n  3D-printed active colloidal cubes",
        "Current Opinions on Memristor-Accelerated Machine Learning Hardware",
        "Time scale competition in the Active Coagulation Model",
        "Trial by FIRE: Probing the dark matter density profile of dwarf galaxies\n  with GraphNPE",
        "Testing the Flux Rope Paradigm for Coronal Mass Ejections Using a Three\n  Spacecraft Encounter Event",
        "Convergence analysis of linearized $\\ell_q$ penalty methods for\n  nonconvex optimization with nonlinear equality constraints",
        "Siegel modular forms associated to Weil representations",
        "A Quantum Signature Validation Algorithm for Efficient Detection of\n  Tampered Transactions in Blockchain",
        "Study of environment friendly gas mixtures for the Resistive Plate\n  Chambers of the ATLAS phase-2 upgrade",
        "Machine Learning for Ground State Preparation via Measurement and\n  Feedback",
        "Image resizing by neural network operators and their convergence rate\n  with respect to the $L^p$-norm and the dissimilarity index defined through\n  the continuous SSIM",
        "DISCD: Distributed Lossy Semantic Communication for Logical Deduction of\n  Hypothesis",
        "Power of $(L_0,L_1)$-Smoothness in Stochastic Convex Optimization:\n  First- and Zero-Order Algorithms",
        "HedgeAgents: A Balanced-aware Multi-agent Financial Trading System"
      ],
      "abstract":[
        "Atmosphere-breathing electric propulsion (ABEP) systems are emerging for\norbit maintenance in very-low-Earth orbit (VLEO) by capturing atmospheric\npropellant \\textit{in situ} using an intake device. A previous study proposed\nthe cryocondensation-regeneration active intake device (CRAID) to significantly\nenhance intake performance. This study investigates the operational feasibility\nof CRAID. A conceptual prototype model (CPM) is presented to verify its\nfeasibility, and numerical analyses demonstrate the practical operational\nsequences, required cryocooler capacity, intake performance, and flight\nenvelope. The numerical analyses employ the direct simulation Monte Carlo\n(DSMC) method with a phase change model and a 0D analytical model for RF ion\nthrusters. A significant improvement in intake performance is estimated based\non the practical sequences, with compression performance at least 1000 times\nhigher than that of prevalent intake devices. The capability for consistent\npropellant supply is observed regardless of atmospheric conditions. A model\nsatellite incorporating CPM confirms that CRAID enables complete drag\ncompensation at altitudes above 190 km without limiting the upper boundary of\nthe flight envelope.",
        "We provide several quantum algorithms for continuous optimization that do not\nrequire any gradient estimation. Instead, we encode the optimization problem\ninto the dynamics of a physical system and coherently simulate the time\nevolution. This allows us, in certain cases, to obtain exponentially better\nquery upper bounds relative to the best known upper bounds for gradient-based\noptimization schemes which utilize quantum computers only for the evaluation of\ngradients. Our first two algorithms can find local optima of a differentiable\nfunction $f: \\mathbb{R}^N \\rightarrow \\mathbb{R}$ by simulating either\nclassical or quantum dynamics with friction via a time-dependent Hamiltonian.\nWe show that these methods require $O(N\\kappa^2\/h_x^2\\epsilon)$ queries to a\nphase oracle to find an $\\epsilon$-approximate local optimum of a locally\nquadratic objective function, where $\\kappa$ is the condition number of the\nHessian matrix and $h_x$ is the discretization spacing. In contrast, we show\nthat gradient-based methods require $O(N(1\/\\epsilon)^{\\kappa \\log(3)\/4})$\nqueries. Our third algorithm can find the global optimum of $f$ by preparing a\nclassical low-temperature thermal state via simulation of the classical\nLiouvillian operator associated with the Nos\\'e Hamiltonian. We use results\nfrom the quantum thermodynamics literature to bound the thermalization time for\nthe discrete system. Additionally, we analyze barren plateau effects that\ncommonly plague quantum optimization algorithms and observe that our approach\nis vastly less sensitive to this problem than standard gradient-based\noptimization. Our results suggests that these dynamical optimization approaches\nmay be far more scalable for future quantum machine learning, optimization and\nvariational experiments than was widely believed.",
        "In this paper, we investigate time-dependent Kantowski--Sachs spherically\nsymmetric teleparallel $F(T)$ gravity with a scalar field source. We begin by\nsetting the exact field equations to be solved and solve conservation laws for\npossible scalar field potential, $V\\left(\\phi\\right)$, solutions. Then, we find\nnew non-trivial teleparallel $F(T)$ solutions by using power-law and\nexponential ansatz for each potential case arising from conservation laws, such\nas linear, quadratic, or logarithmic, to name a few. We find a general formula\nallowing us to compute all possible new teleparallel $F(T)$ solutions\napplicable for any scalar field potential and ansatz. Then, we apply this\nformula and find a large number of exact and approximate new teleparallel\n$F(T)$ solutions for several types of cases. Some new $F(T)$ solution classes\nmay be relevant for future cosmological applications, especially concerning\ndark matter, dark energy quintessence, phantom energy leading to the Big Rip\nevent, and quintom models of physical processes.",
        "In this article, we proved that the reverse Riesz transform on the graphical\nSierpinski gasket is unbounded on $L^p$ for $p\\in (1,2)$. Together with\nprevious results, it shows that the Riesz transform on the graphical Sierpinski\ngasket is bounded on $L^p$ if and only if $p\\in (1,2]$ and the reverse Riesz\ntransform is bounded on $L^p$ if and only if $p\\in [2,\\infty)$.\n  Moreover, our method is quite flexible - but requires explicit computations -\nand hints to the fact that the reverse Riesz transforms is never bounded on\n$L^p$, $p\\in (1,2)$, on graphs with slow diffusions.",
        "A knowledge of polarization properties of coherent radio waves escaping\npulsar polar caps is crucial for calculating radiative transfer through the\nmagnetosphere and for obtaining specific predictions of observable radio\nproperties. We describe the pair cascades in the pulsar polar cap, and for the\nfirst time, determine the Stokes parameters of the escaping radio waves from\nfirst-principle kinetic simulations for a pulsar with an inclination angle of\nthe magnetic axis 60{\\deg}.\n  Our model provides a quantitative and qualitative explanation of the observed\npulsar radio powers and spectra, the pulse profiles and polarization curves,\ntheir temporal variability, the strong Stokes L and weak Stokes V polarization\ncomponents, as well as the fact that linear polarization decreases with\nfrequency and the non-existence of a radius to frequency relationship. We find\nthat the radio emission from the polar cap can produce a diverse range of\nobserved pulsar properties, including single or double peaked profiles. Most of\nthe Stokes V curves from our simulations appear to be antisymmetric, but\nsymmetric curves are also present at some viewing angles. Although the PA swing\nof the radiation from the polar cap can be fitted by the rotating vector model\n(RVM) for most viewing angles, the angles obtained from the RVM do not\ncorrespond to the angular distance of the observer from the magnetic axis.\nInstead, the PA is directly related to the plasma flows in the polar cap and\nnot to the dipole geometry of the magnetic field. The observed range of other\npolarization features, in addition to our results, can be explained by\npropagation effects which are not part of the simulation.\n  Our simulations demonstrate that pair discharges determine the majority of\nits typically observed properties. The usage of RVM for estimations of the\nmagnetic field geometry from observations needs to be reevaluated.",
        "Axion-like particles (ALPs) can be produced in the hot dense plasma of\nfireballs that develop in the initial stage of $\\gamma$-ray burst (GRB)\noutflows. They can transport an enormous amount of energy away from the jet by\npropagating out of the fireball. The photons produced by the eventual decay of\nsuch ALPs do not reach a sufficient density to re-thermalize through pair\nproduction, preventing fireball re-emergence. Thus, the production of heavy\nALPs disrupts the fireball and dims GRBs, allowing bright GRB observations to\nstrongly constrain the existence of heavy ALPs. By adding ALP interactions to\nexisting models of GRB fireballs, we set competitive bounds on the ALP-photon\ncoupling down to $g_{a \\gamma \\gamma} \\sim 4 \\times\n10^{-12}~{\\mathrm{GeV}^{-1}}$ for ALPs in the mass range of 200 MeV - 5 GeV.",
        "TOPCAT and STILTS are related packages for desktop analysis of tabular data,\npresenting GUI and command-line interfaces respectively to much of the same\nfunctionality. This paper presents features in TOPCAT that facilitate use of\nSTILTS.",
        "Medical imaging systems are commonly assessed and optimized by the use of\nobjective measures of image quality (IQ). The performance of the ideal observer\n(IO) acting on imaging measurements has long been advocated as a\nfigure-of-merit to guide the optimization of imaging systems. For computed\nimaging systems, the performance of the IO acting on imaging measurements also\nsets an upper bound on task-performance that no image reconstruction method can\ntranscend. As such, estimation of IO performance can provide valuable guidance\nwhen designing under-sampled data-acquisition techniques by enabling the\nidentification of designs that will not permit the reconstruction of\ndiagnostically inappropriate images for a specified task - no matter how\nadvanced the reconstruction method is or how plausible the reconstructed images\nappear. The need for such analysis is urgent because of the substantial\nincrease of medical device submissions on deep learning-based image\nreconstruction methods and the fact that they may produce clean images\ndisguising the potential loss of diagnostic information when data is\naggressively under-sampled. Recently, convolutional neural network (CNN)\napproximated IOs (CNN-IOs) was investigated for estimating the performance of\ndata space IOs to establish task-based performance bounds for image\nreconstruction, under an X-ray computed tomographic (CT) context. In this work,\nthe application of such data space CNN-IO analysis to multi-coil magnetic\nresonance imaging (MRI) systems has been explored. This study utilized stylized\nmulti-coil sensitivity encoding (SENSE) MRI systems and deep-generated\nstochastic brain models to demonstrate the approach. Signal-known-statistically\nand background-known-statistically (SKS\/BKS) binary signal detection tasks were\nselected to study the impact of different acceleration factors on the data\nspace IO performance.",
        "We discuss and extend some aspects pertaining to the canonical quantisation\nof JT gravity in de Sitter space, including the problem of time and the\nconstruction of a Hilbert space. We then extend this discussion to other two\ndimensional models obtained by changing the dilaton potential and show that the\ncanonical quantisation procedure can be carried out for a large class of such\nmodels. Some discussion leading towards a path integral understanding for\nstates, other than the Hartle Hawking state, is also included here, along with\ncomments pertaining to Holography and the entropy of de Sitter space.",
        "Process theories provide a powerful framework for describing compositional\nstructures across diverse fields, from quantum mechanics to computational\nlinguistics. Traditionally, they have been formalized using symmetric monoidal\ncategories (SMCs). However, various generalizations, including time-neutral,\nhigher-order, and enriched process theories, do not naturally conform to this\nstructure. In this work, we propose an alternative formalization using operad\nalgebras, motivated by recent results connecting SMCs to operadic structures,\nwhich captures a broader class of process theories. By leveraging the\nstring-diagrammatic language, we provide an accessible yet rigorous formulation\nthat unifies and extends traditional process-theoretic approaches. Our operadic\nframework not only recovers standard process theories as a special case but\nalso enables new insights into quantum foundations and compositional\nstructures. This work paves the way for further investigations into the\nalgebraic and operational properties of generalised process theories within an\noperadic setting.",
        "We develop a non-collocated, observer-based output-feedback law for a class\nof continua of linear hyperbolic PDE systems, which are viewed as the continuum\nversion of $n+m$, general heterodirectional hyperbolic systems as $n\\to\\infty$.\nThe design relies on the introduction of a novel, continuum PDE backstepping\ntransformation, which enables the construction of a Lyapunov functional for the\nestimation error system. Stability under the observer-based output-feedback law\nis established by using the Lyapunov functional construction for the estimation\nerror system and proving well-posedness of the complete closed-loop system,\nwhich allows utilization of the separation principle.\n  Motivated by the fact that the continuum-based designs may provide\ncomputationally tractable control laws for large-scale, $n+m$ systems, we then\nutilize the control\/observer kernels and the observer constructed for the\ncontinuum system to introduce an output-feedback control design for the\noriginal $n+m$ system. We establish exponential stability of the resulting\nclosed-loop system, which consists of a mixed $n+m$-continuum PDE system\n(comprising the plant-observer dynamics), introducing a virtual continuum\nsystem with resets, which enables utilization of the continuum approximation\nproperty of the solutions of the $n+m$ system by its continuum counterpart (for\nlarge $n$). We illustrate the potential computational complexity\/flexibility\nbenefits of our approach via a numerical example of stabilization of a\nlarge-scale $n+m$ system, for which we employ the continuum observer-based\ncontroller, while the continuum-based stabilizing control\/observer kernels can\nbe computed in closed form.",
        "The large number of strong lenses discoverable in future astronomical surveys\nwill likely enhance the value of strong gravitational lensing as a cosmic probe\nof dark energy and dark matter. However, leveraging the increased statistical\npower of such large samples will require further development of automated lens\nmodeling techniques. We show that deep learning and simulation-based inference\n(SBI) methods produce informative and reliable estimates of parameter\nposteriors for strong lensing systems in ground-based surveys. We present the\nexamination and comparison of two approaches to lens parameter estimation for\nstrong galaxy-galaxy lenses -- Neural Posterior Estimation (NPE) and Bayesian\nNeural Networks (BNNs). We perform inference on 1-, 5-, and 12-parameter lens\nmodels for ground-based imaging data that mimics the Dark Energy Survey (DES).\nWe find that NPE outperforms BNNs, producing posterior distributions that are\nmore accurate, precise, and well-calibrated for most parameters. For the\n12-parameter NPE model, the calibration is consistently within $<$10\\% of\noptimal calibration for all parameters, while the BNN is rarely within 20\\% of\noptimal calibration for any of the parameters. Similarly, residuals for most of\nthe parameters are smaller (by up to an order of magnitude) with the NPE model\nthan the BNN model. This work takes important steps in the systematic\ncomparison of methods for different levels of model complexity.",
        "We present a simple elementary recursive representation of the so called\nFaulhaber series $\\sum_{k=1}^n k^N$ for integer $n$ and $N$, without reference\nto Bernoulli numbers or polynomials.",
        "Particle Therapy (PT) has emerged as a powerful tool in cancer treatment,\nleveraging the unique dose distribution of charged particles to deliver high\nradiation levels to the tumor while minimizing damage to surrounding healthy\ntissue. Despite its advantages, further improvements in Treatment Planning\nSystems (TPS) are needed to address uncertainties related to fragmentation\nprocess, which can affect both dose deposition and effectiveness. These\nfragmentation effects also play a critical role in Radiation Protection in\nSpace, where astronauts are exposed to high level of radiation, necessitating\nprecise models for shielding optimization. The FOOT (FragmentatiOn Of Target)\nexperiment addresses these challenges by measuring fragmentation cross-section\nwith high precision, providing essential data for improving TPS for PT and\nspace radiation protection strategies. This thesis contributes to the FOOT\nexperiment in two key areas. First, it focuses on the performances of the\nvertex detector, which is responsible for reconstructing particle tracks and\nfragmentation vertexes with high spatial resolution. The study evaluates the\ndetector's reconstruction algorithm and its efficiency to detect particles.\nSecond the thesis present a preliminary calculation of fragmentation cross\nsection, incorporating the vertex detector for the first time in these\nmeasurements.",
        "While cloud-cloud collisions (CCCs) have been proposed as a mechanism for\ntriggering massive star formation, it is suggested that higher collision\nvelocities ($v_{\\rm col}$) and lower GMC mass ($M_{\\rm GMC}$) or\/and density\n($\\Sigma_{\\rm GMC}$) tend to suppress star formation. In this study, we choose\nthe nearby barred galaxy NGC 3627 to examine the SFR and SFE of a colliding GMC\n($m^\\star_{\\rm CCC}$ and $\\epsilon_{\\rm CCC}$) and explore the connections\nbetween $m^\\star_{\\rm CCC}$ and $\\epsilon_{\\rm CCC}$, $M_{\\rm\nGMC}$($\\Sigma_{\\rm GMC}$) and $v_{\\rm col}$, and galactic structures (disk,\nbar, and bar-end). Using ALMA CO(2--1) data (60~pc resolution), we estimated\n$v_{\\rm col}$ within 500~pc apertures, based on line-of-sight GMC velocities,\nassuming random motion in a two-dimensional plane. We extracted apertures where\nat least 0.1 collisions occur per 1 Myr, identifying them as regions dominated\nby CCC-driven star formation, and then calculated $m^\\star_{\\rm CCC}$ and\n$\\epsilon_{\\rm CCC}$ using attenuation-corrected H$\\alpha$ data from VLT MUSE.\nWe found that both $m^\\star_{\\rm CCC}$ and $\\epsilon_{\\rm CCC}$ are lower in\nthe bar (median values: $10^{3.84}~M_\\odot$ and $0.18~\\%$), and higher in the\nbar-end ($10^{4.89}~M_\\odot$ and $1.10~\\%$) compared to the disk\n($10^{4.28}~M_\\odot$ and $0.75~\\%$). Furthermore, we found that structural\ndifferences within the parameter space of $v_{\\rm col}$ and $M_{\\rm\nGMC}$($\\Sigma_{\\rm GMC}$), with higher $M_{\\rm GMC}$($\\Sigma_{\\rm GMC}$) in the\nbar-end and higher $v_{\\rm col}$ in the bar compared to the disk, lead to\nhigher star formation activity in the bar-end and lower activity in the bar.\nOur results support the scenario that variations in CCC properties across\ndifferent galactic structures can explain the observed differences in SFE on a\nkpc scale within a disk galaxy.",
        "Simulations on self-propelling active cubes reveal interesting behaviors at\nboth the individual and the collective level, emphasizing the importance of\ndeveloping experimental analogs that allow to test these theoretical\npredictions. The majority of experimental realizations of active colloidal\ncubes rely on light actuation and or magnetic fields to have a persistent\nactive mechanism, and lack material versatility. Here we propose a system of\nactive bimetallic cubes whose propulsion mechanism is based on a catalytic\nreaction and study their behavior. We realize such a system from synthetic\nsilica cuboids and 3D printed micro cubes, followed by the deposition of gold\nand platinum layers on their surface. We characterize the colloids dynamics for\ndifferent thicknesses of the gold layer at low and high hydrogen peroxide\nconcentrations. We show that the thickness of the base gold layer has only a\nminor effect on the self propulsion speed and in addition induces a\ngravitational torque which leads to particles with a velocity director pointing\nout of the plane thus effectively suppressing propulsion. We find that a higher\nactive force can remedy the effects of torque, resulting in particle\norientations that are favorable for in plane propulsion. Finally, we use 3D\nprinting to compare our results to cubes made from a different material, size\nand roundness, and demonstrate that the speed scaling with increasing particle\nsize originates from the size-dependent drag. Our experiments extend\nfabrication of active cubes to different materials and propulsion mechanisms\nand highlight that the design of active particles with anisotropic shapes\nrequires consideration of the interplay between the shape and activity to\nachieve favorable sedimentation and efficient in plane propulsion.",
        "The unprecedented advancement of artificial intelligence has placed immense\ndemands on computing hardware, but traditional silicon-based semiconductor\ntechnologies are approaching their physical and economic limit, prompting the\nexploration of novel computing paradigms. Memristor offers a promising\nsolution, enabling in-memory analog computation and massive parallelism, which\nleads to low latency and power consumption. This manuscript reviews the current\nstatus of memristor-based machine learning accelerators, highlighting the\nmilestones achieved in developing prototype chips, that not only accelerate\nneural networks inference but also tackle other machine learning tasks. More\nimportantly, it discusses our opinion on current key challenges that remain in\nthis field, such as device variation, the need for efficient peripheral\ncircuitry, and systematic co-design and optimization. We also share our\nperspective on potential future directions, some of which address existing\nchallenges while others explore untouched territories. By addressing these\nchallenges through interdisciplinary efforts spanning device engineering,\ncircuit design, and systems architecture, memristor-based accelerators could\nsignificantly advance the capabilities of AI hardware, particularly for edge\napplications where power efficiency is paramount.",
        "Spreading processes on top of active dynamics provide a novel theoretical\nframework for capturing emerging collective behavior in living systems. I\nconsider run-and-tumble dynamics coupled with coagulation\/decoagulation\nreactions that lead to an absorbing state phase transition. While the active\ndynamics does not change the location of the transition point, the relaxation\ntoward the stationary state depends on motility parameters. Because of the\ncompetition between spreading dynamics and active motion, the system can\nsupport long-living currents whose typical time scale is a nontrivial function\nof motility and reaction rates. Beyond the mean-field regime, instability at\nfinite length scales regulates a crossover from periodic to diffusive modes.\nFinally, it is possible to individuate different mechanisms of pattern\nformation on a large time scale, ranging from Fisher-Kolmogorov to\nKardar-Parisi-Zhang equation.",
        "The Dark Matter (DM) distribution in dwarf galaxies provides crucial insights\ninto both structure formation and the particle nature of DM. GraphNPE (Graph\nNeural Posterior Estimator), first introduced in Nguyen et al. (2023), is a\nnovel simulation-based inference framework that combines graph neural networks\nand normalizing flows to infer the DM density profile from line-of-sight\nstellar velocities. Here, we apply GraphNPE to satellite dwarf galaxies in the\nFIRE-2 Latte simulation suite of Milky Way-mass halos, testing it against both\nCold and Self-Interacting DM scenarios. Our method demonstrates superior\nprecision compared to conventional Jeans-based approaches, recovering DM\ndensity profiles to within the 95% confidence level even in systems with as few\nas 30 tracers. Moreover, we present the first evaluation of mass modeling\nmethods in constraining two key parameters from realistic simulations: the peak\ncircular velocity, $V_\\mathrm{max}$, and the peak virial mass,\n$M_\\mathrm{200m}^\\mathrm{peak}$. Using only line-of-sight velocities, GraphNPE\ncan reliably recover both $V_\\mathrm{max}$ and $M_\\mathrm{200m}^\\mathrm{peak}$\nwithin our quoted uncertainties, including those experiencing tidal effects\n($\\gtrsim$ 63% of systems are recovered with our 68% confidence intervals and\n$\\gtrsim$ 92% within our 95% confidence intervals). The method achieves 10-20%\naccuracy in $V_\\mathrm{max}$ recovery, while $M_\\mathrm{200m}^\\mathrm{peak}$ is\nrecovered to 0.1-0.4 dex accuracy. This work establishes GraphNPE as a robust\ntool for inferring DM density profiles in dwarf galaxies, offering promising\navenues for constraining DM models. The framework's potential extends beyond\nthis study, as it can be adapted to non-spherical and disequilibrium models,\nshowcasing the broader utility of simulation-based inference and graph-based\nlearning in astrophysics.",
        "We present a 3-D morphological and field reconstruction of a coronal mass\nejection (CME) from 2023 November 28, which hits three spacecraft near 1 au:\nWind at Earth's L1 Lagrange point; STEREO-A with a longitudinal separation of\n$6.5^{\\circ}$ west of Earth; and Solar Orbiter (SolO) at $10.7^{\\circ}$ east of\nEarth. The reconstruction assumes a magnetic flux rope (MFR) structure for the\nCME. With this event, we test whether field tracings observed by a spacecraft\npassing near the central axis of a CME MFR (STEREO-A) can be used to\nsuccessfullly predict the field behavior seen by a spacecraft $17^{\\circ}$ away\n(SolO), which has a more grazing encounter with the CME. We find that the MFR\nmodel does have significant success in simultaneously reproducing the field\nsigns and rotations seen at STEREO-A, Wind, and SolO. This provides support for\nthe MFR paradigm for CME structure. However, the SolO measurements, which are\nfarthest from the central axis of the MFR, show less defined MFR signatures,\npresumably due to a greater degree of erosion and degradation of the MFR\nstructure far from its central axis.",
        "In this paper, we consider nonconvex optimization problems with nonlinear\nequality constraints. We assume that the objective function and the functional\nconstraints are locally smooth. To solve this problem, we introduce a\nlinearized $\\ell_q$ penalty based method, where $q \\in (1,2]$ is the parameter\ndefining the norm used in the construction of the penalty function. Our method\ninvolves linearizing the objective function and functional constraints in a\nGauss-Newton fashion at the current iteration in the penalty formulation and\nintroduces a quadratic regularization. This approach yields an easily solvable\nsubproblem, whose solution becomes the next iterate. By using a novel dynamic\nrule for the choice of the regularization parameter, we establish that the\niterates of our method converge to an $\\epsilon$-first-order solution in\n$\\mathcal{O}(1\/{\\epsilon^{2+ (q-1)\/q}})$ outer iterations. Finally, we put\ntheory into practice and evaluate the performance of the proposed algorithm by\nmaking numerical comparisons with existing methods from literature.",
        "We study some explicit Siegel modular forms from Weil representations. For\nthe classical theta group $\\Gamma_m(1,2)$ with $m > 1$, there are some eighth\nroots of unity associated with these modular forms, as noted in the works of\nAndrianov, Friedberg, Maloletkin, Stark, Styer, Richter, and others. We apply\n$2$-cocycles introduced by Rao, Kudla, Perrin, Lion-Vergne, Satake-Takase to\ninvestigate these unities. We extend our study to the full Siegel group\n$\\operatorname{Sp}_{2m}(\\mathbb{Z})$ and obtain two matrix-valued Siegel\nmodular forms from Weil representations; these forms arise from a\nfinite-dimensional representation\n$\\operatorname{Ind}_{\\widetilde{\\Gamma}'_m(1,2)}^{\\widetilde{\\operatorname{Sp}}'_{2m}(\\mathbb{Z})}\n(1_{\\Gamma_m(1,2)} \\cdot \\operatorname{Id}_{\\mu_8})^{-1}$, which is related to\nIgusa's quotient group\n$\\tfrac{\\operatorname{Sp}_{2m}(\\mathbb{Z})}{\\Gamma_m(4,8)}$.",
        "The Quantum Signature Validation Algorithm (QSVA) is introduced as a novel\nquantum-based approach designed to enhance the detection of tampered\ntransactions in blockchain systems. Leveraging the powerful capabilities of\nquantum computing, especially within the framework of transaction-based\nblockchains, the QSVA aims to surpass classical methods in both speed and\nefficiency. By utilizing a quantum walk approach integrated with PageRank-based\nsearch algorithms, QSVA provides a robust mechanism for identifying fraudulent\ntransactions. Our adaptation of the transaction graph representation\nefficiently verifies transactions by maintaining a current set of unspent\ntransaction outputs (UTXOs) characteristic of models like Bitcoin. The QSVA not\nonly amplifies detection efficacy through a quadratic speedup but also\nincorporates two competing quantum search algorithms$-$Quantum SearchRank and\nRandomized SearchRank$-$to explore their effectiveness as foundational\ncomponents. Our results indicate that Randomized SearchRank, in particular,\noutperforms its counterpart in aligning with transaction rankings based on the\nClassical PageRank algorithm, ensuring more consistent detection probabilities.\nThese findings highlight the potential for quantum algorithms to revolutionize\nblockchain security by improving detection times to $O(\\sqrt{N})$. Progress in\nDistributed Ledger Technologies (DLTs) could facilitate future integration of\nquantum solutions into more general distributed systems. As quantum technology\ncontinues to evolve, the QSVA stands as a promising strategy offering\nsignificant advancements in blockchain efficiency and security.",
        "The standard gas mixture for the Resistive Plate Chambers (RPC), composed of\nC2H2F4\/i-C4H10\/SF6, allows the detector operation in avalanche mode, as\nrequired by the high-luminosity collider experiments. The gas density, the low\ncurrent and the comfortable avalanche-streamer separation guarantee high\ndetection efficiency, rate capability and slow detector ageing. The mixture has\na high Global Warming Potential (GWP ~1430), primarily due to the presence of\nC2H2F4. The C2H2F4 and SF6 are not recommended for industrial uses anymore,\nthus their availability will be increasingly difficult over time and the search\nfor an alternative gas mixture is then of absolute priority. CERN is also\ndriving efforts to reduce these gases, as they contribute significantly to the\nLHC greenhouse gas emissions. The thin 1 mm gas gap foreseen for the ATLAS\nupgrade of the latter requires a high-density in order to achieve high\nefficiency, due to the less active target available for the primary ionization.\nThe mixture should also guarantee good timing performance and ensure the\ndetector longevity. In this paper, the results obtained on a RPC operated with\nalternative gas mixtures are shown, following two different approaches. The\nfirst study consists of the replacement of the C2H2F4 with a mixture of\nC3H2F4\/CO2 (GWP~200). The second approach consists in adding a modest fraction\nof CO2 in the standard gas, with the aim to reduce the C2H2F4 emissions. The\npaper provides a detailed study of efficiency, time resolution, and current\nunder different irradiation backgrounds.",
        "We present a recurrent neural network-based approach for ground state\npreparation utilizing mid-circuit measurement and feedback. Unlike previous\nmethods that use machine learning solely as an optimizer, our approach\ndynamically adjusts quantum circuits based on real-time measurement outcomes\nand learns distinct preparation protocols for different Hamiltonians. Notably,\nour machine learning algorithm consistently identifies a state preparation\nstrategy wherein all initial states are first steered toward an intermediate\nstate before transitioning to the target ground state. We demonstrate that\nperformance systematically improves as a larger fraction of ancilla qubits are\nutilized for measurement and feedback, highlighting the efficacy of mid-circuit\nmeasurements in state preparation tasks.",
        "In literature, several algorithms for imaging based on interpolation or\napproximation methods are available. The implementation of theoretical\nprocesses highlighted the necessity of providing theoretical frameworks for the\nconvergence and error estimate analysis to support the experimental setups. In\nthis paper, we establish new techniques for deriving quantitative estimates for\nthe order of approximation for multivariate linear operators of the\npointwise-type, with respect to the $L^p$-norm and to the so-called\ndissimilarity index defined through the continuous SSIM. In particular, we\nconsider a family of approximation operators known as neural network (NN)\noperators, that have been widely studied in the last years in view of their\nconnection with the theory of artificial neural networks. For these operators,\nwe first establish sharp estimates in case of $C^1$ and piecewise (everywhere\ndefined) $C^1$-functions. Then, the case of functions modeling digital images\nis considered, and specific quantitative estimates are achieved, including\nthose with respect to the mentioned dissimilarity index. Moreover, the above\nanalysis has also been extended to $L^p$-spaces, using a new constructive\ntechnique, in which the multivariate averaged modulus of smoothness has been\nemployed. Finally, numerical experiments of image resizing have been given to\nsupport the theoretical results. The accuracy of the proposed algorithm has\nbeen evaluated through similarity indexes such as SSIM, likelihood index\n(S-index) and PSNR, and compared with other rescaling methods, including\nbilinear, bicubic, and upscaling-de la Vall\\'ee-Poussin interpolation (u-VPI).\nNumerical simulations show the effectiveness of the proposed method for image\nprocessing tasks, particularly in terms of the aforementioned SSIM, and are\nconsistent with the provided theoretical analysis.",
        "In this paper, we address hypothesis testing in a distributed network of\nnodes, where each node has only partial information about the State of the\nWorld (SotW) and is tasked with determining which hypothesis, among a given\nset, is most supported by the data available within the node. However, due to\neach node's limited perspective of the SotW, individual nodes cannot reliably\ndetermine the most supported hypothesis independently. To overcome this\nlimitation, nodes must exchange information via an intermediate server. Our\nobjective is to introduce a novel distributed lossy semantic communication\nframework designed to minimize each node's uncertainty about the SotW while\noperating under limited communication budget. In each communication round,\nnodes determine the most content-informative message to send to the server. The\nserver aggregates incoming messages from all nodes, updates its view of the\nSotW, and transmits back the most semantically informative message. We\ndemonstrate that transmitting semantically most informative messages enables\nconvergence toward the true distribution over the state space, improving\ndeductive reasoning performance under communication constraints. For\nexperimental evaluation, we construct a dataset designed for logical deduction\nof hypotheses and compare our approach against random message selection.\nResults validate the effectiveness of our semantic communication framework,\nshowing significant improvements in nodes' understanding of the SotW for\nhypothesis testing, with reduced communication overhead.",
        "This paper is devoted to the study of stochastic optimization problems under\nthe generalized smoothness assumption. By considering the unbiased gradient\noracle in Stochastic Gradient Descent, we provide strategies to achieve the\ndesired accuracy with linear rate. Moreover, in the case of the strong growth\ncondition for smoothness $\\left(L_0 = 0\\right)$, we obtain in the convex setup\nthe iteration complexity: $N = \\mathcal{O}\\left(L_1R \\log\\frac{1}{\\varepsilon}\n+ \\frac{L_1 c R^2}{\\varepsilon}\\right)$ for Clipped Stochastic Gradient Descent\nand $N = \\mathcal{O}\\left(L_1R \\log\\frac{1}{\\varepsilon}\\right)$ for Normalized\nStochastic Gradient Descent. Furthermore, we generalize the convergence results\nto the case with a biased gradient oracle, and show that the power of\n$(L_0,L_1)$-smoothness extends to zero-order algorithms. Finally, we validate\nour theoretical results with a numerical experiment, which has aroused some\ninterest in the machine learning community.",
        "As automated trading gains traction in the financial market, algorithmic\ninvestment strategies are increasingly prominent. While Large Language Models\n(LLMs) and Agent-based models exhibit promising potential in real-time market\nanalysis and trading decisions, they still experience a significant -20% loss\nwhen confronted with rapid declines or frequent fluctuations, impeding their\npractical application. Hence, there is an imperative to explore a more robust\nand resilient framework. This paper introduces an innovative multi-agent\nsystem, HedgeAgents, aimed at bolstering system robustness via ``hedging''\nstrategies. In this well-balanced system, an array of hedging agents has been\ntailored, where HedgeAgents consist of a central fund manager and multiple\nhedging experts specializing in various financial asset classes. These agents\nleverage LLMs' cognitive capabilities to make decisions and coordinate through\nthree types of conferences. Benefiting from the powerful understanding of LLMs,\nour HedgeAgents attained a 70% annualized return and a 400% total return over a\nperiod of 3 years. Moreover, we have observed with delight that HedgeAgents can\neven formulate investment experience comparable to those of human experts\n(https:\/\/hedgeagents.github.io\/)."
      ]
    }
  },
  {
    "id":2411.03389,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Attention Is All You Need",
    "start_abstract":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code"
      ],
      "abstract":[
        "A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores."
      ],
      "categories":[
        "astro-ph.HE"
      ]
    },
    "list":{
      "title":[
        "Investigating Evolving Wormholes in $f(R,T)$ Gravity",
        "Bounded-Confidence Models of Multi-Dimensional Opinions with\n  Topic-Weighted Discordance",
        "Separation of the initial conditions in the inverse problem for 1D\n  non-linear tsunami wave run-up theory",
        "Energy Dispersion, Superconductivity and Magnetic Fluctuations in\n  Stacked Altermagnetism Materials",
        "SpecPT (Spectroscopy Pre-trained Transformer) Model for Extragalactic\n  Spectroscopy: I. Architecture and Automated Redshift Measurement",
        "Time-resolved second-order autocorrelation function of parametric\n  downconversion",
        "FinRLlama: A Solution to LLM-Engineered Signals Challenge at FinRL\n  Contest 2024",
        "Different physical and numerical sources of scatter in the\n  $M_{\\star}$-$M_{\\mathrm{BH}}$ relation and their connection to galaxy\n  evolution",
        "Membrane Charge Effects on Solute Transport in Polyamide Membranes",
        "Cycle Patterns and Mean Payoff Games",
        "Machine Learning-Driven Analytical Models for Threshold Displacement\n  Energy Prediction in Materials",
        "Non-Variational Quantum Random Access Optimization with Alternating\n  Operator Ansatz",
        "Rhizaform algebras",
        "Conical Targets for Enhanced High-Current Positron Sources",
        "Electroweak diboson production in association with a high-mass dijet\n  system in semileptonic final states from $pp$ collisions at $\\sqrt{s} = 13$\n  TeV with the ATLAS detector",
        "Probing the Limit of Heat Transfer in Inorganic Crystals with Deep\n  Learning",
        "A universal preprocessing algorithm of average kernel method with\n  Gauss-Laguerre quadrature for double integrals",
        "P-Order: A Unified Convergence-Analysis Framework for Multivariate\n  Iterative Methods",
        "Lepton flavor violating decays $l_j\\rightarrow l_i\\gamma$, $l_j\n  \\rightarrow 3l_i$ and $\\mu\\rightarrow e+ q\\bar q$ in the N-B-LSSM",
        "The Mass-Angular Momentum Inequality for Multiple Black Holes",
        "FORTE: An Open-Source System for Cost-Effective and Scalable\n  Environmental Monitoring",
        "Visualisation of multi-indication randomised control trial evidence to\n  support decision-making in oncology: a case study on bevacizumab",
        "Weak Error of Dean-Kawasaki Equation with Smooth Mean-Field Interactions",
        "Quantum entanglement of final particle states in the resonant trident\n  pair production in a strong electromagnetic wave",
        "Self-attention-based Diffusion Model for Time-series Imputation in\n  Partial Blackout Scenarios",
        "Structure-preserving and thermodynamically consistent finite element\n  discretization for visco-resistive MHD with thermoelectric effect",
        "Quantitative Flow Approximation Properties of Narrow Neural ODEs",
        "Ceresa Cycles of $X_{0}(N)$",
        "Modeling Cost-Associated Cooperation: A Dilemma of Species Interaction\n  Unveiling New Aspects of Fear Effect"
      ],
      "abstract":[
        "The present work examines whether evolving wormhole solution is possible or\nnot in $f(R,T)$ modified gravity theory. In the background of inhomogeneous\nFLRW type wormhole configuration the field equations are investigated for\ndifferent choices of scale factors and shape functions. For the power law and\nexponential choice of the scale factor from cosmological context and decoupled\npower law of $f(R,T)$ in each variable, wormhole configuration has been\nexamined for two viable choices of shape function. Energy conditions are\nexamined graphically for a range of values of the parameters involved. Finally,\nthe possibility of emergent scenario at early cosmic evolution has been\nexamined.",
        "People's opinions on a wide range of topics often evolve over time through\ntheir interactions with others. Models of opinion dynamics primarily focus on\none-dimensional opinions which represent opinions on one topic. However,\nopinions on various topics are rarely isolated; instead, they can be\ninterdependent and exhibit correlations. In a bounded-confidence model (BCM) of\nopinion dynamics, agents influence each other's opinions only if their opinions\nare sufficiently similar. We extend classical agent-based BCMs -- namely, the\nHegeselmann--Krause BCM, which has synchronous interactions, and the\nDeffuant--Weisbuch BCM, which has asynchronous interactions -- to a\nmultidimensional setting, in which the opinions are multidimensional vectors\nrepresenting opinions of different topics and opinions on different topics are\ninterdependent. To measure opinion differences between agents, we introduce\ntopic-weighted discordance functions that account for opinion differences in\nall topics. We use the regions of receptiveness to characterize the\nsteady-state opinion clusters and provide an analytical approach to compute\nthese regions. In addition, we numerically simulate our models on various\nnetworks with initial opinions drawn from a variety of distributions. When\ninitial opinions are correlated across different topics, our topic-weighted\nBCMs yield significantly different results in both transient and steady states\ncompared to baseline models, where the dynamics of each opinion topic are\nindependent.",
        "We investigate the inverse tsunami wave problem within the framework of the\n1D nonlinear shallow water equations (SWE). Specifically, we focus on\ndetermining the initial displacement $\\eta_0(x)$ and velocity $u_0(x)$ of the\nwave, given the known motion of the shoreline $R(t)$ (the wet\/dry free\nboundary). We demonstrate that for power-shaped inclined bathymetries, this\nproblem admits a complete solution for any $\\eta_0$ and $u_0$, provided the\nwave does not break. In particular, we show that the knowledge of $R(t)$\nenables the unique recovery of both $\\eta_0(x$) and $u_0(x)$ in terms of the\nAbel transform.\n  It is important to note that, in contrast to the direct problem (also known\nas the tsunami wave run-up problem), where $R(t)$ can be computed exactly only\nfor $u_0(x)=0$, our algorithm can recover $\\eta_0$ and $u_0$ exactly for any\nnon-zero $u_0$. This highlights an interesting asymmetry between the direct and\ninverse problems. Our results extend the work presented in\n\\cite{Rybkin23,Rybkin24}, where the inverse problem was solved for $u_0(x)=0$.\nAs in previous work, our approach utilizes the Carrier-Greenspan\ntransformation, which linearizes the SWE for inclined bathymetries. Extensive\nnumerical experiments confirm the efficiency of our algorithms.",
        "Recently, altermagnetism (AM) has emerged as a new category of magnetism,\nalongside conventional antiferromagnetism (AFM) and ferromagnetism (FM). In an\nAM, superconductivity (SC) is faced with a dilemma that the spin-polarized\nbands, induced by the broken time reversal (T ) symmetry, dominantly supports\nspin-triplet pairing. In contrast, AM spin fluctuations routinely facilitate\nspin-singlet pairing as in AFM. Consequently, unconventional SC is either\nabsent or weak in AM materials. Here, we propose that stacking 2D AM materials\ncould resolve this dilemma. Stacked 2D materials have yielded a variety of new\nelectronic properties by altering the symmetries inherent in the monolayer. In\na 2D anisotropic Hubbard model, we investigate the general energy dispersions\nof both single-layer and stacked AM materials. We demonstrate that AM sheet\nstacking can alter the original symmetries, consequently affecting the energy\ndispersion. The interlayer magnetic coupling enhances the low q magnetic\nfluctuations. T symmetry is restored in the AA stacking with an\nantiferromagnetic interlayer coupling, and then both the energy dispersion and\npairing interaction are in favor of spin-singlet SC. The ferromagnetic\ninterlayer coupling in the AB stacking not only recovers T symmetry but also\nsupports spin-triplet pairing. It is further anticipated that twisted bilayer\nAM sheets could exhibit additional novel electronic properties, including\ntopology, flat bands, and collective excitations. Our work illustrates that\nstacking sheets of AM materials could open up a unique research domain in\nexploring novel quantum phenomena and offer a fertile ground for potential\nelectronic applications.",
        "We introduce the Spectroscopy Pre-trained Transformer (SpecPT), a\ntransformer-based model designed to analyze spectroscopic data, with\napplications in spectrum reconstruction and redshift measurement. Using the\nEarly Data Release (EDR) of the DESI survey, we evaluate SpecPT's performance\non two distinct datasets: the Bright Galaxy Survey (BGS) and Emission Line\nGalaxy (ELG) samples. SpecPT successfully reconstructs spectra, accurately\ncapturing emission lines, absorption features, and continuum shapes while\neffectively reducing noise. For redshift prediction, SpecPT achieves\ncompetitive accuracy, with Normalized Median Absolute Deviation (NMAD) values\nof 0.0006 and 0.0008, and catastrophic outlier fractions of 0.20% and 0.80% for\nBGS and ELG, respectively. Notably, SpecPT performs consistently well across\nthe full redshift range ($0 < z < 1.6$), demonstrating its versatility and\nrobustness. By leveraging its learned latent representations, SpecPT lays the\ngroundwork for a foundational spectroscopic model, with potential applications\nin outlier detection, interstellar medium (ISM) property estimation, and\ntransfer learning to other datasets. This work represents a first step in\nbuilding a generalized framework for spectroscopic analysis, capable of scaling\nto the full DESI dataset and beyond.",
        "We study a possibility of measuring the time-resolved second-order\nautocorrelation function of one of two beams generated in type-II parametric\ndownconversion by means of temporal magnification of this beam, bringing its\ncorrelation time from the picosecond to the nanosecond scale, which can be\nresolved by modern photodetectors. We show that such a measurement enables one\nto infer directly the degree of global coherence of that beam, which is linked\nby a simple relation to the number of modes characterizing the entanglement\nbetween the two generated beams. We illustrate the proposed method by an\nexample of photon pairs generated in a periodically poled KTP crystal with a\nsymmetric group velocity matching for various durations of the pump pulse,\nresulting in different numbers of modes. Our theoretical model also shows that\nthe magnified double-heralded autocorrelation function of one beam exhibits a\nlocal maximum around zero delay time, corresponding to photon bunching at a\nshort time scale.",
        "In response to Task II of the FinRL Challenge at ACM ICAIF 2024, this study\nproposes a novel prompt framework for fine-tuning large language models (LLM)\nwith Reinforcement Learning from Market Feedback (RLMF). Our framework\nincorporates market-specific features and short-term price dynamics to generate\nmore precise trading signals. Traditional LLMs, while competent in sentiment\nanalysis, lack contextual alignment for financial market applications. To\nbridge this gap, we fine-tune the LLaMA-3.2-3B-Instruct model using a custom\nRLMF prompt design that integrates historical market data and reward-based\nfeedback. Our evaluation shows that this RLMF-tuned framework outperforms\nbaseline methods in signal consistency and achieving tighter trading outcomes;\nawarded as winner of Task II. You can find the code for this project on GitHub.",
        "Observations have established that the masses of supermassive black holes\n(SMBHs) correlate tightly with the stellar masses of their host galaxies,\nalbeit with substantial scatter. The size of this scatter as a function of\ngalaxy mass and redshift contains valuable information about the origin of\nSMBHs and the physical nature of their co-evolution with galaxies. In this\nwork, we highlight this connection by studying the scatter in the\n$M_{\\mathrm{BH}} - M_{\\star}$ relation for massive galaxies in the Illustris,\nIllustrisTNG (TNG), and EAGLE cosmological simulations. We find that the\nscatter in TNG is significantly lower than in Illustris and EAGLE, reflecting\ntheir different BH feedback models. By performing various numerical\nexperiments, we quantify different contributions to the scatter in the\nsimulations, and also identify a suitably defined intrinsic scatter. The\nintrinsic scatter in Illustris and EAGLE is $\\sim0.3$ dex at $z=0$, and is\ndominated by variations from BH accretion, whereas the smaller scatter of TNG\nis rather dominated by hierarchical merging, suggesting that the massive\ngalaxies in TNG are more tightly quenched. Variations in the BH seed mass can\ncontribute to the scatter of the $M_{\\rm BH}-M_{\\star}$ relation as well, but\nwhether this still plays a role at $z=0$ depends on the feedback model.\nSimulations with disabled AGN feedback produce much higher scatter for low-mass\ngalaxies than seen in our cosmological simulations, demonstrating the crucial\ninfluence of feedback for determining the co-evolution of SMBHs and their host\ngalaxies in this regime. In contrast, an important factor in reducing the\nscatter for massive galaxies is hierarchical merging of mostly quenched\nsystems. Based on our results, we expect that the scatter in the\n$M_{\\mathrm{BH}} - M_{\\star}$ relation at high redshift could be particularly\npowerful in providing clues to the origin of SMBHs.",
        "Polyamide membranes, such as nanofiltration (NF) and reverse osmosis (RO)\nmembranes, are widely used for water desalination and purification. However,\nthe mechanisms of solute transport and solute rejection due to charge\ninteractions remain unclear at the molecular level. Here we use molecular\ndynamics (MD) simulations to examine the transport of single-solute feeds\nthrough charged nanofiltration membranes with different membrane charge\nconcentrations of COO$^{\\text{-}}$ and NH$_2\\!^+$ corresponding to different pH\nlevels. Results show that Na$^+$ and Cl$^{\\text{-}}$ solute ions are better\nrejected when the membrane has a higher concentration of negatively charged\ngroups, corresponding to a higher pH, whereas CaCl$_2$ is well-rejected at all\npH levels studied. These results are consistent with experimental findings\nwhich are performed at the same pH conditions as simulation setup. Moreover,\nsolute transport behavior depends on the membrane functional group\ndistribution. When COO$^{\\text{-}}$ functional groups are concentrated at\nmembrane feed surface, ion permeation into the membrane is reduced.\nCounter-ions tend to associate with charged functional groups while co-ions\nseem to pass by the charged groups more easily. In addition, steric effects\nplay a role when ions of opposite charge cluster in pores of the membrane. This\nstudy reveals solute transport and rejection mechanisms related to membrane\ncharge and provides insights into how membranes might be designed to achieve\nspecific desired solute rejection.",
        "We introduce the concept of a \\emph{cycle pattern} for directed graphs as\nfunctions from the set of cycles to the set $\\{-,0,+\\}$. The key example for\nsuch a pattern is derived from a weight function, giving rise to the sign of\nthe total weight of the edges for each cycle. Hence, cycle patterns describe a\nfundamental structure of a weighted digraph, and they arise naturally in games\non graphs, in particular parity games, mean payoff games, and energy games.\n  Our contribution is threefold: we analyze the structure and derive hardness\nresults for the realization of cycle patterns by weight functions. Then we use\nthem to show hardness of solving games given the limited information of a cycle\npattern. Finally, we identify a novel geometric hardness measure for solving\nmean payoff games (MPG) using the framework of linear decision trees, and use\ncycle patterns to derive lower bounds with respect to this measure, for large\nclasses of algorithms for MPGs.",
        "Understanding the behavior of materials under irradiation is crucial for the\ndesign and safety of nuclear reactors, spacecraft, and other radiation\nenvironments. The threshold displacement energy (Ed) is a critical parameter\nfor understanding radiation damage in materials, yet its determination often\nrelies on costly experiments or simulations. This work leverages the machine\nlearning-based Sure Independence Screening and Sparsifying Operator (SISSO)\nmethod to derive accurate, analytical models for predicting Ed using\nfundamental material properties. The models outperform traditional approaches\nfor monoatomic materials, capturing key trends with high accuracy. While\npredictions for polyatomic materials highlight challenges due to dataset\ncomplexity, they reveal opportunities for improvement with expanded data. This\nstudy identifies cohesive energy and melting temperature as key factors\ninfluencing Ed, offering a robust framework for efficient, data-driven\npredictions of radiation damage in diverse materials.",
        "Solving hard optimization problems is one of the most promising application\ndomains for quantum computers due to the ubiquity of such problems in industry\nand the availability of broadly applicable quantum speedups. However, the\nability of near-term quantum computers to tackle industrial-scale optimization\nproblems is limited by their size and the overheads of quantum error\ncorrection. Quantum Random Access Optimization (QRAO) has been proposed to\nreduce the space requirements of quantum optimization. However, to date QRAO\nhas only been implemented using variational algorithms, which suffer from the\nneed to train instance-specific variational parameters, making them difficult\nto scale. We propose and benchmark a non-variational approach to QRAO based on\nthe Quantum Alternating Operator Ansatz (QAOA) for the MaxCut problem. We show\nthat instance-independent ``fixed'' parameters achieve good performance,\nremoving the need for variational parameter optimization. Additionally, we\nevaluate different design choices, such as various mixers and initial states,\nas well as QAOA operator implementations when customizing for QRAO, and\nidentify a strategy that performs well in practice. Our results pave the way\nfor the practical execution of QRAO on early fault-tolerant quantum computers.",
        "Any anti-associative algebra gives rise to a Jacobi-Jordan algebra by [x, y]\n= xy + yx. This article aims to introduce the concept of \"rhizaform algebras\",\nwhich offer an approach to addressing anti-associativity. These algebras are\ndefined by two operations whose sum is anti-associative, with the left and\nright multiplication operators forming bimodules of the sum of anti-associative\nalgebras. This characterization parallels that of dendriform algebras, where\nthe sum of operations preserves associativity. Additionally, the notions of\nO-operators and Rota-Baxter operators on anti-associative algebras are\npresented as tools to interpret rhizaform algebras. Notably, anti-associative\nalgebras with nondegenerate Connes cocycles admit compatible rhizaform algebra\nstructures.",
        "Previous pair-production-driven positron source designs have assumed that the\ntransverse dimension of the target is significantly greater than the secondary\nbeam it generates. This paper explores the use of targets with different\ntransverse profiles with the aim of enhancing positron production. The starting\npoint of this research is the concept of wire targets, proposed by M. James et\nal. in 1991 for the former SLC positron source. Building on this foundation,\nthis study takes this concept a step further by introducing conical-shaped\ntargets, which can substantially improve the yield by reducing the reabsorption\nof positrons by the target--an issue that is worsened by the high-field\nsolenoid lenses commonly used for positron capture. Using Geant4 simulations,\nwe propose new conical targets adapted for the parameters of the future\ncollider FCC-ee and its positron source test facility P-cubed (PSI Positron\nProduction experiment) at the Paul Scherrer Institute. We find that conical\ntargets can nearly double the positron production at the target and enhance the\nbaseline positron yield of FCC-ee by around 60%. Additionally, we present the\nthermo-mechanical studies for the conical targets based on the FCC-ee primary\nbeam power requirements and outline the mechanical implementation for a future\nproof-of-principle demonstration at the P-cubed facility.",
        "This paper reports the observation of electroweak diboson ($WW\/WZ\/ZZ$)\nproduction in association with a high-mass dijet system, in which final states\nwith one boson decaying leptonically and the other boson decaying hadronically\nare studied. The hadronically decaying $W\/Z$ boson is reconstructed as either\ntwo small-radius jets or one large-radius jet with jet substructure\nrequirements. The data analyzed correspond to an integrated luminosity of 140\nfb$^{-1}$ of proton-proton collisions at a center-of-mass energy of\n$\\sqrt{s}=13$ TeV collected with the ATLAS detector during the 2015-2018 data\ntaking at the Large Hadron Collider. The electroweak production of $WW\/WZ\/ZZ$\nin association with two jets is observed in a phase space dominated by\nvector-boson scattering with a significance of $7.4\\sigma$ (expected\n$6.1\\sigma$) and the signal strength is determined to be\n$1.28^{+0.23}_{-0.21}$. The corresponding production cross section in a\nfiducial phase space is measured in addition. The signal strengths of both\nelectroweak and QCD associated diboson productions are furthermore measured in\na two-dimensional fit, the result of which agrees with the Standard Model\nprediction. The data are interpreted in the context of a dimension-8 effective\nfield theory to probe anomalous quartic gauge couplings resulting in the first\nset of exclusion limits on the Wilson coefficients in the semileptonic channel\nreported by the ATLAS Collaboration. The observed limits for the S02, T0 and M0\noperators are $(-3.96 < f_{S02} \/ \\Lambda^4 < 3.96)$ TeV$^{-4}$, $(-0.25 <\nf_{T0} \/ \\Lambda^4 < 0.22)$ TeV$^{-4}$, $(-1.26 < f_{M0} \/ \\Lambda^4 < 1.25)$\nTeV$^{-4}$.",
        "Heat transfer is a fundamental property of matter. Research spanning decades\nhas attempted to discover materials with exceptional thermal conductivity, yet\nthe upper limit remains unknown. Using deep learning accelerated crystal\nstructure prediction and first-principles calculation, we systematically\nexplore the thermal conductivity landscape of inorganic crystals. We\nbrute-force over half a million ordered crystalline structures, encompassing an\nextensive coverage of local energy minima in binary compounds with up to four\natoms per primitive cell. We confirm diamond sets the upper bound of thermal\nconductivity within our search space, very likely also among all stable\ncrystalline solids at ambient conditions. We identify over 20 novel crystals\nwith high thermal conductivity surpassing silicon at room temperature validated\nby density functional theory. These include a series of metallic compounds,\nespecially MnV, exhibiting high lattice and electronic thermal conductivity\nsimultaneously, a distinctive feature not observed before. The fast deep\nlearning-driven screening method, as well as the large comprehensive thermal\nconductivity database, pave the way for the discovery and design of\nnext-generation materials with tailored thermal properties.",
        "To address the computational challenges posed by nonlinear collision kernels\nin the Smoluchowski equation, this study proposes a universal preprocessing\nalgorithm for the average kernel method based on the Gauss-Laguerre quadrature\nfor double integrals. With this algorithm, the numerical code accurately and\nefficiently determines the pre-exponential factor of the average kernel.\nAdditionally, the exact pre-exponential factors of the four fundamental average\nkernels and their associated truncation error estimations were analyzed. The\nresults demonstrate the reasonability and reliability of the preprocessing\nalgorithm.",
        "We propose P-order (Power-order), a unified, norm-independent framework for\nquantifying the convergence rates of iterative methods. Standard analyses based\non Q-order are norm-dependent and require some uniformity of error reductions\nasymptotically. Although the Ortega--Rheinboldt R-order supports non-uniform\n(including non-monotonic sequences) and is norm-independent, it does not\ndifferentiate among various sublinear rates and has ambiguities for some\nsuperlinear rates. In contrast, our proposed framework parameterizes the\nconvergence rate in direct analogy with asymptotic notation in combinatorial\nalgorithm analysis (including little-$o$, big-$\\Theta$, and little-$\\omega$),\nthereby precisely distinguishing linear, sublinear (e.g., fractional-power),\nand superlinear (e.g., linearithmic) regimes. We also introduce two subclasses\nof P-order: Quasi-Uniform P-order (QUP) and Uniform P-order (UP), and show\ntheir relations with Q-order and R-order. We demonstrate the ease-of-use and\nflexibility of QUP-order by analyzing fixed-point iterations and show that\nP-order can provide tighter bounds on the convergence rate than R-order for\nboth sublinearly and superlinearly convergent cases while avoiding some\nambiguities in R-order. Furthermore, we present a refined analysis of Newton's\nmethod for nonlinear equations with a wide spectrum of sublinear and\nsuperlinear convergence rates (including a newly defined anit-liearithmic\nrate).",
        "The N-B-LSSM is an extension of the minimal supersymmetric standard model\n(MSSM) with the addition of three singlet new Higgs superfields and\nright-handed neutrinos, whose local gauge group is $SU(3)_C\\times SU(2)_L\\times\nU(1)_Y\\times U(1)_{B-L}$. In the N-B-LSSM, we study lepton flavor violating\ndecays $l_j\\rightarrow l_i\\gamma$, $l_j \\rightarrow 3l_i$ and $\\mu\\rightarrow\ne+ q\\bar q$ $(j=\\tau,\\mu,~i=\\mu,e$ and $i\\neq j)$. Based on the current\nexperimental limitations, we carry out detailed parameter scanning and\nnumerical calculations to analyse the effects of different sensitive parameters\non lepton flavor violation (LFV) in the N-B-LSSM. The numerical results show\nthat the non-diagonal elements involving the initial and final leptons are main\nsensitive parameters and LFV sources. This work can provide a strong basis for\nexploring new physics (NP) beyond the Standard Model (SM).",
        "This is the second in a series of two papers to establish the conjectured\nmass-angular momentum inequality for multiple black holes, modulo the extreme\nblack hole 'no hair theorem'. More precisely it is shown that either there is a\ncounterexample to black hole uniqueness, in the form of a regular axisymmetric\nstationary vacuum spacetime with an asymptotically flat end and multiple\ndegenerate horizons which is 'ADM minimizing', or the following statement\nholds. Complete, simply connected, maximal initial data sets for the Einstein\nequations with multiple ends that are either asymptotically flat or\nasymptotically cylindrical, admit an ADM mass lower bound given by the square\nroot of total angular momentum, under the assumption of nonnegative energy\ndensity and axisymmetry. Moreover, equality is achieved in the mass lower bound\nonly for a constant time slice of an extreme Kerr spacetime. The proof is based\non a novel flow of singular harmonic maps with hyperbolic plane target, under\nwhich the renormalized harmonic map energy is monotonically nonincreasing.\nRelevant properties of the flow are achieved through a refined asymptotic\nanalysis of solutions to the harmonic map equations and their linearization.",
        "Forests are an essential part of our biosphere, regulating climate, acting as\na sink for greenhouse gases, and providing numerous other ecosystem services.\nHowever, they are negatively impacted by climatic stressors such as drought or\nheat waves. In this paper, we introduce FORTE, an open-source system for\nenvironmental monitoring with the aim of understanding how forests react to\nsuch stressors. It consists of two key components: (1) a wireless sensor\nnetwork (WSN) deployed in the forest for data collection, and (2) a Data\nInfrastructure for data processing, storage, and visualization. The WSN\ncontains a Central Unit capable of transmitting data to the Data Infrastructure\nvia LTE-M and several spatially independent Satellites that collect data over\nlarge areas and transmit them wirelessly to the Central Unit. Our prototype\ndeployments show that our solution is cost-effective compared to commercial\nsolutions, energy-efficient with sensor nodes lasting for several months on a\nsingle charge, and reliable in terms of data quality. FORTE's flexible\narchitecture makes it suitable for a wide range of environmental monitoring\napplications beyond forest monitoring. The contributions of this paper are\nthree-fold. First, we describe the high-level requirements necessary for\ndeveloping an environmental monitoring system. Second, we present an\narchitecture and prototype implementation of the requirements by introducing\nour FORTE platform and demonstrating its effectiveness through multiple field\ntests. Lastly, we provide source code, documentation, and hardware design\nartifacts as part of our open-source repository.",
        "Background: Evidence maps have been used in healthcare to understand existing\nevidence and to support decision-making. In oncology they have been used to\nsummarise evidence within a disease area but have not been used to compare\nevidence across different diseases. As an increasing number of oncology drugs\nare licensed for multiple indications, visualising the accumulation of evidence\nacross all indications can help inform policy-makers, support evidence\nsynthesis approaches, or to guide expert elicitation on appropriate\ncross-indication assumptions. Methods: The multi-indication oncology therapy\nbevacizumab was selected as a case-study. We used visualisation methods\nincluding timeline, ridgeline and split-violin plots to display evidence across\nseven licensed cancer types, focusing on the evolution of evidence on overall\nand progression-free survival over time as well as the quality of the evidence\navailable. Results: Evidence maps for bevacizumab allow for visualisation of\npatterns in study-level evidence, which can be updated as evidence accumulates\nover time. The developed tools display the observed data and synthesised\nevidence across- and within-indications. Limitations: The effectiveness of the\nplots produced are limited by the lack of complete and consistent reporting of\nevidence in trial reports. Trade-offs were necessary when deciding the level of\ndetail that could be shown while keeping the plots coherent. Conclusions: Clear\ngraphical representations of the evolution and accumulation of evidence can\nprovide a better understanding of the entire evidence base which can inform\njudgements regarding the appropriate use of data within and across indications.\nImplications: Improved visualisations of evidence can help the development of\nmulti-indication evidence synthesis. The proposed evidence displays can lead to\nthe efficient use of information for health technology assessment.",
        "We consider the weak-error rate of the SPDE approximation by regularized\nDean-Kawasaki equation with It\\^o noise for particle systems with mean-field\ninteractions both on the drift and the noise. The global existence and\nuniqueness of the corresponding SPDEs are established using the variational\napproach to SPDEs, and the weak-error rate is estimated using the technique of\nKolmogorov equations on the space of probability measures. In particular, the\nrate derived in this paper coincides with that is the previous work\narXiv:2212.11714, which considered free Brownian particles using Laplace\nduality.",
        "The resonant trident pair production process in the collision of\nultrarelativistic electrons with a strong electromagnetic wave is theoretically\nstudied. Under resonant conditions, the intermediate virtual gamma-quantum\nbecomes real. As a result, the original resonant trident pair production\nprocess effectively splits into two first-order processes by the fine structure\nconstant: the electromagnetic field-stimulated Compton-effect and the\nelectromagnetic field-stimulated Breit-Wheeler process. The kinematics of the\nresonant trident pair production process are studied in detail. It is shown\nthat there are two different cases for the energies and outgoing angles of\nfinal particles (an electron and an electron-positron pair) in which their\nquantum entanglement is realized. In the first case, the energy and outgoing\nangles of final ultrarelativistic particles are uniquely determined by the\nparameters of the electromagnetic field-stimulated Compton-effect (the outgoing\nangle of the final electron and the quantum parameter of the Compton effect).\nIn the second case, the energy and outgoing angles of final particles are\nuniquely determined by the electromagnetic field-stimulated Breit-Wheeler\nprocess (the electron-positron pair outgoing angle and the Breit-Wheeler\nquantum parameter). It is shown that in a sufficiently wide range of\nfrequencies and intensities of a strong electromagnetic wave, and in the case\nof ultrarelativistic initial electrons, the differential probability of the\nresonant trident pair production process with simultaneous registration of the\noutgoing angles of final particles can significantly (by several orders of\nmagnitude) exceed the total probability of the electromagnetic field-stimulated\nCompton-effect.",
        "Missing values in multivariate time series data can harm machine learning\nperformance and introduce bias. These gaps arise from sensor malfunctions,\nblackouts, and human error and are typically addressed by data imputation.\nPrevious work has tackled the imputation of missing data in random, complete\nblackouts and forecasting scenarios. The current paper addresses a more general\nmissing pattern, which we call \"partial blackout,\" where a subset of features\nis missing for consecutive time steps. We introduce a two-stage imputation\nprocess using self-attention and diffusion processes to model feature and\ntemporal correlations. Notably, our model effectively handles missing data\nduring training, enhancing adaptability and ensuring reliable imputation and\nperformance, even with incomplete datasets. Our experiments on benchmark and\ntwo real-world time series datasets demonstrate that our model outperforms the\nstate-of-the-art in partial blackout scenarios and shows better scalability.",
        "We present a structure-preserving and thermodynamically consistent numerical\nscheme for classical magnetohydrodynamics, incorporating viscosity, magnetic\nresistivity, heat transfer, and thermoelectric effect. The governing equations\nare shown to be derived from a generalized Hamilton's principle, with the\nresulting weak formulation being mimicked at the discrete level. The resulting\nnumerical method conserves mass and energy, satisfies Gauss' magnetic law and\nmagnetic helicity balance, and adheres to the Second Law of Thermodynamics, all\nat the fully discrete level. It is shown to perform well on magnetic\nRayleigh-B\\'enard convection.",
        "In this note, we revisit the problem of flow approximation properties of\nneural ordinary differential equations (NODEs). The approximation properties\nhave been considered as a flow controllability problem in recent literature.\nThe neural ODE is considered {\\it narrow} when the parameters have dimension\nequal to the input of the neural network, and hence have limited width. We\nderive the relation of narrow NODEs in approximating flows of shallow but wide\nNODEs. Due to existing results on approximation properties of shallow neural\nnetworks, this facilitates understanding which kind of flows of dynamical\nsystems can be approximated using narrow neural ODEs. While approximation\nproperties of narrow NODEs have been established in literature, the proofs\noften involve extensive constructions or require invoking deep controllability\ntheorems from control theory. In this paper, we provide a simpler proof\ntechnique that involves only ideas from ODEs and Gr{\\\"o}nwall's lemma.\nMoreover, we provide an estimate on the number of switches needed for the time\ndependent weights of the narrow NODE to mimic the behavior of a NODE with a\nsingle layer wide neural network as the velocity field.",
        "The Ceresa cycle is an algebraic 1-cycle on the Jacobian of an algebraic\ncurve. Although it is homologically trivial, Ceresa famously proved that for a\nvery general complex curve of genus at least 3, it is non-trivial in the Chow\ngroup. In this paper we study the Ceresa cycle attached to the complete modular\ncurve $X_{0}(N)$ modulo rational equivalence. For prime level $p$ we give a\ncomplete description, namely we prove that if $X_{0}(p)$ is not hyperelliptic,\nthen its Ceresa cycle is non-torsion. For general level $N$, we prove that\nthere are finitely many $X_{0}(N)$ with torsion Ceresa cycle. Our method relies\non the relationship between the vanishing of the Ceresa cycle and Chow-Heegner\npoints on the Jacobian. We use the geometry and arithmetic of modular Jacobians\nto prove that such points are of infinite order and therefore deduce\nnon-vanishing of the Ceresa cycle.",
        "With limited resources, competition is widespread, yet cooperation persists\nacross taxa, from microorganisms to large mammals. Recent observations reveal\ncontingent factors often drive cooperative interactions, with the intensity\nheterogeneously distributed within species. While cooperation has beneficial\noutcomes, it may also incur significant costs, largely depending on species\ndensity. This creates a dilemma that is pivotal in shaping sustainable\ncooperation strategies. Understanding how cooperation intensity governs the\ncost-benefit balance, and whether an optimal strategy exists for species\nsurvival, is a fundamental question in ecological research, and the focus of\nthis study. We develop a novel mathematical model within the Lotka-Volterra\nframework to explore the dynamics of cost-associated partial cooperation, which\nremains relatively unexplored in ODE model-based studies. Our findings\ndemonstrate that partial cooperation benefits ecosystems up to a certain\nintensity, beyond which costs become dominant, leading to system collapse via\nheteroclinic bifurcation. This outcome captures the cost-cooperation dilemma,\nproviding insights for adopting sustainable strategies and resource management\nfor species survival. We propose a novel mathematical approach to detect and\ntrack heteroclinic orbits in predator-prey systems. Moreover, we show that\nintroducing fear of predation can protect the regime shift, even with a type-I\nfunctional response, challenging traditional ecological views. Although fear is\nknown to resolve the \"paradox of enrichment,\" our results suggest that certain\nlevels of partial cooperation can reestablish this dynamic even at higher fear\nintensity. Finally, we validate the system's dynamical robustness across\nfunctional responses through structural sensitivity analysis."
      ]
    }
  },
  {
    "id":2411.03389,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code",
    "start_abstract":"A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores.",
    "start_categories":[
      "astro-ph.HE"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Attention Is All You Need"
      ],
      "abstract":[
        "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Types of elements in non-commutative Poisson algebras and Dixmier\n  Conjecture",
        "Deep Learning and Foundation Models for Weather Prediction: A Survey",
        "CacheMamba: Popularity Prediction for Mobile Edge Caching Networks via\n  Selective State Spaces",
        "Solving the Catastrophic Forgetting Problem in Generalized Category\n  Discovery",
        "Motion planning for highly-dynamic unconditioned reflexes based on\n  chained Signed Distance Functions",
        "Diffusion-Based Imitation Learning for Social Pose Generation",
        "Hochschild cohomology and extensions of triangulated categories",
        "Provably-Stable Neural Network-Based Control of Nonlinear Systems",
        "Towards a Digital Twin Modeling Method for Container Terminal Port",
        "Multiport Support for Vortex OpenGPU Memory Hierarchy",
        "Anomize: Better Open Vocabulary Video Anomaly Detection",
        "Piano Transcription by Hierarchical Language Modeling with Pretrained\n  Roll-based Encoders",
        "On the limit of random hives with GUE boundary conditions",
        "Transformer Semantic Genetic Programming for Symbolic Regression",
        "Mutation Testing via Iterative Large Language Model-Driven Scientific\n  Debugging",
        "Improving Transducer-Based Spoken Language Understanding with\n  Self-Conditioned CTC and Knowledge Transfer",
        "Simulation of Random LR Fuzzy Intervals",
        "Puntajes m\\'aximos en el juego de domin\\'o",
        "Enhancing Expressive Voice Conversion with Discrete Pitch-Conditioned\n  Flow Matching Model",
        "Navigating Automated Hiring: Perceptions, Strategy Use, and Outcomes\n  Among Young Job Seekers",
        "Community Notes Moderate Engagement With and Diffusion of False\n  Information Online",
        "On the number of cofinalities of cuts in ultraproducts of linear orders",
        "Scalable Sobolev IPM for Probability Measures on a Graph",
        "HeRCULES: Heterogeneous Radar Dataset in Complex Urban Environment for\n  Multi-session Radar SLAM",
        "Route to Chaos and Unified Dynamical Framework of Multi-Species\n  Ecosystems",
        "\"Playing the robot's advocate\": Bystanders' descriptions of a robot's\n  conduct in public settings",
        "Synergizing Deep Learning and Full-Waveform Inversion: Bridging\n  Data-Driven and Theory-Guided Approaches for Enhanced Seismic Imaging",
        "Commonsense Reasoning-Aided Autonomous Vehicle Systems",
        "Class-Conditional Neural Polarizer: A Lightweight and Effective Backdoor\n  Defense by Purifying Poisoned Features"
      ],
      "abstract":[
        "Non-commutative Poisson algebras are the algebras having an associative\nalgebra structure and a Lie algebra structure together with the Leibniz law.\nLet $P$ be a non-commutative Poisson algebra over some algebraically closed\nfield of characteristic zero. For any $z\\in P$, there exist four subalgebras of\n$P$ associated with the inner derivation $ad_z$ on $P$. Based on the\nrelationships between these four subalgebras, elements of $P$ can be divided\ninto eight types. We will mainly focus on two types of non-commutative Poisson\nalgebras: the usual Poisson algebras and the associative algebras with the\ncommutator as the Poisson bracket. The following problems are studied for such\nnon-commutative Poisson algebras: how the type of an element changes under\nhomomorphisms between non-commutative Poisson algebras, how the type of an\nelement changes after localization, and what the type of the elements of the\nform $z_1 \\otimes z_2$ and $z_1 \\otimes 1 + 1 \\otimes z_2$ is in the tensor\nproduct of non-commutative Poisson algebras $P_1\\otimes P_2$. As an application\nof above results, one knows that Dixmier Conjecture for $A_1$ holds under\ncertain conditions. Some properties of the Weyl algebras are also obtained,\nsuch as the commutativity of certain subalgebras.",
        "Physics-based numerical models have been the bedrock of atmospheric sciences\nfor decades, offering robust solutions but often at the cost of significant\ncomputational resources. Deep learning (DL) models have emerged as powerful\ntools in meteorology, capable of analyzing complex weather and climate data by\nlearning intricate dependencies and providing rapid predictions once trained.\nWhile these models demonstrate promising performance in weather prediction,\noften surpassing traditional physics-based methods, they still face critical\nchallenges. This paper presents a comprehensive survey of recent deep learning\nand foundation models for weather prediction. We propose a taxonomy to classify\nexisting models based on their training paradigms: deterministic predictive\nlearning, probabilistic generative learning, and pre-training and fine-tuning.\nFor each paradigm, we delve into the underlying model architectures, address\nmajor challenges, offer key insights, and propose targeted directions for\nfuture research. Furthermore, we explore real-world applications of these\nmethods and provide a curated summary of open-source code repositories and\nwidely used datasets, aiming to bridge research advancements with practical\nimplementations while fostering open and trustworthy scientific practices in\nadopting cutting-edge artificial intelligence for weather prediction. The\nrelated sources are available at https:\/\/github.com\/JimengShi\/\nDL-Foundation-Models-Weather.",
        "Mobile Edge Caching (MEC) plays a pivotal role in mitigating latency in\ndata-intensive services by dynamically caching frequently requested content on\nedge servers. This capability is critical for applications such as Augmented\nReality (AR), Virtual Reality (VR), and Autonomous Vehicles (AV), where\nefficient content caching and accurate popularity prediction are essential for\noptimizing performance. In this paper, we explore the problem of popularity\nprediction in MEC by utilizing historical time-series request data of intended\nfiles, formulating this problem as a ranking task. To this aim, we propose\nCacheMamba model by employing Mamba, a state-space model (SSM)-based\narchitecture, to identify the top-K files with the highest likelihood of being\nrequested. We then benchmark the proposed model against a Transformer-based\napproach, demonstrating its superior performance in terms of cache-hit rate,\nMean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG), and\nFloating-Point Operations Per Second (FLOPS), particularly when dealing with\nlonger sequences.",
        "Generalized Category Discovery (GCD) aims to identify a mix of known and\nnovel categories within unlabeled data sets, providing a more realistic setting\nfor image recognition. Essentially, GCD needs to remember existing patterns\nthoroughly to recognize novel categories. Recent state-of-the-art method SimGCD\ntransfers the knowledge from known-class data to the learning of novel classes\nthrough debiased learning. However, some patterns are catastrophically forgot\nduring adaptation and thus lead to poor performance in novel categories\nclassification. To address this issue, we propose a novel learning approach,\nLegoGCD, which is seamlessly integrated into previous methods to enhance the\ndiscrimination of novel classes while maintaining performance on previously\nencountered known classes. Specifically, we design two types of techniques\ntermed as Local Entropy Regularization (LER) and Dual-views Kullback Leibler\ndivergence constraint (DKL). The LER optimizes the distribution of potential\nknown class samples in unlabeled data, thus ensuring the preservation of\nknowledge related to known categories while learning novel classes. Meanwhile,\nDKL introduces Kullback Leibler divergence to encourage the model to produce a\nsimilar prediction distribution of two view samples from the same image. In\nthis way, it successfully avoids mismatched prediction and generates more\nreliable potential known class samples simultaneously. Extensive experiments\nvalidate that the proposed LegoGCD effectively addresses the known category\nforgetting issue across all datasets, eg, delivering a 7.74% and 2.51% accuracy\nboost on known and novel classes in CUB, respectively. Our code is available\nat: https:\/\/github.com\/Cliffia123\/LegoGCD.",
        "The unconditioned reflex (e.g., protective reflex), which is the innate\nreaction of the organism and usually performed through the spinal cord rather\nthan the brain, can enable organisms to escape harms from environments. In this\npaper, we propose an online, highly-dynamic motion planning algorithm to endow\nmanipulators the highly-dynamic unconditioned reflexes to humans and\/or\nenvironments. Our method is based on a chained version of Signed Distance\nFunctions (SDFs), which can be pre-computed and stored. Our proposed algorithm\nis divided into two stages. In the offline stage, we create 3 groups of local\nSDFs to store the geometric information of the manipulator and its working\nenvironment. In the online stage, the pre-computed local SDFs are chained\ntogether according the configuration of the manipulator, to provide global\ngeometric information about the environment. While the point clouds of the\ndynamic objects serve as query points to look up these local SDFs for quickly\ngenerating escape velocity. Then we propose a modified geometric Jacobian\nmatrix and use the Jacobian-pseudo-inverse method to generate real-time reflex\nbehaviors to avoid the static and dynamic obstacles in the environment. The\nbenefits of our method are validated in both static and dynamic scenarios. In\nthe static scenario, our method identifies the path solutions with lower time\nconsumption and shorter trajectory length compared to existing solutions. In\nthe dynamic scenario, our method can reliably pursue the dynamic target point,\navoid dynamic obstacles, and react to these obstacles within 1ms, which\nsurpasses the unconditioned reflex reaction time of humans.",
        "Intelligent agents, such as robots and virtual agents, must understand the\ndynamics of complex social interactions to interact with humans. Effectively\nrepresenting social dynamics is challenging because we require multi-modal,\nsynchronized observations to understand a scene. We explore how using a single\nmodality, the pose behavior, of multiple individuals in a social interaction\ncan be used to generate nonverbal social cues for the facilitator of that\ninteraction. The facilitator acts to make a social interaction proceed smoothly\nand is an essential role for intelligent agents to replicate in human-robot\ninteractions. In this paper, we adapt an existing diffusion behavior cloning\nmodel to learn and replicate facilitator behaviors. Furthermore, we evaluate\ntwo representations of pose observations from a scene, one representation has\npre-processing applied and one does not. The purpose of this paper is to\nintroduce a new use for diffusion behavior cloning for pose generation in\nsocial interactions. The second is to understand the relationship between\nperformance and computational load for generating social pose behavior using\ntwo different techniques for collecting scene observations. As such, we are\nessentially testing the effectiveness of two different types of conditioning\nfor a diffusion model. We then evaluate the resulting generated behavior from\neach technique using quantitative measures such as mean per-joint position\nerror (MPJPE), training time, and inference time. Additionally, we plot\ntraining and inference time against MPJPE to examine the trade-offs between\nefficiency and performance. Our results suggest that the further pre-processed\ndata can successfully condition diffusion models to generate realistic social\nbehavior, with reasonable trade-offs in accuracy and processing time.",
        "We define a notion of categorical first order deformations for (enhanced)\ntriangulated categories. For a category $\\mathcal{T}$, we show that there is a\nbijection between $\\operatorname{HH}^2(\\mathcal{T})$ and the set of categorical\ndeformations of $\\mathcal{T}$. We show that in the case of curved deformations\nof dg algebras considered in arXiv:2406.04945, the $1$-derived category of the\ndeformation (introduced in arXiv:24020.8660) is a categorical deformation of\nthe derived category of the base; the Hochschild class identified by this\ndeformation is shown to restrict to the class defining the deformation of the\nalgebra. As an application, we give a conceptual proof of the fact that (for a\nsmooth base) the filtered derived category of a dg deformation yields a\ncategorical resolution of the classical derived category.",
        "In recent years, Neural Networks (NNs) have been employed to control\nnonlinear systems due to their potential capability in dealing with situations\nthat might be difficult for conventional nonlinear control schemes. However, to\nthe best of our knowledge, the current literature on NN-based control lacks\ntheoretical guarantees for stability and tracking performance. This precludes\nthe application of NN-based control schemes to systems where stringent\nstability and performance guarantees are required. To address this gap, this\npaper proposes a systematic and comprehensive methodology to design\nprovably-stable NN-based control schemes for affine nonlinear systems. Rigorous\nanalysis is provided to show that the proposed approach guarantees stability of\nthe closed-loop system with the NN in the loop. Also, it is shown that the\nresulting NN-based control scheme ensures that system states asymptotically\nconverge to a neighborhood around the desired equilibrium point, with a tunable\nproximity threshold. The proposed methodology is validated and evaluated via\nsimulation studies on an inverted pendulum and experimental studies on a Parrot\nBebop 2 drone.",
        "This paper introduces a novel strategy aimed at enhancing productivity and\nminimizing non-productive movements within container terminals, specifically\nfocusing on container yards. It advocates for the implementation of a digital\ntwin-based methodology to streamline the operations of stacking cranes (SCs)\nresponsible for container handling. The proposed approach entails the creation\nof a virtual container yard that mirrors the physical yard within a digital\ntwin system, facilitating real-time observation and validation. In addition,\nthis article demonstrates the effectiveness of using a digital twin to reduce\nunproductive movements and improve productivity through simulation. It defines\nvarious operational strategies and takes into account different yard contexts,\nproviding a comprehensive understanding of optimisation possibilities. By\nexploiting the capabilities of the digital twin, managers and operators are\nprovided with crucial information on operational dynamics, enabling them to\nidentify areas for improvement. This visualisation helps decision-makers to\nmake informed choices about their stacking strategies, thereby improving the\nefficiency of overall container terminal operations. Overall, this paper\npresent a digital twin solution in container terminal operations, offering a\npowerful tool for optimising productivity and minimising inefficiencies.",
        "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead.",
        "Open Vocabulary Video Anomaly Detection (OVVAD) seeks to detect and classify\nboth base and novel anomalies. However, existing methods face two specific\nchallenges related to novel anomalies. The first challenge is detection\nambiguity, where the model struggles to assign accurate anomaly scores to\nunfamiliar anomalies. The second challenge is categorization confusion, where\nnovel anomalies are often misclassified as visually similar base instances. To\naddress these challenges, we explore supplementary information from multiple\nsources to mitigate detection ambiguity by leveraging multiple levels of visual\ndata alongside matching textual information. Furthermore, we propose\nincorporating label relations to guide the encoding of new labels, thereby\nimproving alignment between novel videos and their corresponding labels, which\nhelps reduce categorization confusion. The resulting Anomize framework\neffectively tackles these issues, achieving superior performance on UCF-Crime\nand XD-Violence datasets, demonstrating its effectiveness in OVVAD.",
        "Automatic Music Transcription (AMT), aiming to get musical notes from raw\naudio, typically uses frame-level systems with piano-roll outputs or language\nmodel (LM)-based systems with note-level predictions. However, frame-level\nsystems require manual thresholding, while the LM-based systems struggle with\nlong sequences. In this paper, we propose a hybrid method combining pre-trained\nroll-based encoders with an LM decoder to leverage the strengths of both\nmethods. Besides, our approach employs a hierarchical prediction strategy,\nfirst predicting onset and pitch, then velocity, and finally offset. The\nhierarchical prediction strategy reduces computational costs by breaking down\nlong sequences into different hierarchies. Evaluated on two benchmark\nroll-based encoders, our method outperforms traditional piano-roll outputs 0.01\nand 0.022 in onset-offset-velocity F1 score, demonstrating its potential as a\nperformance-enhancing plug-in for arbitrary roll-based music transcription\nencoder.",
        "We show that hives chosen at random with independent GUE boundary conditions\non two sides, weighted by a Vandermonde factor depending on the third side\n(which is necessary in the context of the randomized Horn problem), when\nnormalized so that the eigenvalues at the edge are asymptotically constant,\nconverge in probability to a continuum hive as $n \\rightarrow \\infty.$ It had\npreviously been shown in joint work with Sheffield and Tao \\cite{NST} that the\nvariance of these scaled random hives tends to $0$ and consequently, from\ncompactness, that they converge in probability subsequentially. In the present\npaper, building on \\cite{NST}, we prove convergence in probability to a single\ncontinuum hive, without having to pass to a subsequence. We moreover show that\nthe value at a given point $v$ of this continuum hive equals the supremum of a\ncertain functional acting on asymptotic height functions of lozenge tilings.",
        "In standard genetic programming (stdGP), solutions are varied by modifying\ntheir syntax, with uncertain effects on their semantics. Geometric-semantic\ngenetic programming (GSGP), a popular variant of GP, effectively searches the\nsemantic solution space using variation operations based on linear\ncombinations, although it results in significantly larger solutions. This paper\npresents Transformer Semantic Genetic Programming (TSGP), a novel and flexible\nsemantic approach that uses a generative transformer model as search operator.\nThe transformer is trained on synthetic test problems and learns semantic\nsimilarities between solutions. Once the model is trained, it can be used to\ncreate offspring solutions with high semantic similarity also for unseen and\nunknown problems. Experiments on several symbolic regression problems show that\nTSGP generates solutions with comparable or even significantly better\nprediction quality than stdGP, SLIM_GSGP, DSR, and DAE-GP. Like SLIM_GSGP, TSGP\nis able to create new solutions that are semantically similar without creating\nsolutions of large size. An analysis of the search dynamic reveals that the\nsolutions generated by TSGP are semantically more similar than the solutions\ngenerated by the benchmark approaches allowing a better exploration of the\nsemantic solution space.",
        "Large Language Models (LLMs) can generate plausible test code. Intuitively\nthey generate this by imitating tests seen in their training data, rather than\nreasoning about execution semantics. However, such reasoning is important when\napplying mutation testing, where individual tests need to demonstrate\ndifferences in program behavior between a program and specific artificial\ndefects (mutants). In this paper, we evaluate whether Scientific Debugging,\nwhich has been shown to help LLMs when debugging, can also help them to\ngenerate tests for mutants. In the resulting approach, LLMs form hypotheses\nabout how to kill specific mutants, and then iteratively generate and refine\ntests until they succeed, all with detailed explanations for each step. We\ncompare this method to three baselines: (1) directly asking the LLM to generate\ntests, (2) repeatedly querying the LLM when tests fail, and (3) search-based\ntest generation with Pynguin. Our experiments evaluate these methods based on\nseveral factors, including mutation score, code coverage, success rate, and the\nability to identify equivalent mutants. The results demonstrate that LLMs,\nalthough requiring higher computation cost, consistently outperform Pynguin in\ngenerating tests with better fault detection and coverage. Importantly, we\nobserve that the iterative refinement of test cases is important for achieving\nhigh-quality test suites.",
        "In this paper, we propose to improve end-to-end (E2E) spoken language\nunderstand (SLU) in an RNN transducer model (RNN-T) by incorporating a joint\nself-conditioned CTC automatic speech recognition (ASR) objective. Our proposed\nmodel is akin to an E2E differentiable cascaded model which performs ASR and\nSLU sequentially and we ensure that the SLU task is conditioned on the ASR task\nby having CTC self conditioning. This novel joint modeling of ASR and SLU\nimproves SLU performance significantly over just using SLU optimization. We\nfurther improve the performance by aligning the acoustic embeddings of this\nmodel with the semantically richer BERT model. Our proposed knowledge transfer\nstrategy makes use of a bag-of-entity prediction layer on the aligned\nembeddings and the output of this is used to condition the RNN-T based SLU\ndecoding. These techniques show significant improvement over several strong\nbaselines and can perform at par with large models like Whisper with\nsignificantly fewer parameters.",
        "Random fuzzy variables join the modeling of the impreciseness (due to their\n``fuzzy part'') and randomness. Statistical samples of such objects are widely\nused, and their direct, numerically effective generation is therefore\nnecessary. Usually, these samples consist of triangular or trapezoidal fuzzy\nnumbers. In this paper, we describe theoretical results and simulation\nalgorithms for another family of fuzzy numbers -- LR fuzzy numbers with\ninterval-valued cores. Starting from a simulation perspective on the piecewise\nlinear LR fuzzy numbers with the interval-valued cores, their limiting behavior\nis then considered. This leads us to the numerically efficient algorithm for\nsimulating a sample consisting of such fuzzy values.",
        "In this work, we study the maximum scores that can be achieved in a\nteam-based domino game. Specifically, we show that if the game ends because it\nis blocked, then the maximum score that can be obtained under this assumption\nis 107.\n  --\n  En este trabajo estudiamos los puntajes m\\'aximos que se pueden obtener en\nuna partida, por equipos, en el juego del domin\\'o. Concretamente; nosotros\nmostramos que, si la partida termina porque el juego esta trancado, entonces el\npuntaje m\\'aximo que se puede obtener bajo este supuesto es de 107.",
        "This paper introduces PFlow-VC, a conditional flow matching voice conversion\nmodel that leverages fine-grained discrete pitch tokens and target speaker\nprompt information for expressive voice conversion (VC). Previous VC works\nprimarily focus on speaker conversion, with further exploration needed in\nenhancing expressiveness (such as prosody and emotion) for timbre conversion.\nUnlike previous methods, we adopt a simple and efficient approach to enhance\nthe style expressiveness of voice conversion models. Specifically, we pretrain\na self-supervised pitch VQVAE model to discretize speaker-irrelevant pitch\ninformation and leverage a masked pitch-conditioned flow matching model for\nMel-spectrogram synthesis, which provides in-context pitch modeling\ncapabilities for the speaker conversion model, effectively improving the voice\nstyle transfer capacity. Additionally, we improve timbre similarity by\ncombining global timbre embeddings with time-varying timbre tokens. Experiments\non unseen LibriTTS test-clean and emotional speech dataset ESD show the\nsuperiority of the PFlow-VC model in both timbre conversion and style transfer.\nAudio samples are available on the demo page\nhttps:\/\/speechai-demo.github.io\/PFlow-VC\/.",
        "As the use of automated employment decision tools (AEDTs) has rapidly\nincreased in hiring contexts, especially for computing jobs, there is still\nlimited work on applicants' perceptions of these emerging tools and their\nexperiences navigating them. To investigate, we conducted a survey with 448\ncomputer science students (young, current technology job-seekers) about\nperceptions of the procedural fairness of AEDTs, their willingness to be\nevaluated by different AEDTs, the strategies they use relating to automation in\nthe hiring process, and their job seeking success. We find that young job\nseekers' procedural fairness perceptions of and willingness to be evaluated by\nAEDTs varied with the level of automation involved in the AEDT, the technical\nnature of the task being evaluated, and their own use of strategies, such as\njob referrals. Examining the relationship of their strategies with job\noutcomes, notably, we find that referrals and family household income have\nsignificant and positive impacts on hiring success, while more egalitarian\nstrategies (using free online coding assessment practice or adding keywords to\nresumes) did not. Overall, our work speaks to young job seekers' distrust of\nautomation in hiring contexts, as well as the continued role of social and\nsocioeconomic privilege in job seeking, despite the use of AEDTs that promise\nto make hiring \"unbiased.\"",
        "Social networks scaffold the diffusion of information on social media. Much\nattention has been given to the spread of true vs. false content on online\nsocial platforms, including the structural differences between their diffusion\npatterns. However, much less is known about how platform interventions on false\ncontent alter the engagement with and diffusion of such content. In this work,\nwe estimate the causal effects of Community Notes, a novel fact-checking\nfeature adopted by X (formerly Twitter) to solicit and vet crowd-sourced\nfact-checking notes for false content. We gather detailed time series data for\n40,074 posts for which notes have been proposed and use synthetic control\nmethods to estimate a range of counterfactual outcomes. We find that attaching\nfact-checking notes significantly reduces the engagement with and diffusion of\nfalse content. We estimate that, on average, the notes resulted in reductions\nof 45.7% in reposts, 43.5% in likes, 22.9% in replies, and 14.0% in views after\nbeing attached. Over the posts' entire lifespans, these reductions amount to\n11.4% fewer reposts, 13.0% fewer likes, 7.3% fewer replies, and 5.7% fewer\nviews on average. In reducing reposts, we observe that diffusion cascades for\nfact-checked content are less deep, but not less broad, than synthetic control\nestimates for non-fact-checked content with similar reach. This structural\ndifference contrasts notably with differences between false vs. true content\ndiffusion itself, where false information diffuses farther, but with structural\npatterns that are otherwise indistinguishable from those of true information,\nconditional on reach.",
        "Suppose $\\kappa$ is a regular cardinal and $\\bar a=\\langle \\mu_i: i<\\kappa\n\\rangle$ is a non-decreasing sequence of regular cardinals. We study the set of\npossible cofinalities of cuts Pcut$(\\bar a)=\\{(\\lambda_1, \\lambda_2):$ for some\nultrafilter $D$ on $\\kappa$, $(\\lambda_1, \\lambda_2)$ is the cofinality of a\ncut of $\\prod\\limits_{i<\\kappa} \\mu_i \/ D \\}$.",
        "We investigate the Sobolev IPM problem for probability measures supported on\na graph metric space. Sobolev IPM is an important instance of integral\nprobability metrics (IPM), and is obtained by constraining a critic function\nwithin a unit ball defined by the Sobolev norm. In particular, it has been used\nto compare probability measures and is crucial for several theoretical works in\nmachine learning. However, to our knowledge, there are no efficient algorithmic\napproaches to compute Sobolev IPM effectively, which hinders its practical\napplications. In this work, we establish a relation between Sobolev norm and\nweighted $L^p$-norm, and leverage it to propose a \\emph{novel regularization}\nfor Sobolev IPM. By exploiting the graph structure, we demonstrate that the\nregularized Sobolev IPM provides a \\emph{closed-form} expression for fast\ncomputation. This advancement addresses long-standing computational challenges,\nand paves the way to apply Sobolev IPM for practical applications, even in\nlarge-scale settings. Additionally, the regularized Sobolev IPM is negative\ndefinite. Utilizing this property, we design positive-definite kernels upon the\nregularized Sobolev IPM, and provide preliminary evidences of their advantages\non document classification and topological data analysis for measures on a\ngraph.",
        "Recently, radars have been widely featured in robotics for their robustness\nin challenging weather conditions. Two commonly used radar types are spinning\nradars and phased-array radars, each offering distinct sensor characteristics.\nExisting datasets typically feature only a single type of radar, leading to the\ndevelopment of algorithms limited to that specific kind. In this work, we\nhighlight that combining different radar types offers complementary advantages,\nwhich can be leveraged through a heterogeneous radar dataset. Moreover, this\nnew dataset fosters research in multi-session and multi-robot scenarios where\nrobots are equipped with different types of radars. In this context, we\nintroduce the HeRCULES dataset, a comprehensive, multi-modal dataset with\nheterogeneous radars, FMCW LiDAR, IMU, GPS, and cameras. This is the first\ndataset to integrate 4D radar and spinning radar alongside FMCW LiDAR, offering\nunparalleled localization, mapping, and place recognition capabilities. The\ndataset covers diverse weather and lighting conditions and a range of urban\ntraffic scenarios, enabling a comprehensive analysis across various\nenvironments. The sequence paths with multiple revisits and ground truth pose\nfor each sensor enhance its suitability for place recognition research. We\nexpect the HeRCULES dataset to facilitate odometry, mapping, place recognition,\nand sensor fusion research. The dataset and development tools are available at\nhttps:\/\/sites.google.com\/view\/herculesdataset.",
        "We investigate species-rich mathematical models of ecosystems. Much of the\nexisting literature focuses on the properties of equilibrium fixed points, in\nparticular their stability and feasibility. Here we emphasize the emergence of\nlimit cycles following Hopf bifurcations tuned by the variability of\ninterspecies interaction. As the variability increases, and owing to the large\ndimensionality of the system, limit cycles typically acquire a growing spectrum\nof frequencies. This often leads to the appearance of strange attractors, with\na chaotic dynamics of species abundances characterized by a positive Lyapunov\nexponent. We find that limit cycles and strange attractors preserve\nbiodiversity as they maintain dynamical stability without species extinction.\nWe give numerical evidences that this route to chaos dominates in ecosystems\nwith strong enough interactions and where predator-prey behavior dominates over\ncompetition and mutualism. Based on arguments from random matrix theory, we\nfurther conjecture that this scenario is generic in ecosystems with large\nnumber of species, and identify the key parameters driving it. Overall, our\nwork proposes a unifying framework, where a wide range of population dynamics\nemerge from a single model.",
        "Relying on a large corpus of natural interactions between visitors and a\nrobot in a museum setting, we study a recurrent practice through which humans\n\"worked\" to maintain the robot as a competent participant: the description by\nbystanders, in a way that was made accessible to the main speaker, of the\nsocial action that the robot was taken to be accomplishing. Doing so,\nbystanders maintained the robot's (sometimes incongruous) behaviour as relevant\nto the activity at hand and preserved the robot itself as a competent\nparticipant. Relying on these data, we argue that ex ante definitions of a\nrobot as \"social\" (i.e. before any interaction occurred) run the risk of\nnaturalizing as self-evident the observable result from micro-sociological\nprocesses: namely, the interactional work of co-present humans through which\nthe robot's conduct is reconfigured as contextually relevant.",
        "This review explores the integration of deep learning (DL) with full-waveform\ninversion (FWI) for enhanced seismic imaging and subsurface characterization.\nIt covers FWI and DL fundamentals, geophysical applications (velocity\nestimation, deconvolution, tomography), and challenges (model complexity, data\nquality). The review also outlines future research directions, including\nhybrid, generative, and physics-informed models for improved accuracy,\nefficiency, and reliability in subsurface property estimation. The synergy\nbetween DL and FWI has the potential to transform geophysics, providing new\ninsights into Earth's subsurface.",
        "Autonomous Vehicle (AV) systems have been developed with a strong reliance on\nmachine learning techniques. While machine learning approaches, such as deep\nlearning, are extremely effective at tasks that involve observation and\nclassification, they struggle when it comes to performing higher level\nreasoning about situations on the road. This research involves incorporating\ncommonsense reasoning models that use image data to improve AV systems. This\nwill allow AV systems to perform more accurate reasoning while also making them\nmore adjustable, explainable, and ethical. This paper will discuss the findings\nso far and motivate its direction going forward.",
        "Recent studies have highlighted the vulnerability of deep neural networks to\nbackdoor attacks, where models are manipulated to rely on embedded triggers\nwithin poisoned samples, despite the presence of both benign and trigger\ninformation. While several defense methods have been proposed, they often\nstruggle to balance backdoor mitigation with maintaining benign performance.In\nthis work, inspired by the concept of optical polarizer-which allows light\nwaves of specific polarizations to pass while filtering others-we propose a\nlightweight backdoor defense approach, NPD. This method integrates a neural\npolarizer (NP) as an intermediate layer within the compromised model,\nimplemented as a lightweight linear transformation optimized via bi-level\noptimization. The learnable NP filters trigger information from poisoned\nsamples while preserving benign content. Despite its effectiveness, we identify\nthrough empirical studies that NPD's performance degrades when the target\nlabels (required for purification) are inaccurately estimated. To address this\nlimitation while harnessing the potential of targeted adversarial mitigation,\nwe propose class-conditional neural polarizer-based defense (CNPD). The key\ninnovation is a fusion module that integrates the backdoored model's predicted\nlabel with the features to be purified. This architecture inherently mimics\ntargeted adversarial defense mechanisms without requiring label estimation used\nin NPD. We propose three implementations of CNPD: the first is r-CNPD, which\ntrains a replicated NP layer for each class and, during inference, selects the\nappropriate NP layer for defense based on the predicted class from the\nbackdoored model. To efficiently handle a large number of classes, two variants\nare designed: e-CNPD, which embeds class information as additional features,\nand a-CNPD, which directs network attention using class information."
      ]
    }
  }
]