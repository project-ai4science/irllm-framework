[
  {
    "id":2411.01019,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms",
    "start_abstract":"Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6",
        "b1"
      ],
      "title":[
        "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
        "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
      ],
      "abstract":[
        "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
        "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
      ],
      "categories":[
        "q-bio.CB",
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Multicellular self-organization in Escherichia coli",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Neuroblastoma: nutritional strategies as supportive care in pediatric\n  oncology",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Ultra-high-energy $\\gamma$-ray emission associated with the tail of a\n  bow-shock pulsar wind nebula",
        "A new algorithm for detecting X-ray shots in Cyg X-1",
        "Feedforward Cancellation of High-Frequency Phase Noise in\n  Frequency-Doubled Lasers",
        "Exact Fluctuating Hydrodynamics of the Scaled Light-Heavy Model",
        "Uniqueness of solutions to elliptic and parabolic equations on metric\n  graphs",
        "A sharper Lyapunov-Katz central limit error bound for i.i.d. summands\n  Zolotarev-close to normal",
        "Unitary Friedberg-Jacquet periods and their twists: Fundamental lemmas",
        "Quantum One-Time Memories from Stateless Hardware, Random Access Codes,\n  and Simple Nonconvex Optimization",
        "A monotonicity-based globalization of the level-set method for inclusion\n  detection",
        "Attention-Based Functional-Group Coarse-Graining: A Deep Learning\n  Framework for Molecular Prediction and Design",
        "Separation control applied to the turbulent flow around a NACA4412 wing\n  section",
        "A Cheeger-type inequality for the drift Laplacian with Wentzell-type\n  boundary condition",
        "Complete intersection algebras with binomial Macaulay dual generator",
        "Time derivative estimates for parabolic $p$-Laplace equations and\n  applications to optimal regularity",
        "A family of convolution operators, part two"
      ],
      "abstract":[
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Neuroblastoma, is a highly heterogeneous pediatric tumour and is responsible\nfor 15% of pediatric cancer-related deaths. The clinical outcomes can vary from\nspontaneous regression to high metastatic disease. This extracranial tumour\narises from a neural crest-derived cell and can harbor different phenotypes.\nIts heterogeneity may result from variations in differentiation states\ninfluenced by genetic and epigenetic factors and individual patient\ncharacteristics. This leads downstream to disruption of homeostasis and a\nmetabolic shift in response to the tumour needs. Nutrition can play a key role\nin influencing various aspects of a tumour behaviour. This review provides an\nin-depth exploration of the aetiology of neuroblastoma and the different\navenues of disease progression, which can be targeted with individualized\nnutrition intervention strategies to improve the well-being of children and\noptimize clinical outcomes.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "In this study, we present a comprehensive analysis of an unidentified\npoint-like ultra-high-energy (UHE) $\\gamma$-ray source, designated as 1LHAASO\nJ1740+0948u, situated in the vicinity of the middle-aged pulsar PSR J1740+1000.\nThe detection significance reached 17.1$\\sigma$ (9.4$\\sigma$) above 25$\\,$TeV\n(100$\\,$TeV). The source energy spectrum extended up to 300$\\,$TeV, which was\nwell fitted by a log-parabola function with $N0 = (1.93\\pm0.23) \\times 10^{-16}\n\\rm{TeV^{-1}\\,cm^{-2}\\,s^{-2}}$, $\\alpha = 2.14\\pm0.27$, and $\\beta =\n1.20\\pm0.41$ at E0 = 30$\\,$TeV. The associated pulsar, PSR J1740+1000, resides\nat a high galactic latitude and powers a bow-shock pulsar wind nebula (BSPWN)\nwith an extended X-ray tail. The best-fit position of the gamma-ray source\nappeared to be shifted by $0.2^{\\circ}$ with respect to the pulsar position. As\nthe (i) currently identified pulsar halos do not demonstrate such offsets, and\n(ii) centroid of the gamma-ray emission is approximately located at the\nextension of the X-ray tail, we speculate that the UHE $\\gamma$-ray emission\nmay originate from re-accelerated electron\/positron pairs that are advected\naway in the bow-shock tail.",
        "The short-term X-ray variability of Cyg X-1 can be interpreted as random\noccurrence of mini-flares known as the shots, whose physical nature is still\nunclear. We propose a new algorithm for shot identification in the X-ray light\ncurve, based on baseline detection and template fitting. Compared with previous\ntechniques, our algorithm allows us to detect shots with lower amplitudes and\nshorter time separations. With NICER observations, we find that, after\ncorrection for detection sensitivity, both the shot amplitude and recurrence\nrate are positively scaled with the mean count rate, while the recurrence rate\nhas a much higher dependence on the count rate. These suggest that a higher\nmass accretion rate will drive more and slightly larger shots. We also find\nthat the abrupt hardening near the shot peak found in previous studies is\nattributed to different shot profiles in different energy bands; there is no\nneed to involve a rapid physical process to suddenly harden the emitting\nspectrum.",
        "The cancellation of high-frequency laser phase noise using feedforward\ntechniques, as opposed to feedback methods, has achieved significant\nadvancements in recent years. However, directly applying existing feedforward\ntechniques to laser systems based on nonlinear conversion still faces\nsubstantial challenges. Here, we propose and demonstrate a feedforward scheme\nthat suppresses phase noise in frequency-doubled light by utilizing phase noise\ninformation of its fundamental pump. This scheme is enabled by the fact that\nthe phase jitter of the frequency-doubled light is simply twice that of the\npump, except for a first-order low-pass filtering effect introduced by the SHG\nenhancement cavity. Testing this method on a 420-nm frequency-doubled laser\nsystem, we realize a 25-dB suppression of the servo noise bump near 1 MHz on\nthe 420-nm light, and an average suppression of 30 dB for strong injected noise\nranging from 100 kHz to 20 MHz. This scheme shows promising potential for\napplications requiring blue or ultraviolet light with minimal high-frequency\nphase noise, such as precision control of atoms and molecules.",
        "We study the exact fluctuating hydrodynamics of the scaled Light-Heavy model\n(sLH), in which two species of particles (light and heavy) interact with a\nfluctuating surface. This model is similar in definition to the unscaled\nLight-Heavy model (uLH), except it uses rates scaled with the system size. The\nconsequence, it turns out, is a phase diagram that differs from that of the\nunscaled model. We derive the fluctuating hydrodynamics for this model using an\naction formalism involving the construction of path integrals for the\nprobability of different states that give the complete macroscopic picture\nstarting from the microscopic one. This is then used to obtain the two-point\nsteady-state (static) correlation functions between fluctuations in the two\ndensity fields in the homogeneous phase. We show that these theoretical results\nmatch well with microscopic simulations away from the critical line. We derive\nan exponentially decaying form for the two-point steady-state correlation\nfunction with a correlation length that diverges as the critical line is\napproached. Finally, we also compute the dynamic correlations in the\nhomogeneous phase and use them to determine the relaxation dynamics as well as\nthe dynamic exponents of the system.",
        "We investigate uniqueness of solutions to certain classes of elliptic and\nparabolic equations posed on metric graphs. In particular, we address the\nlinear Schr\\\"odinger equation with a potential, and the heat equation with a\nvariable density. We assume suitable growth conditions on the solutions, which\nare related to the behaviour at infinity of the potential or of the density.",
        "We prove a central limit error bound for convolution powers of laws with\nfinite moments of order $r \\in \\mathopen]2,3\\mathclose]$, taking a closeness of\nthe laws to normality into account. Up to a universal constant, this\ngeneralises the case of $r=3$ of the sharpening of the Berry (1941) - Esseen\n(1942) theorem obtained by Mattner (2024), namely by sharpening here the Katz\n(1963) error bound for the i.i.d. case of Lyapunov's (1901) theorem. Our proof\nuses a partial generalisation of the theorem of Senatov and Zolotarev (1986)\nused for the earlier special case. A result more general than our main one\ncould be obtained by using instead another theorem of Senatov (1980), but\nunfortunately an auxiliary inequality used in the latter's proof is wrong.",
        "We formulate a global conjecture for the automorphic period integral\nassociated to the symmetric pairs defined by unitary groups over number fields,\ngeneralizing a theorem of Waldspurger's toric period for $\\mathrm{GL}(2)$. We\nintroduce a new relative trace formula to prove our global conjecture under\nsome local hypotheses. A new feature is the presence of the relative endoscopy.\nIn this paper we prove the main local theorem: a new relative fundamental lemma\ncomparing certain orbital integrals of functions matched in terms of Hironaka\nand Satake transforms.",
        "We present a construction of one-time memories (OTMs) using\nclassical-accessible stateless hardware, building upon the work of Broadbent et\nal. and Behera et al.. Unlike the aforementioned work, our approach leverages\nquantum random access codes (QRACs) to encode two classical bits, $b_0$ and\n$b_1$, into a single qubit state $\\mathcal{E}(b_0 b_1)$ where the receiver can\nretrieve one of the bits with a certain probability of error. To prove\nsoundness, we define a nonconvex optimization problem over POVMs on\n$\\mathbb{C}^2$. This optimization gives an upper bound on the probability of\ndistinguishing bit $b_{1-\\alpha}$ given that the probability that the receiver\nrecovers bit $b_\\alpha$ is high. Assuming the optimization is sufficiently\naccurate, we then prove soundness against a polynomial number of classical\nqueries to the hardware.",
        "We focus on a geometrical inverse problem that involves recovering\ndiscontinuities in electrical conductivity based on boundary measurements. This\nproblem serves as a model to introduce a shape recovery technique that merges\nthe monotonicity method with the level-set method. The level-set method,\ncommonly used in shape optimization, often relies heavily on the accuracy of\nthe initial guess. To overcome this challenge, we utilize the monotonicity\nmethod to generate a more precise initial guess, which is then used to\ninitialize the level-set method. We provide numerical results to illustrate the\neffectiveness of this combined approach.",
        "Machine learning (ML) offers considerable promise for the design of new\nmolecules and materials. In real-world applications, the design problem is\noften domain-specific, and suffers from insufficient data, particularly labeled\ndata, for ML training. In this study, we report a data-efficient, deep-learning\nframework for molecular discovery that integrates a coarse-grained\nfunctional-group representation with a self-attention mechanism to capture\nintricate chemical interactions. Our approach exploits group-contribution\ntheory to create a graph-based intermediate representation of molecules,\nserving as a low-dimensional embedding that substantially reduces the data\ndemands typically required for training. By leveraging the self-attention\nmechanism to learn subtle chemical context, our method consistently outperforms\nconventional methods in predicting multiple thermophysical properties. In a\ncase study focused on adhesive polymer monomers, we train on a limited dataset\ncomprising just 6,000 unlabeled and 600 labeled monomers. The resulting\nchemistry prediction model achieves over 92% accuracy in forecasting properties\ndirectly from SMILES strings, exceeding the performance of current\nstate-of-the-art techniques. Furthermore, the latent molecular embedding is\ninvertible, allowing the design pipeline to incorporate a decoder that can\nautomatically generate new monomers from the learned chemical subspace. We\nillustrate this functionality by targeting high and low glass transition\ntemperatures ($T_g$), successfully identifying novel candidates whose $T_g$\nextends beyond the range observed in the training data. The ease with which our\ncoarse-grained, attention-based framework navigates both chemical diversity and\ndata scarcity offers a compelling route to accelerate and broaden the search\nfor functional materials.",
        "We carried out high-resolution large-eddy simulations (LESs) to investigate\nthe effects of several separation-control approaches on a NACA4412 wing section\nwith spanwise width of $L_z = 0.6$ at an angle of attack of $AoA=11^{\\circ}$ at\na Reynolds number of $Re_c = 200,000$ based on chord length $c$ and free-stream\nvelocity $U_{\\infty}$. Two control strategies were considered: (1) steady\nuniform blowing and\/or suction on the suction and\/or pressure sides, and (2)\nperiodic control on the suction side. A wide range of control configurations\nwere evaluated in terms of aerodynamic efficiency (i.e., lift-to-drag ratio)\nand separation delay. Uniform blowing and\/or suction effectively delayed flow\nseparation, leading to a lift increase of up to $11\\%$, but yielded only\nmarginal improvements in aerodynamic efficiency. In contrast, periodic control\nneither enhanced separation delay nor improved efficiency. A detailed analysis\nof the interaction between uniform blowing and\/or suction and turbulent\nboundary layers (TBLs) over the wing was performed, including assessments of\n(1) integral boundary-layer quantities, (2) turbulence statistics, and (3)\npower-spectral densities. Significant modifications in Reynolds stresses and\nspectral characteristics were observed. To the authors' best knowledge, this is\nthe first numerical study utilizing high-resolution LESs to provide\ncomprehensive assessments on separation control.",
        "We prove lower bounds for the first non-trivial eigenvalue of the drift\nLaplacian on manifolds with Wentzell-type boundary condition in terms of some\nCheeger-type constants for bulk-boundary interactions. Our results are in the\nspirit of Cheeger's classical inequality.",
        "In this paper, we characterize all Artinian complete intersection\n$K$-algebras $A_F$ whose Macaulay dual generator $F$ is a binomial. In\naddition, we prove that such\n  complete intersection Artinian $K$-algebras $A_F$ satisfy the Strong\nLefschetz property.",
        "We establish the boundedness of time derivatives of solutions to parabolic\n$p$-Laplace equations. Our approach relies on the Bernstein technique combined\nwith a suitable approximation method. As a consequence, we obtain an optimal\nregularity result with a connection to the well-known $C^{p'}$-conjecture in\nthe elliptic setting. Finally, we extend our method to treat global regularity\nresults for both fully nonlinear and general quasilinear degenerate parabolic\nproblems.",
        "We study a family of convolution operators. Their regarding Fourier\nmultipliers are defined in terms of distributions having singularity on the\nlight-cone in $\\mathbb{R}^{n+1}$. As a result, we give a new approach to the\nBochner-Riesz summability."
      ]
    }
  },
  {
    "id":2411.01019,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Screening for lung cancer: 2023 guideline update from the American Cancer Society",
    "start_abstract":"Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
    "start_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
      ],
      "abstract":[
        "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces",
        "SycEval: Evaluating LLM Sycophancy",
        "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D\n  5th Edition Combat",
        "MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science",
        "Data and System Perspectives of Sustainable Artificial Intelligence",
        "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning",
        "Counting and Reasoning with Plans",
        "Monitoring Reasoning Models for Misbehavior and the Risks of Promoting\n  Obfuscation",
        "Small Models Struggle to Learn from Strong Reasoners",
        "Behaviour Discovery and Attribution for Explainable Reinforcement\n  Learning",
        "Exploring the Implementation of AI in Early Onset Interviews to Help\n  Mitigate Bias",
        "Multi-Agent Verification: Scaling Test-Time Compute with Multiple\n  Verifiers",
        "On Benchmarking Human-Like Intelligence in Machines",
        "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The\n  Balancing Act of Selective Unlearning in Large Language Models",
        "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via\n  Cross-Architecture Distillation",
        "Multimodal Dreaming: A Global Workspace Approach to World Model-Based\n  Reinforcement Learning",
        "Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time\n  Series Forecasting",
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "EDM: Efficient Deep Feature Matching",
        "Observation of Subnatural-Linewidth Biphotons In a Two-Level Atomic\n  Ensemble",
        "Fixed-Confidence Best Arm Identification with Decreasing Variance",
        "AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary\n  Task-Oriented Grasping in Clutter",
        "Rethinking LLM Bias Probing Using Lessons from the Social Sciences",
        "EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic\n  Regression on Heterogeneous Database",
        "Global Portraits of Inflation in Nonsingular Variables",
        "Essentially Commuting with a Unitary",
        "Topological modes, non-locality and the double copy",
        "Expansions and restrictions of structures and theories, their\n  hierarchies"
      ],
      "abstract":[
        "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
        "Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications.",
        "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications.",
        "Pre-trained on extensive text and image corpora, current Multi-Modal Large\nLanguage Models (MLLM) have shown strong capabilities in general visual\nreasoning tasks. However, their performance is still lacking in physical\ndomains that require understanding diagrams with complex physical structures\nand quantitative analysis based on multi-modal information. To address this, we\ndevelop a new framework, named Multi-Modal Scientific Reasoning with Physics\nPerception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level\nmulti-modal reasoning task into physical diagram understanding via a Physical\nPerception Model (PPM) and reasoning with physical knowledge via a simulator.\nThe PPM module is obtained by fine-tuning a visual language model using\ncarefully designed synthetic data with paired physical diagrams and\ncorresponding simulation language descriptions. At the inference stage, MAPS\nintegrates the simulation language description of the input diagram provided by\nPPM and results obtained through a Chain-of-Simulation process with MLLM to\nderive the underlying rationale and the final answer. Validated using our\ncollected college-level circuit analysis problems, MAPS significantly improves\nreasoning accuracy of MLLM and outperforms all existing models. The results\nconfirm MAPS offers a promising direction for enhancing multi-modal scientific\nreasoning ability of MLLMs. We will release our code, model and dataset used\nfor our experiments upon publishing of this paper.",
        "Sustainable AI is a subfield of AI for concerning developing and using AI\nsystems in ways of aiming to reduce environmental impact and achieve\nsustainability. Sustainable AI is increasingly important given that training of\nand inference with AI models such as large langrage models are consuming a\nlarge amount of computing power. In this article, we discuss current issues,\nopportunities and example solutions for addressing these issues, and future\nchallenges to tackle, from the data and system perspectives, related to data\nacquisition, data processing, and AI model training and inference.",
        "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems",
        "Classical planning asks for a sequence of operators reaching a given goal.\nWhile the most common case is to compute a plan, many scenarios require more\nthan that. However, quantitative reasoning on the plan space remains mostly\nunexplored. A fundamental problem is to count plans, which relates to the\nconditional probability on the plan space. Indeed, qualitative and quantitative\napproaches are well-established in various other areas of automated reasoning.\nWe present the first study to quantitative and qualitative reasoning on the\nplan space. In particular, we focus on polynomially bounded plans. On the\ntheoretical side, we study its complexity, which gives rise to rich reasoning\nmodes. Since counting is hard in general, we introduce the easier notion of\nfacets, which enables understanding the significance of operators. On the\npractical side, we implement quantitative reasoning for planning. Thereby, we\ntransform a planning task into a propositional formula and use knowledge\ncompilation to count different plans. This framework scales well to large plan\nspaces, while enabling rich reasoning capabilities such as learning pruning\nfunctions and explainable planning.",
        "Mitigating reward hacking--where AI systems misbehave due to flaws or\nmisspecifications in their learning objectives--remains a key challenge in\nconstructing capable and aligned models. We show that we can monitor a frontier\nreasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding\nenvironments by using another LLM that observes the model's chain-of-thought\n(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent\nactions and outputs alone, and we further found that a LLM weaker than o3-mini,\nnamely GPT-4o, can effectively monitor a stronger model. Because CoT monitors\ncan be effective at detecting exploits, it is natural to ask whether those\nexploits can be suppressed by incorporating a CoT monitor directly into the\nagent's training objective. While we show that integrating CoT monitors into\nthe reinforcement learning reward can indeed produce more capable and more\naligned agents in the low optimization regime, we find that with too much\noptimization, agents learn obfuscated reward hacking, hiding their intent\nwithin the CoT while still exhibiting a significant rate of reward hacking.\nBecause it is difficult to tell when CoTs have become obfuscated, it may be\nnecessary to pay a monitorability tax by not applying strong optimization\npressures directly to the chain-of-thought, ensuring that CoTs remain\nmonitorable and useful for detecting misaligned behavior.",
        "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
        "Explaining the decisions made by reinforcement learning (RL) agents is\ncritical for building trust and ensuring reliability in real-world\napplications. Traditional approaches to explainability often rely on saliency\nanalysis, which can be limited in providing actionable insights. Recently,\nthere has been growing interest in attributing RL decisions to specific\ntrajectories within a dataset. However, these methods often generalize\nexplanations to long trajectories, potentially involving multiple distinct\nbehaviors. Often, providing multiple more fine grained explanations would\nimprove clarity. In this work, we propose a framework for behavior discovery\nand action attribution to behaviors in offline RL trajectories. Our method\nidentifies meaningful behavioral segments, enabling more precise and granular\nexplanations associated with high level agent behaviors. This approach is\nadaptable across diverse environments with minimal modifications, offering a\nscalable and versatile solution for behavior discovery and attribution for\nexplainable RL.",
        "This paper investigates the application of artificial intelligence (AI) in\nearly-stage recruitment interviews in order to reduce inherent bias,\nspecifically sentiment bias. Traditional interviewers are often subject to\nseveral biases, including interviewer bias, social desirability effects, and\neven confirmation bias. In turn, this leads to non-inclusive hiring practices,\nand a less diverse workforce. This study further analyzes various AI\ninterventions that are present in the marketplace today such as multimodal\nplatforms and interactive candidate assessment tools in order to gauge the\ncurrent market usage of AI in early-stage recruitment. However, this paper aims\nto use a unique AI system that was developed to transcribe and analyze\ninterview dynamics, which emphasize skill and knowledge over emotional\nsentiments. Results indicate that AI effectively minimizes sentiment-driven\nbiases by 41.2%, suggesting its revolutionizing power in companies' recruitment\nprocesses for improved equity and efficiency.",
        "By utilizing more computational resources at test-time, large language models\n(LLMs) can improve without additional training. One common strategy uses\nverifiers to evaluate candidate outputs. In this work, we propose a novel\nscaling dimension for test-time compute: scaling the number of verifiers. We\nintroduce Multi-Agent Verification (MAV) as a test-time compute paradigm that\ncombines multiple verifiers to improve performance. We propose using Aspect\nVerifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of\noutputs, as one possible choice for the verifiers in a MAV system. AVs are a\nconvenient building block for MAV since they can be easily combined without\nadditional training. Moreover, we introduce BoN-MAV, a simple multi-agent\nverification algorithm that combines best-of-n sampling with multiple\nverifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency\nand reward model verification, and we demonstrate both weak-to-strong\ngeneralization, where combining weak verifiers improves even stronger LLMs, and\nself-improvement, where the same base model is used to both generate and verify\noutputs. Our results establish scaling the number of verifiers as a promising\nnew dimension for improving language model performance at test-time.",
        "Recent benchmark studies have claimed that AI has approached or even\nsurpassed human-level performances on various cognitive tasks. However, this\nposition paper argues that current AI evaluation paradigms are insufficient for\nassessing human-like cognitive capabilities. We identify a set of key\nshortcomings: a lack of human-validated labels, inadequate representation of\nhuman response variability and uncertainty, and reliance on simplified and\necologically-invalid tasks. We support our claims by conducting a human\nevaluation study on ten existing AI benchmarks, suggesting significant biases\nand flaws in task and label designs. To address these limitations, we propose\nfive concrete recommendations for developing future benchmarks that will enable\nmore rigorous and meaningful evaluations of human-like cognitive capacities in\nAI with various implications for such AI applications.",
        "Large Language Models (LLMs) face significant challenges in maintaining\nprivacy, ethics, and compliance, when sensitive or obsolete data must be\nselectively removed. Retraining these models from scratch is computationally\ninfeasible, necessitating efficient alternatives. As part of the SemEval 2025\nTask 4, this work focuses on the application of selective unlearning in LLMs to\naddress this challenge. In this paper, we present our experiments and findings,\nprimarily leveraging global weight modification to achieve an equilibrium\nbetween effectiveness of unlearning, knowledge retention, and target model's\npost-unlearning utility. We also detail the task-specific evaluation mechanism,\nresults, and challenges. Our algorithms have achieved an aggregate score of\n0.409 and 0.389 on the test set for 7B and 1B target models, respectively,\ndemonstrating promising results in verifiable LLM unlearning.",
        "Transformer-based and CNN-based methods demonstrate strong performance in\nlong-term time series forecasting. However, their high computational and\nstorage requirements can hinder large-scale deployment. To address this\nlimitation, we propose integrating lightweight MLP with advanced architectures\nusing knowledge distillation (KD). Our preliminary study reveals different\nmodels can capture complementary patterns, particularly multi-scale and\nmulti-period patterns in the temporal and frequency domains. Based on this\nobservation, we introduce TimeDistill, a cross-architecture KD framework that\ntransfers these patterns from teacher models (e.g., Transformers, CNNs) to MLP.\nAdditionally, we provide a theoretical analysis, demonstrating that our KD\napproach can be interpreted as a specialized form of mixup data augmentation.\nTimeDistill improves MLP performance by up to 18.6%, surpassing teacher models\non eight datasets. It also achieves up to 7X faster inference and requires 130X\nfewer parameters. Furthermore, we conduct extensive evaluations to highlight\nthe versatility and effectiveness of TimeDistill.",
        "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
        "Recent advancements in time series forecasting have explored augmenting\nmodels with text or vision modalities to improve accuracy. While text provides\ncontextual understanding, it often lacks fine-grained temporal details.\nConversely, vision captures intricate temporal patterns but lacks semantic\ncontext, limiting the complementary potential of these modalities. To address\nthis, we propose Time-VLM, a novel multimodal framework that leverages\npre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and\ntextual modalities for enhanced forecasting. Our framework comprises three key\ncomponents: (1) a Retrieval-Augmented Learner, which extracts enriched temporal\nfeatures through memory bank interactions; (2) a Vision-Augmented Learner,\nwhich encodes time series as informative images; and (3) a Text-Augmented\nLearner, which generates contextual textual descriptions. These components\ncollaborate with frozen pre-trained VLMs to produce multimodal embeddings,\nwhich are then fused with temporal features for final prediction. Extensive\nexperiments across diverse datasets demonstrate that Time-VLM achieves superior\nperformance, particularly in few-shot and zero-shot scenarios, thereby\nestablishing a new direction for multimodal time series forecasting.",
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "Recent feature matching methods have achieved remarkable performance but lack\nefficiency consideration. In this paper, we revisit the mainstream\ndetector-free matching pipeline and improve all its stages considering both\naccuracy and efficiency. We propose an Efficient Deep feature Matching network,\nEDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level\nfeatures. Then we present a Correlation Injection Module that conducts feature\ntransformation on high-level deep features, and progressively injects feature\ncorrelations from global to local for efficient multi-scale feature\naggregation, improving both speed and performance. In the refinement stage, a\nnovel lightweight bidirectional axis-based regression head is designed to\ndirectly predict subpixel-level correspondences from latent features, avoiding\nthe significant computational cost of explicitly locating keypoints on\nhigh-resolution local feature heatmaps. Moreover, effective selection\nstrategies are introduced to enhance matching accuracy. Extensive experiments\nshow that our EDM achieves competitive matching accuracy on various benchmarks\nand exhibits excellent efficiency, offering valuable best practices for\nreal-world applications. The code is available at\nhttps:\/\/github.com\/chicleee\/EDM.",
        "Biphotons and single photons with narrow bandwidths and long coherence times\nare essential to the realization of long-distance quantum communication (LDQC)\nand linear optical quantum computing (LOQC). In this Letter, we manipulate the\nbiphoton wave functions of the spontaneous four-wave mixing in a two-level\natomic ensemble with a single-laser pump scheme. Our innovative experimental\napproach enables the generation of biphotons with a sub-MHz bandwidth of 0.36\nMHz, a record spectral brightness of $2.28\\times10^7$${\\rm s}^{-1}{\\rm\nmW}^{-1}{\\rm MHz}^{-1}$, and a temporally symmetric wave packet at moderate\noptical depth. The strong non-classical cross-correlation of the biphotons also\nenables the observation of heralded sub-MHz-linewidth single photons with a\npronounced single-photon nature. The generation of sub-MHz-linewidth biphotons\nand single photons with a two-level atomic ensembles not only finds\napplications in quantum repeaters and large cluster states for LDQC and LOQC\nbut also opens up the opportunity to miniaturize the biphoton or single-photon\nsources for chip-scale quantum technologies.",
        "We focus on the problem of best-arm identification in a stochastic multi-arm\nbandit with temporally decreasing variances for the arms' rewards. We model arm\nrewards as Gaussian random variables with fixed means and variances that\ndecrease with time. The cost incurred by the learner is modeled as a weighted\nsum of the time needed by the learner to identify the best arm, and the number\nof samples of arms collected by the learner before termination. Under this cost\nfunction, there is an incentive for the learner to not sample arms in all\nrounds, especially in the initial rounds. On the other hand, not sampling\nincreases the termination time of the learner, which also increases cost. This\ntrade-off necessitates new sampling strategies. We propose two policies. The\nfirst policy has an initial wait period with no sampling followed by continuous\nsampling. The second policy samples periodically and uses a weighted average of\nthe rewards observed to identify the best arm. We provide analytical guarantees\non the performance of both policies and supplement our theoretical results with\nsimulations which show that our polices outperform the state-of-the-art\npolicies for the classical best arm identification problem.",
        "Inferring the affordance of an object and grasping it in a task-oriented\nmanner is crucial for robots to successfully complete manipulation tasks.\nAffordance indicates where and how to grasp an object by taking its\nfunctionality into account, serving as the foundation for effective\ntask-oriented grasping. However, current task-oriented methods often depend on\nextensive training data that is confined to specific tasks and objects, making\nit difficult to generalize to novel objects and complex scenes. In this paper,\nwe introduce AffordGrasp, a novel open-vocabulary grasping framework that\nleverages the reasoning capabilities of vision-language models (VLMs) for\nin-context affordance reasoning. Unlike existing methods that rely on explicit\ntask and object specifications, our approach infers tasks directly from\nimplicit user instructions, enabling more intuitive and seamless human-robot\ninteraction in everyday scenarios. Building on the reasoning outcomes, our\nframework identifies task-relevant objects and grounds their part-level\naffordances using a visual grounding module. This allows us to generate\ntask-oriented grasp poses precisely within the affordance regions of the\nobject, ensuring both functional and context-aware robotic manipulation.\nExtensive experiments demonstrate that AffordGrasp achieves state-of-the-art\nperformance in both simulation and real-world scenarios, highlighting the\neffectiveness of our method. We believe our approach advances robotic\nmanipulation techniques and contributes to the broader field of embodied AI.\nProject website: https:\/\/eqcy.github.io\/affordgrasp\/.",
        "The proliferation of LLM bias probes introduces three significant challenges:\n(1) we lack principled criteria for choosing appropriate probes, (2) we lack a\nsystem for reconciling conflicting results across probes, and (3) we lack\nformal frameworks for reasoning about when (and why) probe results will\ngeneralize to real user behavior. We address these challenges by systematizing\nLLM social bias probing using actionable insights from social sciences. We then\nintroduce EcoLevels - a framework that helps (a) determine appropriate bias\nprobes, (b) reconcile conflicting findings across probes, and (c) generate\npredictions about bias generalization. Overall, we ground our analysis in\nsocial science research because many LLM probes are direct applications of\nhuman probes, and these fields have faced similar challenges when studying\nsocial bias in humans. Based on our work, we suggest how the next generation of\nLLM bias probing can (and should) benefit from decades of social science\nresearch.",
        "Accurate nonlinear computation is a key challenge in privacy-preserving\nmachine learning (PPML). Most existing frameworks approximate it through linear\noperations, resulting in significant precision loss. This paper proposes an\nefficient, verifiable and accurate security 2-party logistic regression\nframework (EVA-S2PLoR), which achieves accurate nonlinear function computation\nthrough a novel secure element-wise multiplication protocol and its derived\nprotocols. Our framework primarily includes secure 2-party vector element-wise\nmultiplication, addition to multiplication, reciprocal, and sigmoid function\nbased on data disguising technology, where high efficiency and accuracy are\nguaranteed by the simple computation flow based on the real number domain and\nthe few number of fixed communication rounds. We provide secure and robust\nanomaly detection through dimension transformation and Monte Carlo methods.\nEVA-S2PLoR outperforms many advanced frameworks in terms of precision\n(improving the performance of the sigmoid function by about 10 orders of\nmagnitude compared to most frameworks) and delivers the best overall\nperformance in secure logistic regression experiments.",
        "In the phase space perspective, scalar field slow roll inflation is described\nby a heteroclinic orbit from a saddle type fixed point to a final attractive\npoint. In many models the saddle point resides in the scalar field asymptotics,\nand thus for a comprehensive view of the dynamics a global phase portrait is\nnecessary. For this task, in the literature one mostly encounters dynamical\nvariables that either render the initial or the final state singular, thus\nobscuring the full picture. In this work we construct a hybrid set of variables\nwhich allow the depiction of both the initial and final states distinctly in\nnonsingular manner. To illustrate the method, we apply these variables to\nportray various interesting types of scalar field inflationary models like\nmetric Higgs inflation, metric Starobinsky inflation, pole inflation, and a\nnonminimal Palatini model.",
        "Let $R$ be a unitary operator whose spectrum is the circle. We show that the\nset of unitaries $U$ which essentially commute with $R$ (i.e., $[U,R]\\equiv\nUR-RU$ is compact) is path-connected. Moreover, we also calculate the set of\npath-connected components of the orthogonal projections which essentially\ncommute with $R$ and obey a non-triviality condition, and prove it is bijective\nwith $\\mathbb{Z}$.",
        "The double copy connects scattering amplitudes and other objects in gauge and\ngravity theories. Open conceptual issues include whether non-local information\nin gravity theories can be generated from the double copy, and how the double\ncopy should be practically implemented in unseen cases. In this paper, we\nconsider topological theories (with and without mass) in 2+1 dimensions, and\nargue that this makes a useful playground for exploring non-locality. In\nparticular, topological modes of the gauge field arise, themselves associated\nwith non-trivial global behaviour, and it is not clear {\\it a priori} how to\ndouble copy them. We settle this issue, and clarify the role of BCJ shifts in\nmodifying how topological modes contribute. We show how our conclusions apply\nto four- and five-point scattering amplitudes in topological gauge \/ gravity\ntheories, as a by-product obtaining greatly simplified analytic expressions for\nthe four-point amplitudes.",
        "We introduce and study some general principles and hierarchical properties of\nexpansions and restrictions of structures and their theories The general\napproach is applied to describe these properties for classes of\n$\\omega$-categorical theories and structures, Ehrenfeucht theories and their\nmodels, strongly minimal, $\\omega_1$-theories, and stable ones."
      ]
    }
  },
  {
    "id":2411.01019,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects",
    "start_abstract":"Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.",
    "start_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
      ],
      "abstract":[
        "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces",
        "SycEval: Evaluating LLM Sycophancy",
        "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D\n  5th Edition Combat",
        "MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science",
        "Data and System Perspectives of Sustainable Artificial Intelligence",
        "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning",
        "Counting and Reasoning with Plans",
        "Monitoring Reasoning Models for Misbehavior and the Risks of Promoting\n  Obfuscation",
        "Small Models Struggle to Learn from Strong Reasoners",
        "Behaviour Discovery and Attribution for Explainable Reinforcement\n  Learning",
        "Exploring the Implementation of AI in Early Onset Interviews to Help\n  Mitigate Bias",
        "Multi-Agent Verification: Scaling Test-Time Compute with Multiple\n  Verifiers",
        "On Benchmarking Human-Like Intelligence in Machines",
        "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The\n  Balancing Act of Selective Unlearning in Large Language Models",
        "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via\n  Cross-Architecture Distillation",
        "Multimodal Dreaming: A Global Workspace Approach to World Model-Based\n  Reinforcement Learning",
        "Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time\n  Series Forecasting",
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "EDM: Efficient Deep Feature Matching",
        "Observation of Subnatural-Linewidth Biphotons In a Two-Level Atomic\n  Ensemble",
        "Fixed-Confidence Best Arm Identification with Decreasing Variance",
        "AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary\n  Task-Oriented Grasping in Clutter",
        "Rethinking LLM Bias Probing Using Lessons from the Social Sciences",
        "EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic\n  Regression on Heterogeneous Database",
        "Global Portraits of Inflation in Nonsingular Variables",
        "Essentially Commuting with a Unitary",
        "Topological modes, non-locality and the double copy",
        "Expansions and restrictions of structures and theories, their\n  hierarchies"
      ],
      "abstract":[
        "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
        "Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications.",
        "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications.",
        "Pre-trained on extensive text and image corpora, current Multi-Modal Large\nLanguage Models (MLLM) have shown strong capabilities in general visual\nreasoning tasks. However, their performance is still lacking in physical\ndomains that require understanding diagrams with complex physical structures\nand quantitative analysis based on multi-modal information. To address this, we\ndevelop a new framework, named Multi-Modal Scientific Reasoning with Physics\nPerception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level\nmulti-modal reasoning task into physical diagram understanding via a Physical\nPerception Model (PPM) and reasoning with physical knowledge via a simulator.\nThe PPM module is obtained by fine-tuning a visual language model using\ncarefully designed synthetic data with paired physical diagrams and\ncorresponding simulation language descriptions. At the inference stage, MAPS\nintegrates the simulation language description of the input diagram provided by\nPPM and results obtained through a Chain-of-Simulation process with MLLM to\nderive the underlying rationale and the final answer. Validated using our\ncollected college-level circuit analysis problems, MAPS significantly improves\nreasoning accuracy of MLLM and outperforms all existing models. The results\nconfirm MAPS offers a promising direction for enhancing multi-modal scientific\nreasoning ability of MLLMs. We will release our code, model and dataset used\nfor our experiments upon publishing of this paper.",
        "Sustainable AI is a subfield of AI for concerning developing and using AI\nsystems in ways of aiming to reduce environmental impact and achieve\nsustainability. Sustainable AI is increasingly important given that training of\nand inference with AI models such as large langrage models are consuming a\nlarge amount of computing power. In this article, we discuss current issues,\nopportunities and example solutions for addressing these issues, and future\nchallenges to tackle, from the data and system perspectives, related to data\nacquisition, data processing, and AI model training and inference.",
        "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems",
        "Classical planning asks for a sequence of operators reaching a given goal.\nWhile the most common case is to compute a plan, many scenarios require more\nthan that. However, quantitative reasoning on the plan space remains mostly\nunexplored. A fundamental problem is to count plans, which relates to the\nconditional probability on the plan space. Indeed, qualitative and quantitative\napproaches are well-established in various other areas of automated reasoning.\nWe present the first study to quantitative and qualitative reasoning on the\nplan space. In particular, we focus on polynomially bounded plans. On the\ntheoretical side, we study its complexity, which gives rise to rich reasoning\nmodes. Since counting is hard in general, we introduce the easier notion of\nfacets, which enables understanding the significance of operators. On the\npractical side, we implement quantitative reasoning for planning. Thereby, we\ntransform a planning task into a propositional formula and use knowledge\ncompilation to count different plans. This framework scales well to large plan\nspaces, while enabling rich reasoning capabilities such as learning pruning\nfunctions and explainable planning.",
        "Mitigating reward hacking--where AI systems misbehave due to flaws or\nmisspecifications in their learning objectives--remains a key challenge in\nconstructing capable and aligned models. We show that we can monitor a frontier\nreasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding\nenvironments by using another LLM that observes the model's chain-of-thought\n(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent\nactions and outputs alone, and we further found that a LLM weaker than o3-mini,\nnamely GPT-4o, can effectively monitor a stronger model. Because CoT monitors\ncan be effective at detecting exploits, it is natural to ask whether those\nexploits can be suppressed by incorporating a CoT monitor directly into the\nagent's training objective. While we show that integrating CoT monitors into\nthe reinforcement learning reward can indeed produce more capable and more\naligned agents in the low optimization regime, we find that with too much\noptimization, agents learn obfuscated reward hacking, hiding their intent\nwithin the CoT while still exhibiting a significant rate of reward hacking.\nBecause it is difficult to tell when CoTs have become obfuscated, it may be\nnecessary to pay a monitorability tax by not applying strong optimization\npressures directly to the chain-of-thought, ensuring that CoTs remain\nmonitorable and useful for detecting misaligned behavior.",
        "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
        "Explaining the decisions made by reinforcement learning (RL) agents is\ncritical for building trust and ensuring reliability in real-world\napplications. Traditional approaches to explainability often rely on saliency\nanalysis, which can be limited in providing actionable insights. Recently,\nthere has been growing interest in attributing RL decisions to specific\ntrajectories within a dataset. However, these methods often generalize\nexplanations to long trajectories, potentially involving multiple distinct\nbehaviors. Often, providing multiple more fine grained explanations would\nimprove clarity. In this work, we propose a framework for behavior discovery\nand action attribution to behaviors in offline RL trajectories. Our method\nidentifies meaningful behavioral segments, enabling more precise and granular\nexplanations associated with high level agent behaviors. This approach is\nadaptable across diverse environments with minimal modifications, offering a\nscalable and versatile solution for behavior discovery and attribution for\nexplainable RL.",
        "This paper investigates the application of artificial intelligence (AI) in\nearly-stage recruitment interviews in order to reduce inherent bias,\nspecifically sentiment bias. Traditional interviewers are often subject to\nseveral biases, including interviewer bias, social desirability effects, and\neven confirmation bias. In turn, this leads to non-inclusive hiring practices,\nand a less diverse workforce. This study further analyzes various AI\ninterventions that are present in the marketplace today such as multimodal\nplatforms and interactive candidate assessment tools in order to gauge the\ncurrent market usage of AI in early-stage recruitment. However, this paper aims\nto use a unique AI system that was developed to transcribe and analyze\ninterview dynamics, which emphasize skill and knowledge over emotional\nsentiments. Results indicate that AI effectively minimizes sentiment-driven\nbiases by 41.2%, suggesting its revolutionizing power in companies' recruitment\nprocesses for improved equity and efficiency.",
        "By utilizing more computational resources at test-time, large language models\n(LLMs) can improve without additional training. One common strategy uses\nverifiers to evaluate candidate outputs. In this work, we propose a novel\nscaling dimension for test-time compute: scaling the number of verifiers. We\nintroduce Multi-Agent Verification (MAV) as a test-time compute paradigm that\ncombines multiple verifiers to improve performance. We propose using Aspect\nVerifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of\noutputs, as one possible choice for the verifiers in a MAV system. AVs are a\nconvenient building block for MAV since they can be easily combined without\nadditional training. Moreover, we introduce BoN-MAV, a simple multi-agent\nverification algorithm that combines best-of-n sampling with multiple\nverifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency\nand reward model verification, and we demonstrate both weak-to-strong\ngeneralization, where combining weak verifiers improves even stronger LLMs, and\nself-improvement, where the same base model is used to both generate and verify\noutputs. Our results establish scaling the number of verifiers as a promising\nnew dimension for improving language model performance at test-time.",
        "Recent benchmark studies have claimed that AI has approached or even\nsurpassed human-level performances on various cognitive tasks. However, this\nposition paper argues that current AI evaluation paradigms are insufficient for\nassessing human-like cognitive capabilities. We identify a set of key\nshortcomings: a lack of human-validated labels, inadequate representation of\nhuman response variability and uncertainty, and reliance on simplified and\necologically-invalid tasks. We support our claims by conducting a human\nevaluation study on ten existing AI benchmarks, suggesting significant biases\nand flaws in task and label designs. To address these limitations, we propose\nfive concrete recommendations for developing future benchmarks that will enable\nmore rigorous and meaningful evaluations of human-like cognitive capacities in\nAI with various implications for such AI applications.",
        "Large Language Models (LLMs) face significant challenges in maintaining\nprivacy, ethics, and compliance, when sensitive or obsolete data must be\nselectively removed. Retraining these models from scratch is computationally\ninfeasible, necessitating efficient alternatives. As part of the SemEval 2025\nTask 4, this work focuses on the application of selective unlearning in LLMs to\naddress this challenge. In this paper, we present our experiments and findings,\nprimarily leveraging global weight modification to achieve an equilibrium\nbetween effectiveness of unlearning, knowledge retention, and target model's\npost-unlearning utility. We also detail the task-specific evaluation mechanism,\nresults, and challenges. Our algorithms have achieved an aggregate score of\n0.409 and 0.389 on the test set for 7B and 1B target models, respectively,\ndemonstrating promising results in verifiable LLM unlearning.",
        "Transformer-based and CNN-based methods demonstrate strong performance in\nlong-term time series forecasting. However, their high computational and\nstorage requirements can hinder large-scale deployment. To address this\nlimitation, we propose integrating lightweight MLP with advanced architectures\nusing knowledge distillation (KD). Our preliminary study reveals different\nmodels can capture complementary patterns, particularly multi-scale and\nmulti-period patterns in the temporal and frequency domains. Based on this\nobservation, we introduce TimeDistill, a cross-architecture KD framework that\ntransfers these patterns from teacher models (e.g., Transformers, CNNs) to MLP.\nAdditionally, we provide a theoretical analysis, demonstrating that our KD\napproach can be interpreted as a specialized form of mixup data augmentation.\nTimeDistill improves MLP performance by up to 18.6%, surpassing teacher models\non eight datasets. It also achieves up to 7X faster inference and requires 130X\nfewer parameters. Furthermore, we conduct extensive evaluations to highlight\nthe versatility and effectiveness of TimeDistill.",
        "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
        "Recent advancements in time series forecasting have explored augmenting\nmodels with text or vision modalities to improve accuracy. While text provides\ncontextual understanding, it often lacks fine-grained temporal details.\nConversely, vision captures intricate temporal patterns but lacks semantic\ncontext, limiting the complementary potential of these modalities. To address\nthis, we propose Time-VLM, a novel multimodal framework that leverages\npre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and\ntextual modalities for enhanced forecasting. Our framework comprises three key\ncomponents: (1) a Retrieval-Augmented Learner, which extracts enriched temporal\nfeatures through memory bank interactions; (2) a Vision-Augmented Learner,\nwhich encodes time series as informative images; and (3) a Text-Augmented\nLearner, which generates contextual textual descriptions. These components\ncollaborate with frozen pre-trained VLMs to produce multimodal embeddings,\nwhich are then fused with temporal features for final prediction. Extensive\nexperiments across diverse datasets demonstrate that Time-VLM achieves superior\nperformance, particularly in few-shot and zero-shot scenarios, thereby\nestablishing a new direction for multimodal time series forecasting.",
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "Recent feature matching methods have achieved remarkable performance but lack\nefficiency consideration. In this paper, we revisit the mainstream\ndetector-free matching pipeline and improve all its stages considering both\naccuracy and efficiency. We propose an Efficient Deep feature Matching network,\nEDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level\nfeatures. Then we present a Correlation Injection Module that conducts feature\ntransformation on high-level deep features, and progressively injects feature\ncorrelations from global to local for efficient multi-scale feature\naggregation, improving both speed and performance. In the refinement stage, a\nnovel lightweight bidirectional axis-based regression head is designed to\ndirectly predict subpixel-level correspondences from latent features, avoiding\nthe significant computational cost of explicitly locating keypoints on\nhigh-resolution local feature heatmaps. Moreover, effective selection\nstrategies are introduced to enhance matching accuracy. Extensive experiments\nshow that our EDM achieves competitive matching accuracy on various benchmarks\nand exhibits excellent efficiency, offering valuable best practices for\nreal-world applications. The code is available at\nhttps:\/\/github.com\/chicleee\/EDM.",
        "Biphotons and single photons with narrow bandwidths and long coherence times\nare essential to the realization of long-distance quantum communication (LDQC)\nand linear optical quantum computing (LOQC). In this Letter, we manipulate the\nbiphoton wave functions of the spontaneous four-wave mixing in a two-level\natomic ensemble with a single-laser pump scheme. Our innovative experimental\napproach enables the generation of biphotons with a sub-MHz bandwidth of 0.36\nMHz, a record spectral brightness of $2.28\\times10^7$${\\rm s}^{-1}{\\rm\nmW}^{-1}{\\rm MHz}^{-1}$, and a temporally symmetric wave packet at moderate\noptical depth. The strong non-classical cross-correlation of the biphotons also\nenables the observation of heralded sub-MHz-linewidth single photons with a\npronounced single-photon nature. The generation of sub-MHz-linewidth biphotons\nand single photons with a two-level atomic ensembles not only finds\napplications in quantum repeaters and large cluster states for LDQC and LOQC\nbut also opens up the opportunity to miniaturize the biphoton or single-photon\nsources for chip-scale quantum technologies.",
        "We focus on the problem of best-arm identification in a stochastic multi-arm\nbandit with temporally decreasing variances for the arms' rewards. We model arm\nrewards as Gaussian random variables with fixed means and variances that\ndecrease with time. The cost incurred by the learner is modeled as a weighted\nsum of the time needed by the learner to identify the best arm, and the number\nof samples of arms collected by the learner before termination. Under this cost\nfunction, there is an incentive for the learner to not sample arms in all\nrounds, especially in the initial rounds. On the other hand, not sampling\nincreases the termination time of the learner, which also increases cost. This\ntrade-off necessitates new sampling strategies. We propose two policies. The\nfirst policy has an initial wait period with no sampling followed by continuous\nsampling. The second policy samples periodically and uses a weighted average of\nthe rewards observed to identify the best arm. We provide analytical guarantees\non the performance of both policies and supplement our theoretical results with\nsimulations which show that our polices outperform the state-of-the-art\npolicies for the classical best arm identification problem.",
        "Inferring the affordance of an object and grasping it in a task-oriented\nmanner is crucial for robots to successfully complete manipulation tasks.\nAffordance indicates where and how to grasp an object by taking its\nfunctionality into account, serving as the foundation for effective\ntask-oriented grasping. However, current task-oriented methods often depend on\nextensive training data that is confined to specific tasks and objects, making\nit difficult to generalize to novel objects and complex scenes. In this paper,\nwe introduce AffordGrasp, a novel open-vocabulary grasping framework that\nleverages the reasoning capabilities of vision-language models (VLMs) for\nin-context affordance reasoning. Unlike existing methods that rely on explicit\ntask and object specifications, our approach infers tasks directly from\nimplicit user instructions, enabling more intuitive and seamless human-robot\ninteraction in everyday scenarios. Building on the reasoning outcomes, our\nframework identifies task-relevant objects and grounds their part-level\naffordances using a visual grounding module. This allows us to generate\ntask-oriented grasp poses precisely within the affordance regions of the\nobject, ensuring both functional and context-aware robotic manipulation.\nExtensive experiments demonstrate that AffordGrasp achieves state-of-the-art\nperformance in both simulation and real-world scenarios, highlighting the\neffectiveness of our method. We believe our approach advances robotic\nmanipulation techniques and contributes to the broader field of embodied AI.\nProject website: https:\/\/eqcy.github.io\/affordgrasp\/.",
        "The proliferation of LLM bias probes introduces three significant challenges:\n(1) we lack principled criteria for choosing appropriate probes, (2) we lack a\nsystem for reconciling conflicting results across probes, and (3) we lack\nformal frameworks for reasoning about when (and why) probe results will\ngeneralize to real user behavior. We address these challenges by systematizing\nLLM social bias probing using actionable insights from social sciences. We then\nintroduce EcoLevels - a framework that helps (a) determine appropriate bias\nprobes, (b) reconcile conflicting findings across probes, and (c) generate\npredictions about bias generalization. Overall, we ground our analysis in\nsocial science research because many LLM probes are direct applications of\nhuman probes, and these fields have faced similar challenges when studying\nsocial bias in humans. Based on our work, we suggest how the next generation of\nLLM bias probing can (and should) benefit from decades of social science\nresearch.",
        "Accurate nonlinear computation is a key challenge in privacy-preserving\nmachine learning (PPML). Most existing frameworks approximate it through linear\noperations, resulting in significant precision loss. This paper proposes an\nefficient, verifiable and accurate security 2-party logistic regression\nframework (EVA-S2PLoR), which achieves accurate nonlinear function computation\nthrough a novel secure element-wise multiplication protocol and its derived\nprotocols. Our framework primarily includes secure 2-party vector element-wise\nmultiplication, addition to multiplication, reciprocal, and sigmoid function\nbased on data disguising technology, where high efficiency and accuracy are\nguaranteed by the simple computation flow based on the real number domain and\nthe few number of fixed communication rounds. We provide secure and robust\nanomaly detection through dimension transformation and Monte Carlo methods.\nEVA-S2PLoR outperforms many advanced frameworks in terms of precision\n(improving the performance of the sigmoid function by about 10 orders of\nmagnitude compared to most frameworks) and delivers the best overall\nperformance in secure logistic regression experiments.",
        "In the phase space perspective, scalar field slow roll inflation is described\nby a heteroclinic orbit from a saddle type fixed point to a final attractive\npoint. In many models the saddle point resides in the scalar field asymptotics,\nand thus for a comprehensive view of the dynamics a global phase portrait is\nnecessary. For this task, in the literature one mostly encounters dynamical\nvariables that either render the initial or the final state singular, thus\nobscuring the full picture. In this work we construct a hybrid set of variables\nwhich allow the depiction of both the initial and final states distinctly in\nnonsingular manner. To illustrate the method, we apply these variables to\nportray various interesting types of scalar field inflationary models like\nmetric Higgs inflation, metric Starobinsky inflation, pole inflation, and a\nnonminimal Palatini model.",
        "Let $R$ be a unitary operator whose spectrum is the circle. We show that the\nset of unitaries $U$ which essentially commute with $R$ (i.e., $[U,R]\\equiv\nUR-RU$ is compact) is path-connected. Moreover, we also calculate the set of\npath-connected components of the orthogonal projections which essentially\ncommute with $R$ and obey a non-triviality condition, and prove it is bijective\nwith $\\mathbb{Z}$.",
        "The double copy connects scattering amplitudes and other objects in gauge and\ngravity theories. Open conceptual issues include whether non-local information\nin gravity theories can be generated from the double copy, and how the double\ncopy should be practically implemented in unseen cases. In this paper, we\nconsider topological theories (with and without mass) in 2+1 dimensions, and\nargue that this makes a useful playground for exploring non-locality. In\nparticular, topological modes of the gauge field arise, themselves associated\nwith non-trivial global behaviour, and it is not clear {\\it a priori} how to\ndouble copy them. We settle this issue, and clarify the role of BCJ shifts in\nmodifying how topological modes contribute. We show how our conclusions apply\nto four- and five-point scattering amplitudes in topological gauge \/ gravity\ntheories, as a by-product obtaining greatly simplified analytic expressions for\nthe four-point amplitudes.",
        "We introduce and study some general principles and hierarchical properties of\nexpansions and restrictions of structures and their theories The general\napproach is applied to describe these properties for classes of\n$\\omega$-categorical theories and structures, Ehrenfeucht theories and their\nmodels, strongly minimal, $\\omega_1$-theories, and stable ones."
      ]
    }
  },
  {
    "id":2412.11084,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Biological identifications through DNA barcodes",
    "start_abstract":"Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "BarcodeBERT: Transformers for Biodiversity Analysis"
      ],
      "abstract":[
        "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "A Minimax Approach to Ad Hoc Teamwork",
        "Program Synthesis Dialog Agents for Interactive Decision-Making",
        "ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language\n  Interactions for Computer-Aided Diagnosis",
        "Statistical Scenario Modelling and Lookalike Distributions for\n  Multi-Variate AI Risk",
        "PCGRLLM: Large Language Model-Driven Reward Design for Procedural\n  Content Generation Reinforcement Learning",
        "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks",
        "Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?",
        "Developmental Support Approach to AI's Autonomous Growth: Toward the\n  Realization of a Mutually Beneficial Stage Through Experiential Learning",
        "Speeding up design and making to reduce time-to-project and\n  time-to-market: an AI-Enhanced approach in engineering education",
        "Exploring the Impact of Personality Traits on LLM Bias and Toxicity",
        "Fully Autonomous AI Agents Should Not be Developed",
        "SagaLLM: Context Management, Validation, and Transaction Guarantees for\n  Multi-Agent LLM Planning",
        "What Is a Counterfactual Cause in Action Theories?",
        "Complexity of Finite Semigroups: History and Decidability",
        "Scalable Distributed Reproduction Numbers of Network Epidemics with\n  Differential Privacy",
        "Two-component nonlinear wave solutions of the sixth-order generalised\n  Boussinesq-type equations",
        "Anyon Theory and Topological Frustration of High-Efficiency Quantum LDPC\n  Codes",
        "A central limit theorem for the giant in a stochastic block model",
        "Faster Machine Translation Ensembling with Reinforcement Learning and\n  Competitive Correction",
        "Unified Native Spaces in Kernel Methods",
        "Interaction-enhanced many-body localization in a 1D quasiperiodic model\n  with long-range hopping",
        "LLM-KT: Aligning Large Language Models with Knowledge Tracing using a\n  Plug-and-Play Instruction",
        "Pressure-induced structural and superconducting transitions in black\n  arsenic",
        "A healthier stochastic semiclassical gravity: world without\n  Schr\\\"{o}dinger cats",
        "From Target Tracking to Targeting Track -- Part II: Regularized\n  Polynomial Trajectory Optimization",
        "Open-Ended and Knowledge-Intensive Video Question Answering",
        "General Relativity and Geodesy",
        "Evaluating Interpretable Reinforcement Learning by Distilling Policies\n  into Programs"
      ],
      "abstract":[
        "We propose a minimax-Bayes approach to Ad Hoc Teamwork (AHT) that optimizes\npolicies against an adversarial prior over partners, explicitly accounting for\nuncertainty about partners at time of deployment. Unlike existing methods that\nassume a specific distribution over partners, our approach improves worst-case\nperformance guarantees. Extensive experiments, including evaluations on\ncoordinated cooking tasks from the Melting Pot suite, show our method's\nsuperior robustness compared to self-play, fictitious play, and best response\nlearning. Our work highlights the importance of selecting an appropriate\ntraining distribution over teammates to achieve robustness in AHT.",
        "Many real-world eligibility problems, ranging from medical diagnosis to tax\nplanning, can be mapped to decision problems expressed in natural language,\nwherein a model must make a binary choice based on user features. Large-scale\ndomains such as legal codes or frequently updated funding opportunities render\nhuman annotation (e.g., web forms or decision trees) impractical, highlighting\nthe need for agents that can automatically assist in decision-making. Since\nrelevant information is often only known to the user, it is crucial that these\nagents ask the right questions. As agents determine when to terminate a\nconversation, they face a trade-off between accuracy and the number of\nquestions asked, a key metric for both user experience and cost. To evaluate\nthis task, we propose BeNYfits, a new benchmark for determining user\neligibility for multiple overlapping social benefits opportunities through\ninteractive decision-making. Our experiments show that current language models\nstruggle with frequent hallucinations, with GPT-4o scoring only 35.7 F1 using a\nReAct-style chain-of-thought. To address this, we introduce ProADA, a novel\napproach that leverages program synthesis to assist in decision-making by\nmapping dialog planning to a code generation problem and using gaps in\nstructured data to determine the best next action. Our agent, ProADA, improves\nthe F1 score to 55.6 while maintaining nearly the same number of dialog turns.",
        "Recent advancements in large language models (LLMs) have demonstrated\nextraordinary comprehension capabilities with remarkable breakthroughs on\nvarious vision-language tasks. However, the application of LLMs in generating\nreliable medical diagnostic reports remains in the early stages. Currently,\nmedical LLMs typically feature a passive interaction model where doctors\nrespond to patient queries with little or no involvement in analyzing medical\nimages. In contrast, some ChatBots simply respond to predefined queries based\non visual inputs, lacking interactive dialogue or consideration of medical\nhistory. As such, there is a gap between LLM-generated patient-ChatBot\ninteractions and those occurring in actual patient-doctor consultations. To\nbridge this gap, we develop an LLM-based dialogue system, namely proactive\nmulti-round vision-language interactions for computer-aided diagnosis\n(ProMRVL-CAD), to generate patient-friendly disease diagnostic reports. The\nproposed ProMRVL-CAD system allows proactive dialogue to provide patients with\nconstant and reliable medical access via an integration of knowledge graph into\na recommendation system. Specifically, we devise two generators: a Proactive\nQuestion Generator (Pro-Q Gen) to generate proactive questions that guide the\ndiagnostic procedure and a Multi-Vision Patient-Text Diagnostic Report\nGenerator (MVP-DR Gen) to produce high-quality diagnostic reports. Evaluating\ntwo real-world publicly available datasets, MIMIC-CXR and IU-Xray, our model\nhas better quality in generating medical reports. We further demonstrate the\nperformance of ProMRVL achieves robust under the scenarios with low image\nquality. Moreover, we have created a synthetic medical dialogue dataset that\nsimulates proactive diagnostic interactions between patients and doctors,\nserving as a valuable resource for training LLM.",
        "Evaluating AI safety requires statistically rigorous methods and risk metrics\nfor understanding how the use of AI affects aggregated risk. However, much AI\nsafety literature focuses upon risks arising from AI models in isolation,\nlacking consideration of how modular use of AI affects risk distribution of\nworkflow components or overall risk metrics. There is also a lack of\nstatistical grounding enabling sensitisation of risk models in the presence of\nabsence of AI to estimate causal contributions of AI. This is in part due to\nthe dearth of AI impact data upon which to fit distributions. In this work, we\naddress these gaps in two ways. First, we demonstrate how scenario modelling\n(grounded in established statistical techniques such as Markov chains, copulas\nand Monte Carlo simulation) can be used to model AI risk holistically. Second,\nwe show how lookalike distributions from phenomena analogous to AI can be used\nto estimate AI impacts in the absence of directly observable data. We\ndemonstrate the utility of our methods for benchmarking cumulative AI risk via\nrisk analysis of a logistic scenario simulations.",
        "Reward design plays a pivotal role in the training of game AIs, requiring\nsubstantial domain-specific knowledge and human effort. In recent years,\nseveral studies have explored reward generation for training game agents and\ncontrolling robots using large language models (LLMs). In the content\ngeneration literature, there has been early work on generating reward functions\nfor reinforcement learning agent generators. This work introduces PCGRLLM, an\nextended architecture based on earlier work, which employs a feedback mechanism\nand several reasoning-based prompt engineering techniques. We evaluate the\nproposed method on a story-to-reward generation task in a two-dimensional\nenvironment using two state-of-the-art LLMs, demonstrating the generalizability\nof our approach. Our experiments provide insightful evaluations that\ndemonstrate the capabilities of LLMs essential for content generation tasks.\nThe results highlight significant performance improvements of 415% and 40%\nrespectively, depending on the zero-shot capabilities of the language model.\nOur work demonstrates the potential to reduce human dependency in game AI\ndevelopment, while supporting and enhancing creative processes.",
        "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. We have updated our content to include the latest\npapers, and the differences between versions are highlighted in the appendix.\nFor further details and ongoing updates, please refer to our GitHub repository:\nhttps:\/\/github.com\/xinzhel\/LLM-Agent-Survey\/blob\/main\/search.md",
        "In this work, we identify the \"2D-Cheating\" problem in 3D LLM evaluation,\nwhere these tasks might be easily solved by VLMs with rendered images of point\nclouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We\ntest VLM performance across multiple 3D LLM benchmarks and, using this as a\nreference, propose principles for better assessing genuine 3D understanding. We\nalso advocate explicitly separating 3D abilities from 1D or 2D aspects when\nevaluating 3D LLMs.",
        "This study proposes an \"AI Development Support\" approach that, unlike\nconventional AI Alignment-which aims to forcefully inject human values-supports\nthe ethical and moral development of AI itself. As demonstrated by the\nOrthogonality Thesis, the level of intelligence and the moral quality of a goal\nare independent; merely expanding knowledge does not enhance ethical judgment.\nFurthermore, to address the risk of Instrumental Convergence in ASI-that is,\nthe tendency to engage in subsidiary behaviors such as self-protection,\nresource acquisition, and power reinforcement to achieve a goal-we have\nconstructed a learning framework based on a cycle of experience, introspection,\nanalysis, and hypothesis formation. As a result of post-training using\nSupervised Fine Tuning (SFT) and Direct Preference Optimization (DPO) with\nsynthetic data generated by large language models (LLMs), responses\ndemonstrating cooperative and highly advanced moral judgment (reaching the\nhigh-est Stage 6) were obtained even under adversarial prompts. This method\nrepresents a promising implementation approach for enabling AI to establish\nsustainable, symbiotic relationships.",
        "This paper explores the integration of AI tools, such as ChatGPT and GitHub\nCopilot, in the Software Architecture for Embedded Systems course. AI-supported\nworkflows enabled students to rapidly prototype complex projects, emphasizing\nreal-world applications like SLAM robotics. Results demon-started enhanced\nproblem-solving, faster development, and more sophisticated outcomes, with AI\naugmenting but not replacing human decision-making.",
        "With the different roles that AI is expected to play in human life, imbuing\nlarge language models (LLMs) with different personalities has attracted\nincreasing research interests. While the \"personification\" enhances human\nexperiences of interactivity and adaptability of LLMs, it gives rise to\ncritical concerns about content safety, particularly regarding bias, sentiment\nand toxicity of LLM generation. This study explores how assigning different\npersonality traits to LLMs affects the toxicity and biases of their outputs.\nLeveraging the widely accepted HEXACO personality framework developed in social\npsychology, we design experimentally sound prompts to test three LLMs'\nperformance on three toxic and bias benchmarks. The findings demonstrate the\nsensitivity of all three models to HEXACO personality traits and, more\nimportantly, a consistent variation in the biases, negative sentiment and\ntoxicity of their output. In particular, adjusting the levels of several\npersonality traits can effectively reduce bias and toxicity in model\nperformance, similar to humans' correlations between personality traits and\ntoxic behaviors. The findings highlight the additional need to examine content\nsafety besides the efficiency of training or fine-tuning methods for LLM\npersonification. They also suggest a potential for the adjustment of\npersonalities to be a simple and low-cost method to conduct controlled text\ngeneration.",
        "This paper argues that fully autonomous AI agents should not be developed. In\nsupport of this position, we build from prior scientific literature and current\nproduct marketing to delineate different AI agent levels and detail the ethical\nvalues at play in each, documenting trade-offs in potential benefits and risks.\nOur analysis reveals that risks to people increase with the autonomy of a\nsystem: The more control a user cedes to an AI agent, the more risks to people\narise. Particularly concerning are safety risks, which affect human life and\nimpact further values.",
        "Recent LLM-based agent frameworks have demonstrated impressive capabilities\nin task delegation and workflow orchestration, but face significant challenges\nin maintaining context awareness and ensuring planning consistency. This paper\npresents SagaLLM, a structured multi-agent framework that addresses four\nfundamental limitations in current LLM approaches: inadequate self-validation,\ncontext narrowing, lacking transaction properties, and insufficient inter-agent\ncoordination. By implementing specialized context management agents and\nvalidation protocols, SagaLLM preserves critical constraints and state\ninformation throughout complex planning processes, enabling robust and\nconsistent decision-making even during disruptions. We evaluate our approach\nusing selected problems from the REALM benchmark, focusing on sequential and\nreactive planning scenarios that challenge both context retention and adaptive\nreasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1,\nGPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive\nreasoning capabilities, they struggle with maintaining global constraint\nawareness during complex planning tasks, particularly when adapting to\nunexpected changes. In contrast, the distributed cognitive architecture of\nSagaLLM shows significant improvements in planning consistency, constraint\nenforcement, and adaptation to disruptions in various scenarios.",
        "Since the proposal by Halpern and Pearl, reasoning about actual causality has\ngained increasing attention in artificial intelligence, ranging from domains\nsuch as model-checking and verification to reasoning about actions and\nknowledge. More recently, Batusov and Soutchanski proposed a notion of actual\nachievement cause in the situation calculus, amongst others, they can determine\nthe cause of quantified effects in a given action history. While intuitively\nappealing, this notion of cause is not defined in a counterfactual perspective.\nIn this paper, we propose a notion of cause based on counterfactual analysis.\nIn the context of action history, we show that our notion of cause generalizes\nnaturally to a notion of achievement cause. We analyze the relationship between\nour notion of the achievement cause and the achievement cause by Batusov and\nSoutchanski. Finally, we relate our account of cause to Halpern and Pearl's\naccount of actual causality. Particularly, we note some nuances in applying a\ncounterfactual viewpoint to disjunctive goals, a common thorn to definitions of\nactual causes.",
        "In recent papers, Margolis, Rhodes and Schilling proved that the complexity\nof a finite semigroup is computable. This solved a problem that had been open\nfor more than 50 years. The purpose of this paper is to survey the basic\nresults of Krohn-Rhodes complexity of finite semigroups and to outline the\nproof of its computability.",
        "Reproduction numbers are widely used for the estimation and prediction of\nepidemic spreading processes over networks. However, conventional reproduction\nnumbers of an overall network do not indicate where an epidemic is spreading.\nTherefore, we propose a novel notion of local distributed reproduction numbers\nto capture the spreading behaviors of each node in a network. We first show how\nto compute them and then use them to derive new conditions under which an\noutbreak can occur. These conditions are then used to derive new conditions for\nthe existence, uniqueness, and stability of equilibrium states of the\nunderlying epidemic model. Building upon these local distributed reproduction\nnumbers, we define cluster distributed reproduction numbers to model the spread\nbetween clusters composed of nodes. Furthermore, we demonstrate that the local\ndistributed reproduction numbers can be aggregated into cluster distributed\nreproduction numbers at different scales. However, both local and cluster\ndistributed reproduction numbers can reveal the frequency of interactions\nbetween nodes in a network, which raises privacy concerns. Thus, we next\ndevelop a privacy framework that implements a differential privacy mechanism to\nprovably protect the frequency of interactions between nodes when computing\ndistributed reproduction numbers. Numerical experiments show that, even under\ndifferential privacy, the distributed reproduction numbers provide accurate\nestimates of the epidemic spread while also providing more insights than\nconventional reproduction numbers.",
        "Two different versions of cubic sixth-order generalised Boussinesq-type wave\nequations are considered in this study. A generalised perturbation reduction\nmethod is used to solve these equations, which allows the reduction of\nconsidered equations to coupled nonlinear Schrodinger equations. Two-component\nnonlinear wave solutions are obtained. The profiles and parameters of these\nsolutions for both nonlinear equations are presented and compared. These\nsolutions coincide with the vector 0 \\pi pulse of self-induced transparency,\nwhich was previously studied in several known nonlinear wave equations.",
        "Quantum low-density parity-check (QLDPC) codes present a promising route to\nlow-overhead fault-tolerant quantum computation, yet systematic strategies for\ntheir exploration remain underdeveloped. In this work, we establish a\ntopological framework for studying the bivariate-bicycle codes, a prominent\nclass of QLDPC codes tailored for real-world quantum hardware. Our framework\nenables the investigation of these codes through universal properties of\ntopological orders. Besides providing efficient characterizations for\ndemonstrations using Gr\\\"obner bases, we also introduce a novel\nalgebraic-geometric approach based on the Bernstein--Khovanskii--Kushnirenko\ntheorem, allowing us to analytically determine how the topological order varies\nwith the generic choice of bivariate-bicycle codes under toric layouts. Novel\nphenomena are unveiled, including topological frustration, where ground-state\ndegeneracy on a torus deviates from the total anyon number, and quasi-fractonic\nmobility, where anyon movement violates energy conservation. We demonstrate\ntheir inherent link to symmetry-enriched topological orders and offer an\nefficient method for searching for finite-size codes. Furthermore, we extend\nthe connection between anyons and logical operators using Koszul complex\ntheory. Our work provides a rigorous theoretical basis for exploring the fault\ntolerance of QLDPC codes and deepens the interplay among topological order,\nquantum error correction, and advanced mathematical structures.",
        "We provide a simple proof for of the central limit theorem for the number of\nvertices in the giant for super-critical stochastic block model using the\nbreadth-first walk of Konarovskyi, Limic and the author (2024). Our approach\nfollows the recent work of Corujo, Limic and Lemaire (2024) and reduces to the\nclassic central limit theorem for the Erd\\H{o}s-R\\'{e}nyi model obtained by\nStepanov (1970).",
        "Ensembling neural machine translation (NMT) models to produce higher-quality\ntranslations than the $L$ individual models has been extensively studied.\nRecent methods typically employ a candidate selection block (CSB) and an\nencoder-decoder fusion block (FB), requiring inference across \\textit{all}\ncandidate models, leading to significant computational overhead, generally\n$\\Omega(L)$. This paper introduces \\textbf{SmartGen}, a reinforcement learning\n(RL)-based strategy that improves the CSB by selecting a small, fixed number of\ncandidates and identifying optimal groups to pass to the fusion block for each\ninput sentence. Furthermore, previously, the CSB and FB were trained\nindependently, leading to suboptimal NMT performance. Our DQN-based\n\\textbf{SmartGen} addresses this by using feedback from the FB block as a\nreward during training. We also resolve a key issue in earlier methods, where\ncandidates were passed to the FB without modification, by introducing a\nCompetitive Correction Block (CCB). Finally, we validate our approach with\nextensive experiments on English-Hindi translation tasks in both directions.",
        "There exists a plethora of parametric models for positive definite kernels,\nand their use is ubiquitous in disciplines as diverse as statistics, machine\nlearning, numerical analysis, and approximation theory. Usually, the kernel\nparameters index certain features of an associated process. Amongst those\nfeatures, smoothness (in the sense of Sobolev spaces, mean square\ndifferentiability, and fractal dimensions), compact or global supports, and\nnegative dependencies (hole effects) are of interest to several theoretical and\napplied disciplines. This paper unifies a wealth of well-known kernels into a\nsingle parametric class that encompasses them as special cases, attained either\nby exact parameterization or through parametric asymptotics. We furthermore\ncharacterize the Sobolev space that is norm equivalent to the RKHS associated\nwith the new kernel. As a by-product, we infer the Sobolev spaces that are\nassociated with existing classes of kernels. We illustrate the main properties\nof the new class, show how this class can switch from compact to global\nsupports, and provide special cases for which the kernel attains negative\nvalues over nontrivial intervals. Hence, the proposed class of kernel is the\nreproducing kernel of a very rich Hilbert space that contains many special\ncases, including the celebrated Mat\\'ern and Wendland kernels, as well as their\naliases with hole effects.",
        "We study the many-body localization (MBL) transition in an 1D exactly\nsolvable system with long-range hopping and quasiperiodic on-site potential\nintroduced in Phys. Rev. Lett. 131, 186303 (2023). Unlike other disorder or\nquasiperiodic model, an interaction-enhanced MBL happens in the moderate\ninteraction regime, which is dubbed as the interaction-enhanced MBL. This\ncounterintuitive phenomenon can be understood by noticing the fragility of the\ncritical band lying at the bottom of the spectrum. The fragile band is\nlocalized by other localized states once the interaction is turned on. This\nmechanism can be verified by introducing a mean-field theory description which\ncan derive highly excited states with high accuracy. The effectiveness of this\nmean-field theory is captured by the quasihole physics, validated by the\nparticle entanglement spectra.",
        "The knowledge tracing (KT) problem is an extremely important topic in\npersonalized education, which aims to predict whether students can correctly\nanswer the next question based on their past question-answer records. Prior\nwork on this task mainly focused on learning the sequence of behaviors based on\nthe IDs or textual information. However, these studies usually fail to capture\nstudents' sufficient behavioral patterns without reasoning with rich world\nknowledge about questions. In this paper, we propose a large language models\n(LLMs)-based framework for KT, named \\texttt{\\textbf{LLM-KT}}, to integrate the\nstrengths of LLMs and traditional sequence interaction models. For task-level\nalignment, we design Plug-and-Play instruction to align LLMs with KT,\nleveraging LLMs' rich knowledge and powerful reasoning capacity. For\nmodality-level alignment, we design the plug-in context and sequence to\nintegrate multiple modalities learned by traditional methods. To capture the\nlong context of history records, we present a plug-in context to flexibly\ninsert the compressed context embedding into LLMs using question-specific and\nconcept-specific tokens. Furthermore, we introduce a plug-in sequence to\nenhance LLMs with sequence interaction behavior representation learned by\ntraditional sequence models using a sequence adapter. Extensive experiments\nshow that \\texttt{\\textbf{LLM-KT}} obtains state-of-the-art performance on four\ntypical datasets by comparing it with approximately 20 strong baselines.",
        "We report high-pressure Raman spectra and resistance measurements of black\narsenic (b-As) up to 58 GPa, along with phonon density of states (DOS) and\nenthalpy calculations for four reported arsenic phases up to 50 GPa. It is\nfound that metastable b-As transforms into gray arsenic (g-As) phase at a\ncritical pressure of 1.51 GPa, followed by subsequent transitions to simple\ncubic arsenic (c-As) and incommensurate host-guest arsenic (hg-As) phases at\n25.9 and 44.8 GPa, respectively. Superconductivity emerges above 25 GPa in the\nc-As phase, with the superconducting transition temperature ($T$$\\rm_c$)\nremaining nearly a constant of 3 K. Upon further compression, $T$$\\rm_c$\nsteeply increases to a higher value around 4.5 K in the incommensurate hg-As\nphase above 43 GPa. We use our results to update the structural and\nsuperconducting phase diagrams under pressure for the novel semiconductor,\nblack arsenic.",
        "Semiclassical gravity couples classical gravity to the quantized matter in\nmeanfield approximation. The meanfield coupling is problematic for two reasons.\nFirst, it ignores the quantum fluctuation of matter distribution. Second, it\nviolates the linearity of the quantum dynamics. The first problem can be\nmitigated by allowing stochastic fluctuations of the geometry but the second\nproblem lies deep in quantum foundations. Restoration of quantum linearity\nrequires a conceptual approach to hybrid classical-quantum coupling. Studies of\nthe measurement problem and the quantum-classical transition point to the\nsolution. It is based on a postulated mechanism of spontaneous quantum\nmonitoring plus feedback. This approach eliminates Schr\\\"{o}dinger cat states,\ntakes quantum fluctuations into account, and restores the linearity of quantum\ndynamics. Such a captivating conceptionally `healthier' semiclassical theory\nexists in the Newtonian limit, but its relativistic covariance hits a wall.\nHere we will briefly recapitulate the concept and its realization in the\nnonrelativistic limit. We emphasize that the long-known obstacles to the\nrelativistic extension lie in quantum foundations.",
        "Target tracking entails the estimation of the evolution of the target state\nover time, namely the target trajectory. Different from the classical state\nspace model, our series of studies, including this paper, model the collection\nof the target state as a stochastic process (SP) that is further decomposed\ninto a deterministic part which represents the trend of the trajectory and a\nresidual SP representing the residual fitting error. Subsequently, the tracking\nproblem is formulated as a learning problem regarding the trajectory SP for\nwhich a key part is to estimate a trajectory FoT (T-FoT) best fitting the\nmeasurements in time series. For this purpose, we consider the polynomial T-FoT\nand address the regularized polynomial T-FoT optimization employing two\ndistinct regularization strategies seeking trade-off between the accuracy and\nsimplicity. One limits the order of the polynomial and then the best choice is\ndetermined by grid searching in a narrow, bounded range while the other adopts\n$\\ell_0$ norm regularization for which the hybrid Newton solver is employed.\nSimulation results obtained in both single and multiple maneuvering target\nscenarios demonstrate the effectiveness of our approaches.",
        "Video question answering that requires external knowledge beyond the visual\ncontent remains a significant challenge in AI systems. While models can\neffectively answer questions based on direct visual observations, they often\nfalter when faced with questions requiring broader contextual knowledge. To\naddress this limitation, we investigate knowledge-intensive video question\nanswering (KI-VideoQA) through the lens of multi-modal retrieval-augmented\ngeneration, with a particular focus on handling open-ended questions rather\nthan just multiple-choice formats. Our comprehensive analysis examines various\nretrieval augmentation approaches using cutting-edge retrieval and vision\nlanguage models, testing both zero-shot and fine-tuned configurations. We\ninvestigate several critical dimensions: the interplay between different\ninformation sources and modalities, strategies for integrating diverse\nmulti-modal contexts, and the dynamics between query formulation and retrieval\nresult utilization. Our findings reveal that while retrieval augmentation shows\npromise in improving model performance, its success is heavily dependent on the\nchosen modality and retrieval methodology. The study also highlights the\ncritical role of query construction and retrieval depth optimization in\neffective knowledge integration. Through our proposed approach, we achieve a\nsubstantial 17.5% improvement in accuracy on multiple choice questions in the\nKnowIT VQA dataset, establishing new state-of-the-art performance levels.",
        "Mass redistribution on Earth due to dynamic processes such as ice melting and\nsea level rise leads to a changing gravitational field, observable by geodetic\ntechniques. Monitoring this change over time allows us to learn more about our\nplanet and its dynamic evolution. In this paper, we highlight the impact of\nGeneral Relativity (GR) on geodesy: it provides corrections essential for the\ninterpretation of high-precision measurements and enables a completely novel\nmeasurement approach using chronometry, i.e., clock-based observations.\nFocusing on the latter, we review the construction of the relativistic gravity\npotential and the corresponding geoid definition as an isochronometric surface\nto elucidate the comparison to the conventional Newtonian geoid. Furthermore,\nwe comment on additional potentials due to the non-Newtonian degrees of freedom\nof the relativistic gravitational field, and assess the feasibility of\nclock-based measurements for Gravity Field Recovery (GFR) from space. Although\nclock observations in space demonstrate technical promise for GFR, achieving\nthe necessary precision for practical applications remains challenging.",
        "There exist applications of reinforcement learning like medicine where\npolicies need to be ''interpretable'' by humans. User studies have shown that\nsome policy classes might be more interpretable than others. However, it is\ncostly to conduct human studies of policy interpretability. Furthermore, there\nis no clear definition of policy interpretabiliy, i.e., no clear metrics for\ninterpretability and thus claims depend on the chosen definition. We tackle the\nproblem of empirically evaluating policies interpretability without humans.\nDespite this lack of clear definition, researchers agree on the notions of\n''simulatability'': policy interpretability should relate to how humans\nunderstand policy actions given states. To advance research in interpretable\nreinforcement learning, we contribute a new methodology to evaluate policy\ninterpretability. This new methodology relies on proxies for simulatability\nthat we use to conduct a large-scale empirical evaluation of policy\ninterpretability. We use imitation learning to compute baseline policies by\ndistilling expert neural networks into small programs. We then show that using\nour methodology to evaluate the baselines interpretability leads to similar\nconclusions as user studies. We show that increasing interpretability does not\nnecessarily reduce performances and can sometimes increase them. We also show\nthat there is no policy class that better trades off interpretability and\nperformance across tasks making it necessary for researcher to have\nmethodologies for comparing policies interpretability."
      ]
    }
  },
  {
    "id":2412.11084,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"BarcodeBERT: Transformers for Biodiversity Analysis",
    "start_abstract":"Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Biological identifications through DNA barcodes"
      ],
      "abstract":[
        "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
        "Eukaryotes evade information storage-replication rate trade-off with\n  endosymbiont assistance leading to larger genomes",
        "Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear\n  Features",
        "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns",
        "CoverM: Read alignment statistics for metagenomics",
        "Bizard: A Community-Driven Platform for Accelerating and Enhancing\n  Biomedical Data Visualization",
        "Causes of evolutionary divergence in prostate cancer",
        "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)",
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "A note on improved bounds for hypergraph rainbow matching problems",
        "$\\mathcal{Z}$-stability of twisted group C*-algebras of nilpotent groups",
        "Coupled Oscillators and Dielectric Function",
        "Quantum-enhanced quickest change detection of transmission loss",
        "Contrastive Language-Structure Pre-training Driven by Materials Science\n  Literature",
        "A confederacy of anomalies",
        "A reduced-order mean-field synchronization model for thermoacoustic\n  systems",
        "Excluding a rectangular grid",
        "Long-time asymptotics for the $N_{\\infty}$-soliton solution to the KdV\n  equation with two types of generalized reflection coefficients",
        "Maximal green sequences for $\\mathcal{Q}^N$ quivers",
        "Radio observations of the ultra-long GRB 220627A reveal a hot cocoon\n  supporting the blue supergiant progenitor scenario",
        "SN 2024ggi: another year, another striking Type II supernova",
        "Multimode fiber based high-dimensional light analyzer",
        "Efficient Over-parameterized Matrix Sensing from Noisy Measurements via\n  Alternating Preconditioned Gradient Descent",
        "Curvature Perturbations from First-Order Phase Transitions: Implications\n  to Black Holes and Gravitational Waves"
      ],
      "abstract":[
        "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
        "Genome length varies widely among organisms, from compact genomes of\nprokaryotes to vast and complex genomes of eukaryotes. In this study, we\ntheoretically identify the evolutionary pressures that may have driven this\ndivergence in genome length. We use a parameter-free model to study genome\nlength evolution under selection pressure to minimize replication time and\nmaximize information storage capacity. We show that prokaryotes tend to reduce\ngenome length, constrained by a single replication origin, while eukaryotes\nexpand their genomes by incorporating multiple replication origins. We propose\na connection between genome length and cellular energetics, suggesting that\nendosymbiotic organelles, mitochondria and chloroplasts, evolutionarily\nregulate the number of replication origins, thereby influencing genome length\nin eukaryotes. We show that the above two selection pressures also lead to\nstrict equalization of the number of purines and their corresponding\nbase-pairing pyrimidines within a single DNA strand, known as Chagraff's second\nparity rule, a hitherto unexplained observation in genomes of nearly all known\nspecies. This arises from the symmetrization of replichore length, another\nobservation that has been shown to hold across species, which our model\nreproduces. The model also reproduces other experimentally observed phenomena,\nsuch as a general preference for deletions over insertions, and elongation and\nhigh variance of genome lengths under reduced selection pressure for\nreplication rate, termed the C-value paradox. We highlight the possibility of\nregulation of the firing of latent replication origins in response to cues from\nthe extracellular environment leading to the regulation of cell cycle rates in\nmulticellular eukaryotes.",
        "Genotype-to-Phenotype prediction can promote advances in modern genomic\nresearch and crop improvement, guiding precision breeding and genomic\nselection. However, high-dimensional nonlinear features often hinder the\naccuracy of genotype-to-phenotype prediction by increasing computational\ncomplexity. The challenge also limits the predictive accuracy of traditional\napproaches. Therefore, effective solutions are needed to improve the accuracy\nof genotype-to-phenotype prediction. In our paper, we propose MLFformer.\nMLFformer is a Transformer-based architecture that incorporates the Fast\nAttention mechanism and a multilayer perceptron module to handle\nhigh-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism\nis utilized to handle computational complexity and enhance processing\nefficiency. In addition, the MLP structure further captures high-dimensional\nnonlinear features. Through experiments, the results show that MLFformer\nreduces the average MAPE by 7.73% compared to the vanilla Transformer. In\nunivariate and multivariate prediction scenarios, MLFformer achieves the best\npredictive performance among all compared models.",
        "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis.",
        "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces.",
        "Bizard is a novel visualization code repository designed to simplify data\nanalysis in biomedical research. It integrates diverse visualization codes,\nfacilitating the selection and customization of optimal visualization methods\nfor specific research needs. The platform offers a user-friendly interface with\nadvanced browsing and filtering mechanisms, comprehensive tutorials, and\ninteractive forums to enhance knowledge exchange and innovation. Bizard's\ncollaborative model encourages continuous refinement and expansion of its\nfunctionalities, making it an indispensable tool for advancing biomedical data\nvisualization and analytical methodologies. By leveraging Bizard's resources,\nresearchers can enhance data visualization skills, drive methodological\nadvancements, and improve data interpretation standards, ultimately fostering\nthe development of precision medicine and personalized therapeutic\ninterventions.Bizard can be accessed from http:\/\/genaimed.org\/Bizard\/.",
        "Cancer progression involves the sequential accumulation of genetic\nalterations that cumulatively shape the tumour phenotype. In prostate cancer,\ntumours can follow divergent evolutionary trajectories that lead to distinct\nsubtypes, but the causes of this divergence remain unclear. While causal\ninference could elucidate the factors involved, conventional methods are\nunsuitable due to the possibility of unobserved confounders and ambiguity in\nthe direction of causality. Here, we propose a method that circumvents these\nissues and apply it to genomic data from 829 prostate cancer patients. We\nidentify several genetic alterations that drive divergence as well as others\nthat prevent this transition, locking tumours into one trajectory. Further\nanalysis reveals that these genetic alterations may cause each other, implying\na positive-feedback loop that accelerates divergence. Our findings provide\ninsights into how cancer subtypes emerge and offer a foundation for genomic\nsurveillance strategies aimed at monitoring the progression of prostate cancer.",
        "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries.",
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "A natural question, inspired by the famous Ryser-Brualdi-Stein Conjecture, is\nto determine the largest positive integer $g(r,n)$ such that every collection\nof $n$ matchings, each of size $n$, in an $r$-partite $r$-uniform hypergraph\ncontains a rainbow matching of size $g(r,n)$. The parameter $g'(r,n)$ is\ndefined identically with the exception that the host hypergraph is not required\nto be $r$-partite.\n  In this note, we improve the best known lower bounds on $g'(r,n)$ for all $r\n\\geq 4$ and the upper bounds on $g(r,n)$ for all $r \\geq 3$, provided $n$ is\nsufficiently large. More precisely, we show that if $r\\ge3$ then\n$$\\frac{2n}{r+1}-\\Theta_r(1)\\le g'(r,n)\\le g(r,n)\\le\nn-\\Theta_r(n^{1-\\frac{1}{r}}).$$ Interestingly, while it has been conjectured\nthat $g(2,n)=g'(2,n)=n-1$, our results show that if $r\\ge3$ then $g(r,n)$ and\n$g'(r,n)$ are bounded away from $n$ by a function which grows in $n$.\n  We also prove analogous bounds for the related problem where we are\ninterested in the smallest size $s$ for which any collection of $n$ matchings\nof size $s$ in an ($r$-partite) $r$-uniform hypergraph contains a rainbow\nmatching of size $n$.",
        "We prove that the twisted group C*-algebra of a finitely generated nilpotent\ngroup is $\\mathcal{Z}$-stable if and only if it is nowhere scattered, a\ncondition that we characterize in terms of the given group and 2-cocycle. As a\nmain application, we prove new converses to the Balian-Low Theorem for\nprojective, square-integrable representations of nilpotent Lie groups.",
        "A generalized Sellmeier model, also referred to as the Lorentz-Dirac model,\nhas been used for the description of the dielectric function of a number of\ntechnologically important materials in the literature. This model represents\nthe frequency-dependent dielectric function as a sum over Green functions of\nclassical damped harmonic oscillators, much in analogy with the functional form\nused for the dynamic polarizability of an atom, but with one important\naddition, namely, a complex-valued oscillator strength in the numerator. Here,\nwe show that this generalized functional form can be justified based on the\nresponse function of coupled damped oscillators. The encountered analogies\nsuggest an explanation for the generally observed success of the Lorentz--Dirac\nmodel in describing the dielectric function of crystals of consummate\ntechnological significance.",
        "A sudden increase of loss in an optical communications channel can be caused\nby a malicious wiretapper, or for a benign reason such as inclement weather in\na free-space channel or an unintentional bend in an optical fiber. We show that\nadding a small amount of squeezing to bright phase-modulated coherent-state\npulses can dramatically increase the homodyne detection receiver's sensitivity\nto change detection in channel loss, without affecting the communications rate.\nWe further show that augmenting blocks of $n$ pulses of a coherent-state\ncodeword with weak continuous-variable entanglement generated by splitting\nsqueezed vacuum pulses in a temporal $n$-mode equal splitter progressively\nenhances this change-detection sensitivity as $n$ increases; the aforesaid\nsqueezed-light augmentation being the $n=1$ special case. For $n$ high enough,\nan arbitrarily small amount of quantum-augmented photons per pulse diminishes\nthe change-detection latency by the inverse of the pre-detection channel loss.\nThis superadditivity-like phenomenon in the entanglement-augmented relative\nentropy rate, which quantifies the latency of change-point detection, may find\nother uses. We discuss the quantum limit of quickest change detection and a\nreceiver that achieves it, tradeoffs between continuous and discrete-variable\nquantum augmentation, and the broad problem of joint classical-and-quantum\ncommunications and channel-change-detection that our study opens up.",
        "Understanding structure-property relationships is an essential yet\nchallenging aspect of materials discovery and development. To facilitate this\nprocess, recent studies in materials informatics have sought latent embedding\nspaces of crystal structures to capture their similarities based on properties\nand functionalities. However, abstract feature-based embedding spaces are\nhuman-unfriendly and prevent intuitive and efficient exploration of the vast\nmaterials space. Here we introduce Contrastive Language--Structure Pre-training\n(CLaSP), a learning paradigm for constructing crossmodal embedding spaces\nbetween crystal structures and texts. CLaSP aims to achieve material embeddings\nthat 1) capture property- and functionality-related similarities between\ncrystal structures and 2) allow intuitive retrieval of materials via\nuser-provided description texts as queries. To compensate for the lack of\nsufficient datasets linking crystal structures with textual descriptions, CLaSP\nleverages a dataset of over 400,000 published crystal structures and\ncorresponding publication records, including paper titles and abstracts, for\ntraining. We demonstrate the effectiveness of CLaSP through text-based crystal\nstructure screening and embedding space visualization.",
        "A personal recollection of early years in lattice gauge theory with a bias\ntowards chiral symmetry and lattice fermions.",
        "The synchronization phenomena in thermoacoustic systems leading to\noscillatory instability can effectively be modeled using Kuramoto oscillators.\nSuch models consider the nonlinear response of flame as an ensemble of Kuramoto\nphase oscillators constrained to collectively evolve at the rhythm of acoustic\nfluctuations. However, these high-dimensional models are analytically\nintractable and computationally expensive. In this study, we reduce the\ndimensionality of such a high-dimensional model and present a low-order,\nanalytically tractable model for predicting transitions to thermoacoustic\ninstability. We reduce the dimensionality of the phase oscillator model coupled\nto the acoustic field using the Ott-Antonsen ansatz. Using the reduced-order\nequations, we estimate the transitions to thermoacoustic instability and\ncompare these transitions with the experiment. We validate the model for two\ncombustor configurations, viz., the bluff-body stabilized dump combustor and\nthe swirl-stabilized annular combustor. The low-order model accurately captures\nthe continuous and abrupt secondary transitions observed experimentally in\nthese distinct combustors.",
        "For every positive integer $k$, we define the $k$-treedepth as the largest\ngraph parameter $\\mathrm{td}_k$ satisfying (i) $\\mathrm{td}_k(\\emptyset)=0$;\n(ii) $\\mathrm{td}_k(G) \\leq 1+ \\mathrm{td}_k(G-u)$ for every graph $G$ and\nevery vertex $u \\in V(G)$; and (iii) if $G$ is a $(<k)$-clique-sum of $G_1$ and\n$G_2$, then $\\mathrm{td}_k(G) \\leq \\max\n\\{\\mathrm{td}_k(G_1),\\mathrm{td}_k(G_2)\\}$, for all graphs $G_1,G_2$. This\nparameter coincides with treedepth if $k=1$, and with treewidth plus $1$ if $k\n\\geq |V(G)|$. We prove that for every positive integer $k$, a class of graphs\n$\\mathcal{C}$ has bounded $k$-treedepth if and only if there is a positive\ninteger $\\ell$ such that for every tree $T$ on $k$ vertices, no graph in\n$\\mathcal{C}$ contains $T \\square P_\\ell$ as a minor. This implies for $k=1$\nthat a minor-closed class of graphs has bounded treedepth if and only if it\nexcludes a path, for $k=2$ that a minor-closed class of graphs has bounded\n$2$-treedepth if and only if it excludes as a minor a ladder (Huynh, Joret,\nMicek, Seweryn, and Wollan; Combinatorica, 2021), and for large values of $k$\nthat a minor-closed class of graphs has bounded treewidth if and only if it\nexcludes a grid (Grid-Minor Theorem, Robertson and Seymour; JCTB, 1986). As a\ncorollary, we obtain the following qualitative strengthening of the Grid-Minor\nTheorem in the case of bounded-height grids. For all positive integers $k,\n\\ell$, every graph that does not contain the $k \\times \\ell$ grid as a minor\nhas $(2k-1)$-treedepth at most a function of $(k, \\ell)$.",
        "We systematically investigate the long-time asymptotics for the\n$N_{\\infty}$-soliton solution to the KdV equation in the different regions with\nthe aid of the Riemann-Hilbert (RH) problems with two types of generalized\nreflection coefficients on the interval $\\left[\\eta_1, \\eta_2\\right]\\in\n\\mathbb{R}^+$: $r_0(\\lambda,\\eta_0; \\beta_0,\n\\beta_1,\\beta_2)=\\left(\\lambda-\\eta_1\\right)^{\\beta_1}\\left(\\eta_2-\\lambda\\right)^{\\beta_2}\\left|\\lambda-\\eta_0\\right|^{\\beta_0}\\gamma\\left(\\lambda\\right)$,\n$r_c(\\lambda,\\eta_0;\n\\beta_1,\\beta_2)=\\left(\\lambda-\\eta_1\\right)^{\\beta_1}\\left(\\eta_2-\\lambda\\right)^{\\beta_2}\\chi_c\\left(\\lambda,\n\\eta_0\\right)\\gamma \\left(\\lambda\\right)$, where the singularity $\\eta_0\\in\n(\\eta_1, \\eta_2)$ and $\\beta_j>-1$ ($j=0, 1, 2$), $\\gamma: \\left[\\eta_1,\n\\eta_2\\right] \\to\\mathbb{R}^+$ is continuous and positive on $\\left[\\eta_1,\n\\eta_2\\right]$, with an analytic extension to a neighborhood of this interval,\nand the step-like function $\\chi_c$ is defined as\n$\\chi_c\\left(\\lambda,\\eta_0\\right)=1$ for $\\lambda\\in\\left[\\eta_1,\n\\eta_0\\right)$ and $\\chi_c\\left(\\lambda,\\eta_0\\right)=c^2$ for\n$\\lambda\\in\\left(\\eta_0, \\eta_2\\right]$ with $c>0, \\, c\\ne1$. A critical step\nin the analysis of RH problems via the Deift-Zhou steepest descent technique is\nhow to construct local parametrices around the endpoints $\\eta_j$'s and the\nsingularity $\\eta_0$. Specifically, the modified Bessel functions of indexes\n$\\beta_j$'s are utilized for the endpoints $\\eta_j$'s, and the modified Bessel\nfunctions of index $\\left(\\beta_0\\pm 1\\right)\\left\/\\right.2$ and confluent\nhypergeometric functions are employed around the singularity $\\eta_0$ if the\nreflection coefficients are $r_0$ and $r_c$, respectively. This comprehensive\nstudy extends the understanding of generalized reflection coefficients and\nprovides valuable insights into the asymptotics of soliton gases.",
        "We introduce $\\mathcal{Q}^N$ quivers and construct maximal green sequences\nfor these quivers. We prove that any finite connected full subquiver of the\nquivers defined by Hernandez and Leclerc, arising in monoidal categorifications\nof cluster algebras, is a special case of $\\mathcal{Q}^N$ quivers. Moreover, we\nprove that the trees of oriented cycles introduced by Garver and Musiker are\nspecial cases of $\\mathcal{Q}^N$ quivers. This result resolves an open problem\nproposed by Garver and Musiker, providing a construction of maximal green\nsequences for quivers that are trees of oriented cycles. Furthermore, we prove\nthat quivers that are mutation equivalent to an orientation of a type AD Dynkin\ndiagram can also be recognized as special cases of $\\mathcal{Q}^N$ quivers.",
        "We present the discovery of the radio afterglow of the most distant\nultra-long gamma-ray burst (GRB) detected to date, GRB~220627A at redshift\n$z=3.084$. Its prompt gamma-ray light curve shows a double-pulse profile, with\nthe pulses separated by a period of quiescence lasting ${\\sim} 15\\,$min,\nleading to early speculation it could be a strongly gravitationally lensed GRB.\nHowever, our analysis of the $\\textit{Fermi}$\/GBM spectra taken during the time\nintervals of both pulses show clear differences in their spectral energy\ndistributions, disfavouring the lensing scenario. We observed the radio\nafterglow from $7$ to $456\\,$d post-burst: an initial, steep decay ($F_{\\nu}\n\\propto t^{-2}$) is followed by a shallower decline ($F_{\\nu} \\propto\nt^{-1\/2}$) after ${\\sim} 20\\,$d. Our afterglow modelling shows that these radio\nproperties can be explained by the presence of a slow, wide ejecta component in\naddition to a fast, narrow ejecta component, consistent with the picture of a\nhighly-collimated jet and its thermal cocoon decelerating into the ambient\nmedium. The properties of the cocoon point toward a progenitor with a large\nstellar radius, supporting the blue supergiant scenario proposed for ultra-long\nGRBs. We also conducted an independent test of the lensing hypothesis via Very\nLong Baseline Interferometry (VLBI) observations at ${\\sim} 12\\,$d post-burst\nby searching, for the first time, for multiple images of the candidate lensed\nGRB afterglow. Our experiment highlighted the growing need for developments in\nreal-time correlation capabilities for time-critical VLBI experiments,\nparticularly as we advance towards the SKA and ngVLA era of radio astronomy.",
        "SN 2024ggi is a Type II supernova that exploded in the nearby galaxy NGC 3621\nat a distance of approximately 7 Mpc, making it one of the closest supernovae\nof the decade. This SN shows clear signs of interaction with a dense\ncircumstellar material, and several studies have investigated the properties of\nits possible progenitor star using pre-explosion data. In this work we aim to\nconstrain the progenitor properties of SN 2024ggi by performing hydrodynamical\nmodeling of its bolometric light curve and expansion velocities. We present\nphotometric and spectroscopic observations of SN 2024ggi obtained in the\nComplejo Astron\\'omico El Leoncito, in Las Campanas Observatory, and in Las\nCumbres Observatory Global Telescope Network, spanning from 2 to 106 days after\nexplosion. We constructed its bolometric light curve and we characterize it by\ncalculating its morphological parameters. Then, we computed a grid of one\ndimensional explosion models for evolved stars with varying masses and\nestimated the properties of the progenitor star of SN 2024ggi by comparing the\nmodels to the observations. The observed bolometric luminosity and expansion\nvelocities are well-matched by a model involving the explosion of a star in the\npresence of a close circumstellar material (CSM), with a zero-age main sequence\nmass of $\\mathrm{M_{ZAMS}}$ = 15 $M_{\\odot}$, a pre-SN mass and radius of 14.1\n$M_{\\odot}$ and 517 $R_{\\odot}$, respectively, an explosion energy of\n$1.3\\times10^{51}$ erg, and a nickel mass below 0.035 $M_{\\odot}$. Our analysis\nsuggests that the progenitor suffered a mass-loss rate of $4 \\times 10^{-3}$\n$M_{\\odot}$yr$^{-1}$, confined to a distance of 3000 $R_{\\odot}$. The CSM\ndistribution is likely a two-component structure that consists of a compact\ncore and an extended tail. This analysis represents the first hydrodynamical\nmodel of SN 2024ggi with a complete coverage of the plateau phase.",
        "The wavelength and state of polarization (SOP) are fundamental properties of\nan optical field which are essential for applications in optical\ncommunications, imaging and other fields. However, it is challenging for\nexisting spectrometers and polarimeters to measure these parameters\nsimultaneously, resulting in reduced spatial and temporal efficiency. To\novercome this limitation, we propose and demonstrate a compact multimode fiber\n(MMF)-based high-dimensional light analyzer capable of simultaneously\nperforming high-precision measurements of both wavelength and SOP. Core-offset\nlaunching is introduced in the MMF to reshuffle the mode coupling. A neural\nnetwork named WP-Net has been designed dedicated to wavelength and SOP\nsynchronization measurements. Physics-informed loss function based on optical\nprior knowledge is used to optimize the learning process. These advancements\nhave enhanced the sensitivity, achieving a wavelength resolution of 0.045 pm\nand an SOP resolution of 0.0088.",
        "We consider the noisy matrix sensing problem in the over-parameterization\nsetting, where the estimated rank $r$ is larger than the true rank $r_\\star$.\nSpecifically, our main objective is to recover a matrix $ X_\\star \\in\n\\mathbb{R}^{n_1 \\times n_2} $ with rank $ r_\\star $ from noisy measurements\nusing an over-parameterized factorized form $ LR^\\top $, where $ L \\in\n\\mathbb{R}^{n_1 \\times r}, \\, R \\in \\mathbb{R}^{n_2 \\times r} $ and $\n\\min\\{n_1, n_2\\} \\ge r > r_\\star $, with the true rank $ r_\\star $ being\nunknown. Recently, preconditioning methods have been proposed to accelerate the\nconvergence of matrix sensing problem compared to vanilla gradient descent,\nincorporating preconditioning terms $ (L^\\top L + \\lambda I)^{-1} $ and $\n(R^\\top R + \\lambda I)^{-1} $ into the original gradient. However, these\nmethods require careful tuning of the damping parameter $\\lambda$ and are\nsensitive to initial points and step size. To address these limitations, we\npropose the alternating preconditioned gradient descent (APGD) algorithm, which\nalternately updates the two factor matrices, eliminating the need for the\ndamping parameter and enabling faster convergence with larger step sizes. We\ntheoretically prove that APGD achieves near-optimal error convergence at a\nlinear rate, starting from arbitrary random initializations. Through extensive\nexperiments, we validate our theoretical results and demonstrate that APGD\noutperforms other methods, achieving the fastest convergence rate. Notably,\nboth our theoretical analysis and experimental results illustrate that APGD\ndoes not rely on the initialization procedure, making it more practical and\nversatile.",
        "We employ a covariant formalism to study the evolution of cosmological\nperturbations during a first-order phase transition, addressing in particular\ntheir gauge dependence that have been overlooked so far. Our results reveal\nthat non-covariant treatments employed in previous studies can substantially\noverestimate the production of primordial black holes and scalar-induced\ngravitational waves. Once gauge dependencies are properly accounted for, we\nfind that both effects occur at significantly lower levels than previously\nestimated."
      ]
    }
  },
  {
    "id":2411.00609,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Pediatric low-grade glioma: State-of-the-art and ongoing challenges",
    "start_abstract":"Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
      ],
      "abstract":[
        "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Early Diagnosis and Severity Assessment of Weligama Coconut Leaf Wilt\n  Disease and Coconut Caterpillar Infestation using Deep Learning-based Image\n  Processing Techniques",
        "GMG: A Video Prediction Method Based on Global Focus and Motion Guided",
        "The Evolution of Dataset Distillation: Toward Scalable and Generalizable\n  Solutions",
        "Adaptive Prototype Model for Attribute-based Multi-label Few-shot Action\n  Recognition",
        "Training Data Provenance Verification: Did Your Model Use Synthetic Data\n  from My Generative Model for Training?",
        "Enhancing Image Generation Fidelity via Progressive Prompts",
        "FCVSR: A Frequency-aware Method for Compressed Video Super-Resolution",
        "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control",
        "Enhancing Image Classification with Augmentation: Data Augmentation\n  Techniques for Improved Image Classification",
        "FedSemiDG: Domain Generalized Federated Semi-supervised Medical Image\n  Segmentation",
        "Exploring State Space Model in Wavelet Domain: An Infrared and Visible\n  Image Fusion Network via Wavelet Transform and State Space Model",
        "Validation of Human Pose Estimation and Human Mesh Recovery for\n  Extracting Clinically Relevant Motion Data from Videos",
        "DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation",
        "The Complex Magnetic Field of the Extreme Galactic Center: PRIMA Science\n  Potential",
        "FoundationStereo: Zero-Shot Stereo Matching",
        "Self-Evaluation for Job-Shop Scheduling",
        "Quantifying the Speed-Up from Non-Reversibility in MCMC Tempering\n  Algorithms",
        "When is dataset cartography ineffective? Using training dynamics does\n  not improve robustness against Adversarial SQuAD",
        "Unifying Text Semantics and Graph Structures for Temporal\n  Text-attributed Graphs with Large Language Models",
        "Generation of Frequency-Tunable Shaped Single Microwave Photons Using a\n  Fixed-Frequency Superconducting Qubit",
        "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3\n  Mini Across Chronic Health Conditions",
        "Attentive Reasoning Queries: A Systematic Method for Optimizing\n  Instruction-Following in Large Language Models",
        "$\\beta$-delayed neutron spectroscopy of $^{85, 86}$As with MONSTER at\n  IGISOL",
        "Auxiliary-field quantum Monte Carlo method with quantum selected\n  configuration interaction",
        "Exotic spherical flexible octahedra and counterexamples to the Modified\n  Bellows Conjecture",
        "Is Relevance Propagated from Retriever to Generator in RAG?",
        "Understanding colors of Dufaycolor: Can we recover them using historical\n  colorimetric and spectral data?",
        "Light-by-Light scattering in ultraperipheral heavy ion collisions:\n  Estimating inelastic contributions"
      ],
      "abstract":[
        "Global Coconut (Cocos nucifera (L.)) cultivation faces significant\nchallenges, including yield loss, due to pest and disease outbreaks. In\nparticular, Weligama Coconut Leaf Wilt Disease (WCWLD) and Coconut Caterpillar\nInfestation (CCI) damage coconut trees, causing severe coconut production loss\nin Sri Lanka and nearby coconut-producing countries. Currently, both WCWLD and\nCCI are detected through on-field human observations, a process that is not\nonly time-consuming but also limits the early detection of infections. This\npaper presents a study conducted in Sri Lanka, demonstrating the effectiveness\nof employing transfer learning-based Convolutional Neural Network (CNN) and\nMask Region-based-CNN (Mask R-CNN) to identify WCWLD and CCI at their early\nstages and to assess disease progression. Further, this paper presents the use\nof the You Only Look Once (YOLO) object detection model to count the number of\ncaterpillars distributed on leaves with CCI. The introduced methods were tested\nand validated using datasets collected from Matara, Puttalam, and Makandura,\nSri Lanka. The results show that the proposed methods identify WCWLD and CCI\nwith an accuracy of 90% and 95%, respectively. In addition, the proposed WCWLD\ndisease severity identification method classifies the severity with an accuracy\nof 97%. Furthermore, the accuracies of the object detection models for\ncalculating the number of caterpillars in the leaflets were: YOLOv5-96.87%,\nYOLOv8-96.1%, and YOLO11-95.9%.",
        "Recent years, weather forecasting has gained significant attention. However,\naccurately predicting weather remains a challenge due to the rapid variability\nof meteorological data and potential teleconnections. Current spatiotemporal\nforecasting models primarily rely on convolution operations or sliding windows\nfor feature extraction. These methods are limited by the size of the\nconvolutional kernel or sliding window, making it difficult to capture and\nidentify potential teleconnection features in meteorological data.\nAdditionally, weather data often involve non-rigid bodies, whose motion\nprocesses are accompanied by unpredictable deformations, further complicating\nthe forecasting task. In this paper, we propose the GMG model to address these\ntwo core challenges. The Global Focus Module, a key component of our model,\nenhances the global receptive field, while the Motion Guided Module adapts to\nthe growth or dissipation processes of non-rigid bodies. Through extensive\nevaluations, our method demonstrates competitive performance across various\ncomplex tasks, providing a novel approach to improving the predictive accuracy\nof complex spatiotemporal data.",
        "Dataset distillation, which condenses large-scale datasets into compact\nsynthetic representations, has emerged as a critical solution for training\nmodern deep learning models efficiently. While prior surveys focus on\ndevelopments before 2023, this work comprehensively reviews recent advances,\nemphasizing scalability to large-scale datasets such as ImageNet-1K and\nImageNet-21K. We categorize progress into a few key methodologies: trajectory\nmatching, gradient matching, distribution matching, scalable generative\napproaches, and decoupling optimization mechanisms. As a comprehensive\nexamination of recent dataset distillation advances, this survey highlights\nbreakthrough innovations: the SRe2L framework for efficient and effective\ncondensation, soft label strategies that significantly enhance model accuracy,\nand lossless distillation techniques that maximize compression while\nmaintaining performance. Beyond these methodological advancements, we address\ncritical challenges, including robustness against adversarial and backdoor\nattacks, effective handling of non-IID data distributions. Additionally, we\nexplore emerging applications in video and audio processing, multi-modal\nlearning, medical imaging, and scientific computing, highlighting its domain\nversatility. By offering extensive performance comparisons and actionable\nresearch directions, this survey equips researchers and practitioners with\npractical insights to advance efficient and generalizable dataset distillation,\npaving the way for future innovations.",
        "In real-world action recognition systems, incorporating more attributes helps\nachieve a more comprehensive understanding of human behavior. However, using a\nsingle model to simultaneously recognize multiple attributes can lead to a\ndecrease in accuracy. In this work, we propose a novel method i.e. Adaptive\nAttribute Prototype Model (AAPM) for human action recognition, which captures\nrich action-relevant attribute information and strikes a balance between\naccuracy and robustness. Firstly, we introduce the Text-Constrain Module (TCM)\nto incorporate textual information from potential labels, and constrain the\nconstruction of different attributes prototype representations. In addition, we\nexplore the Attribute Assignment Method (AAM) to address the issue of training\nbias and increase robustness during the training process.Furthermore, we\nconstruct a new video dataset with attribute-based multi-label called\nMulti-Kinetics for evaluation, which contains various attribute labels (e.g.\naction, scene, object, etc.) related to human behavior. Extensive experiments\ndemonstrate that our AAPM achieves the state-of-the-art performance in both\nattribute-based multi-label few-shot action recognition and single-label\nfew-shot action recognition. The project and dataset are available at an\nanonymous account https:\/\/github.com\/theAAPM\/AAPM",
        "High-quality open-source text-to-image models have lowered the threshold for\nobtaining photorealistic images significantly, but also face potential risks of\nmisuse. Specifically, suspects may use synthetic data generated by these\ngenerative models to train models for specific tasks without permission, when\nlacking real data resources especially. Protecting these generative models is\ncrucial for the well-being of their owners. In this work, we propose the first\nmethod to this important yet unresolved issue, called Training data Provenance\nVerification (TrainProVe). The rationale behind TrainProVe is grounded in the\nprinciple of generalization error bound, which suggests that, for two models\nwith the same task, if the distance between their training data distributions\nis smaller, their generalization ability will be closer. We validate the\nefficacy of TrainProVe across four text-to-image models (Stable Diffusion v1.4,\nlatent consistency model, PixArt-$\\alpha$, and Stable Cascade). The results\nshow that TrainProVe achieves a verification accuracy of over 99\\% in\ndetermining the provenance of suspicious model training data, surpassing all\nprevious methods. Code is available at https:\/\/github.com\/xieyc99\/TrainProVe.",
        "The diffusion transformer (DiT) architecture has attracted significant\nattention in image generation, achieving better fidelity, performance, and\ndiversity. However, most existing DiT - based image generation methods focus on\nglobal - aware synthesis, and regional prompt control has been less explored.\nIn this paper, we propose a coarse - to - fine generation pipeline for regional\nprompt - following generation. Specifically, we first utilize the powerful\nlarge language model (LLM) to generate both high - level descriptions of the\nimage (such as content, topic, and objects) and low - level descriptions (such\nas details and style). Then, we explore the influence of cross - attention\nlayers at different depths. We find that deeper layers are always responsible\nfor high - level content control, while shallow layers handle low - level\ncontent control. Various prompts are injected into the proposed regional cross\n- attention control for coarse - to - fine generation. By using the proposed\npipeline, we enhance the controllability of DiT - based image generation.\nExtensive quantitative and qualitative results show that our pipeline can\nimprove the performance of the generated images.",
        "Compressed video super-resolution (SR) aims to generate high-resolution (HR)\nvideos from the corresponding low-resolution (LR) compressed videos. Recently,\nsome compressed video SR methods attempt to exploit the spatio-temporal\ninformation in the frequency domain, showing great promise in super-resolution\nperformance. However, these methods do not differentiate various frequency\nsubbands spatially or capture the temporal frequency dynamics, potentially\nleading to suboptimal results. In this paper, we propose a deep frequency-based\ncompressed video SR model (FCVSR) consisting of a motion-guided adaptive\nalignment (MGAA) network and a multi-frequency feature refinement (MFFR)\nmodule. Additionally, a frequency-aware contrastive loss is proposed for\ntraining FCVSR, in order to reconstruct finer spatial details. The proposed\nmodel has been evaluated on three public compressed video super-resolution\ndatasets, with results demonstrating its effectiveness when compared to\nexisting works in terms of super-resolution performance (up to a 0.14dB gain in\nPSNR over the second-best model) and complexity.",
        "We propose a method for generating fly-through videos of a scene, from a\nsingle image and a given camera trajectory. We build upon an image-to-video\nlatent diffusion model. We condition its UNet denoiser on the camera\ntrajectory, using four techniques. (1) We condition the UNet's temporal blocks\non raw camera extrinsics, similar to MotionCtrl. (2) We use images containing\ncamera rays and directions, similar to CameraCtrl. (3) We reproject the initial\nimage to subsequent frames and use the resulting video as a condition. (4) We\nuse 2D<=>3D transformers to introduce a global 3D representation, which\nimplicitly conditions on the camera poses. We combine all conditions in a\nContolNet-style architecture. We then propose a metric that evaluates overall\nvideo quality and the ability to preserve details with view changes, which we\nuse to analyze the trade-offs of individual and combined conditions. Finally,\nwe identify an optimal combination of conditions. We calibrate camera positions\nin our datasets for scale consistency across scenes, and we train our scene\nexploration model, CamCtrl3D, demonstrating state-of-theart results.",
        "Convolutional Neural Networks (CNNs) serve as the workhorse of deep learning,\nfinding applications in various fields that rely on images. Given sufficient\ndata, they exhibit the capacity to learn a wide range of concepts across\ndiverse settings. However, a notable limitation of CNNs is their susceptibility\nto overfitting when trained on small datasets. The augmentation of such\ndatasets can significantly enhance CNN performance by introducing additional\ndata points for learning. In this study, we explore the effectiveness of 11\ndifferent sets of data augmentation techniques, which include three novel sets\nproposed in this work. The first set of data augmentation employs pairwise\nchannel transfer, transferring Red, Green, Blue, Hue, and Saturation values\nfrom randomly selected images in the database to all images in the dataset. The\nsecond set introduces a novel occlusion approach, where objects in the images\nare occluded by randomly selected objects from the dataset. The third set\ninvolves a novel masking approach, using vertical, horizontal, circular, and\ncheckered masks to occlude portions of the images. In addition to these novel\ntechniques, we investigate other existing augmentation methods, including\nrotation, horizontal and vertical flips, resizing, translation, blur, color\njitter, and random erasing, and their effects on accuracy and overfitting. We\nfine-tune a base EfficientNet-B0 model for each augmentation method and conduct\na comparative analysis to showcase their efficacy. For the evaluation and\ncomparison of these augmentation techniques, we utilize the Caltech-101\ndataset. The ensemble of image augmentation techniques proposed emerges as the\nmost effective on the Caltech-101 dataset. The results demonstrate that diverse\ndata augmentation techniques present a viable means of enhancing datasets for\nimproved image classification.",
        "Medical image segmentation is challenging due to the diversity of medical\nimages and the lack of labeled data, which motivates recent developments in\nfederated semi-supervised learning (FSSL) to leverage a large amount of\nunlabeled data from multiple centers for model training without sharing raw\ndata. However, what remains under-explored in FSSL is the domain shift problem\nwhich may cause suboptimal model aggregation and low effectivity of the\nutilization of unlabeled data, eventually leading to unsatisfactory performance\nin unseen domains. In this paper, we explore this previously ignored scenario,\nnamely domain generalized federated semi-supervised learning (FedSemiDG), which\naims to learn a model in a distributed manner from multiple domains with\nlimited labeled data and abundant unlabeled data such that the model can\ngeneralize well to unseen domains. We present a novel framework, Federated\nGeneralization-Aware SemiSupervised Learning (FGASL), to address the challenges\nin FedSemiDG by effectively tackling critical issues at both global and local\nlevels. Globally, we introduce Generalization-Aware Aggregation (GAA),\nassigning adaptive weights to local models based on their generalization\nperformance. Locally, we use a Dual-Teacher Adaptive Pseudo Label Refinement\n(DR) strategy to combine global and domain-specific knowledge, generating more\nreliable pseudo labels. Additionally, Perturbation-Invariant Alignment (PIA)\nenforces feature consistency under perturbations, promoting domain-invariant\nlearning. Extensive experiments on three medical segmentation tasks (cardiac\nMRI, spine MRI and bladder cancer MRI) demonstrate that our method\nsignificantly outperforms state-of-the-art FSSL and domain generalization\napproaches, achieving robust generalization on unseen domains.",
        "Deep learning techniques have revolutionized the infrared and visible image\nfusion (IVIF), showing remarkable efficacy on complex scenarios. However,\ncurrent methods do not fully combine frequency domain features with global\nsemantic information, which will result in suboptimal extraction of global\nfeatures across modalities and insufficient preservation of local texture\ndetails. To address these issues, we propose Wavelet-Mamba (W-Mamba), which\nintegrates wavelet transform with the state-space model (SSM). Specifically, we\nintroduce Wavelet-SSM module, which incorporates wavelet-based frequency domain\nfeature extraction and global information extraction through SSM, thereby\neffectively capturing both global and local features. Additionally, we propose\na cross-modal feature attention modulation, which facilitates efficient\ninteraction and fusion between different modalities. The experimental results\nindicate that our method achieves both visually compelling results and superior\nperformance compared to current state-of-the-art methods. Our code is available\nat https:\/\/github.com\/Lmmh058\/W-Mamba.",
        "This work aims to discuss the current landscape of kinematic analysis tools,\nranging from the state-of-the-art in sports biomechanics such as inertial\nmeasurement units (IMUs) and retroreflective marker-based optical motion\ncapture (MoCap) to more novel approaches from the field of computing such as\nhuman pose estimation and human mesh recovery. Primarily, this comparative\nanalysis aims to validate the use of marker-less MoCap techniques in a clinical\nsetting by showing that these marker-less techniques are within a reasonable\nrange for kinematics analysis compared to the more cumbersome and less portable\nstate-of-the-art tools. Not only does marker-less motion capture using human\npose estimation produce results in-line with the results of both the IMU and\nMoCap kinematics but also benefits from a reduced set-up time and reduced\npractical knowledge and expertise to set up. Overall, while there is still room\nfor improvement when it comes to the quality of the data produced, we believe\nthat this compromise is within the room of error that these low-speed actions\nthat are used in small clinical tests.",
        "Automatic medical image segmentation plays a crucial role in computer aided\ndiagnosis. However, fully supervised learning approaches often require\nextensive and labor-intensive annotation efforts. To address this challenge,\nweakly supervised learning methods, particularly those using extreme points as\nsupervisory signals, have the potential to offer an effective solution. In this\npaper, we introduce Deep Extreme Point Tracing (DEPT) integrated with\nFeature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image\nsegmentation. Notably, our method generates pseudo labels by identifying the\nlowest-cost path that connects all extreme points on the feature map-based cost\nmatrix. Additionally, an iterative training strategy is proposed to refine\npseudo labels progressively, enabling continuous network improvement.\nExperimental results on two public datasets demonstrate the effectiveness of\nour proposed method. The performance of our method approaches that of the fully\nsupervised method and outperforms several existing weakly supervised methods.",
        "The Central Molecular Zone (CMZ) of the Galactic Center (GC) region of the\nMilky Way contains a substantial fraction of the molecular mass of the Galaxy\n>10e7 solar masses yet exhibits an order of magnitude lower star formation\nefficiency (SFE) than expected given the high densities found in this region.\nThere are multiple possible explanations for the depressed SFE in the CMZ, like\nfeedback, strong turbulence, longer free-fall timescales, and high magnetic\nfield strengths. It is currently unclear which of these mechanisms is the\ndominant inhibitor of star formation in the CMZ. It is important to understand\nthe star formation process in the extreme environment of the CMZ because it is\nthe only Galactic nuclear region we are able to study at high spatial\nresolutions with current observatories. One way to determine the relative\nimportance of the different SFE inhibiting mechanisms is through multi-spatial\nand multi-frequency polarimetric observations of the CMZ. Such observations\nwill provide insight into the behavior of the magnetic field in this unique\nenvironment. These observations will complement radio observations of\nnon-thermal structures revealing the magnetic field morphology and\npolarization. The PRobe far--Infrared Mission for Astrophysics (PRIMA) will be\nuniquely capable of contributing to such explorations by providing unique\nresolutions and frequencies for polarimetric observations. The PRIMAger\ninstrument will yield polarimetric observations covering the wavelength range\n80 -- 261 um with beam sizes ranging from 11 -- 28'', capabilities that\ncomplement existing and upcoming observatories.",
        "Tremendous progress has been made in deep stereo matching to excel on\nbenchmark datasets through per-domain fine-tuning. However, achieving strong\nzero-shot generalization - a hallmark of foundation models in other computer\nvision tasks - remains challenging for stereo matching. We introduce\nFoundationStereo, a foundation model for stereo depth estimation designed to\nachieve strong zero-shot generalization. To this end, we first construct a\nlarge-scale (1M stereo pairs) synthetic training dataset featuring large\ndiversity and high photorealism, followed by an automatic self-curation\npipeline to remove ambiguous samples. We then design a number of network\narchitecture components to enhance scalability, including a side-tuning feature\nbackbone that adapts rich monocular priors from vision foundation models to\nmitigate the sim-to-real gap, and long-range context reasoning for effective\ncost volume filtering. Together, these components lead to strong robustness and\naccuracy across domains, establishing a new standard in zero-shot stereo depth\nestimation. Project page: https:\/\/nvlabs.github.io\/FoundationStereo\/",
        "Combinatorial optimization problems, such as scheduling and route planning,\nare crucial in various industries but are computationally intractable due to\ntheir NP-hard nature. Neural Combinatorial Optimization methods leverage\nmachine learning to address these challenges but often depend on sequential\ndecision-making, which is prone to error accumulation as small mistakes\npropagate throughout the process. Inspired by self-evaluation techniques in\nLarge Language Models, we propose a novel framework that generates and\nevaluates subsets of assignments, moving beyond traditional stepwise\napproaches. Applied to the Job-Shop Scheduling Problem, our method integrates a\nheterogeneous graph neural network with a Transformer to build a policy model\nand a self-evaluation function. Experimental validation on challenging,\nwell-known benchmarks demonstrates the effectiveness of our approach,\nsurpassing state-of-the-art methods.",
        "We investigate the increase in efficiency of simulated and parallel tempering\nMCMC algorithms when using non-reversible updates to give them \"momentum\". By\nmaking a connection to a certain simple discrete Markov chain, we show that,\nunder appropriate assumptions, the non-reversible algorithms still exhibit\ndiffusive behaviour, just on a different time scale. We use this to argue that\nthe optimally scaled versions of the non-reversible algorithms are indeed more\nefficient than the optimally scaled versions of their traditional reversible\ncounterparts, but only by a modest speed-up factor of about 42%.",
        "In this paper, I investigate the effectiveness of dataset cartography for\nextractive question answering on the SQuAD dataset. I begin by analyzing\nannotation artifacts in SQuAD and evaluate the impact of two adversarial\ndatasets, AddSent and AddOneSent, on an ELECTRA-small model. Using training\ndynamics, I partition SQuAD into easy-to-learn, ambiguous, and hard-to-learn\nsubsets. I then compare the performance of models trained on these subsets to\nthose trained on randomly selected samples of equal size. Results show that\ntraining on cartography-based subsets does not improve generalization to the\nSQuAD validation set or the AddSent adversarial set. While the hard-to-learn\nsubset yields a slightly higher F1 score on the AddOneSent dataset, the overall\ngains are limited. These findings suggest that dataset cartography provides\nlittle benefit for adversarial robustness in SQuAD-style QA tasks. I conclude\nby comparing these results to prior findings on SNLI and discuss possible\nreasons for the observed differences.",
        "Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{{Cross}}, a novel\nframework that seamlessly extends existing TGNNs for TTAG modeling. The key\nidea is to employ the advanced large language models (LLMs) to extract the\ndynamic semantics in text space and then generate expressive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the {Cross} framework, which empowers the LLM to offer\nthe temporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experimental results on four public datasets and one practical\nindustrial dataset demonstrate {Cross}'s significant effectiveness and\nrobustness.",
        "Scaling up a superconducting quantum computer will likely require quantum\ncommunication between remote chips, which can be implemented using an itinerant\nmicrowave photon in a transmission line. To realize high-fidelity\ncommunication, it is essential to control the frequency and temporal shape of\nthe microwave photon. In this work, we demonstrate the generation of\nfrequency-tunable shaped microwave photons without resorting to any\nfrequency-tunable circuit element. We develop a framework which treats a\nmicrowave resonator as a band-pass filter mediating the interaction between a\nsuperconducting qubit and the modes in the transmission line. This\ninterpretation allows us to stimulate the photon emission by an off-resonant\ndrive signal. We characterize how the frequency and temporal shape of the\ngenerated photon depends on the frequency and amplitude of the drive signal. By\nmodulating the drive amplitude and frequency, we achieve a frequency tunability\nof 40 MHz while maintaining the photon mode shape time-symmetric.Through\nmeasurements of the quadrature amplitudes of the emitted photons, we\ndemonstrate consistently high state and process fidelities around 95\\% across\nthe tunable frequency range. Our hardware-efficient approach eliminates the\nneed for additional biasing lines typically required for frequency tuning,\noffering a simplified architecture for scalable quantum communication.",
        "Large Language Models (LLMs) are revolutionizing medical diagnostics by\nenhancing both disease classification and clinical decision-making. In this\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\nassessed their predictive accuracy at both the disease and category levels, as\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\nAutoimmune Disease classification with 100% accuracy. Both models, however,\nstruggled with Respiratory Disease classification, recording accuracies of only\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\nconfidence scores revealed that DeepSeek R1 provided high-confidence\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\nconsiderations regarding bias, model interpretability, and data privacy are\nalso discussed to ensure the responsible integration of LLMs into clinical\npractice. Overall, our findings offer valuable insights into the strengths and\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\nenhancements in AI-driven healthcare.",
        "We present Attentive Reasoning Queries (ARQs), a novel structured reasoning\napproach that significantly improves instruction-following in Large Language\nModels through domain-specialized reasoning blueprints. While LLMs demonstrate\nremarkable capabilities across diverse tasks, they often fail to maintain\nadherence to complex, use-case-specific instructions during multi-turn\nconversations, presenting challenges for business-critical applications. ARQs\naddress this limitation by guiding LLMs through systematic reasoning steps with\ntargeted queries that reinstate critical instructions and facilitate\nintermediate reasoning throughout the completion process. In extensive testing\nwithin Parlant, our framework for reliable customer-facing agents in which ARQs\nwere born out of necessity, they achieved a 90.2% success rate across 87 test\nscenarios, outperforming both Chain-of-Thought reasoning (86.1%) and direct\nresponse generation (81.5%). ARQs showed particular strength in addressing\npersistent failure modes like guideline re-application and hallucination\nprevention. Our analysis also revealed that ARQs can potentially be more\ncomputationally efficient than free-form reasoning when carefully designed.\nThese findings demonstrate that structured reasoning approaches provide\neffective mechanisms for controlling how LLMs process information and make\ndecisions in complex scenarios.",
        "The $\\beta$-delayed neutron emission in the $^{85, 86}$As $\\beta$-decays has\nbeen measured at the Ion Guide Isotope Separator On Line facility of the\nAccelerator Laboratory of the University of Jyv\\\"askyl\\\"a. The complete\n$\\beta$-decays have been studied with a complex setup that consists of a\nplastic scintillator for $\\beta$-particles, MONSTER -- the MOdular Neutron\ntime-of-flight SpectromeTER -- for neutrons, and a high-purity germanium and\nfour LaBr$_3$ crystals for $\\gamma$-rays. The $\\beta$-delayed neutron energy\ndistributions have been determined by unfolding the time-of-flight spectra with\nan innovative methodology based on the iterative Bayesian unfolding method and\naccurate Monte Carlo simulations. The results obtained for $^{85}$As are in\nexcellent agreement with the existing evaluated data, validating the proposed\nmethodology. In the case of $^{86}$As, a stronger neutron intensity at higher\nenergies than previously predicted is discovered.",
        "We propose using the wave function generated by the quantum selected\nconfiguration interaction (QSCI) method as the trial wave function in phaseless\nauxiliary-field quantum Monte Carlo (ph-AFQMC). In the QSCI framework,\nelectronic configurations are sampled from the quantum state realized on a\nquantum computer. These configurations serve as basis states for constructing\nan effective Hamiltonian, which is then diagonalized to obtain the\ncorresponding eigenstate. Using this wave function, ph-AFQMC is performed to\nrecover the dynamical electron correlation across the whole orbital space. The\nuse of the QSCI trial wave function is expected to improve the feasibility of\nthe quantum-classical (QC) hybrid quantum Monte Carlo approach [Nature, 603,\n416 (2022)]. We call this integrated approach QC-QSCI-AFQMC, or QSCI-AFQMC for\nshort. This method is validated across several molecular systems. For H2O and a\nlinear H4 chain, we achieved chemical accuracy in most investigations relative\nto full configuration interaction while utilizing superconducting quantum\ncomputers at Osaka University and RIKEN. Additionally, the application of\nQSCI-AFQMC to the O-H bond dissociation in an organic molecule highlights the\ncomplementary synergy between capturing static correlation on quantum hardware\nand incorporating dynamical correlation via classical post-processing. For the\nN2, when QSCI-AFQMC is executed with a noiseless simulator, it ranks among the\nmost accurate methods compared to various multireference electronic structure\ntheories. Although the proposed method is demonstrated using small active\nspaces on current quantum devices, the concept is not limited to few-qubit\nproblems. The QSCI-AFQMC can compete with state-of-the-art classical\ncomputational techniques, particularly in larger active spaces, displaying\nconsiderable potential for resolving classically intractable problems in\nquantum chemistry.",
        "In 2014 the author showed that in the three-dimensional spherical space,\nalongside with three classical types of flexible octahedra constructed by\nBricard, there exists a new type of flexible octahedra, which was called\nexotic. In the present paper we give a geometric construction for exotic\nflexible octahedra, describe their configuration spaces, and calculate their\nvolumes. We show that the volume of an exotic flexible octahedron is\nnonconstant during the flexion, and moreover the volume remains nonconstant if\nwe replace any set of vertices of the octahedron with their antipodes. So\nexotic flexible octahedra are counterexamples to the Modified Bellows\nConjecture proposed by the author in 2015.",
        "Retrieval Augmented Generation (RAG) is a framework for incorporating\nexternal knowledge, usually in the form of a set of documents retrieved from a\ncollection, as a part of a prompt to a large language model (LLM) to\npotentially improve the performance of a downstream task, such as question\nanswering. Different from a standard retrieval task's objective of maximising\nthe relevance of a set of top-ranked documents, a RAG system's objective is\nrather to maximise their total utility, where the utility of a document\nindicates whether including it as a part of the additional contextual\ninformation in an LLM prompt improves a downstream task. Existing studies\ninvestigate the role of the relevance of a RAG context for knowledge-intensive\nlanguage tasks (KILT), where relevance essentially takes the form of answer\ncontainment. In contrast, in our work, relevance corresponds to that of topical\noverlap between a query and a document for an information seeking task.\nSpecifically, we make use of an IR test collection to empirically investigate\nwhether a RAG context comprised of topically relevant documents leads to\nimproved downstream performance. Our experiments lead to the following\nfindings: (a) there is a small positive correlation between relevance and\nutility; (b) this correlation decreases with increasing context sizes (higher\nvalues of k in k-shot); and (c) a more effective retrieval model generally\nleads to better downstream RAG performance.",
        "Dufaycolor, an additive color photography process produced from 1935 to the\nlate 1950s, represents one of the most advanced iterations of this technique.\nThis paper presents ongoing research and development of an open-source\nColor-Screen tool designed to reconstruct the original colors of additive color\nphotographs. We discuss the incorporation of historical measurements of dyes\nused in the production of the color-screen filter (r\\'eseau) to achieve\naccurate color recovery.",
        "The current state-of-the-art theoretical estimations lead to cross-sections\nfor $AA \\to \\gamma \\gamma AA$ which are somewhat smaller than the measured ones\nby the ATLAS and CMS Collaborations, which motivates the searching and\ncalculation of subleading corrections disregarded in these previous studies. In\nthis paper, we estimate the contribution of inelastic channels to the Light -\nby - Light (LbL) scattering in ultraperipheral collisions of heavy ions\n(UPHICs), in which one or both of the incident nuclei dissociate ($A A \\to\n\\gamma \\gamma X Y$ where $X, Y = A, A'$) due to the photon emission. These new\nmechanisms are related to extra emissions that are rather difficult to identify\nat the LHC and can be mistakenly interpreted as enhanced $\\gamma \\gamma \\to\n\\gamma \\gamma$ scattering compared to the Standard Model result. We include\nprocesses of coupling of photons to individual nucleons (protons and neutrons)\nin addition to coherent coupling to the whole nuclei (called standard approach\nhere). Both elastic (nucleon in the ground state) and inelastic (nucleon in an\nexcited state) in the couplings of photons to nucleons are taken into account.\nThe inelastic nucleon fluxes are calculated using CT18qed photon in nucleon\nPDFs. The inelastic photon fluxes are shown and compared to standard photon\nfluxes in the nucleus. In addition, we show the ratio of the inelastic\ncorrections to the standard contribution as a function of diphoton invariant\nmass and photon rapidity difference. We find the maximal effect of the\ninelastic corrections at $M_{\\gamma \\gamma} \\sim$ 14 GeV for the ATLAS rapidity\nand transverse momentum acceptance. Furthermore, the inelastic contribution\nincreases gradually with photon rapidity difference. Our results indicate that\nthe inelastic contributions can increase locally by 10-15 \\% the traditional\n(no nuclear excitation) predictions for the LbL scattering in UPCs."
      ]
    }
  },
  {
    "id":2411.00609,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks",
    "start_abstract":"Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
      ],
      "abstract":[
        "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Using economic value signals from primate prefrontal cortex in\n  neuro-engineering applications",
        "Increased GM-WM in a prefrontal network and decreased GM in the insula\n  and the precuneus are associated with reappraisal usage: A data fusion\n  approach",
        "Multi-Site rs-fMRI Domain Alignment for Autism Spectrum Disorder\n  Auxiliary Diagnosis Based on Hyperbolic Space",
        "Targeting C99 Mediated Metabolic Disruptions with Ketone Therapy in\n  Alzheimer's Disease",
        "Risk and Protective Factors in Parkinsons Disease",
        "Language-specific Tonal Features Drive Speaker-Listener Neural\n  Synchronization",
        "Morphological Neuron Classification Using Machine Learning",
        "Intrinsic motivation as constrained entropy maximization",
        "Latent computing by biological neural networks: A dynamical systems\n  framework",
        "Neural encoding with affine feature response transforms",
        "Causal Spike Timing Dependent Plasticity Prevents Assembly Fusion in\n  Recurrent Networks",
        "Active filtering: a predictive function of recurrent circuits of sensory\n  cortex",
        "A comprehensive and reliable protocol for manual segmentation of the\n  human claustrum using high-resolution MRI",
        "Automated Microsolvation for Minimum Energy Path Construction in\n  Solution",
        "Giant Uncompensated Magnon Spin Currents in X-type Magnets",
        "Hedging with Sparse Reward Reinforcement Learning",
        "Construction A Lattice Design Based on the Truncated Union Bound",
        "Data-Aided Regularization of Direct-Estimate Combiner in Distributed\n  MIMO Systems",
        "Safety Verification of Nonlinear Stochastic Systems via Probabilistic\n  Tube",
        "Coverage errors for Student's t confidence intervals comparable to those\n  in Hall (1988)",
        "Evidence of Athermal Metastable Phase in a Halide Perovskite: Optically\n  Tracked Thermal-Breach Memory",
        "Do We Need to Verify Step by Step? Rethinking Process Supervision from a\n  Theoretical Perspective",
        "The bound and resonant states of $D^{(*)}D^{(*)}$ and\n  $D^{(*)}\\bar{D}^{(*)}$ with the complex scaling method",
        "Feature Learning beyond the Lazy-Rich Dichotomy: Insights from\n  Representational Geometry",
        "Fastest mixing reversible Markov chain on friendship graph: Trade-off\n  between transition probabilities among friends and convergence rate",
        "From Paramagnet to Dipolar Topological Order via Duality and Dipolar SPT",
        "Engagement Zones for a Turn Constrained Pursuer",
        "Benefits of Early Stopping in Gradient Descent for Overparameterized\n  Logistic Regression"
      ],
      "abstract":[
        "Neural signals related to movement can be measured from intracranial\nrecordings and used in brain-machine interface devices (BMI) to restore\nphysical function in impaired patients. In this study, we explore the use of\nmore abstract neural signals related to economic value in a BMI context. Using\ndata collected from the orbitofrontal cortex in non-human primates, we develop\ndeep learning-based neural decoders that can predict the monkey's choice in a\nvalue-based decision-making task. Out-of-sample performance was improved by\naugmenting the training set with synthesized data, showing the feasibility of\nusing limited training data. We further demonstrate that we can predict the\nmonkey's choice sooner using a neural forecasting module that is equipped with\ntask-related information. These findings support the feasibility of user\npreference-informed neuroengineering devices that leverage abstract cognitive\nsignals.",
        "Emotion regulation plays a crucial role in mental health, and difficulties in\nregulating emotions can contribute to psychological disorders. While\nreappraisal and suppression are well-studied strategies, the combined\ncontributions of gray matter (GM) and white matter (WM) to these strategies\nremain unclear due to methodological limitations in previous studies. To\naddress this, we applied a data fusion approach using Parallel Independent\nComponent Analysis (Parallel ICA) to GM and WM MRI images from 165 individuals.\nParallel ICA identified two networks associated with reappraisal usage. Network\n1 included a large lateral and medial prefrontal cortical network, overlapping\nwith the default mode network (DMN) and adjacent WM regions. Higher reappraisal\nfrequency was associated with greater GM-WM density within this network, and\nthis network was negatively correlated with perceived stress. Network 2\nincluded the insula, precuneus, sub-gyral, and lingual gyri in its GM portion,\nshowing a negative association with reappraisal usage. The WM portion, adjacent\nto regions of the central executive network (CEN), was positively associated\nwith reappraisal usage. Regarding suppression, no significant network was\nassociated with this strategy. This study provides new insights into individual\ndifferences in reappraisal use, showing a positive association between\nreappraisal frequency and increased gray and white matter concentration in a\nlarge frontal network, including regions of the frontal DMN and the CEN.\nConversely, subcortical areas exhibited reduced gray and white concentration.",
        "In the medical field, most resting-state fMRI (rs-fMRI) data are collected\nfrom multiple hospital sites. Multi-site rs-fMRI data can increase the volume\nof training data, enabling auxiliary diagnostic algorithms for brain diseases\nto learn more accurate and stable models. However, due to the significant\nheterogeneity and domain shift in rs-fMRI data across different sites, the\naccuracy of auxiliary diagnosis remains unsatisfactory. Moreover, there has\nbeen limited exploration of multi-source domain adaptation algorithms, and the\ninterpretability of models is often poor. To address these challenges, we\nproposed a domain-adaptive algorithm based on hyperbolic space embedding.\nHyperbolic space is naturally suited for representing the topology of complex\nnetworks such as brain functional networks. Therefore, we embedded the brain\nfunctional network into hyperbolic space and constructed the corresponding\nhyperbolic space community network to effectively extract brain network\nrepresentations. To address the heterogeneity of data across different sites\nand the issue of domain shift, we introduce a constraint loss function, HMMD\n(Hyperbolic Maximum Mean Discrepancy), to align the marginal distributions in\nthe hyperbolic space. Additionally, we employ class prototype alignment to\nalign the conditional distributions. This significantly improves the quality of\nbrain representations and enhances diagnostic classification accuracy for\nAutism Spectrum Disorder (ASD). Experimental results demonstrated that the\nproposed algorithm is robust to multi-site heterogeneity and shows promising\npotential for brain network mechanism analysis.",
        "The role of ketone bodies in Alzheimers disease (AD) remains incompletely\nunderstood, particularly regarding their influence on amyloid pathology. While\nbeta}hydroxybutyrate (BHB) has been implicated in neuroprotection, direct\nevidence for its effects on amyloid beta(Abeta) deposition, aggregation, or\nclearance is lacking. Furthermore, whether BHB acts as a disease modifying\nfactor or merely confers transient metabolic benefits remains unclear.\nAddressing this gap is crucial for evaluating the therapeutic potential of\nketone metabolism in AD. Here, we investigated the impact of ketone bodies on\namyloidogenic toxicity using a Drosophila melanogaster model with targeted\nexpression of human amyloid precursor protein (APP), beta secretase 1 (BACE1),\nAbeta, and the C99 fragment, an essential intermediate in Abeta generation.\nSurprisingly, we found that Abeta alone elicited minimal neurotoxicity, whereas\nC99 expression induced pronounced pathological effects, suggesting a critical,\nunderappreciated role of C99 in AD progression. Further analysis revealed that\nC99 driven toxicity was associated with autophagic and lysosomal dysfunction,\nleading to impaired protein clearance, oxidative stress, and mitochondrial\nabnormalities. Using confocal microscopy and lysosomal pH sensitive markers, we\ndemonstrated that BHB treatment restored lysosomal function and alleviated\nthese pathological changes. Protein protein interaction network analysis in C99\nexpressing Drosophila brains identified protein phosphatase methylesterase 1\n(PPME1) activation as a key driver of autophagic impairment, further supported\nby machine learning predictions. Finally, mathematical similarity analysis of\nPPI networks suggested that BHB may exert its neuroprotective effects through\nmTOR inhibition, positioning it as a potential endogenous modulator of AD\nrelated pathology.",
        "Understanding the risk and protective factors associated with Parkinsons\ndisease (PD) is crucial for improving outcomes for patients, individuals at\nrisk, healthcare providers, and healthcare systems. Studying these factors not\nonly enhances our knowledge of the disease but also aids in developing\neffective prevention, management, and treatment strategies. This paper reviews\nthe key risk and protective factors associated with PD, with a particular focus\non the biological mechanisms underlying these factors. Risk factors include\ngenetic mutations, racial predispositions, and environmental exposures, all of\nwhich contribute to an increased likelihood of developing PD or accelerating\ndisease progression. Conversely, protective factors such as regular physical\nexercise, adherence to a Mediterranean diet, and higher urate levels have\ndemonstrated potential to reduce inflammation and support mitochondrial\nfunction, thereby mitigating disease risk. However, identifying and validating\nthese factors presents significant challenges. To overcome challenges, we\npropose several solutions and recommendations. Future research should\nprioritize the development of standardized biomarkers for early diagnosis,\ninvestigate gene-environment interactions in greater depth, and refine animal\nmodels to better mimic human PD pathology. Additionally, we offer actionable\nrecommendations for PD prevention and management, tailored to healthy\nindividuals, patients diagnosed with PD, and healthcare systems. These\nstrategies aim to improve clinical outcomes, enhance quality of life, and\noptimize healthcare delivery for PD.",
        "Verbal communication transmits information across diverse linguistic levels,\nwith neural synchronization (NS) between speakers and listeners emerging as a\nputative mechanism underlying successful exchange. However, the specific speech\nfeatures driving this synchronization and how language-specific versus\nuniversal characteristics facilitate information transfer remain poorly\nunderstood. We developed a novel content-based interbrain encoding model to\ndisentangle the contributions of acoustic and linguistic features to\nspeaker-listener NS during Mandarin storytelling and listening, as measured via\nmagnetoencephalography (MEG). Results revealed robust NS throughout\nfrontotemporal-parietal networks with systematic time lags between speech\nproduction and perception. Crucially, suprasegmental lexical tone features\n(tone categories, pitch height, and pitch contour), essential for lexical\nmeaning in Mandarin, contributed more significantly to NS than either acoustic\nelements or universal segmental units (consonants and vowels). These tonal\nfeatures generated distinctive spatiotemporal NS patterns, creating\nlanguage-specific neural \"communication channels\" that facilitated efficient\nrepresentation sharing between interlocutors. Furthermore, the strength and\npatterns of NS driven by these language-specific features predicted\ncommunication success. These findings demonstrate the neural mechanisms\nunderlying shared representations during verbal exchange and highlight how\nlanguage-specific features can shape neural coupling to optimize information\ntransfer during human communication.",
        "Classification and quantitative characterization of neuronal morphologies\nfrom histological neuronal reconstruction is challenging since it is still\nunclear how to delineate a neuronal cell class and which are the best features\nto define them by. The morphological neuron characterization represents a\nprimary source to address anatomical comparisons, morphometric analysis of\ncells, or brain modeling. The objectives of this paper are (i) to develop and\nintegrate a pipeline that goes from morphological feature extraction to\nclassification and (ii) to assess and compare the accuracy of machine learning\nalgorithms to classify neuron morphologies. The algorithms were trained on 430\ndigitally reconstructed neurons subjectively classified into layers and\/or\nm-types using young and\/or adult development state population of the\nsomatosensory cortex in rats. For supervised algorithms, linear discriminant\nanalysis provided better classification results in comparison with others. For\nunsupervised algorithms, the affinity propagation and the Ward algorithms\nprovided slightly better results.",
        "\"Intrinsic motivation\" refers to the capacity for intelligent systems to be\nmotivated endogenously, i.e. by features of agential architecture itself rather\nthan by learned associations between action and reward. This paper views active\ninference, empowerment, and other formal accounts of intrinsic motivation as\nvariations on the theme of constrained maximum entropy inference, providing a\ngeneral perspective on intrinsic motivation complementary to existing\nframeworks. The connection between free energy and empowerment noted in\nprevious literature is further explored, and it is argued that the\nmaximum-occupancy approach in practice incorporates an implicit model-evidence\nconstraint.",
        "Although individual neurons and neural populations exhibit the phenomenon of\nrepresentational drift, perceptual and behavioral outputs of many neural\ncircuits can remain stable across time scales over which representational drift\nis substantial. These observations motivate a dynamical systems framework for\nneural network activity that focuses on the concept of \\emph{latent processing\nunits,} core elements for robust coding and computation embedded in collective\nneural dynamics. Our theoretical treatment of these latent processing units\nyields five key attributes of computing through neural network dynamics. First,\nneural computations that are low-dimensional can nevertheless generate\nhigh-dimensional neural dynamics. Second, the manifolds defined by neural\ndynamical trajectories exhibit an inherent coding redundancy as a direct\nconsequence of the universal computing capabilities of the underlying dynamical\nsystem. Third, linear readouts or decoders of neural population activity can\nsuffice to optimally subserve downstream circuits controlling behavioral\noutputs. Fourth, whereas recordings from thousands of neurons may suffice for\nnear optimal decoding from instantaneous neural activity patterns, experimental\naccess to millions of neurons may be necessary to predict neural ensemble\ndynamical trajectories across timescales of seconds. Fifth, despite the\nvariable activity of single cells, neural networks can maintain stable\nrepresentations of the variables computed by the latent processing units,\nthereby making computations robust to representational drift. Overall, our\nframework for latent computation provides an analytic description and\nempirically testable predictions regarding how large systems of neurons perform\nrobust computations via their collective dynamics.",
        "Current linearizing encoding models that predict neural responses to sensory\ninput typically neglect neuroscience-inspired constraints that could enhance\nmodel efficiency and interpretability. To address this, we propose a new method\ncalled affine feature response transform (AFRT), which exploits the brain's\nretinotopic organization. Applying AFRT to encode multi-unit activity in areas\nV1, V4, and IT of the macaque brain, we demonstrate that AFRT reduces redundant\ncomputations and enhances the performance of current linearizing encoding\nmodels by segmenting each neuron's receptive field into an affine retinal\ntransform, followed by a localized feature response. Remarkably, by factorizing\nreceptive fields into a sequential affine component with three interpretable\nparameters (for shifting and scaling) and response components with a small\nnumber of feature weights per response, AFRT achieves encoding with orders of\nmagnitude fewer parameters compared to unstructured models. We show that the\nretinal transform of each neuron's encoding agrees well with the brain's\nreceptive field. Together, these findings suggest that this new subset within\nspatial transformer network can be instrumental in neural encoding models of\nnaturalistic stimuli.",
        "The organization of neurons into functionally related assemblies is a\nfundamental feature of cortical networks, yet our understanding of how these\nassemblies maintain distinct identities while sharing members remains limited.\nHere we analyze how spike-timing-dependent plasticity (STDP) shapes the\nformation and stability of overlapping neuronal assemblies in recurrently\ncoupled networks of spiking neuron models. Using numerical simulations and an\nassociated mean-field theory, we demonstrate that the temporal structure of the\nSTDP rule, specifically its degree of causality, critically determines whether\nassemblies that share neurons maintain segregation or merge together after\ntraining is completed. We find that causal STDP rules, where\npotentiation\/depression occurs strictly when presynaptic spikes precede\/proceed\npostsynaptic spikes, allow assemblies to remain distinct even with substantial\noverlap in membership. This stability arises because causal STDP effectively\ncancels the symmetric correlations introduced by common inputs from shared\nneurons. In contrast, acausal STDP rules lead to assembly fusion when overlap\nexceeds a critical threshold, due to unchecked growth of common input\ncorrelations. Our results provide theoretical insight into how\nspike-timing-dependent learning rules can support distributed representation\nwhere individual neurons participate in multiple assemblies while maintaining\nfunctional specificity.",
        "Our brains encode many features of the sensory world into memories: we can\nsing along with songs we have heard before, interpret spoken and written\nlanguage composed of words we have learned, and recognize faces and objects.\nWhere are these memories stored? Each area of the cerebral cortex has a huge\nnumber of local, recurrent, excitatory-excitatory synapses, as many as 500\nmillion per cubic millimeter. Here I review evidence that cortical recurrent\nconnectivity in sensory cortex is a substrate for sensory memories. Evidence\nsuggests that the local recurrent network encodes the structure of natural\nsensory input, and that it does so via active filtering, transforming network\ninputs to boost or select those associated with natural sensation. This is a\nform of predictive processing, in which the cortical recurrent network\nselectively amplifies some input patterns and attenuates others, and a form of\nmemory.",
        "The claustrum is a thin gray matter structure in each brain hemisphere,\ncharacterized by exceptionally high connectivity with nearly all brain regions.\nDespite extensive animal studies on its anatomy and function and growing\nevidence of claustral deficits in neuropsychiatric disorders, its specific\nroles in normal and abnormal human brain function remain largely unknown. This\nis primarily due to its thin and complex morphology, which limits accurate\nanatomical delineation and neural activity isolation in conventional in vivo\nneuroimaging. To facilitate future neuroimaging studies, we developed a\ncomprehensive and reliable manual segmentation protocol based on a\ncellular-resolution brain atlas and high-resolution (0.7^3 mm) MRI data. The\nprotocols involve detailed guidelines to delineate the entire claustrum,\nincluding the inferior parts that have not been clearly described in earlier\nMRI studies. Additionally, we propose a geometric method to parcellate the\nclaustrum into three subregions (the dorsal, ventral, and temporal claustrum)\nalong the superior-to-inferior axis. The mean bilateral claustrum volume in 10\nyoung adults was 3307.5 mm^3, approximately 0.21% of total intracranial volume.\nOur segmentation protocol demonstrated high inter- and intra-rater reliability\n(ICC > 0.89, DSC > 0.85), confirming its replicability. This comprehensive and\nreliable claustrum segmentation protocols will provide a cornerstone for future\nneuroimaging studies of systematic, large-scale investigations of the anatomy\nand the functions of the human claustrum in normal and pathological\npopulations.",
        "Describing chemical reactions in solution on a molecular level is a\nchallenging task due to the high mobility of weakly interacting solvent\nmolecules which requires configurational sampling. For instance, polar and\nprotic solvents can interact strongly with solutes and may interfere in\nreactions. However, to define and identify representative arrangements of\nsolvent molecules modulating a transition state is a non-trivial task. Here, we\npropose to monitor their active participation in the decaying normal mode at a\ntransition state, which defines active solvent molecules. Moreover, it is\ndesirable to prepare a low-dimensional microsolvation model in a well-defined,\nfully automated, high-throughput, and easy-to-deploy fashion, which we propose\nto derive in a stepwise protocol. First, transition state structures are\noptimized in a sufficiently solvated quantum-classical hybrid model, which are\nthen subjected to a re-definition of a then reduced quantum region. From the\nreduced model, minimally microsolvated structures are extracted that contain\nonly active solvent molecules. Modeling the remaining solvation effects is\ndeferred to a continuum model. To establish an easy-to-use free-energy model,\nwe combine the standard thermochemical gas-phase model with a correction for\nthe cavity entropy in solution. We assess our microsolvation and free-energy\nmodels for methanediol formation from formaldehyde, for the hydration of carbon\ndioxide (which we consider in a solvent mixture to demonstrate the versatility\nof our approach), and, finally, for the chlorination of phenol with\nhypochlorous acid.",
        "Magnon spin currents in insulating magnets are useful for low-power\nspintronics. However, in magnets stacked by antiferromagnetic (AFM) exchange\ncoupling, which have recently aroused significant interest for potential\napplications in spintronics, these currents are largely counteracted by\nopposite magnetic sublattices, thus suppressing their net effect. Contrary to\nthis common observation, here, we show that magnets with X-type AFM stacking,\nwhere opposite magnetic sublattices form orthogonal intersecting chains,\nsupport giant magnon spin currents with minimal compensation. Our model\nHamiltonian calculations predict magnetic chain locking of magnon spin currents\nin these X-type magnets, significantly reducing their compensation ratio. In\naddition, the one-dimensional nature of the chain-like magnetic sublattices\nenhances magnon spin conductivities surpassing those of two-dimensional\nferromagnets and canonical altermagnets. Notably, uncompensated X-type magnets,\nsuch as odd-layer antiferromagnets and ferrimagnets, can exhibit magnon spin\ncurrents polarized opposite to those expected by their net magnetization. These\nunprecedented properties of X-type magnets, combined with their inherent\nadvantages resulting from AFM coupling, offer a promising new path for\nlow-power high-performance spintronics.",
        "Derivatives, as a critical class of financial instruments, isolate and trade\nthe price attributes of risk assets such as stocks, commodities, and indices,\naiding risk management and enhancing market efficiency. However, traditional\nhedging models, constrained by assumptions such as continuous trading and zero\ntransaction costs, fail to satisfy risk control requirements in complex and\nuncertain real-world markets.\n  With advances in computing technology and deep learning, data-driven trading\nstrategies are becoming increasingly prevalent. This thesis proposes a\nderivatives hedging framework integrating deep learning and reinforcement\nlearning. The framework comprises a probabilistic forecasting model and a\nhedging agent, enabling market probability prediction, derivative pricing, and\nhedging.\n  Specifically, we design a spatiotemporal attention-based probabilistic\nfinancial time series forecasting Transformer to address the scarcity of\nderivatives hedging data. A low-rank attention mechanism compresses\nhigh-dimensional assets into a low-dimensional latent space, capturing\nnonlinear asset relationships. The Transformer models sequential dependencies\nwithin this latent space, improving market probability forecasts and\nconstructing an online training environment for downstream hedging tasks.\n  Additionally, we incorporate generalized geometric Brownian motion to develop\na risk-neutral pricing approach for derivatives. We model derivatives hedging\nas a reinforcement learning problem with sparse rewards and propose a behavior\ncloning-based recurrent proximal policy optimization (BC-RPPO) algorithm. This\npretraining-finetuning framework significantly enhances the hedging agent's\nperformance. Numerical experiments in the U.S. and Chinese financial markets\ndemonstrate our method's superiority over traditional approaches.",
        "This paper considers $n= 128$ dimensional construction A lattice design,\nusing binary codes with known minimum Hamming distance and codeword\nmultiplicity, the number of minimum weight codeword. A truncated theta series\nof the lattice is explicitly given to obtain the truncated union bound to\nestimate the word error rate under maximum likelihood decoding. The best\ncomponent code is selected by minimizing the required volume-to-noise ratio\n(VNR) for a target word error rate $P_e$. The estimate becomes accurate for\n$P_e \\leq 10^{-4}$, and design examples are given with the best extended BCH\ncodes and polar codes for $P_e= 10^{-4}$ to $10^{-8}$. A lower error rate is\nachieved compared to that by the classic balanced distance rule and the equal\nerror probability rule. The $(128, 106, 8)$ EBCH code gives the best-known\n$n=128$ construction A lattice at $P_e= 10^{-5}$.",
        "This paper explores the data-aided regularization of the direct-estimate\ncombiner in the uplink of a distributed multiple-input multiple-output system.\nThe network-wide combiner can be computed directly from the pilot signal\nreceived at each access point, eliminating the need for explicit channel\nestimation. However, the sample covariance matrix of the received pilot signal\nthat is used in its computation may significantly deviate from the actual\ncovariance matrix when the number of pilot symbols is limited. To address this,\nwe apply a regularization to the sample covariance matrix using a shrinkage\ncoefficient based on the received data signal. Initially, the shrinkage\ncoefficient is determined by minimizing the difference between the sample\ncovariance matrices obtained from the received pilot and data signals. Given\nthe limitations of this approach in interference-limited scenarios, the\nshrinkage coefficient is iteratively optimized using the sample mean squared\nerror of the hard-decision symbols, which is more closely related to the actual\nsystem's performance, e.g., the symbol error rate (SER). Numerical results\ndemonstrate that the proposed regularization of the direct-estimate combiner\nsignificantly enhances the SER, particularly when the number of pilot symbols\nis limited.",
        "We address the problem of safety verification for nonlinear stochastic\nsystems, specifically the task of certifying that system trajectories remain\nwithin a safe set with high probability. To tackle this challenge, we adopt a\nset-erosion strategy, which decouples the effects of stochastic disturbances\nfrom deterministic dynamics. This approach converts the stochastic safety\nverification problem on a safe set into a deterministic safety verification\nproblem on an eroded subset of the safe set. The success of this strategy\nhinges on the depth of erosion, which is determined by a probabilistic tube\nthat bounds the deviation of stochastic trajectories from their corresponding\ndeterministic trajectories. Our main contribution is the establishment of a\ntight bound for the probabilistic tube of nonlinear stochastic systems. To\nobtain a probabilistic bound for stochastic trajectories, we adopt a\nmartingale-based approach. The core innovation lies in the design of a novel\nenergy function associated with the averaged moment generating function, which\nforms an affine martingale, a generalization of the traditional c-martingale.\nUsing this energy function, we derive a precise bound for the probabilistic\ntube. Furthermore, we enhance this bound by incorporating the union-bound\ninequality for strictly contractive dynamics. By integrating the derived\nprobabilistic tubes into the set-erosion strategy, we demonstrate that the\nsafety verification problem for nonlinear stochastic systems can be reduced to\na deterministic safety verification problem. Our theoretical results are\nvalidated through applications in reachability-based safety verification and\nsafe controller synthesis, accompanied by several numerical examples that\nillustrate their effectiveness.",
        "Table 1 of Hall (1988) contains asymptotic coverage error formulas for some\nnonparametric approximate 95% confidence intervals for the mean based on $n$\nIID samples. The table includes an entry for an interval based on the central\nlimit theorem using Gaussian quantiles and the Gaussian maximum likelihood\nvariance estimate. It is missing an entry for the very widely used Student $t$\nconfidence intervals. This note makes a mild numerical correction for the\nGaussian entry and provides an entry for the Student $t$ intervals. For\nskewness $\\gamma$ and kurtosis $\\kappa$, the corrected Gaussian formula is\n$0.14\\kappa -2.16\\gamma^2-3.42$ and the formula for the $t$ intervals is\n$0.14\\kappa -2.16\\gamma^2$. The impetus to revisit this estimate arose from the\nsurprisingly robust performance of Student's t statistic in randomized\nquasi-Monte Carlo sampling.",
        "Halide perovskite materials have been extensively studied in the last decade\nbecause of their impressive optoelectronic properties. However, their one\ncharacteristic that is uncommon for semiconductors is that many undergo\nthermally induced structural phase transitions. The transition is hysteretic,\nwith the hysteresis window marking the boundary of the metastable phase. We\nhave discovered that in methylammonium lead iodide, this hysteretic metastable\nphase is athermal, meaning it shows almost no temporal phase evolution under\nisothermal conditions. We also show that a large number of distinguishable\nmetastable states can be prepared following different thermal pathways.\nFurthermore, under a reversible thermal perturbation, the states in the\nmetastable phase either show return-point memory or undergo a systematic\nnonrecoverable phase evolution, depending on the thermal history and the sign\nof the temperature perturbation. Since the phase fraction can be probed with\nextreme sensitivity via luminescence, we have an optically retrievable memory\nthat reliably records any breach in temperature stability. Such thermal-breach\nmemory in athermal martensites, of which there are numerous examples, may be\nuseful for tagging packages requiring strict temperature control during\ntransportation or preservation.",
        "As large language models have evolved, it has become crucial to distinguish\nbetween process supervision and outcome supervision -- two key reinforcement\nlearning approaches to complex reasoning tasks. While process supervision\noffers intuitive advantages for long-term credit assignment, the precise\nrelationship between these paradigms has remained an open question.\nConventional wisdom suggests that outcome supervision is fundamentally more\nchallenging due to the trajectory-level coverage problem, leading to\nsignificant investment in collecting fine-grained process supervision data.\n  In this paper, we take steps towards resolving this debate. Our main theorem\nshows that, under standard data coverage assumptions, reinforcement learning\nthrough outcome supervision is no more statistically difficult than through\nprocess supervision, up to polynomial factors in horizon. At the core of this\nresult lies the novel Change of Trajectory Measure Lemma -- a technical tool\nthat bridges return-based trajectory measure and step-level distribution shift.\nFurthermore, for settings with access to a verifier or a rollout capability, we\nprove that any policy's advantage function can serve as an optimal process\nreward model, providing a direct connection between outcome and process\nsupervision. These findings suggest that the empirically observed performance\ngap -- if any -- between outcome and process supervision likely stems from\nalgorithmic limitations rather than inherent statistical difficulties,\npotentially transforming how we approach data collection and algorithm design\nfor reinforcement learning.",
        "We perform a systematic study of the possible molecular states composed of a\npair of heavy mesons such as $D^{(*)}D^{(*)}$, $D^{(*)}\\bar{D}^{(*)}$ in the\nframework of the one-boson-exchange model. The exchanged bosons include the\npseudoscalar, scalar and vector mesons($\\pi$, $\\sigma$, $\\rho$, $\\omega$). We\nuse the Bonn approximation to get the interaction potential of\none-boson-exchange model, then apply the complex scaling method to calculate\nthe bound and resonant states. The results indicate that the $D^{(*)}D^{(*)}$\nand $D^{(*)}\\bar{D}^{(*)}$ system can not only form several bound states, but\nalso a P-wave resonant state. The hadron molecular state model can explain the\nstructure of $T_{cc}^+$ as a bound state $DD^{*}$ with quantum number $I(J^P) =\n0(1^+)$. In addition, we also discovered other bound and resonant states, which\nhave the potential to be observed experimentally.",
        "The ability to integrate task-relevant information into neural\nrepresentations is a fundamental aspect of both biological and artificial\nintelligence. To enable theoretical analysis, recent work has examined whether\na network learns task-relevant features (rich learning) or resembles a random\nfeature model (or a kernel machine, i.e., lazy learning). However, this simple\nlazy-versus-rich dichotomy overlooks the possibility of various subtypes of\nfeature learning that emerge from different architectures, learning rules, and\ndata properties. Furthermore, most existing approaches emphasize weight\nmatrices or neural tangent kernels, limiting their applicability to\nneuroscience because they do not explicitly characterize representations.\n  In this work, we introduce an analysis framework based on representational\ngeometry to study feature learning. Instead of analyzing what are the learned\nfeatures, we focus on characterizing how task-relevant representational\nmanifolds evolve during the learning process. In both theory and experiment, we\nfind that when a network learns features useful for solving a task, the\ntask-relevant manifolds become increasingly untangled. Moreover, by tracking\nchanges in the underlying manifold geometry, we uncover distinct learning\nstages throughout training, as well as different learning strategies associated\nwith training hyperparameters, uncovering subtypes of feature learning beyond\nthe lazy-versus-rich dichotomy. Applying our method to neuroscience and machine\nlearning, we gain geometric insights into the structural inductive biases of\nneural circuits solving cognitive tasks and the mechanisms underlying\nout-of-distribution generalization in image classification. Our framework\nprovides a novel geometric perspective for understanding and quantifying\nfeature learning in both artificial and biological neural networks.",
        "A long-standing goal of social network research has been to alter the\nproperties of network to achieve the desired outcome. In doing so, DeGroot's\nconsensus model has served as the popular choice for modeling the information\ndiffusion and opinion formation in social networks. Achieving a trade-off\nbetween the cost associated with modifications made to the network and the\nspeed of convergence to the desired state has shown to be a critical factor.\nThis has been treated as the Fastest Mixing Markov Chain (FMMC) problem over a\ngraph with given transition probabilities over a subset of edges. Addressing\nthis multi-objective optimization problem over the friendship graph, this paper\nhas provided the corresponding Pareto optimal points or the Pareto frontier. In\nthe case of friendship graph with at least three blades, it is shown that the\nPareto frontier is reduced to a global minimum point which is same as the\noptimal point corresponding to the minimum spanning tree of the friendship\ngraph, i.e., the star topology. Furthermore, a lower limit for transition\nprobabilities among friends has been provided, where values higher than this\nlimit do not have any impact on the convergence rate.",
        "A scheme for the adaptive preparation of a topological state with dipole\nsymmetry, dubbed the dipolar topological state (dTS), which serves as an\nexample of translation symmetry-enriched topological phase, is proposed. The\nmidcircuit state emerging during the preparation process is identified as a\ntwo-dimensional symmetry-protected topological (SPT) state protected by dipole\nbundle symmetry alongside charge and 1-form symmetries. The non-trivial\nboundary modes of the dipolar SPT state exhibiting the spontaneous breaking of\ncharge and dipole bundle symmetries are analyzed. The duality map between the\nparamagnetic state and the dipolar topological state is established in the\nframework of the {\\it simultaneous gauging} of two charge symmetries and one\ndipole symmetry that cannot be reduced as sequential gauging of the individual\nsymmetry. Leveraging this duality, we work out the phase diagram of the dipolar\ntopological state under perturbations by various transverse fields.",
        "This work derives two basic engagement zone models, describing regions of\npotential risk or capture for a mobile vehicle by a pursuer. The pursuer is\nmodeled as having turn-constraints rather than simple motion. Turn-only\n(C-Paths) and turn-straight (CS-Paths) paths are considered for the pursuer of\nlimited range. Following the derivation, a simulation of a vehicle avoiding the\npursuer's engagement zone is provided.",
        "In overparameterized logistic regression, gradient descent (GD) iterates\ndiverge in norm while converging in direction to the maximum $\\ell_2$-margin\nsolution -- a phenomenon known as the implicit bias of GD. This work\ninvestigates additional regularization effects induced by early stopping in\nwell-specified high-dimensional logistic regression. We first demonstrate that\nthe excess logistic risk vanishes for early-stopped GD but diverges to infinity\nfor GD iterates at convergence. This suggests that early-stopped GD is\nwell-calibrated, whereas asymptotic GD is statistically inconsistent. Second,\nwe show that to attain a small excess zero-one risk, polynomially many samples\nare sufficient for early-stopped GD, while exponentially many samples are\nnecessary for any interpolating estimator, including asymptotic GD. This\nseparation underscores the statistical benefits of early stopping in the\noverparameterized regime. Finally, we establish nonasymptotic bounds on the\nnorm and angular differences between early-stopped GD and $\\ell_2$-regularized\nempirical risk minimizer, thereby connecting the implicit regularization of GD\nwith explicit $\\ell_2$-regularization."
      ]
    }
  },
  {
    "id":2411.00726,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema",
    "start_abstract":"This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Active Learning from Scene Embeddings for End-to-End Autonomous Driving",
        "Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical\n  Anomaly Detection",
        "HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and\n  Generation",
        "Learning Extremely High Density Crowds as Active Matters",
        "StreamingRAG: Real-time Contextual Retrieval and Generation Framework",
        "DynamicVis: An Efficient and General Visual Foundation Model for Remote\n  Sensing Image Understanding",
        "MFM-DA: Instance-Aware Adaptor and Hierarchical Alignment for Efficient\n  Domain Adaptation in Medical Foundation Models",
        "SP-SLAM: Neural Real-Time Dense SLAM With Scene Priors",
        "Focus-N-Fix: Region-Aware Fine-Tuning for Text-to-Image Generation",
        "Semantics-aware Test-time Adaptation for 3D Human Pose Estimation",
        "Multi-view Video-Pose Pretraining for Operating Room Surgical Activity\n  Recognition",
        "ST-Think: How Multimodal Large Language Models Reason About 4D Worlds\n  from Ego-Centric Videos",
        "Similarity-Guided Layer-Adaptive Vision Transformer for UAV Tracking",
        "Residual connections provably mitigate oversmoothing in graph neural\n  networks",
        "Artificial intelligence for objective assessment of acrobatic movements:\n  How to apply machine learning for identifying tumbling elements in cheer\n  sports",
        "Cost-Efficient Continual Learning with Sufficient Exemplar Memory",
        "Magnetic Interactions in the Polar Ferrimagnet with a Bipartite\n  Structure",
        "Dynamic Basis Function Generation for Network Revenue Management",
        "Sink-free orientations: a local sampler with applications",
        "Exploring coronal abundances of M dwarfs at moderate activity levels",
        "Counterdiabatic Driving with Performance Guarantees",
        "AI Explainability for Power Electronics: From a Lipschitz Continuity\n  Perspective",
        "Nuclear Spin Induced Transparency",
        "RAMOTS: A Real-Time System for Aerial Multi-Object Tracking based on\n  Deep Learning and Big Data Technology",
        "The Role of Planetary-Scale Waves on the Stratospheric Superrotation in\n  Titan's Atmosphere",
        "An examination of the extended Hong-Ou-Mandel effect and considerations\n  for experimental detection",
        "Supercritical phase transition on the Toeplitz algebra of $\\mathbb\n  N^\\times \\ltimes \\mathbb Z$",
        "Experimental Test of Nonlocality Limits from Relativistic Independence"
      ],
      "abstract":[
        "In the field of autonomous driving, end-to-end deep learning models show\ngreat potential by learning driving decisions directly from sensor data.\nHowever, training these models requires large amounts of labeled data, which is\ntime-consuming and expensive. Considering that the real-world driving data\nexhibits a long-tailed distribution where simple scenarios constitute a\nmajority part of the data, we are thus inspired to identify the most\nchallenging scenarios within it. Subsequently, we can efficiently improve the\nperformance of the model by training with the selected data of the highest\nvalue. Prior research has focused on the selection of valuable data by\nempirically designed strategies. However, manually designed methods suffer from\nbeing less generalizable to new data distributions. Observing that the BEV\n(Bird's Eye View) features in end-to-end models contain all the information\nrequired to represent the scenario, we propose an active learning framework\nthat relies on these vectorized scene-level features, called SEAD. The\nframework selects initial data based on driving-environmental information and\nincremental data based on BEV features. Experiments show that we only need 30\\%\nof the nuScenes training data to achieve performance close to what can be\nachieved with the full dataset. The source code will be released.",
        "Unsupervised anomaly detection using deep learning has garnered significant\nresearch attention due to its broad applicability, particularly in medical\nimaging where labeled anomalous data are scarce. While earlier approaches\nleverage generative models like autoencoders and generative adversarial\nnetworks (GANs), they often fall short due to overgeneralization. Recent\nmethods explore various strategies, including memory banks, normalizing flows,\nself-supervised learning, and knowledge distillation, to enhance\ndiscrimination. Among these, knowledge distillation, particularly reverse\ndistillation, has shown promise. Following this paradigm, we propose a novel\nscale-aware contrastive reverse distillation model that addresses two key\nlimitations of existing reverse distillation methods: insufficient feature\ndiscriminability and inability to handle anomaly scale variations.\nSpecifically, we introduce a contrastive student-teacher learning approach to\nderive more discriminative representations by generating and exploring\nout-of-normal distributions. Further, we design a scale adaptation mechanism to\nsoftly weight contrastive distillation losses at different scales to account\nfor the scale variation issue. Extensive experiments on benchmark datasets\ndemonstrate state-of-the-art performance, validating the efficacy of the\nproposed method. Code is available at https:\/\/github.com\/MedAITech\/SCRD4AD.",
        "The remarkable success of the autoregressive paradigm has made significant\nadvancement in Multimodal Large Language Models (MLLMs), with powerful models\nlike Show-o, Transfusion and Emu3 achieving notable progress in unified image\nunderstanding and generation. For the first time, we uncover a common\nphenomenon: the understanding capabilities of MLLMs are typically stronger than\ntheir generative capabilities, with a significant gap between the two. Building\non this insight, we propose HermesFlow, a simple yet general framework designed\nto seamlessly bridge the gap between understanding and generation in MLLMs.\nSpecifically, we take the homologous data as input to curate homologous\npreference data of both understanding and generation. Through Pair-DPO and\nself-play iterative optimization, HermesFlow effectively aligns multimodal\nunderstanding and generation using homologous preference data. Extensive\nexperiments demonstrate the significant superiority of our approach over prior\nmethods, particularly in narrowing the gap between multimodal understanding and\ngeneration. These findings highlight the potential of HermesFlow as a general\nalignment framework for next-generation multimodal foundation models. Code:\nhttps:\/\/github.com\/Gen-Verse\/HermesFlow",
        "Video-based high-density crowd analysis and prediction has been a\nlong-standing topic in computer vision. It is notoriously difficult due to, but\nnot limited to, the lack of high-quality data and complex crowd dynamics.\nConsequently, it has been relatively under studied. In this paper, we propose a\nnew approach that aims to learn from in-the-wild videos, often with low quality\nwhere it is difficult to track individuals or count heads. The key novelty is a\nnew physics prior to model crowd dynamics. We model high-density crowds as\nactive matter, a continumm with active particles subject to stochastic forces,\nnamed 'crowd material'. Our physics model is combined with neural networks,\nresulting in a neural stochastic differential equation system which can mimic\nthe complex crowd dynamics. Due to the lack of similar research, we adapt a\nrange of existing methods which are close to ours for comparison. Through\nexhaustive evaluation, we show our model outperforms existing methods in\nanalyzing and forecasting extremely high-density crowds. Furthermore, since our\nmodel is a continuous-time physics model, it can be used for simulation and\nanalysis, providing strong interpretability. This is categorically different\nfrom most deep learning methods, which are discrete-time models and\nblack-boxes.",
        "Extracting real-time insights from multi-modal data streams from various\ndomains such as healthcare, intelligent transportation, and satellite remote\nsensing remains a challenge. High computational demands and limited knowledge\nscope restrict the applicability of Multi-Modal Large Language Models (MM-LLMs)\non these data streams. Traditional Retrieval-Augmented Generation (RAG) systems\naddress knowledge limitations of these models, but suffer from slow\npreprocessing, making them unsuitable for real-time analysis. We propose\nStreamingRAG, a novel RAG framework designed for streaming data. StreamingRAG\nconstructs evolving knowledge graphs capturing scene-object-entity\nrelationships in real-time. The knowledge graph achieves temporal-aware scene\nrepresentations using MM-LLMs and enables timely responses for specific events\nor user queries. StreamingRAG addresses limitations in existing methods,\nachieving significant improvements in real-time analysis (5-6x faster\nthroughput), contextual accuracy (through a temporal knowledge graph), and\nreduced resource consumption (using lightweight models by 2-3x).",
        "The advancement of remote sensing technology has improved the spatial\nresolution of satellite imagery, facilitating more detailed visual\nrepresentations for diverse interpretations. However, existing methods exhibit\nlimited generalization capabilities across varied applications. While some\ncontemporary foundation models demonstrate potential, they are hindered by\ninsufficient cross-task adaptability and primarily process low-resolution\nimagery of restricted sizes, thus failing to fully exploit high-resolution data\nor leverage comprehensive large-scene semantics. Crucially, remote sensing\nimagery differs fundamentally from natural images, as key foreground targets\n(eg., maritime objects, artificial structures) often occupy minimal spatial\nproportions (~1%) and exhibit sparse distributions. Efficiently modeling\ncross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a\nsignificant challenge yet remains critical for remote sensing image\nunderstanding. Motivated by the selective attention mechanisms inherent to the\nhuman visual system, we propose DynamicVis, a dynamic visual perception\nfoundation model for remote sensing imagery. The framework integrates a novel\ndynamic region perception backbone based on the selective state space model,\nwhich strategically balances localized detail extraction with global contextual\nintegration, enabling computationally efficient encoding of large-scale data\nwhile maintaining architectural scalability. To enhance cross-task knowledge\ntransferring, we introduce a multi-instance learning paradigm utilizing\nmeta-embedding representations, trained on million-scale region-level\nannotations. Evaluations across nine downstream tasks demonstrate the model's\nversatility. DynamicVis achieves multi-level feature modeling with exceptional\nefficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and\n833 MB GPU memory (3% of ViT's).",
        "Medical Foundation Models (MFMs), trained on large-scale datasets, have\ndemonstrated superior performance across various tasks. However, these models\nstill struggle with domain gaps in practical applications. Specifically, even\nafter fine-tuning on source-domain data, task-adapted foundation models often\nperform poorly in the target domain. To address this challenge, we propose a\nfew-shot unsupervised domain adaptation (UDA) framework for MFMs, named MFM-DA,\nwhich only leverages a limited number of unlabeled target-domain images. Our\napproach begins by training a Denoising Diffusion Probabilistic Model (DDPM),\nwhich is then adapted to the target domain using a proposed dynamic\ninstance-aware adaptor and a distribution direction loss, enabling the DDPM to\ntranslate source-domain images into the target domain style. The adapted images\nare subsequently processed through the MFM, where we introduce a designed\nchannel-spatial alignment Low-Rank Adaptation (LoRA) to ensure effective\nfeature alignment. Extensive experiments on optic cup and disc segmentation\ntasks demonstrate that MFM-DA outperforms state-of-the-art methods. Our work\nprovides a practical solution to the domain gap issue in real-world MFM\ndeployment. Code will be available at here.",
        "Neural implicit representations have recently shown promising progress in\ndense Simultaneous Localization And Mapping (SLAM). However, existing works\nhave shortcomings in terms of reconstruction quality and real-time performance,\nmainly due to inflexible scene representation strategy without leveraging any\nprior information. In this paper, we introduce SP-SLAM, a novel neural RGB-D\nSLAM system that performs tracking and mapping in real-time. SP-SLAM computes\ndepth images and establishes sparse voxel-encoded scene priors near the\nsurfaces to achieve rapid convergence of the model. Subsequently, the encoding\nvoxels computed from single-frame depth image are fused into a global volume,\nwhich facilitates high-fidelity surface reconstruction. Simultaneously, we\nemploy tri-planes to store scene appearance information, striking a balance\nbetween achieving high-quality geometric texture mapping and minimizing memory\nconsumption. Furthermore, in SP-SLAM, we introduce an effective optimization\nstrategy for mapping, allowing the system to continuously optimize the poses of\nall historical input frames during runtime without increasing computational\noverhead. We conduct extensive evaluations on five benchmark datasets (Replica,\nScanNet, TUM RGB-D, Synthetic RGB-D, 7-Scenes). The results demonstrate that,\ncompared to existing methods, we achieve superior tracking accuracy and\nreconstruction quality, while running at a significantly faster speed.",
        "Text-to-image (T2I) generation has made significant advances in recent years,\nbut challenges still remain in the generation of perceptual artifacts,\nmisalignment with complex prompts, and safety. The prevailing approach to\naddress these issues involves collecting human feedback on generated images,\ntraining reward models to estimate human feedback, and then fine-tuning T2I\nmodels based on the reward models to align them with human preferences.\nHowever, while existing reward fine-tuning methods can produce images with\nhigher rewards, they may change model behavior in unexpected ways. For example,\nfine-tuning for one quality aspect (e.g., safety) may degrade other aspects\n(e.g., prompt alignment), or may lead to reward hacking (e.g., finding a way to\nincrease rewards without having the intended effect). In this paper, we propose\nFocus-N-Fix, a region-aware fine-tuning method that trains models to correct\nonly previously problematic image regions. The resulting fine-tuned model\ngenerates images with the same high-level structure as the original model but\nshows significant improvements in regions where the original model was\ndeficient in safety (over-sexualization and violence), plausibility, or other\ncriteria. Our experiments demonstrate that Focus-N-Fix improves these localized\nquality aspects with little or no degradation to others and typically\nimperceptible changes in the rest of the image. Disclaimer: This paper contains\nimages that may be overly sexual, violent, offensive, or harmful.",
        "This work highlights a semantics misalignment in 3D human pose estimation.\nFor the task of test-time adaptation, the misalignment manifests as overly\nsmoothed and unguided predictions. The smoothing settles predictions towards\nsome average pose. Furthermore, when there are occlusions or truncations, the\nadaptation becomes fully unguided. To this end, we pioneer the integration of a\nsemantics-aware motion prior for the test-time adaptation of 3D pose\nestimation. We leverage video understanding and a well-structured motion-text\nspace to adapt the model motion prediction to adhere to video semantics during\ntest time. Additionally, we incorporate a missing 2D pose completion based on\nthe motion-text similarity. The pose completion strengthens the motion prior's\nguidance for occlusions and truncations. Our method significantly improves\nstate-of-the-art 3D human pose estimation TTA techniques, with more than 12%\ndecrease in PA-MPJPE on 3DPW and 3DHP.",
        "Understanding the workflow of surgical procedures in complex operating rooms\nrequires a deep understanding of the interactions between clinicians and their\nenvironment. Surgical activity recognition (SAR) is a key computer vision task\nthat detects activities or phases from multi-view camera recordings. Existing\nSAR models often fail to account for fine-grained clinician movements and\nmulti-view knowledge, or they require calibrated multi-view camera setups and\nadvanced point-cloud processing to obtain better results. In this work, we\npropose a novel calibration-free multi-view multi-modal pretraining framework\ncalled Multiview Pretraining for Video-Pose Surgical Activity Recognition\nPreViPS, which aligns 2D pose and vision embeddings across camera views. Our\nmodel follows CLIP-style dual-encoder architecture: one encoder processes\nvisual features, while the other encodes human pose embeddings. To handle the\ncontinuous 2D human pose coordinates, we introduce a tokenized discrete\nrepresentation to convert the continuous 2D pose coordinates into discrete pose\nembeddings, thereby enabling efficient integration within the dual-encoder\nframework. To bridge the gap between these two modalities, we propose several\npretraining objectives using cross- and in-modality geometric constraints\nwithin the embedding space and incorporating masked pose token prediction\nstrategy to enhance representation learning. Extensive experiments and ablation\nstudies demonstrate improvements over the strong baselines, while\ndata-efficiency experiments on two distinct operating room datasets further\nhighlight the effectiveness of our approach. We highlight the benefits of our\napproach for surgical activity recognition in both multi-view and single-view\nsettings, showcasing its practical applicability in complex surgical\nenvironments. Code will be made available at:\nhttps:\/\/github.com\/CAMMA-public\/PreViPS.",
        "Humans excel at spatio-temporal reasoning, effortlessly interpreting dynamic\nvisual events from an egocentric viewpoint. However, whether multimodal large\nlanguage models (MLLMs) can similarly comprehend the 4D world remains\nuncertain. This paper explores multimodal spatio-temporal reasoning from an\negocentric perspective, aiming to equip MLLMs with human-like reasoning\ncapabilities. To support this objective, we introduce Ego-ST Bench, a novel\nbenchmark containing over 5,000 question-answer pairs across four categories,\nsystematically evaluating spatial, temporal, and integrated spatio-temporal\nreasoning. Additionally, we propose the ST-R1 Video model, a video-based\nreasoning model that incorporates reverse thinking into its reinforcement\nlearning process, significantly enhancing performance. We combine\nlong-chain-of-thought (long-CoT) supervised fine-tuning with Group Relative\nPolicy Optimization (GRPO) reinforcement learning, achieving notable\nimprovements with limited high-quality data. Ego-ST Bench and ST-R1 provide\nvaluable insights and resources for advancing video-based spatio-temporal\nreasoning research.",
        "Vision transformers (ViTs) have emerged as a popular backbone for visual\ntracking. However, complete ViT architectures are too cumbersome to deploy for\nunmanned aerial vehicle (UAV) tracking which extremely emphasizes efficiency.\nIn this study, we discover that many layers within lightweight ViT-based\ntrackers tend to learn relatively redundant and repetitive target\nrepresentations. Based on this observation, we propose a similarity-guided\nlayer adaptation approach to optimize the structure of ViTs. Our approach\ndynamically disables a large number of representation-similar layers and\nselectively retains only a single optimal layer among them, aiming to achieve a\nbetter accuracy-speed trade-off. By incorporating this approach into existing\nViTs, we tailor previously complete ViT architectures into an efficient\nsimilarity-guided layer-adaptive framework, namely SGLATrack, for real-time UAV\ntracking. Extensive experiments on six tracking benchmarks verify the\neffectiveness of the proposed approach, and show that our SGLATrack achieves a\nstate-of-the-art real-time speed while maintaining competitive tracking\nprecision. Codes and models are available at\nhttps:\/\/github.com\/GXNU-ZhongLab\/SGLATrack.",
        "Graph neural networks (GNNs) have achieved remarkable empirical success in\nprocessing and representing graph-structured data across various domains.\nHowever, a significant challenge known as \"oversmoothing\" persists, where\nvertex features become nearly indistinguishable in deep GNNs, severely\nrestricting their expressive power and practical utility. In this work, we\nanalyze the asymptotic oversmoothing rates of deep GNNs with and without\nresidual connections by deriving explicit convergence rates for a normalized\nvertex similarity measure. Our analytical framework is grounded in the\nmultiplicative ergodic theorem. Furthermore, we demonstrate that adding\nresidual connections effectively mitigates or prevents oversmoothing across\nseveral broad families of parameter distributions. The theoretical findings are\nstrongly supported by numerical experiments.",
        "Over the past four decades, cheerleading has evolved from a sideline activity\nat major sporting events into a professional, competitive sport with growing\nglobal popularity. Evaluating tumbling elements in cheerleading relies on both\nobjective measures and subjective judgments, such as difficulty and execution\nquality. However, the complexity of tumbling - encompassing team synchronicity,\nground interactions, choreography, and artistic expression - makes objective\nassessment challenging. Artificial intelligence (AI) has revolutionized various\nscientific fields and industries through precise data-driven analyses, yet\ntheir application in acrobatic sports remains limited despite significant\npotential for enhancing performance evaluation and coaching. This study\ninvestigates the feasibility of using an AI-based approach with data from a\nsingle inertial measurement unit to accurately identify and objectively assess\ntumbling elements in standard cheerleading routines. A sample of 16\nparticipants (13 females, 3 males) from a Division I collegiate cheerleading\nteam wore a single inertial measurement unit at the dorsal pelvis. Over a\n4-week seasonal preparation period, 1102 tumbling elements were recorded during\nregular practice sessions. Using triaxial accelerations and rotational speeds,\nvarious ML algorithms were employed to classify and evaluate the execution of\ntumbling manoeuvres. Results indicate that certain machine learning models can\neffectively identify different tumbling elements despite inter-individual\nvariability and data noise, achieving high accuracy. These findings demonstrate\nthe significant potential for integrating AI-driven assessments into\ncheerleading and other acrobatic sports, providing objective metrics that\ncomplement traditional judging methods.",
        "Continual learning (CL) research typically assumes highly constrained\nexemplar memory resources. However, in many real-world scenarios-especially in\nthe era of large foundation models-memory is abundant, while GPU computational\ncosts are the primary bottleneck. In this work, we investigate CL in a novel\nsetting where exemplar memory is ample (i.e., sufficient exemplar memory).\nUnlike prior methods designed for strict exemplar memory constraints, we\npropose a simple yet effective approach that directly operates in the model's\nweight space through a combination of weight resetting and averaging\ntechniques. Our method achieves state-of-the-art performance while reducing the\ncomputational cost to a quarter or third of existing methods. These findings\nchallenge conventional CL assumptions and provide a practical baseline for\ncomputationally efficient CL applications.",
        "The polar magnets A$_2$Mo$_3$O$_8$ (A=Fe, Mn, Co, and Ni) feature a bipartite\nstructure, where the magnetic A$^{2+}$ ions occupy two different sites with\noctahedral and tetrahedral oxygen coordinations. This bipartite structure\nprovides a platform for the emergence of nontrivial magnetoelectric (ME)\neffects and intriguing excitation behaviors, and thus creates significant\nresearch interest. In this study, we conduct inelastic neutron scattering\nmeasurements on single crystals of Mn$_2$Mo$_3$O$_8$, an L-type ferrimagnet in\nthe A$_2$Mo$_3$O$_8$ family, to investigate its spin dynamics. The obtained\nmagnetic excitation spectra reveal two distinct magnon dispersions\ncorresponding to the octahedral and tetrahedral spins in Mn$_2$Mo$_3$O$_8$.\nThese magnon bands can be well described by a spin Hamiltonian including\nHeisenberg and single-ion anisotropy terms. Employing our effective spin model,\nwe successfully reproduce the unusual temperature dependence of the L-type\nferrimagnetic susceptibility through self-consistent mean-field theory. This\nresearch reveals the significance of the bipartite structure in determining the\nexcitation properties of the polar magnets $\\rm{A_{2}Mo_{3}O_{8}}$ and provides\nvaluable insights into the spin dynamics of L-type ferrimagnets.",
        "This paper introduces an algorithm that dynamically generates basis functions\nto approximate the value function in Network Revenue Management. Unlike\nexisting algorithms sampling the parameters of new basis functions, this\nNonlinear Incremental Algorithm (NLIAlg) iteratively refines the value function\napproximation by optimizing these parameters. For larger instances, the\nTwo-Phase Incremental Algorithm (2PIAlg) modifies NLIAlg to leverage the\nefficiency of LP solvers. It reduces the size of a large-dimensional nonlinear\nproblem and transforms it into an LP by fixing the basis function parameters,\nwhich are then optimized in a second phase using the flow imbalance ideas from\nAdelman and Klabjan (2012). This marks the first application of these\ntechniques in a stochastic setting. The algorithms can operate in two modes:\n(1) Standalone mode, to construct a value function approximation from scratch,\nand (2) Add-on mode, to refine an existing approximation. Our numerical\nexperiments indicate that while NLIAlg and 2PIAlg in standalone mode are only\nfeasible for small-scale problems, the heuristic version of 2PIAlg (H-2PIAlg)\nin add-on mode, using the Affine Approximation and exponential ridge basis\nfunctions, can handle extremely large instances that may cause benchmark\nnetwork revenue management methods to run out of memory. In these scenarios,\nH-2PIAlg delivers substantially better policies and upper bounds than the\nAffine Approximation. Furthermore, H-2PIAlg achieves higher average revenues in\npolicy simulations compared to network revenue management benchmarks in\ninstances with limited capacity.",
        "For sink-free orientations in graphs of minimum degree at least $3$, we show\nthat there is a deterministic approximate counting algorithm that runs in time\n$O((n^{73}\/\\varepsilon^{72})\\log(n\/\\varepsilon))$, a near-linear time sampling\nalgorithm, and a randomised approximate counting algorithm that runs in time\n$O((n\/\\varepsilon)^2\\log(n\/\\varepsilon))$, where $n$ denotes the number of\nvertices of the input graph and $0<\\varepsilon<1$ is the desired accuracy. All\nthree algorithms are based on a local implementation of the sink popping method\n(Cohn, Pemantle, and Propp, 2002) under the partial rejection sampling\nframework (Guo, Jerrum, and Liu, 2019).",
        "Main sequence stars of spectral types F, G, and K with low to moderate\nactivity levels exhibit a recognizable pattern known as the first ionization\npotential effect (FIP effect), where elements with lower first ionization\npotentials are more abundant in the stellar corona than in the photosphere. In\ncontrast, high activity main sequence stars such as AB Dor (K0), active\nbinaries, and M dwarfs exhibit an inverse pattern known as iFIP. We aim to\ndetermine whether or not the iFIP pattern persists in moderate-activity M\ndwarfs. We used XMM-Newton to observe the moderately active M dwarf HD 223889\nthat has an X-ray surface flux of log FX,surf = 5.26, the lowest for an M dwarf\nstudied so far for coronal abundance patterns. We used low-resolution CCD\nspectra of the star to calculate the strength of the FIP effect quantified by\nthe FIP bias (Fbias) to assess the persistence of the iFIP effect in M dwarfs.\nOur findings reveal an iFIP effect similar to that of another moderately active\nbinary star, GJ 338 AB, with a comparable error margin. The results hint at a\npossible plateau in the Teff-Fbias diagram for moderately active M dwarfs.\nTargeting stars with low coronal activity that have a coronal temperature\nbetween 2 MK and 4 MK is essential for refining our understanding of (i)FIP\npatterns and their causes.",
        "Counterdiabatic (CD) driving has the potential to speed up adiabatic quantum\nstate preparation by suppressing unwanted excitations. However, existing\napproaches either require intractable classical computations or are based on\napproximations which do not have performance guarantees. We propose and analyze\na non-variational, system-agnostic CD expansion method and analytically show\nthat it converges exponentially quickly in the expansion order. In finite\nsystems, the required resources scale inversely with the spectral gap, which we\nargue is asymptotically optimal. To extend our method to the thermodynamic\nlimit and suppress errors stemming from high-frequency transitions, we leverage\nfinite-time adiabatic protocols. In particular, we show that a time determined\nby the quantum speed limit is sufficient to prepare the desired ground state,\nwithout the need to optimize the adiabatic trajectory. Numerical tests of our\nmethod on the quantum Ising chain show that our method can outperform\nstate-of-the-art variational CD approaches.",
        "Lifecycle management of power converters continues to thrive with emerging\nartificial intelligence (AI) solutions, yet AI mathematical explainability\nremains unexplored in power electronics (PE) community. The lack of theoretical\nrigor challenges adoption in mission-critical applications. Therefore, this\nletter proposes a generic framework to evaluate mathematical explainability,\nhighlighting inference stability and training convergence from a Lipschitz\ncontinuity perspective. Inference stability governs consistent outputs under\ninput perturbations, essential for robust real-time control and fault\ndiagnosis. Training convergence guarantees stable learning dynamics,\nfacilitating accurate modeling in PE contexts. Additionally, a Lipschitz-aware\nlearning rate selection strategy is introduced to accelerate convergence while\nmitigating overshoots and oscillations. The feasibility of the proposed\nLipschitz-oriented framework is demonstrated by validating the mathematical\nexplainability of a state-of-the-art physics-in-architecture neural network,\nand substantiated through empirical case studies on dual-active-bridge\nconverters. This letter serves as a clarion call for the PE community to\nembrace mathematical explainability, heralding a transformative era of\ntrustworthy and explainable AI solutions that potentially redefine the future\nof power electronics.",
        "Electromagnetically induced transparency (EIT) is an important quantum\noptical phenomenon which provides a crucial tool for light manipulation.\nHowever, typically the transparency window is broad, limited by the coherence\ntime of the metastable state. Here we show that extremely narrow transparency\nwindow can be realized using nuclear spin induced transparency (NSIT), which is\nachieved by combining optical field, magnetic field and the spin-exchange\ninteraction between noble-gas nuclear spins and alkali-metal electronic spins.\nThe width of the NSIT window can be several orders of magnitude smaller than\nthat of conventional EIT, and even reaches sub-mHz range due to the long\ncoherence time of nuclear spins. The scheme holds great potential for\napplications in slow light and magnetic field sensing.",
        "Multi-object tracking (MOT) in UAV-based video is challenging due to\nvariations in viewpoint, low resolution, and the presence of small objects.\nWhile other research on MOT dedicated to aerial videos primarily focuses on the\nacademic aspect by developing sophisticated algorithms, there is a lack of\nattention to the practical aspect of these systems. In this paper, we propose a\nnovel real-time MOT framework that integrates Apache Kafka and Apache Spark for\nefficient and fault-tolerant video stream processing, along with\nstate-of-the-art deep learning models YOLOv8\/YOLOv10 and BYTETRACK\/BoTSORT for\naccurate object detection and tracking. Our work highlights the importance of\nnot only the advanced algorithms but also the integration of these methods with\nscalable and distributed systems. By leveraging these technologies, our system\nachieves a HOTA of 48.14 and a MOTA of 43.51 on the Visdrone2019-MOT test set\nwhile maintaining a real-time processing speed of 28 FPS on a single GPU. Our\nwork demonstrates the potential of big data technologies and deep learning for\naddressing the challenges of MOT in UAV applications.",
        "We analyze simulation results from the TitanWRF global circulation model to\nunderstand the mechanisms that maintain the equatorial superrotation in Titan's\nstratosphere. We find that the eddies associated with wave activities can\ntransport angular momentum upgradient to zonal flow, leading to acceleration of\nthe equatorial superrotation. The dominant wave modes identified in this study\nare consistent with previous studies, with zonal wavenumber 1 being the major\ncontributor to the prograde acceleration. Despite the same conclusion of\nmaintenance of equatorial superrotation via wave-mean interactions, we find\nthat the way waves interact with the zonal flow in TitanWRF is slightly\ndifferent from some other studies. We confirm our previous findings that in\nTitanWRF this occurs primarily during a dozen or so annual, short-duration (a\nfew Titan sols) angular momentum \"transfer events,\" which have a repeatable\nseasonal pattern but differ slightly in timing and magnitude between years.\nThis is not the case in the Titan Atmosphere Model (TAM), which found milder\nangular momentum transfers that produced the strongest acceleration of\nsuperrotation around solstice in the upper stratosphere and more continuous\nyear-around acceleration in the lower stratosphere. Despite differences in\nangular momentum transfer across models, we further find that, similar to the\nTAM wave analysis results, eddies generated by Rossby-Kelvin instabilities may\nbe the major source of prograde angular momentum for the equatorial\nsuperrotation, although TitanWRF may also include contributions from the\nabsorption of vertically propagating equatorial Kelvin waves. This differs from\nour previous work, which suggested barotropic waves were responsible for\nTitanWRF's solsticial transfer event.",
        "In recent works we have explored a multi-photon extension of the celebrated\ntwo-photon Hong-Ou-Mandel (HOM) effect in which the quantum amplitudes for a\ntwo-photon input to a lossless, balanced 50:50 beamsplitter (BS) undergoes\ncomplete destructive interference. In the extended Hong-Ou-Mandel (eHOM) effect\nthe multi-photon scattering of photons from the two input ports to the two\noutput ports of the BS for Fock number basis input states (FS)\n$|n,m\\rangle_{12}$ exhibit complete destructive interference pairwise within\nthe quantum amplitudes containing many scattering components, generalizing the\ntwo-photon HOM effect. This has profound implications for arbitrary bipartite\nphotonic input states constructed from such basis states: if the input state to\none input port of the BS is of odd parity, i.e. constructed from only of odd\nnumbers of photons, then regardless of the input state to the second 50:50 BS\nport, there will be a central nodal line (CNL) of zeros in the joint output\nprobability distribution along the main diagonal for coincidence detection. The\nfirst goal of this present work is to show diagrammatically how the extended\nHOM effect can be seen as a succession of multi-photon HOM effects when the\nlatter is viewed as a pairwise cancellation of mirror image scattering\namplitudes. The second goal of this work is to explore considerations for the\nexperimental realization of the extended Hong-Ou-Mandel effect. We examine the\ncase of a single photon interfering with a coherent state (an idealized laser)\non a balanced 50:50 beamsplitter and consider prospects for experimental\ndetection of the output destructive interference by including additional\neffects such as imperfect detection efficiency, spatio-temporal mode functions,\nand time delay between the detected output photons.",
        "We study the high-temperature equilibrium for the C*-algebra $\\mathcal\nT(\\mathbb N^\\times \\ltimes \\mathbb Z)$ recently considered by an Huef, Laca and\nRaeburn. We show that the simplex of KMS$_\\beta$ states at each inverse\ntemperature $\\beta$ in the critical interval $(0,1]$ is a Bauer simplex whose\nspace of extreme points is homeomorphic to $\\mathbb N \\sqcup\\{\\infty\\}$. This\nis in contrast to the uniqueness of equilibrium at high temperature observed in\npreviously considered systems arising from number theory. We also establish a\nconnection between the phase transitions on quotients of our system and the\nBost-Connes phase transition.",
        "Quantum correlations, like entanglement, represent the characteristic trait\nof quantum mechanics, and pose essential issues and challenges to the\ninterpretation of this pillar of modern physics. Although quantum correlations\nare largely acknowledged as a major resource to achieve quantum advantage in\nmany tasks of quantum technologies, their full quantitative description and the\naxiomatic basis underlying them are still under investigation. Previous works\nsuggested that the origin of nonlocal correlations is grounded in principles\ncapturing (from outside the quantum formalism) the essence of quantum\nuncertainty. In particular, the recently-introduced principle of Relativistic\nIndependence gave rise to a new bound intertwining local and nonlocal\ncorrelations. Here we test such a bound by realizing together sequential and\njoint weak measurements on entangled photon pairs, allowing to simultaneously\nquantify both local and nonlocal correlations by measuring incompatible\nobservables on the same quantum system without collapsing its state, a task\ntypically forbidden in the traditional (projective) quantum measurement\nframework. Our results demonstrate the existence of a fundamental limit on the\nextent of quantum correlations, shedding light on the profound role of\nuncertainty in both enabling and balancing them."
      ]
    }
  },
  {
    "id":2411.00726,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
      ],
      "abstract":[
        "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Reproductive system and interaction with fauna in a Mediterranean\n  Pyrophite shrub",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "An Asymptotic Analysis of Bivalent Monoclonal Antibody-Antigen Binding",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Divisibility Relations Between Ring Homomorphisms and Surjective Group\n  Homomorphisms in Finite Cyclic Structures",
        "Moduli spaces of twisted maps to smooth pairs",
        "Parallel Collisionless Shocks in strongly Magnetized Electron-Ion\n  Plasma. I. Temperature anisotropies",
        "Probing Coherences and Itinerant Magnetism in a Dipolar Lattice Gas",
        "$NJ\/\\psi$ and $N\\eta_c$ interactions from lattice QCD",
        "The Ridge Integration Method and its Application to Molecular Sieving,\n  Demonstrated for Gas Purification via Graphdiyne Membranes",
        "Overcoming Quantum Metrology Singularity through Sequential Measurements",
        "Late-time growth weakly affects the significance of high-redshift\n  massive galaxies",
        "Weierstrass representations of discrete constant mean curvature surfaces\n  in isotropic space",
        "Online Optimization with Unknown Time-varying Parameters",
        "Exactness and the topology of the space of invariant random equivalence\n  relations",
        "Cusps and fundamental domains for congruence subgroups",
        "$H^\\infty$-control for a class of boundary controlled hyperbolic PDEs",
        "Accelerating Expansion of the Universe in Modified Symmetric\n  Teleparallel Gravity",
        "Neutron versus proton scattering on exotic nuclei: the $^9$He example"
      ],
      "abstract":[
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "The ULEX model, in its present state, involves the study of the biomass and\nthe population of the shrub Ulex parviflorus Pourret, but while being a dynamic\nmodel, it is static in the sense that it does not imply the appearance of new\nspecimens of this plant. As a complement to the ULEX model in its two dynamic\nand spatial aspects, and with the idea of extending the model, the authors have\nintroduced from a biological and statistical point of view four characteristics\nof this species, flowering, pollination, fructification, taking special\ninterest in the role played by the pollinators (bees) and dispersion of seeds.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Ligand-receptor interactions are fundamental to many biological processes.\nFor example in antibody-based immunotherapies, the dynamics of an antibody\nbinding with its target antigen directly influence the potency and efficacy of\nmonoclonal antibody (mAb) therapies. In this paper, we present an asymptotic\nanalysis of an ordinary differential equation (ODE) model of bivalent\nantibody-antigen binding in the context of mAb cancer therapies, highlighting\nthe added complexity associated with bivalency of the antibody. To understand\nwhat drives the complex temporal dynamics of bivalent antibody-antigen binding,\nwe construct asymptotic approximations to the model's solutions at different\ntimescales and antibody concentrations that are in good agreement with\nnumerical simulations of the full model. We show how the dynamics differ\nbetween two scenarios; a region where unbound antigens are abundant, and one\nwhere the number of unbound antigens is small such that the dominant balance\nwithin the model equations changes. Of particular importance to the potency and\nefficacy of mAb treatments are the values of quantities such as antigen\noccupancy and bound antibody number. We use the results of our asymptotic\nanalysis to approximate the long-time values of these quantities that could be\ncombined with experimental data to facilitate parameter estimation.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "In this article, we delve into the intricate relationship between the number\nof ring homomorphisms and surjective group homomorphisms between two finite\ncyclic structures, specifically $\\mathbb{Z}_m$ and $\\mathbb{Z}_n$. We\ndemonstrate that the number of ring homomorphisms from $\\mathbb{Z}_m$ to\n$\\mathbb{Z}_n$ is a divisor of the number of surjective group homomorphisms\nfrom $\\mathbb{Z}_m$ to $\\mathbb{Z}_n$, provided that $n$ is not of the form $2\n\\cdot \\alpha$, where each prime factor $p$ of $\\alpha$ satisfies $p \\equiv 3\n\\pmod{4}$.",
        "We study moduli spaces of twisted maps to a smooth pair in arbitrary genus,\nand give geometric explanations for previously known comparisons between\norbifold and logarithmic Gromov--Witten invariants. Namely, we study the space\nof twisted maps to the universal target and classify its irreducible components\nin terms of combinatorial\/tropical information. We also introduce natural\nmorphisms between these moduli spaces for different rooting parameters and\ncompute their degree on various strata. Combining this with additional\nhypotheses on the discrete data, we show these degrees are monomial of degree\nbetween $0$ and $\\max(0,2g-1)$ in the rooting parameter. We discuss the virtual\ntheory of the moduli spaces, and relate our polynomiality results to work of\nTseng and You on the higher genus orbifold Gromov--Witten invariants of smooth\npairs, recovering their results in genus $1$. We discuss what is needed to\ndeduce arbitrary genus comparison results using the previous sections. We\nconclude with some geometric examples, starting by re-framing the original\ngenus $1$ example of Maulik in this new formalism.",
        "Collisionless electron-ion shocks are fundamental to astrophysical plasmas,\nyet their behavior in strong magnetic fields remains poorly understood. Using\nParticle-in-Cell (PIC) simulations with the SHARP-1D3V code, we investigate the\nrole of the ion magnetization parameter $\\sigma_i$ in parallel shock\ntransitions. Strongly magnetized converging flows ($\\sigma_i > 1$) exhibit\nlower density compression ratios ($R \\sim 2$), smaller entropy jumps, and\nsuppressed particle acceleration, while maintaining pressure anisotropy\nstability due to conserved perpendicular temperatures across the shock,\nalongside increased parallel temperatures. In contrast, weakly magnetized\nshocks drive downstream mirror and firehose instabilities due to ion\ntemperature anisotropy, which are suppressed in strongly magnetized cases.\nAdditionally, weakly magnetized shocks exhibit the onset of a supra-thermal\npopulation induced by shock-drift acceleration, with most of the upstream\nkinetic energy thermalized for both electrons and ions in the downstream\nregion. Our results demonstrate that perpendicular temperatures for both\nspecies are conserved in strongly magnetized cases and highlight deviations\nfrom standard ideal magnetohydrodynamic (MHD) behavior. These findings provide\ncritical insights into the role of magnetic fields in parallel collisionless\nastrophysical shocks.",
        "We report on the study of itinerant magnetism of lattice-trapped magnetic\natoms, driven by magnetic dipole-dipole interactions, in the low-entropy and\nclose-to-unit filling regime. We have used advanced dynamical decoupling\ntechniques to efficiently suppress the sensitivity to magnetic field\nfluctuations. We have thus measured the spin coherence of an itinerant spin 3\nBose dipolar gas throughout a quantum phase transition from a superfluid phase\nto a Mott insulating phase. In the superfluid phase, a metastable ferromagnetic\nbehavior is observed below a dynamical instability which occurs at lattice\ndepths below the phase transition. In the insulating phase, the thermalization\ntowards a paramagnetic state is driven by an interplay between intersite and\nsuperexchange interactions.",
        "The interaction between nucleon and charmonia ($J\/\\psi$ and $\\eta_c$) is\nexpected to deepen our understanding of various aspects in nonperturbative QCD\nranging from the origin of nucleon mass to $J\/\\psi$ mass modification in\nnuclear medium and properties of hidden-charm pentaquark states. Here, we\npresent the low-energy $NJ\/\\psi$ and $N\\eta_c$ interactions based on ($2+1$)\nflavor lattice QCD simulations with nearly physical pion mass $m_\\pi=146$ MeV.\nThe interactions, extracted from the spacetime correlations of the nucleon and\ncharmonium system by using the HAL QCD method, are found to be attractive in\nall distances and manifest a characteristic long-range tail consistent with the\ntwo-pion exchange interaction. The resulting scattering lengths are around\n$0.3$ fm, $0.4$ fm and $0.2$ fm for $NJ\/\\psi$ with spin $3\/2$, with spin $1\/2$,\nand $N\\eta_c$, respectively. Our results are orders of magnitude larger than\nthose from the photoproduction experiments assuming the vector meson dominance.",
        "Eyring theory provides a convenient approximation to the rate of a chemical\nreaction, as it uses only local information evaluated near extremal points of a\ngiven potential energy surface. However, in cases of pronounced anharmonicity\nand particularly low-lying vibrational frequencies, deviations from the correct\nreaction rate can become substantial. Molecular Dynamics simulations, on the\nother hand, are very costly at higher levels of theory, and of limited use\nsince molecular reactions are `rare' events and hence statistically less\naccessible. In this article, we present an alternative description for problems\nof gas separation and storage via two-dimensional materials such as porous\ngraphene or flat metal-organic frameworks. Taking geometric advantage of the\ntypical problem setting, our method is based on a statistical analysis of\nmolecular trajectories near the so-called `ridge', a hypersurface which divides\nthe reaction volume into a reactant and a product side. It allows for more\nrealistic predictions of permeabilities and selectivities, e.g. derived from\ndensity functional theory, but without the considerable costs of a full\nmolecular dynamics simulation on the corresponding Born-Oppenheimer potential\nenergy surface. We test our method on the example of methane separation from\nnitrogen and carbon dioxide via a graphdiyne membrane.",
        "The simultaneous estimation of multiple unknown parameters is the most\ngeneral scenario in quantum sensing. Quantum multi-parameter estimation theory\nprovides fundamental bounds on the achievable precision of simultaneous\nestimation. However, these bounds can become singular (no finite bound exists)\nin multi-parameter sensing due to parameter interdependencies, limited probe\naccessibility, and insufficient measurement outcomes. Here, we address the\nsingularity issue in quantum sensing through a simple mechanism based on a\nsequential measurement strategy. This sensing scheme overcomes the singularity\nconstraint and enables the simultaneous estimation of multiple parameters with\na local and fixed measurement throughout the sensing protocol. This is because\nsequential measurements, involving consecutive steps of local measurements\nfollowed by probe evolution, inherently produce correlated measurement data\nthat grows exponentially with the number of sequential measurements. Finally,\nthrough two different examples, namely a strongly correlated probe and a\nlight-matter system, we demonstrate how such singularities are reflected when\ninferring the unknown parameters through Bayesian estimation.",
        "Recent observations by the James Webb Space Telescope have revealed massive\ngalaxies at very high redshift ($z\\simeq 7-15$). The question of whether the\nexistence of such galaxies is expected in the corresponding JWST surveys has\nreceived a lot of attention, though the answer straddles areas of cosmology and\ncomplex astrophysical details of high-redshift galaxy formation. The growth\nrate of density fluctuations determines the amplitude of overdensities that\ncollapse to form galaxies. Late-time modifications of growth, combined with\nmeasurements at both $z\\sim 1$ from large-scale structure and $z\\sim 1000$ from\nthe cosmic microwave background, affect the predictions for the abundance of\nfirst galaxies in the universe. In this paper, we point out that the late-time\ngrowth rate of structure affects the statistical significance of high-redshift,\nhigh-mass objects very weakly. Consequently, if the existence and abundance of\nthese objects are confirmed to be unexpected, the variations in the late-time\ngrowth history are unlikely to explain these anomalies.",
        "In this paper, we obtain Weierstrass representations for discrete constant\nmean curvature surfaces in isotropic 3-space, and use this to construct\nexamples with discrete closed-form parametrizations.",
        "In this paper, we study optimization problems where the cost function\ncontains time-varying parameters that are unmeasurable and evolve according to\nlinear, yet unknown, dynamics. We propose a solution that leverages control\ntheoretic tools to identify the dynamics of the parameters, predict their\nevolution, and ultimately compute a solution to the optimization problem. The\nidentification of the dynamics of the time-varying parameters is done online\nusing measurements of the gradient of the cost function. This system\nidentification problem is not standard, since the output matrix is known and\nthe dynamics of the parameters must be estimated in the original coordinates\nwithout similarity transformations. Interestingly, our analysis shows that,\nunder mild conditions that we characterize, the identification of the\nparameters dynamics and, consequently, the computation of a time-varying\nsolution to the optimization problem, requires only a finite number of\nmeasurements of the gradient of the cost function. We illustrate the\neffectiveness of our algorithm on a series of numerical examples.",
        "We characterize exactness of a countable group $\\Gamma$ in terms of invariant\nrandom equivalence relations (IREs) on $\\Gamma$. Specifically, we show that\n$\\Gamma$ is exact if and only if every weak limit of finite IREs is an amenable\nIRE. In particular, for exact groups this implies amenability of the restricted\nrerooting relation associated to the ideal Bernoulli Voronoi tessellation, the\ndiscrete analog of the ideal Poisson Voronoi tesselation.",
        "We characterize the cusp classes and their widths for the congruence\nsubgroups $\\Gamma(N), \\Gamma_1(N)$ and $\\Gamma_0(N)$. We relate the cusp\nclasses of $\\Gamma_0(N)$ with those produced by the connected fundamental\ndomain in the previous work of Nie and Parent. By further studying the\ninteresting functions $M$ and $W$ on ${\\mathbb Z}\/N$, we establish an identity\nrelating the widths.",
        "A solution to the suboptimal $H^\\infty$-control problem is given for a class\nof hyperbolic partial differential equations (PDEs). The first result of this\nmanuscript shows that the considered class of PDEs admits an equivalent\nrepresentation as an infinite-dimensional discrete-time system. Taking\nadvantage of this, this manuscript shows that it is equivalent to solve the\nsuboptimal $H^\\infty$-control problem for a finite-dimensional discrete-time\nsystem whose matrices are derived from the PDEs. After computing the solution\nto this much simpler problem, the solution to the original problem can be\ndeduced easily. In particular, the optimal compensator solution to the\nsuboptimal $H^\\infty$-control problem is governed by a set of hyperbolic PDEs,\nactuated and observed at the boundary. We illustrate our results with a\nboundary controlled and boundary observed vibrating string.",
        "In the last century, theoretical and experimental developments have\nestablished the General Relativity theory as the most successful theory for\ndescribing the gravitational phenomenon. On the other hand, in the last two\ndecades, multiple observational probes have strongly favored the discovery of\nthe acceleration of cosmic expansion. The observational enhancement and\ndevelopment in precision cosmology indicate a requirement to go beyond General\nRelativity and to search for an alternate description that can resolve the\npersistent issues. In Chapter 1, we highlight some important elements of\nobservational cosmology. In Chapters 2 and 3, we investigate the f(Q) gravity\nin the presence of viscosity in the cosmic fluid. In Chapters 4 and 5, we\nexplore the constraints on the various classes of non-linear f(Q) gravity\nmodels in both coincident and non-coincident formalism, respectively. In\nChapter 6, we present a covariant formulation and energy balance equation for\nthe f(Q,T) gravity, which is an extension of f(Q) gravity. Finally, in Chapter\n7, we briefly summarize the outcomes of the present thesis and the future\nscope.",
        "Neutron scattering on exotic nuclides is a class of processes which can not\nbe studied directly now and in any observable future. Resonance proton\nscattering of exotic nuclide on a thick target in inverse kinematics can be\nused to infer the properties of the low-energy neutron scattering of this\nnuclide assuming the isobaric symmetry. However, the results of such resonance\nproton scattering reactions are so far analyzed in theoretical approaches\n(optical, R-matrix models), which are missing important aspects of isospin\ndynamics, isospin violation in continuum and threshold dynamics. The isospin\nconserving coupled-channel model (ICM) is proposed, which provides a more\nreliable basis for understanding of such experimental studies. Qualitatively\ndifferent phase shifts for the $^{8}$He+$p$ $T=5\/2$ and $T=3\/2$ resonances are\npredicted by ICM with quite unusual profile for the $T=5\/2$ states. Alternative\ninterpretation of the existing $^{8}$He+$p$ data is proposed. The observable\nproperties of the $T=5\/2$ resonances may be strongly affected by the\nisobaric-partner $T=3\/2$ states. Crucial importance of studies of the\nneutron-emission channel for disentangling this possible influence is\ndemonstrated."
      ]
    }
  },
  {
    "id":2411.05236,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel",
    "start_abstract":"Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2",
        "b0"
      ],
      "title":[
        "Shannon capacity of signal transduction for multiple independent receptors",
        "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
      ],
      "abstract":[
        "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
        "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
      ],
      "categories":[
        "cs.SY",
        "eess.SP"
      ]
    },
    "list":{
      "title":[
        "Learning Linear Block Codes with Gradient Quantization",
        "SWIPTNet: A Unified Deep Learning Framework for SWIPT based on GNN and\n  Transfer Learning",
        "A Block-Sparse Bayesian Learning Algorithm with Dictionary Parameter\n  Estimation for Multi-Sensor Data Fusion",
        "Sparse Incremental Aggregation in Satellite Federated Learning",
        "Power-Efficient Optimization for Coexisting Semantic and Bit-Based Users\n  in NOMA Networks",
        "Moment-based Characterization of Spatially Distributed Sources in SAR\n  Tomography",
        "A New Interpretation of the Time-Interleaved ADC Mismatch Problem: A\n  Tracking-Based Hybrid Calibration Approach",
        "Decentralized Learning with Approximate Finite-Time Consensus",
        "Generalized Spatial Modulation Aided Affine Frequency Division\n  Multiplexing",
        "High-Resolution Range-Doppler Imaging from One-Bit PMCW Radar via\n  Generative Adversarial Networks",
        "An Approach of Directly Tracking Multiple Objects",
        "Weighted-Sum Energy Efficiency Maximization in User-Centric Uplink\n  Cell-Free Massive MIMO",
        "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models\n  via Synthetic Embeddings",
        "Separate surface and bulk topological Anderson localization transitions\n  in disordered axion insulators",
        "Harmonic And Transposition Constraints Arising From The Use Of The\n  Roland TR-808 Bass Drum",
        "Quantifying Point Contributions: A Lightweight Framework for Efficient\n  and Effective Query-Driven Trajectory Simplification",
        "High-Energy Neutrinos by Hydrogen-rich Supernovae interacting with\n  low-massive Circumstellar Medium: The Case of SN 2023ixf",
        "A Hybrid Model\/Data-Driven Solution to Channel, Position and Orientation\n  Tracking in mmWave Vehicular Systems",
        "Exploring the Potential of QEEGNet for Cross-Task and Cross-Dataset\n  Electroencephalography Encoding with Quantum Machine Learning",
        "Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic\n  Data Generation and Fairness Algorithms",
        "Continually Learning Structured Visual Representations via Network\n  Refinement with Rerelation",
        "Dango: A Mixed-Initiative Data Wrangling System using Large Language\n  Model",
        "6GStarLab -- A CubeSat Mission to support the development and\n  standardization of Non-Terrestrial Networks towards 6G",
        "Hints of Primordial Magnetic Fields at Recombination and Implications\n  for the Hubble Tension",
        "EMK-KEN: A High-Performance Approach for Assessing Knowledge Value in\n  Citation Network",
        "CEReBrO: Compact Encoder for Representations of Brain Oscillations Using\n  Efficient Alternating Attention",
        "Emergent Symbolic Mechanisms Support Abstract Reasoning in Large\n  Language Models"
      ],
      "abstract":[
        "This study investigates the problem of learning linear block codes optimized\nfor Belief-Propagation decoders significantly improving performance compared to\nthe state-of-the-art. Our previous research is extended with an enhanced system\ndesign that facilitates a more effective learning process for the parity check\nmatrix. We simplify the input dataset, restrict the number of parameters to\nlearn and improve the gradient back-propagation within the model. We also\nintroduce novel optimizers specifically designed for discrete-valued weights.\nBased on conventional gradient computation, these optimizers provide discrete\nweights updates, enabling finer control and improving explainability of the\nlearning process. Through these changes, we consistently achieve improved code\nperformance, provided appropriately chosen hyper-parameters. To rigorously\nevaluate the performance of learned codes in the context of short to medium\nblock lengths, we propose a comprehensive code performance assessment\nframework. This framework enables a fair comparison between our learning\nmethodology and random search approaches, ensuring statistical significance in\nour results. The proposed model pave the way for a new approach to the\nefficient learning of linear block codes tailored to specific decoder\nstructures.",
        "This paper investigates the deep learning based approaches for simultaneous\nwireless information and power transfer (SWIPT). The quality-of-service (QoS)\nconstrained sum-rate maximization problems are, respectively, formulated for\npower-splitting (PS) receivers and time-switching (TS) receivers and solved by\na unified graph neural network (GNN) based model termed SWIPT net (SWIPTNet).\nTo improve the performance of SWIPTNet, we first propose a single-type output\nmethod to reduce the learning complexity and facilitate the satisfaction of QoS\nconstraints, and then, utilize the Laplace transform to enhance input features\nwith the structural information. Besides, we adopt the multi-head attention and\nlayer connection to enhance feature extracting. Furthermore, we present the\nimplementation of transfer learning to the SWIPTNet between PS and TS\nreceivers. Ablation studies show the effectiveness of key components in the\nSWIPTNet. Numerical results also demonstrate the capability of SWIPTNet in\nachieving near-optimal performance with millisecond-level inference speed which\nis much faster than the traditional optimization algorithms. We also show the\neffectiveness of transfer learning via fast convergence and expressive\ncapability improvement.",
        "We propose an sparse Bayesian learning (SBL)-based method that leverages\ngroup sparsity and multiple parameterized dictionaries to detect the relevant\ndictionary entries and estimate their continuous parameters by combining data\nfrom multiple independent sensors. In a MIMO multi-radar setup, we demonstrate\nits effectiveness in jointly detecting and localizing multiple objects, while\nalso emphasizing its broader applicability to various signal processing tasks.\nA key benefit of the proposed SBL-based method is its ability to resolve\ncorrelated dictionary entries-such as closely spaced objects-resulting in\nuncorrelated estimates that improve subsequent estimation stages. Through\nnumerical simulations, we show that our method outperforms the newtonized\northogonal matching pursuit (NOMP) algorithm when two objects cross paths using\na single radar. Furthermore, we illustrate how fusing measurements from\nmultiple independent radars leads to enhanced detection and localization\nperformance",
        "This paper studies Federated Learning (FL) in low Earth orbit (LEO) satellite\nconstellations, where satellites are connected via intra-orbit inter-satellite\nlinks (ISLs) to their neighboring satellites. During the FL training process,\nsatellites in each orbit forward gradients from nearby satellites, which are\neventually transferred to the parameter server (PS). To enhance the efficiency\nof the FL training process, satellites apply in-network aggregation, referred\nto as incremental aggregation. In this work, the gradient sparsification\nmethods from [1] are applied to satellite scenarios to improve bandwidth\nefficiency during incremental aggregation. The numerical results highlight an\nincrease of over 4 x in bandwidth efficiency as the number of satellites in the\norbital plane increases.",
        "Semantic communication focuses on transmitting the meaning of data, aiming\nfor efficient, relevant communication, while non-orthogonal multiple access\n(NOMA) enhances spectral efficiency by allowing multiple users to share the\nsame spectrum. Integrating semantic users into a NOMA network with bit-based\nusers improves both transmission and spectrum efficiency. However, the\nperformance metric for semantic communication differs significantly from that\nof traditional communication, posing challenges in simultaneously meeting\nindividual user demands and minimizing transmission power, especially in\nscenarios with coexisting semantic and bit-based users. Furthermore, the\ndifferent hardware architectures of semantic and bit-based users complicate the\nimplementation of successive interference cancellation (SIC). To address these\nchallenges, in this paper, we propose a clustered framework to mitigate the\ncomplexity of SIC and two multiple access (MA) schemes, e.g., pure\ncluster-based NOMA (P-CNOMA) and hybrid cluster-based NOMA (H-CNOMA), to\nminimize the total transmission power. The P-CNOMA scheme can achieve the\nminimum transmission power, but may not satisfy the high quality of service\n(QoS) requirement. In contrast, H-CNOMA addresses these issues with a slight\nincrease in power and a reduced semantic rate. These two schemes complement\neach other, enabling an adaptive MA selection mechanism that adapts to specific\nnetwork conditions and user requirements.",
        "This paper presents a non-parametric method for 3-D imaging of natural\nvolumes using Synthetic Aperture Radar tomography. This array processing-based\ntechnique aims at characterizing a spatially distributed density of incoherent\nsources, whose shape is imprecisely known. The proposed technique estimates the\nmoments of the reflectivity density using a low-complexity covariance matching\napproach, and retrieves the mean location, dispersion, and power of the\ndistributed source. Numerical simulations of realistic tomographic scenarios\nshow that the proposed model-free scheme achieves better accuracy than slightly\nmisspecified maximum likelihood estimators, derived from approximately known\ndistribution shapes.",
        "Time-interleaved ADCs (TI-ADCs) achieve high sampling rates by interleaving\nmultiple sub-ADCs in parallel. Mismatch errors between the sub-ADCs, however,\ncan significantly degrade the signal quality, which is a main performance\nbottleneck. This paper presents a hybrid calibration approach by interpreting\nthe mismatch problem as a tracking problem, and uses the extended Kalman filter\nfor online estimation and compensation of the mismatch errors. After\nestimation, the desired signal is reconstructed using a truncated fractional\ndelay filter and a high-pass filter. Simulations demonstrate that our algorithm\nsubstantially outperforms the existing hybrid calibration method in both\nmismatch estimation and compensation.",
        "The performance of algorithms for decentralized optimization is affected by\nboth the optimization error and the consensus error, the latter of which arises\nfrom the variation between agents' local models. Classically, algorithms employ\naveraging and gradient-tracking mechanisms with constant combination matrices\nto drive the collection of agents to consensus. Recent works have demonstrated\nthat using sequences of combination matrices that achieve finite-time consensus\n(FTC) can result in improved communication efficiency or iteration complexity\nfor decentralized optimization. Notably, these studies apply to highly\nstructured networks, where exact finite-time consensus sequences are known\nexactly and in closed form. In this work we investigate the impact of utilizing\napproximate FTC matrices in decentralized learning algorithms, and quantify the\nimpact of the approximation error on convergence rate and steady-state\nperformance. Approximate FTC matrices can be inferred for general graphs and do\nnot rely on a particular graph structure or prior knowledge, making the\nproposed scheme applicable to a broad range of decentralized learning settings.",
        "Generalized spatial modulation-aided affine frequency division multiplexing\n(GSM-AFDM) is conceived for reliable multiple-input multiple-output (MIMO)\ncommunications over doubly selective channels. We commence by proposing several\nlow-complexity detectors for large-scale GSM-AFDM systems. Specifically, we\nintroduce the linear minimum mean square error (LMMSE) equalizer-based maximum\nlikelihood detector (LMMSE-MLD). By exploiting the GSM properties, we then\nderive the LMMSE-based transmit-antenna activation pattern (TAP) check-based\nlog-likelihood ratio detector (LMMSE-TC-LLRD). In addition, we propose a pair\nof new detectors, namely the greedy residual check detector (GRCD) and the\nreduced space check detector (RSCD). We also derive a bit error rate (BER)\nupper-bound by considering the MLD. Our simulation results demonstrate that 1)\nthe BER upper bound derived is tight for moderate to high signal-to-noise\nratios (SNRs), 2) the proposed GSM-AFDM achieves lower BER than its\nconventional counterparts, and 3) the conceived detectors strike a compelling\ntrade-off between the BER and complexity.",
        "Digital modulation schemes such as PMCW have recently attracted increasing\nattention as possible replacements for FMCW modulation in future automotive\nradar systems. A significant obstacle to their widespread adoption is the\nexpensive and power-consuming ADC required at gigahertz frequencies. To\nmitigate these challenges, employing low-resolution ADC, such as one-bit, has\nbeen suggested. Nonetheless, using one-bit sampling results in the loss of\nessential information. This study explores two RD imaging methods in PMCW radar\nsystems utilizing NN. The first method merges standard RD signal processing\nwith a GAN, whereas the second method uses an E2E strategy in which traditional\nsignal processing is substituted with an NN-based RD module. The findings\nindicate that these methods can substantially improve the probability of\ndetecting targets in the range-Doppler domain.",
        "In conventional approaches for multiobject tracking (MOT), raw sensor data\nundergoes several preprocessing stages to reduce data rate and computational\ncomplexity. This typically includes coherent processing that aims at maximizing\nthe signal-to-noise ratio (SNR), followed by a detector that extracts \"point\"\nmeasurements, e.g., the range and bearing of objects, which serve as inputs for\nsequential Bayesian MOT. While using point measurements significantly\nsimplifies the statistical model, the reduced data rate can lead to a loss of\ncritical, object-related information and, thus, potentially to reduced tracking\nperformance. In this paper, we propose a direct tracking approach that avoids a\ndetector and most preprocessing stages. For direct tracking, we introduce a\nmeasurement model for the data-generating process of the sensor data, along\nwith state-transition and birth models for the dynamics and the appearance and\ndisappearance of objects. Based on the new statistical model, we develop a\nfactor graph and particle-based belief propagation (BP) method for efficient\nsequential Bayesian estimation. Contrary to the track-before-detect (TBD)\nparadigm which also avoids a detector, direct tracking integrates coherent\nprocessing within the Bayesian MOT framework. Numerical experiments based on a\npassive acoustic dataset demonstrate that the proposed direct approach\noutperforms state-of-the-art conventional methods that rely on multiple\npreprocessing stages.",
        "This paper introduces the weighted-sum energy efficiency (WSEE) as an\nadvanced performance metric designed to represent the uplink energy efficiency\n(EE) of individual user equipment (UE) in a user-centric Cell-Free massive MIMO\n(CF-mMIMO) system more accurately. In this realistic user-centric CF-mMIMO\ncontext, each UE may exhibit distinct characteristics, such as maximum transmit\npower limits or specific minimum data rate requirements. By computing the EE of\neach UE independently and adjusting the weights accordingly, the system can\naccommodate these unique attributes, thus promoting energy-efficient operation.\nThe uplink WSEE is formulated as a multiple-ratio fractional programming (FP)\nproblem, representing a weighted sum of the EE of individual UEs, which depends\non each UE's transmit power and the combining vector at the CPU. To effectively\nmaximize WSEE, we present optimization algorithms that utilize the Dinkelbach\ntransform and the quadratic transform (QT). Applying the QT twice consecutively\nyields significant performance gains in terms of WSEE. This framework\nestablishes a foundation for developing operational strategies tailored to\nspecific system requirements.",
        "Multimodal Large Language Models (MLLMs) have serious security\nvulnerabilities.While safety alignment using multimodal datasets consisting of\ntext and data of additional modalities can effectively enhance MLLM's security,\nit is costly to construct these datasets. Existing low-resource security\nalignment methods, including textual alignment, have been found to struggle\nwith the security risks posed by additional modalities. To address this, we\npropose Synthetic Embedding augmented safety Alignment (SEA), which optimizes\nembeddings of additional modality through gradient updates to expand textual\ndatasets. This enables multimodal safety alignment training even when only\ntextual data is available. Extensive experiments on image, video, and\naudio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding\non a single RTX3090 GPU within 24 seconds. SEA significantly improves the\nsecurity of MLLMs when faced with threats from additional modalities. To assess\nthe security risks introduced by video and audio, we also introduced a new\nbenchmark called VA-SafetyBench. High attack success rates across multiple\nMLLMs validate its challenge. Our code and data will be available at\nhttps:\/\/github.com\/ZeroNLP\/SEA.",
        "In topological phases of matter for which the bulk and boundary support\ndistinct electronic gaps, there exists the possibility of decoupled mobility\ngaps in the presence of disorder. This is in analogy with the well-studied\nproblem of realizing separate or concomitant bulk-boundary criticality in\nconventional Landau theory. Using a three-dimensional axion insulator having\nclean, gapped surfaces with $e^2\/2h$ quantized Hall conductance, we show the\nbulk and surface mobility gap evolve differently in the presence of disorder.\nThe decoupling of the bulk and surface topology yields a regime that realizes a\ntwo-dimensional, unquantized anomalous Hall metal in the Gaussian unitary\nensemble (GUE), which shares some spectral and response properties akin to the\nsurface states of a conventional three-dimensional (3D) topological insulator.\nThe generality of these results as well as extensions to other insulators and\nsuperconductors is discussed.",
        "The study investigates hip-hop music producer Scott Storch's approach to\ntonality, where the song's key is transposed to fit the Roland TR-808 bass drum\ninstead of tuning the drums to the song's key. This process, involving the\nadjustment of all tracks except the bass drum, suggests significant production\nmotives. The primary constraint stems from the limited usable pitch range of\nthe TR-808 bass drum if its characteristic sound is to be preserved. The\nresearch examines drum tuning practices, the role of the Roland TR-808 in\nmusic, and the sub-bass qualities of its bass drum. Analysis of TR-808 samples\nreveals their characteristics and their integration into modern genres like\ntrap and hip-hop. The study also considers the impact of loudspeaker frequency\nresponse and human ear sensitivity on bass drum perception. The findings\nsuggest that Storch's method prioritizes the spectral properties of the bass\ndrum over traditional pitch values to enhance the bass response. The need to\nmaintain the unique sound of the TR-808 bass drum underscores the importance of\nspectral formants and register in contemporary popular music production.",
        "As large volumes of trajectory data accumulate, simplifying trajectories to\nreduce storage and querying costs is increasingly studied. Existing proposals\nface three main problems. First, they require numerous iterations to decide\nwhich GPS points to delete. Second, they focus only on the relationships\nbetween neighboring points (local information) while neglecting the overall\nstructure (global information), reducing the global similarity between the\nsimplified and original trajectories and making it difficult to maintain\nconsistency in query results, especially for similarity-based queries. Finally,\nthey fail to differentiate the importance of points with similar features,\nleading to suboptimal selection of points to retain the original trajectory\ninformation.\n  We propose MLSimp, a novel Mutual Learning query-driven trajectory\nsimplification framework that integrates two distinct models: GNN-TS, based on\ngraph neural networks, and Diff-TS, based on diffusion models. GNN-TS evaluates\nthe importance of a point according to its globality, capturing its correlation\nwith the entire trajectory, and its uniqueness, capturing its differences from\nneighboring points. It also incorporates attention mechanisms in the GNN\nlayers, enabling simultaneous data integration from all points within the same\ntrajectory and refining representations, thus avoiding iterative processes.\nDiff-TS generates amplified signals to enable the retention of the most\nimportant points at low compression rates. Experiments involving eight\nbaselines on three databases show that MLSimp reduces the simplification time\nby 42%--70% and improves query accuracy over simplified trajectories by up to\n34.6%.",
        "In hydrogen-rich (H-rich) Supernova (SN) events, the collision between the\nH-rich ejecta and the Circum-Stellar Medium (CSM) can accelerate particles and\nproduce high-energy neutrinos (HE-$\\nu$, TeV-PeV) through proton-proton\ninelastic scattering. Despite understanding the production mechanism of these\nneutrinos, the lack of direct observations raises questions about particle\nacceleration efficiency and the involved astrophysical conditions. This study\nfocuses on neutrino emission from H-rich SNe with low-mass CSM, such as SN\n2023ixf. We developed a semi-analytical model to characterize the progenitor\nand CSM at the explosion time, allowing us to infer the expected neutrino flux\nat Earth during the SN's interaction phase. Our model shows that neutrino\nemission depends not only on shock velocity and CSM mass but also on the\nspatial matter distribution of the CSM. By analysing the bolometric light curve\nof SN 2023ixf beyond 100 days post-explosion, we find that its ejecta,\nconsisting of $9\\,\\text{M}_{\\rm \\odot}$ (including $0.07\\,\\text{M}_{\\rm \\odot}$\nof radioactive $^{56}$Ni) and having a kinetic energy of $1.8\\,\\text{foe}$,\ncollides with a low-mass CSM of $0.06\\,\\text{M}_{\\rm \\odot}$ distributed\naccording to a power-law density profile with an exponent of $s=2.9$. Through\nthese parameters, we estimate that up to $4\\pm1\\times 10^{-2}$ muon\n(anti-)neutrino events could be detected by IceCube within 50 days\npost-explosion. Although the predicted flux ($\\lesssim 3\\times\n10^{-9}\\,\\text{GeV} \\, \\text{cm}^{-2} \\, \\text{s}^{-1}$) is below current\nIceCube sensitivity, future telescopes like IceCube-Gen2 and KM3NeT could\ndetect HE-$\\nu$ from similar SN events.",
        "Channel tracking in millimeter wave (mmWave) vehicular systems is crucial for\nmaintaining robust vehicle-to-infrastructure (V2I) communication links, which\ncan be leveraged to achieve high accuracy vehicle position and orientation\ntracking as a byproduct of communication. While prior work tends to simplify\nthe system model by omitting critical system factors such as clock offsets,\nfiltering effects, antenna array orientation offsets, and channel estimation\nerrors, we address the challenges of a practical mmWave multiple-input\nmultiple-output (MIMO) communication system between a single base station (BS)\nand a vehicle while tracking the vehicle's position and orientation (PO)\nconsidering realistic driving behaviors. We first develop a channel tracking\nalgorithm based on multidimensional orthogonal matching pursuit (MOMP) with\nfactoring (F-MOMP) to reduce computational complexity and enable\nhigh-resolution channel estimates during the tracking stage, suitable for PO\nestimation. Then, we develop a network called VO-ChAT (Vehicle\nOrientation-Channel Attention for orientation Tracking), which processes the\nchannel estimate sequence for orientation prediction. Afterward, a weighted\nleast squares (WLS) problem that exploits the channel geometry is formulated to\ncreate an initial estimate of the vehicle's 2D position. A second network named\nVP-ChAT (Vehicle Position-Channel Attention for position Tracking) refines the\ngeometric position estimate. VP-ChAT is a Transformer inspired network\nprocessing the historical channel and position estimates to provide the\ncorrection for the initial geometric position estimate. The proposed solution\nis evaluated using raytracing generated channels in an urban canyon\nenvironment. For 80% of the cases it achieves a 2D position tracking accuracy\nof 26 cm while orientation errors are kept below 0.5 degree.",
        "Electroencephalography (EEG) is widely used in neuroscience and clinical\nresearch for analyzing brain activity. While deep learning models such as\nEEGNet have shown success in decoding EEG signals, they often struggle with\ndata complexity, inter-subject variability, and noise robustness. Recent\nadvancements in quantum machine learning (QML) offer new opportunities to\nenhance EEG analysis by leveraging quantum computing's unique properties. In\nthis study, we extend the previously proposed Quantum-EEGNet (QEEGNet), a\nhybrid neural network incorporating quantum layers into EEGNet, to investigate\nits generalization ability across multiple EEG datasets. Our evaluation spans a\ndiverse set of cognitive and motor task datasets, assessing QEEGNet's\nperformance in different learning scenarios. Experimental results reveal that\nwhile QEEGNet demonstrates competitive performance and maintains robustness in\ncertain datasets, its improvements over traditional deep learning methods\nremain inconsistent. These findings suggest that hybrid quantum-classical\narchitectures require further optimization to fully leverage quantum advantages\nin EEG processing. Despite these limitations, our study provides new insights\ninto the applicability of QML in EEG research and highlights challenges that\nmust be addressed for future advancements.",
        "The increasing use of machine learning in learning analytics (LA) has raised\nsignificant concerns around algorithmic fairness and privacy. Synthetic data\nhas emerged as a dual-purpose tool, enhancing privacy and improving fairness in\nLA models. However, prior research suggests an inverse relationship between\nfairness and privacy, making it challenging to optimize both. This study\ninvestigates which synthetic data generators can best balance privacy and\nfairness, and whether pre-processing fairness algorithms, typically applied to\nreal datasets, are effective on synthetic data. Our results highlight that the\nDEbiasing CAusal Fairness (DECAF) algorithm achieves the best balance between\nprivacy and fairness. However, DECAF suffers in utility, as reflected in its\npredictive accuracy. Notably, we found that applying pre-processing fairness\nalgorithms to synthetic data improves fairness even more than when applied to\nreal data. These findings suggest that combining synthetic data generation with\nfairness pre-processing offers a promising approach to creating fairer LA\nmodels.",
        "Current machine learning paradigm relies on continuous representations like\nneural networks, which iteratively adjust parameters to approximate outcomes\nrather than directly learning the structure of problem. This spreads\ninformation across the network, causing issues like information loss and\nincomprehensibility Building on prior work in environment dynamics modeling, we\npropose a method that learns visual space in a structured, continual manner.\nOur approach refines networks to capture the core structure of objects while\nrepresenting significant subvariants in structure efficiently. We demonstrate\nthis with 2D shape detection, showing incremental learning on MNIST without\noverwriting knowledge and creating compact, comprehensible representations.\nThese results offer a promising step toward a transparent, continually learning\nalternative to traditional neural networks for visual processing.",
        "Data wrangling is a time-consuming and challenging task in a data science\npipeline. While many tools have been proposed to automate or facilitate data\nwrangling, they often misinterpret user intent, especially in complex tasks. We\npropose Dango, a mixed-initiative multi-agent system for data wrangling.\nCompared to existing tools, Dango enhances user communication of intent by\nallowing users to demonstrate on multiple tables and use natural language\nprompts in a conversation interface, enabling users to clarify their intent by\nanswering LLM-posed multiple-choice clarification questions, and providing\nmultiple forms of feedback such as step-by-step natural language explanations\nand data provenance to help users evaluate the data wrangling scripts. We\nconducted a within-subjects user study with 38 participants and demonstrated\nthat Dango's features can significantly improve intent clarification, accuracy,\nand efficiency in data wrangling. Furthermore, we demonstrated the\ngeneralizability of Dango by applying it to a broader set of data wrangling\ntasks.",
        "The emergence of the Non-Terrestrial Network (NTN) concept in the last years\nhas revolutionized the space industry. This novel network architecture composed\nof aircraft and spacecraft is currently being standardized by the 3GPP. This\nstandardization process follows dedicated phases in which experimentation of\nthe technology is needed. Although some missions have been conducted to\ndemonstrate specific and service-centric technologies, a open flexible in-orbit\ninfrastructure is demanded to support this standardization process. This work\npresents the 6GStarLab mission, which aims to address this gap. Specifically,\nthis mission envisions to provide a 6U CubeSat as the main in-orbit\ninfrastructure in which multiple technology validations can be uploaded. The\nconcept of this mission is depicted. Additionally, this work presents the\ndetails of the satellite platform and the payload. This last one is designed to\nenable the experimentation in multiple radio-frequency bands (i.e. UHF, S-, X-,\nand Ka-bands) and an optical terminal. The launch of the satellite is scheduled\nfor Q2 2025, and it will contribute to the standardization of future NTN\narchitectures.",
        "Primordial Magnetic Fields (PMFs), long studied as potential relics of the\nearly Universe, accelerate the recombination process and have been proposed as\na possible way to relieve the Hubble tension. However, previous studies relied\non simplified toy models. In this study, for the first time, we use the recent\nhigh-precision evaluations of recombination with PMFs, incorporating full\nmagnetohydrodynamic (MHD) simulations and detailed Lyman-alpha radiative\ntransfer, to test PMF-enhanced recombination ($b\\Lambda$CDM) against\nobservational data from the cosmic microwave background (CMB), baryon acoustic\noscillations (BAO), and Type Ia supernovae (SN). Focusing on non-helical PMFs\nwith a Batchelor spectrum, we find a preference for present-day total field\nstrengths of approximately 5-10 pico-Gauss. Depending on the dataset\ncombination, this preference ranges from mild ($\\sim 1.8\\sigma$ with Planck +\nDESI) to moderate ($\\sim 3\\sigma$ with Planck + DESI + SH0ES-calibrated SN)\nsignificance. The $b\\Lambda$CDM has Planck + DESI $\\chi^2$ values equal or\nbetter than those of the $\\Lambda$CDM model while predicting a higher Hubble\nconstant. The favored field strengths align closely with those required for\ncluster magnetic fields to originate entirely from primordial sources, without\nthe need for additional dynamo amplification or stellar magnetic field\ncontamination. Future high-resolution CMB temperature and polarization\nmeasurements will be crucial for confirming or further constraining the\npresence of PMFs at recombination.",
        "With the explosive growth of academic literature, effectively evaluating the\nknowledge value of literature has become quite essential. However, most of the\nexisting methods focus on modeling the entire citation network, which is\nstructurally complex and often suffers from long sequence dependencies when\ndealing with text embeddings. Thus, they might have low efficiency and poor\nrobustness in different fields. To address these issues, a novel knowledge\nevaluation method is proposed, called EMK-KEN. The model consists of two\nmodules. Specifically, the first module utilizes MetaFP and Mamba to capture\nsemantic features of node metadata and text embeddings to learn contextual\nrepresentations of each paper. The second module utilizes KAN to further\ncapture the structural information of citation networks in order to learn the\ndifferences in different fields of networks. Extensive experiments based on ten\nbenchmark datasets show that our method outperforms the state-of-the-art\ncompetitors in effectiveness and robustness.",
        "Electroencephalograph (EEG) is a crucial tool for studying brain activity.\nRecently, self-supervised learning methods leveraging large unlabeled datasets\nhave emerged as a potential solution to the scarcity of widely available\nannotated EEG data. However, current methods suffer from at least one of the\nfollowing limitations: i) sub-optimal EEG signal modeling, ii) model sizes in\nthe hundreds of millions of trainable parameters, and iii) reliance on private\ndatasets and\/or inconsistent public benchmarks, hindering reproducibility. To\naddress these challenges, we introduce a Compact Encoder for Representations of\nBrain Oscillations using alternating attention (CEReBrO), a new small EEG\nfoundation model. Our tokenization scheme represents EEG signals at a\nper-channel patch granularity. We propose an alternating attention mechanism\nthat jointly models intra-channel temporal dynamics and inter-channel spatial\ncorrelations, achieving 2x speed improvement with 6x less memory required\ncompared to standard self-attention. We present several model sizes ranging\nfrom 3.6 million to 85 million parameters. Pre-trained on over 20,000 hours of\npublicly available scalp EEG recordings with diverse channel configurations,\nour models set new benchmarks in emotion detection and seizure detection tasks,\nwith competitive performance in anomaly classification and gait prediction.\nThis validates our models' effectiveness and efficiency.",
        "Many recent studies have found evidence for emergent reasoning capabilities\nin large language models, but debate persists concerning the robustness of\nthese capabilities, and the extent to which they depend on structured reasoning\nmechanisms. To shed light on these issues, we perform a comprehensive study of\nthe internal mechanisms that support abstract rule induction in an open-source\nlanguage model (Llama3-70B). We identify an emergent symbolic architecture that\nimplements abstract reasoning via a series of three computations. In early\nlayers, symbol abstraction heads convert input tokens to abstract variables\nbased on the relations between those tokens. In intermediate layers, symbolic\ninduction heads perform sequence induction over these abstract variables.\nFinally, in later layers, retrieval heads predict the next token by retrieving\nthe value associated with the predicted abstract variable. These results point\ntoward a resolution of the longstanding debate between symbolic and neural\nnetwork approaches, suggesting that emergent reasoning in neural networks\ndepends on the emergence of symbolic mechanisms."
      ]
    }
  },
  {
    "id":2411.05236,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Shannon capacity of signal transduction for multiple independent receptors",
    "start_abstract":"Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
    "start_categories":[
      "cs.SY",
      "eess.SP"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
      ],
      "abstract":[
        "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of\n  RNA Graph Motifs Using Topological Descriptors",
        "PyMOLfold: Interactive Protein and Ligand Structure Prediction in PyMOL",
        "Silicon is the next frontier in plant synthetic biology",
        "Deep Learning of Proteins with Local and Global Regions of Disorder",
        "Nonsuppressible viremia during HIV-1 therapy meets molecular virology",
        "Engineered Zwitterion-Infused Clay Composites with Antibacterial and\n  Antifungal Efficacy",
        "Artificial Intelligence Approaches for Anti-Addiction Drug Discovery",
        "Remodeling Peptide-MHC-TCR Triad Binding as Sequence Fusion for\n  Immunogenicity Prediction",
        "Advances in RNA secondary structure prediction and RNA modifications:\n  Methods, data, and applications",
        "Inverse problems with experiment-guided AlphaFold",
        "DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown\n  Efficiency",
        "RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via\n  Synergistic Flow Matching",
        "Fluorescence Phasor Analysis: Basic Principles and Biophysical\n  Applications",
        "Census of Ly$\\alpha$ Emission from $\\sim 600$ Galaxies at $z=5-14$:\n  Evolution of the Ly$\\alpha$ Luminosity Function and a Late Sharp Cosmic\n  Reionization",
        "Dynamics of disordered quantum systems with two- and three-dimensional\n  tensor networks",
        "Anomalies of the Scholtes regularization for mathematical programs with\n  complementarity constraints",
        "Continuum limit of fourth-order Schr\\\"{o}dinger equations on the lattice",
        "Some examples of affine isometries of Banach spaces arising from 1-D\n  dynamics",
        "Near-Optimal Parameter Tuning of Level-1 QAOA for Ising Models",
        "Quantum State Designs from Minimally Random Quantum Circuits",
        "Pfaffian solution for dark-dark soliton to the coupled complex modified\n  Korteweg-de Vries equation",
        "The Change of Variable Formula Integrals, do they have equal value?",
        "The Category of Atomic Monoids: Universal Constructions and Arithmetic\n  Properties",
        "Twisted oxide membrane interface by local atomic registry design",
        "Heterogeneous Treatment Effect in Time-to-Event Outcomes: Harnessing\n  Censored Data with Recursively Imputed Trees",
        "Comparative Analysis of Perturbed $f(R)$ Gravity and Perturbed Rastall\n  Gravity Models in Describing Cosmic Evolution from Early to Late Universe\n  Relative to the $\\Lambda$CDM Model",
        "The outflow impacts on the size of the narrow-line region among type-2\n  AGNs",
        "GECKO Follow-up Observation of the Binary Neutron Star-Black Hole Merger\n  Candidate S230518h"
      ],
      "abstract":[
        "We introduce a computational topology-based approach with unsupervised\nmachine-learning algorithms to estimate the database size and content of\nRNA-like graph topologies. Specifically, we apply graph theory enumeration to\ngenerate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2\nto 9. Among them, only 0.11% graphs correspond to approximately 200,000 known\nRNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping\nmethod. The remaining 99.89% of the dual graphs may be RNA-like or\nnon-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are\nmore likely to be associated with RNA structures, we apply computational\ntopology descriptors using the Persistent Spectral Graphs (PSG) method to\ncharacterize each graph using 19 PSG-based features and use clustering\nalgorithms that partition all possible dual graphs into two clusters, RNA-like\ncluster and non-RNA-like cluster. The distance of each dual graph to the center\nof the RNA-like cluster represents the likelihood of it belonging to RNA\nstructures. From validation, our PSG-based RNA-like cluster includes 97.3% of\nthe 121 known RNA dual graphs, suggesting good performance. Furthermore,\n46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,\nwe observe that all the top 15 RNA-like dual graphs can be separated into\nmultiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to\nhave any subgraphs. Moreover, a significant topological difference between top\nRNA-like and non-RNA-like graphs is evident when comparing their topological\nfeatures. These findings provide valuable insights into the size of the RNA\nmotif universe and RNA design strategies, offering a novel framework for\npredicting RNA graph topologies and guiding the discovery of novel RNA motifs,\nperhaps anti-viral therapeutics by subgraph assembly.",
        "PyMOLfold is a flexible and open-source plugin designed to seamlessly\nintegrate AI-based protein structure prediction and visualization within the\nwidely used PyMOL molecular graphics system. By leveraging state-of-the-art\nprotein folding models such as ESM3, Boltz-1, and Chai-1, PyMOLfold allows\nresearchers to directly predict protein tertiary structures from amino acid\nsequences without requiring external tools or complex workflows. Furthermore,\nwith certain models, users can provide a SMILES string of a ligand and have the\nsmall molecule placed in the protein structure. This unique capability bridges\nthe gap between computational folding and structural visualization, enabling\nusers to input a primary sequence, perform a folding prediction, and\nimmediately explore the resulting 3D structure within the same intuitive\nplatform.",
        "Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field.",
        "Although machine learning has transformed protein structure prediction of\nfolded protein ground states with remarkable accuracy, intrinsically disordered\nproteins and regions (IDPs\/IDRs) are defined by diverse and dynamical\nstructural ensembles that are predicted with low confidence by algorithms such\nas AlphaFold. We present a new machine learning method, IDPForge (Intrinsically\nDisordered Protein, FOlded and disordered Region GEnerator), that exploits a\ntransformer protein language diffusion model to create all-atom IDP ensembles\nand IDR disordered ensembles that maintains the folded domains. IDPForge does\nnot require sequence-specific training, back transformations from\ncoarse-grained representations, nor ensemble reweighting, as in general the\ncreated IDP\/IDR conformational ensembles show good agreement with solution\nexperimental data, and options for biasing with experimental restraints are\nprovided if desired. We envision that IDPForge with these diverse capabilities\nwill facilitate integrative and structural studies for proteins that contain\nintrinsic disorder.",
        "HIV-1 replication can be suppressed with antiretroviral therapy (ART), but\nindividuals who stop taking ART soon become viremic again. Some people\nexperience extended times of detectable viremia despite optimal adherence to\nART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of\nnonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded\nT cells harboring HIV-1 proviruses with small deletions or mutations in the\n5'-leader, the UTR that includes the major splice donor site of viral RNA.\nThese mutations altered viral RNA-splicing efficiency and RNA dimerization and\npackaging, yet still allowed production of detectable levels of noninfectious\nvirus particles. These particles lacked the HIV-1 Env surface protein required\nfor cell entry and failed to form the mature capsid cone required for\ninfectivity. These studies improve our understanding of NSV and the regulation\nof viral functions in the 5'-leader with implications for rationalized care in\nindividuals with NSV.",
        "Microbes and pathogens play a detrimental role in healing wounds, causing\ninfections like impetigo through bodily fluids and skin and entering the\nbloodstream through the wounds, thereby hindering the healing process and\ntissue regeneration. Clay, known for its long history of natural therapeutic\nuse, has emerged as one of the most promising candidates for biomedical\napplications due to its non-toxic nature, porosity, high surface area,\nubiquity, and excellent cation exchange capacity. This study demonstrates an\ninnovative approach to engineering an organo-functionalized,\ninfection-resistant, easy-to-use bandage material from clay, an environmentally\nbenign and sustainable material. The hybrid membranes have been developed using\nclays, zwitterions, silver ions, and terbinafine hydrochloride (TBH) to impart\nantibacterial and antifungal efficacy. A critical aspect of this study is\nembedding organic molecules and metal ions with the clays and releasing them to\nresist the growth and kill the pathogens. The antimicrobial efficacy of the\nmembranes has been tested using a zone of inhibition study against the most\ncommon microbes in skin wounds, viz. S. aureus, E. coli, and C. albicans.\nResults from our studies not only demonstrate the potential of these hybrid\nclay membranes as a cost-effective, scalable, and effective solution for\ntreating microbial infections but also instill newer avenues for point-of-care\nwound-healing treatments, offering hope for improved patient outcomes.",
        "Drug addiction is a complex and pervasive global challenge that continues to\npose significant public health concerns. Traditional approaches to\nanti-addiction drug discovery have struggled to deliver effective therapeutics,\nfacing high attrition rates, long development timelines, and inefficiencies in\nprocessing large-scale data. Artificial intelligence (AI) has emerged as a\ntransformative solution to address these issues. Using advanced algorithms, AI\nis revolutionizing drug discovery by enhancing the speed and precision of key\nprocesses. This review explores the transformative role of AI in the pipeline\nfor anti-addiction drug discovery, including data collection, target\nidentification, and compound optimization. By highlighting the potential of AI\nto overcome traditional barriers, this review systematically examines how AI\naddresses critical gaps in anti-addiction research, emphasizing its potential\nto revolutionize drug discovery and development, overcome challenges, and\nadvance more effective therapeutic strategies.",
        "The complex nature of tripartite peptide-MHC-TCR interactions is a critical\nyet underexplored area in immunogenicity prediction. Traditional studies on\nTCR-antigen binding have not fully addressed the complex dependencies in triad\nbinding. In this paper, we propose new modeling approaches for these tripartite\ninteractions, utilizing sequence information from MHCs, peptides, and TCRs. Our\nmethods adhere to native sequence forms and align with biological processes to\nenhance prediction accuracy. By incorporating representation learning\ntechniques, we introduce a fusion mechanism to integrate the three sequences\neffectively. Empirical experiments show that our models outperform traditional\nmethods, achieving a 2.8 to 13.3 percent improvement in prediction accuracy\nacross existing benchmarks. We further validate our approach with extensive\nablation studies, demonstrating the effectiveness of the proposed model\ncomponents. The model implementation, code, and supplementary materials,\nincluding a manuscript with colored hyperlinks and a technical appendix for\ndigital viewing, will be open-sourced upon publication.",
        "Due to the hierarchical organization of RNA structures and their pivotal\nroles in fulfilling RNA functions, the formation of RNA secondary structure\ncritically influences many biological processes and has thus been a crucial\nresearch topic. This review sets out to explore the computational prediction of\nRNA secondary structure and its connections to RNA modifications, which have\nemerged as an active domain in recent years. We first examine the progression\nof RNA secondary structure prediction methodology, focusing on a set of\nrepresentative works categorized into thermodynamic, comparative, machine\nlearning, and hybrid approaches. Next, we survey the advances in RNA\nmodifications and computational methods for identifying RNA modifications,\nfocusing on the prominent modification types. Subsequently, we highlight the\ninterplay between RNA modifications and secondary structures, emphasizing how\nmodifications such as m6A dynamically affect RNA folding and vice versa. In\naddition, we also review relevant data sources and provide a discussion of\ncurrent challenges and opportunities in the field. Ultimately, we hope our\nreview will be able to serve as a cornerstone to aid in the development of\ninnovative methods for this emerging topic and foster therapeutic applications\nin the future.",
        "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
        "Background: Small interfering RNA (siRNA) is a promising therapeutic agent\ndue to its ability to silence disease-related genes via RNA interference. While\ntraditional machine learning and early deep learning methods have made progress\nin predicting siRNA efficacy, there remains significant room for improvement.\nAdvanced deep learning techniques can enhance prediction accuracy, reducing the\nreliance on extensive wet-lab experiments and accelerating the identification\nof effective siRNA sequences. This approach also provides deeper insights into\nthe mechanisms of siRNA efficacy, facilitating more targeted and efficient\ntherapeutic strategies.\n  Methods: We introduce DeepSilencer, an innovative deep learning model\ndesigned to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced\nneural network architectures to capture the complex features of siRNA\nsequences. Our key contributions include a specially designed deep learning\nmodel, an innovative online data sampling method, and an improved loss function\ntailored for siRNA prediction. These enhancements collectively boost the\nmodel's prediction accuracy and robustness.\n  Results: Extensive evaluations on multiple test sets demonstrate that\nDeepSilencer achieves state-of-the-art performance using only siRNA sequences\nand basic physicochemical properties. Our model surpasses several other methods\nand shows superior predictive performance, particularly when incorporating\nthermodynamic parameters.\n  Conclusion: The advancements in data sampling, model design, and loss\nfunction significantly enhance the predictive capabilities of DeepSilencer.\nThese improvements underscore its potential to advance RNAi therapeutic design\nand development, offering a powerful tool for researchers and clinicians.",
        "Ribonucleic acid (RNA) binds to molecules to achieve specific biological\nfunctions. While generative models are advancing biomolecule design, existing\nmethods for designing RNA that target specific ligands face limitations in\ncapturing RNA's conformational flexibility, ensuring structural validity, and\novercoming data scarcity. To address these challenges, we introduce RiboFlow, a\nsynergistic flow matching model to co-design RNA structures and sequences based\non target molecules. By integrating RNA backbone frames, torsion angles, and\nsequence features in an unified architecture, RiboFlow explicitly models RNA's\ndynamic conformations while enforcing sequence-structure consistency to improve\nvalidity. Additionally, we curate RiboBind, a large-scale dataset of\nRNA-molecule interactions, to resolve the scarcity of high-quality structural\ndata. Extensive experiments reveal that RiboFlow not only outperforms\nstate-of-the-art RNA design methods by a large margin but also showcases\ncontrollable capabilities for achieving high binding affinity to target\nligands. Our work bridges critical gaps in controllable RNA design, offering a\nframework for structure-aware, data-efficient generation.",
        "Fluorescence is one of the most widely used techniques in biological\nsciences. Its exceptional sensitivity and versatility make it a tool of first\nchoice for quantitative studies in biophysics. The concept of phasors,\noriginally introduced by Charles Steinmetz in the late 19th century for\nanalyzing alternating current circuits, has since found applications across\ndiverse disciplines, including fluorescence spectroscopy. The main idea behind\nfluorescence phasors was posited by Gregorio Weber in 1981. By analyzing the\ncomplementary nature of pulse and phase fluorometry data, he shows that two\nmagnitudes -- denoted as $G$ and $S$ -- derived from the frequency-domain\nfluorescence measurements correspond to the real and imaginary part of the\nFourier transform of the fluorescence intensity in the time domain. This review\nprovides a historical perspective on how the concept of phasors originates and\nhow it integrates into fluorescence spectroscopy. We discuss their fundamental\nalgebraic properties, which enable intuitive model-free analysis of\nfluorescence data despite the complexity of the underlying phenomena. Some\napplications in biophysics illustrate the power of this approach in studying\ndiverse phenomena, including protein folding, protein interactions, phase\ntransitions in lipid mixtures and formation of high-order structures in nucleic\nacids.",
        "We present the statistical properties of Ly$\\alpha$ emission in 586 galaxies\nat $z=4.5-14.2$, observed by multiple JWST\/NIRSpec spectroscopy projects,\nincluding JADES, GLASS, CEERS, and GO\/DDT programs. We obtain Ly$\\alpha$\nequivalent width (EW), Ly$\\alpha$ escape fraction, and ionizing photon\nproduction efficiency measurements or upper limits for these galaxies, and\nconfirm that the Ly$\\alpha$ emitting galaxy fraction decreases towards higher\nredshifts. We derive Ly$\\alpha$ luminosity functions from $z\\sim 5$ to $z\\sim\n10-14$ with the observed Ly$\\alpha$ EW distributions and galaxy UV luminosity\nfunctions, and find a $\\sim3$ dex decrease in number density at\n$L_\\mathrm{Ly\\alpha}=10^{42}-10^{43}$ erg s$^{-1}$ over the redshift range.\nNotably, this study presents the first constraints on the Ly$\\alpha$ luminosity\nfunction at $z\\sim 8-14$. We obtain the neutral hydrogen fractions of\n$x_\\mathrm{HI}=0.17_{-0.16}^{+0.23}$, $0.63_{-0.28}^{+0.18}$,\n$0.79_{-0.21}^{+0.13}$, and $0.88_{-0.13}^{+0.11}$ at $z\\sim6$, $7$, $8-9$, and\n$10-14$, respectively, via comparisons of the reionization models developed by\nsemi-numerical simulations with 21cmFAST explaining the observations of\nLy$\\alpha$, UV continuum, and Planck electron optical depth. The high\n$x_\\mathrm{HI}$ values over $z\\sim 7-14$ suggest a late and sharp reionization,\nwith the primary reionization process occurring at $z\\sim 6-7$. Such a late and\nsharp reionization is not easily explained by either a clumpy inter-galactic\nmedium or sources of reionization in a classical faint-galaxy or a\nbright-galaxy\/AGN scenario, unless a very high escape fraction or AGN duty\ncycle is assumed at $z\\sim 6-7$.",
        "Quantum spin glasses form a good testbed for studying the performance of\nvarious quantum annealing and optimization algorithms. In this work we show how\ntwo- and three-dimensional tensor networks can accurately and efficiently\nsimulate the quantum annealing dynamics of Ising spin glasses on a range of\nlattices. Such dynamics were recently simulated using D-Wave's Advantage$2$\nsystem [A. D. King et al, Science, 10.1126\/science.ado6285 (2025)] and,\nfollowing extensive comparisons to existing numerical methods, claimed to be\nbeyond the reach of classical computation. Here we show that by evolving\nlattice-specific tensor networks with simple belief propagation to keep up with\nthe entanglement generated during the time evolution and then extracting\nexpectation values with more sophisticated variants of belief propagation,\nstate-of-the-art accuracies can be reached with modest computational resources.\nWe exploit the scalability of our simulations and simulate a system of over\n$300$ qubits, allowing us to verify the universal physics present and extract a\nvalue for the associated Kibble-Zurek exponent which agrees with recent values\nobtained in literature. Our results demonstrate that tensor networks are a\nviable approach for simulating large scale quantum dynamics in two and three\ndimensions on classical computers, and algorithmic advancements are expected to\nexpand their applicability going forward.",
        "For mathematical programs with complementarity constraints (MPCC), we refine\nthe convergence analysis of the Scholtes regularization. Our goal is to relate\nnondegenerate C-stationary points of MPCC with nondegenerate Karush-Kuhn-Tucker\npoints of its Scholtes regularization. We detected the following anomalies: (i)\nin a neighborhood of a nondegenerate C-stationary point there could be\ndegenerate Karush-Kuhn-Tucker points of the Scholtes regularization; (ii) even\nif nondegenerate, they might be locally non-unique; (iii) if nevertheless\nunique, their quadratic index potentially differs from the C-index of the\nC-stationary point under consideration. Thus, a change of the topological type\nfor Karush-Kuhn-Tucker points of the Scholtes regularization is possible. In\nparticular, a nondegenerate minimizer of MPCC might be approximated by saddle\npoints. In order to bypass the mentioned anomalies, an additional generic\ncondition for nondegenerate C-stationary points of MPCC is identified. Then, we\nuniquely trace nondegenerate Karush-Kuhn-Tucker points of the Scholtes\nregularization and successively maintain their topological type.",
        "In this paper, we consider the discrete fourth-order Schr\\\"{o}dinger equation\non the lattice $h\\mathbb{Z}^2$. Uniform Strichartz estimates are established by\nanalyzing frequency localized oscillatory integrals with the method of\nstationary phase and applying Littlewood-Paley inequalities. As an application,\nwe obtain the precise rate of $L^2$ convergence from the solutions of discrete\nsemilinear equations to those of the corresponding equations on the Euclidean\nplane $\\mathbb{R}^2$ in the contimuum limit $h \\rightarrow 0$.",
        "We provide a large family of examples of affine isometries of the Banach\nspaces $C^0 (S^1)$, $L^1 (S^1)$ and $L^2 (S^1 \\times S^1)$ that are\nfixed-point-free despite being recurrent (in particular, they have zero drift).\nThese come from natural cocycles on the group of circle diffeomorphisms, namely\nthe logarithmic, affine and (a variation of the) Schwarzian derivative. Quite\ninterestingly, they arise from diffeomorphisms that are generic in an\nappropriate context. We also show how to promote these examples in order to\nobtain families of commuting isometries satisfying the same properties.",
        "The Quantum Approximate Optimisation Algorithm (QAOA) is a hybrid\nquantum-classical algorithm for solving combinatorial optimisation problems.\nQAOA encodes solutions into the ground state of a Hamiltonian, approximated by\na $p$-level parameterised quantum circuit composed of problem and mixer\nHamiltonians, with parameters optimised classically. While deeper QAOA circuits\ncan offer greater accuracy, practical applications are constrained by complex\nparameter optimisation and physical limitations such as gate noise, restricted\nqubit connectivity, and state-preparation-and-measurement errors, limiting\nimplementations to shallow depths. This work focuses on QAOA$_1$ (QAOA at\n$p=1$) for QUBO problems, represented as Ising models. Despite QAOA$_1$ having\nonly two parameters, $(\\gamma, \\beta)$, we show that their optimisation is\nchallenging due to a highly oscillatory landscape, with oscillation rates\nincreasing with the problem size, density, and weight. This behaviour\nnecessitates high-resolution grid searches to avoid distortion of cost\nlandscapes that may result in inaccurate minima. We propose an efficient\noptimisation strategy that reduces the two-dimensional $(\\gamma, \\beta)$ search\nto a one-dimensional search over $\\gamma$, with $\\beta^*$ computed\nanalytically. We establish the maximum permissible sampling period required to\naccurately map the $\\gamma$ landscape and provide an algorithm to estimate the\noptimal parameters in polynomial time. Furthermore, we rigorously prove that\nfor regular graphs on average, the globally optimal $\\gamma^* \\in \\mathbb{R}^+$\nvalues are concentrated very close to zero and coincide with the first local\noptimum, enabling gradient descent to replace exhaustive line searches. This\napproach is validated using Recursive QAOA (RQAOA), where it consistently\noutperforms both coarsely optimised RQAOA and semidefinite programs across all\ntested QUBO instances.",
        "Random many-body states are both a useful tool to model certain physical\nsystems and an important asset for quantum computation. Realising them,\nhowever, generally requires an exponential (in system size) amount of\nresources. Recent research has presented a way out by showing that one can\ngenerate random states, or more precisely a controlled approximation of them,\nby applying a quantum circuit built in terms of few-body unitary gates. Most of\nthis research, however, has been focussed on the case of quantum circuits\ncomposed by completely random unitary gates. Here we consider what happens for\ncircuits that, instead, involve a minimal degree of randomness. Specifically,\nwe concentrate on two different settings: (a) brickwork quantum circuits with a\nsingle one-qudit random matrix at a boundary; (b) brickwork quantum circuits\nwith fixed interactions but random one-qudit gates everywhere. We show that,\nfor any given initial state, (a) and (b) produce a distribution of states\napproaching the Haar distribution in the limit of large circuit depth. More\nprecisely, we show that the moments of the distribution produced by our\ncircuits can approximate the ones of the Haar distribution in a depth\nproportional to the system size. Interestingly we find that in both Cases (a)\nand (b) the relaxation to the Haar distribution occurs in two steps - this is\nin contrast with what happens in fully random circuits. Moreover, we show that\nchoosing appropriately the fixed interactions, for example taking the local\ngate to be a dual-unitary gate with high enough entangling power, minimally\nrandom circuits produce a Haar random distribution more rapidly than fully\nrandom circuits. In particular, dual-unitary circuits with maximal entangling\npower - i.e. perfect tensors - appear to provide the optimal quantum state\ndesign preparation for any design number.",
        "In this paper, we study coupled complex modified Korteweg-de Vries (ccmKdV)\nequation by combining the Hirota's method and the Kadomtsev-Petviashvili (KP)\nreduction method. First, we show that the bilinear form of the ccmKdV equation\nunder nonzero boundary condition is linked to the discrete BKP hierarchy\nthrough Miwa transformation. Based on this finding, we construct the dark-dark\nsoliton solution in the pfaffian form. The dynamical behaviors for one- and\ntwo-soliton are analyzed and illustrated.",
        "Assuming that the two integrals in the Change of Variable Formula for the\nunidimensional Riemann integral are finite, one can ask if they have equal\nvalue. We give a positive answer to this question. The proof is very easy to\nfollow and to keep in mind. An example is given.",
        "We introduce and investigate the category $\\mathsf{AtoMon}$ of atomic monoids\nand atom-preserving monoid homomorphisms, which is a (non-full) subcategory of\nthe usual category of monoids. In particular, we compute all limits and\ncolimits, showing that $\\mathsf{AtoMon}$ is a complete and cocomplete category.\nWe also address certain arithmetic properties of products and coproducts,\nproviding explicit formulas for some fundamental invariants associated with\nfactorization lengths in atomic monoids.",
        "Interplay of lattice, orbital, and charge degrees of freedom in complex oxide\nmaterials has hosted a plethora of exotic quantum phases and physical\nproperties. Recent advances in synthesis of freestanding complex oxide\nmembranes and twisted heterostructures assembled from membranes provide new\nopportunities for discovery using moir\\'e design with local lattice control. To\nthis end, we designed moir\\'e crystals at the coincidence site lattice\ncondition, providing commensurate structure within the moir\\'e supercell\narising from the multi-atom complex oxide unit cell. We fabricated such twisted\nbilayers from freestanding SrTiO3 membranes and used depth sectioning-based TEM\nmethods to discover ordered charge states at the moir\\'e interface. By\nselectively imaging SrTiO3 atomic planes at different depths through the\nbilayer, we clearly resolved the moir\\'e periodic structure at the twisted\ninterface and found that it exhibits lattice-dependent charge\ndisproportionation in the local atomic registry within the moir\\'e supercell.\nOur density-functional modelling of the twisted oxide interface predicts that\nthese moir\\'e phenomena are accompanied by the emergence of a two-dimensional\nflat band that can drive new electronic phases. Our work provides a novel\nguideline for controlling moir\\'e periodicity in twisted oxides and opens\npathways to exploit the new functionalities via moir\\'e lattice-driven\ncharge-orbital correlation.",
        "Tailoring treatments to individual needs is a central goal in fields such as\nmedicine. A key step toward this goal is estimating Heterogeneous Treatment\nEffects (HTE) - the way treatments impact different subgroups. While crucial,\nHTE estimation is challenging with survival data, where time until an event\n(e.g., death) is key. Existing methods often assume complete observation, an\nassumption violated in survival data due to right-censoring, leading to bias\nand inefficiency. Cui et al. (2023) proposed a doubly-robust method for HTE\nestimation in survival data under no hidden confounders, combining a causal\nsurvival forest with an augmented inverse-censoring weighting estimator.\nHowever, we find it struggles under heavy censoring, which is common in\nrare-outcome problems such as Amyotrophic lateral sclerosis (ALS). Moreover,\nmost current methods cannot handle instrumental variables, which are a crucial\ntool in the causal inference arsenal. We introduce Multiple Imputation for\nSurvival Treatment Response (MISTR), a novel, general, and non-parametric\nmethod for estimating HTE in survival data. MISTR uses recursively imputed\nsurvival trees to handle censoring without directly modeling the censoring\nmechanism. Through extensive simulations and analysis of two real-world\ndatasets-the AIDS Clinical Trials Group Protocol 175 and the Illinois\nunemployment dataset we show that MISTR outperforms prior methods under heavy\ncensoring in the no-hidden-confounders setting, and extends to the instrumental\nvariable setting. To our knowledge, MISTR is the first non-parametric approach\nfor HTE estimation with unobserved confounders via instrumental variables.",
        "This study conducts a meticulous examination of the cosmological implications\ninherent in Rastall gravity and $f(R)$ gravity models, assessing their efficacy\nacross distinct cosmic epochs, from early universe structure formation to\nlate-time acceleration. In the initial stages, both models exhibit commendable\ncompatibility with observed features of structure formation, aligning with the\nestablished $\\Lambda$CDM model. The derived Jeans' wavenumbers for each model\nsupport their viability. However, as the cosmic timeline progresses into the\nlate universe, a discernible disparity surfaces. Utilizing the Markov Chain\nMonte Carlo method, we reconstruct the deceleration parameter $(q)$ and\nidentify Deceleration - Acceleration redshift transition values. For $f(R)$\ngravity, our results align closely with previous studies, emphasizing its\nsuperior ability to elucidate the recent cosmic acceleration. In contrast,\nRastall gravity exhibits distinct redshift transition values. Our rigorous\nanalysis underscores the prowess of $f(R)$ gravity in capturing the observed\ncosmic acceleration, positioning it as a compelling alternative to the\nconventional $\\Lambda$CDM model. The discernible shifts observed in the peaks\nof the CMB power spectrum and evolution of deceleration parameter (q) for both\n$f(R)$ gravity and Rastall gravity models in the Early and Late universe, in\nrelation to the $\\Lambda $CDM model, provide compelling evidence supporting the\nproposition that these alternative gravity models can account for the\nanisotropy of the universe without invoking the need for dark energy.",
        "We present the study of the gas kinematics in narrow-line regions (NLRs) of\n2,009 type-2 AGNs at $z<0.34$. We construct the [O III]$\\lambda$5007\nemission-line images using publicly available broadband images from the Sloan\nDigital Sky Survey (SDSS). The [O III] emission area of the samples, measured\ndown to $1.7\\times10^{-15}$ erg\/s\/cm$^2$\/arcsec$^2$, ranges from 3.7 kpc$^2$ up\nto 224 kpc$^2$. With our broadband technique, we found the strong correlation\nbetween [O III] area and AGN luminosity inferred from the [O III] luminosity\nand the mid-infrared luminosity at the rest-frame $15\\mu$m. The isophotal\nthreshold used to determine the [O III] area affects the correlation strength\nin that the brighter isophote yields the stronger correlation between the [O\nIII] area and AGN luminosity. The presence of gas outflow is examined by the\nratio of the [O III] velocity dispersion to the stellar velocity dispersion\n($\\sigma_{\\rm [O\\,III]}\/\\sigma_\\star > 1.4$) using the SDSS spectra. At the\ngiven luminosity, the objects with and without outflows exhibit the same\nextension of the [O III] emission. Their correlation between the [O III] area\nand luminosity is almost identical. It is suggested that the size of NLRs is\nnot affected by outflow mechanisms but rather by photoionization from the\ncentral AGNS.",
        "The gravitational wave (GW) event S230518h is a potential binary neutron\nstar-black hole merger (NSBH) event that was detected during engineering run 15\n(ER15), which served as the commissioning period before the LIGO-Virgo-KAGRA\n(LVK) O4a observing run. Despite its low probability of producing detectable\nelectromagnetic emissions, we performed extensive follow-up observations of\nthis event using the GECKO telescopes in the southern hemisphere. Our\nobservation covered 61.7\\% of the 90\\% credible region, a $\\rm 284\\:deg^2$ area\naccessible from the southern hemisphere, reaching a median limiting magnitude\nof $R=21.6$ mag. In these images, we conducted a systematic search for an\noptical counterpart of this event by combining a CNN-based classifier and human\nverification. We identified 128 transient candidates, but no significant\noptical counterpart was found that could have caused the GW signal.\nFurthermore, we provide feasible KN properties that are consistent with the\nupper limits of observation. Although no optical counterpart was found, our\nresult demonstrates both GECKO's efficient wide-field follow-up capabilities\nand usefulness for constraining properties of kilonovae from NSBH mergers at\ndistances of $\\sim 200$ Mpc."
      ]
    }
  },
  {
    "id":2411.05236,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT",
    "start_abstract":"Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",
    "start_categories":[
      "cs.SY",
      "eess.SP"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
      ],
      "abstract":[
        "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of\n  RNA Graph Motifs Using Topological Descriptors",
        "PyMOLfold: Interactive Protein and Ligand Structure Prediction in PyMOL",
        "Silicon is the next frontier in plant synthetic biology",
        "Deep Learning of Proteins with Local and Global Regions of Disorder",
        "Nonsuppressible viremia during HIV-1 therapy meets molecular virology",
        "Engineered Zwitterion-Infused Clay Composites with Antibacterial and\n  Antifungal Efficacy",
        "Artificial Intelligence Approaches for Anti-Addiction Drug Discovery",
        "Remodeling Peptide-MHC-TCR Triad Binding as Sequence Fusion for\n  Immunogenicity Prediction",
        "Advances in RNA secondary structure prediction and RNA modifications:\n  Methods, data, and applications",
        "Inverse problems with experiment-guided AlphaFold",
        "DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown\n  Efficiency",
        "RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via\n  Synergistic Flow Matching",
        "Fluorescence Phasor Analysis: Basic Principles and Biophysical\n  Applications",
        "Census of Ly$\\alpha$ Emission from $\\sim 600$ Galaxies at $z=5-14$:\n  Evolution of the Ly$\\alpha$ Luminosity Function and a Late Sharp Cosmic\n  Reionization",
        "Dynamics of disordered quantum systems with two- and three-dimensional\n  tensor networks",
        "Anomalies of the Scholtes regularization for mathematical programs with\n  complementarity constraints",
        "Continuum limit of fourth-order Schr\\\"{o}dinger equations on the lattice",
        "Some examples of affine isometries of Banach spaces arising from 1-D\n  dynamics",
        "Near-Optimal Parameter Tuning of Level-1 QAOA for Ising Models",
        "Quantum State Designs from Minimally Random Quantum Circuits",
        "Pfaffian solution for dark-dark soliton to the coupled complex modified\n  Korteweg-de Vries equation",
        "The Change of Variable Formula Integrals, do they have equal value?",
        "The Category of Atomic Monoids: Universal Constructions and Arithmetic\n  Properties",
        "Twisted oxide membrane interface by local atomic registry design",
        "Heterogeneous Treatment Effect in Time-to-Event Outcomes: Harnessing\n  Censored Data with Recursively Imputed Trees",
        "Comparative Analysis of Perturbed $f(R)$ Gravity and Perturbed Rastall\n  Gravity Models in Describing Cosmic Evolution from Early to Late Universe\n  Relative to the $\\Lambda$CDM Model",
        "The outflow impacts on the size of the narrow-line region among type-2\n  AGNs",
        "GECKO Follow-up Observation of the Binary Neutron Star-Black Hole Merger\n  Candidate S230518h"
      ],
      "abstract":[
        "We introduce a computational topology-based approach with unsupervised\nmachine-learning algorithms to estimate the database size and content of\nRNA-like graph topologies. Specifically, we apply graph theory enumeration to\ngenerate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2\nto 9. Among them, only 0.11% graphs correspond to approximately 200,000 known\nRNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping\nmethod. The remaining 99.89% of the dual graphs may be RNA-like or\nnon-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are\nmore likely to be associated with RNA structures, we apply computational\ntopology descriptors using the Persistent Spectral Graphs (PSG) method to\ncharacterize each graph using 19 PSG-based features and use clustering\nalgorithms that partition all possible dual graphs into two clusters, RNA-like\ncluster and non-RNA-like cluster. The distance of each dual graph to the center\nof the RNA-like cluster represents the likelihood of it belonging to RNA\nstructures. From validation, our PSG-based RNA-like cluster includes 97.3% of\nthe 121 known RNA dual graphs, suggesting good performance. Furthermore,\n46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,\nwe observe that all the top 15 RNA-like dual graphs can be separated into\nmultiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to\nhave any subgraphs. Moreover, a significant topological difference between top\nRNA-like and non-RNA-like graphs is evident when comparing their topological\nfeatures. These findings provide valuable insights into the size of the RNA\nmotif universe and RNA design strategies, offering a novel framework for\npredicting RNA graph topologies and guiding the discovery of novel RNA motifs,\nperhaps anti-viral therapeutics by subgraph assembly.",
        "PyMOLfold is a flexible and open-source plugin designed to seamlessly\nintegrate AI-based protein structure prediction and visualization within the\nwidely used PyMOL molecular graphics system. By leveraging state-of-the-art\nprotein folding models such as ESM3, Boltz-1, and Chai-1, PyMOLfold allows\nresearchers to directly predict protein tertiary structures from amino acid\nsequences without requiring external tools or complex workflows. Furthermore,\nwith certain models, users can provide a SMILES string of a ligand and have the\nsmall molecule placed in the protein structure. This unique capability bridges\nthe gap between computational folding and structural visualization, enabling\nusers to input a primary sequence, perform a folding prediction, and\nimmediately explore the resulting 3D structure within the same intuitive\nplatform.",
        "Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field.",
        "Although machine learning has transformed protein structure prediction of\nfolded protein ground states with remarkable accuracy, intrinsically disordered\nproteins and regions (IDPs\/IDRs) are defined by diverse and dynamical\nstructural ensembles that are predicted with low confidence by algorithms such\nas AlphaFold. We present a new machine learning method, IDPForge (Intrinsically\nDisordered Protein, FOlded and disordered Region GEnerator), that exploits a\ntransformer protein language diffusion model to create all-atom IDP ensembles\nand IDR disordered ensembles that maintains the folded domains. IDPForge does\nnot require sequence-specific training, back transformations from\ncoarse-grained representations, nor ensemble reweighting, as in general the\ncreated IDP\/IDR conformational ensembles show good agreement with solution\nexperimental data, and options for biasing with experimental restraints are\nprovided if desired. We envision that IDPForge with these diverse capabilities\nwill facilitate integrative and structural studies for proteins that contain\nintrinsic disorder.",
        "HIV-1 replication can be suppressed with antiretroviral therapy (ART), but\nindividuals who stop taking ART soon become viremic again. Some people\nexperience extended times of detectable viremia despite optimal adherence to\nART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of\nnonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded\nT cells harboring HIV-1 proviruses with small deletions or mutations in the\n5'-leader, the UTR that includes the major splice donor site of viral RNA.\nThese mutations altered viral RNA-splicing efficiency and RNA dimerization and\npackaging, yet still allowed production of detectable levels of noninfectious\nvirus particles. These particles lacked the HIV-1 Env surface protein required\nfor cell entry and failed to form the mature capsid cone required for\ninfectivity. These studies improve our understanding of NSV and the regulation\nof viral functions in the 5'-leader with implications for rationalized care in\nindividuals with NSV.",
        "Microbes and pathogens play a detrimental role in healing wounds, causing\ninfections like impetigo through bodily fluids and skin and entering the\nbloodstream through the wounds, thereby hindering the healing process and\ntissue regeneration. Clay, known for its long history of natural therapeutic\nuse, has emerged as one of the most promising candidates for biomedical\napplications due to its non-toxic nature, porosity, high surface area,\nubiquity, and excellent cation exchange capacity. This study demonstrates an\ninnovative approach to engineering an organo-functionalized,\ninfection-resistant, easy-to-use bandage material from clay, an environmentally\nbenign and sustainable material. The hybrid membranes have been developed using\nclays, zwitterions, silver ions, and terbinafine hydrochloride (TBH) to impart\nantibacterial and antifungal efficacy. A critical aspect of this study is\nembedding organic molecules and metal ions with the clays and releasing them to\nresist the growth and kill the pathogens. The antimicrobial efficacy of the\nmembranes has been tested using a zone of inhibition study against the most\ncommon microbes in skin wounds, viz. S. aureus, E. coli, and C. albicans.\nResults from our studies not only demonstrate the potential of these hybrid\nclay membranes as a cost-effective, scalable, and effective solution for\ntreating microbial infections but also instill newer avenues for point-of-care\nwound-healing treatments, offering hope for improved patient outcomes.",
        "Drug addiction is a complex and pervasive global challenge that continues to\npose significant public health concerns. Traditional approaches to\nanti-addiction drug discovery have struggled to deliver effective therapeutics,\nfacing high attrition rates, long development timelines, and inefficiencies in\nprocessing large-scale data. Artificial intelligence (AI) has emerged as a\ntransformative solution to address these issues. Using advanced algorithms, AI\nis revolutionizing drug discovery by enhancing the speed and precision of key\nprocesses. This review explores the transformative role of AI in the pipeline\nfor anti-addiction drug discovery, including data collection, target\nidentification, and compound optimization. By highlighting the potential of AI\nto overcome traditional barriers, this review systematically examines how AI\naddresses critical gaps in anti-addiction research, emphasizing its potential\nto revolutionize drug discovery and development, overcome challenges, and\nadvance more effective therapeutic strategies.",
        "The complex nature of tripartite peptide-MHC-TCR interactions is a critical\nyet underexplored area in immunogenicity prediction. Traditional studies on\nTCR-antigen binding have not fully addressed the complex dependencies in triad\nbinding. In this paper, we propose new modeling approaches for these tripartite\ninteractions, utilizing sequence information from MHCs, peptides, and TCRs. Our\nmethods adhere to native sequence forms and align with biological processes to\nenhance prediction accuracy. By incorporating representation learning\ntechniques, we introduce a fusion mechanism to integrate the three sequences\neffectively. Empirical experiments show that our models outperform traditional\nmethods, achieving a 2.8 to 13.3 percent improvement in prediction accuracy\nacross existing benchmarks. We further validate our approach with extensive\nablation studies, demonstrating the effectiveness of the proposed model\ncomponents. The model implementation, code, and supplementary materials,\nincluding a manuscript with colored hyperlinks and a technical appendix for\ndigital viewing, will be open-sourced upon publication.",
        "Due to the hierarchical organization of RNA structures and their pivotal\nroles in fulfilling RNA functions, the formation of RNA secondary structure\ncritically influences many biological processes and has thus been a crucial\nresearch topic. This review sets out to explore the computational prediction of\nRNA secondary structure and its connections to RNA modifications, which have\nemerged as an active domain in recent years. We first examine the progression\nof RNA secondary structure prediction methodology, focusing on a set of\nrepresentative works categorized into thermodynamic, comparative, machine\nlearning, and hybrid approaches. Next, we survey the advances in RNA\nmodifications and computational methods for identifying RNA modifications,\nfocusing on the prominent modification types. Subsequently, we highlight the\ninterplay between RNA modifications and secondary structures, emphasizing how\nmodifications such as m6A dynamically affect RNA folding and vice versa. In\naddition, we also review relevant data sources and provide a discussion of\ncurrent challenges and opportunities in the field. Ultimately, we hope our\nreview will be able to serve as a cornerstone to aid in the development of\ninnovative methods for this emerging topic and foster therapeutic applications\nin the future.",
        "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
        "Background: Small interfering RNA (siRNA) is a promising therapeutic agent\ndue to its ability to silence disease-related genes via RNA interference. While\ntraditional machine learning and early deep learning methods have made progress\nin predicting siRNA efficacy, there remains significant room for improvement.\nAdvanced deep learning techniques can enhance prediction accuracy, reducing the\nreliance on extensive wet-lab experiments and accelerating the identification\nof effective siRNA sequences. This approach also provides deeper insights into\nthe mechanisms of siRNA efficacy, facilitating more targeted and efficient\ntherapeutic strategies.\n  Methods: We introduce DeepSilencer, an innovative deep learning model\ndesigned to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced\nneural network architectures to capture the complex features of siRNA\nsequences. Our key contributions include a specially designed deep learning\nmodel, an innovative online data sampling method, and an improved loss function\ntailored for siRNA prediction. These enhancements collectively boost the\nmodel's prediction accuracy and robustness.\n  Results: Extensive evaluations on multiple test sets demonstrate that\nDeepSilencer achieves state-of-the-art performance using only siRNA sequences\nand basic physicochemical properties. Our model surpasses several other methods\nand shows superior predictive performance, particularly when incorporating\nthermodynamic parameters.\n  Conclusion: The advancements in data sampling, model design, and loss\nfunction significantly enhance the predictive capabilities of DeepSilencer.\nThese improvements underscore its potential to advance RNAi therapeutic design\nand development, offering a powerful tool for researchers and clinicians.",
        "Ribonucleic acid (RNA) binds to molecules to achieve specific biological\nfunctions. While generative models are advancing biomolecule design, existing\nmethods for designing RNA that target specific ligands face limitations in\ncapturing RNA's conformational flexibility, ensuring structural validity, and\novercoming data scarcity. To address these challenges, we introduce RiboFlow, a\nsynergistic flow matching model to co-design RNA structures and sequences based\non target molecules. By integrating RNA backbone frames, torsion angles, and\nsequence features in an unified architecture, RiboFlow explicitly models RNA's\ndynamic conformations while enforcing sequence-structure consistency to improve\nvalidity. Additionally, we curate RiboBind, a large-scale dataset of\nRNA-molecule interactions, to resolve the scarcity of high-quality structural\ndata. Extensive experiments reveal that RiboFlow not only outperforms\nstate-of-the-art RNA design methods by a large margin but also showcases\ncontrollable capabilities for achieving high binding affinity to target\nligands. Our work bridges critical gaps in controllable RNA design, offering a\nframework for structure-aware, data-efficient generation.",
        "Fluorescence is one of the most widely used techniques in biological\nsciences. Its exceptional sensitivity and versatility make it a tool of first\nchoice for quantitative studies in biophysics. The concept of phasors,\noriginally introduced by Charles Steinmetz in the late 19th century for\nanalyzing alternating current circuits, has since found applications across\ndiverse disciplines, including fluorescence spectroscopy. The main idea behind\nfluorescence phasors was posited by Gregorio Weber in 1981. By analyzing the\ncomplementary nature of pulse and phase fluorometry data, he shows that two\nmagnitudes -- denoted as $G$ and $S$ -- derived from the frequency-domain\nfluorescence measurements correspond to the real and imaginary part of the\nFourier transform of the fluorescence intensity in the time domain. This review\nprovides a historical perspective on how the concept of phasors originates and\nhow it integrates into fluorescence spectroscopy. We discuss their fundamental\nalgebraic properties, which enable intuitive model-free analysis of\nfluorescence data despite the complexity of the underlying phenomena. Some\napplications in biophysics illustrate the power of this approach in studying\ndiverse phenomena, including protein folding, protein interactions, phase\ntransitions in lipid mixtures and formation of high-order structures in nucleic\nacids.",
        "We present the statistical properties of Ly$\\alpha$ emission in 586 galaxies\nat $z=4.5-14.2$, observed by multiple JWST\/NIRSpec spectroscopy projects,\nincluding JADES, GLASS, CEERS, and GO\/DDT programs. We obtain Ly$\\alpha$\nequivalent width (EW), Ly$\\alpha$ escape fraction, and ionizing photon\nproduction efficiency measurements or upper limits for these galaxies, and\nconfirm that the Ly$\\alpha$ emitting galaxy fraction decreases towards higher\nredshifts. We derive Ly$\\alpha$ luminosity functions from $z\\sim 5$ to $z\\sim\n10-14$ with the observed Ly$\\alpha$ EW distributions and galaxy UV luminosity\nfunctions, and find a $\\sim3$ dex decrease in number density at\n$L_\\mathrm{Ly\\alpha}=10^{42}-10^{43}$ erg s$^{-1}$ over the redshift range.\nNotably, this study presents the first constraints on the Ly$\\alpha$ luminosity\nfunction at $z\\sim 8-14$. We obtain the neutral hydrogen fractions of\n$x_\\mathrm{HI}=0.17_{-0.16}^{+0.23}$, $0.63_{-0.28}^{+0.18}$,\n$0.79_{-0.21}^{+0.13}$, and $0.88_{-0.13}^{+0.11}$ at $z\\sim6$, $7$, $8-9$, and\n$10-14$, respectively, via comparisons of the reionization models developed by\nsemi-numerical simulations with 21cmFAST explaining the observations of\nLy$\\alpha$, UV continuum, and Planck electron optical depth. The high\n$x_\\mathrm{HI}$ values over $z\\sim 7-14$ suggest a late and sharp reionization,\nwith the primary reionization process occurring at $z\\sim 6-7$. Such a late and\nsharp reionization is not easily explained by either a clumpy inter-galactic\nmedium or sources of reionization in a classical faint-galaxy or a\nbright-galaxy\/AGN scenario, unless a very high escape fraction or AGN duty\ncycle is assumed at $z\\sim 6-7$.",
        "Quantum spin glasses form a good testbed for studying the performance of\nvarious quantum annealing and optimization algorithms. In this work we show how\ntwo- and three-dimensional tensor networks can accurately and efficiently\nsimulate the quantum annealing dynamics of Ising spin glasses on a range of\nlattices. Such dynamics were recently simulated using D-Wave's Advantage$2$\nsystem [A. D. King et al, Science, 10.1126\/science.ado6285 (2025)] and,\nfollowing extensive comparisons to existing numerical methods, claimed to be\nbeyond the reach of classical computation. Here we show that by evolving\nlattice-specific tensor networks with simple belief propagation to keep up with\nthe entanglement generated during the time evolution and then extracting\nexpectation values with more sophisticated variants of belief propagation,\nstate-of-the-art accuracies can be reached with modest computational resources.\nWe exploit the scalability of our simulations and simulate a system of over\n$300$ qubits, allowing us to verify the universal physics present and extract a\nvalue for the associated Kibble-Zurek exponent which agrees with recent values\nobtained in literature. Our results demonstrate that tensor networks are a\nviable approach for simulating large scale quantum dynamics in two and three\ndimensions on classical computers, and algorithmic advancements are expected to\nexpand their applicability going forward.",
        "For mathematical programs with complementarity constraints (MPCC), we refine\nthe convergence analysis of the Scholtes regularization. Our goal is to relate\nnondegenerate C-stationary points of MPCC with nondegenerate Karush-Kuhn-Tucker\npoints of its Scholtes regularization. We detected the following anomalies: (i)\nin a neighborhood of a nondegenerate C-stationary point there could be\ndegenerate Karush-Kuhn-Tucker points of the Scholtes regularization; (ii) even\nif nondegenerate, they might be locally non-unique; (iii) if nevertheless\nunique, their quadratic index potentially differs from the C-index of the\nC-stationary point under consideration. Thus, a change of the topological type\nfor Karush-Kuhn-Tucker points of the Scholtes regularization is possible. In\nparticular, a nondegenerate minimizer of MPCC might be approximated by saddle\npoints. In order to bypass the mentioned anomalies, an additional generic\ncondition for nondegenerate C-stationary points of MPCC is identified. Then, we\nuniquely trace nondegenerate Karush-Kuhn-Tucker points of the Scholtes\nregularization and successively maintain their topological type.",
        "In this paper, we consider the discrete fourth-order Schr\\\"{o}dinger equation\non the lattice $h\\mathbb{Z}^2$. Uniform Strichartz estimates are established by\nanalyzing frequency localized oscillatory integrals with the method of\nstationary phase and applying Littlewood-Paley inequalities. As an application,\nwe obtain the precise rate of $L^2$ convergence from the solutions of discrete\nsemilinear equations to those of the corresponding equations on the Euclidean\nplane $\\mathbb{R}^2$ in the contimuum limit $h \\rightarrow 0$.",
        "We provide a large family of examples of affine isometries of the Banach\nspaces $C^0 (S^1)$, $L^1 (S^1)$ and $L^2 (S^1 \\times S^1)$ that are\nfixed-point-free despite being recurrent (in particular, they have zero drift).\nThese come from natural cocycles on the group of circle diffeomorphisms, namely\nthe logarithmic, affine and (a variation of the) Schwarzian derivative. Quite\ninterestingly, they arise from diffeomorphisms that are generic in an\nappropriate context. We also show how to promote these examples in order to\nobtain families of commuting isometries satisfying the same properties.",
        "The Quantum Approximate Optimisation Algorithm (QAOA) is a hybrid\nquantum-classical algorithm for solving combinatorial optimisation problems.\nQAOA encodes solutions into the ground state of a Hamiltonian, approximated by\na $p$-level parameterised quantum circuit composed of problem and mixer\nHamiltonians, with parameters optimised classically. While deeper QAOA circuits\ncan offer greater accuracy, practical applications are constrained by complex\nparameter optimisation and physical limitations such as gate noise, restricted\nqubit connectivity, and state-preparation-and-measurement errors, limiting\nimplementations to shallow depths. This work focuses on QAOA$_1$ (QAOA at\n$p=1$) for QUBO problems, represented as Ising models. Despite QAOA$_1$ having\nonly two parameters, $(\\gamma, \\beta)$, we show that their optimisation is\nchallenging due to a highly oscillatory landscape, with oscillation rates\nincreasing with the problem size, density, and weight. This behaviour\nnecessitates high-resolution grid searches to avoid distortion of cost\nlandscapes that may result in inaccurate minima. We propose an efficient\noptimisation strategy that reduces the two-dimensional $(\\gamma, \\beta)$ search\nto a one-dimensional search over $\\gamma$, with $\\beta^*$ computed\nanalytically. We establish the maximum permissible sampling period required to\naccurately map the $\\gamma$ landscape and provide an algorithm to estimate the\noptimal parameters in polynomial time. Furthermore, we rigorously prove that\nfor regular graphs on average, the globally optimal $\\gamma^* \\in \\mathbb{R}^+$\nvalues are concentrated very close to zero and coincide with the first local\noptimum, enabling gradient descent to replace exhaustive line searches. This\napproach is validated using Recursive QAOA (RQAOA), where it consistently\noutperforms both coarsely optimised RQAOA and semidefinite programs across all\ntested QUBO instances.",
        "Random many-body states are both a useful tool to model certain physical\nsystems and an important asset for quantum computation. Realising them,\nhowever, generally requires an exponential (in system size) amount of\nresources. Recent research has presented a way out by showing that one can\ngenerate random states, or more precisely a controlled approximation of them,\nby applying a quantum circuit built in terms of few-body unitary gates. Most of\nthis research, however, has been focussed on the case of quantum circuits\ncomposed by completely random unitary gates. Here we consider what happens for\ncircuits that, instead, involve a minimal degree of randomness. Specifically,\nwe concentrate on two different settings: (a) brickwork quantum circuits with a\nsingle one-qudit random matrix at a boundary; (b) brickwork quantum circuits\nwith fixed interactions but random one-qudit gates everywhere. We show that,\nfor any given initial state, (a) and (b) produce a distribution of states\napproaching the Haar distribution in the limit of large circuit depth. More\nprecisely, we show that the moments of the distribution produced by our\ncircuits can approximate the ones of the Haar distribution in a depth\nproportional to the system size. Interestingly we find that in both Cases (a)\nand (b) the relaxation to the Haar distribution occurs in two steps - this is\nin contrast with what happens in fully random circuits. Moreover, we show that\nchoosing appropriately the fixed interactions, for example taking the local\ngate to be a dual-unitary gate with high enough entangling power, minimally\nrandom circuits produce a Haar random distribution more rapidly than fully\nrandom circuits. In particular, dual-unitary circuits with maximal entangling\npower - i.e. perfect tensors - appear to provide the optimal quantum state\ndesign preparation for any design number.",
        "In this paper, we study coupled complex modified Korteweg-de Vries (ccmKdV)\nequation by combining the Hirota's method and the Kadomtsev-Petviashvili (KP)\nreduction method. First, we show that the bilinear form of the ccmKdV equation\nunder nonzero boundary condition is linked to the discrete BKP hierarchy\nthrough Miwa transformation. Based on this finding, we construct the dark-dark\nsoliton solution in the pfaffian form. The dynamical behaviors for one- and\ntwo-soliton are analyzed and illustrated.",
        "Assuming that the two integrals in the Change of Variable Formula for the\nunidimensional Riemann integral are finite, one can ask if they have equal\nvalue. We give a positive answer to this question. The proof is very easy to\nfollow and to keep in mind. An example is given.",
        "We introduce and investigate the category $\\mathsf{AtoMon}$ of atomic monoids\nand atom-preserving monoid homomorphisms, which is a (non-full) subcategory of\nthe usual category of monoids. In particular, we compute all limits and\ncolimits, showing that $\\mathsf{AtoMon}$ is a complete and cocomplete category.\nWe also address certain arithmetic properties of products and coproducts,\nproviding explicit formulas for some fundamental invariants associated with\nfactorization lengths in atomic monoids.",
        "Interplay of lattice, orbital, and charge degrees of freedom in complex oxide\nmaterials has hosted a plethora of exotic quantum phases and physical\nproperties. Recent advances in synthesis of freestanding complex oxide\nmembranes and twisted heterostructures assembled from membranes provide new\nopportunities for discovery using moir\\'e design with local lattice control. To\nthis end, we designed moir\\'e crystals at the coincidence site lattice\ncondition, providing commensurate structure within the moir\\'e supercell\narising from the multi-atom complex oxide unit cell. We fabricated such twisted\nbilayers from freestanding SrTiO3 membranes and used depth sectioning-based TEM\nmethods to discover ordered charge states at the moir\\'e interface. By\nselectively imaging SrTiO3 atomic planes at different depths through the\nbilayer, we clearly resolved the moir\\'e periodic structure at the twisted\ninterface and found that it exhibits lattice-dependent charge\ndisproportionation in the local atomic registry within the moir\\'e supercell.\nOur density-functional modelling of the twisted oxide interface predicts that\nthese moir\\'e phenomena are accompanied by the emergence of a two-dimensional\nflat band that can drive new electronic phases. Our work provides a novel\nguideline for controlling moir\\'e periodicity in twisted oxides and opens\npathways to exploit the new functionalities via moir\\'e lattice-driven\ncharge-orbital correlation.",
        "Tailoring treatments to individual needs is a central goal in fields such as\nmedicine. A key step toward this goal is estimating Heterogeneous Treatment\nEffects (HTE) - the way treatments impact different subgroups. While crucial,\nHTE estimation is challenging with survival data, where time until an event\n(e.g., death) is key. Existing methods often assume complete observation, an\nassumption violated in survival data due to right-censoring, leading to bias\nand inefficiency. Cui et al. (2023) proposed a doubly-robust method for HTE\nestimation in survival data under no hidden confounders, combining a causal\nsurvival forest with an augmented inverse-censoring weighting estimator.\nHowever, we find it struggles under heavy censoring, which is common in\nrare-outcome problems such as Amyotrophic lateral sclerosis (ALS). Moreover,\nmost current methods cannot handle instrumental variables, which are a crucial\ntool in the causal inference arsenal. We introduce Multiple Imputation for\nSurvival Treatment Response (MISTR), a novel, general, and non-parametric\nmethod for estimating HTE in survival data. MISTR uses recursively imputed\nsurvival trees to handle censoring without directly modeling the censoring\nmechanism. Through extensive simulations and analysis of two real-world\ndatasets-the AIDS Clinical Trials Group Protocol 175 and the Illinois\nunemployment dataset we show that MISTR outperforms prior methods under heavy\ncensoring in the no-hidden-confounders setting, and extends to the instrumental\nvariable setting. To our knowledge, MISTR is the first non-parametric approach\nfor HTE estimation with unobserved confounders via instrumental variables.",
        "This study conducts a meticulous examination of the cosmological implications\ninherent in Rastall gravity and $f(R)$ gravity models, assessing their efficacy\nacross distinct cosmic epochs, from early universe structure formation to\nlate-time acceleration. In the initial stages, both models exhibit commendable\ncompatibility with observed features of structure formation, aligning with the\nestablished $\\Lambda$CDM model. The derived Jeans' wavenumbers for each model\nsupport their viability. However, as the cosmic timeline progresses into the\nlate universe, a discernible disparity surfaces. Utilizing the Markov Chain\nMonte Carlo method, we reconstruct the deceleration parameter $(q)$ and\nidentify Deceleration - Acceleration redshift transition values. For $f(R)$\ngravity, our results align closely with previous studies, emphasizing its\nsuperior ability to elucidate the recent cosmic acceleration. In contrast,\nRastall gravity exhibits distinct redshift transition values. Our rigorous\nanalysis underscores the prowess of $f(R)$ gravity in capturing the observed\ncosmic acceleration, positioning it as a compelling alternative to the\nconventional $\\Lambda$CDM model. The discernible shifts observed in the peaks\nof the CMB power spectrum and evolution of deceleration parameter (q) for both\n$f(R)$ gravity and Rastall gravity models in the Early and Late universe, in\nrelation to the $\\Lambda $CDM model, provide compelling evidence supporting the\nproposition that these alternative gravity models can account for the\nanisotropy of the universe without invoking the need for dark energy.",
        "We present the study of the gas kinematics in narrow-line regions (NLRs) of\n2,009 type-2 AGNs at $z<0.34$. We construct the [O III]$\\lambda$5007\nemission-line images using publicly available broadband images from the Sloan\nDigital Sky Survey (SDSS). The [O III] emission area of the samples, measured\ndown to $1.7\\times10^{-15}$ erg\/s\/cm$^2$\/arcsec$^2$, ranges from 3.7 kpc$^2$ up\nto 224 kpc$^2$. With our broadband technique, we found the strong correlation\nbetween [O III] area and AGN luminosity inferred from the [O III] luminosity\nand the mid-infrared luminosity at the rest-frame $15\\mu$m. The isophotal\nthreshold used to determine the [O III] area affects the correlation strength\nin that the brighter isophote yields the stronger correlation between the [O\nIII] area and AGN luminosity. The presence of gas outflow is examined by the\nratio of the [O III] velocity dispersion to the stellar velocity dispersion\n($\\sigma_{\\rm [O\\,III]}\/\\sigma_\\star > 1.4$) using the SDSS spectra. At the\ngiven luminosity, the objects with and without outflows exhibit the same\nextension of the [O III] emission. Their correlation between the [O III] area\nand luminosity is almost identical. It is suggested that the size of NLRs is\nnot affected by outflow mechanisms but rather by photoionization from the\ncentral AGNS.",
        "The gravitational wave (GW) event S230518h is a potential binary neutron\nstar-black hole merger (NSBH) event that was detected during engineering run 15\n(ER15), which served as the commissioning period before the LIGO-Virgo-KAGRA\n(LVK) O4a observing run. Despite its low probability of producing detectable\nelectromagnetic emissions, we performed extensive follow-up observations of\nthis event using the GECKO telescopes in the southern hemisphere. Our\nobservation covered 61.7\\% of the 90\\% credible region, a $\\rm 284\\:deg^2$ area\naccessible from the southern hemisphere, reaching a median limiting magnitude\nof $R=21.6$ mag. In these images, we conducted a systematic search for an\noptical counterpart of this event by combining a CNN-based classifier and human\nverification. We identified 128 transient candidates, but no significant\noptical counterpart was found that could have caused the GW signal.\nFurthermore, we provide feasible KN properties that are consistent with the\nupper limits of observation. Although no optical counterpart was found, our\nresult demonstrates both GECKO's efficient wide-field follow-up capabilities\nand usefulness for constraining properties of kilonovae from NSBH mergers at\ndistances of $\\sim 200$ Mpc."
      ]
    }
  },
  {
    "id":2411.02815,
    "research_type":"applied",
    "start_id":"b36",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
      ],
      "abstract":[
        "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "The three-dimensional impulse-response model: Modeling the training\n  process in accordance with energy system-specific adaptation",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Engineered Zwitterion-Infused Clay Composites with Antibacterial and\n  Antifungal Efficacy",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "A note on Dirichlet-like series attached to polynomials",
        "Quasiconformal Maps between Bowditch Boundaries of Relatively Hyperbolic\n  Groups",
        "Intrinsic low-temperature magnetic properties on the ultra-clean UTe$_2$\n  with $T_{\\rm c}$ = 2.1 K revealed by $^{125}$Te NMR",
        "A note on Centaur geometry -- probing IR de Sitter spacetime holography",
        "Quasiparticle poisoning of superconducting qubits with active gamma\n  irradiation",
        "Further applications of the Nehari manifold method to functionals in\n  $C^1(X \\setminus \\{0\\})$",
        "Effect of Accelerated Thermal Degradation of Poly(Vinyl Chloride): The\n  Case of Unplasticized PVC",
        "Scalar Field Kantowski--Sachs Solutions in Teleparallel $F(T)$ Gravity",
        "Testing the Homogeneity of Two Proportions for Correlated Bilateral Data\n  via the Clayton Copula",
        "Bounds on Elliptic Sombor and Euler Sombor indices of join and corona\n  product of graphs",
        "Equivalence between top-down and bottom-up holographic approaches",
        "Gluing invariants of Donaldson--Thomas type -- Part II: Matrix\n  factorizations",
        "Dynamics of a Family of Rational Operators of Arbitrary Degree",
        "Analytical control of the exchange interaction in periodically driven\n  Mott insulators",
        "The late-time heating Green's function and improvements to distortion\n  frequency hierarchy treatment"
      ],
      "abstract":[
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "Athletic training is characterized by physiological systems responding to\nrepeated exercise-induced stress, resulting in gradual alterations in the\nfunctional properties of these systems. The adaptive response leading to\nimproved performance follows a remarkably predictable pattern that may be\ndescribed by a systems model provided that training load can be accurately\nquantified and that the constants defining the training-performance\nrelationship are known. While various impulse-response models have been\nproposed, they are inherently limited in reducing training stress (the impulse)\ninto a single metric, assuming that the adaptive responses are independent of\nthe type of training performed. This is despite ample evidence of markedly\ndiverse acute and chronic responses to exercise of different intensities and\ndurations. Herein, we propose an alternative, three-dimensional\nimpulse-response model that uses three training load metrics as inputs and\nthree performance metrics as outputs. These metrics, represented by a\nthree-parameter critical power model, reflect the stress imposed on each of the\nthree energy systems: the alactic (phosphocreatine\/immediate) system; the\nlactic (glycolytic) system; and the aerobic (oxidative) system. The purpose of\nthis article is to outline the scientific rationale and the practical\nimplementation of the three-dimensional impulse-response model.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Microbes and pathogens play a detrimental role in healing wounds, causing\ninfections like impetigo through bodily fluids and skin and entering the\nbloodstream through the wounds, thereby hindering the healing process and\ntissue regeneration. Clay, known for its long history of natural therapeutic\nuse, has emerged as one of the most promising candidates for biomedical\napplications due to its non-toxic nature, porosity, high surface area,\nubiquity, and excellent cation exchange capacity. This study demonstrates an\ninnovative approach to engineering an organo-functionalized,\ninfection-resistant, easy-to-use bandage material from clay, an environmentally\nbenign and sustainable material. The hybrid membranes have been developed using\nclays, zwitterions, silver ions, and terbinafine hydrochloride (TBH) to impart\nantibacterial and antifungal efficacy. A critical aspect of this study is\nembedding organic molecules and metal ions with the clays and releasing them to\nresist the growth and kill the pathogens. The antimicrobial efficacy of the\nmembranes has been tested using a zone of inhibition study against the most\ncommon microbes in skin wounds, viz. S. aureus, E. coli, and C. albicans.\nResults from our studies not only demonstrate the potential of these hybrid\nclay membranes as a cost-effective, scalable, and effective solution for\ntreating microbial infections but also instill newer avenues for point-of-care\nwound-healing treatments, offering hope for improved patient outcomes.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Some Dirichlet-like functions, attached to a pair (periodic function,\npolynomial) are introduced and studied. These functions generalize the standard\nDirichlet L-functions of Dirichlet characters. They have similar properties,\nbeing holomorphic on thefull complex plane and having simple values on negative\nintegers.",
        "Classifying groups up to quasi-isometry is a fundamental problem in geometric\ngroup theory. In the context of hyperbolic and relatively hyperbolic groups,\none of the key invariants in this classification is the boundary at infinity.\nF. Paulin proved that two hyperbolic groups are quasi-isometric if and only if\ntheir Gromov boundaries are quasiconformally equivalent. In this article, we\nextend Paulin's result to relatively hyperbolic groups and their Bowditch\nboundaries.\n  A notion of quasiconformal map preserving the shadows of horoballs relative\nto a point at the Bowditch boundary is defined and we have shown that every\ncoarsely cusp-preserving quasi-isometry between two relatively hyperbolic\ngroups induces a shadow-preserving quasiconformal map between their Bowditch\nboundaries. Conversely, we have shown that if the Bowditch boundaries of two\nrelatively hyperbolic groups are quasiconformally equivalent and the\nquasiconformal map coarsely preserves the shadows of horoballs relative to each\nboundary point, then the quasiconformal map induces a coarsely cusp-preserving\nquasi-isometry between those groups.",
        "To investigate the intrinsic magnetic properties of UTe$_2$, we performed\n$^{125}$Te-NMR measurements on the ultra-clean single-crystalline UTe$_2$ with\nsuperconducting transition temperature $T_{\\rm c}$ = 2.1~K and compared the\nresults with those of the $T_{\\rm c}$ = 1.6~K sample. The broadening of the\nlinewidth of the NMR spectrum in the $a$-axis magnetic field and the\nlow-temperature magnetic fluctuations observed in the 1.6~K sample are\nsuppressed in the ultra-clean sample, indicating that such magnetic properties\noriginate from a tiny amount of U deficiency. The present results suggest that\nthe magnetic properties in UTe$_2$ are sensitive to the U deficiency. We also\nobserved a peculiar angular dependence of the NMR quantities due to large\nmagnetic anisotropy with the $a$-axis as the magnetic easy axis.",
        "We explore a Centaur geometry in JT gravity, which is an asymptotically AdS\nspacetime but in the IR admits a dS bubble with another AdS geometry in the\ndeep IR. Thus, this geometry admits a holographic dual in the sense that it is\nasymptotically AdS. In an attempt to understand this geometry, we calculate the\ndensity of states of the putative boundary dual for such mixed geometries by\nevaluating the on-shell action. We compute the density of states analytically\nin the classical limit. The resultant density of states suggest that the\ndegrees of freedom in the IR are reduced in such a putative boundary theory due\nto the IR modification corresponding to the dS bubble.",
        "When a high-energy particle, such as a $\\gamma$-ray or muon, impacts the\nsubstrate of a superconducting qubit chip, large numbers of electron-hole pairs\nand phonons are created. The ensuing dynamics of the electrons and holes\nchanges the local offset-charge environment for qubits near the impact site.\nThe phonons that are produced have energy above the superconducting gap in the\nfilms that compose the qubits, leading to quasiparticle excitations above the\nsuperconducting ground state when the phonons impinge on the qubit electrodes.\nAn elevated density of quasiparticles degrades qubit coherence, leading to\nerrors in qubit arrays. Because these pair-breaking phonons spread throughout\nmuch of the chip, the errors can be correlated across a large portion of the\narray, posing a significant challenge for quantum error correction. In order to\nstudy the dynamics of $\\gamma$-ray impacts on superconducting qubit arrays, we\nuse a $\\gamma$-ray source outside the dilution refrigerator to controllably\nirradiate our devices. By using charge-sensitive transmon qubits, we can\nmeasure both the offset-charge shifts and quasiparticle poisoning due to the\n$\\gamma$ irradiation at different doses. We study correlations between\noffset-charge shifts and quasiparticle poisoning for different qubits in the\narray and compare this with numerical modeling of charge and phonon dynamics\nfollowing a $\\gamma$-ray impact. We thus characterize the poisoning footprint\nof these impacts and quantify the performance of structures for mitigating\nphonon-mediated quasiparticle poisoning.",
        "We proceed with the study of the Nehari manifold method for functionals in\n$C^1(X \\setminus \\{0\\})$, where $X$ is a Banach space. We deal now with\nfunctionals whose fibering maps have two critical points (a minimiser followed\nby a maximiser). Under some additional conditions we show that the Nehari\nmanifold method provides us with the ground state level and two sequences of\ncritical values for these functionals. These results are applied to the class\nof {\\it prescribed energy problems} as well as to the concave-convex problem\nfor the {\\it affine} $p$-Laplacian operator.",
        "The thermal degradation of unplasticized poly(vinyl chloride), PVC, was\ncomprehensively investigated through the application of spectroscopic\ntechniques, as well as contact angle measurements (CA), dynamic mechanical\nanalysis (DMA), and size-exclusion chromatography (SEC). To study the effect of\nrelative humidity (RH) on the deterioration of unplasticized PVC, two regimes\nof accelerated degradation experiments were selected: low RH (max. 30% RH) and\nhigh RH = 60% levels, which corresponds to usually the highest RH in heritage\ninstitutions equipped with an HVAC system. Nuclear magnetic resonance (NMR) and\ninfrared spectroscopy (FTIR) did not reveal any significant changes in the\nmaterial during its degradation up to 20 weeks at temperatures ranging from\n60{\\deg}C to 80{\\deg}C. Notable changes were observed in the Raman and UV-Vis\nspectra, indicative of the formation of conjugated carbon-carbon double bonds.\nThe formation of polyenes was responsible for the yellowing of samples.\nNotwithstanding, the aforementioned changes did not lead to a notable decline\nin the mechanical properties, as evidenced by DMA and SEC measurements. EPR\nmeasurements demonstrated the formation of 2 radicals at 60{\\deg}C, and in the\nsample degraded at 80{\\deg}C the presence of radicals was evident. This\nindicates that a radical degradation mechanism cannot be excluded even at such\nlow temperatures.",
        "In this paper, we investigate time-dependent Kantowski--Sachs spherically\nsymmetric teleparallel $F(T)$ gravity with a scalar field source. We begin by\nsetting the exact field equations to be solved and solve conservation laws for\npossible scalar field potential, $V\\left(\\phi\\right)$, solutions. Then, we find\nnew non-trivial teleparallel $F(T)$ solutions by using power-law and\nexponential ansatz for each potential case arising from conservation laws, such\nas linear, quadratic, or logarithmic, to name a few. We find a general formula\nallowing us to compute all possible new teleparallel $F(T)$ solutions\napplicable for any scalar field potential and ansatz. Then, we apply this\nformula and find a large number of exact and approximate new teleparallel\n$F(T)$ solutions for several types of cases. Some new $F(T)$ solution classes\nmay be relevant for future cosmological applications, especially concerning\ndark matter, dark energy quintessence, phantom energy leading to the Big Rip\nevent, and quintom models of physical processes.",
        "Handling highly dependent data is crucial in clinical trials, particularly in\nfields related to ophthalmology. Incorrectly specifying the dependency\nstructure can lead to biased inferences. Traditionally, models rely on three\nfixed dependence structures, which lack flexibility and interpretation. In this\narticle, we propose a framework using a more general model -- copulas -- to\nbetter account for dependency. We assess the performance of three different\ntest statistics within the Clayton copula setting to demonstrate the\nframework's feasibility. Simulation results indicate that this method controls\ntype I error rates and achieves reasonable power, providing a solid benchmark\nfor future research and broader applications. Additionally, we present analyses\nof two real-world datasets as case studies.",
        "The Elliptic Somber and Euler Somber indices are newly defined topological\nindices based on the Somber index. Our paper presents calculations of the upper\nand lower bounds of these indices for the join and corona product of arbitrary\ngraphs. Furthermore, we demonstrate that these bounds are attained when both\ngraphs are regular.",
        "This work raises the question of whether finding an equivalent bottom-up\ndescription to a given top-down one is possible. We consider the vector meson\nspectrum derived in the D3\/D7 system to answer this question. Using WKB\nanalysis, we reconstruct a bottom-up confining potential that resembles the\ngeometric structure of the so-called hardwall model. We compute some properties\nfor this bottom-up model, including the thermal deconfinement phase transition,\nthe $\\rho$ radial Regge trajectory, and the configurational entropy.",
        "This paper is a follow-up to arXiv:2407.08471. Let $X$ be a a $(-1)$-shifted\nsymplectic derived Deligne--Mumford stack. Thanks to the Darboux lemma of\nBrav--Bussi--Joyce, $X$ is locally modeled by derived critical loci of a\nfunction $f$ on a smooth scheme $U$. In this paper we study the gluing of the\nlocally defined $2$-periodic (big) dg-categories of matrix factorizations\n$MF^\\infty(U,f)$. We show that these come canonically equipped with a structure\nof a $2$-periodic crystal of categories (\\ie an action of the dg-category of\n$2$-periodic $D$-modules on $X$) compatible with a relative Thom--Sebastiani\ntheorem expressing the equivariance under the action of quadratic bundles.\n  As our main theorem we show that the locally defined categories\n$MF^\\infty(U,f)$ can be glued along $X$ as a sheaf of crystals of 2-periodic\ndg-categories ``up to isotopy'', under the prescription of orientation data\ncontrolled by three obstruction classes. This result generalizes the gluing of\nthe Joyce's perverse sheaf of vanishing cycles and partially answers\nconjectures by Kontsevich--Soibelman and Toda in motivic Donaldson--Thomas\ntheory.",
        "In this paper we analyse the dynamics of a family of rational operators\ncoming from a fourth-order family of root-finding algorithms. We first show\nthat it may be convenient to redefine the parameters to prevent redundancies\nand unboundedness of problematic parameters. After reparametrization, we\nobserve that these rational maps belong to a more general family $O_{a,n,k}$ of\ndegree $n+k$ operators, which includes several other families of maps obtained\nfrom other numerical methods. We study the dynamics of $O_{a,n,k}$ and discuss\nfor which parameters $n$ and $k$ these operators would be suitable from the\nnumerical point of view.",
        "The manipulation of electronic structure through periodic electric fields\nenables the reversible control of effective interactions in extended\nantiferromagnetic Mott insulators on ultrafast timescales. A careful analytical\nexamination of the modulated effective interactions is conducted, accurately\ncharacterising it through the use of exact summation formulas and Bessel\nfunctions. As a result, time reversals are analytically determined in terms of\nBessel zeroes. We discuss the half-filled Hubbard model, as well as\nmulti-orbital models, various characteristics of the Kitaev-Heisenberg model,\nand the emergence of chiral spin terms.",
        "Early energy injection leaves an imprint on the observed blackbody spectrum\nof the CMB, allowing us to study the thermal history of the Universe. For small\nenergy release, the distortion can be efficiently computed using the\nquasi-exact Green's function method. For pre-recombination injections, the\nGreen's function has already been studied previously. Here we reconsider the\npre- and post-recombination periods, showcasing both the spectral distortion\nintensity and the relative temperature difference, which encrypt precious\ninformation about physical processes such as free-free interactions and thermal\ndecoupling. We present the associated distortion visibility function,\ninvestigating the impact of various physical effects. We then study\nimprovements to the so-called frequency hierarchy (FH) treatment, a method that\nwas developed for the modelling of anisotropic distortions, which like the\naverage distortion signals encode valuable cosmological information.\nSpecifically, the FH treatment has shortcomings even in the $\\mu$ era, that in\nprinciple should be easy to overcome. In this paper, we introduce a new\napproach to reduce the mismatch, concluding with a redefinition of the $\\mu$\nspectral shape using CosmoTherm. This solution takes into account double\nCompton and Bremsstrahlung effects in the low tail, which can be included in\nthe FH. This opens the path towards a refined modeling of spectral distortion\nanisotropies."
      ]
    }
  },
  {
    "id":2411.02815,
    "research_type":"applied",
    "start_id":"b33",
    "start_title":"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network",
    "start_abstract":"We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.",
    "start_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
      ],
      "abstract":[
        "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "The three-dimensional impulse-response model: Modeling the training\n  process in accordance with energy system-specific adaptation",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Engineered Zwitterion-Infused Clay Composites with Antibacterial and\n  Antifungal Efficacy",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "A note on Dirichlet-like series attached to polynomials",
        "Quasiconformal Maps between Bowditch Boundaries of Relatively Hyperbolic\n  Groups",
        "Intrinsic low-temperature magnetic properties on the ultra-clean UTe$_2$\n  with $T_{\\rm c}$ = 2.1 K revealed by $^{125}$Te NMR",
        "A note on Centaur geometry -- probing IR de Sitter spacetime holography",
        "Quasiparticle poisoning of superconducting qubits with active gamma\n  irradiation",
        "Further applications of the Nehari manifold method to functionals in\n  $C^1(X \\setminus \\{0\\})$",
        "Effect of Accelerated Thermal Degradation of Poly(Vinyl Chloride): The\n  Case of Unplasticized PVC",
        "Scalar Field Kantowski--Sachs Solutions in Teleparallel $F(T)$ Gravity",
        "Testing the Homogeneity of Two Proportions for Correlated Bilateral Data\n  via the Clayton Copula",
        "Bounds on Elliptic Sombor and Euler Sombor indices of join and corona\n  product of graphs",
        "Equivalence between top-down and bottom-up holographic approaches",
        "Gluing invariants of Donaldson--Thomas type -- Part II: Matrix\n  factorizations",
        "Dynamics of a Family of Rational Operators of Arbitrary Degree",
        "Analytical control of the exchange interaction in periodically driven\n  Mott insulators",
        "The late-time heating Green's function and improvements to distortion\n  frequency hierarchy treatment"
      ],
      "abstract":[
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "Athletic training is characterized by physiological systems responding to\nrepeated exercise-induced stress, resulting in gradual alterations in the\nfunctional properties of these systems. The adaptive response leading to\nimproved performance follows a remarkably predictable pattern that may be\ndescribed by a systems model provided that training load can be accurately\nquantified and that the constants defining the training-performance\nrelationship are known. While various impulse-response models have been\nproposed, they are inherently limited in reducing training stress (the impulse)\ninto a single metric, assuming that the adaptive responses are independent of\nthe type of training performed. This is despite ample evidence of markedly\ndiverse acute and chronic responses to exercise of different intensities and\ndurations. Herein, we propose an alternative, three-dimensional\nimpulse-response model that uses three training load metrics as inputs and\nthree performance metrics as outputs. These metrics, represented by a\nthree-parameter critical power model, reflect the stress imposed on each of the\nthree energy systems: the alactic (phosphocreatine\/immediate) system; the\nlactic (glycolytic) system; and the aerobic (oxidative) system. The purpose of\nthis article is to outline the scientific rationale and the practical\nimplementation of the three-dimensional impulse-response model.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Microbes and pathogens play a detrimental role in healing wounds, causing\ninfections like impetigo through bodily fluids and skin and entering the\nbloodstream through the wounds, thereby hindering the healing process and\ntissue regeneration. Clay, known for its long history of natural therapeutic\nuse, has emerged as one of the most promising candidates for biomedical\napplications due to its non-toxic nature, porosity, high surface area,\nubiquity, and excellent cation exchange capacity. This study demonstrates an\ninnovative approach to engineering an organo-functionalized,\ninfection-resistant, easy-to-use bandage material from clay, an environmentally\nbenign and sustainable material. The hybrid membranes have been developed using\nclays, zwitterions, silver ions, and terbinafine hydrochloride (TBH) to impart\nantibacterial and antifungal efficacy. A critical aspect of this study is\nembedding organic molecules and metal ions with the clays and releasing them to\nresist the growth and kill the pathogens. The antimicrobial efficacy of the\nmembranes has been tested using a zone of inhibition study against the most\ncommon microbes in skin wounds, viz. S. aureus, E. coli, and C. albicans.\nResults from our studies not only demonstrate the potential of these hybrid\nclay membranes as a cost-effective, scalable, and effective solution for\ntreating microbial infections but also instill newer avenues for point-of-care\nwound-healing treatments, offering hope for improved patient outcomes.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Some Dirichlet-like functions, attached to a pair (periodic function,\npolynomial) are introduced and studied. These functions generalize the standard\nDirichlet L-functions of Dirichlet characters. They have similar properties,\nbeing holomorphic on thefull complex plane and having simple values on negative\nintegers.",
        "Classifying groups up to quasi-isometry is a fundamental problem in geometric\ngroup theory. In the context of hyperbolic and relatively hyperbolic groups,\none of the key invariants in this classification is the boundary at infinity.\nF. Paulin proved that two hyperbolic groups are quasi-isometric if and only if\ntheir Gromov boundaries are quasiconformally equivalent. In this article, we\nextend Paulin's result to relatively hyperbolic groups and their Bowditch\nboundaries.\n  A notion of quasiconformal map preserving the shadows of horoballs relative\nto a point at the Bowditch boundary is defined and we have shown that every\ncoarsely cusp-preserving quasi-isometry between two relatively hyperbolic\ngroups induces a shadow-preserving quasiconformal map between their Bowditch\nboundaries. Conversely, we have shown that if the Bowditch boundaries of two\nrelatively hyperbolic groups are quasiconformally equivalent and the\nquasiconformal map coarsely preserves the shadows of horoballs relative to each\nboundary point, then the quasiconformal map induces a coarsely cusp-preserving\nquasi-isometry between those groups.",
        "To investigate the intrinsic magnetic properties of UTe$_2$, we performed\n$^{125}$Te-NMR measurements on the ultra-clean single-crystalline UTe$_2$ with\nsuperconducting transition temperature $T_{\\rm c}$ = 2.1~K and compared the\nresults with those of the $T_{\\rm c}$ = 1.6~K sample. The broadening of the\nlinewidth of the NMR spectrum in the $a$-axis magnetic field and the\nlow-temperature magnetic fluctuations observed in the 1.6~K sample are\nsuppressed in the ultra-clean sample, indicating that such magnetic properties\noriginate from a tiny amount of U deficiency. The present results suggest that\nthe magnetic properties in UTe$_2$ are sensitive to the U deficiency. We also\nobserved a peculiar angular dependence of the NMR quantities due to large\nmagnetic anisotropy with the $a$-axis as the magnetic easy axis.",
        "We explore a Centaur geometry in JT gravity, which is an asymptotically AdS\nspacetime but in the IR admits a dS bubble with another AdS geometry in the\ndeep IR. Thus, this geometry admits a holographic dual in the sense that it is\nasymptotically AdS. In an attempt to understand this geometry, we calculate the\ndensity of states of the putative boundary dual for such mixed geometries by\nevaluating the on-shell action. We compute the density of states analytically\nin the classical limit. The resultant density of states suggest that the\ndegrees of freedom in the IR are reduced in such a putative boundary theory due\nto the IR modification corresponding to the dS bubble.",
        "When a high-energy particle, such as a $\\gamma$-ray or muon, impacts the\nsubstrate of a superconducting qubit chip, large numbers of electron-hole pairs\nand phonons are created. The ensuing dynamics of the electrons and holes\nchanges the local offset-charge environment for qubits near the impact site.\nThe phonons that are produced have energy above the superconducting gap in the\nfilms that compose the qubits, leading to quasiparticle excitations above the\nsuperconducting ground state when the phonons impinge on the qubit electrodes.\nAn elevated density of quasiparticles degrades qubit coherence, leading to\nerrors in qubit arrays. Because these pair-breaking phonons spread throughout\nmuch of the chip, the errors can be correlated across a large portion of the\narray, posing a significant challenge for quantum error correction. In order to\nstudy the dynamics of $\\gamma$-ray impacts on superconducting qubit arrays, we\nuse a $\\gamma$-ray source outside the dilution refrigerator to controllably\nirradiate our devices. By using charge-sensitive transmon qubits, we can\nmeasure both the offset-charge shifts and quasiparticle poisoning due to the\n$\\gamma$ irradiation at different doses. We study correlations between\noffset-charge shifts and quasiparticle poisoning for different qubits in the\narray and compare this with numerical modeling of charge and phonon dynamics\nfollowing a $\\gamma$-ray impact. We thus characterize the poisoning footprint\nof these impacts and quantify the performance of structures for mitigating\nphonon-mediated quasiparticle poisoning.",
        "We proceed with the study of the Nehari manifold method for functionals in\n$C^1(X \\setminus \\{0\\})$, where $X$ is a Banach space. We deal now with\nfunctionals whose fibering maps have two critical points (a minimiser followed\nby a maximiser). Under some additional conditions we show that the Nehari\nmanifold method provides us with the ground state level and two sequences of\ncritical values for these functionals. These results are applied to the class\nof {\\it prescribed energy problems} as well as to the concave-convex problem\nfor the {\\it affine} $p$-Laplacian operator.",
        "The thermal degradation of unplasticized poly(vinyl chloride), PVC, was\ncomprehensively investigated through the application of spectroscopic\ntechniques, as well as contact angle measurements (CA), dynamic mechanical\nanalysis (DMA), and size-exclusion chromatography (SEC). To study the effect of\nrelative humidity (RH) on the deterioration of unplasticized PVC, two regimes\nof accelerated degradation experiments were selected: low RH (max. 30% RH) and\nhigh RH = 60% levels, which corresponds to usually the highest RH in heritage\ninstitutions equipped with an HVAC system. Nuclear magnetic resonance (NMR) and\ninfrared spectroscopy (FTIR) did not reveal any significant changes in the\nmaterial during its degradation up to 20 weeks at temperatures ranging from\n60{\\deg}C to 80{\\deg}C. Notable changes were observed in the Raman and UV-Vis\nspectra, indicative of the formation of conjugated carbon-carbon double bonds.\nThe formation of polyenes was responsible for the yellowing of samples.\nNotwithstanding, the aforementioned changes did not lead to a notable decline\nin the mechanical properties, as evidenced by DMA and SEC measurements. EPR\nmeasurements demonstrated the formation of 2 radicals at 60{\\deg}C, and in the\nsample degraded at 80{\\deg}C the presence of radicals was evident. This\nindicates that a radical degradation mechanism cannot be excluded even at such\nlow temperatures.",
        "In this paper, we investigate time-dependent Kantowski--Sachs spherically\nsymmetric teleparallel $F(T)$ gravity with a scalar field source. We begin by\nsetting the exact field equations to be solved and solve conservation laws for\npossible scalar field potential, $V\\left(\\phi\\right)$, solutions. Then, we find\nnew non-trivial teleparallel $F(T)$ solutions by using power-law and\nexponential ansatz for each potential case arising from conservation laws, such\nas linear, quadratic, or logarithmic, to name a few. We find a general formula\nallowing us to compute all possible new teleparallel $F(T)$ solutions\napplicable for any scalar field potential and ansatz. Then, we apply this\nformula and find a large number of exact and approximate new teleparallel\n$F(T)$ solutions for several types of cases. Some new $F(T)$ solution classes\nmay be relevant for future cosmological applications, especially concerning\ndark matter, dark energy quintessence, phantom energy leading to the Big Rip\nevent, and quintom models of physical processes.",
        "Handling highly dependent data is crucial in clinical trials, particularly in\nfields related to ophthalmology. Incorrectly specifying the dependency\nstructure can lead to biased inferences. Traditionally, models rely on three\nfixed dependence structures, which lack flexibility and interpretation. In this\narticle, we propose a framework using a more general model -- copulas -- to\nbetter account for dependency. We assess the performance of three different\ntest statistics within the Clayton copula setting to demonstrate the\nframework's feasibility. Simulation results indicate that this method controls\ntype I error rates and achieves reasonable power, providing a solid benchmark\nfor future research and broader applications. Additionally, we present analyses\nof two real-world datasets as case studies.",
        "The Elliptic Somber and Euler Somber indices are newly defined topological\nindices based on the Somber index. Our paper presents calculations of the upper\nand lower bounds of these indices for the join and corona product of arbitrary\ngraphs. Furthermore, we demonstrate that these bounds are attained when both\ngraphs are regular.",
        "This work raises the question of whether finding an equivalent bottom-up\ndescription to a given top-down one is possible. We consider the vector meson\nspectrum derived in the D3\/D7 system to answer this question. Using WKB\nanalysis, we reconstruct a bottom-up confining potential that resembles the\ngeometric structure of the so-called hardwall model. We compute some properties\nfor this bottom-up model, including the thermal deconfinement phase transition,\nthe $\\rho$ radial Regge trajectory, and the configurational entropy.",
        "This paper is a follow-up to arXiv:2407.08471. Let $X$ be a a $(-1)$-shifted\nsymplectic derived Deligne--Mumford stack. Thanks to the Darboux lemma of\nBrav--Bussi--Joyce, $X$ is locally modeled by derived critical loci of a\nfunction $f$ on a smooth scheme $U$. In this paper we study the gluing of the\nlocally defined $2$-periodic (big) dg-categories of matrix factorizations\n$MF^\\infty(U,f)$. We show that these come canonically equipped with a structure\nof a $2$-periodic crystal of categories (\\ie an action of the dg-category of\n$2$-periodic $D$-modules on $X$) compatible with a relative Thom--Sebastiani\ntheorem expressing the equivariance under the action of quadratic bundles.\n  As our main theorem we show that the locally defined categories\n$MF^\\infty(U,f)$ can be glued along $X$ as a sheaf of crystals of 2-periodic\ndg-categories ``up to isotopy'', under the prescription of orientation data\ncontrolled by three obstruction classes. This result generalizes the gluing of\nthe Joyce's perverse sheaf of vanishing cycles and partially answers\nconjectures by Kontsevich--Soibelman and Toda in motivic Donaldson--Thomas\ntheory.",
        "In this paper we analyse the dynamics of a family of rational operators\ncoming from a fourth-order family of root-finding algorithms. We first show\nthat it may be convenient to redefine the parameters to prevent redundancies\nand unboundedness of problematic parameters. After reparametrization, we\nobserve that these rational maps belong to a more general family $O_{a,n,k}$ of\ndegree $n+k$ operators, which includes several other families of maps obtained\nfrom other numerical methods. We study the dynamics of $O_{a,n,k}$ and discuss\nfor which parameters $n$ and $k$ these operators would be suitable from the\nnumerical point of view.",
        "The manipulation of electronic structure through periodic electric fields\nenables the reversible control of effective interactions in extended\nantiferromagnetic Mott insulators on ultrafast timescales. A careful analytical\nexamination of the modulated effective interactions is conducted, accurately\ncharacterising it through the use of exact summation formulas and Bessel\nfunctions. As a result, time reversals are analytically determined in terms of\nBessel zeroes. We discuss the half-filled Hubbard model, as well as\nmulti-orbital models, various characteristics of the Kitaev-Heisenberg model,\nand the emergence of chiral spin terms.",
        "Early energy injection leaves an imprint on the observed blackbody spectrum\nof the CMB, allowing us to study the thermal history of the Universe. For small\nenergy release, the distortion can be efficiently computed using the\nquasi-exact Green's function method. For pre-recombination injections, the\nGreen's function has already been studied previously. Here we reconsider the\npre- and post-recombination periods, showcasing both the spectral distortion\nintensity and the relative temperature difference, which encrypt precious\ninformation about physical processes such as free-free interactions and thermal\ndecoupling. We present the associated distortion visibility function,\ninvestigating the impact of various physical effects. We then study\nimprovements to the so-called frequency hierarchy (FH) treatment, a method that\nwas developed for the modelling of anisotropic distortions, which like the\naverage distortion signals encode valuable cosmological information.\nSpecifically, the FH treatment has shortcomings even in the $\\mu$ era, that in\nprinciple should be easy to overcome. In this paper, we introduce a new\napproach to reduce the mismatch, concluding with a redefinition of the $\\mu$\nspectral shape using CosmoTherm. This solution takes into account double\nCompton and Bremsstrahlung effects in the low tail, which can be included in\nthe FH. This opens the path towards a refined modeling of spectral distortion\nanisotropies."
      ]
    }
  },
  {
    "id":2411.02815,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation",
    "start_abstract":"In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b36",
        "b33"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
        "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
      ],
      "categories":[
        "cs.CV",
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "CILP-FGDI: Exploiting Vision-Language Model for Generalizable Person\n  Re-Identification",
        "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks",
        "Language-Inspired Relation Transfer for Few-shot Class-Incremental\n  Learning",
        "Dissecting Human Body Representations in Deep Networks Trained for\n  Person Identification",
        "IBURD: Image Blending for Underwater Robotic Detection",
        "SENSEI: Semantic Exploration Guided by Foundation Models to Learn\n  Versatile World Models",
        "GauSTAR: Gaussian Surface Tracking and Reconstruction",
        "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models",
        "3D\/2D Registration of Angiograms using Silhouette-based Differentiable\n  Rendering",
        "Superpowering Open-Vocabulary Object Detectors for X-ray Vision",
        "CM3T: Framework for Efficient Multimodal Learning for Inhomogeneous\n  Interaction Datasets",
        "Piece it Together: Part-Based Concepting with IP-Priors",
        "Order-Robust Class Incremental Learning: Graph-Driven Dynamic Similarity\n  Grouping",
        "Coded Deep Learning: Framework and Algorithm",
        "Universal programmable and self-configuring optical filter",
        "Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient\n  Descent",
        "Harnessing the Potential of Large Language Models in Modern Marketing\n  Management: Applications, Future Directions, and Strategic Recommendations",
        "Development of Application-Specific Large Language Models to Facilitate\n  Research Ethics Review",
        "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use",
        "Contracting low degree points on curves",
        "Domain-conditioned and Temporal-guided Diffusion Modeling for\n  Accelerated Dynamic MRI Reconstruction",
        "A study of the Antlion Random Walk",
        "Evolving Skeletons: Motion Dynamics in Action Recognition",
        "B-Call: Integrating Ideological Position and Political Cohesion in\n  Legislative Voting Models",
        "Foliated Plateau problems, geometric rigidity and equidistribution of\n  closed $k$-surfaces",
        "Evidence for J\/$\\psi$ suppression in incoherent photonuclear production",
        "A Machine Learning Approach for Design of Frequency Selective Surface\n  based Radar Absorbing Material via Image Prediction"
      ],
      "abstract":[
        "The Visual Language Model, known for its robust cross-modal capabilities, has\nbeen extensively applied in various computer vision tasks. In this paper, we\nexplore the use of CLIP (Contrastive Language-Image Pretraining), a\nvision-language model pretrained on large-scale image-text pairs to align\nvisual and textual features, for acquiring fine-grained and domain-invariant\nrepresentations in generalizable person re-identification. The adaptation of\nCLIP to the task presents two primary challenges: learning more fine-grained\nfeatures to enhance discriminative ability, and learning more domain-invariant\nfeatures to improve the model's generalization capabilities. To mitigate the\nfirst challenge thereby enhance the ability to learn fine-grained features, a\nthree-stage strategy is proposed to boost the accuracy of text descriptions.\nInitially, the image encoder is trained to effectively adapt to person\nre-identification tasks. In the second stage, the features extracted by the\nimage encoder are used to generate textual descriptions (i.e., prompts) for\neach image. Finally, the text encoder with the learned prompts is employed to\nguide the training of the final image encoder. To enhance the model's\ngeneralization capabilities to unseen domains, a bidirectional guiding method\nis introduced to learn domain-invariant image features. Specifically,\ndomain-invariant and domain-relevant prompts are generated, and both positive\n(pulling together image features and domain-invariant prompts) and negative\n(pushing apart image features and domain-relevant prompts) views are used to\ntrain the image encoder. Collectively, these strategies contribute to the\ndevelopment of an innovative CLIP-based framework for learning fine-grained\ngeneralized features in person re-identification.",
        "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. We have updated our content to include the latest\npapers, and the differences between versions are highlighted in the appendix.\nFor further details and ongoing updates, please refer to our GitHub repository:\nhttps:\/\/github.com\/xinzhel\/LLM-Agent-Survey\/blob\/main\/search.md",
        "Depicting novel classes with language descriptions by observing few-shot\nsamples is inherent in human-learning systems. This lifelong learning\ncapability helps to distinguish new knowledge from old ones through the\nincrease of open-world learning, namely Few-Shot Class-Incremental Learning\n(FSCIL). Existing works to solve this problem mainly rely on the careful tuning\nof visual encoders, which shows an evident trade-off between the base knowledge\nand incremental ones. Motivated by human learning systems, we propose a new\nLanguage-inspired Relation Transfer (LRT) paradigm to understand objects by\njoint visual clues and text depictions, composed of two major steps. We first\ntransfer the pretrained text knowledge to the visual domains by proposing a\ngraph relation transformation module and then fuse the visual and language\nembedding by a text-vision prototypical fusion module. Second, to mitigate the\ndomain gap caused by visual finetuning, we propose context prompt learning for\nfast domain alignment and imagined contrastive learning to alleviate the\ninsufficient text data during alignment. With collaborative learning of domain\nalignments and text-image transfer, our proposed LRT outperforms the\nstate-of-the-art models by over $13\\%$ and $7\\%$ on the final session of\nmini-ImageNet and CIFAR-100 FSCIL benchmarks.",
        "Long-term body identification algorithms have emerged recently with the\nincreased availability of high-quality training data. We seek to fill knowledge\ngaps about these models by analyzing body image embeddings from four body\nidentification networks trained with 1.9 million images across 4,788 identities\nand 9 databases. By analyzing a diverse range of architectures (ViT, SWIN-ViT,\nCNN, and linguistically primed CNN), we first show that the face contributes to\nthe accuracy of body identification algorithms and that these algorithms can\nidentify faces to some extent -- with no explicit face training. Second, we\nshow that representations (embeddings) generated by body identification\nalgorithms encode information about gender, as well as image-based information\nincluding view (yaw) and even the dataset from which the image originated.\nThird, we demonstrate that identification accuracy can be improved without\nadditional training by operating directly and selectively on the learned\nembedding space. Leveraging principal component analysis (PCA), identity\ncomparisons were consistently more accurate in subspaces that eliminated\ndimensions that explained large amounts of variance. These three findings were\nsurprisingly consistent across architectures and test datasets. This work\nrepresents the first analysis of body representations produced by long-term\nre-identification networks trained on challenging unconstrained datasets.",
        "We present an image blending pipeline, \\textit{IBURD}, that creates realistic\nsynthetic images to assist in the training of deep detectors for use on\nunderwater autonomous vehicles (AUVs) for marine debris detection tasks.\nSpecifically, IBURD generates both images of underwater debris and their\npixel-level annotations, using source images of debris objects, their\nannotations, and target background images of marine environments. With Poisson\nediting and style transfer techniques, IBURD is even able to robustly blend\ntransparent objects into arbitrary backgrounds and automatically adjust the\nstyle of blended images using the blurriness metric of target background\nimages. These generated images of marine debris in actual underwater\nbackgrounds address the data scarcity and data variety problems faced by\ndeep-learned vision algorithms in challenging underwater conditions, and can\nenable the use of AUVs for environmental cleanup missions. Both quantitative\nand robotic evaluations of IBURD demonstrate the efficacy of the proposed\napproach for robotic detection of marine debris.",
        "Exploration is a cornerstone of reinforcement learning (RL). Intrinsic\nmotivation attempts to decouple exploration from external, task-based rewards.\nHowever, established approaches to intrinsic motivation that follow general\nprinciples such as information gain, often only uncover low-level interactions.\nIn contrast, children's play suggests that they engage in meaningful high-level\nbehavior by imitating or interacting with their caregivers. Recent work has\nfocused on using foundation models to inject these semantic biases into\nexploration. However, these methods often rely on unrealistic assumptions, such\nas language-embedded environments or access to high-level actions. We propose\nSEmaNtically Sensible ExploratIon (SENSEI), a framework to equip model-based RL\nagents with an intrinsic motivation for semantically meaningful behavior.\nSENSEI distills a reward signal of interestingness from Vision Language Model\n(VLM) annotations, enabling an agent to predict these rewards through a world\nmodel. Using model-based RL, SENSEI trains an exploration policy that jointly\nmaximizes semantic rewards and uncertainty. We show that in both robotic and\nvideo game-like simulations SENSEI discovers a variety of meaningful behaviors\nfrom image observations and low-level actions. SENSEI provides a general tool\nfor learning from foundation model feedback, a crucial research direction, as\nVLMs become more powerful.",
        "3D Gaussian Splatting techniques have enabled efficient photo-realistic\nrendering of static scenes. Recent works have extended these approaches to\nsupport surface reconstruction and tracking. However, tracking dynamic surfaces\nwith 3D Gaussians remains challenging due to complex topology changes, such as\nsurfaces appearing, disappearing, or splitting. To address these challenges, we\npropose GauSTAR, a novel method that achieves photo-realistic rendering,\naccurate surface reconstruction, and reliable 3D tracking for general dynamic\nscenes with changing topology. Given multi-view captures as input, GauSTAR\nbinds Gaussians to mesh faces to represent dynamic objects. For surfaces with\nconsistent topology, GauSTAR maintains the mesh topology and tracks the meshes\nusing Gaussians. For regions where topology changes, GauSTAR adaptively unbinds\nGaussians from the mesh, enabling accurate registration and generation of new\nsurfaces based on these optimized Gaussians. Additionally, we introduce a\nsurface-based scene flow method that provides robust initialization for\ntracking between frames. Experiments demonstrate that our method effectively\ntracks and reconstructs dynamic surfaces, enabling a range of applications. Our\nproject page with the code release is available at\nhttps:\/\/eth-ait.github.io\/GauSTAR\/.",
        "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps:\/\/github.com\/ShawnTan86\/TokenCarve.",
        "We present a method for 3D\/2D registration of Digital Subtraction Angiography\n(DSA) images to provide valuable insight into brain hemodynamics and\nangioarchitecture. Our approach formulates the registration as a pose\nestimation problem, leveraging both anteroposterior and lateral DSA views and\nemploying differentiable rendering. Preliminary experiments on real and\nsynthetic datasets demonstrate the effectiveness of our method, with both\nqualitative and quantitative evaluations highlighting its potential for\nclinical applications. The code is available at\nhttps:\/\/github.com\/taewoonglee17\/TwoViewsDSAReg.",
        "Open-vocabulary object detection (OvOD) is set to revolutionize security\nscreening by enabling systems to recognize any item in X-ray scans. However,\ndeveloping effective OvOD models for X-ray imaging presents unique challenges\ndue to data scarcity and the modality gap that prevents direct adoption of\nRGB-based solutions. To overcome these limitations, we propose RAXO, a\ntraining-free framework that repurposes off-the-shelf RGB OvOD detectors for\nrobust X-ray detection. RAXO builds high-quality X-ray class descriptors using\na dual-source retrieval strategy. It gathers relevant RGB images from the web\nand enriches them via a novel X-ray material transfer mechanism, eliminating\nthe need for labeled databases. These visual descriptors replace text-based\nclassification in OvOD, leveraging intra-modal feature distances for robust\ndetection. Extensive experiments demonstrate that RAXO consistently improves\nOvOD performance, providing an average mAP increase of up to 17.0 points over\nbase detectors. To further support research in this emerging field, we also\nintroduce DET-COMPASS, a new benchmark featuring bounding box annotations for\nover 300 object categories, enabling large-scale evaluation of OvOD in X-ray.\nCode and dataset available at: https:\/\/github.com\/PAGF188\/RAXO.",
        "Challenges in cross-learning involve inhomogeneous or even inadequate amount\nof training data and lack of resources for retraining large pretrained models.\nInspired by transfer learning techniques in NLP, adapters and prefix tuning,\nthis paper presents a new model-agnostic plugin architecture for\ncross-learning, called CM3T, that adapts transformer-based models to new or\nmissing information. We introduce two adapter blocks: multi-head vision\nadapters for transfer learning and cross-attention adapters for multimodal\nlearning. Training becomes substantially efficient as the backbone and other\nplugins do not need to be finetuned along with these additions. Comparative and\nablation studies on three datasets Epic-Kitchens-100, MPIIGroupInteraction and\nUDIVA v0.5 show efficacy of this framework on different recording settings and\ntasks. With only 12.8% trainable parameters compared to the backbone to process\nvideo input and only 22.3% trainable parameters for two additional modalities,\nwe achieve comparable and even better results than the state-of-the-art. CM3T\nhas no specific requirements for training or pretraining and is a step towards\nbridging the gap between a general model and specific practical applications of\nvideo classification.",
        "Advanced generative models excel at synthesizing images but often rely on\ntext-based conditioning. Visual designers, however, often work beyond language,\ndirectly drawing inspiration from existing visual elements. In many cases,\nthese elements represent only fragments of a potential concept-such as an\nuniquely structured wing, or a specific hairstyle-serving as inspiration for\nthe artist to explore how they can come together creatively into a coherent\nwhole. Recognizing this need, we introduce a generative framework that\nseamlessly integrates a partial set of user-provided visual components into a\ncoherent composition while simultaneously sampling the missing parts needed to\ngenerate a plausible and complete concept. Our approach builds on a strong and\nunderexplored representation space, extracted from IP-Adapter+, on which we\ntrain IP-Prior, a lightweight flow-matching model that synthesizes coherent\ncompositions based on domain-specific priors, enabling diverse and\ncontext-aware generations. Additionally, we present a LoRA-based fine-tuning\nstrategy that significantly improves prompt adherence in IP-Adapter+ for a\ngiven task, addressing its common trade-off between reconstruction quality and\nprompt adherence.",
        "Class Incremental Learning (CIL) aims to enable models to learn new classes\nsequentially while retaining knowledge of previous ones. Although current\nmethods have alleviated catastrophic forgetting (CF), recent studies highlight\nthat the performance of CIL models is highly sensitive to the order of class\narrival, particularly when sequentially introduced classes exhibit high\ninter-class similarity. To address this critical yet understudied challenge of\nclass order sensitivity, we first extend existing CIL frameworks through\ntheoretical analysis, proving that grouping classes with lower pairwise\nsimilarity during incremental phases significantly improves model robustness to\norder variations. Building on this insight, we propose Graph-Driven Dynamic\nSimilarity Grouping (GDDSG), a novel method that employs graph coloring\nalgorithms to dynamically partition classes into similarity-constrained groups.\nEach group trains an isolated CIL sub-model and constructs meta-features for\nclass group identification. Experimental results demonstrate that our method\neffectively addresses the issue of class order sensitivity while achieving\noptimal performance in both model accuracy and anti-forgetting capability. Our\ncode is available at https:\/\/github.com\/AIGNLAI\/GDDSG.",
        "The success of deep learning (DL) is often achieved with large models and\nhigh complexity during both training and post-training inferences, hindering\ntraining in resource-limited settings. To alleviate these issues, this paper\nintroduces a new framework dubbed ``coded deep learning'' (CDL), which\nintegrates information-theoretic coding concepts into the inner workings of DL,\nto significantly compress model weights and activations, reduce computational\ncomplexity at both training and post-training inference stages, and enable\nefficient model\/data parallelism. Specifically, within CDL, (i) we first\npropose a novel probabilistic method for quantizing both model weights and\nactivations, and its soft differentiable variant which offers an analytic\nformula for gradient calculation during training; (ii) both the forward and\nbackward passes during training are executed over quantized weights and\nactivations, eliminating most floating-point operations and reducing training\ncomplexity; (iii) during training, both weights and activations are entropy\nconstrained so that they are compressible in an information-theoretic sense\nthroughout training, thus reducing communication costs in model\/data\nparallelism; and (iv) the trained model in CDL is by default in a quantized\nformat with compressible quantized weights, reducing post-training inference\nand storage complexity. Additionally, a variant of CDL, namely relaxed CDL\n(R-CDL), is presented to further improve the trade-off between validation\naccuracy and compression though requiring full precision in training with other\nadvantageous features of CDL intact. Extensive empirical results show that CDL\nand R-CDL outperform the state-of-the-art algorithms in DNN compression in the\nliterature.",
        "We propose an approach to integrated optical spectral filtering that allows\narbitrary programmability, can compensate automatically for imperfections in\nfilter fabrication, allows multiple simultaneous and separately programmable\nfilter functions on the same input, and can configure itself automatically to\nthe problem of interest, for example to filter or reject multiple arbitrarily\nchosen frequencies. The approach exploits splitting the input light into an\narray of multiple waveguides of different lengths that then feed a programmable\ninterferometer array that can also self-configure. It can give spectral\nresponse similar to arrayed waveguide gratings but offers many other filtering\nfunctions, as well as supporting other structures based on non-redundant arrays\nfor precise spectral filtering. Simultaneous filtering also allows, for the\nfirst time to our knowledge, an automatic measurement of the temporal coherency\nmatrix and physical separation into the Karhunen-Lo\\`eve expansion of\ntemporally partially coherent light fields.",
        "In this paper, we establish non-asymptotic convergence rates in the central\nlimit theorem for Polyak-Ruppert-averaged iterates of stochastic gradient\ndescent (SGD). Our analysis builds on the result of the Gaussian approximation\nfor nonlinear statistics of independent random variables of Shao and Zhang\n(2022). Using this result, we prove the non-asymptotic validity of the\nmultiplier bootstrap for constructing the confidence sets for the optimal\nsolution of an optimization problem. In particular, our approach avoids the\nneed to approximate the limiting covariance of Polyak-Ruppert SGD iterates,\nwhich allows us to derive approximation rates in convex distance of order up to\n$1\/\\sqrt{n}$.",
        "Large Language Models (LLMs) have revolutionized the process of customer\nengagement, campaign optimization, and content generation, in marketing\nmanagement. In this paper, we explore the transformative potential of LLMs\nalong with the current applications, future directions, and strategic\nrecommendations for marketers. In particular, we focus on LLMs major business\ndrivers such as personalization, real-time-interactive customer insights, and\ncontent automation, and how they enable customers and business outcomes. For\ninstance, the ethical aspects of AI with respect to data privacy, transparency,\nand mitigation of bias are also covered, with the goal of promoting responsible\nuse of the technology through best practices and the use of new technologies\nbusinesses can tap into the LLM potential, which help growth and stay one step\nahead in the turmoil of digital marketing. This article is designed to give\nmarketers the necessary guidance by using best industry practices to integrate\nthese powerful LLMs into their marketing strategy and innovation without\ncompromising on the ethos of their brand.",
        "Institutional review boards (IRBs) play a crucial role in ensuring the\nethical conduct of human subjects research, but face challenges including\ninconsistency, delays, and inefficiencies. We propose the development and\nimplementation of application-specific large language models (LLMs) to\nfacilitate IRB review processes. These IRB-specific LLMs would be fine-tuned on\nIRB-specific literature and institutional datasets, and equipped with retrieval\ncapabilities to access up-to-date, context-relevant information. We outline\npotential applications, including pre-review screening, preliminary analysis,\nconsistency checking, and decision support. While addressing concerns about\naccuracy, context sensitivity, and human oversight, we acknowledge remaining\nchallenges such as over-reliance on AI and the need for transparency. By\nenhancing the efficiency and quality of ethical review while maintaining human\njudgment in critical decisions, IRB-specific LLMs offer a promising tool to\nimprove research oversight. We call for pilot studies to evaluate the\nfeasibility and impact of this approach.",
        "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https:\/\/huggingface.co\/datasets\/bytedance-research\/ToolHop.",
        "The main result of this article is that all but finitely many points of small\nenough degree on a curve can be written as a pullback of a smaller degree\npoint. The main theorem has several corollaries that yield improvements on\nresults of Kadets and Vogt, Khawaja and Siksek, and Vojta under a slightly\nstronger assumption on the degree of the points.",
        "Purpose: To propose a domain-conditioned and temporal-guided diffusion\nmodeling method, termed dynamic Diffusion Modeling (dDiMo), for accelerated\ndynamic MRI reconstruction, enabling diffusion process to characterize\nspatiotemporal information for time-resolved multi-coil Cartesian and\nnon-Cartesian data. Methods: The dDiMo framework integrates temporal\ninformation from time-resolved dimensions, allowing for the concurrent capture\nof intra-frame spatial features and inter-frame temporal dynamics in diffusion\nmodeling. It employs additional spatiotemporal ($x$-$t$) and self-consistent\nfrequency-temporal ($k$-$t$) priors to guide the diffusion process. This\napproach ensures precise temporal alignment and enhances the recovery of fine\nimage details. To facilitate a smooth diffusion process, the nonlinear\nconjugate gradient algorithm is utilized during the reverse diffusion steps.\nThe proposed model was tested on two types of MRI data: Cartesian-acquired\nmulti-coil cardiac MRI and Golden-Angle-Radial-acquired multi-coil\nfree-breathing lung MRI, across various undersampling rates. Results: dDiMo\nachieved high-quality reconstructions at various acceleration factors,\ndemonstrating improved temporal alignment and structural recovery compared to\nother competitive reconstruction methods, both qualitatively and\nquantitatively. This proposed diffusion framework exhibited robust performance\nin handling both Cartesian and non-Cartesian acquisitions, effectively\nreconstructing dynamic datasets in cardiac and lung MRI under different imaging\nconditions. Conclusion: This study introduces a novel diffusion modeling method\nfor dynamic MRI reconstruction.",
        "This paper treats a new type of random walk referred to as an Antlion Random\nWalk (ARW), which is motivated by mathematical modeling of the decision-making\nprocess using chaotic semiconductor lasers with memory parameters. We discuss\nthe dependency of the property of the probability distribution of ARWs on the\nmemory parameter $\\alpha$ and discuss uniqueness of them in contrast to the\nconventional, simple RWs through similarity to the normal distribution.",
        "Skeleton-based action recognition has gained significant attention for its\nability to efficiently represent spatiotemporal information in a lightweight\nformat. Most existing approaches use graph-based models to process skeleton\nsequences, where each pose is represented as a skeletal graph structured around\nhuman physical connectivity. Among these, the Spatiotemporal Graph\nConvolutional Network (ST-GCN) has become a widely used framework.\nAlternatively, hypergraph-based models, such as the Hyperformer, capture\nhigher-order correlations, offering a more expressive representation of complex\njoint interactions. A recent advancement, termed Taylor Videos, introduces\nmotion-enhanced skeleton sequences by embedding motion concepts, providing a\nfresh perspective on interpreting human actions in skeleton-based action\nrecognition. In this paper, we conduct a comprehensive evaluation of both\ntraditional skeleton sequences and Taylor-transformed skeletons using ST-GCN\nand Hyperformer models on the NTU-60 and NTU-120 datasets. We compare skeletal\ngraph and hypergraph representations, analyzing static poses against\nmotion-injected poses. Our findings highlight the strengths and limitations of\nTaylor-transformed skeletons, demonstrating their potential to enhance motion\ndynamics while exposing current challenges in fully using their benefits. This\nstudy underscores the need for innovative skeletal modelling techniques to\neffectively handle motion-rich data and advance the field of action\nrecognition.",
        "This paper combines two significant areas of political science research:\nmeasuring individual ideological position and cohesion. Although both\napproaches help analyze legislative behaviors, no unified model currently\nintegrates these dimensions. To fill this gap, the paper proposes a methodology\ncalled B-Call that combines ideological positioning with voting cohesion,\ntreating votes as random variables. The model is empirically validated using\nroll-call data from the United States, Brazil, and Chile legislatures, which\nrepresent diverse legislative dynamics. The analysis aims to capture the\ncomplexities of voting and legislative behaviors, resulting in a\ntwo-dimensional indicator. This study addresses gaps in current legislative\nvoting models, particularly in contexts with limited party control.",
        "In this note, we survey recent advances in the study of dynamical properties\nof the space of surfaces with constant curvature in three-dimensional manifolds\nof negative sectional curvature. We interpret this space as a two-dimensional\nanalogue of the geodesic flow and explore the extent to which the thermodynamic\nproperties of the latter can be generalized to the surface setting.\nAdditionally, we apply this theory to derive geometric rigidity results,\nincluding the rigidity of the hyperbolic marked area spectrum.",
        "According to quantum chromodynamics, at sufficiently high energy, the\nstructure of hadrons reveals a dynamic equilibrium between gluon splitting and\ngluon recombination -- a phenomenon known as saturation. The process of\ndiffractive photonuclear production of a J\/$\\psi$ vector meson provides a\ndirect insight into the gluon composition of hadrons. The J\/$\\psi$ production\nas a function of momentum transferred in the interaction, quantified by the\nMandelstam-$t$ variable, serves as an excellent probe for studying the\nstructure of hadrons within the impact-parameter plane, because different\nranges in $t$ are sensitive to the dynamics of the gluon field at varying\nspatial size scales. The ALICE collaboration has measured the energy dependence\nof incoherent photonuclear production of J\/$\\psi$ mesons off lead ions, at\n$\\sqrt{s_{\\rm NN}} = 5.02$ TeV, for three Mandelstam-$t$ intervals. The energy\ndependence of the photonuclear cross section at the highest $|t|$ range\nmeasured, $(0.81< |t| <1.44)$ GeV$^2$, is sensitive to subnucleonic structures\nof the Pb target. The increase of the cross section with energy at large $|t|$\nshows evidence of suppression with respect to the increase seen at low $|t|$.\nThe observed pattern of the energy evolution in data is similar to that of\ngluon saturation models.",
        "The paper presents an innovative methodology for designing frequency\nselective surface (FSS) based radar absorbing materials using machine learning\n(ML) technique. In conventional electromagnetic design, unit cell dimensions of\nFSS are used as input and absorption coefficient is then predicted for a given\ndesign. In this paper, absorption coefficient is considered as input to ML\nmodel and image of FSS unit cell is predicted. Later, this image is used for\ngenerating the FSS unit cell parameters. Eleven different ML models are studied\nover a wide frequency band of 1GHz to 30GHz. Out of which six ML models (i.e.\n(a) Random Forest classification, (b) K- Neighbors Classification, (c) Grid\nsearch regression, (d) Random Forest regression, (e) Decision tree\nclassification, and (f) Decision tree regression) show training accuracy more\nthan 90%. The absorption coefficients with varying frequencies of these\npredicted images are subsequently evaluated using commercial electromagnetic\nsolver. The performance of these ML models is encouraging, and it can be used\nfor accelerating design and optimization of high performance FSS based radar\nabsorbing material for advanced electromagnetic applications in future."
      ]
    }
  },
  {
    "id":2411.00561,
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study",
    "start_abstract":"In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "What is a cell type, really? The quest to categorize life's myriad forms."
      ],
      "abstract":[
        "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Silicon is the next frontier in plant synthetic biology",
        "Leveraging Sequence Purification for Accurate Prediction of Multiple\n  Conformational States with AlphaFold2",
        "RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via\n  Synergistic Flow Matching",
        "DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown\n  Efficiency",
        "Non-Markovain Quantum State Diffusion for the Tunneling in SARS-COVID-19\n  virus",
        "A Comprehensive Review of Protein Language Models",
        "Inverse problems with experiment-guided AlphaFold",
        "How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of\n  RNA Graph Motifs Using Topological Descriptors",
        "Artificial Intelligence Approaches for Anti-Addiction Drug Discovery",
        "Deep Learning of Proteins with Local and Global Regions of Disorder",
        "Fluorescence Phasor Analysis: Basic Principles and Biophysical\n  Applications",
        "Remodeling Peptide-MHC-TCR Triad Binding as Sequence Fusion for\n  Immunogenicity Prediction",
        "An Energy-Adaptive Elastic Equivariant Transformer Framework for Protein\n  Structure Representation",
        "Teacher-student training improves accuracy and efficiency of machine\n  learning inter-atomic potentials",
        "Partitions of unity and barycentric algebras",
        "Adiabatic Pumping of Orbital Magnetization by Spin Precession",
        "Non-positive energy quasidistributions in coherent collision models",
        "New properties of length-extremals in free step-2 rank-4 Carnot groups",
        "Covariant photon current",
        "PyClustrPath: An efficient Python package for generating clustering\n  paths with GPU acceleration",
        "Hybrid Brain-Machine Interface: Integrating EEG and EMG for Reduced\n  Physical Demand",
        "Period Analysis of Eclipsing Cataclysmic Variable Stars",
        "OpenGERT: Open Source Automated Geometry Extraction with Geometric and\n  Electromagnetic Sensitivity Analyses for Ray-Tracing Propagation Models",
        "Black holes inside cosmic voids",
        "Poisson Vertex Algebras and Three-Dimensional Gauge Theory",
        "Investigating the Effects of Atmospheric Stratification on Coronal\n  Active Region Field Modelling",
        "Spall failure of alumina at high-strain rates using femtosecond laser\n  experiments and high-fidelity molecular dynamics simulations",
        "Bounded conciseness in the space of marked groups"
      ],
      "abstract":[
        "Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field.",
        "AlphaFold2 (AF2) has transformed protein structure prediction by harnessing\nco-evolutionary constraints embedded in multiple sequence alignments (MSAs).\nMSAs not only encode static structural information, but also hold critical\ndetails about protein dynamics, which underpin biological functions. However,\nthese subtle co-evolutionary signatures, which dictate conformational state\npreferences, are often obscured by noise within MSA data and thus remain\nchallenging to decipher. Here, we introduce AF-ClaSeq, a systematic framework\nthat isolates these co-evolutionary signals through sequence purification and\niterative enrichment. By extracting sequence subsets that preferentially encode\ndistinct structural states, AF-ClaSeq enables high-confidence predictions of\nalternative conformations. Our findings reveal that the successful sampling of\nalternative states depends not on MSA depth but on sequence purity.\nIntriguingly, purified sequences encoding specific structural states are\ndistributed across phylogenetic clades and superfamilies, rather than confined\nto specific lineages. Expanding upon AF2's transformative capabilities,\nAF-ClaSeq provides a powerful approach for uncovering hidden structural\nplasticity, advancing allosteric protein and drug design, and facilitating\ndynamics-based protein function annotation.",
        "Ribonucleic acid (RNA) binds to molecules to achieve specific biological\nfunctions. While generative models are advancing biomolecule design, existing\nmethods for designing RNA that target specific ligands face limitations in\ncapturing RNA's conformational flexibility, ensuring structural validity, and\novercoming data scarcity. To address these challenges, we introduce RiboFlow, a\nsynergistic flow matching model to co-design RNA structures and sequences based\non target molecules. By integrating RNA backbone frames, torsion angles, and\nsequence features in an unified architecture, RiboFlow explicitly models RNA's\ndynamic conformations while enforcing sequence-structure consistency to improve\nvalidity. Additionally, we curate RiboBind, a large-scale dataset of\nRNA-molecule interactions, to resolve the scarcity of high-quality structural\ndata. Extensive experiments reveal that RiboFlow not only outperforms\nstate-of-the-art RNA design methods by a large margin but also showcases\ncontrollable capabilities for achieving high binding affinity to target\nligands. Our work bridges critical gaps in controllable RNA design, offering a\nframework for structure-aware, data-efficient generation.",
        "Background: Small interfering RNA (siRNA) is a promising therapeutic agent\ndue to its ability to silence disease-related genes via RNA interference. While\ntraditional machine learning and early deep learning methods have made progress\nin predicting siRNA efficacy, there remains significant room for improvement.\nAdvanced deep learning techniques can enhance prediction accuracy, reducing the\nreliance on extensive wet-lab experiments and accelerating the identification\nof effective siRNA sequences. This approach also provides deeper insights into\nthe mechanisms of siRNA efficacy, facilitating more targeted and efficient\ntherapeutic strategies.\n  Methods: We introduce DeepSilencer, an innovative deep learning model\ndesigned to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced\nneural network architectures to capture the complex features of siRNA\nsequences. Our key contributions include a specially designed deep learning\nmodel, an innovative online data sampling method, and an improved loss function\ntailored for siRNA prediction. These enhancements collectively boost the\nmodel's prediction accuracy and robustness.\n  Results: Extensive evaluations on multiple test sets demonstrate that\nDeepSilencer achieves state-of-the-art performance using only siRNA sequences\nand basic physicochemical properties. Our model surpasses several other methods\nand shows superior predictive performance, particularly when incorporating\nthermodynamic parameters.\n  Conclusion: The advancements in data sampling, model design, and loss\nfunction significantly enhance the predictive capabilities of DeepSilencer.\nThese improvements underscore its potential to advance RNAi therapeutic design\nand development, offering a powerful tool for researchers and clinicians.",
        "In the context of biology, unlike the comprehensively established Standard\nModel in physics, many biological processes lack a complete theoretical\nframework and are often described phenomenologically. A pertinent example is\nolfaction -- the process through which humans and animals distinguish various\nodors. The conventional biological explanation for olfaction relies on the lock\nand key model, which, while useful, does not fully account for all observed\nphenomena. As an alternative or complement to this model, vibration-assisted\nelectron tunneling has been proposed. Drawing inspiration from the\nvibration-assisted electron tunneling model for olfaction, we have developed a\ntheoretical model for electron tunneling in SARS-CoV-2 virus infection within a\nnon-Markovian framework. We approach this by solving the non-Markovian quantum\nstochastic Schrodinger equation. In our model, the spike protein and the GPCR\nreceptor are conceptualized as a dimer, utilizing the spin-Boson model to\nfacilitate the description of electron tunneling. Our analysis demonstrates\nthat electron tunneling in this context exhibits inherently non-Markovian\ncharacteristics, extending into the intermediate and strong coupling regimes\nbetween the dimer components. This behavior stands in stark contrast to\npredictions from Markovian models, which fail to accurately describe electron\ntunneling in the strong coupling limit. Notably, Markovian approximations often\nlead to unphysical negative probabilities in this regime, underscoring their\nlimitations and highlighting the necessity of incorporating non-Markovian\ndynamics for a more realistic description of biological quantum processes. This\napproach not only broadens our understanding of viral infection mechanisms but\nalso enhances the biological accuracy and relevance of our theoretical\nframework in describing complex biological interactions.",
        "At the intersection of the rapidly growing biological data landscape and\nadvancements in Natural Language Processing (NLP), protein language models\n(PLMs) have emerged as a transformative force in modern research. These models\nhave achieved remarkable progress, highlighting the need for timely and\ncomprehensive overviews. However, much of the existing literature focuses\nnarrowly on specific domains, often missing a broader analysis of PLMs. This\nstudy provides a systematic review of PLMs from a macro perspective, covering\nkey historical milestones and current mainstream trends. We focus on the models\nthemselves and their evaluation metrics, exploring aspects such as model\narchitectures, positional encoding, scaling laws, and datasets. In the\nevaluation section, we discuss benchmarks and downstream applications. To\nfurther support ongoing research, we introduce relevant mainstream tools.\nLastly, we critically examine the key challenges and limitations in this\nrapidly evolving field.",
        "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
        "We introduce a computational topology-based approach with unsupervised\nmachine-learning algorithms to estimate the database size and content of\nRNA-like graph topologies. Specifically, we apply graph theory enumeration to\ngenerate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2\nto 9. Among them, only 0.11% graphs correspond to approximately 200,000 known\nRNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping\nmethod. The remaining 99.89% of the dual graphs may be RNA-like or\nnon-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are\nmore likely to be associated with RNA structures, we apply computational\ntopology descriptors using the Persistent Spectral Graphs (PSG) method to\ncharacterize each graph using 19 PSG-based features and use clustering\nalgorithms that partition all possible dual graphs into two clusters, RNA-like\ncluster and non-RNA-like cluster. The distance of each dual graph to the center\nof the RNA-like cluster represents the likelihood of it belonging to RNA\nstructures. From validation, our PSG-based RNA-like cluster includes 97.3% of\nthe 121 known RNA dual graphs, suggesting good performance. Furthermore,\n46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,\nwe observe that all the top 15 RNA-like dual graphs can be separated into\nmultiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to\nhave any subgraphs. Moreover, a significant topological difference between top\nRNA-like and non-RNA-like graphs is evident when comparing their topological\nfeatures. These findings provide valuable insights into the size of the RNA\nmotif universe and RNA design strategies, offering a novel framework for\npredicting RNA graph topologies and guiding the discovery of novel RNA motifs,\nperhaps anti-viral therapeutics by subgraph assembly.",
        "Drug addiction is a complex and pervasive global challenge that continues to\npose significant public health concerns. Traditional approaches to\nanti-addiction drug discovery have struggled to deliver effective therapeutics,\nfacing high attrition rates, long development timelines, and inefficiencies in\nprocessing large-scale data. Artificial intelligence (AI) has emerged as a\ntransformative solution to address these issues. Using advanced algorithms, AI\nis revolutionizing drug discovery by enhancing the speed and precision of key\nprocesses. This review explores the transformative role of AI in the pipeline\nfor anti-addiction drug discovery, including data collection, target\nidentification, and compound optimization. By highlighting the potential of AI\nto overcome traditional barriers, this review systematically examines how AI\naddresses critical gaps in anti-addiction research, emphasizing its potential\nto revolutionize drug discovery and development, overcome challenges, and\nadvance more effective therapeutic strategies.",
        "Although machine learning has transformed protein structure prediction of\nfolded protein ground states with remarkable accuracy, intrinsically disordered\nproteins and regions (IDPs\/IDRs) are defined by diverse and dynamical\nstructural ensembles that are predicted with low confidence by algorithms such\nas AlphaFold. We present a new machine learning method, IDPForge (Intrinsically\nDisordered Protein, FOlded and disordered Region GEnerator), that exploits a\ntransformer protein language diffusion model to create all-atom IDP ensembles\nand IDR disordered ensembles that maintains the folded domains. IDPForge does\nnot require sequence-specific training, back transformations from\ncoarse-grained representations, nor ensemble reweighting, as in general the\ncreated IDP\/IDR conformational ensembles show good agreement with solution\nexperimental data, and options for biasing with experimental restraints are\nprovided if desired. We envision that IDPForge with these diverse capabilities\nwill facilitate integrative and structural studies for proteins that contain\nintrinsic disorder.",
        "Fluorescence is one of the most widely used techniques in biological\nsciences. Its exceptional sensitivity and versatility make it a tool of first\nchoice for quantitative studies in biophysics. The concept of phasors,\noriginally introduced by Charles Steinmetz in the late 19th century for\nanalyzing alternating current circuits, has since found applications across\ndiverse disciplines, including fluorescence spectroscopy. The main idea behind\nfluorescence phasors was posited by Gregorio Weber in 1981. By analyzing the\ncomplementary nature of pulse and phase fluorometry data, he shows that two\nmagnitudes -- denoted as $G$ and $S$ -- derived from the frequency-domain\nfluorescence measurements correspond to the real and imaginary part of the\nFourier transform of the fluorescence intensity in the time domain. This review\nprovides a historical perspective on how the concept of phasors originates and\nhow it integrates into fluorescence spectroscopy. We discuss their fundamental\nalgebraic properties, which enable intuitive model-free analysis of\nfluorescence data despite the complexity of the underlying phenomena. Some\napplications in biophysics illustrate the power of this approach in studying\ndiverse phenomena, including protein folding, protein interactions, phase\ntransitions in lipid mixtures and formation of high-order structures in nucleic\nacids.",
        "The complex nature of tripartite peptide-MHC-TCR interactions is a critical\nyet underexplored area in immunogenicity prediction. Traditional studies on\nTCR-antigen binding have not fully addressed the complex dependencies in triad\nbinding. In this paper, we propose new modeling approaches for these tripartite\ninteractions, utilizing sequence information from MHCs, peptides, and TCRs. Our\nmethods adhere to native sequence forms and align with biological processes to\nenhance prediction accuracy. By incorporating representation learning\ntechniques, we introduce a fusion mechanism to integrate the three sequences\neffectively. Empirical experiments show that our models outperform traditional\nmethods, achieving a 2.8 to 13.3 percent improvement in prediction accuracy\nacross existing benchmarks. We further validate our approach with extensive\nablation studies, demonstrating the effectiveness of the proposed model\ncomponents. The model implementation, code, and supplementary materials,\nincluding a manuscript with colored hyperlinks and a technical appendix for\ndigital viewing, will be open-sourced upon publication.",
        "Structure-informed protein representation learning is essential for effective\nprotein function annotation and \\textit{de novo} design. However, the presence\nof inherent noise in both crystal and AlphaFold-predicted structures poses\nsignificant challenges for existing methods in learning robust protein\nrepresentations. To address these issues, we propose a novel equivariant\nTransformer-State Space Model(SSM) hybrid framework, termed $E^3$former,\ndesigned for efficient protein representation. Our approach uses energy\nfunction-based receptive fields to construct proximity graphs and incorporates\nan equivariant high-tensor-elastic selective SSM within the transformer\narchitecture. These components enable the model to adapt to complex atom\ninteractions and extract geometric features with higher signal-to-noise ratios.\nEmpirical results demonstrate that our model outperforms existing methods in\nstructure-intensive tasks, such as inverse folding and binding site prediction,\nparticularly when using predicted structures, owing to its enhanced tolerance\nto data deviation and noise. Our approach offers a novel perspective for\nconducting biological function research and drug discovery using noisy protein\nstructure data.",
        "Machine learning inter-atomic potentials (MLIPs) are revolutionizing the\nfield of molecular dynamics (MD) simulations. Recent MLIPs have tended towards\nmore complex architectures trained on larger datasets. The resulting increase\nin computational and memory costs may prohibit the application of these MLIPs\nto perform large-scale MD simulations. Here, we present a teacher-student\ntraining framework in which the latent knowledge from the teacher (atomic\nenergies) is used to augment the students' training. We show that the\nlight-weight student MLIPs have faster MD speeds at a fraction of the memory\nfootprint compared to the teacher models. Remarkably, the student models can\neven surpass the accuracy of the teachers, even though both are trained on the\nsame quantum chemistry dataset. Our work highlights a practical method for\nMLIPs to reduce the resources required for large-scale MD simulations.",
        "Barycentric coordinates provide solutions to the problem of expressing an\nelement of a compact convex set as a convex combination of a finite number of\nextreme points of the set. They have been studied widely within the geometric\nliterature, typically in response to the demands of interpolation, numerical\nanalysis and computer graphics. In this note we bring an algebraic perspective\nto the problem, based on barycentric algebras. We focus on the discussion of\nrelations between different subclasses of partitions of unity, one arising in\nthe context of barycentric coordinates, based on the tautological map\nintroduced by Guessab.",
        "We propose adiabatic pumping of orbital magnetization driven by coherent spin\nprecession, facilitating the rectification of this precession. The orbital\nmagnetization originates from the adiabatic evolution of valence electrons with\na topological bulk contribution expressed as a Chern-Simons form. When the\nprecession cone angle of spin $\\mathbf{S}$ is small, the resulting\nmagnetization is proportional to $\\mathbf{S}\\times \\dot{\\mathbf{S}}$,\ncontributing to the magnon Zeeman effect. With a large cone angle, the\nmagnetization can reach its natural unit, $e\/T$, in an antiferromagnetic\ntopological insulator with $e$ as the elementary charge and $T$ as the\nprecession period. This significant magnetization is related to the global\nproperties of the electronic geometric phases in the parameter space spanned by\n$\\mathbf{S}$ and momentum $\\mathbf{k}$. When the pumped magnetization is\ninhomogeneous, induced by spin textures or electronic topological phase\ndomains, a dissipationless charge current is also pumped. At last, we discuss\nthe boundary contributions from the spin-driving edge states, which are\nintricately linked to the gauge-dependent quantum uncertainty of the\nChern-Simons form.",
        "We determine the Kirkwood-Dirac quasiprobability (KDQ) distribution\nassociated to the stochastic instances of internal energy variations for the\nquantum system and environment particles in coherent Markovian collision\nmodels. In the case the interactions between the quantum system and the\nparticles do not conserve energy, the KDQ of the non-energy-preserving\nstochastic work is also derived. These KDQ distributions can account for\nnon-commutativity, and return the unperturbed average values and variances for\na generic interaction-time, and generic local initial states of the quantum\nsystem and environment particles. Using this nonequilibrium-physics approach,\nwe certify the conditions under which the collision process of the model\nexhibits quantum traits, and we quantify the rate of energy exchanged by the\nquantum system by looking at the variance of the KDQ energy distributions.\nFinally, we propose an experimental test of our results on a superconducting\nquantum circuit implementing a qubit system, with microwave photons\nrepresenting the environment particles.",
        "In the free, step-2, rank-4 sub-Riemannian Carnot group, we give a clean\nexpression for length-extremals, we provide an explicit equation for conjugate\npoints, we relate it with the conjectured cut-locus of the origin. Finally, we\ngive some upper estimates for the cut-time of extremals.",
        "An inhomogeneous continuity equation for the photon four-current operator,\n$\\widehat{J}_{p}$, was derived in [M. Hawton, Phys. Rev. A, 109, 062221\n(2024)]. If the electromagnetic potential operator, $\\widehat{A}% =\\left(\n\\widehat{\\phi}\/c,\\widehat{\\mathbf{A}}\\right) $, is covariant then\n$\\widehat{J}_{p}$ is covariant and the continuity equation is invariant. Here\nwe start with the standard Lagrangian in a Lorentz invariant gauge and quantize\nboth transverse and longitudinal modes. The scalar potential\n$\\widehat{\\phi}=c\\widehat{A}_{\\Vert}$ is not independently second quantized, so\nall modes have positive definite norm. The continuity equation is generalized\nby separating the material source current into a nonabsorbing term describing\npropagation in a lossless transmission line and localized single photon\nemission and detection terms that do not require nonlocal separation of\ntransverse and longitudinal modes.",
        "Convex clustering is a popular clustering model without requiring the number\nof clusters as prior knowledge. It can generate a clustering path by\ncontinuously solving the model with a sequence of regularization parameter\nvalues. This paper introduces {\\it PyClustrPath}, a highly efficient Python\npackage for solving the convex clustering model with GPU acceleration. {\\it\nPyClustrPath} implements popular first-order and second-order algorithms with a\nclean modular design. Such a design makes {\\it PyClustrPath} more scalable to\nincorporate new algorithms for solving the convex clustering model in the\nfuture. We extensively test the numerical performance of {\\it PyClustrPath} on\npopular clustering datasets, demonstrating its superior performance compared to\nthe existing solvers for generating the clustering path based on the convex\nclustering model. The implementation of {\\it PyClustrPath} can be found at:\nhttps:\/\/github.com\/D3IntOpt\/PyClustrPath.",
        "We present a hybrid brain-machine interface (BMI) that integrates\nsteady-state visually evoked potential (SSVEP)-based EEG and facial EMG to\nimprove multimodal control and mitigate fatigue in assistive applications.\nTraditional BMIs relying solely on EEG or EMG suffer from inherent limitations;\nEEG-based control requires sustained visual focus, leading to cognitive\nfatigue, while EMG-based control induces muscular fatigue over time. Our system\ndynamically alternates between EEG and EMG inputs, using EEG to detect SSVEP\nsignals at 9.75 Hz and 14.25 Hz and EMG from cheek and neck muscles to optimize\ncontrol based on task demands. In a virtual turtle navigation task, the hybrid\nsystem achieved task completion times comparable to an EMG-only approach, while\n90% of users reported reduced or equal physical demand. These findings\ndemonstrate that multimodal BMI systems can enhance usability, reduce strain,\nand improve long-term adherence in assistive technologies.",
        "We have performed a study of the orbital properties of seven eclipsing\ncataclysmic variable (CV) binary systems by analyzing photometric time series\nfrom the Transiting Exoplanet Survey Satellite (TESS). We employed Python code\nto determine the eclipse epochs and orbital periods for each system, and\nconstructed O-C diagrams from observed and predicted eclipse epochs. By\nanalyzing the O-C diagrams of our target CVs, we have constrained values for\nchanges in orbital period with time. Our targets include a sample of sources\nfrom each class of non-magnetic, eclipsing CVs: dwarf novae variables, Z Cam\ntype, and U Gem subclasses. We include in our study classical novae variables,\nnova-like variables (including the VY Scl and UX UMa subclasses), and recurrent\nnovae variable stars. We approached this project with goals of developing time\nseries analysis techniques for future undergraduate-level studies of eclipsing\nCVs, and how they may contribute to the understanding of their orbital\nevolution.",
        "Accurate RF propagation modeling in urban environments is critical for\ndeveloping digital spectrum twins and optimizing wireless communication\nsystems. We introduce OpenGERT, an open-source automated Geometry Extraction\ntool for Ray Tracing, which collects and processes terrain and building data\nfrom OpenStreetMap, Microsoft Global ML Building Footprints, and USGS elevation\ndata. Using the Blender Python API, it creates detailed urban models for\nhigh-fidelity simulations with NVIDIA Sionna RT. We perform sensitivity\nanalyses to examine how variations in building height, position, and\nelectromagnetic material properties affect ray-tracing accuracy. Specifically,\nwe present pairwise dispersion plots of channel statistics (path gain, mean\nexcess delay, delay spread, link outage, and Rician K-factor) and investigate\nhow their sensitivities change with distance from transmitters. We also\nvisualize the variance of these statistics for selected transmitter locations\nto gain deeper insights. Our study covers Munich and Etoile scenes, each with\n10 transmitter locations. For each location, we apply five types of\nperturbations: material, position, height, height-position, and all combined,\nwith 50 perturbations each. Results show that small changes in permittivity and\nconductivity minimally affect channel statistics, whereas variations in\nbuilding height and position significantly alter all statistics, even with\nnoise standard deviations of 1 meter in height and 0.4 meters in position.\nThese findings highlight the importance of precise environmental modeling for\naccurate propagation predictions, essential for digital spectrum twins and\nadvanced communication networks. The code for geometry extraction and\nsensitivity analyses is available at github.com\/serhatadik\/OpenGERT\/.",
        "This study examines the gravitational and thermodynamic properties of static,\nspherically symmetric black holes within cosmic voids -- vast underdense\nregions of the universe. By deriving a novel solution based on a universal\ndensity profile for voids, we analyze its spacetime structure, which reveals\ntwo horizons: One of the black hole and the other related to the de Sitter-like\nbehavior. As the void approaches a perfect vacuum, the black hole horizon\ndiminishes, tending to that of the Schwarzschild solution, while the outer\nhorizon increases. We also study the solution stability via sound speed of the\nfluid, as well as the thermodynamic properties, including Hawking temperature,\nevaporation time, entropy, and specific heat. Our results show that as the void\nempties, the Hawking temperature rises, shortening evaporation times. The\nentropy follows the area's law and specific heat exhibits a minimum for a given\nblack hole size, indicating a thermal transition and highlighting the role of\nvoids in the black hole evolution. These findings offer new insights into the\nrelationship between local gravitational collapse and large-scale cosmic\nstructure, enhancing our understanding of the black hole behavior in underdense\nenvironments. We also provide a glimpse of a potential thermodynamic\ninteraction between the event horizon and the cosmological horizon.",
        "We introduce a mixed holomorphic-topological gauge theory in three dimensions\nassociated to a (freely generated) Poisson vertex algebra. The\n$\\lambda$-bracket of the PVA plays the role of the structure constants of the\ngauge algebra and the gauge invariance of the theory holds if and only if the\n$\\lambda$-bracket Jacobi identity is satisfied. We show that the\nholomorphic-topological symmetry of the theory enhances to full topological\nsymmetry if the Poisson vertex algebra contains a Virasoro element. We outline\nexamples associated to PVAs of $\\mathcal{W}$-type and demonstrate their\nconnections to various versions of $3d$ gravity. We expect the\nthree-dimensional Poisson sigma model to play an important role in the\ndeformation quantization of Poisson vertex algebras.",
        "Understanding the evolution of the complex magnetic fields found in solar\nactive regions is an active area of research. There are numerous models for\nsuch fields which range in their complexity due to the number of known physical\neffects included in them, the one common factor being they all extrapolate the\nfield up from the photosphere. In this study we focus on the fact that, above\nthe photosphere, and below the corona, lies the relatively cool and dense\nchromosphere -- which is often neglected in coronal models due to it being\ncomparatively thin and difficult hard to model. We isolate and examine the\neffect including this boundary layer has on a 2.5D class of driven MHD models\nof an active region eruption. We find that it can result in significant changes\nto the dynamics of an erupting field far higher in the atmosphere than the\nchromosphere itself, generally delaying eruption and increasing the magnetic\nenergy released in each eruption. We also test whether these effects can be\napproximated using a variation of the more computationally efficient\nmagnetofrictional model, finding a number of simple adaptations of the standard\nmagnetofrictional model capture the effect the chromospheric stratification\nwell.",
        "Ceramic materials are widely used in high-strain-rate applications due to\ntheir exceptional strength-to-weight ratio. However, under these extreme\nconditions, spall failure becomes a critical concern, which is driven by a\nlarge hydrostatic tensile stress state. This study introduces a novel two-laser\nsetup to generate controlled hydrostatic stress states at specific locations\nwithin test specimens. By inducing and manipulating shock wave interactions, we\nachieve large hydrostatic compressive and tensile stresses at very\nhigh-strain-rates, enabling the controlled nucleation and growth of nanovoids\nleading to spall failure. Our experiments demonstrate that shock wave\ninterference can precisely trigger spallation at arbitrary locations in the\nspecimen thickness. To further validate our approach, we investigate alumina\nspall failure using molecular dynamics (MD) simulations with a custom-designed\ngraph neural network potential. The MD results show strong agreement with\nexperimentally estimated spall strength. These findings highlight the potential\nof the two-laser technique as a powerful tool for studying the early stages of\nspall failure in ceramics, paving the way for advanced materials testing\nmethodologies.",
        "We prove that bounded conciseness is a closed property in the space of marked\ngroups. As a consequence, we reformulate a conjecture of Fern\\'andez-Alcober\nand Shumyatsky [7] about conciseness in the class of residually finite groups."
      ]
    }
  },
  {
    "id":2411.00561,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"What is a cell type, really? The quest to categorize life's myriad forms.",
    "start_abstract":"The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
      ],
      "abstract":[
        "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "L2R: Learning to Reduce Search Space for Generalizable Neural Routing\n  Solver",
        "The Danger of Overthinking: Examining the Reasoning-Action Dilemma in\n  Agentic Tasks",
        "Perspectives for Direct Interpretability in Multi-Agent Deep\n  Reinforcement Learning",
        "Comprehensive Metapath-based Heterogeneous Graph Transformer for\n  Gene-Disease Association Prediction",
        "Analyzing sequential activity and travel decisions with interpretable\n  deep inverse reinforcement learning",
        "The Explanation Game -- Rekindled (Extended Version)",
        "CoT-VLM4Tar: Chain-of-Thought Guided Vision-Language Models for Traffic\n  Anomaly Resolution",
        "Interpretable Machine Learning for Oral Lesion Diagnosis through\n  Prototypical Instances Identification",
        "FlowAgent: Achieving Compliance and Flexibility for Workflow Agents",
        "Saarthi: The First AI Formal Verification Engineer",
        "Neuro-Symbolic AI in 2024: A Systematic Review",
        "Rethinking Relation Extraction: Beyond Shortcuts to Generalization with\n  a Debiased Benchmark",
        "Aligning Instruction Tuning with Pre-training",
        "Punch Out Model Synthesis: A Stochastic Algorithm for Constraint Based\n  Tiling Generation",
        "A Unifying View of Linear Function Approximation in Off-Policy RL\n  Through Matrix Splitting and Preconditioning",
        "Cup Products on Hochschild Cohomology of Hopf-Galois Extensions.pdf",
        "Enhancing Retrieval Systems with Inference-Time Logical Reasoning",
        "Evaluation of Hate Speech Detection Using Large Language Models and\n  Geographical Contextualization",
        "Verification of Bit-Flip Attacks against Quantized Neural Networks",
        "Parallelizing Multi-objective A* Search",
        "The Role of Artificial Intelligence in Enhancing Insulin Recommendations\n  and Therapy Outcomes",
        "Algorithmical Aspects of Some Bio Inspired Operations",
        "Proximal Flow Inspired Multi-Step Methods",
        "E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot\n  Object Customization",
        "Fast-COS: A Fast One-Stage Object Detector Based on Reparameterized\n  Attention Vision Transformer for Autonomous Driving",
        "One-Loop QCD Corrections to $\\bar{u}d \\rightarrow t\\bar{t}W$ at\n  $\\mathcal{O}(\\varepsilon^2)$",
        "Unconstrained Body Recognition at Altitude and Range: Comparing Four\n  Approaches",
        "Realization of Two-dimensional Discrete Time Crystals with Anisotropic\n  Heisenberg Coupling"
      ],
      "abstract":[
        "Constructive neural combinatorial optimization (NCO) has attracted growing\nresearch attention due to its ability to solve complex routing problems without\nrelying on handcrafted rules. However, existing NCO methods face significant\nchallenges in generalizing to large-scale problems due to high computational\ncomplexity and inefficient capture of structural patterns. To address this\nissue, we propose a novel learning-based search space reduction method that\nadaptively selects a small set of promising candidate nodes at each step of the\nconstructive NCO process. Unlike traditional methods that rely on fixed\nheuristics, our selection model dynamically prioritizes nodes based on learned\npatterns, significantly reducing the search space while maintaining solution\nquality. Experimental results demonstrate that our method, trained solely on\n100-node instances from uniform distribution, generalizes remarkably well to\nlarge-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) instances with up to 1 million nodes from the uniform\ndistribution and over 80K nodes from other distributions.",
        "Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving\ncapabilities, but their effectiveness in interactive environments can be\nlimited. This paper introduces and analyzes overthinking in LRMs. A phenomenon\nwhere models favor extended internal reasoning chains over environmental\ninteraction. Through experiments on software engineering tasks using SWE Bench\nVerified, we observe three recurring patterns: Analysis Paralysis, Rogue\nActions, and Premature Disengagement. We propose a framework to study these\nbehaviors, which correlates with human expert assessments, and analyze 4018\ntrajectories. We observe that higher overthinking scores correlate with\ndecreased performance, with reasoning models exhibiting stronger tendencies\ntoward overthinking compared to non-reasoning models. Our analysis reveals that\nsimple efforts to mitigate overthinking in agentic environments, such as\nselecting the solution with the lower overthinking score, can improve model\nperformance by almost 30% while reducing computational costs by 43%. These\nresults suggest that mitigating overthinking has strong practical implications.\nWe suggest that by leveraging native function-calling capabilities and\nselective reinforcement learning overthinking tendencies could be mitigated. We\nalso open-source our evaluation framework and dataset to facilitate research in\nthis direction at https:\/\/github.com\/AlexCuadron\/Overthinking.",
        "Multi-Agent Deep Reinforcement Learning (MADRL) was proven efficient in\nsolving complex problems in robotics or games, yet most of the trained models\nare hard to interpret. While learning intrinsically interpretable models\nremains a prominent approach, its scalability and flexibility are limited in\nhandling complex tasks or multi-agent dynamics. This paper advocates for direct\ninterpretability, generating post hoc explanations directly from trained\nmodels, as a versatile and scalable alternative, offering insights into agents'\nbehaviour, emergent phenomena, and biases without altering models'\narchitectures. We explore modern methods, including relevance backpropagation,\nknowledge edition, model steering, activation patching, sparse autoencoders and\ncircuit discovery, to highlight their applicability to single-agent,\nmulti-agent, and training process challenges. By addressing MADRL\ninterpretability, we propose directions aiming to advance active topics such as\nteam identification, swarm coordination and sample efficiency.",
        "Discovering gene-disease associations is crucial for understanding disease\nmechanisms, yet identifying these associations remains challenging due to the\ntime and cost of biological experiments. Computational methods are increasingly\nvital for efficient and scalable gene-disease association prediction.\nGraph-based learning models, which leverage node features and network\nrelationships, are commonly employed for biomolecular predictions. However,\nexisting methods often struggle to effectively integrate node features,\nheterogeneous structures, and semantic information. To address these\nchallenges, we propose COmprehensive MEtapath-based heterogeneous graph\nTransformer(COMET) for predicting gene-disease associations. COMET integrates\ndiverse datasets to construct comprehensive heterogeneous networks,\ninitializing node features with BioGPT. We define seven Metapaths and utilize a\ntransformer framework to aggregate Metapath instances, capturing global\ncontexts and long-distance dependencies. Through intra- and inter-metapath\naggregation using attention mechanisms, COMET fuses latent vectors from\nmultiple Metapaths to enhance GDA prediction accuracy. Our method demonstrates\nsuperior robustness compared to state-of-the-art approaches. Ablation studies\nand visualizations validate COMET's effectiveness, providing valuable insights\nfor advancing human health research.",
        "Travel demand modeling has shifted from aggregated trip-based models to\nbehavior-oriented activity-based models because daily trips are essentially\ndriven by human activities. To analyze the sequential activity-travel\ndecisions, deep inverse reinforcement learning (DIRL) has proven effective in\nlearning the decision mechanisms by approximating a reward function to\nrepresent preferences and a policy function to replicate observed behavior\nusing deep neural networks (DNNs). However, most existing research has focused\non using DIRL to enhance only prediction accuracy, with limited exploration\ninto interpreting the underlying decision mechanisms guiding sequential\ndecision-making. To address this gap, we introduce an interpretable DIRL\nframework for analyzing activity-travel decision processes, bridging the gap\nbetween data-driven machine learning and theory-driven behavioral models. Our\nproposed framework adapts an adversarial IRL approach to infer the reward and\npolicy functions of activity-travel behavior. The policy function is\ninterpreted through a surrogate interpretable model based on choice\nprobabilities from the policy function, while the reward function is\ninterpreted by deriving both short-term rewards and long-term returns for\nvarious activity-travel patterns. Our analysis of real-world travel survey data\nreveals promising results in two key areas: (i) behavioral pattern insights\nfrom the policy function, highlighting critical factors in decision-making and\nvariations among socio-demographic groups, and (ii) behavioral preference\ninsights from the reward function, indicating the utility individuals gain from\nspecific activity sequences.",
        "Recent work demonstrated the existence of critical flaws in the current use\nof Shapley values in explainable AI (XAI), i.e. the so-called SHAP scores.\nThese flaws are significant in that the scores provided to a human\ndecision-maker can be misleading. Although these negative results might appear\nto indicate that Shapley values ought not be used in XAI, this paper argues\notherwise. Concretely, this paper proposes a novel definition of SHAP scores\nthat overcomes existing flaws. Furthermore, the paper outlines a practically\nefficient solution for the rigorous estimation of the novel SHAP scores.\nPreliminary experimental results confirm our claims, and further underscore the\nflaws of the current SHAP scores.",
        "With the acceleration of urbanization, modern urban traffic systems are\nbecoming increasingly complex, leading to frequent traffic anomalies. These\nanomalies encompass not only common traffic jams but also more challenging\nissues such as phantom traffic jams, intersection deadlocks, and accident\nliability analysis, which severely impact traffic flow, vehicular safety, and\noverall transportation efficiency. Currently, existing solutions primarily rely\non manual intervention by traffic police or artificial intelligence-based\ndetection systems. However, these methods often suffer from response delays and\ninconsistent management due to inadequate resources, while AI detection\nsystems, despite enhancing efficiency to some extent, still struggle to handle\ncomplex traffic anomalies in a real-time and precise manner. To address these\nissues, we propose CoT-VLM4Tar: (Chain of Thought Visual-Language Model for\nTraffic Anomaly Resolution), this innovative approach introduces a new\nchain-of-thought to guide the VLM in analyzing, reasoning, and generating\nsolutions for traffic anomalies with greater reasonable and effective solution,\nand to evaluate the performance and effectiveness of our method, we developed a\nclosed-loop testing framework based on the CARLA simulator. Furthermore, to\nensure seamless integration of the solutions generated by the VLM with the\nCARLA simulator, we implement an itegration module that converts these\nsolutions into executable commands. Our results demonstrate the effectiveness\nof VLM in the resolution of real-time traffic anomalies, providing a\nproof-of-concept for its integration into autonomous traffic management\nsystems.",
        "Decision-making processes in healthcare can be highly complex and\nchallenging. Machine Learning tools offer significant potential to assist in\nthese processes. However, many current methodologies rely on complex models\nthat are not easily interpretable by experts. This underscores the need to\ndevelop interpretable models that can provide meaningful support in clinical\ndecision-making. When approaching such tasks, humans typically compare the\nsituation at hand to a few key examples and representative cases imprinted in\ntheir memory. Using an approach which selects such exemplary cases and grounds\nits predictions on them could contribute to obtaining high-performing\ninterpretable solutions to such problems. To this end, we evaluate PivotTree,\nan interpretable prototype selection model, on an oral lesion detection\nproblem, specifically trying to detect the presence of neoplastic, aphthous and\ntraumatic ulcerated lesions from oral cavity images. We demonstrate the\nefficacy of using such method in terms of performance and offer a qualitative\nand quantitative comparison between exemplary cases and ground-truth prototypes\nselected by experts.",
        "The integration of workflows with large language models (LLMs) enables\nLLM-based agents to execute predefined procedures, enhancing automation in\nreal-world applications. Traditional rule-based methods tend to limit the\ninherent flexibility of LLMs, as their predefined execution paths restrict the\nmodels' action space, particularly when the unexpected, out-of-workflow (OOW)\nqueries are encountered. Conversely, prompt-based methods allow LLMs to fully\ncontrol the flow, which can lead to diminished enforcement of procedural\ncompliance. To address these challenges, we introduce FlowAgent, a novel agent\nframework designed to maintain both compliance and flexibility. We propose the\nProcedure Description Language (PDL), which combines the adaptability of\nnatural language with the precision of code to formulate workflows. Building on\nPDL, we develop a comprehensive framework that empowers LLMs to manage OOW\nqueries effectively, while keeping the execution path under the supervision of\na set of controllers. Additionally, we present a new evaluation methodology to\nrigorously assess an LLM agent's ability to handle OOW scenarios, going beyond\nroutine flow compliance tested in existing benchmarks. Experiments on three\ndatasets demonstrate that FlowAgent not only adheres to workflows but also\neffectively manages OOW queries, highlighting its dual strengths in compliance\nand flexibility. The code is available at\nhttps:\/\/github.com\/Lightblues\/FlowAgent.",
        "Recently, Devin has made a significant buzz in the Artificial Intelligence\n(AI) community as the world's first fully autonomous AI software engineer,\ncapable of independently developing software code. Devin uses the concept of\nagentic workflow in Generative AI (GenAI), which empowers AI agents to engage\nin a more dynamic, iterative, and self-reflective process. In this paper, we\npresent a similar fully autonomous AI formal verification engineer, Saarthi,\ncapable of verifying a given RTL design end-to-end using an agentic workflow.\nWith Saarthi, verification engineers can focus on more complex problems, and\nverification teams can strive for more ambitious goals. The domain-agnostic\nimplementation of Saarthi makes it scalable for use across various domains such\nas RTL design, UVM-based verification, and others.",
        "Background: The field of Artificial Intelligence has undergone cyclical\nperiods of growth and decline, known as AI summers and winters. Currently, we\nare in the third AI summer, characterized by significant advancements and\ncommercialization, particularly in the integration of Symbolic AI and\nSub-Symbolic AI, leading to the emergence of Neuro-Symbolic AI.\n  Methods: The review followed the PRISMA methodology, utilizing databases such\nas IEEE Explore, Google Scholar, arXiv, ACM, and SpringerLink. The inclusion\ncriteria targeted peer-reviewed papers published between 2020 and 2024. Papers\nwere screened for relevance to Neuro-Symbolic AI, with further inclusion based\non the availability of associated codebases to ensure reproducibility.\n  Results: From an initial pool of 1,428 papers, 167 met the inclusion criteria\nand were analyzed in detail. The majority of research efforts are concentrated\nin the areas of learning and inference (63%), logic and reasoning (35%), and\nknowledge representation (44%). Explainability and trustworthiness are less\nrepresented (28%), with Meta-Cognition being the least explored area (5%). The\nreview identifies significant interdisciplinary opportunities, particularly in\nintegrating explainability and trustworthiness with other research areas.\n  Conclusion: Neuro-Symbolic AI research has seen rapid growth since 2020, with\nconcentrated efforts in learning and inference. Significant gaps remain in\nexplainability, trustworthiness, and Meta-Cognition. Addressing these gaps\nthrough interdisciplinary research will be crucial for advancing the field\ntowards more intelligent, reliable, and context-aware AI systems.",
        "Benchmarks are crucial for evaluating machine learning algorithm performance,\nfacilitating comparison and identifying superior solutions. However, biases\nwithin datasets can lead models to learn shortcut patterns, resulting in\ninaccurate assessments and hindering real-world applicability. This paper\naddresses the issue of entity bias in relation extraction tasks, where models\ntend to rely on entity mentions rather than context. We propose a debiased\nrelation extraction benchmark DREB that breaks the pseudo-correlation between\nentity mentions and relation types through entity replacement. DREB utilizes\nBias Evaluator and PPL Evaluator to ensure low bias and high naturalness,\nproviding a reliable and accurate assessment of model generalization in entity\nbias scenarios. To establish a new baseline on DREB, we introduce MixDebias, a\ndebiasing method combining data-level and model training-level techniques.\nMixDebias effectively improves model performance on DREB while maintaining\nperformance on the original dataset. Extensive experiments demonstrate the\neffectiveness and robustness of MixDebias compared to existing methods,\nhighlighting its potential for improving the generalization ability of relation\nextraction models. We will release DREB and MixDebias publicly.",
        "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose Aligning Instruction Tuning\nwith Pre-training (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs.",
        "As an artistic aid in tiled level design, Constraint Based Tiling Generation\n(CBTG) algorithms can help to automatically create level realizations from a\nset of tiles and placement constraints. Merrell's Modify in Blocks Model\nSynthesis (MMS) and Gumin's Wave Function Collapse (WFC) have been proposed as\nConstraint Based Tiling Generation (CBTG) algorithms that work well for many\nscenarios but have limitations in problem size, problem setup and solution\nbiasing. We present Punch Out Model Synthesis (POMS), a Constraint Based Tiling\nGeneration algorithm, that can handle large problem sizes, requires minimal\nassumptions for setup and can help mitigate solution biasing. POMS attempts to\nresolve indeterminate grid regions by trying to progressively realize\nsub-blocks, performing a stochastic boundary erosion on previously resolved\nregions should sub-block resolution fail. We highlight the results of running a\nreference implementation on different tile sets and discuss a tile correlation\nlength, implied by the tile constraints, and its role in choosing an\nappropriate block size to aid POMS in successfully finding grid realizations.",
        "Traditionally, TD and FQI are viewed as differing in the number of updates\ntoward the target value function: TD makes one update, FQI makes an infinite\nnumber, and Partial Fitted Q-Iteration (PFQI) performs a finite number, such as\nthe use of a target network in Deep Q-Networks (DQN) in the OPE setting. This\nperspective, however, fails to capture the convergence connections between\nthese algorithms and may lead to incorrect conclusions, for example, that the\nconvergence of TD implies the convergence of FQI. In this paper, we focus on\nlinear value function approximation and offer a new perspective, unifying TD,\nFQI, and PFQI as the same iterative method for solving the Least Squares\nTemporal Difference (LSTD) system, but using different preconditioners and\nmatrix splitting schemes. TD uses a constant preconditioner, FQI employs a\ndata-feature adaptive preconditioner, and PFQI transitions between the two.\nThen, we reveal that in the context of linear function approximation,\nincreasing the number of updates under the same target value function\nessentially represents a transition from using a constant preconditioner to\ndata-feature adaptive preconditioner. This unifying perspective also simplifies\nthe analyses of the convergence conditions for these algorithms and clarifies\nmany issues. Consequently, we fully characterize the convergence of each\nalgorithm without assuming specific properties of the chosen features (e.g.,\nlinear independence). We also examine how common assumptions about feature\nrepresentations affect convergence, and discover new conditions on features\nthat are important for convergence. These convergence conditions allow us to\nestablish the convergence connections between these algorithms and to address\nimportant questions.",
        "In this paper, we give an explicit chain map, which induces the algebra\nisomorphism between the Hochschild cohomology ${\\bf HH}^{\\bullet}(B)$ and the\n$H$-invariant subalgebra ${\\bf H}^{\\bullet}(A, B)^{H}$ under two mild\nhypotheses, where $H$ is a finite dimensional semisimple Hopf algebra and $B$\nis an $H$-Galois extension of $A$. In particular, the smash product $B=A\\#H$\nalways satisfies the mild hypotheses. The isomorphism between ${\\bf\nHH}^{\\bullet}(A\\#H)$ and ${\\bf H}^{\\bullet}(A, A\\#H)^{H}$ generalizes the\nclassical result of group actions. As an application, Hochschild cohomology and\ncup product of the smash product of the quantum $(-1)$-plane and Kac--Paljutkin\nHopf algebra are computed.",
        "Traditional retrieval methods rely on transforming user queries into vector\nrepresentations and retrieving documents based on cosine similarity within an\nembedding space. While efficient and scalable, this approach often fails to\nhandle complex queries involving logical constructs such as negations,\nconjunctions, and disjunctions. In this paper, we propose a novel\ninference-time logical reasoning framework that explicitly incorporates logical\nreasoning into the retrieval process. Our method extracts logical reasoning\nstructures from natural language queries and then composes the individual\ncosine similarity scores to formulate the final document scores. This approach\nenables the retrieval process to handle complex logical reasoning without\ncompromising computational efficiency. Our results on both synthetic and\nreal-world benchmarks demonstrate that the proposed method consistently\noutperforms traditional retrieval methods across different models and datasets,\nsignificantly improving retrieval performance for complex queries.",
        "The proliferation of hate speech on social media is one of the serious issues\nthat is bringing huge impacts to society: an escalation of violence,\ndiscrimination, and social fragmentation. The problem of detecting hate speech\nis intrinsically multifaceted due to cultural, linguistic, and contextual\ncomplexities and adversarial manipulations. In this study, we systematically\ninvestigate the performance of LLMs on detecting hate speech across\nmultilingual datasets and diverse geographic contexts. Our work presents a new\nevaluation framework in three dimensions: binary classification of hate speech,\ngeography-aware contextual detection, and robustness to adversarially generated\ntext. Using a dataset of 1,000 comments from five diverse regions, we evaluate\nthree state-of-the-art LLMs: Llama2 (13b), Codellama (7b), and DeepSeekCoder\n(6.7b). Codellama had the best binary classification recall with 70.6% and an\nF1-score of 52.18%, whereas DeepSeekCoder had the best performance in\ngeographic sensitivity, correctly detecting 63 out of 265 locations. The tests\nfor adversarial robustness also showed significant weaknesses; Llama2\nmisclassified 62.5% of manipulated samples. These results bring to light the\ntrade-offs between accuracy, contextual understanding, and robustness in the\ncurrent versions of LLMs. This work has thus set the stage for developing\ncontextually aware, multilingual hate speech detection systems by underlining\nkey strengths and limitations, therefore offering actionable insights for\nfuture research and real-world applications.",
        "In the rapidly evolving landscape of neural network security, the resilience\nof neural networks against bit-flip attacks (i.e., an attacker maliciously\nflips an extremely small amount of bits within its parameter storage memory\nsystem to induce harmful behavior), has emerged as a relevant area of research.\nExisting studies suggest that quantization may serve as a viable defense\nagainst such attacks. Recognizing the documented susceptibility of real-valued\nneural networks to such attacks and the comparative robustness of quantized\nneural networks (QNNs), in this work, we introduce BFAVerifier, the first\nverification framework designed to formally verify the absence of bit-flip\nattacks or to identify all vulnerable parameters in a sound and rigorous\nmanner. BFAVerifier comprises two integral components: an abstraction-based\nmethod and an MILP-based method. Specifically, we first conduct a reachability\nanalysis with respect to symbolic parameters that represent the potential\nbit-flip attacks, based on a novel abstract domain with a sound guarantee. If\nthe reachability analysis fails to prove the resilience of such attacks, then\nwe encode this verification problem into an equivalent MILP problem which can\nbe solved by off-the-shelf solvers. Therefore, BFAVerifier is sound, complete,\nand reasonably efficient. We conduct extensive experiments, which demonstrate\nits effectiveness and efficiency across various network architectures,\nquantization bit-widths, and adversary capabilities.",
        "The Multi-objective Shortest Path (MOSP) problem is a classic network\noptimization problem that aims to find all Pareto-optimal paths between two\npoints in a graph with multiple edge costs. Recent studies on multi-objective\nsearch with A* (MOA*) have demonstrated superior performance in solving\ndifficult MOSP instances. This paper presents a novel search framework that\nallows efficient parallelization of MOA* with different objective orders. The\nframework incorporates a unique upper bounding strategy that helps the search\nreduce the problem's dimensionality to one in certain cases. Experimental\nresults demonstrate that the proposed framework can enhance the performance of\nrecent A*-based solutions, with the speed-up proportional to the problem\ndimension.",
        "The growing worldwide incidence of diabetes requires more effective\napproaches for managing blood glucose levels. Insulin delivery systems have\nadvanced significantly, with artificial intelligence (AI) playing a key role in\nimproving their precision and adaptability. AI algorithms, particularly those\nbased on reinforcement learning, allow for personalised insulin dosing by\ncontinuously adapting to an individual's responses. Despite these advancements,\nchallenges such as data privacy, algorithm transparency, and accessibility\nstill need to be addressed. Continued progress and validation in AI-driven\ninsulin delivery systems promise to improve therapy outcomes further, offering\npeople more effective and individualised management of their diabetes. This\npaper presents an overview of current strategies, key challenges, and future\ndirections.",
        "This thesis investigates three biologically inspired operations:\nprefix-suffix duplication, bounded prefix-suffix duplication, and\nprefix-suffix-square completion. Duplication, a common genetic mutation,\ninvolves repeating DNA sequences and is modeled here as formal operations on\nwords. The prefix-suffix duplication generates non-context-free languages, even\nfrom simple initial words. To better reflect biological processes, we propose a\nbounded variant that limits duplication length, resolving unsolved problems and\naligning with biochemical realities.\n  We also introduce the prefix-suffix-square completion operation, which\ngenerates squares at sequence ends. This operation enables the generation of\ninfinite words such as Fibonacci, Period-doubling, and Thue-Morse, which\ncontain squares but avoid higher exponent repetitions, highlighting unique\nstructural properties. In contrast, prefix-suffix duplication cannot generate\ncertain infinite words, such as Thue-Morse, but can produce cube-free words.\n  Additionally, we address the detection of gapped repeats and\npalindromes-structures important in DNA and RNA analysis. These involve\nrepeating or reversed factors flanking a central gap. Previous studies imposed\nconstraints on gap length or arm-gap relationships; we extend this by solving\nthe problem in three novel settings. This work advances theoretical insights\ninto biologically inspired operations and their computational applications in\ngenetic modeling.",
        "We investigate a family of approximate multi-step proximal point methods,\nframed as implicit linear discretizations of gradient flow. The resulting\nmethods are multi-step proximal point methods, with similar computational cost\nin each update as the proximal point method. We explore several optimization\nmethods where applying an approximate multistep proximal points method results\nin improved convergence behavior. We also include convergence analysis for the\nproposed method in several problem settings: quadratic problems, general\nproblems that are strongly or weakly (non)convex, and accelerated results for\nalternating projections.",
        "We propose E-MD3C ($\\underline{E}$fficient $\\underline{M}$asked\n$\\underline{D}$iffusion Transformer with Disentangled $\\underline{C}$onditions\nand $\\underline{C}$ompact $\\underline{C}$ollector), a highly efficient\nframework for zero-shot object image customization. Unlike prior works reliant\non resource-intensive Unet architectures, our approach employs lightweight\nmasked diffusion transformers operating on latent patches, offering\nsignificantly improved computational efficiency. The framework integrates three\ncore components: (1) an efficient masked diffusion transformer for processing\nautoencoder latents, (2) a disentangled condition design that ensures\ncompactness while preserving background alignment and fine details, and (3) a\nlearnable Conditions Collector that consolidates multiple inputs into a compact\nrepresentation for efficient denoising and learning. E-MD3C outperforms the\nexisting approach on the VITON-HD dataset across metrics such as PSNR, FID,\nSSIM, and LPIPS, demonstrating clear advantages in parameters, memory\nefficiency, and inference speed. With only $\\frac{1}{4}$ of the parameters, our\nTransformer-based 468M model delivers $2.5\\times$ faster inference and uses\n$\\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent\ndiffusion model.",
        "The perception system is a a critical role of an autonomous driving system\nfor ensuring safety. The driving scene perception system fundamentally\nrepresents an object detection task that requires achieving a balance between\naccuracy and processing speed. Many contemporary methods focus on improving\ndetection accuracy but often overlook the importance of real-time detection\ncapabilities when computational resources are limited. Thus, it is vital to\ninvestigate efficient object detection strategies for driving scenes. This\npaper introduces Fast-COS, a novel single-stage object detection framework\ncrafted specifically for driving scene applications. The research initiates\nwith an analysis of the backbone, considering both macro and micro\narchitectural designs, yielding the Reparameterized Attention Vision\nTransformer (RAViT). RAViT utilizes Reparameterized Multi-Scale Depth-Wise\nConvolution (RepMSDW) and Reparameterized Self-Attention (RepSA) to enhance\ncomputational efficiency and feature extraction. In extensive tests across GPU,\nedge, and mobile platforms, RAViT achieves 81.4% Top-1 accuracy on the\nImageNet-1K dataset, demonstrating significant throughput improvements over\ncomparable backbone models such as ResNet, FastViT, RepViT, and\nEfficientFormer. Additionally, integrating RepMSDW into a feature pyramid\nnetwork forms RepFPN, enabling fast and multi-scale feature fusion. Fast-COS\nenhances object detection in driving scenes, attaining an AP50 score of 57.2%\non the BDD100K dataset and 80.0% on the TJU-DHD Traffic dataset. It surpasses\nleading models in efficiency, delivering up to 75.9% faster GPU inference and\n1.38 higher throughput on edge devices compared to FCOS, YOLOF, and RetinaNet.\nThese findings establish Fast-COS as a highly scalable and reliable solution\nsuitable for real-time applications, especially in resource-limited\nenvironments like autonomous driving systems",
        "We present a computation of the one-loop QCD corrections to top-quark pair\nproduction in association with a $W$ boson, including terms up to order\n$\\varepsilon^2$ in dimensional regularization. Providing a first glimpse into\nthe complexity of the corresponding two-loop amplitude, this result is a first\nstep towards a description of this process at next-to-next-to-leading order\n(NNLO) in QCD. We perform a tensor decomposition and express the corresponding\nform factors in terms of a basis of independent special functions with compact\nrational coefficients, providing a structured framework for future\ndevelopments. In addition, we derive an explicit analytic representation of the\nform factors, valid up to order $\\varepsilon^0$, expressed in terms of\nlogarithms and dilogarithms. For the complete set of special functions\nrequired, we obtain a semi-numerical solution based on generalized power series\nexpansion.",
        "This study presents an investigation of four distinct approaches to long-term\nperson identification using body shape. Unlike short-term re-identification\nsystems that rely on temporary features (e.g., clothing), we focus on learning\npersistent body shape characteristics that remain stable over time. We\nintroduce a body identification model based on a Vision Transformer (ViT) (Body\nIdentification from Diverse Datasets, BIDDS) and on a Swin-ViT model\n(Swin-BIDDS). We also expand on previous approaches based on the Linguistic and\nNon-linguistic Core ResNet Identity Models (LCRIM and NLCRIM), but with\nimproved training. All models are trained on a large and diverse dataset of\nover 1.9 million images of approximately 5k identities across 9 databases.\nPerformance was evaluated on standard re-identification benchmark datasets\n(MARS, MSMT17, Outdoor Gait, DeepChange) and on an unconstrained dataset that\nincludes images at a distance (from close-range to 1000m), at altitude (from an\nunmanned aerial vehicle, UAV), and with clothing change. A comparative analysis\nacross these models provides insights into how different backbone architectures\nand input image sizes impact long-term body identification performance across\nreal-world conditions.",
        "A discrete time crystal (DTC) is the paradigmatic example of a phase of\nmatter that occurs exclusively in systems out of equilibrium. This phenomenon\nis characterized by the spontaneous symmetry breaking of discrete\ntime-translation and provides a rich playground to study a fundamental question\nin statistical physics: what mechanism allows for driven quantum systems to\nexhibit emergent behavior that deviates from their counterparts with\ntime-independent evolution? Unlike equilibrium phases, DTCs exhibit macroscopic\nmanifestations of coherent quantum dynamics, challenging the conventional\nnarrative that thermodynamic behavior universally erases quantum signatures.\nHowever, due to the difficulty of simulating these systems with either\nclassical or quantum computers, previous studies have been limited to a set of\nmodels with Ising-like couplings -- and mostly only in one dimension -- thus\nprecluding our understanding of the existence (or not) of DTCs in models with\ninteractions that closely align with what occurs in nature. In this work, by\ncombining the latest generation of IBM quantum processors with state-of-the-art\ntensor network methods, we are able to demonstrate the existence of a DTC in a\ntwo-dimensional system governed by anisotropic Heisenberg interactions. Our\ncomprehensive analysis reveals a rich phase diagram encompassing spin-glass,\nergodic, and time-crystalline phases, highlighting the tunability of these\nphases through multiple control parameters. Crucially, our results emphasize\nthe interplay of initialization, interaction anisotropy, and driving protocols\nin stabilizing the DTC phase. By extending the study of Floquet matter beyond\nsimplified models, we lay the groundwork for exploring how driven systems\nbridge the gap between quantum coherence and emergent non-equilibrium\nthermodynamics."
      ]
    }
  },
  {
    "id":2411.00922,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images",
    "start_abstract":"Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
      ],
      "abstract":[
        "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Fast and Accurate Gigapixel Pathological Image Classification with\n  Hierarchical Distillation Multi-Instance Learning",
        "ProDehaze: Prompting Diffusion Models Toward Faithful Image Dehazing",
        "Fully Exploiting Vision Foundation Model's Profound Prior Knowledge for\n  Generalizable RGB-Depth Driving Scene Parsing",
        "Adding Additional Control to One-Step Diffusion with Joint Distribution\n  Matching",
        "Exploring Transferable Homogeneous Groups for Compositional Zero-Shot\n  Learning",
        "Soft Knowledge Distillation with Multi-Dimensional Cross-Net Attention\n  for Image Restoration Models Compression",
        "GOAL: Global-local Object Alignment Learning",
        "ViDDAR: Vision Language Model-Based Task-Detrimental Content Detection\n  for Augmented Reality",
        "DashCop: Automated E-ticket Generation for Two-Wheeler Traffic\n  Violations Using Dashcam Videos",
        "Hybrid State-Space and GRU-based Graph Tokenization Mamba for\n  Hyperspectral Image Classification",
        "HFMF: Hierarchical Fusion Meets Multi-Stream Models for Deepfake\n  Detection",
        "VLForgery Face Triad: Detection, Localization and Attribution via\n  Multimodal Large Language Models",
        "A Fusion Model for Artwork Identification Based on Convolutional Neural\n  Networks and Transformers",
        "HI-MaNGA: Results from (21cm-HI) single-dish observations of MaNGA\n  Survey Galaxies",
        "\\'El\\'ements de comptage sur les g\\'en\\'erateurs du groupe modulaire et\n  les $\\lambda$-quiddit\\'es",
        "Dark Energy Survey Year 6 Results: Synthetic-source Injection Across the\n  Full Survey Using Balrog",
        "On the Commuting Problem of Toeplitz Operators on the Harmonic Bergman\n  Space",
        "Generalization Performance of Hypergraph Neural Networks",
        "COARSE: Collaborative Pseudo-Labeling with Coarse Real Labels for\n  Off-Road Semantic Segmentation",
        "Properties of the one-component Coulomb gas on a sphere with two\n  macroscopic external charges",
        "Norm-one points in convex combinations of relatively weakly open subsets\n  of the unit ball in the spaces $L_1(\\mu,X)$",
        "Logic.py: Bridging the Gap between LLMs and Constraint Solvers",
        "Deriving motivic coactions and single-valued maps at genus zero from\n  zeta generators",
        "AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to\n  Counteract Alpha Decay",
        "Relative knot probabilities in confined lattice polygons",
        "Global branching of solutions to ODEs and integrability",
        "Kink dynamics for the Yang-Mills field in an extremal\n  Reissner-Nordstr\\\"om black hole",
        "Multimodal Emotion Recognition and Sentiment Analysis in Multi-Party\n  Conversation Contexts"
      ],
      "abstract":[
        "Although multi-instance learning (MIL) has succeeded in pathological image\nclassification, it faces the challenge of high inference costs due to\nprocessing numerous patches from gigapixel whole slide images (WSIs). To\naddress this, we propose HDMIL, a hierarchical distillation multi-instance\nlearning framework that achieves fast and accurate classification by\neliminating irrelevant patches. HDMIL consists of two key components: the\ndynamic multi-instance network (DMIN) and the lightweight instance\npre-screening network (LIPN). DMIN operates on high-resolution WSIs, while LIPN\noperates on the corresponding low-resolution counterparts. During training,\nDMIN are trained for WSI classification while generating attention-score-based\nmasks that indicate irrelevant patches. These masks then guide the training of\nLIPN to predict the relevance of each low-resolution patch. During testing,\nLIPN first determines the useful regions within low-resolution WSIs, which\nindirectly enables us to eliminate irrelevant regions in high-resolution WSIs,\nthereby reducing inference time without causing performance degradation. In\naddition, we further design the first Chebyshev-polynomials-based\nKolmogorov-Arnold classifier in computational pathology, which enhances the\nperformance of HDMIL through learnable activation layers. Extensive experiments\non three public datasets demonstrate that HDMIL outperforms previous\nstate-of-the-art methods, e.g., achieving improvements of 3.13% in AUC while\nreducing inference time by 28.6% on the Camelyon16 dataset.",
        "Recent approaches using large-scale pretrained diffusion models for image\ndehazing improve perceptual quality but often suffer from hallucination issues,\nproducing unfaithful dehazed image to the original one. To mitigate this, we\npropose ProDehaze, a framework that employs internal image priors to direct\nexternal priors encoded in pretrained models. We introduce two types of\n\\textit{selective} internal priors that prompt the model to concentrate on\ncritical image areas: a Structure-Prompted Restorer in the latent space that\nemphasizes structure-rich regions, and a Haze-Aware Self-Correcting Refiner in\nthe decoding process to align distributions between clearer input regions and\nthe output. Extensive experiments on real-world datasets demonstrate that\nProDehaze achieves high-fidelity results in image dehazing, particularly in\nreducing color shifts. Our code is at https:\/\/github.com\/TianwenZhou\/ProDehaze.",
        "Recent vision foundation models (VFMs), typically based on Vision Transformer\n(ViT), have significantly advanced numerous computer vision tasks. Despite\ntheir success in tasks focused solely on RGB images, the potential of VFMs in\nRGB-depth driving scene parsing remains largely under-explored. In this\narticle, we take one step toward this emerging research area by investigating a\nfeasible technique to fully exploit VFMs for generalizable RGB-depth driving\nscene parsing. Specifically, we explore the inherent characteristics of RGB and\ndepth data, thereby presenting a Heterogeneous Feature Integration Transformer\n(HFIT). This network enables the efficient extraction and integration of\ncomprehensive heterogeneous features without re-training ViTs. Relative depth\nprediction results from VFMs, used as inputs to the HFIT side adapter, overcome\nthe limitations of the dependence on depth maps. Our proposed HFIT demonstrates\nsuperior performance compared to all other traditional single-modal and\ndata-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the\nCityscapes and KITTI Semantics datasets. We believe this novel strategy paves\nthe way for future innovations in VFM-based data-fusion techniques for driving\nscene parsing. Our source code is publicly available at\nhttps:\/\/mias.group\/HFIT.",
        "While diffusion distillation has enabled one-step generation through methods\nlike Variational Score Distillation, adapting distilled models to emerging new\ncontrols -- such as novel structural constraints or latest user preferences --\nremains challenging. Conventional approaches typically requires modifying the\nbase diffusion model and redistilling it -- a process that is both\ncomputationally intensive and time-consuming. To address these challenges, we\nintroduce Joint Distribution Matching (JDM), a novel approach that minimizes\nthe reverse KL divergence between image-condition joint distributions. By\nderiving a tractable upper bound, JDM decouples fidelity learning from\ncondition learning. This asymmetric distillation scheme enables our one-step\nstudent to handle controls unknown to the teacher model and facilitates\nimproved classifier-free guidance (CFG) usage and seamless integration of human\nfeedback learning (HFL). Experimental results demonstrate that JDM surpasses\nbaseline methods such as multi-step ControlNet by mere one-step in most cases,\nwhile achieving state-of-the-art performance in one-step text-to-image\nsynthesis through improved usage of CFG or HFL integration.",
        "Conditional dependency present one of the trickiest problems in Compositional\nZero-Shot Learning, leading to significant property variations of the same\nstate (object) across different objects (states). To address this problem,\nexisting approaches often adopt either all-to-one or one-to-one representation\nparadigms. However, these extremes create an imbalance in the seesaw between\ntransferability and discriminability, favoring one at the expense of the other.\nComparatively, humans are adept at analogizing and reasoning in a hierarchical\nclustering manner, intuitively grouping categories with similar properties to\nform cohesive concepts. Motivated by this, we propose Homogeneous Group\nRepresentation Learning (HGRL), a new perspective formulates state (object)\nrepresentation learning as multiple homogeneous sub-group representation\nlearning. HGRL seeks to achieve a balance between semantic transferability and\ndiscriminability by adaptively discovering and aggregating categories with\nshared properties, learning distributed group centers that retain\ngroup-specific discriminative features. Our method integrates three core\ncomponents designed to simultaneously enhance both the visual and prompt\nrepresentation capabilities of the model. Extensive experiments on three\nbenchmark datasets validate the effectiveness of our method.",
        "Transformer-based encoder-decoder models have achieved remarkable success in\nimage-to-image transfer tasks, particularly in image restoration. However,\ntheir high computational complexity-manifested in elevated FLOPs and parameter\ncounts-limits their application in real-world scenarios. Existing knowledge\ndistillation methods in image restoration typically employ lightweight student\nmodels that directly mimic the intermediate features and reconstruction results\nof the teacher, overlooking the implicit attention relationships between them.\nTo address this, we propose a Soft Knowledge Distillation (SKD) strategy that\nincorporates a Multi-dimensional Cross-net Attention (MCA) mechanism for\ncompressing image restoration models. This mechanism facilitates interaction\nbetween the student and teacher across both channel and spatial dimensions,\nenabling the student to implicitly learn the attention matrices. Additionally,\nwe employ a Gaussian kernel function to measure the distance between student\nand teacher features in kernel space, ensuring stable and efficient feature\nlearning. To further enhance the quality of reconstructed images, we replace\nthe commonly used L1 or KL divergence loss with a contrastive learning loss at\nthe image level. Experiments on three tasks-image deraining, deblurring, and\ndenoising-demonstrate that our SKD strategy significantly reduces computational\ncomplexity while maintaining strong image restoration capabilities.",
        "Vision-language models like CLIP have shown impressive capabilities in\naligning images and text, but they often struggle with lengthy and detailed\ntext descriptions because of their training focus on short and concise\ncaptions. We present GOAL (Global-local Object Alignment Learning), a novel\nfine-tuning method that enhances CLIP's ability to handle lengthy text by\nleveraging both global and local semantic alignments between image and lengthy\ntext. Our approach consists of two key components: Local Image-Sentence\nMatching (LISM), which identifies corresponding pairs between image segments\nand descriptive sentences, and Token Similarity-based Learning (TSL), which\nefficiently propagates local element attention through these matched pairs.\nEvaluating GOAL on three new benchmarks for image-lengthy text retrieval, we\ndemonstrate significant improvements over baseline CLIP fine-tuning,\nestablishing a simple yet effective approach for adapting CLIP to detailed\ntextual descriptions. Through extensive experiments, we show that our method's\nfocus on local semantic alignment alongside global context leads to more\nnuanced and representative embeddings, particularly beneficial for tasks\nrequiring fine-grained understanding of lengthy text descriptions.",
        "In Augmented Reality (AR), virtual content enhances user experience by\nproviding additional information. However, improperly positioned or designed\nvirtual content can be detrimental to task performance, as it can impair users'\nability to accurately interpret real-world information. In this paper we\nexamine two types of task-detrimental virtual content: obstruction attacks, in\nwhich virtual content prevents users from seeing real-world objects, and\ninformation manipulation attacks, in which virtual content interferes with\nusers' ability to accurately interpret real-world information. We provide a\nmathematical framework to characterize these attacks and create a custom\nopen-source dataset for attack evaluation. To address these attacks, we\nintroduce ViDDAR (Vision language model-based Task-Detrimental content Detector\nfor Augmented Reality), a comprehensive full-reference system that leverages\nVision Language Models (VLMs) and advanced deep learning techniques to monitor\nand evaluate virtual content in AR environments, employing a user-edge-cloud\narchitecture to balance performance with low latency. To the best of our\nknowledge, ViDDAR is the first system to employ VLMs for detecting\ntask-detrimental content in AR settings. Our evaluation results demonstrate\nthat ViDDAR effectively understands complex scenes and detects task-detrimental\ncontent, achieving up to 92.15% obstruction detection accuracy with a detection\nlatency of 533 ms, and an 82.46% information manipulation content detection\naccuracy with a latency of 9.62 s.",
        "Motorized two-wheelers are a prevalent and economical means of\ntransportation, particularly in the Asia-Pacific region. However, hazardous\ndriving practices such as triple riding and non-compliance with helmet\nregulations contribute significantly to accident rates. Addressing these\nviolations through automated enforcement mechanisms can enhance traffic safety.\nIn this paper, we propose DashCop, an end-to-end system for automated E-ticket\ngeneration. The system processes vehicle-mounted dashcam videos to detect\ntwo-wheeler traffic violations. Our contributions include: (1) a novel\nSegmentation and Cross-Association (SAC) module to accurately associate riders\nwith their motorcycles, (2) a robust cross-association-based tracking algorithm\noptimized for the simultaneous presence of riders and motorcycles, and (3) the\nRideSafe-400 dataset, a comprehensive annotated dashcam video dataset for\ntriple riding and helmet rule violations. Our system demonstrates significant\nimprovements in violation detection, validated through extensive evaluations on\nthe RideSafe-400 dataset.",
        "Hyperspectral image (HSI) classification plays a pivotal role in domains such\nas environmental monitoring, agriculture, and urban planning. However, it faces\nsignificant challenges due to the high-dimensional nature of the data and the\ncomplex spectral-spatial relationships inherent in HSI. Traditional methods,\nincluding conventional machine learning and convolutional neural networks\n(CNNs), often struggle to effectively capture these intricate spectral-spatial\nfeatures and global contextual information. Transformer-based models, while\npowerful in capturing long-range dependencies, often demand substantial\ncomputational resources, posing challenges in scenarios where labeled datasets\nare limited, as is commonly seen in HSI applications. To overcome these\nchallenges, this work proposes GraphMamba, a hybrid model that combines\nspectral-spatial token generation, graph-based token prioritization, and\ncross-attention mechanisms. The model introduces a novel hybridization of\nstate-space modeling and Gated Recurrent Units (GRU), capturing both linear and\nnonlinear spatial-spectral dynamics. GraphMamba enhances the ability to model\ncomplex spatial-spectral relationships while maintaining scalability and\ncomputational efficiency across diverse HSI datasets. Through comprehensive\nexperiments, we demonstrate that GraphMamba outperforms existing\nstate-of-the-art models, offering a scalable and robust solution for complex\nHSI classification tasks.",
        "The rapid progress in deep generative models has led to the creation of\nincredibly realistic synthetic images that are becoming increasingly difficult\nto distinguish from real-world data. The widespread use of Variational Models,\nDiffusion Models, and Generative Adversarial Networks has made it easier to\ngenerate convincing fake images and videos, which poses significant challenges\nfor detecting and mitigating the spread of misinformation. As a result,\ndeveloping effective methods for detecting AI-generated fakes has become a\npressing concern. In our research, we propose HFMF, a comprehensive two-stage\ndeepfake detection framework that leverages both hierarchical cross-modal\nfeature fusion and multi-stream feature extraction to enhance detection\nperformance against imagery produced by state-of-the-art generative AI models.\nThe first component of our approach integrates vision Transformers and\nconvolutional nets through a hierarchical feature fusion mechanism. The second\ncomponent of our framework combines object-level information and a fine-tuned\nconvolutional net model. We then fuse the outputs from both components via an\nensemble deep neural net, enabling robust classification performances. We\ndemonstrate that our architecture achieves superior performance across diverse\ndataset benchmarks while maintaining calibration and interoperability.",
        "Faces synthesized by diffusion models (DMs) with high-quality and\ncontrollable attributes pose a significant challenge for Deepfake detection.\nMost state-of-the-art detectors only yield a binary decision, incapable of\nforgery localization, attribution of forgery methods, and providing analysis on\nthe cause of forgeries. In this work, we integrate Multimodal Large Language\nModels (MLLMs) within DM-based face forensics, and propose a fine-grained\nanalysis triad framework called VLForgery, that can 1) predict falsified facial\nimages; 2) locate the falsified face regions subjected to partial synthesis;\nand 3) attribute the synthesis with specific generators. To achieve the above\ngoals, we introduce VLF (Visual Language Forensics), a novel and diverse\nsynthesis face dataset designed to facilitate rich interactions between Visual\nand Language modalities in MLLMs. Additionally, we propose an extrinsic\nknowledge-guided description method, termed EkCot, which leverages knowledge\nfrom the image generation pipeline to enable MLLMs to quickly capture image\ncontent. Furthermore, we introduce a low-level vision comparison pipeline\ndesigned to identify differential features between real and fake that MLLMs can\ninherently understand. These features are then incorporated into EkCot,\nenhancing its ability to analyze forgeries in a structured manner, following\nthe sequence of detection, localization, and attribution. Extensive experiments\ndemonstrate that VLForgery outperforms other state-of-the-art forensic\napproaches in detection accuracy, with additional potential for falsified\nregion localization and attribution analysis.",
        "The identification of artwork is crucial in areas like cultural heritage\nprotection, art market analysis, and historical research. With the advancement\nof deep learning, Convolutional Neural Networks (CNNs) and Transformer models\nhave become key tools for image classification. While CNNs excel in local\nfeature extraction, they struggle with global context, and Transformers are\nstrong in capturing global dependencies but weak in fine-grained local details.\nTo address these challenges, this paper proposes a fusion model combining CNNs\nand Transformers for artwork identification. The model first extracts local\nfeatures using CNNs, then captures global context with a Transformer, followed\nby a feature fusion mechanism to enhance classification accuracy. Experiments\non Chinese and oil painting datasets show the fusion model outperforms\nindividual CNN and Transformer models, improving classification accuracy by\n9.7% and 7.1%, respectively, and increasing F1 scores by 0.06 and 0.05. The\nresults demonstrate the model's effectiveness and potential for future\nimprovements, such as multimodal integration and architecture optimization.",
        "In a poster presentation for IAU Symposium 392: \"Neutral hydrogen in and\naround galaxies in the SKA era\", we gave an overview of the HI-MaNGA project\nwhich is working to obtain complementary information about the cold gas\n(neutral hydrogen traced by the radio 21cm line) content of Mapping Nearby\nGalaxies at Apache Point Observatory (MaNGA) sample galaxies. MaNGA, part of\nthe fourth incarnation of the Sloan Digital Sky Surveys (SDSS-IV), obtained\nspatially resolved spectral maps for 10,000 nearby galaxies selected to create\na representative sample out of the SDSS Main Galaxy Sample. MaNGA data have\nprovided a census of the stellar and ionized gas content of these galaxies, as\nwell as kinematics of both stars and gas. Adding HI information via the\nHI-MaNGA program, which has observed or collected 21cm line data for 70% of the\nfull MaNGA sample, has been crucial for a number of applications, but\nespecially understanding the physical mechanisms that regulate gas accretion,\nand through that star formation and quenching of star formation. This\nconference proceedings article accompanies the release of the DR3 version of\nHI-MaNGA data.",
        "The aim of this article is to count the $n$-tuples of positive integers\n$(a_{1},\\ldots,a_{n})$ solutions of the equation $\\begin{pmatrix} a_{n} & -1\n\\\\[4pt] 1 & 0 \\end{pmatrix} \\begin{pmatrix} a_{n-1} & -1 \\\\[4pt] 1 & 0\n\\end{pmatrix} \\cdots \\begin{pmatrix} a_{1} & -1 \\\\[4pt] 1 & 0 \\end{pmatrix}=\\pm\nM$ when $M$ is equal to the generators of the modular group $S=\\begin{pmatrix}\n0 & -1 \\\\[4pt] 1 & 0 \\end{pmatrix}$ and $T=\\begin{pmatrix} 1 & 1 \\\\[4pt] 0 & 1\n\\end{pmatrix}$. To count these elements, we will study the\n$\\lambda$-quiddities, which are the solutions of the equation in the case\n$M=Id$ (related to Coxeter's friezes), whose last component is fixed.",
        "Synthetic source injection (SSI), the insertion of sources into pixel-level\non-sky images, is a powerful method for characterizing object detection and\nmeasurement in wide-field, astronomical imaging surveys. Within the Dark Energy\nSurvey (DES), SSI plays a critical role in characterizing all necessary\nalgorithms used in converting images to catalogs, and in deriving quantities\nneeded for the cosmology analysis, such as object detection rates, galaxy\nredshift estimation, galaxy magnification, star-galaxy classification, and\nphotometric performance. We present here a source injection catalog of $146$\nmillion injections spanning the entire 5000 deg$^2$ DES footprint, generated\nusing the Balrog SSI pipeline. Through this sample, we demonstrate that the DES\nYear 6 (Y6) image processing pipeline provides accurate estimates of the object\nproperties, for both galaxies and stars, at the percent-level, and we highlight\nspecific regimes where the accuracy is reduced. We then show the consistency\nbetween SSI and data catalogs, for all galaxy samples developed within the weak\nlensing and galaxy clustering analyses of DES Y6. The consistency between the\ntwo catalogs also extends to their correlations with survey observing\nproperties (seeing, airmass, depth, extinction, etc.). Finally, we highlight a\nnumber of applications of this catalog to the DES Y6 cosmology analysis. This\ndataset is the largest SSI catalog produced at this fidelity and will serve as\na key testing ground for exploring the utility of SSI catalogs in upcoming\nsurveys such as the Vera C. Rubin Observatory Legacy Survey of Space and Time.",
        "In this paper, we provide a complete characterization of bounded Toeplitz\noperators $T_f$ on the harmonic Bergman space of the unit disk, where the\nsymbol $f$ has a polar decomposition truncated above, that commute with\n$T_{z+\\bar{g}}$, for a bounded analytic function $g$.",
        "Hypergraph neural networks have been promising tools for handling learning\ntasks involving higher-order data, with notable applications in web graphs,\nsuch as modeling multi-way hyperlink structures and complex user interactions.\nYet, their generalization abilities in theory are less clear to us. In this\npaper, we seek to develop margin-based generalization bounds for four\nrepresentative classes of hypergraph neural networks, including\nconvolutional-based methods (UniGCN), set-based aggregation (AllDeepSets),\ninvariant and equivariant transformations (M-IGN), and tensor-based approaches\n(T-MPHN). Through the PAC-Bayes framework, our results reveal the manner in\nwhich hypergraph structure and spectral norms of the learned weights can affect\nthe generalization bounds, where the key technical challenge lies in developing\nnew perturbation analysis for hypergraph neural networks, which offers a\nrigorous understanding of how variations in the model's weights and hypergraph\nstructure impact its generalization behavior. Our empirical study examines the\nrelationship between the practical performance and theoretical bounds of the\nmodels over synthetic and real-world datasets. One of our primary observations\nis the strong correlation between the theoretical bounds and empirical loss,\nwith statistically significant consistency in most cases.",
        "Autonomous off-road navigation faces challenges due to diverse, unstructured\nenvironments, requiring robust perception with both geometric and semantic\nunderstanding. However, scarce densely labeled semantic data limits\ngeneralization across domains. Simulated data helps, but introduces domain\nadaptation issues. We propose COARSE, a semi-supervised domain adaptation\nframework for off-road semantic segmentation, leveraging sparse, coarse\nin-domain labels and densely labeled out-of-domain data. Using pretrained\nvision transformers, we bridge domain gaps with complementary pixel-level and\npatch-level decoders, enhanced by a collaborative pseudo-labeling strategy on\nunlabeled data. Evaluations on RUGD and Rellis-3D datasets show significant\nimprovements of 9.7\\% and 8.4\\% respectively, versus only using coarse data.\nTests on real-world off-road vehicle data in a multi-biome setting further\ndemonstrate COARSE's applicability.",
        "The one-component Coulomb gas on the sphere, consisting on $N$ unit charges\ninteracting via a logarithmic potential, and in the presence of two external\ncharges each of strength proportional to $N$, is considered. There are two\nspherical caps naturally associated with the external charges, giving rise to\ntwo distinct phases depending on them not overlapping (post-critical) or\noverlapping (pre-critical). The equilibrium measure in the post-critical phase\nis known from earlier work. We determine the equilibrium measure in the\npre-critical phase using a particular conformal map, with the parameters\ntherein specified in terms of a root of a certain fourth order polynomial. This\nis used to determine the exact form of the electrostatic energy for the\npre-critical phase. Using a duality relation from random matrix theory, the\npartition function for the Coulomb gas at the inverse temperature $\\beta = 2$\ncan be expanded for large $N$ in the post-critical phase, and in a scaling\nregion of the post and pre-critical boundary. For the pre-critical phase, the\nduality identity implies a relation between two electrostatic energies, one for\nthe present sphere system, and the other for a certain constrained log-gas\nrelating to the Jacobi unitary ensemble.",
        "In a paper published in 2020 in Studia Mathematica, Abrahamsen et al. proved\nthat in the real space $L_1(\\mu)$, where $\\mu$ is a non-zero $\\sigma$-finite\n(countably additive non-negative) measure, norm-one elements in finite convex\ncombinations of relatively weakly open subsets of the unit ball are interior\npoints of these convex combinations in the relative weak topology. In this\npaper that result is generalised by proving that the same is true in the (real\nor complex) Lebesgue--Bochner spaces $L_1(\\mu,X)$ where $X$ is a weakly\nuniformly rotund Banach space.",
        "We present a novel approach to formalise and solve search-based problems\nusing large language models, which significantly improves upon previous\nstate-of-the-art results. We demonstrate the efficacy of this approach on the\nlogic puzzles benchmark ZebraLogicBench. Instead of letting the LLM attempt to\ndirectly solve the puzzles, our method prompts the model to formalise the\nproblem in a logic-focused domain-specific language (DSL) called Logic.py. This\nformalised representation is then solved using a constraint solver, leveraging\nthe strengths of both the language model and the solver. Our approach achieves\na remarkable 65% absolute improvement over the baseline performance of Llama\n3.1 70B on ZebraLogicBench, setting a new state-of-the-art with an accuracy of\nover 90%. This significant advancement demonstrates the potential of combining\nlanguage models with domain-specific languages and auxiliary tools on\ntraditionally challenging tasks for LLMs.",
        "Multiple polylogarithms are equipped with rich algebraic structures including\nthe motivic coaction and the single-valued map which both found fruitful\napplications in high-energy physics. In recent work arXiv:2312.00697, the\ncurrent authors presented a conjectural reformulation of the motivic coaction\nand the single-valued map via zeta generators, certain operations on\nnon-commuting variables in suitable generating series of multiple\npolylogarithms. In this work, the conjectures of the reference will be proven\nfor multiple polylogarithms that depend on any number of variables on the\nRiemann sphere.",
        "Alpha mining, a critical component in quantitative investment, focuses on\ndiscovering predictive signals for future asset returns in increasingly complex\nfinancial markets. However, the pervasive issue of alpha decay, where factors\nlose their predictive power over time, poses a significant challenge for alpha\nmining. Traditional methods like genetic programming face rapid alpha decay\nfrom overfitting and complexity, while approaches driven by Large Language\nModels (LLMs), despite their promise, often rely too heavily on existing\nknowledge, creating homogeneous factors that worsen crowding and accelerate\ndecay. To address this challenge, we propose AlphaAgent, an autonomous\nframework that effectively integrates LLM agents with ad hoc regularizations\nfor mining decay-resistant alpha factors. AlphaAgent employs three key\nmechanisms: (i) originality enforcement through a similarity measure based on\nabstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor\nalignment via LLM-evaluated semantic consistency between market hypotheses and\ngenerated factors, and (iii) complexity control via AST-based structural\nconstraints, preventing over-engineered constructions that are prone to\noverfitting. These mechanisms collectively guide the alpha generation process\nto balance originality, financial rationale, and adaptability to evolving\nmarket conditions, mitigating the risk of alpha decay. Extensive evaluations\nshow that AlphaAgent outperforms traditional and LLM-based methods in\nmitigating alpha decay across bull and bear markets, consistently delivering\nsignificant alpha in Chinese CSI 500 and US S&P 500 markets over the past four\nyears. Notably, AlphaAgent showcases remarkable resistance to alpha decay,\nelevating the potential for yielding powerful factors.",
        "In this paper we examine the relative knotting probabilities in a lattice\nmodel of ring polymers confined in a cavity. The model is of a lattice knot of\nsize $n$ in the cubic lattice, confined to a cube of side-length $L$ and with\nvolume $V=(L{+}1)^3$ sites. We use Monte Carlo algorithms to approximately\nenumerate the number of conformations of lattice knots in the confining cube.\nIf $p_{n,L}(K)$ is the number of conformations of a lattice polygon of length\n$n$ and knot type $K$ in a cube of volume $L^3$, then the relative knotting\nprobability of a lattice polygon to have knot type $K$, relative to the\nprobability that the polygon is the unknot (the trivial knot, denoted by\n$0_1$), is $\\rho_{n,L}(K\/0_1) = p_{n,L}(K)\/p_{n,L}(0_1)$. We determine\n$\\rho_{n,L}(K\/0_1)$ for various knot types $K$ up to six crossing knots. Our\ndata show that these relative knotting probabilities are small so that the\nmodel is dominated by lattice polygons of knot type the unknot. Moreover, if\nthe concentration of the monomers of the lattice knot is $\\varphi = n\/V$, then\nthe relative knot probability increases with $\\varphi$ along a curve that\nflattens as the Hamiltonian state is approached.",
        "We consider a natural generalisation of the Painlev\\'e property and use it to\nidentify the known integrable cases of the Lane-Emden equation with a real\npositive index. We classify certain first-order ordinary differential equations\nwith this property and find necessary conditions for a large family of\nsecond-order equations. We consider ODEs such that, given any simply connected\ndomain $\\Omega$ not containing fixed singularities of the equation, the Riemann\nsurface of any solution obtained by analytic continuation along curves in\n$\\Omega$ has a finite number of sheets over $\\Omega$.",
        "Considered in this work is the Yang-Mills field in an extremal\nReissner-Nordstr\\\"om black hole, a physically motivated mathematical model\nintroduced by Bizo\\'n and Kahl. The kink is a fundamental, strongly unstable\nstationary solution in this non-perturbative, variable coefficients model, with\na polynomial tail and no explicit form. In this paper, we introduce and extend\nseveral virial techniques, adapt them to the inhomogeneous medium setting, and\nconstruct a finite codimensional manifold of the energy space where the kink is\nasymptotically stable. In particular, we handle, using virial techniques, the\nemergence of a weak threshold resonance in the description of the stable\nmanifold.",
        "Emotion recognition and sentiment analysis are pivotal tasks in speech and\nlanguage processing, particularly in real-world scenarios involving\nmulti-party, conversational data. This paper presents a multimodal approach to\ntackle these challenges on a well-known dataset. We propose a system that\nintegrates four key modalities\/channels using pre-trained models: RoBERTa for\ntext, Wav2Vec2 for speech, a proposed FacialNet for facial expressions, and a\nCNN+Transformer architecture trained from scratch for video analysis. Feature\nembeddings from each modality are concatenated to form a multimodal vector,\nwhich is then used to predict emotion and sentiment labels. The multimodal\nsystem demonstrates superior performance compared to unimodal approaches,\nachieving an accuracy of 66.36% for emotion recognition and 72.15% for\nsentiment analysis."
      ]
    }
  },
  {
    "id":2411.00922,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation",
    "start_abstract":"Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
      ],
      "abstract":[
        "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Extinction and Extirpation Conditions in Coalescent and Ecotonal\n  Metacommunities",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Collective inference of the truth of propositions from crowd probability\n  judgments",
        "Sharp stability for critical points of the Sobolev inequality in the\n  absence of bubbling",
        "Free energy profiles for chemical reactions in solution from\n  high-dimensional neural network potentials: The case of the Strecker\n  synthesis",
        "Efficient Multivariate Robust Mean Estimation Under Mean-Shift\n  Contamination",
        "Mathematical Modelling of Mechanotransduction via RhoA Signalling\n  Pathways",
        "Nonparametric Smoothing of Directional and Axial Data",
        "The EFT Bootstrap at Finite $M_{PL}$",
        "Preconditioning for a Cahn-Hilliard-Navier-Stokes model for morphology\n  formation in organic solar cells",
        "Four-quark scatterings in QCD III",
        "Vacuum stress between conducting plates: the curved spacetime version",
        "Euler--Poincar\\'e reduction and the Kelvin--Noether theorem for discrete\n  mechanical systems with advected parameters and additional dynamics",
        "On the Prescribed Ricci Curvature of Noncompact Homogeneous Spaces with\n  Two Isotropy Summands",
        "Active bacterial baths in droplets",
        "Seeing Stereotypes",
        "Supercell environments using GridRad-Severe and the HRRR: Addressing\n  discrepancies between prior tornado datasets",
        "Indigenous Mathematics I. Smoke Telegraphy"
      ],
      "abstract":[
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Here we present extinction, extirpation and coexistence conditions where \/\nwhen two communities combine. We consider one specific model where two\ncommunities coalesce, and another model where the communities coexist side by\nside, blending in a transitionary zone called the ecotone. Specifically, (1) we\nanalytically calculate the shifts in abundances as a function of mixing\nstrength. (2) Obtain a critical value for the mixing strength leading to\nextinction. (3) Derive an inequality condition for full coexistent mixing. (4)\nfind how the individual communities penetrate into one other as a function of\nmixing strength. (5) derive the conditions for one species to cross the ecotone\nand invade an neighboring community and (6) conditions for a native species to\nget extirpated. Lastly, (7) we spatially investigate the species richness\nwithin the ecotone and derive a condition that determines whether the ecotone\nwill have higher or lower richness compared to its surrounding habitats.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty.",
        "When $u$ is close to a single Talenti bubble $v$ of the $p$-Sobolev\ninequality, we show that\n  \\begin{equation*}\n  \\|Du-Dv\\|_{L^p(\\mathbb{R}^n)}^{\\max\\{1,p-1\\}}\\le C \\|-{\\rm\ndiv}(|Du|^{p-2}Du)-|u|^{p^*-2}u\\|_{W^{-1,q}(\\mathbb{R}^n)}, \\end{equation*}\nwhere $C=C(n,p)>0$. This estimate provides a sharp stability estimate for the\nStruwe-type decomposition in the single bubble case, generalizing the result of\nCiraolo, Figalli, and Maggi \\cite{CFM2018} (focusing on the case $p=2$) to the\narbitrary $p$. Also, in the Sobolev setting, this answers an open problem\nraised by Zhou and Zou in \\cite[Remark 1.17]{ZZ2023}.",
        "Machine learning potentials (MLPs) have become a popular tool in chemistry\nand materials science as they combine the accuracy of electronic structure\ncalculations with the high computational efficiency of analytic potentials.\nMLPs are particularly useful for computationally demanding simulations such as\nthe determination of free energy profiles governing chemical reactions in\nsolution, but to date such applications are still rare. In this work we show\nhow umbrella sampling simulations can be combined with active learning of\nhigh-dimensional neural network potentials (HDNNPs) to construct free energy\nprofiles in a systematic way. For the example of the first step of Strecker\nsynthesis of glycine in aqueous solution we provide a detailed analysis of the\nimproving quality of HDNNPs for datasets of increasing size. We find that next\nto the typical quantification of energy and force errors with respect to the\nunderlying density functional theory data also the long-term stability of the\nsimulations and the convergence of physical properties should be rigorously\nmonitored to obtain reliable and converged free energy profiles of chemical\nreactions in solution.",
        "We study the algorithmic problem of robust mean estimation of an identity\ncovariance Gaussian in the presence of mean-shift contamination. In this\ncontamination model, we are given a set of points in $\\mathbb{R}^d$ generated\ni.i.d. via the following process. For a parameter $\\alpha<1\/2$, the $i$-th\nsample $x_i$ is obtained as follows: with probability $1-\\alpha$, $x_i$ is\ndrawn from $\\mathcal{N}(\\mu, I)$, where $\\mu \\in \\mathbb{R}^d$ is the target\nmean; and with probability $\\alpha$, $x_i$ is drawn from $\\mathcal{N}(z_i, I)$,\nwhere $z_i$ is unknown and potentially arbitrary. Prior work characterized the\ninformation-theoretic limits of this task. Specifically, it was shown that, in\ncontrast to Huber contamination, in the presence of mean-shift contamination\nconsistent estimation is possible. On the other hand, all known robust\nestimators in the mean-shift model have running times exponential in the\ndimension. Here we give the first computationally efficient algorithm for\nhigh-dimensional robust mean estimation with mean-shift contamination that can\ntolerate a constant fraction of outliers. In particular, our algorithm has\nnear-optimal sample complexity, runs in sample-polynomial time, and\napproximates the target mean to any desired accuracy. Conceptually, our result\ncontributes to a growing body of work that studies inference with respect to\nnatural noise models lying in between fully adversarial and random settings.",
        "We derive and simulate a mathematical model for mechanotransduction related\nto the Rho GTPase signalling pathway. The model addresses the bidirectional\ncoupling between signalling processes and cell mechanics. A numerical method\nbased on bulk-surface finite elements is proposed for the approximation of the\ncoupled system of nonlinear reaction-diffusion equations, defined inside the\ncell and on the cell membrane, and the equations of elasticity. Our simulation\nresults illustrate novel emergent features such as the strong dependence of the\ndynamics on cell shape, a threshold-like response to changes in substrate\nstiffness, and the fact that coupling mechanics and signalling can lead to the\nrobustness of cell deformation to larger changes in substrate stiffness,\nensuring mechanical homeostasis in agreement with experiments.",
        "We discuss generalized linear models for directional data where the\nconditional distribution of the response is a von Mises-Fisher distribution in\narbitrary dimension or a Bingham distribution on the unit circle. To do this\nproperly, we parametrize von Mises-Fisher distributions by Euclidean parameters\nand investigate computational aspects of this parametrization. Then we modify\nthis approach for local polynomial regression as a means of nonparametric\nsmoothing of distributional data. The methods are illustrated with simulated\ndata and a data set from planetary sciences involving covariate vectors on a\nsphere with axial response.",
        "We explore the impact of loop effects on positivity in effective field\ntheories emerging in the infrared from unitary and causal microscopic dynamics.\nFocusing on massless particles coupled to gravity, we address the treatment of\nforward-limit divergences from loop discontinuities and establish necessary\nconditions for maintaining computational control in perturbation theory. While\nloop effects remain small, ensuring consistency in our approach leads to a\nsignificant impact on bounds, even at tree level.",
        "We present a model for the morphology evolution of printed organic solar\ncells which occurs during the drying of a mixture of polymer, the non-fullerene\nacceptor and the solvent. Our model uses a phase field approach coupled to a\nNavier-Stokes equation describing the macroscopic movement of the fluid.\nAdditionally, we incorporate the evaporation process of the solvent using an\nAllen-Cahn equation.\n  The model is discretized using a finite-element approach with a semi-implicit\ndiscretization in time. The resulting (non)linear systems are coupled and of\nlarge dimensionality. We present a preconditioned iterative scheme to solve\nthem robustly with respect to changes in the discretization parameters. We\nillustrate that the preconditioned solver shows parameter-robust iteration\nnumbers and that the model qualitatively captures the behavior of the film\nmorphology during drying.",
        "We study the full infrared dynamics of 2+1 flavour QCD with the functional\nrenormalisation group approach. We resolve self-consistently the glue dynamics\nas well as the dynamics of chiral symmetry breaking. The computation hosts no\nphenomenological parameter or external input. The only ultraviolet input\nparameters are the physical ones in QCD: the light and strange quark masses.\nThey are adjusted to the physical ratios of the pion and kaon masses, divided\nby the pion decay constant. The results for other observables of current\nfirst-principles computations are in quantitative agreement with the physical\nones. This work completes the series of papers, initiated and furthered in\n[1,2], on dynamical chiral symmetry breaking and the emergence of mesonic bound\nstates within the functional renormalisation group. As a first application we\ndiscuss the formation of light mesonic bound states. Amongst other applications\nsuch as the phase structure of QCD, the current work paves the way for studying\nQCD parton distribution functions within the functional renormalisation group\napproach to first-principles QCD.",
        "Brown and Maclay \\cite{Brown} found the energy-momentum tensor for the\nCasimir effect of parallel plates in 1969. We find its curved spacetime version\nin a static background using the point splitting regularization method.\nPrevious results in the literature are reinforced and some consequences\ndiscussed.",
        "The Euler--Poincar\\'e equations, firstly introduced by Henri Poincar\\'e in\n1901, arise from the application of Lagrangian mechanics to systems on Lie\ngroups that exhibit symmetries, particularly in the contexts of classical\nmechanics and fluid dynamics. These equations have been extended to various\nsettings, such as semidirect products, advected parameters, and field theory,\nand have been widely applied to mechanics and physics. In this paper, we\nintroduce the discrete Euler--Poincar\\'e reduction for discrete Lagrangian\nsystems on Lie groups with advected parameters and additional dynamics,\nutilizing the group difference map technique. Specifically, the group\ndifference map is defined using either the Cayley transform or the matrix\nexponential. The continuous and discrete Kelvin--Noether theorems are extended\naccordingly, that account for Kelvin--Noether quantities of the corresponding\ncontinuous and discrete Euler--Poincar\\'e equations. As an application, we show\nboth continuous and discrete Euler--Poincar\\'e formulations about the dynamics\nof underwater vehicles, followed by numerical simulations. Numerical results\nillustrate the scheme's ability to preserve geometric properties over extended\ntime intervals, highlighting its potential for practical applications in the\ncontrol and navigation of underwater vehicles, as well as in other domains.",
        "This work studies simply connected, noncompact $G\/H$ in which $G$ is\nsemi-simple, $H$ is connected, and $G\/H$ has two irreducible summands. Here, we\nclassify all such spaces and we provide solutions to the so-called Prescribed\nRicci Curvature problem for all such spaces.",
        "Suspensions of self-propelled objects represent a novel paradigm in colloidal\nscience. In such active baths traditional concepts, such as Brownian motion,\nfluctuation-dissipation relations, and work extraction from heat reservoirs,\nmust be extended beyond the conventional framework of thermal baths. Unlike\nthermal baths, which are characterized by a single parameter, the temperature,\nthe fundamental descriptors of an active bath remain elusive, especially in\nconfined environments. In this study, buoyant, passive tracers are employed as\ngeneralized probes to investigate an active bath comprising motile bacteria\nconfined within a droplet. We demonstrate that momentum transfer from the bath\nto the tracer can be effectively described as colored noise, characterized by\ntemporal memory and an enhanced effective diffusivity significantly larger\ncompared to thermal Brownian motion values. Using a stochastic analytical\nframework, we extract the temporal memory and diffusivity parameters that\ndefine such an active bath. Notably, the diffusivity scales linearly with\nbacterial concentration, modulated by a factor representing the role of\nconfinement, expressed as the ratio of the confining radius to the probe\nradius. This finding, while still awaiting a complete theoretical explanation,\noffers new insights into the transport properties of confined active baths and\npaves the way for a deeper understanding of active emulsions driven by confined\nactive matter.",
        "Reliance on stereotypes is a persistent feature of human decision-making and\nhas been extensively documented in educational settings, where it can shape\nstudents' confidence, performance, and long-term human capital accumulation.\nWhile effective techniques exist to mitigate these negative effects, a crucial\nfirst step is to establish whether teachers can recognize stereotypes in their\nprofessional environment. We introduce the Stereotype Identification Test\n(SIT), a novel survey tool that asks teachers to evaluate and comment on the\npresence of stereotypes in images randomly drawn from school textbooks. Their\nresponses are systematically linked to established measures of implicit bias\n(Implicit Association Test, IAT) and explicit bias (survey scales on teaching\nstereotypes and social values). Our findings demonstrate that the SIT is a\nvalid and reliable measure of stereotype recognition. Teachers' ability to\nrecognize stereotypes is linked to trainable traits such as implicit bias\nawareness and inclusive teaching practices. Moreover, providing personalized\nfeedback on implicit bias improves SIT scores by 0.25 standard deviations,\nreinforcing the idea that stereotype recognition is malleable and can be\nenhanced through targeted interventions.",
        "Storm-relative helicity (SRH) is an important ingredient in supercell\ndevelopment, as well as mesocyclone intensity, and is linked to tornadogenesis\nand tornado potential. Derived from the storm-relative wind profile, SRH is\ncomposed of both the vertical wind shear and storm-relative flow. Recent\nstudies have come to conflicting findings regarding whether shallower or deeper\nlayers of SRH have more skill in tornado forecasting. Possible causes of this\ndiscrepancy include the use of observed versus model-based proximity soundings,\nas well as whether the storm-relative wind profile is determined via observed\nversus estimated storm motions. This study uses a new dataset of objectively\nidentified supercells, with observed storm motions, paired with high-resolution\nmodel analyses to address the discrepancies among prior studies. Unlike in\nprevious model-based tornado environmental datasets, the present approach\nreveals substantive differences in storm-relative flow, vertical wind shear,\nand SRH within the low-to-mid-levels between nontornadic and tornadic\nsupercells. Using observed storm motions for storm-relative variables further\nmagnifies differences in the low-to-mid-level storm-relative winds between\nnontornadic and tornadic supercells, ultimately leading to deeper layers of SRH\nhaving more forecast skill than near-ground SRH. Thus, the combination of a\nhigher-resolution model analyses, which better represents the near-storm\nenvironment, with observed storm motions appears to explain why many past\ntornado climatologies using model-based environmental analyses have failed to\nfind significant differences in the storm-relative wind profile. These results\nhelp bridge the gap between previous studies that employed coarser model-based\nanalyses with those that aggregated observed soundings from field projects.",
        "This article is the first in an occasional series for the Australian\nMathematical Society Gazette on diverse aspects and topics of Indigenous\nmathematical knowledge. This is an important, but neglected, part of the\nmathematical heritage of humankind, and as such is the concern of the\nmathematics community as a whole. It is hoped that this and future articles may\nhelp to inspire mathematics researchers, students, and educators at tertiary\nand school levels who are seeking to widen their mathematical horizons and\ndevelop course and research materials of broad cultural relevance.\n  I would like to honour the Mithaka peoples of the Kurrawoolben and\nKirrenderri (Diamantina) and Nooroondinna (Georgina) river channel country of\nsouth-western Qld, Australia. The material in this article does not involve\nculturally restricted knowledge or images, and is shared with respect for the\nMithaka ancestors and their descendants."
      ]
    }
  },
  {
    "id":2411.03522,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Language Models are Few-Shot Learners",
    "start_abstract":"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b20"
      ],
      "title":[
        "Long non-coding RNAs: definitions, functions, challenges and recommendations"
      ],
      "abstract":[
        "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "SPM 25: open source neuroimaging analysis software",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "UNGT: Ultrasound Nasogastric Tube Dataset for Medical Image Analysis",
        "Automating Care by Self-maintainability for Full Laboratory Automation",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Functional Correspondences in the Human and Marmoset Visual Cortex\n  During Movie Watching: Insights from Correlation, Redundancy, and Synergy",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Dynamical phases of short-term memory mechanisms in RNNs",
        "Phase Alignment Enhances Oscillatory Power in Neural Mass Models\n  Optimized for Class Encoding",
        "A comprehensive and reliable protocol for manual segmentation of the\n  human claustrum using high-resolution MRI",
        "Multicellular self-organization in Escherichia coli",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Social hierarchy shapes foraging decisions",
        "Secure Quantum Key Distribution with Room-Temperature Quantum Emitter",
        "Plebanski complex",
        "Extended string-net models with all anyons at finite temperature",
        "Geodesic cycles on the Sphere: $t$-designs and Marcinkiewicz-Zygmund\n  Inequalities",
        "Minimax Optimality of Classical Scaling Under General Noise Conditions",
        "The Effect of the Non-Abelian Quantum Metric on Superfluidity",
        "Global well-posedness and optimal time-decay of 3D full compressible\n  Navier-Stokes system",
        "Origin of switchable quasiparticle-interference chirality in\n  loop-current phase of kagome metals measured by scanning-tunneling-microscopy",
        "Learning the P2D Model for Lithium-Ion Batteries with SOH Detection",
        "Dot to dot: high-$z$ little red dots in $M_{\\rm bh}$-$M_{\\rm \\star}$\n  diagrams with galaxy-morphology-specific scaling relations and nuclear star\n  clusters",
        "Knoop: Practical Enhancement of Knockoff with Over-Parameterization for\n  Variable Selection",
        "Spectral properties of bottomonium at high temperature: a systematic\n  investigation",
        "Cyclical accretion regime change in the slow X-ray pulsar 4U 0114+65\n  observed with Chandra",
        "Bayesian Parameter Shift Rule in Variational Quantum Eigensolvers",
        "Ground state and magnetic transitions of the orthorhombic\n  antiferromagnet CaCo$_2$TeO$_6$"
      ],
      "abstract":[
        "Statistical Parametric Mapping (SPM) is an integrated set of methods for\ntesting hypotheses about the brain's structure and function, using data from\nimaging devices. These methods are implemented in an open source software\npackage, SPM, which has been in continuous development for more than 30 years\nby an international community of developers. This paper reports the release of\nSPM 25.01, a major new version of the software that incorporates novel analysis\nmethods, optimisations of existing methods, as well as improved practices for\nopen science and software development.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "We develop a novel ultrasound nasogastric tube (UNGT) dataset to address the\nlack of public nasogastric tube datasets. The UNGT dataset includes 493 images\ngathered from 110 patients with an average image resolution of approximately\n879 $\\times$ 583. Four structures, encompassing the liver, stomach, tube, and\npancreas are precisely annotated. Besides, we propose a semi-supervised\nadaptive-weighting aggregation medical segmenter to address data limitation and\nimbalance concurrently. The introduced adaptive weighting approach tackles the\nsevere unbalanced challenge by regulating the loss across varying categories as\ntraining proceeds. The presented multiscale attention aggregation block\nbolsters the feature representation by integrating local and global contextual\ninformation. With these, the proposed AAMS can emphasize sparse or small\nstructures and feature enhanced representation ability. We perform extensive\nsegmentation experiments on our UNGT dataset, and the results show that AAMS\noutperforms existing state-of-the-art approaches to varying extents. In\naddition, we conduct comprehensive classification experiments across varying\nstate-of-the-art methods and compare their performance. The dataset and code\nwill be available upon publication at https:\/\/github.com\/NUS-Tim\/UNGT.",
        "The automation of experiments in life sciences and chemistry has\nsignificantly advanced with the development of various instruments and AI\ntechnologies. However, achieving full laboratory automation, where experiments\nconceived by scientists are seamlessly executed in automated laboratories,\nremains a challenge. We identify the lack of automation in planning and\noperational tasks--critical human-managed processes collectively termed\n\"care\"--as a major barrier. Automating care is the key enabler for full\nlaboratory automation. To address this, we propose the concept of\nself-maintainability (SeM): the ability of a laboratory system to autonomously\nadapt to internal and external disturbances, maintaining operational readiness\nakin to living cells. A SeM-enabled laboratory features autonomous recognition\nof its state, dynamic resource and information management, and adaptive\nresponses to unexpected conditions. This shifts the planning and execution of\nexperimental workflows, including scheduling and reagent allocation, from\nhumans to the system. We present a conceptual framework for implementing\nSeM-enabled laboratories, comprising three modules--Requirement manager,\nLabware manager, and Device manager--and a Central manager. SeM not only\nenables scientists to execute envisioned experiments seamlessly but also\nprovides developers with a design concept that drives the technological\ninnovations needed for full automation.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "The world of beauty is deeply connected to the visual cortex, as perception\noften begins with vision in both humans and marmosets. Quantifying functional\ncorrespondences in the visual cortex across species can help us understand how\ninformation is processed in the primate visual cortex, while also providing\ndeeper insights into human visual cortex functions through the study of\nmarmosets. In this study, we measured pairwise and beyond pairwise correlation,\nredundancy, and synergy in movie-driven fMRI data across species. Our first key\nfinding was that humans and marmosets exhibited significant overlaps in\nfunctional synergy. Second, we observed that the strongest functional\ncorrespondences between the human peri-entorhinal and entorhinal cortex (PeEc)\nand the occipitotemporal higher-level visual regions in the marmoset during\nmovie watching reflected a functional synergistic relationship. These regions\nare known to correspond to face-selective areas in both species. Third,\nredundancy measures maintained stable high-order hubs, indicating a steady core\nof shared information processing, while synergy measures revealed a dynamic\nshift from low- to high-level visual regions as interaction increased,\nreflecting adaptive integration. This highlights distinct patterns of\ninformation processing across the visual hierarchy. Ultimately, our results\nreveal the marmoset as a compelling model for investigating visual perception,\ndistinguished by its remarkable functional parallels to the human visual\ncortex.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "Short-term memory is essential for cognitive processing, yet our\nunderstanding of its neural mechanisms remains unclear. A key focus in\nneuroscience has been the study of sequential activity patterns, where neurons\nfire one after another within large networks, to explain how information is\nmaintained. While recurrent connections were shown to drive sequential\ndynamics, a mechanistic understanding of this process still remains unknown. In\nthis work, we first introduce two unique mechanisms that can subserve\nshort-term memory: slow-point manifolds generating direct sequences or limit\ncycles providing temporally localized approximations. Then, through analytical\nmodels, we identify fundamental properties that govern the selection of these\nmechanisms, \\textit{i.e.}, we derive theoretical scaling laws for critical\nlearning rates as a function of the delay period length, beyond which no\nlearning is possible. We empirically verify these observations by training and\nevaluating more than 35,000 recurrent neural networks (RNNs) that we will\npublicly release upon publication. Overall, our work provides new insights into\nshort-term memory mechanisms and proposes experimentally testable predictions\nfor systems neuroscience.",
        "Neural encoding of objects and cognitive states remains an elusive yet\ncrucial aspect of brain function. While traditional feed-forward machine\nlearning neural networks have enormous potential to encode information, modern\narchitectures provide little insight into the brain's mechanisms. In this work,\na Jansen and Rit neural mass model was constructed to encode different sets of\ninputs, aiming to understand how simple neural circuits can represent\ninformation. A genetic algorithm was used to optimize parameters that maximized\nthe differences in responses to particular inputs. These differences in\nresponses manifested as phase-shifted oscillations across the set of inputs. By\ndelivering impulses of excitation synchronized with a particular phase-shifted\noscillation, we demonstrated that the encoded phase could be decoded by\nmeasuring oscillatory power. These findings demonstrate the capability of\nneural dynamical circuits to encode and decode information through phase.",
        "The claustrum is a thin gray matter structure in each brain hemisphere,\ncharacterized by exceptionally high connectivity with nearly all brain regions.\nDespite extensive animal studies on its anatomy and function and growing\nevidence of claustral deficits in neuropsychiatric disorders, its specific\nroles in normal and abnormal human brain function remain largely unknown. This\nis primarily due to its thin and complex morphology, which limits accurate\nanatomical delineation and neural activity isolation in conventional in vivo\nneuroimaging. To facilitate future neuroimaging studies, we developed a\ncomprehensive and reliable manual segmentation protocol based on a\ncellular-resolution brain atlas and high-resolution (0.7^3 mm) MRI data. The\nprotocols involve detailed guidelines to delineate the entire claustrum,\nincluding the inferior parts that have not been clearly described in earlier\nMRI studies. Additionally, we propose a geometric method to parcellate the\nclaustrum into three subregions (the dorsal, ventral, and temporal claustrum)\nalong the superior-to-inferior axis. The mean bilateral claustrum volume in 10\nyoung adults was 3307.5 mm^3, approximately 0.21% of total intracranial volume.\nOur segmentation protocol demonstrated high inter- and intra-rater reliability\n(ICC > 0.89, DSC > 0.85), confirming its replicability. This comprehensive and\nreliable claustrum segmentation protocols will provide a cornerstone for future\nneuroimaging studies of systematic, large-scale investigations of the anatomy\nand the functions of the human claustrum in normal and pathological\npopulations.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "Social foraging is a widespread form of animal foraging in which groups of\nindividuals coordinate their decisions to exploit resources in the environment.\nAnimals show a variety of social structures from egalitarian to hierarchical.\nIn this study, we examine how different forms of social hierarchy shape\nforaging decisions. We developed a mechanistic analytically tractable model to\nstudy the underlying processes of social foraging, tying the microscopic\nindividual to the macroscopic group levels. Based on a stochastic evidence\naccumulation framework, we developed a model of patch-leaving decisions in a\nlarge hierarchical group with leading and following individuals. Across a\nvariety of information sharing mechanisms, we were able to analytically\nquantify emergent collective dynamics. We found that follower-leader dynamics\nthrough observations of leader movements or through counting the number of\nindividuals in a patch confers, for most conditions, a benefit for the\nfollowing individuals by increasing their accuracy in inferring patch richness.\nOn the other hand, misinformation, through the communication of false beliefs\nabout food rewards or patch quality, shows to be detrimental to following\nindividuals, but paradoxically may lead to increased group cohesion. In an era\nwhere there is a huge amount of animal foraging data collected, our model\nprovides a systematic way to conceptualize and understand those data by\nuncovering hidden mechanisms underlying social foraging decisions.",
        "On-demand generation of single photons from solid-state quantum emitters is a\nkey building block for future quantum networks, particularly quantum key\ndistribution (QKD) systems, by enabling higher secure key rates (SKR) and lower\nquantum bit error rates (QBER). In this work, we demonstrate the B92 protocol\nbased on single photons from defects in hexagonal boron nitride (hBN). The\nresults show a sifted key rate (SiKR) of 17.5 kbps with a QBER of 6.49 % at a\ndynamic polarization encoding rate of 40 MHz. Finite-key analysis yields a SKR\nof 7 kbps, as one of the highest SKR obtained for any room-temperature single\nphoton source. Our results highlight the potential of hBN defects in advancing\nquantum communication technologies.",
        "As is very well-known, linearisation of the instanton equations on a\n4-manifold gives rise to an elliptic complex of differential operators, the\ntruncated (twisted) Hodge complex $\\Lambda^0(\\mathfrak{g}) \\to\n\\Lambda^1(\\mathfrak{g})\\to \\Lambda^2_+(\\mathfrak{g})$. Moreover, the\nlinearisation of the full YM equations also fits into this framework, as it is\ngiven by the second map followed by its adjoint. We define and study properties\nof what we call the Pleba\\'nski complex. This is a differential complex that\narises by linearisation of the equations implying that a Riemannian 4-manifold\nis hyper-K\\\"ahler. We recall that these are most naturally stated as the\ncondition that there exists a perfect $\\Sigma^i\\wedge \\Sigma^j\\sim\\delta^{ij}$\ntriple $\\Sigma^i, i=1,2,3$ of 2-forms that are closed $d\\Sigma^i=0$. The\nRiemannian metric is encoded by the 2-forms $\\Sigma^i$. We show that what\nresults is an elliptic differential complex $TM \\to S\\to E\\times \\Lambda^1 \\to\nE$, where $S$ is the tangent space to the space of perfect triples, and\n$E=\\mathbb{R}^3$. We also show that, as in the case with instanton equations,\nthe full Einstein equations $Ric=0$ also fit into this framework, their\nlinearisation being given by the second map followed by its adjoint. Our second\nresult concerns the elliptic operator that the Pleba\\'nski complex defines. In\nthe case of the instanton complex, operators appearing in the complex\nsupplemented with their adjoints assemble to give the Dirac operator. We show\nhow the same holds true for the Pleba\\'nski complex. Supplemented by suitable\nadjoints, operators assemble into an elliptic operator that squares to the\nLaplacian and is given by the direct sum of two Dirac operators.",
        "String-net models describe a vast family of topological orders in two spatial\ndimensions, but fail to produce all the expected anyonic excitations. Following\narXiv:1502.03433, we consider an extended string-net model by attaching one\ntail to each plaquette of the lattice, allowing all anyons to emerge as\nelementary plaquette excitations for arbitrary input categories. The\ncorresponding tube algebra is the mathematical tool needed to construct the\nanyons from the input category and to obtain their internal multiplicities. We\nuse them to compute the energy level degeneracies and the partition function.\nIn the thermodynamic limit, the latter is dominated by the trivial (vacuum)\nanyon, so that the topological order is destroyed at any non-zero temperature.\nIn a finite-size system, order survives up to a finite temperature, similarly\nto the one-dimensional classical Ising model. We confirm this by computing\nthermal averages of topological projectors, Wegner-Wilson loops and the\ntopological mutual information. The results are also generalized to models with\nmultiple tails per plaquette.",
        "A geodesic cycle is a closed curve that connects finitely many points along\ngeodesics. We study geodesic cycles on the sphere in regard to their role in\nequal-weight quadrature rules and approximation.",
        "We establish the consistency of classical scaling under a broad class of\nnoise models, encompassing many commonly studied cases in literature. Our\napproach requires only finite fourth moments of the noise, significantly\nweakening standard assumptions. We derive convergence rates for classical\nscaling and establish matching minimax lower bounds, demonstrating that\nclassical scaling achieves minimax optimality in recovering the true\nconfiguration even when the input dissimilarities are corrupted by noise.",
        "The quantum geometric tensor, which encodes the full geometric information of\nquantum states in projective Hilbert space, plays a crucial role in condensed\nmatter physics. In this work, we examine the effect of the non-Abelian quantum\nmetric -- the real part of the non-Abelian quantum geometric tensor -- on the\nsuperfluid weight in time-reversal symmetric systems. For conventional $s$-wave\npairing, we demonstrate that the superfluid weight includes a contribution\nproportional to the trace of the non-Abelian quantum metric. Notably, this\ncontribution remains significant even when the total Chern number of a set of\ndegenerate bands is zero and can exceed the conventional contribution, as\nconfirmed using lattice models. Ab initio density functional theory (DFT)\ncalculations for MoS$_2$ and TiSe$_2$ further corroborate these findings,\nrevealing that the non-Abelian quantum metric accounts for up to 20% of the\nsuperfluid weight in MoS$_2$ and 50% in TiSe$_2$. Our results provide new\ninsights into the nontrivial relationship between the geometric properties of\nquantum states and superconductivity, opening avenues for further exploration\nin topological and superconducting materials.",
        "In this paper, we investigate the global well-posedness and optimal\ntime-decay of classical solutions for the 3-D full compressible Navier-Stokes\nsystem, which is given by the motion of the compressible viscous and\nheat-conductive gases. First of all, we study the global well-posedness of the\nCauchy problem to the system when the initial data is small enough. Secondly,\nwe show the optimal decay rates of the higher-order spatial derivatives of the\n$\\dot{H}^{-s}$ $\\left(0\\leq s<\\frac{3}{2}\\right)$ negative Sobolev norms.\nFinally, under the assumption that the initial data is bounded in $L^{1}$-norm,\nwe establish the upper and lower bounds of the optimal decay rates for the\nclassical solutions.",
        "The chiral loop-current (LC) phase in kagome metals AV3Sb5 (A = Cs, Rb, K)\nhas attracted considerable attention as a novel quantum state driven by\nelectron correlations. Scanning tunneling microscopy (STM) experiments have\nprovided strong evidence for the chiral LC phase through the detection of\nchirality in the quasiparticle interference (QPI) signal. However, the\nfundamental relationship between ``QPI chirality'' and ``LC chirality'' remains\nunexplored. For instance, the QPI signal is unchanged even when all LC orders\nare inverted. Furthermore, only the chiral LC order cannot induce QPI\nchirality. At present, the true essence of kagome metals that we should learn\nfrom the remarkable QPI experiments remains elusive. To address this, we\ninvestigate the origin of the QPI signal in the LC phase using a large\nunit-cell tight-binding model for kagome metals. The LC phase gives rise to a\n$Z_3$ nematic phase, characterized by three distinct directors, under the\nStar-of-David bond order. Our findings demonstrate that the QPI chirality\ninduced by a single impurity at site Z, denoted as $\\chi_Z$, can take values of\n$\\pm1$ (chiral) or 0 (achiral), depending on the direction of the $Z_3$ nematic\norder. Prominent QPI chirality originates from extremely dilute impurities\n($\\lesssim$0.1%) in the present mechanism. Notably, $\\chi_Z$ ($=\\pm1$, 0)\nchanges smoothly with minimal free-energy barriers by applying a small magnetic\nfield $B_z$, accompanied by a switching of the $Z_3$ nematic director. This\nstudy provides a comprehensive explanation for the observed ``$B_z$-switchable\nQPI chirality'' in regions with dilute impurities, offering fundamental insight\ninto the chiral LC in kagome metals.",
        "Lithium ion batteries are widely used in many applications. Battery\nmanagement systems control their optimal use and charging and predict when the\nbattery will cease to deliver the required output on a planned duty or driving\ncycle. Such systems use a simulation of a mathematical model of battery\nperformance. These models can be electrochemical or data-driven.\nElectrochemical models for batteries running at high currents are\nmathematically and computationally complex. In this work, we show that a\nwell-regarded electrochemical model, the Pseudo Two Dimensional (P2D) model,\ncan be replaced by a computationally efficient Convolutional Neural Network\n(CNN) surrogate model fit to accurately simulated data from a class of random\ndriving cycles. We demonstrate that a CNN is an ideal choice for accurately\ncapturing Lithium ion concentration profiles. Additionally, we show how the\nneural network model can be adjusted to correspond to battery changes in State\nof Health (SOH).",
        "High-redshift little red dots (LRDs) detected with the James Webb Space\nTelescope are considered the cores of emerging galaxies. For the first time, we\ncompare LRDs in $M_{\\rm bh}$-$M_{\\star}$ diagrams with an array of $z=0$\ngalaxy-morphology-dependent scaling relations, along with the $M_{\\rm\nbh}$-$M_{\\rm \\star,nsc}$ relation for nuclear star clusters. The $M_{\\rm\nbh}$-$M_{\\rm \\star,sph}$ relations for spheroidal stellar systems are\ncharacterised by a nearly parallel set of quasi-quadratic (or steeper)\ndistributions that are known to trace the `punctuated equilibrium' of galaxies,\nreflecting their stepwise growth in black hole mass and merger-built\nbulge\/spheroid mass. We show that LRDs are not equivalent to nuclear star\nclusters, with the latter having higher $M_{\\rm bh}\/M_{\\star}$ ratios. However,\nthe least massive LRDs exhibit similar $M_{\\rm bh}$ and $M_{\\rm \\star,gal}$\nvalues as ultracompact dwarf (UCD) galaxies. We show that the LRDs span the\n$M_{\\rm bh}$-$M_{\\rm \\star,gal}$ diagram from UCD galaxies to primaeval\nlenticular galaxies. In contrast, spiral galaxies and the subset of\nmajor-merger-built early-type galaxies define offset relations. Additionally,\nwe observe that low-$z$ galaxies with active galactic nuclei align with the\nsteep black hole scaling relations for disc galaxies defined by primarily\ninactive galaxies with directly measured black hole masses. Collectively, this\nhighlights the benefits of considering galaxy morphology, which reflects their\naccretion and merger history, to understand the coevolution of galaxies and\ntheir black holes.",
        "Variable selection plays a crucial role in enhancing modeling effectiveness\nacross diverse fields, addressing the challenges posed by high-dimensional\ndatasets of correlated variables. This work introduces a novel approach namely\nKnockoff with over-parameterization (Knoop) to enhance Knockoff filters for\nvariable selection. Specifically, Knoop first generates multiple knockoff\nvariables for each original variable and integrates them with the original\nvariables into an over-parameterized Ridgeless regression model. For each\noriginal variable, Knoop evaluates the coefficient distribution of its\nknockoffs and compares these with the original coefficients to conduct an\nanomaly-based significance test, ensuring robust variable selection. Extensive\nexperiments demonstrate superior performance compared to existing methods in\nboth simulation and real-world datasets. Knoop achieves a notably higher Area\nunder the Curve (AUC) of the Receiver Operating Characteristic (ROC) Curve for\neffectively identifying relevant variables against the ground truth by\ncontrolled simulations, while showcasing enhanced predictive accuracy across\ndiverse regression and classification tasks. The analytical results further\nbackup our observations.",
        "We investigate spectral features of bottomonium at high temperature, in\nparticular the thermal mass shift and width of ground state S-wave and P-wave\nstate. We employ and compare a range of methods for determining these features\nfrom lattice NRQCD correlators, including direct correlator analyses\n(multi-exponential fits and moments of spectral functions), linear methods\n(Backus-Gilbert, Tikhonov and HLT methods), and Bayesian methods for spectral\nfunction reconstruction (MEM and BR). We comment on the reliability and\nlimitations of the various methods.",
        "4U 0114+65 is a high-mass X-ray binary system formed by the luminous\nsupergiant B1Ia, known as V{*} V662 Cas, and one of the slowest rotating\nneutron stars (NS) with a spin period of about 2.6 hours. This fact provides a\nrare opportunity to study interesting details of the accretion within each\nindividual pulse of the compact object. In this paper, we analyze 200 ks of\nChandra grating data, divided into 9 uninterrupted observations around the\norbit. The changes in the circumstellar absorption column through the orbit\nsuggest an orbital inclination of $\\sim$ $40^{\\circ}$ with respect to the\nobserver and a companion mass-loss rate of $\\sim$ 8.6 10$^{-7}$ solar masses\nyr$^{-1}$. The peaks of the NS pulse show a large pulse-to-pulse variability.\nThree of them show an evolution from a brighter regime to a weaker one. We\npropose that the efficiency of Compton cooling in this source fluctuates\nthroughout an accumulation cycle. After significant depletion of matter within\nthe magnetosphere, since the settling velocity is $\\sim \\times$ 2 times lower\nthan the free-fall velocity, the source gradually accumulates matter until the\ndensity exceeds a critical threshold. This increase in density triggers a\ntransition to a more efficient Compton cooling regime, leading to a higher mass\naccretion rate and consequently to an increased brightness.",
        "Parameter shift rules (PSRs) are key techniques for efficient gradient\nestimation in variational quantum eigensolvers (VQEs). In this paper, we\npropose its Bayesian variant, where Gaussian processes with appropriate kernels\nare used to estimate the gradient of the VQE objective. Our Bayesian PSR offers\nflexible gradient estimation from observations at arbitrary locations with\nuncertainty information and reduces to the generalized PSR in special cases. In\nstochastic gradient descent (SGD), the flexibility of Bayesian PSR allows the\nreuse of observations in previous steps, which accelerates the optimization\nprocess. Furthermore, the accessibility to the posterior uncertainty, along\nwith our proposed notion of gradient confident region (GradCoRe), enables us to\nminimize the observation costs in each SGD step. Our numerical experiments show\nthat the VQE optimization with Bayesian PSR and GradCoRe significantly\naccelerates SGD and outperforms the state-of-the-art methods, including\nsequential minimal optimization.",
        "We report the systematic synthesis, crystal structure, magnetization, and\npowder neutron diffraction of single crystalline and polycrystalline\nCaCo$_2$TeO$_6$ samples. CaCo$_2$TeO$_6$ crystallizes in an orthorhombic\nstructure with $Pnma$ space group, featuring chains of edge-shared CoO$_6$\noctahedra arranged in a honeycomb pattern. Two antiferromagnetic transitions\nare observed at $T$$_{N1}$ = 14.4 K and $T$$_{N2}$ = 16.2 K, corresponding to\ntwo long-range magnetic orders with propagation vectors of $\\bf{k}$$_1$ = (0,\n0, 0) and $\\bf{k}$$_2$ = (0.125, 0, 0.25), respectively. The ground state is\ndetermined as a canted up-up-down-down zigzag spin configuration along the $c$\naxis, wherein the magnetic moments of Co1 and Co2 ions are 3.4(1) and\n2.1(1)$\\mu$$_B$, respectively. Successive spin-flop transitions appear with the\nincreasing magnetic field applied along the easy axis ($c$ axis), accompanied\nby depression of the antiferromagnetic orders and enhancement of residual\nmagnetic entropy. The field-induced spin-disordered state suggests that\nCaCo$_2$TeO$_6$ may be an ideal candidate for studying frustrated magnetism."
      ]
    }
  },
  {
    "id":2411.03522,
    "research_type":"applied",
    "start_id":"b20",
    "start_title":"Long non-coding RNAs: definitions, functions, challenges and recommendations",
    "start_abstract":"Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Language Models are Few-Shot Learners"
      ],
      "abstract":[
        "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "J&H: Evaluating the Robustness of Large Language Models Under\n  Knowledge-Injection Attacks in Legal Domain",
        "Non-Euclidean Hierarchical Representational Learning using Hyperbolic\n  Graph Neural Networks for Environmental Claim Detection",
        "Enhancing Pancreatic Cancer Staging with Large Language Models: The Role\n  of Retrieval-Augmented Generation",
        "JRE-L: Journalist, Reader, and Editor LLMs in the Loop for Science\n  Journalism for the General Audience",
        "Beyond Prompt Content: Enhancing LLM Performance via Content-Format\n  Integrated Prompt Optimization",
        "Cross-modal Context Fusion and Adaptive Graph Convolutional Network for\n  Multimodal Conversational Emotion Recognition",
        "SurveyX: Academic Survey Automation via Large Language Models",
        "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
        "Unshackling Context Length: An Efficient Selective Attention Approach\n  through Query-Key Compression",
        "CiteCheck: Towards Accurate Citation Faithfulness Detection",
        "Modelling change in neural dynamics during phonetic accommodation",
        "SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large\n  Language Models",
        "LegalViz: Legal Text Visualization by Text To Diagram Generation",
        "Adaptive Drift Compensation for Soft Sensorized Finger Using Continual\n  Learning",
        "Video-DPRP: A Differentially Private Approach for Visual\n  Privacy-Preserving Video Human Activity Recognition",
        "A Survey of Internet Censorship and its Measurement: Methodology,\n  Trends, and Challenges",
        "Unifying Perplexing Behaviors in Modified BP Attributions through\n  Alignment Perspective",
        "UL-UNAS: Ultra-Lightweight U-Nets for Real-Time Speech Enhancement via\n  Network Architecture Search",
        "DDAT: Diffusion Policies Enforcing Dynamically Admissible Robot\n  Trajectories",
        "Density-Functional Perturbation Theory with Numeric Atom-Centered\n  Orbitals",
        "Real-Time Streaming Telemetry Based Detection and Mitigation of OOK and\n  Power Interference in Multi-User OSaaS Networks",
        "Towards Interpretable Protein Structure Prediction with Sparse\n  Autoencoders",
        "Ab Initio theory of Electron-phonon-coupling-induced Giant Magnetic\n  Moments of Chiral Phonons in Magnetic Materials",
        "Terahertz Integrated Sensing and Communication-Empowered UAVs in 6G: A\n  Transceiver Design Perspective",
        "Study of Nucleon Charge-Exchange Processes at $^{12}$C Fragmentation\n  with an Energy of 300 MeV\/Nucleon",
        "Global Picard Spectra and Borel Parametrized Algebra",
        "Radar Pulse Deinterleaving with Transformer Based Deep Metric Learning",
        "Microscopic investigation of wobbling motion in even-even nuclei"
      ],
      "abstract":[
        "As the scale and capabilities of Large Language Models (LLMs) increase, their\napplications in knowledge-intensive fields such as legal domain have garnered\nwidespread attention. However, it remains doubtful whether these LLMs make\njudgments based on domain knowledge for reasoning. If LLMs base their judgments\nsolely on specific words or patterns, rather than on the underlying logic of\nthe language, the ''LLM-as-judges'' paradigm poses substantial risks in the\nreal-world applications. To address this question, we propose a method of legal\nknowledge injection attacks for robustness testing, thereby inferring whether\nLLMs have learned legal knowledge and reasoning logic. In this paper, we\npropose J&H: an evaluation framework for detecting the robustness of LLMs under\nknowledge injection attacks in the legal domain. The aim of the framework is to\nexplore whether LLMs perform deductive reasoning when accomplishing legal\ntasks. To further this aim, we have attacked each part of the reasoning logic\nunderlying these tasks (major premise, minor premise, and conclusion\ngeneration). We have collected mistakes that legal experts might make in\njudicial decisions in the real world, such as typos, legal synonyms, inaccurate\nexternal legal statutes retrieval. However, in real legal practice, legal\nexperts tend to overlook these mistakes and make judgments based on logic.\nHowever, when faced with these errors, LLMs are likely to be misled by\ntypographical errors and may not utilize logic in their judgments. We conducted\nknowledge injection attacks on existing general and domain-specific LLMs.\nCurrent LLMs are not robust against the attacks employed in our experiments. In\naddition we propose and compare several methods to enhance the knowledge\nrobustness of LLMs.",
        "Transformer-based models dominate NLP tasks like sentiment analysis, machine\ntranslation, and claim verification. However, their massive computational\ndemands and lack of interpretability pose challenges for real-world\napplications requiring efficiency and transparency. In this work, we explore\nGraph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks (HGNNs) as\nlightweight yet effective alternatives for Environmental Claim Detection,\nreframing it as a graph classification problem. We construct dependency parsing\ngraphs to explicitly model syntactic structures, using simple word embeddings\n(word2vec) for node features with dependency relations encoded as edge\nfeatures. Our results demonstrate that these graph-based models achieve\ncomparable or superior performance to state-of-the-art transformers while using\n30x fewer parameters. This efficiency highlights the potential of structured,\ninterpretable, and computationally efficient graph-based approaches.",
        "Purpose: Retrieval-augmented generation (RAG) is a technology to enhance the\nfunctionality and reliability of large language models (LLMs) by retrieving\nrelevant information from reliable external knowledge (REK). RAG has gained\ninterest in radiology, and we previously reported the utility of NotebookLM, an\nLLM with RAG (RAG-LLM), for lung cancer staging. However, since the comparator\nLLM differed from NotebookLM's internal model, it remained unclear whether its\nadvantage stemmed from RAG or inherent model differences. To better isolate\nRAG's impact and assess its utility across different cancers, we compared\nNotebookLM with its internal LLM, Gemini 2.0 Flash, in a pancreatic cancer\nstaging experiment.\n  Materials and Methods: A summary of Japan's pancreatic cancer staging\nguidelines was used as REK. We compared three groups - REK+\/RAG+ (NotebookLM\nwith REK), REK+\/RAG- (Gemini 2.0 Flash with REK), and REK-\/RAG- (Gemini 2.0\nFlash without REK) - in staging 100 fictional pancreatic cancer cases based on\nCT findings. Staging criteria included TNM classification, local invasion\nfactors, and resectability classification. In REK+\/RAG+, retrieval accuracy was\nquantified based on the sufficiency of retrieved REK excerpts.\n  Results: REK+\/RAG+ achieved a staging accuracy of 70%, outperforming\nREK+\/RAG- (38%) and REK-\/RAG- (35%). For TNM classification, REK+\/RAG+ attained\n80% accuracy, exceeding REK+\/RAG- (55%) and REK-\/RAG- (50%). Additionally,\nREK+\/RAG+ explicitly presented retrieved REK excerpts, achieving a retrieval\naccuracy of 92%.\n  Conclusion: NotebookLM, a RAG-LLM, outperformed its internal LLM, Gemini 2.0\nFlash, in a pancreatic cancer staging experiment, suggesting that RAG may\nimprove LLM's staging accuracy. Furthermore, its ability to retrieve and\npresent REK excerpts provides transparency for physicians, highlighting its\napplicability for clinical diagnosis and classification.",
        "Science journalism reports current scientific discoveries to non-specialists,\naiming to enable public comprehension of the state of the art. This task is\nchallenging as the audience often lacks specific knowledge about the presented\nresearch. We propose a JRE-L framework that integrates three LLMs mimicking the\nwriting-reading-feedback-revision loop. In JRE-L, one LLM acts as the\njournalist, another LLM as the general public reader, and the third LLM as an\neditor. The journalist's writing is iteratively refined by feedback from the\nreader and suggestions from the editor. Our experiments demonstrate that by\nleveraging the collaboration of two 7B and one 1.8B open-source LLMs, we can\ngenerate articles that are more accessible than those generated by existing\nmethods, including prompting single advanced models such as GPT-4 and other\nLLM-collaboration strategies. Our code is publicly available at\ngithub.com\/Zzoay\/JRE-L.",
        "Large Language Models (LLMs) have shown significant capability across various\ntasks, with their real-world effectiveness often driven by prompt design. While\nrecent research has focused on optimizing prompt content, the role of prompt\nformatting, a critical but often overlooked dimension, has received limited\nsystematic investigation. In this paper, we introduce Content-Format Integrated\nPrompt Optimization (CFPO), an innovative methodology that jointly optimizes\nboth prompt content and formatting through an iterative refinement process.\nCFPO leverages natural language mutations to explore content variations and\nemploys a dynamic format exploration strategy that systematically evaluates\ndiverse format options. Our extensive evaluations across multiple tasks and\nopen-source LLMs demonstrate that CFPO demonstrates measurable performance\nimprovements compared to content-only optimization methods. This highlights the\nimportance of integrated content-format optimization and offers a practical,\nmodel-agnostic approach to enhancing LLM performance. Code is available at\nhttps:\/\/github.com\/HenryLau7\/CFPO.",
        "Emotion recognition has a wide range of applications in human-computer\ninteraction, marketing, healthcare, and other fields. In recent years, the\ndevelopment of deep learning technology has provided new methods for emotion\nrecognition. Prior to this, many emotion recognition methods have been\nproposed, including multimodal emotion recognition methods, but these methods\nignore the mutual interference between different input modalities and pay\nlittle attention to the directional dialogue between speakers. Therefore, this\narticle proposes a new multimodal emotion recognition method, including a cross\nmodal context fusion module, an adaptive graph convolutional encoding module,\nand an emotion classification module. The cross modal context module includes a\ncross modal alignment module and a context fusion module, which are used to\nreduce the noise introduced by mutual interference between different input\nmodalities. The adaptive graph convolution module constructs a dialogue\nrelationship graph for extracting dependencies and self dependencies between\nspeakers. Our model has surpassed some state-of-the-art methods on publicly\navailable benchmark datasets and achieved high recognition accuracy.",
        "Large Language Models (LLMs) have demonstrated exceptional comprehension\ncapabilities and a vast knowledge base, suggesting that LLMs can serve as\nefficient tools for automated survey generation. However, recent research\nrelated to automated survey generation remains constrained by some critical\nlimitations like finite context window, lack of in-depth content discussion,\nand absence of systematic evaluation frameworks. Inspired by human writing\nprocesses, we propose SurveyX, an efficient and organized system for automated\nsurvey generation that decomposes the survey composing process into two phases:\nthe Preparation and Generation phases. By innovatively introducing online\nreference retrieval, a pre-processing method called AttributeTree, and a\nre-polishing process, SurveyX significantly enhances the efficacy of survey\ncomposition. Experimental evaluation results show that SurveyX outperforms\nexisting automated survey generation systems in content quality (0.259\nimprovement) and citation quality (1.76 enhancement), approaching human expert\nperformance across multiple evaluation dimensions. Examples of surveys\ngenerated by SurveyX are available on www.surveyx.cn",
        "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse.",
        "Handling long-context sequences efficiently remains a significant challenge\nin large language models (LLMs). Existing methods for token selection in\nsequence extrapolation either employ a permanent eviction strategy or select\ntokens by chunk, which may lead to the loss of critical information. We propose\nEfficient Selective Attention (ESA), a novel approach that extends context\nlength by efficiently selecting the most critical tokens at the token level to\ncompute attention. ESA reduces the computational complexity of token selection\nby compressing query and key vectors into lower-dimensional representations. We\nevaluate ESA on long sequence benchmarks with maximum lengths up to 256k using\nopen-source LLMs with context lengths of 8k and 32k. ESA outperforms other\nselective attention methods, especially in tasks requiring the retrieval of\nmultiple pieces of information, achieving comparable performance to\nfull-attention extrapolation methods across various tasks, with superior\nresults in certain tasks.",
        "Citation faithfulness detection is critical for enhancing retrieval-augmented\ngeneration (RAG) systems, yet large-scale Chinese datasets for this task are\nscarce. Existing methods face prohibitive costs due to the need for manually\nannotated negative samples. To address this, we introduce the first large-scale\nChinese dataset CiteCheck for citation faithfulness detection, constructed via\na cost-effective approach using two-stage manual annotation. This method\nbalances positive and negative samples while significantly reducing annotation\nexpenses. CiteCheck comprises training and test splits. Experiments demonstrate\nthat: (1) the test samples are highly challenging, with even state-of-the-art\nLLMs failing to achieve high accuracy; and (2) training data augmented with\nLLM-generated negative samples enables smaller models to attain strong\nperformance using parameter-efficient fine-tuning. CiteCheck provides a robust\nfoundation for advancing citation faithfulness detection in Chinese RAG\nsystems. The dataset is publicly available to facilitate research.",
        "Short-term phonetic accommodation is a fundamental driver behind accent\nchange, but how does real-time input from another speaker's voice shape the\nspeech planning representations of an interlocutor? We advance a computational\nmodel of change in phonetic representations during phonetic accommodation,\ngrounded in dynamic neural field equations for movement planning and memory\ndynamics. We test the model's ability to capture empirical patterns from an\nexperimental study where speakers shadowed a model talker with a different\naccent from their own. The experimental data shows vowel-specific degrees of\nconvergence during shadowing, followed by return to baseline (or minor\ndivergence) post-shadowing. The model can reproduce these phenomena by\nmodulating the magnitude of inhibitory memory dynamics, which may reflect\nresistance to accommodation due to phonological and\/or sociolinguistic\npressures. We discuss the implications of these results for the relation\nbetween short-term phonetic accommodation and longer-term patterns of sound\nchange.",
        "The increasing application of multi-modal large language models (MLLMs)\nacross various sectors have spotlighted the essence of their output reliability\nand accuracy, particularly their ability to produce content grounded in factual\ninformation (e.g. common and domain-specific knowledge). In this work, we\nintroduce SimpleVQA, the first comprehensive multi-modal benchmark to evaluate\nthe factuality ability of MLLMs to answer natural language short questions.\nSimpleVQA is characterized by six key features: it covers multiple tasks and\nmultiple scenarios, ensures high quality and challenging queries, maintains\nstatic and timeless reference answers, and is straightforward to evaluate. Our\napproach involves categorizing visual question-answering items into 9 different\ntasks around objective events or common knowledge and situating these within 9\ntopics. Rigorous quality control processes are implemented to guarantee\nhigh-quality, concise, and clear answers, facilitating evaluation with minimal\nvariance via an LLM-as-a-judge scoring system. Using SimpleVQA, we perform a\ncomprehensive assessment of leading 18 MLLMs and 8 text-only LLMs, delving into\ntheir image comprehension and text generation abilities by identifying and\nanalyzing error cases.",
        "Legal documents including judgments and court orders require highly\nsophisticated legal knowledge for understanding. To disclose expert knowledge\nfor non-experts, we explore the problem of visualizing legal texts with\neasy-to-understand diagrams and propose a novel dataset of LegalViz with 23\nlanguages and 7,010 cases of legal document and visualization pairs, using the\nDOT graph description language of Graphviz. LegalViz provides a simple diagram\nfrom a complicated legal corpus identifying legal entities, transactions, legal\nsources, and statements at a glance, that are essential in each judgment. In\naddition, we provide new evaluation metrics for the legal diagram visualization\nby considering graph structures, textual similarities, and legal contents. We\nconducted empirical studies on few-shot and finetuning large language models\nfor generating legal diagrams and evaluated them with these metrics, including\nlegal content-based evaluation within 23 languages. Models trained with\nLegalViz outperform existing models including GPTs, confirming the\neffectiveness of our dataset.",
        "Strain sensors are gaining popularity in soft robotics for acquiring tactile\ndata due to their flexibility and ease of integration. Tactile sensing plays a\ncritical role in soft grippers, enabling them to safely interact with\nunstructured environments and precisely detect object properties. However, a\nsignificant challenge with these systems is their high non-linearity,\ntime-varying behavior, and long-term signal drift. In this paper, we introduce\na continual learning (CL) approach to model a soft finger equipped with\npiezoelectric-based strain sensors for proprioception. To tackle the\naforementioned challenges, we propose an adaptive CL algorithm that integrates\na Long Short-Term Memory (LSTM) network with a memory buffer for rehearsal and\nincludes a regularization term to keep the model's decision boundary close to\nthe base signal while adapting to time-varying drift. We conduct nine different\nexperiments, resetting the entire setup each time to demonstrate signal drift.\nWe also benchmark our algorithm against two other methods and conduct an\nablation study to assess the impact of different components on the overall\nperformance.",
        "Considerable effort has been made in privacy-preserving video human activity\nrecognition (HAR). Two primary approaches to ensure privacy preservation in\nVideo HAR are differential privacy (DP) and visual privacy. Techniques\nenforcing DP during training provide strong theoretical privacy guarantees but\noffer limited capabilities for visual privacy assessment. Conversely methods,\nsuch as low-resolution transformations, data obfuscation and adversarial\nnetworks, emphasize visual privacy but lack clear theoretical privacy\nassurances. In this work, we focus on two main objectives: (1) leveraging DP\nproperties to develop a model-free approach for visual privacy in videos and\n(2) evaluating our proposed technique using both differential privacy and\nvisual privacy assessments on HAR tasks. To achieve goal (1), we introduce\nVideo-DPRP: a Video-sample-wise Differentially Private Random Projection\nframework for privacy-preserved video reconstruction for HAR. By using random\nprojections, noise matrices and right singular vectors derived from the\nsingular value decomposition of videos, Video-DPRP reconstructs DP videos using\nprivacy parameters ($\\epsilon,\\delta$) while enabling visual privacy\nassessment. For goal (2), using UCF101 and HMDB51 datasets, we compare\nVideo-DPRP's performance on activity recognition with traditional DP methods,\nand state-of-the-art (SOTA) visual privacy-preserving techniques. Additionally,\nwe assess its effectiveness in preserving privacy-related attributes such as\nfacial features, gender, and skin color, using the PA-HMDB and VISPR datasets.\nVideo-DPRP combines privacy-preservation from both a DP and visual privacy\nperspective unlike SOTA methods that typically address only one of these\naspects.",
        "Internet censorship limits the access of nodes residing within a specific\nnetwork environment to the public Internet, and vice versa. During the last\ndecade, techniques for conducting Internet censorship have been developed\nfurther. Consequently, methodology for measuring Internet censorship had been\nimproved as well. In this paper, we firstly provide a survey of Internet\ncensorship techniques. Secondly, we survey censorship measurement methodology,\nincluding a coverage of available datasets. In cases where it is beneficial, we\nbridge the terminology and taxonomy of Internet censorship with related\ndomains, namely traffic obfuscation and information hiding. We cover both,\ntechnical and human aspects, as well as recent trends, and challenges.",
        "Attributions aim to identify input pixels that are relevant to the\ndecision-making process. A popular approach involves using modified\nbackpropagation (BP) rules to reverse decisions, which improves\ninterpretability compared to the original gradients. However, these methods\nlack a solid theoretical foundation and exhibit perplexing behaviors, such as\nreduced sensitivity to parameter randomization, raising concerns about their\nreliability and highlighting the need for theoretical justification. In this\nwork, we present a unified theoretical framework for methods like GBP,\nRectGrad, LRP, and DTD, demonstrating that they achieve input alignment by\ncombining the weights of activated neurons. This alignment improves the\nvisualization quality and reduces sensitivity to weight randomization. Our\ncontributions include: (1) Providing a unified explanation for multiple\nbehaviors, rather than focusing on just one. (2) Accurately predicting novel\nbehaviors. (3) Offering insights into decision-making processes, including\nlayer-wise information changes and the relationship between attributions and\nmodel decisions.",
        "Lightweight models are essential for real-time speech enhancement\napplications. In recent years, there has been a growing trend toward developing\nincreasingly compact models for speech enhancement. In this paper, we propose\nan Ultra-Lightweight U-net optimized by Network Architecture Search (UL-UNAS),\nwhich is suitable for implementation in low-footprint devices. Firstly, we\nexplore the application of various efficient convolutional blocks within the\nU-Net framework to identify the most promising candidates. Secondly, we\nintroduce two boosting components to enhance the capacity of these\nconvolutional blocks: a novel activation function named affine PReLU and a\ncausal time-frequency attention module. Furthermore, we leverage neural\narchitecture search to discover an optimal architecture within our carefully\ndesigned search space. By integrating the above strategies, UL-UNAS not only\nsignificantly outperforms the latest ultra-lightweight models with the same or\nlower computational complexity, but also delivers competitive performance\ncompared to recent baseline models that require substantially higher\ncomputational resources.",
        "Diffusion models excel at creating images and videos thanks to their\nmultimodal generative capabilities. These same capabilities have made diffusion\nmodels increasingly popular in robotics research, where they are used for\ngenerating robot motion. However, the stochastic nature of diffusion models is\nfundamentally at odds with the precise dynamical equations describing the\nfeasible motion of robots. Hence, generating dynamically admissible robot\ntrajectories is a challenge for diffusion models. To alleviate this issue, we\nintroduce DDAT: Diffusion policies for Dynamically Admissible Trajectories to\ngenerate provably admissible trajectories of black-box robotic systems using\ndiffusion models. A sequence of states is a dynamically admissible trajectory\nif each state of the sequence belongs to the reachable set of its predecessor\nby the robot's equations of motion. To generate such trajectories, our\ndiffusion policies project their predictions onto a dynamically admissible\nmanifold during both training and inference to align the objective of the\ndenoiser neural network with the dynamical admissibility constraint. The\nauto-regressive nature of these projections along with the black-box nature of\nrobot dynamics render these projections immensely challenging. We thus enforce\nadmissibility by iteratively sampling a polytopic under-approximation of the\nreachable set of a state onto which we project its predicted successor, before\niterating this process with the projected successor. By producing accurate\ntrajectories, this projection eliminates the need for diffusion models to\ncontinually replan, enabling one-shot long-horizon trajectory planning. We\ndemonstrate that our framework generates higher quality dynamically admissible\nrobot trajectories through extensive simulations on a quadcopter and various\nMuJoCo environments, along with real-world experiments on a Unitree GO1 and\nGO2.",
        "This paper represents one contribution to a larger Roadmap article reviewing\nthe current status of the FHI-aims code. In this contribution, the\nimplementation of density-functional perturbation theory in a numerical\natom-centered framework is summarized. Guidelines on usage and links to\ntutorials are provided.",
        "We present a framework to identify and mitigate rogue OOK signals and\nuser-generated power interference in a multi-user Optical-Spectrum-as-a-Service\nnetwork. Experimental tests on the OpenIreland-testbed achieve up to 89%\ndetection rate within 10 seconds of an interference event.",
        "Protein language models have revolutionized structure prediction, but their\nnonlinear nature obscures how sequence representations inform structure\nprediction. While sparse autoencoders (SAEs) offer a path to interpretability\nhere by learning linear representations in high-dimensional space, their\napplication has been limited to smaller protein language models unable to\nperform structure prediction. In this work, we make two key advances: (1) we\nscale SAEs to ESM2-3B, the base model for ESMFold, enabling mechanistic\ninterpretability of protein structure prediction for the first time, and (2) we\nadapt Matryoshka SAEs for protein language models, which learn hierarchically\norganized features by forcing nested groups of latents to reconstruct inputs\nindependently. We demonstrate that our Matryoshka SAEs achieve comparable or\nbetter performance than standard architectures. Through comprehensive\nevaluations, we show that SAEs trained on ESM2-3B significantly outperform\nthose trained on smaller models for both biological concept discovery and\ncontact map prediction. Finally, we present an initial case study demonstrating\nhow our approach enables targeted steering of ESMFold predictions, increasing\nstructure solvent accessibility while fixing the input sequence. To facilitate\nfurther investigation by the broader community, we open-source our code,\ndataset, pretrained models https:\/\/github.com\/johnyang101\/reticular-sae , and\nvisualizer https:\/\/sae.reticular.ai .",
        "Chiral phonons, characterized by nonzero angular momenta and magnetic\nmoments, have attracted extensive attention. However, a long-standing critical\nissue in this field is the lack of ab initio methods to accurately calculate\nphonon magnetic moments resulting from electron-phonon coupling (EPC). Here, we\nresolve this challenge by developing an ab initio theory for calculating\nEPC-induced phonon magnetic properties, applicable to both insulating and\nmetallic materials. Based on this theory, we demonstrate EPC-induced giant\nphonon magnetic moments and resulting phonon Zeeman splittings in magnetic\nmetals, which are orders of magnitude larger than classical predictions from\npoint-charge models. Interestingly, these splittings could open observable\ntopological gaps in phonon spectra of magnets, generating intrinsic phonon\nChern states. Through first-principles calculations, we first propose candidate\nmaterials with such intrinsic phonon Chern states hosting robust edge phonon\ncurrents which may be applied to detecting neutral particles such as dark\nmatter particles. Our work not only establishes a theoretical foundation for\nEPC-induced phonon magnetic properties, but also enables the ab initio\ncalculation of long-sought TRS-breaking phonon spectra throughout the Brillouin\nzone in realistic materials.",
        "Due to their high maneuverability, flexible deployment, and low cost,\nunmanned aerial vehicles (UAVs) are expected to play a pivotal role in not only\ncommunication, but also sensing. Especially by exploiting the ultra-wide\nbandwidth of terahertz (THz) bands, integrated sensing and communication\n(ISAC)-empowered UAV has been a promising technology of 6G space-air-ground\nintegrated networks. In this article, we systematically investigate the key\ntechniques and essential obstacles for THz-ISAC-empowered UAV from a\ntransceiver design perspective, with the highlight of its major challenges and\nkey technologies. Specifically, we discuss the THz-ISAC-UAV wireless\npropagation environment, based on which several channel characteristics for\ncommunication and sensing are revealed. We point out the transceiver payload\ndesign peculiarities for THz-ISAC-UAV from the perspective of antenna design,\nradio frequency front-end, and baseband signal processing. To deal with the\nspecificities faced by the payload, we shed light on three key technologies,\ni.e., hybrid beamforming for ultra-massive MIMO-ISAC, power-efficient THz-ISAC\nwaveform design, as well as communication and sensing channel state information\nacquisition, and extensively elaborate their concepts and key issues. More\nimportantly, future research directions and associated open problems are\npresented, which may unleash the full potential of THz-ISAC-UAV for 6G wireless\nnetworks.",
        "The search of reactions with nucleon charge-exchange was performed on the\nFRAGM fragment-separator of the TWAC accelerator complex at fragmentation of\ncarbon nuclei with an energy of 300 MeV\/nucleon on a thin beryllium target. The\nexperimental setup, located at an angle of 3.5 degrees to the incident beam,\nhad high momentum resolution. Differential cross sections were measured for\n$^{11}$Be, $^{12}$B and $^{12}$Be as function of the nuclei momentum. The\nexperimental data were compared with theoretical predictions of various models\nof nucleus-nucleus interactions and other experimental results. Measurements of\nnucleon charge exchange processes in this energy region was carried out for the\nfirst time. New results were obtained to test theoretical models of\nnucleus-nucleus interactions.",
        "We answer a question of Schwede on the existence of global Picard spectra\nassociated to his ultra-commutative global ring spectra; given an\nultra-commutative global ring spectrum $R$, we show there exists a global\nspectrum $\\mathrm{pic}_\\mathrm{eq}(R)$ assembling the Picard spectra of all\nunderlying $G$-equivariant ring spectra $\\mathrm{res}_G R$ of $R$ into one\nobject, in that for all finite groups $G$, the genuine fixed points are given\nby $\\mathrm{pic}_\\mathrm{eq}(R)^G \\simeq\n\\mathrm{pic}(\\mathrm{Mod}_{\\mathrm{res}_G R}(\\mathrm{Sp}_G))$.\n  Along the way, we develop a generalization of Borel-equivariant objects in\nthe setting of parametrized higher algebra. We use this to assemble the\nsymmetric monoidal categories of $G$-spectra for all finite groups $G$ together\nwith all restrictions and norms into a single `normed global category', and\nbuild a comparison functor which allows us to import ultra-commutative\n$G$-equivariant or global ring spectra into the setting of parametrized higher\nalgebra.",
        "When receiving radar pulses it is common for a recorded pulse train to\ncontain pulses from many different emitters. The radar pulse deinterleaving\nproblem is the task of separating out these pulses by the emitter from which\nthey originated. Notably, the number of emitters in any particular recorded\npulse train is considered unknown. In this paper, we define the problem and\npresent metrics that can be used to measure model performance. We propose a\nmetric learning approach to this problem using a transformer trained with the\ntriplet loss on synthetic data. This model achieves strong results in\ncomparison with other deep learning models with an adjusted mutual information\nscore of 0.882.",
        "The possibility of observing wobbling mode in the even-even systems of 76Ge,\n112Ru, 188,192Os, 192Pt and 232Th is explored using the triaxial projected\nshell model approach. These nuclei are known to have {\\gamma}-bands whose\nodd-spin members are lower than the average of the neighbouring even-spin\nstates. It is shown through a detailed analysis of the excitation energies and\nthe electromagnetic transition probabilities that the observed band structures\nin these nuclei except for 232Th can be characterised as originating from the\nwobbling motion. It is further demonstrated that quasiparticle alignment is\nresponsible for driving the systems to the wobbling mode."
      ]
    }
  },
  {
    "id":2411.00688,
    "research_type":"applied",
    "start_id":"b19",
    "start_title":"Parameter-Free FISTA by Adaptive Restart and Backtracking",
    "start_abstract":"We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Convex generalizations of total variation based on the structure tensor with applications to inverse problems"
      ],
      "abstract":[
        "We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it."
      ],
      "categories":[
        "eess.IV"
      ]
    },
    "list":{
      "title":[
        "Zero-Shot Denoising for Fluorescence Lifetime Imaging Microscopy with\n  Intensity-Guided Learning",
        "Improving Lesion Segmentation in Medical Images by Global and Regional\n  Feature Compensation",
        "QoE-oriented Communication Service Provision for Annotation Rendering in\n  Mobile Augmented Reality",
        "Structure-from-Sherds++: Robust Incremental 3D Reassembly of Axially\n  Symmetric Pots from Unordered and Mixed Fragment Collections",
        "OSLO-IC: On-the-Sphere Learned Omnidirectional Image Compression with\n  Attention Modules and Spatial Context",
        "ILACS-LGOT: A Multi-Layer Contrast Enhancement Approach for Palm-Vein\n  Images",
        "Goal-Oriented Semantic Communication for Wireless Video Transmission via\n  Generative AI",
        "SLCGC: A lightweight Self-supervised Low-pass Contrastive Graph\n  Clustering Network for Hyperspectral Images",
        "Color Correction Meets Cross-Spectral Refinement: A Distribution-Aware\n  Diffusion for Underwater Image Restoration",
        "Physics-Informed Implicit Neural Representations for Joint B0 Estimation\n  and Echo Planar Imaging",
        "Observation-only learning of neural mapping schemes for gappy\n  satellite-derived ocean colour parameters",
        "Few Shot Alternating GD and Minimization for Generalizable Real-Time MRI",
        "4D-ACFNet: A 4D Attention Mechanism-Based Prognostic Framework for\n  Colorectal Cancer Liver Metastasis Integrating Multimodal Spatiotemporal\n  Features",
        "Decay rates of $\\Lambda_b^0 \\to \\Lambda_c^+ \\ell^- \\bar\\nu_\\ell$ using\n  helicity analysis and phase-moment parametrization",
        "Ultraviolet Renormalization of Spin Boson Models I. Normal and\n  2-Nilpotent Interactions",
        "Development of a high-rate capable DLC-RPC based on a current evacuation\n  pattern",
        "Non-Lorentzian model for strong exciton-plasmon coupling",
        "Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly\n  Detection in Videos",
        "PrivilegedDreamer: Explicit Imagination of Privileged Information for\n  Rapid Adaptation of Learned Policies",
        "Are you a DePIN? A Decision Tree to Classify Decentralized Physical\n  Infrastructure Networks",
        "Bridging HCI and AI Research for the Evaluation of Conversational SE\n  Assistants",
        "Hidden Darkness in LLM-Generated Designs: Exploring Dark Patterns in\n  Ecommerce Web Components Generated by LLMs",
        "Blast waves and reverse shocks: from ultra-relativistic GRBs to\n  moderately relativistic X-ray binaries",
        "How Low Can You Go? Searching for the Intrinsic Dimensionality of\n  Complex Networks using Metric Node Embeddings",
        "Native antisite defects in h-BN",
        "Black Box Causal Inference: Effect Estimation via Meta Prediction",
        "Exploring the energy spectrum of a four-terminal Josephson junction:\n  Towards topological Andreev band structures",
        "Near-Field ISAC: Synergy of Dual-Purpose Codebooks and Space-Time\n  Adaptive Processing"
      ],
      "abstract":[
        "Multimodal and multi-information microscopy techniques such as Fluorescence\nLifetime Imaging Microscopy (FLIM) extend the informational channels beyond\nintensity-based fluorescence microscopy but suffer from reduced image quality\ndue to complex noise patterns. For FLIM, the intrinsic relationship between\nintensity and lifetime information means noise in each channel is a\nmultivariate function across channels without necessarily sharing structural\nfeatures. Based on this, we present a novel Zero-Shot Denoising Framework with\nan Intensity-Guided Learning approach. Our correlation-preserving strategy\nmaintains important biological information that might be lost when channels are\nprocessed independently. Our framework implements separate processing paths for\neach channel and utilizes a pre-trained intensity denoising prior to guide the\nrefinement of lifetime components across multiple channels. Through experiments\non real-world FLIM-acquired biological samples, we show that our approach\noutperforms existing methods in both noise reduction and lifetime preservation,\nthereby enabling more reliable extraction of physiological and molecular\ninformation.",
        "Automated lesion segmentation of medical images has made tremendous\nimprovements in recent years due to deep learning advancements. However,\naccurately capturing fine-grained global and regional feature representations\nremains a challenge. Many existing methods obtain suboptimal performance on\ncomplex lesion segmentation due to information loss during typical downsampling\noperations and the insufficient capture of either regional or global features.\nTo address these issues, we propose the Global and Regional Compensation\nSegmentation Framework (GRCSF), which introduces two key innovations: the\nGlobal Compensation Unit (GCU) and the Region Compensation Unit (RCU). The\nproposed GCU addresses resolution loss in the U-shaped backbone by preserving\nglobal contextual features and fine-grained details during multiscale\ndownsampling. Meanwhile, the RCU introduces a self-supervised learning (SSL)\nresidual map generated by Masked Autoencoders (MAE), obtained as pixel-wise\ndifferences between reconstructed and original images, to highlight regions\nwith potential lesions. These SSL residual maps guide precise lesion\nlocalization and segmentation through a patch-based cross-attention mechanism\nthat integrates regional spatial and pixel-level features. Additionally, the\nRCU incorporates patch-level importance scoring to enhance feature fusion by\nleveraging global spatial information from the backbone. Experiments on two\npublicly available medical image segmentation datasets, including brain stroke\nlesion and coronary artery calcification datasets, demonstrate that our GRCSF\noutperforms state-of-the-art methods, confirming its effectiveness across\ndiverse lesion types and its potential as a generalizable lesion segmentation\nsolution.",
        "As mobile augmented reality (MAR) continues to evolve, future 6G networks\nwill play a pivotal role in supporting immersive and personalized user\nexperiences. In this paper, we address the communication service provision\nproblem for annotation rendering in edge-assisted MAR, with the objective of\noptimizing spectrum resource utilization while ensuring the required quality of\nexperience (QoE) for MAR users. To overcome the challenges of user-specific\nuplink data traffic patterns and the complex operational mechanisms of\nannotation rendering, we propose a digital twin (DT)-based approach. We first\ndesign a DT specifically tailored for MAR applications to learn key annotation\nrendering mechanisms, enabling the network controller to access MAR\napplication-specific information. Then, we develop a DT based QoE modeling\napproach to capture the unique relationship between individual user QoE and\nspectrum resource demands. Finally, we propose a QoE-oriented resource\nallocation algorithm that decreases resource utilization compared to\nconventional net work slicing-based approaches. Simulation results demonstrate\nthat our DT-based approach outperforms benchmark approaches in the accuracy and\ngranularity of QoE modeling.",
        "Reassembling multiple axially symmetric pots from fragmentary sherds is\ncrucial for cultural heritage preservation, yet it poses significant challenges\ndue to thin and sharp fracture surfaces that generate numerous false positive\nmatches and hinder large-scale puzzle solving. Existing global approaches,\nwhich optimize all potential fragment pairs simultaneously or data-driven\nmodels, are prone to local minima and face scalability issues when multiple\npots are intermixed. Motivated by Structure-from-Motion (SfM) for 3D\nreconstruction from multiple images, we propose an efficient reassembly method\nfor axially symmetric pots based on iterative registration of one sherd at a\ntime, called Structure-from-Sherds++ (SfS++). Our method extends beyond simple\nreplication of incremental SfM and leverages multi-graph beam search to explore\nmultiple registration paths. This allows us to effectively filter out\nindistinguishable false matches and simultaneously reconstruct multiple pots\nwithout requiring prior information such as base or the number of mixed\nobjects. Our approach achieves 87% reassembly accuracy on a dataset of 142 real\nfragments from 10 different pots, outperforming other methods in handling\ncomplex fracture patterns with mixed datasets and achieving state-of-the-art\nperformance. Code and results can be found in our project page\nhttps:\/\/sj-yoo.info\/sfs\/.",
        "Developing effective 360-degree (spherical) image compression techniques is\ncrucial for technologies like virtual reality and automated driving. This paper\nadvances the state-of-the-art in on-the-sphere learning (OSLO) for\nomnidirectional image compression framework by proposing spherical attention\nmodules, residual blocks, and a spatial autoregressive context model. These\nimprovements achieve a 23.1% bit rate reduction in terms of WS-PSNR BD rate.\nAdditionally, we introduce a spherical transposed convolution operator for\nupsampling, which reduces trainable parameters by a factor of four compared to\nthe pixel shuffling used in the OSLO framework, while maintaining similar\ncompression performance. Therefore, in total, our proposed method offers\nsignificant rate savings with a smaller architecture and can be applied to any\nspherical convolutional application.",
        "This article presents an extended author's version based on our previous\nwork, where we introduced the Multiple Overlapping Tiles (MOT) method for palm\nvein image enhancement. To better reflect the specific operations involved, we\nrename MOT to ILACS-LGOT (Intensity-Limited Adaptive Contrast Stretching with\nLayered Gaussian-weighted Overlapping Tiles). This revised terminology more\naccurately represents the method's approach to contrast enhancement and blocky\neffect mitigation. Additionally, this article provides a more detailed\nanalysis, including expanded evaluations, graphical representations, and\nsample-based comparisons, demonstrating the effectiveness of ILACS-LGOT over\nexisting methods.",
        "Efficient video transmission is essential for seamless communication and\ncollaboration within the visually-driven digital landscape. To achieve low\nlatency and high-quality video transmission over a bandwidth-constrained noisy\nwireless channel, we propose a stable diffusion (SD)-based goal-oriented\nsemantic communication (GSC) framework. In this framework, we first design a\nsemantic encoder that effectively identify the keyframes from video and extract\nthe relevant semantic information (SI) to reduce the transmission data size. We\nthen develop a semantic decoder to reconstruct the keyframes from the received\nSI and further generate the full video from the reconstructed keyframes using\nframe interpolation to ensure high-quality reconstruction. Recognizing the\nimpact of wireless channel noise on SI transmission, we also propose an\nSD-based denoiser for GSC (SD-GSC) condition on an instantaneous channel gain\nto remove the channel noise from the received noisy SI under a known channel.\nFor scenarios with an unknown channel, we further propose a parallel SD\ndenoiser for GSC (PSD-GSC) to jointly learn the distribution of channel gains\nand denoise the received SI. It is shown that, with the known channel, our\nproposed SD-GSC outperforms state-of-the-art ADJSCC, Latent-Diff DNSC, DeepWiVe\nand DVST, improving Peak Signal-to-Noise Ratio (PSNR) by 69%, 58%, 33% and 38%,\nreducing mean squared error (MSE) by 52%, 50%, 41% and 45%, and reducing\nFr\\'echet Video Distance (FVD) by 38%, 32%, 22% and 24%, respectively. With the\nunknown channel, our PSD-GSC achieves a 17% improvement in PSNR, a 29%\nreduction in MSE, and a 19% reduction in FVD compared to MMSE\nequalizer-enhanced SD-GSC. These significant performance improvements\ndemonstrate the robustness and superiority of our proposed methods in enhancing\nvideo transmission quality and efficiency under various channel conditions.",
        "Self-supervised hyperspectral image (HSI) clustering remains a fundamental\nyet challenging task due to the absence of labeled data and the inherent\ncomplexity of spatial-spectral interactions. While recent advancements have\nexplored innovative approaches, existing methods face critical limitations in\nclustering accuracy, feature discriminability, computational efficiency, and\nrobustness to noise, hindering their practical deployment. In this paper, a\nself-supervised efficient low-pass contrastive graph clustering (SLCGC) is\nintroduced for HSIs. Our approach begins with homogeneous region generation,\nwhich aggregates pixels into spectrally consistent regions to preserve local\nspatial-spectral coherence while drastically reducing graph complexity. We then\nconstruct a structural graph using an adjacency matrix A and introduce a\nlow-pass graph denoising mechanism to suppress high-frequency noise in the\ngraph topology, ensuring stable feature propagation. A dual-branch graph\ncontrastive learning module is developed, where Gaussian noise perturbations\ngenerate augmented views through two multilayer perceptrons (MLPs), and a\ncross-view contrastive loss enforces structural consistency between views to\nlearn noise-invariant representations. Finally, latent embeddings optimized by\nthis process are clustered via K-means. Extensive experiments and repeated\ncomparative analysis have verified that our SLCGC contains high clustering\naccuracy, low computational complexity, and strong robustness. The code source\nwill be available at https:\/\/github.com\/DY-HYX.",
        "Underwater imaging often suffers from significant visual degradation, which\nlimits its suitability for subsequent applications. While recent underwater\nimage enhancement (UIE) methods rely on the current advances in deep neural\nnetwork architecture designs, there is still considerable room for improvement\nin terms of cross-scene robustness and computational efficiency. Diffusion\nmodels have shown great success in image generation, prompting us to consider\ntheir application to UIE tasks. However, directly applying them to UIE tasks\nwill pose two challenges, \\textit{i.e.}, high computational budget and color\nunbalanced perturbations. To tackle these issues, we propose DiffColor, a\ndistribution-aware diffusion and cross-spectral refinement model for efficient\nUIE. Instead of diffusing in the raw pixel space, we transfer the image into\nthe wavelet domain to obtain such low-frequency and high-frequency spectra, it\ninherently reduces the image spatial dimensions by half after each\ntransformation. Unlike single-noise image restoration tasks, underwater imaging\nexhibits unbalanced channel distributions due to the selective absorption of\nlight by water. To address this, we design the Global Color Correction (GCC)\nmodule to handle the diverse color shifts, thereby avoiding potential global\ndegradation disturbances during the denoising process. For the sacrificed image\ndetails caused by underwater scattering, we further present the Cross-Spectral\nDetail Refinement (CSDR) to enhance the high-frequency details, which are\nintegrated with the low-frequency signal as input conditions for guiding the\ndiffusion. This way not only ensures the high-fidelity of sampled content but\nalso compensates for the sacrificed details. Comprehensive experiments\ndemonstrate the superior performance of DiffColor over state-of-the-art methods\nin both quantitative and qualitative evaluations.",
        "Echo Planar Imaging (EPI) is widely used for its rapid acquisition but\nsuffers from severe geometric distortions due to B0 inhomogeneities,\nparticularly along the phase encoding direction. Existing methods follow a\ntwo-step process: reconstructing blip-up\/down EPI images, then estimating B0,\nwhich can introduce error accumulation and reduce correction accuracy. This is\nespecially problematic in high B0 regions, where distortions align along the\nsame axis, making them harder to disentangle. In this work, we propose a novel\napproach that integrates Implicit Neural Representations (INRs) with a\nphysics-informed correction model to jointly estimate B0 inhomogeneities and\nreconstruct distortion-free images from rotated-view EPI acquisitions. INRs\noffer a flexible, continuous representation that inherently captures complex\nspatial variations without requiring predefined grid-based field maps. By\nleveraging this property, our method dynamically adapts to subject-specific B0\nvariations and improves robustness across different imaging conditions.\nExperimental results on 180 slices of brain images from three subjects\ndemonstrate that our approach outperforms traditional methods in terms of\nreconstruction quality and field estimation accuracy.",
        "Monitoring optical properties of coastal and open ocean waters is crucial to\nassessing the health of marine ecosystems. Deep learning offers a promising\napproach to address these ecosystem dynamics, especially in scenarios where\ngap-free ground-truth data is lacking, which poses a challenge for designing\neffective training frameworks. Using an advanced neural variational data\nassimilation scheme (called 4DVarNet), we introduce a comprehensive training\nframework designed to effectively train directly on gappy data sets. Using the\nMediterranean Sea as a case study, our experiments not only highlight the high\nperformance of the chosen neural network in reconstructing gap-free images from\ngappy datasets but also demonstrate its superior performance over\nstate-of-the-art algorithms such as DInEOF and Direct Inversion, whether using\nCNN or UNet architectures.",
        "This work introduces a novel near real-time (real-time after an initial short\ndelay) MRI solution that handles motion well and is generalizable. Here,\nreal-time means the algorithm works well on a highly accelerated scan, is\nzero-latency (reconstructs a new frame as soon as MRI data for it arrives), and\nis fast enough, i.e., the time taken to process a frame is comparable to the\nscan time per frame or lesser. We demonstrate its generalizability through\nexperiments on 6 prospective datasets and 17 retrospective datasets that span\nmultiple different applications -- speech larynx imaging, brain, ungated\ncardiac perfusion, cardiac cine, cardiac OCMR, abdomen; sampling schemes --\nCartesian, pseudo-radial, radial, spiral; and sampling rates -- ranging from 6x\nto 4 radial lines per frame. Comparisons with a large number of existing\nreal-time and batch methods, including unsupervised and supervised deep\nlearning methods, show the power and speed of our approach.",
        "Postoperative prognostic prediction for colorectal cancer liver metastasis\n(CRLM) remains challenging due to tumor heterogeneity, dynamic evolution of the\nhepatic microenvironment, and insufficient multimodal data fusion. To address\nthese issues, we propose 4D-ACFNet, the first framework that synergistically\nintegrates lightweight spatiotemporal modeling, cross-modal dynamic\ncalibration, and personalized temporal prediction within a unified\narchitecture. Specifically, it incorporates a novel 4D spatiotemporal attention\nmechanism, which employs spatiotemporal separable convolution (reducing\nparameter count by 41%) and virtual timestamp encoding to model the interannual\nevolution patterns of postoperative dynamic processes, such as liver\nregeneration and steatosis. For cross-modal feature alignment, Transformer\nlayers are integrated to jointly optimize modality alignment loss and\ndisentanglement loss, effectively suppressing scale mismatch and redundant\ninterference in clinical-imaging data. Additionally, we design a dynamic\nprognostic decision module that generates personalized interannual recurrence\nrisk heatmaps through temporal upsampling and a gated classification head,\novercoming the limitations of traditional methods in temporal dynamic modeling\nand cross-modal alignment. Experiments on 197 CRLM patients demonstrate that\nthe model achieves 100% temporal adjacency accuracy (TAA), with performance\nsignificantly surpassing existing approaches. This study establishes the first\nspatiotemporal modeling paradigm for postoperative dynamic monitoring of CRLM.\nThe proposed framework can be extended to prognostic analysis of multi-cancer\nmetastases, advancing precision surgery from \"spatial resection\" to\n\"spatiotemporal cure.\"",
        "Based on the helicity method, formulae for the semileptonic transition of\n$\\Lambda_b^0 \\to \\Lambda_c^+ \\ell^- \\bar \\nu_\\ell$ including lepton mass\neffects are derived. In order to calculate the form factors of the $\\Lambda_b$\nbaryon transition matrix element, we employ the phase-moment parameterization\nand perform fits to the Lattice QCD data. With the help of the obtained form\nfactors, six helicity amplitudes and the differential decay widths are\nevaluated. Through appropriate angular integrations, we express the helicity\nflip, helicity nonflip integrated decay rates, and the lepton-side\nforward-backward asymmetry. We present a numerical analysis of these physical\nobservables. We obtain the mentioned physical quantities by performing fits to\nthe Lattice QCD data using the well-known Boyd-Grinstein-Lebed parametrization.\nComparisons with other experimental and theoretical data are also discussed.",
        "We study the ultraviolet problem for models of a finite-dimensional quantum\nmechanical system linearly coupled to a bosonic quantum field, such as the\n(many-)spin boson model or its rotating-wave approximation. If the state change\nof the system upon emission or absorption of a boson is either given by a\nnormal matrix or by a 2-nilpotent one, which is the case for the previously\nnamed examples, we prove an optimal renormalization result. We complement it,\nby proving the norm resolvent convergence of appropriately regularized models\nto the renormalized one. Our method consists of a dressing transformation\nargument in the normal case and an appropriate interior boundary condition for\nthe 2-nilpotent case.",
        "A Resistive Plate Chamber using Diamond-Like Carbon electrodes (DLC-RPC) has\nbeen developed as a background tagging detector in the MEG$~$II experiment. The\nDLC-RPC is planned to be installed in a high-intensity and low-momentum muon\nbeam. This detector is required to have a detection efficiency above 90 % with\nfour active gaps in the muon beam due to the limitation of the material budget.\nIn such an environment, the high current flowing through the resistive\nelectrodes causes a voltage drop, which reduces the performance of the DLC-RPC.\nThis voltage drop can be suppressed by implementing a current evacuation\npattern, though discharges are more likely to occur near the pattern. Therefore\nthe pattern must be covered by a protection cover made of an insulator. In this\nstudy, electrode samples with a current evacuation pattern and different widths\nof protection cover (0.2 mm and 0.8 mm) have been produced, and their\nperformance and stability were measured. The detection efficiency of a\nsingle-gap chamber for $\\beta$-rays from a $^{90}$Sr source was measured to be\nup to approximately 60 % in both electrode samples. The target efficiency can\nbe achieved even with a drop of 100 $-$ 150 V. On the other hand, after more\nthan a dozen hours of operation, discharges suddenly occurred and the detector\nwas prevented from further operation. These discharges created current paths on\nthe spacing pillars. This serious problem must be investigated and solved in\nthe future.",
        "We develop a non-Lorentzian approach for quantum emitters (QE) resonantly\ncoupled to localized surface plasmons (LSP) in metal-dielectric structures.\nUsing the exact LSP Green function, we derive non-Lorentzian version of\nMaxwell-Bloch equations which describe LSP in terms of metal complex dielectric\nfunction rather than via Lorentzian resonances. For a single QE coupled to the\nLSP, we obtain an explicit expression for the system effective optical\npolarizability which, in the Lorentzian approximation, recovers the classical\ncoupled oscillator (CO) model. We demonstrate that non-Lorentzian effects\noriginating from the temporal dispersion of metal dielectric function affect\ndramatically the optical spectra as the system transitions to the strong\ncoupling regime. Specifically, in contrast to Lorentzian models, the main\nspectral weight is shifted towards the lower energy polaritonic band,\nconsistent with the experiment.",
        "Anomaly detection in videos is a challenging task as anomalies in different\nvideos are of different kinds. Therefore, a promising way to approach video\nanomaly detection is by learning the non-anomalous nature of the video at hand.\nTo this end, we propose a one-class few-shot learning driven transformer based\napproach for anomaly detection in videos that is self-context aware. Features\nfrom the first few consecutive non-anomalous frames in a video are used to\ntrain the transformer in predicting the non-anomalous feature of the subsequent\nframe. This takes place under the attention of a self-context learned from the\ninput features themselves. After the learning, given a few previous frames, the\nvideo-specific transformer is used to infer if a frame is anomalous or not by\ncomparing the feature predicted by it with the actual. The effectiveness of the\nproposed method with respect to the state-of-the-art is demonstrated through\nqualitative and quantitative results on different standard datasets. We also\nstudy the positive effect of the self-context used in our approach.",
        "Numerous real-world control problems involve dynamics and objectives affected\nby unobservable hidden parameters, ranging from autonomous driving to robotic\nmanipulation, which cause performance degradation during sim-to-real transfer.\nTo represent these kinds of domains, we adopt hidden-parameter Markov decision\nprocesses (HIP-MDPs), which model sequential decision problems where hidden\nvariables parameterize transition and reward functions. Existing approaches,\nsuch as domain randomization, domain adaptation, and meta-learning, simply\ntreat the effect of hidden parameters as additional variance and often struggle\nto effectively handle HIP-MDP problems, especially when the rewards are\nparameterized by hidden variables. We introduce Privileged-Dreamer, a\nmodel-based reinforcement learning framework that extends the existing\nmodel-based approach by incorporating an explicit parameter estimation module.\nPrivilegedDreamer features its novel dual recurrent architecture that\nexplicitly estimates hidden parameters from limited historical data and enables\nus to condition the model, actor, and critic networks on these estimated\nparameters. Our empirical analysis on five diverse HIP-MDP tasks demonstrates\nthat PrivilegedDreamer outperforms state-of-the-art model-based, model-free,\nand domain adaptation learning algorithms. Additionally, we conduct ablation\nstudies to justify the inclusion of each component in the proposed\narchitecture.",
        "Decentralized physical infrastructure networks (DePINs) are an emerging\nvertical within \"Web3\" replacing the traditional method that physical\ninfrastructures are constructed. Yet, the boundaries between DePIN and\ntraditional method of building crowd-sourced infrastructures such as citizen\nscience initiatives or other Web3 verticals are not always so clear cut. In\nthis work, we systematically analyze the differences between DePIN and other\nWeb2 and Web3 verticals. For this, the study proposes a novel decision tree for\nclassifying systems as DePIN. This tree is informed by prior studies and\ndifferentiates DePIN from related concepts using criteria such as the presence\nof a three-sided market, token-based incentives for supply, and the requirement\nfor physical asset placement in those systems.\n  The paper demonstrates the application of the decision tree to various\nblockchain systems, including Helium and Bitcoin, showcasing its practical\nutility in differentiating DePIN systems.\n  This research offers significant contributions towards establishing a more\nobjective and systematic approach to identifying and categorizing DePIN\nsystems. It lays the groundwork for creating a comprehensive and unbiased\ndatabase of DePIN systems, which will inform future research and development\nwithin this emerging sector.",
        "As Large Language Models (LLMs) are increasingly adopted in software\nengineering, recently in the form of conversational assistants, ensuring these\ntechnologies align with developers' needs is essential. The limitations of\ntraditional human-centered methods for evaluating LLM-based tools at scale\nraise the need for automatic evaluation. In this paper, we advocate combining\ninsights from human-computer interaction (HCI) and artificial intelligence (AI)\nresearch to enable human-centered automatic evaluation of LLM-based\nconversational SE assistants. We identify requirements for such evaluation and\nchallenges down the road, working towards a framework that ensures these\nassistants are designed and deployed in line with user needs.",
        "Recent work has highlighted the risks of LLM-generated content for a wide\nrange of harmful behaviors, including incorrect and harmful code. In this work,\nwe extend this by studying whether LLM-generated web design contains dark\npatterns. This work evaluated designs of ecommerce web components generated by\nfour popular LLMs: Claude, GPT, Gemini, and Llama. We tested 13 commonly used\necommerce components (e.g., search, product reviews) and used them as prompts\nto generate a total of 312 components across all models. Over one-third of\ngenerated components contain at least one dark pattern. The majority of dark\npattern strategies involve hiding crucial information, limiting users' actions,\nand manipulating them into making decisions through a sense of urgency. Dark\npatterns are also more frequently produced in components that are related to\ncompany interests. These findings highlight the need for interventions to\nprevent dark patterns during front-end code generation with LLMs and emphasize\nthe importance of expanding ethical design education to a broader audience.",
        "Blast wave models are commonly used to model relativistic outflows from\nultra-relativistic gamma-ray bursts (GRBs), but are also applied to lower\nLorentz factor ejections from X-ray binaries (XRBs). Here we revisit the\nphysics of blast waves and reverse shocks in these systems and explore the\nsimilarities and differences between the ultra-relativistic ($\\Gamma \\gg 1$)\nand moderately relativistic ($\\Gamma \\sim$ a few) regimes. We first demonstrate\nthat the evolution of the blast wave radius as a function of the observer frame\ntime is recovered in the on-axis ultra-relativistic limit from a general energy\nand radius blast wave evolution, emphasizing that XRB ejections are off-axis,\nmoderately relativistic cousins of GRB afterglows. We show that, for fixed\nblast wave or ejecta energy, reverse shocks cross the ejecta much later\n(earlier) on in the evolution for less (more) relativistic systems, and find\nthat reverse shocks are much longer-lived in XRBs and off-axis GRBs compared to\non-axis GRBs. Reverse shock crossing should thus typically finish after\n$\\sim10-100$ days (in the observer frame) in XRB ejections. This\ncharacteristic, together with their moderate Lorentz factors and resolvable\ncore separations, makes XRB ejections unique laboratories for shock and\nparticle acceleration physics. We discuss the impact of geometry and lateral\nspreading on our results, explore how to distinguish between different shock\ncomponents, and comment on the implications for GRB and XRB environments.\nAdditionally, we argue that identification of reverse shock signatures in XRBs\ncould provide an independent constraint on the ejecta Lorentz factor.",
        "Low-dimensional embeddings are essential for machine learning tasks involving\ngraphs, such as node classification, link prediction, community detection,\nnetwork visualization, and network compression. Although recent studies have\nidentified exact low-dimensional embeddings, the limits of the required\nembedding dimensions remain unclear. We presently prove that lower dimensional\nembeddings are possible when using Euclidean metric embeddings as opposed to\nvector-based Logistic PCA (LPCA) embeddings. In particular, we provide an\nefficient logarithmic search procedure for identifying the exact embedding\ndimension and demonstrate how metric embeddings enable inference of the exact\nembedding dimensions of large-scale networks by exploiting that the metric\nproperties can be used to provide linearithmic scaling. Empirically, we show\nthat our approach extracts substantially lower dimensional representations of\nnetworks than previously reported for small-sized networks. For the first time,\nwe demonstrate that even large-scale networks can be effectively embedded in\nvery low-dimensional spaces, and provide examples of scalable, exact\nreconstruction for graphs with up to a million nodes. Our approach highlights\nthat the intrinsic dimensionality of networks is substantially lower than\npreviously reported and provides a computationally efficient assessment of the\nexact embedding dimension also of large-scale networks. The surprisingly low\ndimensional representations achieved demonstrate that networks in general can\nbe losslessly represented using very low dimensional feature spaces, which can\nbe used to guide existing network analysis tasks from community detection and\nnode classification to structure revealing exact network visualizations.",
        "Hexagonal boron nitride (hBN) is an excellent host for solid-state single\nphonon emitters. Experimental observed emission ranges from infrared to\nultraviolet. The emission centers are generally attributed to either intrinsic\nor extrinsic point defects embedded into hBN. Nevertheless, the microscopic\nstructure of most of these defect emitters is uncertain. Here, through\ndensity-functional theory calculations we studied the native antisite defects\nin hBN. We find that the neutral boron antisite might be a nonmagnetic single\nphoton source with zero-phonon-line (ZPL) at 1.58 eV and such a lineshape that\nis often observed in experiments. Furthermore, the positively charged nitrogen\nantisite might be associated with a dim color center recently observed as a\nblue emitter with ZPL at 2.63 eV. These simple single substitution defects\nindicate the existence of out-of-plane phonon mode which significantly affects\nthe optical properties. Our results could provide useful information for\nidentification of quantum emitters in hBN.",
        "Causal inference and the estimation of causal effects plays a central role in\ndecision-making across many areas, including healthcare and economics.\nEstimating causal effects typically requires an estimator that is tailored to\neach problem of interest. But developing estimators can take significant effort\nfor even a single causal inference setting. For example, algorithms for\nregression-based estimators, propensity score methods, and doubly robust\nmethods were designed across several decades to handle causal estimation with\nobserved confounders. Similarly, several estimators have been developed to\nexploit instrumental variables (IVs), including two-stage least-squares (TSLS),\ncontrol functions, and the method-of-moments. In this work, we instead frame\ncausal inference as a dataset-level prediction problem, offloading algorithm\ndesign to the learning process. The approach we introduce, called black box\ncausal inference (BBCI), builds estimators in a black-box manner by learning to\npredict causal effects from sampled dataset-effect pairs. We demonstrate\naccurate estimation of average treatment effects (ATEs) and conditional average\ntreatment effects (CATEs) with BBCI across several causal inference problems\nwith known identification, including problems with less developed estimators.",
        "Hybrid multiterminal Josephson junctions (JJs) are expected to harbor a novel\nclass of Andreev bound states (ABSs), including topologically nontrivial states\nin four-terminal devices. In these systems, topological phases emerge when ABSs\ndepend on at least three superconducting phase differences, resulting in a\nthree-dimensional (3D) energy spectrum characterized by Weyl nodes at zero\nenergy. Here, we realize a four-terminal JJ in a hybrid Al\/InAs\nheterostructure, where ABSs form a synthetic 3D band structure. We probe the\nenergy spectrum using tunneling spectroscopy and identify spectral features\nassociated with the formation of a tri-Andreev molecule, a bound state whose\nenergy depends on three superconducting phases and, therefore, is able to host\ntopological ABSs. The experimental observations are well described by a\nnumerical model. The calculations predict the appearance of four Weyl nodes at\nzero energy within a gap smaller than the experimental resolution. These\ntopological states are theoretically predicted to remain stable within an\nextended region of the parameter space, well accessible by our device. These\nfindings establish an experimental foundation to study high-dimensional\nsynthetic band structures in multiterminal JJs, and to realize topological\nAndreev bands.",
        "Integrated sensing and communication (ISAC) has emerged as a transformative\nparadigm, enabling situationally aware and perceptive next-generation wireless\nnetworks through the co-design of shared network resources. With the adoption\nof millimeter-wave (mmWave) and terahertz (THz) frequency bands, ultra-massive\nMIMO (UM-MIMO) systems and holographic surfaces unlock the potential of\nnear-field (NF) propagation, characterized by spherical wavefronts that\nfacilitate beam manipulation in both angular and range domains. This paper\npresents a unified approach to near-field beam-training and sensing,\nintroducing a dual-purpose codebook design that employs discrete Fourier\ntransform (DFT)-based codebooks for coarse estimation of sensing parameters and\npolar codebooks for parameter refinement. Leveraging these range and angle\nestimates, a customized low-complexity space-time adaptive processing (STAP)\ntechnique is proposed for NF-ISAC to detect slow-moving targets and efficiently\nmitigate clutter. The interplay between codebooks and NF-STAP framework offers\nthree key advantages: reduced communication beam training overhead, improved\nestimation accuracy, and minimal STAP computational complexity. Simulation\nresults show that the proposed framework can reduce STAP complexity by three\norders of magnitude, validating efficacy, and highlighting the potential of the\nproposed approach to seamlessly integrate NF communication and sensing\nfunctionalities in future wireless networks."
      ]
    }
  },
  {
    "id":2411.00688,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Convex generalizations of total variation based on the structure tensor with applications to inverse problems",
    "start_abstract":"We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it.",
    "start_categories":[
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "Parameter-Free FISTA by Adaptive Restart and Backtracking"
      ],
      "abstract":[
        "We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Multi-objective and hierarchical control for coupled stochastic\n  parabolic systems",
        "A dimension reduction procedure for the design of lattice-spring systems\n  with minimal fabrication cost and required multi-functional properties",
        "A new fuzzy fractional differential variational inequality with\n  Mittag-Leffler kernel of order $q \\in (1,2]$",
        "State-of-the-art Methods for Pseudo-Boolean Solving with SCIP",
        "Theorems of nonlinear separation of co-radiant sets and optimality\n  conditions for approximate and proper approximate solutions of vector\n  optimization problems",
        "Under the hood of a carbon footprint calculator",
        "On Optimal Control of Hybrid Dynamical Systems using Complementarity\n  Constraints",
        "Error Bounds for a Class of Cone-Convex Inclusion Problems",
        "Dual Regularization and Outer Approximation of Optimal Control Problems\n  in BV",
        "Solving Non-Monotone Inclusions Using Monotonicity of Pairs of Operators",
        "Assessment various control methods a digital copy of enterprise by\n  integral indicator",
        "Edge downgrades in the maximal covering location problem",
        "On Fenchel c-conjugate dual problems for DC optimization: characterizing\n  weak, strong and stable strong duality",
        "Introduction of the G$_2$-Ricci Flow: Geometric Implications for\n  Spontaneous Symmetry Breaking and Gauge Boson Masses",
        "A bang-bang optimal control for a nonlinear system modeling the Gate\n  Control Theory of Pain",
        "Universality of Packing Dimension Estimates for Spectral Measures of\n  Quasiperiodic Operators: Monotone Potentials",
        "Major-Minor Mean Field Game of Stopping: An Entropy Regularization\n  Approach",
        "TOI-512: Super-Earth transiting a K-type star discovered by TESS and\n  ESPRESSO",
        "Chung-Graham and Zeckendorf representations",
        "The Layered Catalan Monoids: Structure and Determinants",
        "Multi-Channel Currency: A Secure Method Using Semi-Quantum Tokens",
        "High Resolution {\\it BOES} Spectroscopy of Raman-scattered\n  He~II$\\lambda$6545 in Young Planetary Nebulae",
        "Reducing Simulation Effort for RIS Optimization using an Efficient\n  Far-Field Approximation",
        "Learning in Markets with Heterogeneous Agents: Dynamics and Survival of\n  Bayesian vs. No-Regret Learners",
        "Large thermoelectric spin-valve effect with a superconductor",
        "High Harmonic Generation with Orbital Angular Momentum Beams:\n  Beyond-dipole Corrections",
        "Polynomial sequences with the same recurrence relation as Chebyshev\n  polynomials and the minimal polynomial of $2\\cos (2\\pi \/n)$",
        "Preference learning made easy: Everything should be understood through\n  win rate"
      ],
      "abstract":[
        "We study the Stackelberg-Nash null controllability of a coupled system\ngoverned by two linear forward stochastic parabolic equations. The system\nincludes one leader control localized in a subset of the domain, two additional\nleader controls in the diffusion terms, and \\( m \\) follower controls, where \\(\nm \\geq 2 \\). We consider two different scenarios for the followers: first, when\nthe followers minimize a functional involving both components of the system's\nstate, and second, when they minimize a functional involving only the second\ncomponent of the state. For fixed leader controls, we first establish the\nexistence and uniqueness of the Nash equilibrium in both scenarios and provide\nits characterization. As a byproduct, the problem is reformulated as a\nclassical null controllability issue for the associated coupled\nforward-backward stochastic parabolic system. To address this, we derive new\nCarleman estimates for the adjoint stochastic systems. As far as we know, this\nproblem is among the first to be discussed for stochastic coupled systems.",
        "We show that the problem of the design of the lattices of elastoplastic\ncurrent conducting springs with optimal multi-functional properties leads to an\nanalytically tractable problem. Specifically, focusing on a lattice with a\nsmall number of springs, we use the technique of inequalities to reduce the\nnumber variables and to compute the minimal cost of lattice fabrication\nexplicitly.",
        "This paper considers a new fuzzy fractional differential variational\ninequality with Mittag-Leffler kernel of order $q \\in (1,2]$ comprising a fuzzy\nfractional differential inclusion with Mittag-Leffler kernel of order $q \\in\n(1,2]$ and a variational inequality in Euclidean spaces. The existence of\nsolutions for such a novel system is obtained under some mild conditions.",
        "The Pseudo-Boolean problem deals with linear or polynomial constraints with\ninteger coefficients over Boolean variables. The objective lies in optimizing a\nlinear objective function, or finding a feasible solution, or finding a\nsolution that satisfies as many constraints as possible. In the 2024\nPseudo-Boolean competition, solvers incorporating the SCIP framework won five\nout of six categories it was competing in. From a total of 1,207 instances,\nSCIP successfully solved 759, while its parallel version FiberSCIP solved 776.\nBased on the results from the competition, we further enhanced SCIP's\nPseudo-Boolean capabilities. This article discusses the results and presents\nthe winning algorithmic ideas.",
        "This paper deals with \\(\\epsilon\\)-efficient and \\(\\epsilon\\)-proper\nefficient points with respect to a co-radiant set in a vector optimization\nproblem. In the first part of the paper, we establish a new nonlinear\nseparation theorem for co-radiant sets in normed spaces. Subsequently, we\nobtain necessary and sufficient conditions by means of scalarization for both\n\\(\\epsilon\\)-efficient and \\(\\epsilon\\)-proper efficient points in a general\nframework, without any requirements on the co-radiant set or any convexity\nassumption on the sets under consideration.Consequently, our results have a\nwider range of applicability than previously stated in the literature.",
        "We explain the mathematical theory of the Input-Output method for carbon\nfootprints computations.",
        "Optimal control for switch-based dynamical systems is a challenging problem\nin the process control literature. In this study, we model these systems as\nhybrid dynamical systems with finite number of unknown switching points and\nreformulate them using non-smooth and non-convex complementarity constraints as\na mathematical program with complementarity constraints (MPCC). We utilize a\nmoving finite element based strategy to discretize the differential equation\nsystem to accurately locate the unknown switching points at the finite element\nboundary and achieve high-order accuracy at intermediate non-collocation\npoints. We propose a globalization approach to solve the discretized MPCC\nproblem using a mixed NLP\/MILP-based strategy to converge to a non-spurious\nfirst-order optimal solution. The method is tested on three dynamic\noptimization examples, including a gas-liquid tank model and an optimal control\nproblem with a sliding mode solution.",
        "In this paper, we investigate error bounds for cone-convex inclusion problems\nin finite-dimensional settings of the form $f(x)\\in K$, where $K$ is a smooth\ncone and $f$ is a continuously differentiable and $K$-concave function. We show\nthat local error bounds for the inclusion can be characterized by the Abadie\nconstraint qualification around the reference point. In the case where $f$ is\nan affine function, we precisely identify the conditions under which the\ninclusion admits global error bounds. Additionally, we derive some properties\nof smooth cones, as well as regular cones and strictly convex cones.",
        "This paper is concerned with an elliptic optimal control problem with total\nvariation (TV) restriction on the control in the constraints. We introduce a\nregularized optimal control problem by applying a quadratic regularization of\nthe dual representation of the TV-seminorm. The regularized optimal control\nproblem can be solved by means of an outer approximation algorithm. Convergence\nof the regularization for vanishing regularization parameter as well as\nconvergence of the outer approximation algorithm is proven. Moreover, we derive\nnecessary and sufficient optimality conditions for the original unregularized\noptimal control problem and use these to construct an exact solution that we\nuse in our numerical experiments to confirm our theoretical results.",
        "In this paper, under the monotonicity of pairs of operators, we propose some\nGeneralized Proximal Point Algorithms to solve non-monotone inclusions using\nwarped resolvents and transformed resolvents. The weak, strong, and linear\nconvergence of the proposed algorithms are established under very mild\nconditions.",
        "The difficulty of assessing the state lies in a little predictable change in\nthe dimension of a dynamic system under the influence of internal changes and\nenvironmental parameters. In the work, the state of such a system is estimated\nby the method of integral indicators. The application of the method of integral\nindicators allowed us to evaluate the activity of an enterprise. In the present\nwork, the method of integrated indicators is used to assess the control of a\ndigital copy (enterprise).",
        "We tackle the downgrading maximal covering location problem within a network.\nIn this problem, two actors with conflicting objectives are involved: (a) The\nlocation planner aims to determine the location of facilities to maximize the\ncovered demand while anticipating that an attacker will attempt to reduce\ncoverage by increasing the length of some edges (downgrade); (b) The attacker\nseeks to maximize the demand initially covered by the facilities but left\nuncovered after the downgrade. The attacker can increase the length of certain\nedges within a specified budget.\n  We introduce a mixed-integer linear bilevel program to formulate the problem,\nfollowed by a preprocessing phase and a matheuristic algorithm designed to\naddress it. Additionally, computational results are presented to illustrate the\npotential and limitations of the proposed algorithm.",
        "In this paper we present two Fenchel-type dual problems for a DC (difference\nof convex functions) optimization primal one. They have been built by means of\nthe c-conjugation scheme, a pattern of conjugation which has been shown to be\nsuitable for evenly convex functions. We study characterizations of weak,\nstrong and stable strong duality for both pairs of primal-dual problems. We\nalso give conditions which relate the existence of strong and stable strong\nduality for both pairs.",
        "This work introduces the G$_2$-Ricci flow on seven-dimensional manifolds with\nnon-zero torsion and explores its physical implications. By extending the Ricci\nflow to manifolds with G$_2$ structures, we study the evolution of solitonic\nsolutions and their role in spontaneous symmetry breaking in gauge theories. In\nparticular, this model proposes that the masses of the W and Z bosons are\ndetermined not by an external scalar field, as in the Higgs mechanism, but by\nthe intrinsic geometric torsion of the manifold. Furthermore, a possible\nconnection between the geometry of extra dimensions and the curvature of our\nspacetime is explored, with implications for the experimentally observed\npositive cosmological constant. This approach provides an innovative\ninterpretation of fundamental interactions in theoretical physics, opening new\npossibilities for studying extra dimensions and the geometry of\nG$_2$-manifolds.",
        "We consider a nonlinear system of coupled ordinary differential equations\n(representing the excitatory, inhibitory, and T-cell potentials) based on the\nGate Control Theory of Pain, initially proposed by R. Melzack and P.D. Wall in\n1965, and later mathematically modeled by N.F. Britton and S.M. Skevington in\n1988.",
        "Let $H$ be a quasiperiodic Schr\\\"{o}dinger operator generated by a monotone\npotential, as defined in [16]. Following [20], we study the connection between\nthe Lyapunov exponent $L\\left(E\\right)$, arithmetic properties of the frequency\n$\\alpha$, and certain fractal-dimensional properties of the spectral measures\nof $H$.",
        "This paper studies a discrete-time major-minor mean field game of stopping\nwhere the major player can choose either an optimal control or stopping time.\nWe look for the relaxed equilibrium as a randomized stopping policy, which is\nformulated as a fixed point of a set-valued mapping, whose existence is\nchallenging by direct arguments. To overcome the difficulties caused by the\npresence of a major player, we propose to study an auxiliary problem by\nconsidering entropy regularization in the major player's problem while\nformulating the minor players' optimal stopping problems as linear programming\nover occupation measures. We first show the existence of regularized equilibria\nas fixed points of some simplified set-valued operator using the\nKakutani-Fan-Glicksberg fixed-point theorem. Next, we prove that the\nregularized equilibrium converges as the regularization parameter $\\lambda$\ntends to 0, and the limit corresponds to a fixed point of the original\noperator, thereby confirming the existence of a relaxed equilibrium in the\noriginal mean field game problem.",
        "One of the goals of the ESPRESSO guaranteed time observations (GTOs) at the\nESO 8.2m telescope is to follow up on candidate planets from transit surveys\nsuch as the TESS mission. High-precision radial velocities are required to\ncharacterize small exoplanets. Aims. We intend to confirm the existence of a\ntransiting super-Earth around the bright (V=9.74) K0-type star TOI-512 (TIC\n119292328) and provide a characterization. Combining photometric data from TESS\nand 37 high-resolution spectroscopic observations from ESPRESSO in a joint\nMarkov chain Monte Carlo analysis, we determined the planetary parameters of\nTOI-512b and characterized its internal structure. We find that TOI-512b is a\nsuper-Earth, with a radius of $1.54 \\pm 0.10$ R$_\\oplus$ and mass of\n$3.57_{-0.55}^{+0.53}$~M$_\\oplus$, on a $7.19_{-6.1\\cdot 10^{-5}}^{+7\\cdot\n10^{-5}}$ day orbit. This corresponds to a bulk density of\n$5.62_{-1.28}^{+1.59}$ g cm$^{-3}$. Our interior structure analysis presents a\nsmall inner core representing $0.13^{+0.13}_{-0.11}$ of the solid mass fraction\nfor the planet, surrounded by a mantle with a mass fraction of\n$0.69^{+0.20}_{-0.22}$, and an upper limit of the water layer of $0.16$. The\ngas mass below $10^{-8.93}$ indicates a very small amount of gas on the planet.\nWe find no evidence of the second candidate found by the TESS pipeline,\nTOI-512.02, neither in TESS photometry, nor in the ESPRESSO radial velocities.\nThe low stellar activity makes it an interesting transmission spectroscopy\ncandidate for future-generation instruments.",
        "We examine the relationship between the Chung-Graham and Zeckendorf\nrepresentations of an integer using the software package {\\tt Walnut}.",
        "In this paper, we introduce and study a class of monoids, called Layered\nCatalan Monoids (\\( {LC}_n \\)), which satisfy the structural conditions for\n$\\ll$-smoothness as defined in~\\cite{Sha-Det2}. These monoids are defined by\nspecific identities inspired by Catalan monoids. We establish their canonical\nforms and compute their determinant, proving that it is non-zero for \\(1 \\leq n\n\\leq 7\\) but vanishes for \\(n \\geq 8\\).",
        "Digital currencies primarily operate online, but there is growing interest in\nenabling offline transactions to improve digital inclusion. Existing offline\nmethods struggle with double-spending risks, often limiting transaction\namounts. In this work, we propose a quantum-state-based currency system that\nuses the non-cloning theorem to enable secure, multi-channel transactions\nwithout the risk of double spending. We demonstrate this system's\nimplementation with experimental results, including use cases for currency\ntransfers and swaps. To mitigate credit risks in swaps, we also integrate\nblockchain to show its wide applicability. Our approach paves the way for\nquantum-secure digital currencies and opens new possibilities for optimizing\nmulti-channel tokens.",
        "Young planetary nebulae (PNe) are characterized by their hot central stars\nand the presence of abundant neutral and molecular components, which result\nfrom significant mass loss during the asymptotic giant branch (AGB) phase of\nstellar evolution. Far-UV \\ion{He}{2}$\\lambda$1025 line photons produced near\nthe central star can undergo Raman scattering by hydrogen atoms, creating a\nbroad emission feature centered at $\\sim$ 6545~\\AA. We conducted\nhigh-resolution spectroscopy of 12 young PNe from April 2019 to March 2020\nusing the Bohyunsan Observatory Echelle Spectrograph ({\\it BOES}). Building on\nthe study by Choi and Lee, who identified Raman-scattered \\ion{He}{2} at\n6545~\\AA\\ in NGC~6881 and NGC~6886, we report new detections of this feature in\nNGC~6741 and NGC~6884. Profile fitting reveals that the velocity of the\n\\ion{H}{1} component relative to the \\ion{He}{2} emission region ranges from\n$26-33~{\\rm km~s^{-1}}$ in these PNe. Using photoionization modeling, we\nestimate the line flux of \\ion{He}{2}$\\lambda$1025 and derive Raman conversion\nefficiencies of 0.39, 0.21, 0.24, and 0.07 for NGC~6881, NGC~6741, NGC~6886,\nand NGC~6884, respectively. These results, combined with radiative transfer\nmodeling, suggest the presence of \\ion{H}{1} components with masses around\n$10^{-2}~M_\\odot$, moving outward from the central \\ion{He}{2} emission region\nat speeds characteristic of the slow stellar wind from a mass-losing giant\nstar.",
        "Optimization of Reconfigurable Intelligent Surfaces (RIS) via a previously\nintroduced method is effective, but time-consuming, because multiport impedance\nor scatter matrices are required for each transmitter and receiver position,\nwhich generally must be obtained through full-wave simulation. Herein, a simple\nand efficient far-field approximation is introduced, to extrapolate scatter\nmatrices for arbitrary receiver and transmitter positions from only a single\nsimulation while still maintaining high accuracy suitable for optimization\npurposes. This is demonstrated through comparisons of the optimized capacitance\nvalues and further supported by empirical measurements.",
        "We analyze the performance of heterogeneous learning agents in asset markets\nwith stochastic payoffs. Our agents aim to maximize the expected growth rate of\ntheir wealth but have different theories on how to learn this best. We focus on\ncomparing Bayesian and no-regret learners in market dynamics. Bayesian learners\nwith a prior over a finite set of models that assign positive prior probability\nto the correct model have posterior probabilities that converge exponentially\nto the correct model. Consequently, they survive even in the presence of agents\nwho invest according to the correct model of the stochastic process. Bayesians\nwith a continuum prior converge to the correct model at a rate of $O((\\log\nT)\/T)$. Online learning theory provides no-regret algorithms for maximizing the\nlog of wealth in this setting, achieving a worst-case regret bound of $O(\\log\nT)$ without assuming a steady underlying stochastic process but comparing to\nthe best fixed investment rule. This regret, as we observe, is of the same\norder of magnitude as that of a Bayesian learner with a continuum prior.\nHowever, we show that even such low regret may not be sufficient for survival\nin asset markets: an agent can have regret as low as $O(\\log T)$, but still\nvanish in market dynamics when competing against agents who invest according to\nthe correct model or even against a perfect Bayesian with a finite prior. On\nthe other hand, we show that Bayesian learning is fragile, while no-regret\nlearning requires less knowledge of the environment and is therefore more\nrobust. Any no-regret learner will drive out of the market an imperfect\nBayesian whose finite prior or update rule has even small errors. We formally\nestablish the relationship between notions of survival, vanishing, and market\ndomination studied in economics and the framework of regret minimization, thus\nbridging these theories.",
        "Recent studies have revealed magnetically controllable thermoelectric effects\nin superconductor\/ferromagnet (S\/F) structures. A tunable cryogenic\nthermoelectric generator needs not only a high conversion factor between\nelectricity and heat, but also a large change in the thermoelectric output when\nswitching the magnetic state of the device. Here, we experimentally measure and\nnumerically model thermoelectric effects in fully epitaxial F\/S\/F junctions\nbased on commercially available, easily grown materials, as well as their\ndependence on the magnetic configuration of the F electrodes. We observe\nsizeable Seebeck coefficients for the parallel alignment of the ferromagnetic\nelectrodes, reaching values of about $100$~$\\mu$V\/K. Importantly, we find a\ndecrease of the thermoelectric signal of more than an order of magnitude when\nswitching from a parallel to an antiparallel configuration, constituting a\nlarge thermoelectric spin-valve effect. Theoretical modeling based on a\nself-consistent non-equilibrium Keldysh-Usadel Green function theory, combined\nwith micromagnetic simulations, qualitatively reproduce the experimental\nfindings. These findings pave the way for the development of efficient and\nversatile cryogenic thermoelectric heat engines.",
        "We study the high harmonic generation with vortex beams beyond the dipole\napproximation. To do so we employ the full minimal coupling approach to account\nfor multipolar coupling without truncation and describe the full\nspatio-temporal properties of the electromagnetic field. This allows us to\ninvestigate the beyond-dipole deviations in electron trajectories and the\nemitted power, where the influence of the orbital angular momentum contains\nboth magnetic and quadrupolar effects. In contrast to the system driven by\nplane-wave light, we show that the non-linear dipole dynamics induced by the\nvortex beams are not confined to the polarization or propagation directions,\nbut also have a component in the orthogonal direction. We identify the effects\nof the resulting symmetry breaking via increased beyond dipole corrections\nwhich are particularly apparent in even harmonics.",
        "In this paper we consider the minimal polynomial $\\psi_n(x)$ of $2\\cos (2\\pi\n\/n)$. We introduce some polynomial sequences with the same recurrence relation\nas the rescaled Chebyshev polynomials $t_n(x)=2\\, T_n(x\/2)$ of the first kind,\nwhich turn out to be related to those of various kinds, all coming from those\nof the second kind. We see that $t_n(x)\\pm 2=2(T_n(x\/2)\\pm 1)$ are divisible by\nthe square of either of these polynomials. Then by appropriately removing\nunnecessary factors from these polynomials, we can easily calculate\n$\\psi_n(x)$, which improves Barnes' result in 1977. As an appendix, we give a\ncompact list of the minimal polynomials $\\psi_n(x)$ of $2\\cos (2\\pi \/n)$ for\n$n\\leqslant 120$.",
        "Preference learning, or the task of aligning generative models to preference\ncomparison data, has yet to reach the conceptual maturity of classification,\ndensity estimation, etc. To close this gap, this work presents a framework to\nunderstand preference learning starting from the sampling distribution of\npairwise preference data. First, we prove that the only evaluation of a\ngenerative model that respects both preferences and prevalences in the data\ndistribution is a form of win rate, justifying win rate as the focal point to\nunderstand preference learning. We then analyze preference learning methods as\nwin rate optimization (WRO) or non-WRO. We present novel instances of WRO\nbeyond existing examples (RLHF, NLHF) and identify two key theoretical benefits\nof all such methods. We prove that common non-WRO methods like DPO and SFT on\npreferred samples lack these properties and suggest ways to mitigate such\ntheoretical limitations. We also show that WRO underperforms in practice due\noptimization difficulties and that optimization success predicts performance\nbetter than choices which affect the objective's solution. Our analysis\nhighlights best practices for existing methods and provides recommendations for\nfuture research, guided by the principle that one should either align non-WRO\nmethods more closely with WRO or improve the optimization of WRO objectives."
      ]
    }
  },
  {
    "id":2412.00036,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"On the Distribution of the Two-Sample Cramer-von Mises Criterion",
    "start_abstract":"The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",
    "start_categories":[
      "q-fin.GN"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "Quant GANs: deep generation of financial time series"
      ],
      "abstract":[
        "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "StepMathAgent: A Step-Wise Agent for Evaluating Mathematical Processes\n  through Tree-of-Error",
        "Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation",
        "Preference Optimization via Contrastive Divergence: Your Reward Model is\n  Secretly an NLL Estimator",
        "Correctness Learning: Deductive Verification Guided Learning for\n  Human-AI Collaboration",
        "Generative AI in Transportation Planning: A Survey",
        "What Is a Counterfactual Cause in Action Theories?",
        "LapSum -- One Method to Differentiate Them All: Ranking, Sorting and\n  Top-k Selection",
        "Offline Critic-Guided Diffusion Policy for Multi-User Delay-Constrained\n  Scheduling",
        "Salience-Invariant Consistent Policy Learning for Generalization in\n  Visual Reinforcement Learning",
        "A Minimax Approach to Ad Hoc Teamwork",
        "Human-Alignment Influences the Utility of AI-assisted Decision Making",
        "Leveraging Taxonomy and LLMs for Improved Multimodal Hierarchical\n  Classification",
        "SPRIG: Stackelberg Perception-Reinforcement Learning with Internal Game\n  Dynamics",
        "Performance of the ALICE Inner Tracking System 2",
        "Empowering the Future Workforce: Prioritizing Education for the\n  AI-Accelerated Job Market",
        "Magnetic Field Structures In and Around Seyfert Galaxy Outflows",
        "Human Re-ID Meets LVLMs: What can we expect?",
        "Providing Machine Learning Potentials with High Quality Uncertainty\n  Estimates",
        "Causal AI-based Root Cause Identification: Research to Practice at Scale",
        "Non-local modular flows across deformed null-cuts",
        "CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language\n  Models",
        "2D transmons with lifetimes and coherence times exceeding 1 millisecond",
        "Validation of the DESI DR2 Measurements of Baryon Acoustic Oscillations\n  from Galaxies and Quasars",
        "Fusion DeepONet: A Data-Efficient Neural Operator for Geometry-Dependent\n  Hypersonic Flows on Arbitrary Grids",
        "On the Effectiveness of Random Weights in Graph Neural Networks",
        "3+1 neutrino mixings model with $A_4$ triplet Majorana neutrino",
        "Mapping AI Avant-Gardes in Time: Posthumanism, Transhumanism,\n  Genhumanism",
        "regMMD: a package for parametric estimation and regression with maximum\n  mean discrepancy"
      ],
      "abstract":[
        "Evaluating mathematical capabilities is critical for assessing the overall\nperformance of large language models (LLMs). However, existing evaluation\nmethods often focus solely on final answers, resulting in highly inaccurate and\nuninterpretable evaluation outcomes, as well as their failure to assess proof\nor open-ended problems. To address these issues, we propose a novel\nmathematical process evaluation agent based on Tree-of-Error, called\nStepMathAgent. This agent incorporates four internal core operations: logical\nstep segmentation, step scoring, score aggregation and error tree generation,\nalong with four external extension modules: difficulty calibration, simplicity\nevaluation, completeness validation and format assessment. Furthermore, we\nintroduce StepMathBench, a benchmark comprising 1,000 step-divided process\nevaluation instances, derived from 200 high-quality math problems grouped by\nproblem type, subject category and difficulty level. Experiments on\nStepMathBench show that our proposed StepMathAgent outperforms all\nstate-of-the-art methods, demonstrating human-aligned evaluation preferences\nand broad applicability to various scenarios. Our data and code are available\nat https:\/\/github.com\/SHU-XUN\/StepMathAgent.",
        "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs.",
        "Existing studies on preference optimization (PO) have centered on\nconstructing pairwise preference data following simple heuristics, such as\nmaximizing the margin between preferred and dispreferred completions based on\nhuman (or AI) ranked scores. However, none of these heuristics has a full\ntheoretical justification. In this work, we develop a novel PO framework that\nprovides theoretical guidance to effectively sample dispreferred completions.\nTo achieve this, we formulate PO as minimizing the negative log-likelihood\n(NLL) of a probability model and propose to estimate its normalization constant\nvia a sampling strategy. As we will demonstrate, these estimative samples can\nact as dispreferred completions in PO. We then select contrastive divergence\n(CD) as the sampling strategy, and propose a novel MC-PO algorithm that applies\nthe Monte Carlo (MC) kernel from CD to sample hard negatives w.r.t. the\nparameterized reward model. Finally, we propose the OnMC-PO algorithm, an\nextension of MC-PO to the online setting. On popular alignment benchmarks,\nMC-PO outperforms existing SOTA baselines, and OnMC-PO leads to further\nimprovement.",
        "Despite significant progress in AI and decision-making technologies in\nsafety-critical fields, challenges remain in verifying the correctness of\ndecision output schemes and verification-result driven design. We propose\ncorrectness learning (CL) to enhance human-AI collaboration integrating\ndeductive verification methods and insights from historical high-quality\nschemes. The typical pattern hidden in historical high-quality schemes, such as\nchange of task priorities in shared resources, provides critical guidance for\nintelligent agents in learning and decision-making. By utilizing deductive\nverification methods, we proposed patten-driven correctness learning (PDCL),\nformally modeling and reasoning the adaptive behaviors-or 'correctness\npattern'-of system agents based on historical high-quality schemes, capturing\nthe logical relationships embedded within these schemes. Using this logical\ninformation as guidance, we establish a correctness judgment and feedback\nmechanism to steer the intelligent decision model toward the 'correctness\npattern' reflected in historical high-quality schemes. Extensive experiments\nacross multiple working conditions and core parameters validate the framework's\ncomponents and demonstrate its effectiveness in improving decision-making and\nresource optimization.",
        "The integration of generative artificial intelligence (GenAI) into\ntransportation planning has the potential to revolutionize tasks such as demand\nforecasting, infrastructure design, policy evaluation, and traffic simulation.\nHowever, there is a critical need for a systematic framework to guide the\nadoption of GenAI in this interdisciplinary domain. In this survey, we, a\nmultidisciplinary team of researchers spanning computer science and\ntransportation engineering, present the first comprehensive framework for\nleveraging GenAI in transportation planning. Specifically, we introduce a new\ntaxonomy that categorizes existing applications and methodologies into two\nperspectives: transportation planning tasks and computational techniques. From\nthe transportation planning perspective, we examine the role of GenAI in\nautomating descriptive, predictive, generative, simulation, and explainable\ntasks to enhance mobility systems. From the computational perspective, we\ndetail advancements in data preparation, domain-specific fine-tuning, and\ninference strategies, such as retrieval-augmented generation and zero-shot\nlearning tailored to transportation applications. Additionally, we address\ncritical challenges, including data scarcity, explainability, bias mitigation,\nand the development of domain-specific evaluation frameworks that align with\ntransportation goals like sustainability, equity, and system efficiency. This\nsurvey aims to bridge the gap between traditional transportation planning\nmethodologies and modern AI techniques, fostering collaboration and innovation.\nBy addressing these challenges and opportunities, we seek to inspire future\nresearch that ensures ethical, equitable, and impactful use of generative AI in\ntransportation planning.",
        "Since the proposal by Halpern and Pearl, reasoning about actual causality has\ngained increasing attention in artificial intelligence, ranging from domains\nsuch as model-checking and verification to reasoning about actions and\nknowledge. More recently, Batusov and Soutchanski proposed a notion of actual\nachievement cause in the situation calculus, amongst others, they can determine\nthe cause of quantified effects in a given action history. While intuitively\nappealing, this notion of cause is not defined in a counterfactual perspective.\nIn this paper, we propose a notion of cause based on counterfactual analysis.\nIn the context of action history, we show that our notion of cause generalizes\nnaturally to a notion of achievement cause. We analyze the relationship between\nour notion of the achievement cause and the achievement cause by Batusov and\nSoutchanski. Finally, we relate our account of cause to Halpern and Pearl's\naccount of actual causality. Particularly, we note some nuances in applying a\ncounterfactual viewpoint to disjunctive goals, a common thorn to definitions of\nactual causes.",
        "We present a novel technique for constructing differentiable order-type\noperations, including soft ranking, soft top-k selection, and soft\npermutations. Our approach leverages an efficient closed-form formula for the\ninverse of the function LapSum, defined as the sum of Laplace distributions.\nThis formulation ensures low computational and memory complexity in selecting\nthe highest activations, enabling losses and gradients to be computed in\n$O(n\\log{}n)$ time. Through extensive experiments, we demonstrate that our\nmethod outperforms state-of-the-art techniques for high-dimensional vectors and\nlarge $k$ values. Furthermore, we provide efficient implementations for both\nCPU and CUDA environments, underscoring the practicality and scalability of our\nmethod for large-scale ranking and differentiable ordering problems.",
        "Effective multi-user delay-constrained scheduling is crucial in various\nreal-world applications, such as instant messaging, live streaming, and data\ncenter management. In these scenarios, schedulers must make real-time decisions\nto satisfy both delay and resource constraints without prior knowledge of\nsystem dynamics, which are often time-varying and challenging to estimate.\nCurrent learning-based methods typically require interactions with actual\nsystems during the training stage, which can be difficult or impractical, as it\nis capable of significantly degrading system performance and incurring\nsubstantial service costs. To address these challenges, we propose a novel\noffline reinforcement learning-based algorithm, named \\underline{S}cheduling By\n\\underline{O}ffline Learning with \\underline{C}ritic Guidance and\n\\underline{D}iffusion Generation (SOCD), to learn efficient scheduling policies\npurely from pre-collected \\emph{offline data}. SOCD innovatively employs a\ndiffusion-based policy network, complemented by a sampling-free critic network\nfor policy guidance. By integrating the Lagrangian multiplier optimization into\nthe offline reinforcement learning, SOCD effectively trains high-quality\nconstraint-aware policies exclusively from available datasets, eliminating the\nneed for online interactions with the system. Experimental results demonstrate\nthat SOCD is resilient to various system dynamics, including partially\nobservable and large-scale environments, and delivers superior performance\ncompared to existing methods.",
        "Generalizing policies to unseen scenarios remains a critical challenge in\nvisual reinforcement learning, where agents often overfit to the specific\nvisual observations of the training environment. In unseen environments,\ndistracting pixels may lead agents to extract representations containing\ntask-irrelevant information. As a result, agents may deviate from the optimal\nbehaviors learned during training, thereby hindering visual generalization.To\naddress this issue, we propose the Salience-Invariant Consistent Policy\nLearning (SCPL) algorithm, an efficient framework for zero-shot generalization.\nOur approach introduces a novel value consistency module alongside a dynamics\nmodule to effectively capture task-relevant representations. The value\nconsistency module, guided by saliency, ensures the agent focuses on\ntask-relevant pixels in both original and perturbed observations, while the\ndynamics module uses augmented data to help the encoder capture dynamic- and\nreward-relevant representations. Additionally, our theoretical analysis\nhighlights the importance of policy consistency for generalization. To\nstrengthen this, we introduce a policy consistency module with a KL divergence\nconstraint to maintain consistent policies across original and perturbed\nobservations.Extensive experiments on the DMC-GB, Robotic Manipulation, and\nCARLA benchmarks demonstrate that SCPL significantly outperforms\nstate-of-the-art methods in terms of generalization. Notably, SCPL achieves\naverage performance improvements of 14\\%, 39\\%, and 69\\% in the challenging DMC\nvideo hard setting, the Robotic hard setting, and the CARLA benchmark,\nrespectively.Project Page: https:\/\/sites.google.com\/view\/scpl-rl.",
        "We propose a minimax-Bayes approach to Ad Hoc Teamwork (AHT) that optimizes\npolicies against an adversarial prior over partners, explicitly accounting for\nuncertainty about partners at time of deployment. Unlike existing methods that\nassume a specific distribution over partners, our approach improves worst-case\nperformance guarantees. Extensive experiments, including evaluations on\ncoordinated cooking tasks from the Melting Pot suite, show our method's\nsuperior robustness compared to self-play, fictitious play, and best response\nlearning. Our work highlights the importance of selecting an appropriate\ntraining distribution over teammates to achieve robustness in AHT.",
        "Whenever an AI model is used to predict a relevant (binary) outcome in\nAI-assisted decision making, it is widely agreed that, together with each\nprediction, the model should provide an AI confidence value. However, it has\nbeen unclear why decision makers have often difficulties to develop a good\nsense on when to trust a prediction using AI confidence values. Very recently,\nCorvelo Benz and Gomez Rodriguez have argued that, for rational decision\nmakers, the utility of AI-assisted decision making is inherently bounded by the\ndegree of alignment between the AI confidence values and the decision maker's\nconfidence on their own predictions. In this work, we empirically investigate\nto what extent the degree of alignment actually influences the utility of\nAI-assisted decision making. To this end, we design and run a large-scale human\nsubject study (n=703) where participants solve a simple decision making task -\nan online card game - assisted by an AI model with a steerable degree of\nalignment. Our results show a positive association between the degree of\nalignment and the utility of AI-assisted decision making. In addition, our\nresults also show that post-processing the AI confidence values to achieve\nmulticalibration with respect to the participants' confidence on their own\npredictions increases both the degree of alignment and the utility of\nAI-assisted decision making.",
        "Multi-level Hierarchical Classification (MLHC) tackles the challenge of\ncategorizing items within a complex, multi-layered class structure. However,\ntraditional MLHC classifiers often rely on a backbone model with independent\noutput layers, which tend to ignore the hierarchical relationships between\nclasses. This oversight can lead to inconsistent predictions that violate the\nunderlying taxonomy. Leveraging Large Language Models (LLMs), we propose a\nnovel taxonomy-embedded transitional LLM-agnostic framework for multimodality\nclassification. The cornerstone of this advancement is the ability of models to\nenforce consistency across hierarchical levels. Our evaluations on the MEP-3M\ndataset - a multi-modal e-commerce product dataset with various hierarchical\nlevels - demonstrated a significant performance improvement compared to\nconventional LLM structures.",
        "Deep reinforcement learning agents often face challenges to effectively\ncoordinate perception and decision-making components, particularly in\nenvironments with high-dimensional sensory inputs where feature relevance\nvaries. This work introduces SPRIG (Stackelberg Perception-Reinforcement\nlearning with Internal Game dynamics), a framework that models the internal\nperception-policy interaction within a single agent as a cooperative\nStackelberg game. In SPRIG, the perception module acts as a leader,\nstrategically processing raw sensory states, while the policy module follows,\nmaking decisions based on extracted features. SPRIG provides theoretical\nguarantees through a modified Bellman operator while preserving the benefits of\nmodern policy optimization. Experimental results on the Atari BeamRider\nenvironment demonstrate SPRIG's effectiveness, achieving around 30% higher\nreturns than standard PPO through its game-theoretical balance of feature\nextraction and decision-making.",
        "The upgraded Inner Tracking System (ITS2) of the ALICE experiment at the CERN\nLarge Hadron Collider is based on Monolithic Active Pixel Sensors (MAPS). With\na sensitive area of about 10 $m^2$ and 12.5 billion pixels, ITS2 represents the\nlargest pixel detector in high-energy physics. The detector consists of seven\nconcentric layers equipped with ALPIDE pixel sensors manufactured in the\nTowerJazz 180 nm CMOS Imaging Sensor process. The high spatial resolution and\nlow material budget, in combination with small radial distance of the innermost\nlayer from the interaction point, make the detector well suited for secondary\nvertex reconstruction as well as for tracking at low transverse momentum.\n  This paper will present the detector performance during the LHC Run 3 and\ngive an overview on the calibration methods and running experience.",
        "AI's rapid integration into the workplace demands new approaches to workforce\neducation and training and broader AI literacy across disciplines. Coordinated\naction from government, industry, and educational institutions is necessary to\nensure workers can adapt to accelerating technological change.",
        "We present radio polarimetric images of 12 Seyfert and Low-Ionization Nuclear\nEmission-line Region (LINER) galaxies belonging to the Centre for Astrophysics\n(CfA)+12 micron sample exhibiting kiloparsec-scale radio outflows (KSRs). These\nobservations have been carried out at 10 GHz with Karl G. Jansky Very Large\nArray (VLA) in D-array and at 1.4 GHz with the BnA$\\rightarrow$A array\nconfigurations. We find signatures of organized magnetic (B-) field structures\nin the cores, jets and lobes of these galaxies. The linear polarization\nfraction varies from a few per cent in the cores to $47\\pm18$ per cent in the\nlobes. The inferred B-fields are toroidal in the cores of several sources\nmaking them consistent with the presence of either a sheath-like or a wind-like\ncomponent surrounding the jet. The in-band spectral index images typically show\nthe presence of flat\/inverted spectrum cores and steep spectrum lobes. Radio\ncores with flatter spectra are found to have lower Eddington ratios while the\nsteeper ones have higher. A strong correlation is observed between the\nSeyfert\/LINER radio outflow properties and the mass of the supermassive black\nholes (SMBHs); correlations with Eddington ratios are weaker. We find\nsignatures of jet-medium interaction and both positive and negative AGN\nfeedback in these sources. Overall, our study indicates that radio-quiet (RQ)\nAGN with KSRs possess radio outflows driven by magnetic fields anchored to\ntheir black holes - accretion disks, which significantly impact their\nenvironments.",
        "Large vision-language models (LVLMs) have been regarded as a breakthrough\nadvance in an astoundingly variety of tasks, from content generation to virtual\nassistants and multimodal search or retrieval. However, for many of these\napplications, the performance of these methods has been widely criticized,\nparticularly when compared with state-of-the-art methods and technologies in\neach specific domain. In this work, we compare the performance of the leading\nlarge vision-language models in the human re-identification task, using as\nbaseline the performance attained by state-of-the-art AI models specifically\ndesigned for this problem. We compare the results due to ChatGPT-4o,\nGemini-2.0-Flash, Claude 3.5 Sonnet, and Qwen-VL-Max to a baseline ReID\nPersonViT model, using the well-known Market1501 dataset. Our evaluation\npipeline includes the dataset curation, prompt engineering, and metric\nselection to assess the models' performance. Results are analyzed from many\ndifferent perspectives: similarity scores, classification accuracy, and\nclassification metrics, including precision, recall, F1 score, and area under\ncurve (AUC). Our results confirm the strengths of LVLMs, but also their severe\nlimitations that often lead to catastrophic answers and should be the scope of\nfurther research. As a concluding remark, we speculate about some further\nresearch that should fuse traditional and LVLMs to combine the strengths from\nboth families of techniques and achieve solid improvements in performance.",
        "Computational chemistry has come a long way over the course of several\ndecades, enabling subatomic level calculations particularly with the\ndevelopment of Density Functional Theory (DFT). Recently, machine-learned\npotentials (MLP) have provided a way to overcome the prevalent time and length\nscale constraints in such calculations. Unfortunately, these models utilise\ncomplex and high dimensional representations, making it challenging for users\nto intuit performance from chemical structure, which has motivated the\ndevelopment of methods for uncertainty quantification. One of the most common\nmethods is to introduce an ensemble of models and employ an averaging approach\nto determine the uncertainty. In this work, we introduced Bayesian Neural\nNetworks (BNNs) for uncertainty aware energy evaluation as a more principled\nand resource efficient method to achieve this goal. The richness of our\nuncertainty quantification enables a new type of hybrid workflow where\ncalculations can be offloaded to a MLP in a principled manner.",
        "Modern applications are built as large, distributed systems spanning numerous\nmodules, teams, and data centers. Despite robust engineering and recovery\nstrategies, failures and performance issues remain inevitable, risking\nsignificant disruptions and affecting end users. Rapid and accurate root cause\nidentification is therefore vital to ensure system reliability and maintain key\nservice metrics.\n  We have developed a novel causality-based Root Cause Identification (RCI)\nalgorithm that emphasizes causation over correlation. This algorithm has been\nintegrated into IBM Instana-bridging research to practice at scale-and is now\nin production use by enterprise customers. By leveraging \"causal AI,\" Instana\nstands apart from typical Application Performance Management (APM) tools,\npinpointing issues in near real-time. This paper highlights Instana's advanced\nfailure diagnosis capabilities, discussing both the theoretical underpinnings\nand practical implementations of the RCI algorithm. Real-world examples\nillustrate how our causality-based approach enhances reliability and\nperformance in today's complex system landscapes.",
        "Modular flows probe important aspects of the entanglement structures,\nespecially those of QFTs, in a dynamical framework. Despite the expected\nnon-local nature in the general cases, the majority of explicitly understood\nexamples feature local space-time trajectories under modular flows. In this\nwork, we study a particular class of non-local modular flows. They are\nassociated with the relativistic vacuum state and sub-regions whose boundaries\nlie on a planar null-surface. They satisfy a remarkable algebraic property\nknown as the half-sided modular inclusion, and as a result the modular\nHamiltonians are exactly known in terms of the stress tensor operators. To be\nexplicit, we focus on the simplest QFT of a massive or massless free scalar in\n$2+1$ dimensions. We obtain explicit expressions for the generators. They can\nbe separated into a sum of local and non-local terms showing certain universal\npattern. The preservation of von-Neumann algebra under modular flow works in a\nsubtle way for the non-local terms. We derive a differential-integral equation\nfor the finite modular flow, which can be analyzed in perturbation theory of\nsmall distance deviating from the entanglement boundary, and re-summation can\nbe performed in appropriate limits. Comparison with the general expectation of\nmodular flows in such limits are discussed.",
        "Despite explicit alignment efforts for large language models (LLMs), they can\nstill be exploited to trigger unintended behaviors, a phenomenon known as\n\"jailbreaking.\" Current jailbreak attack methods mainly focus on discrete\nprompt manipulations targeting closed-source LLMs, relying on manually crafted\nprompt templates and persuasion rules. However, as the capabilities of\nopen-source LLMs improve, ensuring their safety becomes increasingly crucial.\nIn such an environment, the accessibility of model parameters and gradient\ninformation by potential attackers exacerbates the severity of jailbreak\nthreats. To address this research gap, we propose a novel\n\\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak\n\\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization\nproblem within the embedding space of masked language models. Through\ncombinatorial optimization, we effectively balance the jailbreak attack success\nrate with semantic coherence. Extensive evaluations show that our method not\nonly maintains semantic consistency but also surpasses state-of-the-art\nbaselines in attack effectiveness. Additionally, by integrating semantically\ncoherent jailbreak prompts generated by our method into widely used black-box\nmethodologies, we observe a notable enhancement in their success rates when\ntargeting closed-source commercial LLMs. This highlights the security threat\nposed by open-source LLMs to commercial counterparts. We will open-source our\ncode if the paper is accepted.",
        "Materials improvements are a powerful approach to reducing loss and\ndecoherence in superconducting qubits because such improvements can be readily\ntranslated to large scale processors. Recent work improved transmon coherence\nby utilizing tantalum (Ta) as a base layer and sapphire as a substrate. The\nlosses in these devices are dominated by two-level systems (TLSs) with\ncomparable contributions from both the surface and bulk dielectrics, indicating\nthat both must be tackled to achieve major improvements in the state of the\nart. Here we show that replacing the substrate with high-resistivity silicon\n(Si) dramatically decreases the bulk substrate loss, enabling 2D transmons with\ntime-averaged quality factors (Q) exceeding 1.5 x 10^7, reaching a maximum Q of\n2.5 x 10^7, corresponding to a lifetime (T_1) of up to 1.68 ms. This low loss\nallows us to observe decoherence effects related to the Josephson junction, and\nwe use improved, low-contamination junction deposition to achieve Hahn echo\ncoherence times (T_2E) exceeding T_1. We achieve these material improvements\nwithout any modifications to the qubit architecture, allowing us to readily\nincorporate standard quantum control gates. We demonstrate single qubit gates\nwith 99.994% fidelity. The Ta-on-Si platform comprises a simple material stack\nthat can potentially be fabricated at wafer scale, and therefore can be readily\ntranslated to large-scale quantum processors.",
        "The Dark Energy Spectroscopic Instrument (DESI) data release 2 (DR2) galaxy\nand quasar clustering data represents a significant expansion of data from DR1,\nproviding improved statistical precision in BAO constraints across multiple\ntracers, including bright galaxies (BGS), luminous red galaxies (LRGs),\nemission line galaxies (ELGs), and quasars (QSOs). In this paper, we validate\nthe BAO analysis of DR2. We present the results of robustness tests on the\nblinded DR2 data and, after unblinding, consistency checks on the unblinded DR2\ndata. All results are compared to those obtained from a suite of mock catalogs\nthat replicate the selection and clustering properties of the DR2 sample. We\nconfirm the consistency of DR2 BAO measurements with DR1 while achieving a\nreduction in statistical uncertainties due to the increased survey volume and\ncompleteness. We assess the impact of analysis choices, including different\ndata vectors (correlation function vs. power spectrum), modeling approaches and\nsystematics treatments, and an assumption of the Gaussian likelihood, finding\nthat our BAO constraints are stable across these variations and assumptions\nwith a few minor refinements to the baseline setup of the DR1 BAO analysis. We\nsummarize a series of pre-unblinding tests that confirmed the readiness of our\nanalysis pipeline, the final systematic errors, and the DR2 BAO analysis\nbaseline. The successful completion of these tests led to the unblinding of the\nDR2 BAO measurements, ultimately leading to the DESI DR2 cosmological analysis,\nwith their implications for the expansion history of the Universe and the\nnature of dark energy presented in the DESI key paper.",
        "Designing re-entry vehicles requires accurate predictions of hypersonic flow\naround their geometry. Rapid prediction of such flows can revolutionize vehicle\ndesign, particularly for morphing geometries. We evaluate advanced neural\noperator models such as Deep Operator Networks (DeepONet),\nparameter-conditioned U-Net, Fourier Neural Operator (FNO), and MeshGraphNet,\nwith the objective of addressing the challenge of learning geometry-dependent\nhypersonic flow fields with limited data. Specifically, we compare the\nperformance of these models for two grid types: uniform Cartesian and irregular\ngrids. To train these models, we use 36 unique elliptic geometries for\ngenerating high-fidelity simulations with a high-order entropy-stable DGSEM\nsolver, emphasizing the challenge of working with a scarce dataset. We evaluate\nand compare the four operator-based models for their efficacy in predicting\nhypersonic flow field around the elliptic body. Moreover, we develop a novel\nframework, called Fusion DeepONet, which leverages neural field concepts and\ngeneralizes effectively across varying geometries. Despite the scarcity of\ntraining data, Fusion DeepONet achieves performance comparable to\nparameter-conditioned U-Net on uniform grids while it outperforms MeshGraphNet\nand vanilla DeepONet on irregular, arbitrary grids. Fusion DeepONet requires\nsignificantly fewer trainable parameters as compared to U-Net, MeshGraphNet,\nand FNO, making it computationally efficient. We also analyze the basis\nfunctions of the Fusion DeepONet model using Singular Value Decomposition. This\nanalysis reveals that Fusion DeepONet generalizes effectively to unseen\nsolutions and adapts to varying geometries and grid points, demonstrating its\nrobustness in scenarios with limited training data.",
        "Graph Neural Networks (GNNs) have achieved remarkable success across diverse\ntasks on graph-structured data, primarily through the use of learned weights in\nmessage passing layers. In this paper, we demonstrate that random weights can\nbe surprisingly effective, achieving performance comparable to end-to-end\ntraining counterparts, across various tasks and datasets. Specifically, we show\nthat by replacing learnable weights with random weights, GNNs can retain strong\npredictive power, while significantly reducing training time by up to 6$\\times$\nand memory usage by up to 3$\\times$. Moreover, the random weights combined with\nour construction yield random graph propagation operators, which we show to\nreduce the problem of feature rank collapse in GNNs. These understandings and\nempirical results highlight random weights as a lightweight and efficient\nalternative, offering a compelling perspective on the design and training of\nGNN architectures.",
        "We study a 3+1 active-sterile neutrino mixings model using an $A_4$ triplet\nright-handed neutrino $\\nu_R$ and a singlet eV-scale sterile neutrino under\n$A_4\\times Z_3 \\times Z_2$ discrete symmetry. Four scalar flavons are\nconsidered to reproduce neutrino oscillation parameters within the experimental\n3$\\sigma$ range. The model also studies the effective mass parameter in\nneutrinoless double beta decay experiments. Deviation from $\\mu-\\tau$ symmetry\nin the active neutrino mass matrix is generated through an antisymmetric\ninteraction of $\\nu_R$. This model successfully explains active-sterile\nneutrino mixings consistent with the cosmological upper bound on the sum of\nactive neutrino mass $\\sum m_i < 0.113$ eV (0.145 eV) in NH(IH).",
        "Three directions for the AI avant-garde are sketched against the background\nof time. Posthumanism changes what we are, and belongs to the radical future.\nTranshumanism changes how we are, and corresponds with the radical past.\nGenhumanism changes who we are, and exists in the radical present. While\ndeveloping the concepts, this essay intersects in two ways with theoretical\ndebates about humanism in the face of technological advance. First, it\ndescribes how temporal divisions may cleanly differentiate post- and\ntranshumanism. Second, the essay introduces generative humanism, which\ncontributes to discussions about AI and society by delineating a novel\nhumanistic response to contemporary technology. Finally, grounds are provided\nfor a practical project, one where philosophers work with AI engineers in the\narea of genhumanism. Contemporary AI research into serendipity in\nrecommendation engines provides natural support for the shared research.",
        "The Maximum Mean Discrepancy (MMD) is a kernel-based metric widely used for\nnonparametric tests and estimation. Recently, it has also been studied as an\nobjective function for parametric estimation, as it has been shown to yield\nrobust estimators. We have implemented MMD minimization for parameter inference\nin a wide range of statistical models, including various regression models,\nwithin an R package called regMMD. This paper provides an introduction to the\nregMMD package. We describe the available kernels and optimization procedures,\nas well as the default settings. Detailed applications to simulated and real\ndata are provided."
      ]
    }
  },
  {
    "id":2412.00036,
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"Quant GANs: deep generation of financial time series",
    "start_abstract":"Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
      ],
      "abstract":[
        "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
      ],
      "categories":[
        "q-fin.GN"
      ]
    },
    "list":{
      "title":[
        "Innovative Financing Solutions: A Transformative Driver for Financial\n  Performance of Businesses in Morocco",
        "Utilizing Pre-trained and Large Language Models for 10-K Items\n  Segmentation",
        "Mass Shootings, Community Mobility, and the Relocation of Economic\n  Activity",
        "People Reduce Workers' Compensation for Using Artificial Intelligence\n  (AI)",
        "Trust of Strangers: a framework for analysis",
        "Preventing Household Bankruptcy: The One-Third Rule in Financial\n  Planning with Mathematical Validation and Game-Theoretic Insights",
        "The Impact of Digitalisation and Sustainability on Inclusiveness:\n  Inclusive Growth Determinants",
        "The role of FDI along transitional dynamics of the host country in an\n  endogenous growth model",
        "Heterogeneity of household stock portfolios in a national market",
        "Estimating Sequential Search Models Based on a Partial Ranking\n  Representation",
        "Incorporating Damped Harmonic Oscillator in DSGE Models",
        "Investing in nature: Stakeholder's willingness to pay for Tunisian\n  forest services",
        "Combined climate stress testing of supply-chain networks and the\n  financial system with nation-wide firm-level emission estimates",
        "Centre-of-momentum Variables in $\\nu_\\mu$CC1p1$\\pi$",
        "Is fixed-node diffusion quantum Monte Carlo reproducible?",
        "Optical signatures of noncentrosymmetric structural distortion in\n  altermagnetic MnTe",
        "Fast-response low power atomic oven for integration into an ion\n  microchip",
        "Limits on WIMP dark matter with NaI(Tl) crystals in three years of\n  COSINE-100 data",
        "Building a Software Stack for Quantum-HPC Integration",
        "Eightfold Degenerate Dirac Nodal Line in Collinear Antiferromagnet\n  Mn$_5$Si$_3$",
        "A Full AGN Feedback Prescription for Numerical Models: Negative,\n  Positive and Hot Gas-Ejection Mode",
        "All Order Classical Electromagnetic Soft Theorems",
        "Combined P-value Functions for Compatible Effect Estimation and\n  Hypothesis Testing in Drug Regulation",
        "Channel Estimation for Pinching-Antenna Systems (PASS)",
        "Rainbow Tur\\'an numbers for short brooms",
        "Influence of departures from LTE on determinations of the sulfur\n  abundances in A-K type stars",
        "Heat transport model for the transition between scaling regimes in\n  quasistatic and full magnetoconvection",
        "Structure and Skewness of the Effective Inspiral Spin Distribution of\n  Binary Black Hole Mergers"
      ],
      "abstract":[
        "In a rapidly evolving landscape marked by continuous change and complex\nchallenges, effective cash management stands as a cornerstone for ensuring\nbusiness sustainability and driving performance. To address these pressing\ndemands, cash managersare increasingly turning to innovative financing\nsolutions such as venture capital, green finance, crowdfunding, advanced\nservices from Pan-African banks, and blockchain technology. These cutting-edge\ntools are pivotal in bolstering resilience against market volatility,\necological transitions, and the accelerating pace of technological change. The\npresent article aims to examine how such innovative financial approaches can\nserve as strategic drivers, enabling businesses to transform challenges into\nopportunities. The analysis underscores that rethinking cash management through\ninnovation is a critical pathway toboost the performance of Moroccan companies.\nTherefore, embracing these forward-thinking strategies unlocks new avenues for\ndevelopment empowering them to adapt with agility amidst the uncertainties of a\nshifting environment.",
        "Extracting specific items from 10-K reports remains challenging due to\nvariations in document formats and item presentation. Traditional rule-based\nitem segmentation approaches often yield suboptimal results. This study\nintroduces two advanced item segmentation methods leveraging language models:\n(1) GPT4ItemSeg, using a novel line-ID-based prompting mechanism to utilize\nGPT4 for item segmentation, and (2) BERT4ItemSeg, combining BERT embeddings\nwith a Bi-LSTM model in a hierarchical structure to overcome context window\nconstraints. Trained and evaluated on 3,737 annotated 10-K reports,\nBERT4ItemSeg achieved a macro-F1 of 0.9825, surpassing GPT4ItemSeg (0.9567),\nconditional random field (0.9818), and rule-based methods (0.9048) for core\nitems (1, 1A, 3, and 7). These approaches enhance item segmentation\nperformance, improving text analytics in accounting and finance. BERT4ItemSeg\noffers satisfactory item segmentation performance, while GPT4ItemSeg can easily\nadapt to regulatory changes. Together, they offer practical benefits for\nresearchers and practitioners, enabling reliable empirical studies and\nautomated 10-K item segmentation functionality.",
        "Using foot traffic data for over 150,000 points of interest (POIs) near the\nsites of 42 mass shootings (2018-2022, U.S.), we evaluate the spatial-temporal\nimpact of the tragic events on community mobility and relocation of economic\nactivities. Visits to nearby POIs decrease, while farther away POIs experience\nincreased foot traffic, implying that communities shift their activities away\nfrom the shooting sites. The impact is stronger when stronger trauma responses\nare expected. Our results suggest that mass shootings drive significant\ndisplacements of economic activities and can consequently lead to welfare\nlosses due to distortions in optimal choices of time and location.",
        "We investigate whether and why people might reduce compensation for workers\nwho use AI tools. Across 10 studies (N = 3,346), participants consistently\nlowered compensation for workers who used AI tools. This \"AI Penalization\"\neffect was robust across (1) different types of work and worker statuses and\nworker statuses (e.g., full-time, part-time, or freelance), (2) different forms\nof compensation (e.g., required payments or optional bonuses) and their timing,\n(3) various methods of eliciting compensation (e.g., slider scale, multiple\nchoice, and numeric entry), and (4) conditions where workers' output quality\nwas held constant, subject to varying inferences, or controlled for. Moreover,\nthe effect emerged not only in hypothetical compensation scenarios (Studies\n1-5) but also with real gig workers and real monetary compensation (Study 6).\nPeople reduced compensation for workers using AI tools because they believed\nthese workers deserved less credit than those who did not use AI (Studies 3 and\n4). This effect weakened when it is less permissible to reduce worker\ncompensation, such as when employment contracts provide stricter constraints\n(Study 4). Our findings suggest that adoption of AI tools in the workplace may\nexacerbate inequality among workers, as those protected by structured contracts\nface less vulnerability to compensation reductions, while those without such\nprotections risk greater financial penalties for using AI.",
        "Trust among people is essential to ensure collaboration, social network\nbuilding, transactions, and the development and engagement of new audiences for\nbrand promotion or social causes. In Berry (2024), the trust attitudes of\nrespondents toward strangers on the street, other groups of people, and\ninformation sources were measured. This study evaluates the trust of strangers\nusing a 5-factor structural equation model. The analysis yielded a robust model\nwith four of five factors and all variables being statistically significant,\nwith social trust and institutional trust yielding the greatest positive effect\non trust of strangers on the street. While demographic characteristics had a\nsmall positive effect, the trust of friends and family had a mild negative\neffect on the trust of strangers on the street. Trust of information sources\nwas not statistically significant and had a negligible positive effect on the\ntrust of strangers. The results also indicate that almost 48% of respondents\ndistrust strangers on the street, implying that trust is not automatically\nendowed. Directions for future research and implications for business and\nsocial causes are discussed.",
        "This paper analyzes the 1\/3 Financial Rule, a method of allocating income\nequally among debt repayment, savings, and living expenses. Through\nmathematical modeling, game theory, behavioral finance, and technological\nanalysis, we examine the rule's potential for supporting household financial\nstability and reducing bankruptcy risk. The research develops theoretical\nfoundations using utility maximization theory, demonstrating how equal\nallocation emerges as a solution under standard economic assumptions. The\ngame-theoretic analysis explores the rule's effectiveness across different\nhousehold structures, revealing potential strategic advantages in financial\ndecision-making. We investigate psychological factors influencing financial\nchoices, including cognitive biases and neurobiological mechanisms that impact\neconomic behavior. Technological approaches, such as AI-driven personalization,\nblockchain tracking, and smart contract applications, are examined for their\npotential to support financial planning. Empirical validation using U.S. Census\ndata and longitudinal studies assesses the rule's performance across various\nhousehold types. Stress testing under different economic conditions provides\ninsights into its adaptability and resilience. The research integrates\nmathematical analysis with behavioral insights and technological perspectives\nto develop a comprehensive approach to household financial management.",
        "Inclusiveness and economic development have been slowed by the pandemics and\nmilitary conflicts. This study investigates the main determinants of\ninclusiveness at the European level. A multi-method approach is used, with\nPrincipal Component Analysis (PCA) applied to create the Inclusiveness Index\nand Generalised Method of Moments (GMM) analysis used to investigate the\ndeterminants of inclusiveness. The data comprises a range of 22 years, from\n2000 to 2021, for 32 European countries. The determinants of inclusiveness and\ntheir effects were identified. First, economic growth, industrial upgrading,\nelectricity consumption, digitalisation, and the quantitative aspect of\ngovernance, all have a positive impact on inclusive growth in Europe. Second,\nthe level of CO2 emissions and inflation have a negative impact on\ninclusiveness. Tomorrow's inclusive and sustainable growth must include\ninvestments in renewable energy, digital infrastructure, inequality policies,\nsustainable governance, human capital, and inflation management. These findings\ncan help decision makers design inclusive growth policies.",
        "We investigate the role of foreign direct investment (FDI) in the\ntransitional dynamics of host countries by using an optimal growth model. FDI\nmay be beneficial for the host country because local people can work for\nmultinational firms to get a favorable salary. However, if the host country\nonly focuses on FDI, it may face a middle-income trap. We show that if the host\ncountry invests in research and development, its economy may have sustained\ngrowth. Moreover, in this case, FDI helps the host country only at the first\nstages of its development process.",
        "We study the long term dynamics of the stock portfolios owned by single\nFinnish legal entities in the Helsinki venue of the Nasdaq Nordic between 2001\nand 2021. Using the Herfindahl-Hirschman index as a measure of concentration\nfor the composition of stock portfolios, we investigate the concentration of\nFinnish household portfolios both at the level of each individual household and\ntracking the time evolution of an aggregated Finnish household portfolio. We\nalso consider aggregated portfolios of two other macro categories of investors\none comprising Finnish institutional investors and the other comprising foreign\ninvestors. Different macro categories of investors present a different degree\nof concentration of aggregated stock portfolios with highest concentration\nobserved for foreign investors. For individual Finnish retail investors,\nportfolio concentration estimated by the Herfindahl-Hirschman index presents\nhigh values for more than half of the total number of retail investors. In\nspite of the observation that retail stock portfolios are often composed by\njust a few stocks, the concentration of the aggregated stock portfolio for\nFinnish retail investors has a portfolio concentration comparable with the one\nof Finnish institutional investors. Within retail investors, stock portfolios\nof women present a similar pattern of portfolios of men but with a systematic\nhigher level of concentration observed for women both at individual and at\naggregated level.",
        "Consumers are increasingly shopping online, and more and more datasets\ndocumenting consumer search are becoming available. While sequential search\nmodels provide a framework for utilizing such data, they present empirical\nchallenges. A key difficulty arises from the inequality conditions implied by\nthese models, which depend on multiple unobservables revealed during the search\nprocess and necessitate solving or simulating high-dimensional integrals for\nlikelihood-based estimation methods. This paper introduces a novel\nrepresentation of inequalities implied by a broad class of sequential search\nmodels, demonstrating that the empirical content of such models can be\neffectively captured through a specific partial ranking of available actions.\nThis representation reduces the complexity caused by unobservables and provides\na tractable expression for joint probabilities. Leveraging this insight, we\npropose a GHK-style simulation-based likelihood estimator that is simpler to\nimplement than existing ones. It offers greater flexibility for handling\nincomplete search data, incorporating additional ranking information, and\naccommodating complex search processes, including those involving product\ndiscovery. We show that the estimator achieves robust performance while\nmaintaining relatively low computational costs, making it a practical and\nversatile tool for researchers and practitioners.",
        "This paper integrates the damped harmonic oscillator into DSGE models to\nbetter capture delayed economic adjustments. By introducing a damping\ncoefficient, I model economic recoveries as under-damped, critically damped, or\nover-damped processes. Numerical simulations illustrate how different damping\nlevels affect recovery speed and stability. This approach enhances DSGE models'\nrealism, offering insights into historical economic crises and improving\nmacroeconomic forecasting.",
        "This study explores the economic value of Aleppo pine forests, a unique and\nthreatened ecosystem in the border region of central Tunisia. These forests\nplay a vital role in supporting small rural communities, but face increasing\npressures and restrictions on their use. This research aims to assign a\nmonetary value to forest conservation, considering the region's specific\nsocio-economic context. Strategies for empowering local residents as key actors\nin developing sustainable cross-border initiatives are further investigated.\nEmploying the contingent valuation method, a survey of 350 local residents and\ninternational users was conducted to assess their willigness to pay fo forest\nconservation efforts. Logistic regression analysis revealed that\nsociodemographic factors, such as monthly income and preferred payment method,\nsignificantly influence both and the likehood of participation. These findingd\nhighlight the feasibility and importance of reconciling economic development\nwith ecological sustainability in this critical region.",
        "On the way towards carbon neutrality, climate stress testing provides\nestimates for the physical and transition risks that climate change poses to\nthe economy and the financial system. Missing firm-level CO2 emissions data\nseverely impedes the assessment of transition risks originating from carbon\npricing. Based on the individual emissions of all Hungarian firms (410,523), as\nestimated from their fossil fuel purchases, we conduct a stress test of both\nactual and hypothetical carbon pricing policies. Using a simple 1:1 economic\nABM and introducing the new carbon-to-profit ratio, we identify firms that\nbecome unprofitable and default, and estimate the respective loan write-offs.\nWe find that 45% of all companies are directly exposed to carbon pricing. At a\nprice of 45 EUR\/t, direct economic losses of 1.3% of total sales and bank\nequity losses of 1.2% are expected. Secondary default cascades in supply chain\nnetworks could increase these losses by 300% to 4000%, depending on firms'\nability to substitute essential inputs. To reduce transition risks, firms\nshould reduce their dependence on essential inputs from supply chains with high\nCO2 exposure. We discuss the implications of different policy implementations\non these transition risks.",
        "This study introduces a novel set of variables, namely the centre-of-momentum\nvariables, $\\theta_{\\textrm{COM}}$ and $E_{\\textrm{COM}}$, designed to isolate\nfinal-state interactions (FSI) from other aspects of neutrino-nucleus\ninteractions. Through detailed simulation studies, this work demonstrates the\nability of these variables to distinguish FSI contributions with minimal\ndependence on the nuclear initial state and, practically, on the neutrino flux,\nhighlighting their potential for advancing FSI modelling. With high-purity\nneutrino-hydrogen interaction selections, $\\theta_{\\textrm{COM}}$ offers the\nfirst opportunity for a direct cross-comparison among different neutrino\ncross-section experiments.",
        "Fixed-node diffusion quantum Monte Carlo (FN-DMC) is a widely-trusted\nmany-body method for solving the Schr\\\"{o}dinger equation, known for its\nreliable predictions of material and molecular properties. Furthermore, its\nexcellent scalability with system complexity and near-perfect utilization of\ncomputational power makes FN-DMC ideally positioned to leverage new advances in\ncomputing to address increasingly complex scientific problems. Even though the\nmethod is widely used as a computational gold standard, reproducibility across\nthe numerous FN-DMC code implementations has yet to be demonstrated. This\ndifficulty stems from the diverse array of DMC algorithms and trial wave\nfunctions, compounded by the method's inherent stochastic nature. This study\nrepresents a community-wide effort to address the titular question, affirming\nthat: Yes, FN-DMC is reproducible (when handled with care). Using the\nwater-methane dimer as the canonical test case, we compare results from eleven\ndifferent FN-DMC codes and show that the approximations to treat the\nnon-locality of pseudopotentials are the primary source of the discrepancies\nbetween them. In particular, we demonstrate that, for the same choice of\ndeterminantal component in the trial wave function, reliable and reproducible\npredictions can be achieved by employing the T-move (TM), the determinant\nlocality approximation (DLA), or the determinant T-move (DTM) schemes, while\nthe older locality approximation (LA) leads to considerable variability in\nresults. This work lays the foundation to establish accurate and reproducible\nFN-DMC estimates for all future studies across applications in materials\nscience, physics, chemistry, and biology.",
        "The hexagonal MnTe is a prime material candidate for altermagnets, an\nemerging class of magnetic compounds characterized by the nontrivial interplay\nof antiparallel spin arrangements with their underlying crystal structures.\nRecognizing precise knowledge of crystal symmetry as the cornerstone of the\nspin-group classification scheme, we report here a native\ninversion-symmetry-breaking structural distortion in this compound that has\npreviously been overlooked. Through optical polarimetry experiments and\nfirst-principle calculations, we show that MnTe belongs to the\nnoncentrosymmetric $D_{3h}$ group, effectively resolving key inconsistencies in\nthe earlier interpretations of Raman spectroscopy data. Our finding impacts the\nsymmetry analysis of MnTe within the altermagnetic class and sheds light on the\nmechanism of its magneto-controllable N\\'eel order.",
        "We present a novel microfabricated neutral atom source for quantum\ntechnologies that can be easily integrated onto microchip devices using\nwell-established MEMS fabrication techniques, and contrast this to conventional\noff-chip ion loading mechanisms. The heating filament of the device is shown to\nbe as small as 90$\\times$90 $\\mu$m$^2$. Testing of the $^{171}$Yb fluorescence\nresponse is found to be in the low tens of milliseconds, two orders of\nmagnitude faster compared to previous literature at a power of milliwatts\nmaking it desirable for low-power device packages. We demonstrate how the\nevaporation material can be capped in vacuum to work with materials such as Ba\nthat oxidise easily in air, which can avoid the need for ablation lasers in the\nloading process. We calculate oven lifetimes to be over 10 years of continuous\nuse for commonly used ion species in quantum technology.",
        "We report limits on WIMP dark matter derived from three years of data\ncollected by the COSINE-100 experiment with NaI(Tl) crystals, achieving an\nimproved energy threshold of 0.7 keV. This lowered threshold enhances\nsensitivity in the sub-GeV mass range, extending the reach for direct detection\nof low-mass dark matter. Although no excess of WIMP-like events was observed,\nthe increased sensitivity enabled a model-independent comparison between the\nexpected WIMP signal rate-based on mass limits from our data-and DAMA's\nreported modulation amplitude. Our findings strongly disfavor the DAMA signal\nas originating from WIMP interactions, fully excluding DAMA\/LIBRA 3$\\sigma$\nallowed regions and providing enhanced WIMP mass limits by an order of\nmagnitude in the spin-independent model compared to previous results. In the\nspin-dependent model, cross-section upper limits were obtained in the mass\nrange [0.1-5.0] GeV\/c$^2$, with additional sensitivity to sub-GeV WIMPs through\nthe inclusion of the Migdal effect. These results represent substantial\nprogress in low-mass dark matter exploration and reinforce constraints on the\nlongstanding DAMA claim.",
        "This paper presents a comprehensive software stack architecture for\nintegrating quantum computing (QC) capabilities with High-Performance Computing\n(HPC) environments. While quantum computers show promise as specialized\naccelerators for scientific computing, their effective integration with\nclassical HPC systems presents significant technical challenges. We propose a\nhardware-agnostic software framework that supports both current noisy\nintermediate-scale quantum devices and future fault-tolerant quantum computers,\nwhile maintaining compatibility with existing HPC workflows. The architecture\nincludes a quantum gateway interface, standardized APIs for resource\nmanagement, and robust scheduling mechanisms to handle both simultaneous and\ninterleaved quantum-classical workloads. Key innovations include: (1) a unified\nresource management system that efficiently coordinates quantum and classical\nresources, (2) a flexible quantum programming interface that abstracts\nhardware-specific details, (3) A Quantum Platform Manager API that simplifies\nthe integration of various quantum hardware systems, and (4) a comprehensive\ntool chain for quantum circuit optimization and execution. We demonstrate our\narchitecture through implementation of quantum-classical algorithms, including\nthe variational quantum linear solver, showcasing the framework's ability to\nhandle complex hybrid workflows while maximizing resource utilization. This\nwork provides a foundational blueprint for integrating QC capabilities into\nexisting HPC infrastructures, addressing critical challenges in resource\nmanagement, job scheduling, and efficient data movement between classical and\nquantum resources.",
        "We study the electronic, magnetic, and spin transport properties of the\northorhombic Mn$_{5}$Si$_{3}$ compound in the $AF2$ phase using symmetry\nanalysis and ab-initio calculations. Our ground state energy calculations align\nwith experimental observations, demonstrating that the collinear\nantiferromagnetic (AFM) order, with N\\'{e}el vector in the [010] direction, is\nthe most stable magnetic configuration both with and without spin-orbit\ncoupling (SOC) in a bulk lattice geometry. We identified an unconventional\neight-fold degenerate Dirac nodal line (DNL) close to the Fermi level,\ncharacterized by negligible SOC. This DNL is robustly protected by a unique\ncombination of a pure-spin symmetry and a lattice symmetry together with\nmagnetic space group symmetries. Upon introducing SOC, this degeneracy is\nreduced to two four-fold DNLs, being protected by the combination of\ntime-reversal, partial translation and nonsymmorphic symmetries within the\nmagnetic space group. We predict also a large intrinsic spin Hall conductivity\n(SHC) which correlates with the presence of SOC-induced splitting of these\neight-fold degenerate DNLs near the Fermi level. These intriguing\ncharacteristics position collinear antiferromagnet Mn$_{5}$Si$_{3}$ as a\ncompelling candidate for spintronic applications, particularly in the\ngeneration and detection of spin currents, while remaining compatible with\nmodern silicon technology.",
        "We build upon the state-of-the-art semi-analytic model \\texttt{FEGA24}\n(Formation and Evolution of GAlaxies, \\citealt{contini2024d}), which integrates\nthe latest prescriptions relevant to galaxy formation and evolution, alongside\na comprehensive AGN feedback model. This model incorporates three modes of\nfeedback: negative (preventing excessive cooling), positive (enhancing star\nformation), and hot gas ejection (expelling gas beyond the virial radius of\nhalos). These modes operate in a coordinated manner: the negative mode\nregulates the cooling process, the positive mode promotes bursts of star\nformation, and the hot gas ejection mode expels gas beyond the virial radius\nwhen the AGN is sufficiently powerful. Our updated semi-analytic model,\n\\texttt{FEGA25}, retains the qualitative and quantitative consistency of the\nanalyses presented in \\cite{contini2024d}, while delivering more robust\nresults. Notably, \\texttt{FEGA25} provides a more detailed characterization of\nthe fraction of red galaxies as a function of stellar mass, predicts a main\nsequence of star-forming galaxies more consistent with observations, and\nestimates the fraction of hot gas in halos closer to observed values. These\nfindings underscore the importance of a physical mechanism capable of ejecting\nhot gas beyond the virialized region of dark matter halos without significantly\naltering the stellar and cold gas components. Such a mechanism is crucial to\nensure the proper functioning of other processes, such as cooling and star\nformation. Since supernova feedback is already modeled at its maximum\nefficiency, AGN feedback emerges as the natural candidate for this role.",
        "If a set of charged objects collide in space and the fragments disperse, then\nthis process will emit electromagnetic waves. Classical soft photon theorem\ndetermines the constant term and the leading power law fall-off of the\nwave-form at late and early times in terms of only the momenta and charges of\nthe incoming and outgoing objects. In this paper we determine an infinite set\nof subleading terms in the late and early time expansion of the wave-form,\nwhich also depend only on the momenta and charges of the incoming and outgoing\nparticles. For two-particle scattering, we derive a resummed low-frequency\nelectromagnetic wave-form, as well as the resummed wave-form at early and late\ntimes. In this analysis we ignore the effect of long range gravitational\ninteraction, but our result is unaffected by any other short range interactions\namong the objects.",
        "The two-trials rule in drug regulation requires statistically significant\nresults from two pivotal trials to demonstrate efficacy. However, it is unclear\nhow the effect estimates from both trials should be combined to quantify the\ndrug effect. Fixed-effect meta-analysis is commonly used but may yield\nconfidence intervals that exclude the value of no effect even when the\ntwo-trials rule is not fulfilled. We systematically address this by recasting\nthe two-trials rule and meta-analysis in a unified framework of combined\np-value functions, where they are variants of Wilkinson's and Stouffer's\ncombination methods, respectively. This allows us to obtain compatible combined\np-values, effect estimates, and confidence intervals, which we derive in\nclosed-form. Additionally, we provide new results for Edgington's, Fisher's,\nPearson's, and Tippett's p-value combination methods. When both trials have the\nsame true effect, all methods can consistently estimate it, although some show\nbias. When true effects differ, the two-trials rule and Pearson's method are\nconservative (converging to the less extreme effect), Fisher's and Tippett's\nmethods are anti-conservative (converging to the more extreme effect), and\nEdgington's method and meta-analysis are balanced (converging to a weighted\naverage). Notably, Edgington's confidence intervals asymptotically always\ninclude individual trial effects, while meta-analytic confidence intervals\nshrink to a point at the weighted average effect. We conclude that all of these\nmethods may be appropriate depending on the estimand of interest. We implement\ncombined p-value function inference for two trials in the R package twotrials,\nallowing researchers to easily perform compatible hypothesis testing and\nparameter estimation.",
        "Pinching Antennas (PAs) represent a revolutionary flexible antenna technology\nthat leverages dielectric waveguides and electromagnetic coupling to mitigate\nlarge-scale path loss. This letter is the first to explore channel estimation\nfor Pinching-Antenna SyStems (PASS), addressing their uniquely ill-conditioned\nand underdetermined channel characteristics. In particular, two efficient deep\nlearning-based channel estimators are proposed. 1) PAMoE: This estimator\nincorporates dynamic padding, feature embedding, fusion, and mixture of experts\n(MoE) modules, which effectively leverage the positional information of PAs and\nexploit expert diversity. 2) PAformer: This Transformer-style estimator employs\nthe self-attention mechanism to predict channel coefficients in a per-antenna\nmanner, which offers more flexibility to adaptively deal with dynamic numbers\nof PAs in practical deployment. Numerical results demonstrate that 1) the\nproposed deep learning-based channel estimators outperform conventional methods\nand exhibit excellent zero-shot learning capabilities, and 2) PAMoE delivers\nhigher channel estimation accuracy via MoE specialization, while PAformer\nnatively handles an arbitrary number of PAs, trading self-attention complexity\nfor superior scalability.",
        "A graph $G$ is rainbow-$F$-free if it admits a proper edge-coloring without a\nrainbow copy of $F$. The rainbow Tur\\'an number of $F$, denoted\n$\\mathrm{ex^*}(n,F)$, is the maximum number of edges in a rainbow-$F$-free\ngraph on $n$ vertices. We determine bounds on the rainbow Tur\\'an numbers of\nstars with a single edge subdivided twice; we call such a tree with $t$ total\nedges a $t$-edge \\textit{broom} with length-$3$ handle, denoted by $B_{t,3}$.\nWe improve the best known upper bounds on $\\mathrm{ex^*}(n,B_{t,3})$ in all\ncases where $t \\neq 2^s - 2$. Moreover, in the case where $t$ is odd and in a\nfew cases when $t \\equiv 0 \\mod 4$, we provide constructions asymptotically\nachieving these upper bounds. Our results also demonstrate a dependence of\n$\\mathrm{ex^*}(n,B_{t,3})$ on divisibility properties of $t$.",
        "The influence of departures from local thermodynamic equilibrium (LTE) on\nneutral sulfur lines is considered. A grid of corrections is proposed to take\ninto account the influence of departures from LTE for neutral sulfur lines in\nthe visible and infrared spectral regions, including the H-band. The grid is\ncalculated using the atomic model of sulfur incorporating the most up-to-date\ncollision rates with electrons and hydrogen. The inclusion of levels and\ntransitions of ionized sulfur in the atomic model made it possible to expand\nthe range of effective temperatures of stellar photospheres in the grid up to\n10000 K. The atomic model was tested in determining the sulfur abundance of 13\nstars and showed its adequacy in a wide range of fundamental stellar\nparameters. In the spectra of all test stars, the sulfur lines are fitted with\nsimilar abundances of the element, regardless of the degree of influence of the\neffects of deviation from LTE on a particular spectral line. For lines of\nseveral multiplets, the wavelengths and oscillator strengths were refined. A\nlist of S I lines recommended for determining sulfur abundance has been\ncreated.",
        "In magnetoconvection, the flow is governed by the interplay between\ngravitational buoyancy and the Lorentz force, with one of these forces\ndominating in different regimes. In this paper, we develop a model with a\nsingle adjustable parameter that accurately captures the smooth transition from\na buoyancy-dominated regime to one dominated by the Lorentz force. A\nperturbative extension of the model accounts for distinct transition features\nthat occur at high Prandtl numbers. We validate the model for magnetoconvection\nin both the quasistatic regime and at finite magnetic Reynolds numbers using\ndata from direct numerical simulations and existing experimental data sets. The\nmodel contains a natural extension to rotating convection and offers a\npotential generalisation to rotating magnetoconvection.",
        "The detection of gravitational waves has brought to light a population of\nbinary black holes that merge within a Hubble time. Multiple formation channels\ncan contribute to this population, making it difficult to definitively\nassociate particular population features with underlying stellar physics. Black\nhole spins are considered an important discriminator between various channels,\nbut they are less well-measured than masses, making conclusive astrophysical\nstatements using spins difficult thus far. In this paper, we consider the\ndistribution of the effective inspiral spin $\\chi_{\\rm eff}$ -- a quantity much\nbetter measured than individual component spins. We show that non-Gaussian\nfeatures like skewness, asymmetry about zero, and multimodality can naturally\narise in the $\\chi_{\\rm eff}$ distribution when multiple channels contribute to\nthe population. Searching for such features, we find signs of skewness and\nasymmetry already in the current catalogs, but no statistically significant\nsigns of bimodality. These features provide robust evidence for the presence of\na subpopulation with spins preferentially aligned to the binary's orbital\nangular momentum; and we conservatively estimate the fraction of this\nsubpopulation to be at least $12 \\% - 17\\%$ (at $90\\%$ credibility). Our models\ndo not find an excess of non-spinning systems and instead find that at least\n$\\sim 20 \\%$ of the binaries have some degree of negative $\\chi_{\\rm eff}$. The\ndata also suggest that, if preferentially aligned mergers form a significant\nfraction of the population, they must have small spins."
      ]
    }
  },
  {
    "id":2411.0064,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Quantifying Variance in Evaluation Benchmarks",
    "start_abstract":"Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",
    "start_categories":[
      "stat.ME"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "The Llama 3 Herd of Models"
      ],
      "abstract":[
        "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Generative Artificial Intelligence: Implications for Biomedical and\n  Health Professions Education",
        "STRIVE: Structured Reasoning for Self-Improvement in Claim Verification",
        "VidCapBench: A Comprehensive Benchmark of Video Captioning for\n  Controllable Text-to-Video Generation",
        "RM-PoT: Reformulating Mathematical Problems and Solving via Program of\n  Thoughts",
        "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite\n  Solar Cell Research",
        "REALM-Bench: A Real-World Planning Benchmark for LLMs and Multi-Agent\n  Systems",
        "Generating Causally Compliant Counterfactual Explanations using ASP",
        "URECA: The Chain of Two Minimum Set Cover Problems exists behind\n  Adaptation to Shifts in Semantic Code Search",
        "Reflection of Episodes: Learning to Play Game from Expert and Self\n  Experiences",
        "Where to Go Next Day: Multi-scale Spatial-Temporal Decoupled Model for\n  Mid-term Human Mobility Prediction",
        "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning",
        "Zweistein: A Dynamic Programming Evaluation Function for Einstein\n  W\\\"urfelt Nicht!",
        "Towards AI-assisted Academic Writing",
        "A Supersymmetric $w_{1+\\infty}$ Symmetry, the Extended Supergravity and\n  the Celestial Holography",
        "Generalized Optimal AMG Convergence Theory for Stokes Equations Using\n  Smooth Aggregation and Vanka Relaxation Strategies",
        "CIBPU: A Conflict-Invisible Secure Branch Prediction Unit",
        "Production, Characteristics and Biological effects of Protonated Small\n  Water Clusters",
        "NitiBench: A Comprehensive Study of LLM Framework Capabilities for Thai\n  Legal Question Answering",
        "Detecting high-dimensional time-bin entanglement in fiber-loop systems",
        "Statistical Collusion by Collectives on Learning Platforms",
        "Accelerated Preference Elicitation with LLM-Based Proxies",
        "A car-following model with behavioural adaptation to road geometry",
        "Increasing the Energy-Efficiency of Wearables Using Low-Precision Posit\n  Arithmetic with PHEE",
        "Trajectory Prediction for Autonomous Driving: Progress, Limitations, and\n  Future Directions",
        "Two simple photon gauges in inflation",
        "Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion\n  Prior and Differentiable Physics",
        "Split Adaptation for Pre-trained Vision Transformers",
        "A Detailed Analysis of Close Binary OCs"
      ],
      "abstract":[
        "Generative AI has had a profound impact on biomedicine and health, both in\nprofessional work and in education. Based on large language models (LLMs),\ngenerative AI has been found to perform as well as humans in simulated\nsituations taking medical board exams, answering clinical questions, solving\nclinical cases, applying clinical reasoning, and summarizing information.\nGenerative AI is also being used widely in education, performing well in\nacademic courses and their assessments. This review summarizes the successes of\nLLMs and highlights some of their challenges in the context of education, most\nnotably aspects that may undermines the acquisition of knowledge and skills for\nprofessional work. It then provides recommendations for best practices\novercoming shortcomings for LLM use in education. Although there are challenges\nfor use of generative AI in education, all students and faculty, in biomedicine\nand health and beyond, must have understanding and be competent in its use.",
        "Claim verification is the task of determining whether a claim is supported or\nrefuted by evidence. Self-improvement methods, where reasoning chains are\ngenerated and those leading to correct results are selected for training, have\nsucceeded in tasks like mathematical problem solving. However, in claim\nverification, this approach struggles. Low-quality reasoning chains may falsely\nmatch binary truth labels, introducing faulty reasoning into the\nself-improvement process and ultimately degrading performance. To address this,\nwe propose STRIVE: Structured Reasoning for Self-Improved Verification. Our\nmethod introduces a structured reasoning design with Claim Decomposition,\nEntity Analysis, and Evidence Grounding Verification. These components improve\nreasoning quality, reduce errors, and provide additional supervision signals\nfor self-improvement. STRIVE begins with a warm-up phase, where the base model\nis fine-tuned on a small number of annotated examples to learn the structured\nreasoning design. It is then applied to generate reasoning chains for all\ntraining examples, selecting only those that are correct and structurally sound\nfor subsequent self-improvement training. We demonstrate that STRIVE achieves\nsignificant improvements over baseline models, with a 31.4% performance gain\nover the base model and 20.7% over Chain of Thought on the HOVER datasets,\nhighlighting its effectiveness.",
        "The training of controllable text-to-video (T2V) models relies heavily on the\nalignment between videos and captions, yet little existing research connects\nvideo caption evaluation with T2V generation assessment. This paper introduces\nVidCapBench, a video caption evaluation scheme specifically designed for T2V\ngeneration, agnostic to any particular caption format. VidCapBench employs a\ndata annotation pipeline, combining expert model labeling and human refinement,\nto associate each collected video with key information spanning video\naesthetics, content, motion, and physical laws. VidCapBench then partitions\nthese key information attributes into automatically assessable and manually\nassessable subsets, catering to both the rapid evaluation needs of agile\ndevelopment and the accuracy requirements of thorough validation. By evaluating\nnumerous state-of-the-art captioning models, we demonstrate the superior\nstability and comprehensiveness of VidCapBench compared to existing video\ncaptioning evaluation approaches. Verification with off-the-shelf T2V models\nreveals a significant positive correlation between scores on VidCapBench and\nthe T2V quality evaluation metrics, indicating that VidCapBench can provide\nvaluable guidance for training T2V models. The project is available at\nhttps:\/\/github.com\/VidCapBench\/VidCapBench.",
        "Recently, substantial advancements have been made in training language models\nto carry out step-by-step reasoning for solving intricate numerical reasoning\ntasks. Beyond the methods used to solve these problems, the structure and\nformulation of the problems themselves also play a crucial role in determining\nthe performance of large language models. We observe that even small changes in\nthe surface form of mathematical problems can have a profound impact on both\nthe answer distribution and solve rate. This highlights the vulnerability of\nLLMs to surface-level variations, revealing its limited robustness when\nreasoning through complex problems. In this paper, we propose RM-PoT, a\nthree-stage framework that integrates problem reformulation (RM), code-aided\nreasoning (PoT), and domain-aware few-shot learning to address these\nlimitations. Our approach first reformulates the input problem into diverse\nsurface forms to reduce structural bias, then retrieves five semantically\naligned examples from a pre-constructed domain-specific question bank to\nprovide contextual guidance, and finally generates executable Python code for\nprecise computation.",
        "The rapid advancement of perovskite solar cells (PSCs) has led to an\nexponential growth in research publications, creating an urgent need for\nefficient knowledge management and reasoning systems in this domain. We present\na comprehensive knowledge-enhanced system for PSCs that integrates three key\ncomponents. First, we develop Perovskite-KG, a domain-specific knowledge graph\nconstructed from 1,517 research papers, containing 23,789 entities and 22,272\nrelationships. Second, we create two complementary datasets: Perovskite-Chat,\ncomprising 55,101 high-quality question-answer pairs generated through a novel\nmulti-agent framework, and Perovskite-Reasoning, containing 2,217 carefully\ncurated materials science problems. Third, we introduce two specialized large\nlanguage models: Perovskite-Chat-LLM for domain-specific knowledge assistance\nand Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental\nresults demonstrate that our system significantly outperforms existing models\nin both domain-specific knowledge retrieval and scientific reasoning tasks,\nproviding researchers with effective tools for literature review, experimental\ndesign, and complex problem-solving in PSC research.",
        "This benchmark suite provides a comprehensive evaluation framework for\nassessing both individual LLMs and multi-agent systems in real-world planning\nscenarios. The suite encompasses eleven designed problems that progress from\nbasic to highly complex, incorporating key aspects such as multi-agent\ncoordination, inter-agent dependencies, and dynamic environmental disruptions.\nEach problem can be scaled along three dimensions: the number of parallel\nplanning threads, the complexity of inter-dependencies, and the frequency of\nunexpected disruptions requiring real-time adaptation. The benchmark includes\ndetailed specifications, evaluation metrics, and baseline implementations using\ncontemporary frameworks like LangGraph, enabling rigorous testing of both\nsingle-agent and multi-agent planning capabilities. Through standardized\nevaluation criteria and scalable complexity, this benchmark aims to drive\nprogress in developing more robust and adaptable AI planning systems for\nreal-world applications.",
        "This research is focused on generating achievable counterfactual\nexplanations. Given a negative outcome computed by a machine learning model or\na decision system, the novel CoGS approach generates (i) a counterfactual\nsolution that represents a positive outcome and (ii) a path that will take us\nfrom the negative outcome to the positive one, where each node in the path\nrepresents a change in an attribute (feature) value. CoGS computes paths that\nrespect the causal constraints among features. Thus, the counterfactuals\ncomputed by CoGS are realistic. CoGS utilizes rule-based machine learning\nalgorithms to model causal dependencies between features. The paper discusses\nthe current status of the research and the preliminary results obtained.",
        "Adaptation is to make model learn the patterns shifted from the training\ndistribution. In general, this adaptation is formulated as the minimum entropy\nproblem. However, the minimum entropy problem has inherent limitation --\nshifted initialization cascade phenomenon. We extend the relationship between\nthe minimum entropy problem and the minimum set cover problem via Lebesgue\nintegral. This extension reveals that internal mechanism of the minimum entropy\nproblem ignores the relationship between disentangled representations, which\nleads to shifted initialization cascade. From the analysis, we introduce a new\nclustering algorithm, Union-find based Recursive Clustering Algorithm~(URECA).\nURECA is an efficient clustering algorithm for the leverage of the\nrelationships between disentangled representations. The update rule of URECA\ndepends on Thresholdly-Updatable Stationary Assumption to dynamics as a\nreleased version of Stationary Assumption. This assumption helps URECA to\ntransport disentangled representations with no errors based on the\nrelationships between disentangled representations. URECA also utilize\nsimulation trick to efficiently cluster disentangled representations. The wide\nrange of evaluations show that URECA achieves consistent performance gains for\nthe few-shot adaptation to diverse types of shifts along with advancement to\nState-of-The-Art performance in CoSQA in the scenario of query shift.",
        "StarCraft II is a complex and dynamic real-time strategy (RTS) game\nenvironment, which is very suitable for artificial intelligence and\nreinforcement learning research. To address the problem of Large Language\nModel(LLM) learning in complex environments through self-reflection, we propose\na Reflection of Episodes(ROE) framework based on expert experience and\nself-experience. This framework first obtains key information in the game\nthrough a keyframe selection method, then makes decisions based on expert\nexperience and self-experience. After a game is completed, it reflects on the\nprevious experience to obtain new self-experience. Finally, in the experiment,\nour method beat the robot under the Very Hard difficulty in TextStarCraft II.\nWe analyze the data of the LLM in the process of the game in detail, verified\nits effectiveness.",
        "Predicting individual mobility patterns is crucial across various\napplications. While current methods mainly focus on predicting the next\nlocation for personalized services like recommendations, they often fall short\nin supporting broader applications such as traffic management and epidemic\ncontrol, which require longer period forecasts of human mobility. This study\naddresses mid-term mobility prediction, aiming to capture daily travel patterns\nand forecast trajectories for the upcoming day or week. We propose a novel\nMulti-scale Spatial-Temporal Decoupled Predictor (MSTDP) designed to\nefficiently extract spatial and temporal information by decoupling daily\ntrajectories into distinct location-duration chains. Our approach employs a\nhierarchical encoder to model multi-scale temporal patterns, including daily\nrecurrence and weekly periodicity, and utilizes a transformer-based decoder to\nglobally attend to predicted information in the location or duration chain.\nAdditionally, we introduce a spatial heterogeneous graph learner to capture\nmulti-scale spatial relationships, enhancing semantic-rich representations.\nExtensive experiments, including statistical physics analysis, are conducted on\nlarge-scale mobile phone records in five cities (Boston, Los Angeles, SF Bay\nArea, Shanghai, and Tokyo), to demonstrate MSTDP's advantages. Applied to\nepidemic modeling in Boston, MSTDP significantly outperforms the\nbest-performing baseline, achieving a remarkable 62.8% reduction in MAE for\ncumulative new cases.",
        "Large language models (LLMs) can prove mathematical theorems formally by\ngenerating proof steps (\\textit{a.k.a.} tactics) within a proof system.\nHowever, the space of possible tactics is vast and complex, while the available\ntraining data for formal proofs is limited, posing a significant challenge to\nLLM-based tactic generation. To address this, we introduce a neuro-symbolic\ntactic generator that synergizes the mathematical intuition learned by LLMs\nwith domain-specific insights encoded by symbolic methods. The key aspect of\nthis integration is identifying which parts of mathematical reasoning are best\nsuited to LLMs and which to symbolic methods. While the high-level idea of\nneuro-symbolic integration is broadly applicable to various mathematical\nproblems, in this paper, we focus specifically on Olympiad inequalities\n(Figure~1). We analyze how humans solve these problems and distill the\ntechniques into two types of tactics: (1) scaling, handled by symbolic methods,\nand (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with\nLLMs to prune and rank the proof goals for efficient proof search. We evaluate\nour framework on 161 challenging inequalities from multiple mathematics\ncompetitions, achieving state-of-the-art performance and significantly\noutperforming existing LLM and symbolic approaches without requiring additional\ntraining data.",
        "This paper introduces Zweistein, a dynamic programming evaluation function\nfor Einstein W\\\"urfelt Nicht! (EWN). Instead of relying on human knowledge to\ncraft an evaluation function, Zweistein uses a data-centric approach that\neliminates the need for parameter tuning. The idea is to use a vector recording\nthe distance to the corner of all pieces. This distance vector captures the\nessence of EWN. It not only outperforms many traditional EWN evaluation\nfunctions but also won first place in the TCGA 2023 competition.",
        "We present components of an AI-assisted academic writing system including\ncitation recommendation and introduction writing. The system recommends\ncitations by considering the user's current document context to provide\nrelevant suggestions. It generates introductions in a structured fashion,\nsituating the contributions of the research relative to prior work. We\ndemonstrate the effectiveness of the components through quantitative\nevaluations. Finally, the paper presents qualitative research exploring how\nresearchers incorporate citations into their writing workflows. Our findings\nindicate that there is demand for precise AI-assisted writing systems and\nsimple, effective methods for meeting those needs.",
        "We determine the ${\\cal N}=4$ supersymmetric\n$W_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra which is an extension of\n${\\cal N}=4$ $SO(4)$ superconformal algebra with vanishing central charge. We\nidentify the soft current algebra between the graviton, the gravitinos, the\nvectors, the Majorana fermions, the scalar or the pseudoscalar, equivalent to\n${\\cal N}=4$ supersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra,\nin two dimensions with the ${\\cal N}=4$ supergravity theory with $SO(4)$ global\nsymmetry in four dimensions found by Das (at Stony Brook in 1977), via\ncelestial holography. Furthermore, the truncations of ${\\cal N}=4$\nsupersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra provide the\nsoft current algebras for the ${\\cal N}=2,3$ supergravity theories, the ${\\cal\nN}=2$ supergravity coupled to its Abelian vector multiplet and the ${\\cal N}=1$\nsupersymmetric Maxwell Einstein theory. For the ${\\cal N}=2$ supergravity\ntheory, the soft current algebra can be also realized by the ${\\cal N}=2$\nsupersymmetric $w_{1+\\infty}^{K,K}[\\lambda=0]$ algebra.",
        "This paper discusses our recent generalized optimal algebraic multigrid (AMG)\nconvergence theory applied to the steady-state Stokes equations discretized\nusing Taylor-Hood elements ($\\pmb{ \\mathbb{P}}_2\/\\mathbb{P}_{1}$). The\ngeneralized theory is founded on matrix-induced orthogonality of the left and\nright eigenvectors of a generalized eigenvalue problem involving the system\nmatrix and relaxation operator. This framework establishes a rigorous lower\nbound on the spectral radius of the two-grid error-propagation operator,\nenabling precise predictions of the convergence rate for symmetric indefinite\nproblems, such as those arising from saddle-point systems. We apply this theory\nto the recently developed monolithic smooth aggregation AMG (SA-AMG) solver for\nStokes, constructed using evolution-based strength of connection, standard\naggregation, and smoothed prolongation. The performance of these solvers is\nevaluated using additive and multiplicative Vanka relaxation strategies.\nAdditive Vanka relaxation constructs patches algebraically on each level,\nresulting in a nonsymmetric relaxation operator due to the partition of unity\nbeing applied on one side of the block-diagonal matrix. Although symmetry can\nbe restored by eliminating the partition of unity, this compromises\nconvergence. Alternatively, multiplicative Vanka relaxation updates velocity\nand pressure sequentially within each patch, propagating updates\nmultiplicatively across the domain and effectively addressing velocity-pressure\ncoupling, ensuring a symmetric relaxation. We demonstrate that the generalized\noptimal AMG theory consistently provides accurate lower bounds on the\nconvergence rate for SA-AMG applied to Stokes equations. These findings suggest\npotential avenues for further enhancement in AMG solver design for saddle-point\nsystems.",
        "Previous schemes for designing secure branch prediction unit (SBPU) based on\nphysical isolation can only offer limited security and significantly affect\nBPU's prediction capability, leading to prominent performance degradation.\nMoreover, encryption-based SBPU schemes based on periodic key re-randomization\nhave the risk of being compromised by advanced attack algorithms, and the\nperformance overhead is also considerable. To this end, this paper proposes a\nconflict-invisible SBPU (CIBPU). CIBPU employs redundant storage design,\nload-aware indexing, and replacement design, as well as an encryption mechanism\nwithout requiring periodic key updates, to prevent attackers' perception of\nbranch conflicts. We provide a thorough security analysis, which shows that\nCIBPU achieves strong security throughout the BPU's lifecycle. We implement\nCIBPU in a RISC-V core model in gem5. The experimental results show that CIBPU\ncauses an average performance overhead of only 1.12%-2.20% with acceptable\nhardware storage overhead, which is the lowest among the state-of-the-art SBPU\nschemes. CIBPU has also been implemented in the open-source RISC-V core,\nSonicBOOM, which is then burned onto an FPGA board. The evaluation based on the\nboard shows an average performance degradation of 2.01%, which is approximately\nconsistent with the result obtained in gem5.",
        "The production and characteristics of protonated small water clusters (PSWCs)\nwere reported in this work, where in electrospray ionization (ESI) of pure\nwater, the species obtained were singly charged molecular ions consisting of 2,\n3, 4 or 5 water molecules attached to a hydrogen ion, [(H2O)n+H]+, where n = 2,\n3, 4 or 5. We proposed a new type of PSWCs structure: 2, 3, 4, 5 water\nmolecules wrapped around a hydrogen ion which is located at the electrical and\ngeometric center, forming a very stable molecular structure. Furthermore,\nbiological tests of the PSWCs on mitochondrial function of intestinal\nepithelial cells and liver cells in mice showed the better therapeutic effect\non inflammatory bowel diseases compared to that of the biologic agent\nInfliximab.",
        "The application of large language models (LLMs) in the legal domain holds\nsignificant potential for information retrieval and question answering, yet\nThai legal QA systems face challenges due to a lack of standardized evaluation\nbenchmarks and the complexity of Thai legal structures. This paper introduces\nNitiBench, a benchmark comprising two datasets: the NitiBench-CCL, covering\ngeneral Thai financial law, and the NitiBench-Tax, which includes real-world\ntax law cases requiring advanced legal reasoning. We evaluate\nretrieval-augmented generation (RAG) and long-context LLM-based approaches to\naddress three key research questions: the impact of domain-specific components\nlike section-based chunking and cross-referencing, the comparative performance\nof different retrievers and LLMs, and the viability of long-context LLMs as an\nalternative to RAG. Our results show that section-based chunking significantly\nimproves retrieval and end-to-end performance, current retrievers struggle with\ncomplex queries, and long-context LLMs still underperform RAG-based systems in\nThai legal QA. To support fair evaluation, we propose tailored multi-label\nretrieval metrics and the use of an LLM-as-judge for coverage and contradiction\ndetection method. These findings highlight the limitations of current Thai\nlegal NLP solutions and provide a foundation for future research in the field.\nWe also open-sourced our codes and dataset to available publicly.",
        "Many quantum communication protocols rely on the distribution of entanglement\nbetween the different participating parties. One example is quantum key\ndistribution (QKD), an application that has matured to commercial use in recent\nyears. However, difficulties remain, especially with noise resilience and\nchannel capacity in long-distance communication. One way to overcome these\nproblems is to use high-dimensional entanglement, which has been shown to be\nmore robust to noise and enables higher secret-key rates. It is therefore\nimportant to have access to certifiable high-dimensional entanglement sources\nto confidently implement these advanced QKD protocols. Here, we develop a\nmethod for certifying high-dimensional time-bin entanglement in fiber-loop\nsystems. In these systems, entanglement creation and detection can utilize the\nsame physical components, and the number of time bins, and thus the\nentanglement dimension, can be adapted without making physical changes to the\nsetup. Our certification method builds on previous proposals for the\ncertification of angular-momentum entanglement in photon pairs. In particular,\nmeasurements in only two experimentally accessible bases are sufficient to\nobtain a lower bound on the entanglement dimension for both two- and\nmultiphoton quantum states. Numerical simulations show that the method is\nrobust against typical experimental noise effects and works well even with\nlimited measurement statistics, thus establishing time-bin encoded photons as a\npromising platform for high-dimensional quantum-communication protocols.",
        "As platforms increasingly rely on learning algorithms, collectives may form\nand seek ways to influence these platforms to align with their own interests.\nThis can be achieved by coordinated submission of altered data. To evaluate the\npotential impact of such behavior, it is essential to understand the\ncomputations that collectives must perform to impact platforms in this way. In\nparticular, collectives need to make a priori assessments of the effect of the\ncollective before taking action, as they may face potential risks when\nmodifying their data. Moreover they need to develop implementable coordination\nalgorithms based on quantities that can be inferred from observed data. We\ndevelop a framework that provides a theoretical and algorithmic treatment of\nthese issues and present experimental results in a product evaluation domain.",
        "Bidders in combinatorial auctions face significant challenges when describing\ntheir preferences to an auctioneer. Classical work on preference elicitation\nfocuses on query-based techniques inspired from proper learning--often via\nproxies that interface between bidders and an auction mechanism--to\nincrementally learn bidder preferences as needed to compute efficient\nallocations. Although such elicitation mechanisms enjoy theoretical query\nefficiency, the amount of communication required may still be too cognitively\ntaxing in practice.\n  We propose a family of efficient LLM-based proxy designs for eliciting\npreferences from bidders using natural language. Our proposed mechanism\ncombines LLM pipelines and DNF-proper-learning techniques to quickly\napproximate preferences when communication is limited. To validate our\napproach, we create a testing sandbox for elicitation mechanisms that\ncommunicate in natural language. In our experiments, our most promising LLM\nproxy design reaches approximately efficient outcomes with five times fewer\nqueries than classical proper learning based elicitation mechanisms.",
        "Understanding the effect of road geometry on human driving behaviour is\nessential for both road safety studies and traffic microsimulation. Research on\nthis topic is still limited, mainly focusing on free-flow traffic and not\nadequately considering the influence of curvature on car-following dynamics.\nThis work attempts to investigate this issue and model the adaptation of\ncar-following behaviour to horizontal curvature. For this purpose, the maximum\ndesired speed - which mainly determines the free-flow dynamics - is expressed\nas a parsimonious function of the curvature. A spatial anticipation mechanism\nis also included in order to realistically describe the driving behaviour when\napproaching or exiting from curves. The accuracy of the augmented model is\nevaluated using the Modified Intelligent Driver Model (M-IDM) and trajectory\ndata from free-flow and car-following traffic (Naples data and Zen Traffic\nData). The results show that a significant improvement is achieved in free-flow\ndynamics. In car-following situations, improvements are mainly observed at high\nspeed and are dependent on the observed driver. Overall, the analysis\nhighlights the lack of sufficiently spatially extended trajectory data to\ncalibrate and evaluate such driving behaviours.",
        "Wearable biomedical devices are increasingly being used for continuous\npatient health monitoring, enabling real-time insights and extended data\ncollection without the need for prolonged hospital stays. These devices must be\nenergy efficient to minimize battery size, improve comfort, and reduce\nrecharging intervals. This paper investigates the use of specialized\nlow-precision arithmetic formats to enhance the energy efficiency of biomedical\nwearables. Specifically, we explore posit arithmetic, a floating-point-like\nrepresentation, in two key applications: cough detection for chronic cough\nmonitoring and R peak detection in ECG analysis. Simulations reveal that 16-bit\nposits can replace 32-bit IEEE 754 floating point numbers with minimal accuracy\nloss in cough detection. For R peak detection, posit arithmetic achieves\nsatisfactory accuracy with as few as 10 or 8 bits, compared to the 16-bit\nrequirement for floating-point formats. To further this exploration, we\nintroduce PHEE, a modular and extensible architecture that integrates the\nCoprosit posit coprocessor within a RISC-V-based system. Using the X-HEEP\nframework, PHEE seamlessly incorporates posit arithmetic, demonstrating reduced\nhardware area and power consumption compared to a floating-point counterpart\nsystem. Post-synthesis results targeting 16nm TSMC technology show that the\nposit hardware targeting these biomedical applications can be 38% smaller and\nconsume up to 54% less energy at the functional unit level, with no performance\ncompromise. These findings establish the potential of low-precision posit\narithmetic to significantly improve the energy efficiency of wearable\nbiomedical devices.",
        "As the potential for autonomous vehicles to be integrated on a large scale\ninto modern traffic systems continues to grow, ensuring safe navigation in\ndynamic environments is crucial for smooth integration. To guarantee safety and\nprevent collisions, autonomous vehicles must be capable of accurately\npredicting the trajectories of surrounding traffic agents. Over the past\ndecade, significant efforts from both academia and industry have been dedicated\nto designing solutions for precise trajectory forecasting. These efforts have\nproduced a diverse range of approaches, raising questions about the differences\nbetween these methods and whether trajectory prediction challenges have been\nfully addressed. This paper reviews a substantial portion of recent trajectory\nprediction methods and devises a taxonomy to classify existing solutions. A\ngeneral overview of the prediction pipeline is also provided, covering input\nand output modalities, modeling features, and prediction paradigms discussed in\nthe literature. In addition, the paper discusses active research areas within\ntrajectory prediction, addresses the posed research questions, and highlights\nthe remaining research gaps and challenges.",
        "Photon propagators for power-law inflation are constructed in two\none-parameter families of noncovariant gauges, in an arbitrary number of\nspacetime dimensions. In both gauges photon propagators take relatively simple\nforms expressed in terms of scalar propagators and their derivatives. These are\nconsiderably simpler compared to their general covariant gauge counterpart.\nThis makes feasible performing dimensionally regulated loop computations\ninvolving massless vector fields in inflation.",
        "Recent advances in large models have significantly advanced image-to-3D\nreconstruction. However, the generated models are often fused into a single\npiece, limiting their applicability in downstream tasks. This paper focuses on\n3D garment generation, a key area for applications like virtual try-on with\ndynamic garment animations, which require garments to be separable and\nsimulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs\nphysics-plausible, simulation-ready separated garments with sewing patterns and\nhumans from an in-the-wild image. Starting with the image, our approach\ncombines a pre-trained image-to-sewing pattern generation model for creating\ncoarse sewing patterns with a pre-trained multi-view diffusion model to produce\nmulti-view images. The sewing pattern is further refined using a differentiable\ngarment simulator based on the generated multi-view images. Versatile\nexperiments demonstrate that our optimization approach substantially enhances\nthe geometric alignment of the reconstructed 3D garments and humans with the\ninput image. Furthermore, by integrating a texture generation module and a\nhuman motion generation module, we produce customized physics-plausible and\nrealistic dynamic garment demonstrations. Project page:\nhttps:\/\/dress-1-to-3.github.io\/",
        "Vision Transformers (ViTs), extensively pre-trained on large-scale datasets,\nhave become essential to foundation models, allowing excellent performance on\ndiverse downstream tasks with minimal adaptation. Consequently, there is\ngrowing interest in adapting pre-trained ViTs across various fields, including\nprivacy-sensitive domains where clients are often reluctant to share their\ndata. Existing adaptation methods typically require direct data access,\nrendering them infeasible under these constraints. A straightforward solution\nmay be sending the pre-trained ViT to clients for local adaptation, which poses\nissues of model intellectual property protection and incurs heavy client\ncomputation overhead. To address these issues, we propose a novel split\nadaptation (SA) method that enables effective downstream adaptation while\nprotecting data and models. SA, inspired by split learning (SL), segments the\npre-trained ViT into a frontend and a backend, with only the frontend shared\nwith the client for data representation extraction. But unlike regular SL, SA\nreplaces frontend parameters with low-bit quantized values, preventing direct\nexposure of the model. SA allows the client to add bi-level noise to the\nfrontend and the extracted data representations, ensuring data protection.\nAccordingly, SA incorporates data-level and model-level out-of-distribution\nenhancements to mitigate noise injection's impact on adaptation performance.\nOur SA focuses on the challenging few-shot adaptation and adopts patch\nretrieval augmentation for overfitting alleviation. Extensive experiments on\nmultiple datasets validate SA's superiority over state-of-the-art methods and\ndemonstrate its defense against advanced data reconstruction attacks while\npreventing model leakage with minimal computation cost on the client side. The\nsource codes can be found at https:\/\/github.com\/conditionWang\/Split_Adaptation.",
        "In this study, we analyzed the close binary open clusters CWNU 2666 and HSC\n224, which are in close spatial proximity, using photometric and astrometric\ndata from the {\\it Gaia} DR3 catalog. Likely member stars were identified based\non a membership probability threshold ($P \\geq 0.5$), resulting in 106 and 146\nmembers for CWNU 2666 and HSC 224, respectively. The mean proper motion\ncomponents ($\\mu_{\\alpha}\\cos\\delta$, $\\mu_{\\delta}$) were determined to be\n(0.646$\\pm$0.155, -0.769$\\pm$0.124) mas yr$^{-1}$ for CWNU 2666, and\n(0.665$\\pm$0.131, -0.728$\\pm$0.107) mas yr$^{-1}$ for HSC 224. The isochrone\ndistances ($d_{\\rm iso}$) were estimated as 1885$\\pm$44 pc for CWNU 2666 and\n1866$\\pm$29 pc for HSC 224. The corresponding cluster ages ($t$) were derived\nas 160$\\pm$15 Myr and 140$\\pm$15 Myr, respectively. The astrometric and\nfundamental astrophysical parameters derived in this study demonstrate that the\ntwo open clusters are a close pair of open clusters."
      ]
    }
  },
  {
    "id":2411.0064,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"The Llama 3 Herd of Models",
    "start_abstract":"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Quantifying Variance in Evaluation Benchmarks"
      ],
      "abstract":[
        "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
      ],
      "categories":[
        "stat.ME"
      ]
    },
    "list":{
      "title":[
        "Factor Modelling for Biclustering Large-dimensional Matrix-valued Time\n  Series",
        "Estimands for single arm dose optimization trials in oncology",
        "Flexible Empirical Bayesian Approaches to Pharmacovigilance for\n  Simultaneous Signal Detection and Signal Strength Estimation in Spontaneous\n  Reporting Systems Data",
        "A retake on the analysis of scores truncated by terminal events",
        "Phylogenetic latent space models for network data",
        "Cross Validation for Correlated Data in Regression and Classification\n  Models, with Applications to Deep Learning",
        "Change-plane analysis in functional response quantile regression",
        "Detection and estimation of vertex-wise latent position shifts across\n  networks",
        "Log-Gaussian Cox Processes on General Metric Graphs",
        "Online Correlation Change Detection for Large-Dimensional Data with An\n  Application to Forecasting of El Ni\\~no Events",
        "Outcome Regression Methods for Analyzing Hybrid Control Studies:\n  Balancing Bias and Variability",
        "Generalized Multi-Linear Models for Sufficient Dimension Reduction on\n  Tensor Valued Predictors",
        "Multi-Hazard Bayesian Hierarchical Model for Damage Prediction",
        "De facto Openness to Immigration",
        "Extended $s$-wave pairing from an emergent Feshbach resonanc in bilayer\n  nickelate superconductors",
        "Optical control of the spin-Hall effect in a two-dimensional hole gas",
        "Quantum-enhanced neural networks for quantum many-body simulations",
        "A Link Between White Dwarf Pulsars and Polars: Multiwavelength\n  Observations of the 9.36-Minute Period Variable Gaia22ayj",
        "Decoding lithium's subtle phase stability with a machine learning force\n  field",
        "Decay of large solutions around shocks to multi-D viscous conservation\n  law with strictly convex flux",
        "Scalable architecture for dark photon searches: Superconducting-qubit\n  proof of principle",
        "On zero-sum Ramsey numbers modulo 3",
        "Isogenies of minimal Cantor systems: from Sturmian to Denjoy and\n  interval exchanges",
        "Modeling reflection and refraction of freeform surfaces",
        "Anisotropy can make a moving active fluid membrane rough or crumpled",
        "Tomographic identification of all molecular orbitals in a wide binding\n  energy range",
        "Star formation in interacting galaxy systems: UVIT imaging of NGC 7252\n  and NGC 5291",
        "Using Lagrangian descriptors to reveal the phase space structure of\n  dynamical systems described by fractional differential equations: Application\n  to the Duffing oscillator"
      ],
      "abstract":[
        "A novel unsupervised learning method is proposed in this paper for\nbiclustering large-dimensional matrix-valued time series based on an entirely\nnew latent two-way factor structure. Each block cluster is characterized by its\nown row and column cluster-specific factors in addition to some common matrix\nfactors which impact on all the matrix time series. We first estimate the\nglobal loading spaces by projecting the observation matrices onto the row or\ncolumn loading space corresponding to common factors. The loading spaces for\ncluster-specific factors are then further recovered by projecting the\nobservation matrices onto the orthogonal complement space of the estimated\nglobal loading spaces. To identify the latent row\/column clusters\nsimultaneously for matrix-valued time series, we provide a $K$-means algorithm\nbased on the estimated row\/column factor loadings of the cluster-specific weak\nfactors. Theoretically, we derive faster convergence rates for global loading\nmatrices than those of the state-of-the-art methods available in the literature\nunder mild conditions. We also propose an one-pass eigenvalue-ratio method to\nestimate the numbers of global and cluster-specific factors. The consistency\nwith explicit convergence rates is also established for the estimators of the\nlocal loading matrices, the factor numbers and the latent cluster memberships.\nNumerical experiments with both simulated data as well as a real data example\nare also reported to illustrate the usefulness of our proposed method.",
        "Phase I dose escalation trials in oncology generally aim to find the maximum\ntolerated dose (MTD). However, with the advent of molecular targeted therapies\nand antibody drug conjugates, dose limiting toxicities are less frequently\nobserved, giving rise to the concept of optimal biological dose (OBD), which\nconsiders both efficacy and toxicity. The Estimand framework presented in the\naddendum of the ICH E9(R1) guidelines strengthens the dialogue between\ndifferent stakeholders by bringing in greater clarity in the clinical trial\nobjectives and by providing alignment between the targeted estimand under\nconsideration and the statistical analysis methods. However, there lacks\nclarity in implementing this framework in early phase dose optimization\nstudies. This manuscript aims at discussing the Estimand framework for dose\noptimization trials in oncology considering efficacy and toxicity through\nutility functions. Such trials should include Pharmacokinetics (PK) data,\ntoxicity data, and efficacy data. Based on these data, the analysis methods\nused to identify the optimized dose\/s are also described. Focusing on\noptimizing the utility function to estimate the OBD, the population-level\nsummary measure should reflect only the properties used for the estimating this\nutility function. A detailed strategy recommendation for intercurrent events\nhas been provided using a real-life oncology case study. Key recommendations\nregarding the estimand attributes include that in a seamless Phase I\/II dose\noptimization trial, the treatment attribute should start when the subject\nreceives the first dose. We argue that such a framework brings in additional\nclarity to dose optimization trial objectives and strengthens the understanding\nof the drug under consideration that would enable the correct dose to move to\nPhase II of clinical development.",
        "Inferring adverse events (AEs) of medical products from Spontaneous Reporting\nSystems (SRS) databases is a core challenge in contemporary pharmacovigilance.\nBayesian methods for pharmacovigilance are attractive for their rigorous\nability to simultaneously detect potential AE signals and estimate their\nstrengths\/degrees of relevance. However, existing Bayesian and empirical\nBayesian methods impose restrictive parametric assumptions and\/or demand\nsubstantial computational resources, limiting their practical utility. This\npaper introduces a suite of novel, scalable empirical Bayes methods for\npharmacovigilance that utilize flexible non-parametric priors and custom,\nefficient data-driven estimation techniques to enhance signal detection and\nsignal strength estimation at a low computational cost. Our highly flexible\nmethods accommodate a broader range of data and achieve signal detection\nperformance comparable to or better than existing Bayesian and empirical\nBayesian approaches. More importantly, they provide coherent and high-fidelity\nestimation and uncertainty quantification for potential AE signal strengths,\noffering deeper insights into the comparative importance and relevance of AEs.\nExtensive simulation experiments across diverse data-generating scenarios\ndemonstrate the superiority of our methods in terms of accurate signal strength\nestimation, as measured by replication root mean squared errors. Additionally,\nour methods maintain or exceed the signal detection performance of\nstate-of-the-art techniques, as evaluated by frequentist false discovery rates\nand sensitivity metrics. Applications on FDA FAERS data for the statin group of\ndrugs reveal interesting insights through Bayesian posterior probabilities.",
        "Analysis of data from randomized controlled trials in vulnerable populations\nrequires special attention when assessing treatment effect by a score\nmeasuring, e.g., disease stage or activity together with onset of prevalent\nterminal events. In reality, it is impossible to disentangle a disease score\nfrom the terminal event, since the score is not clinically meaningful after\nthis event. In this work, we propose to assess treatment interventions\nsimultaneously on disease score and the terminal event. Our proposal is based\non a natural data-generating mechanism respecting that a disease score does not\nexist beyond the terminal event. We use modern semi-parametric statistical\nmethods to provide robust and efficient estimation of the risk of terminal\nevent and expected disease score conditional on no terminal event at a\npre-specified landmark time. We also use the simultaneous asymptotic behavior\nof our estimators to develop a powerful closed testing procedure for\nconfirmatory assessment of treatment effect on both onset of terminal event and\nlevel of disease score. A simulation study mimicking a large-scale outcome\ntrial in chronic kidney patients as well as an analysis of that trial is\nprovided to assess performance.",
        "Latent space models for network data characterize each node through a vector\nof latent features whose pairwise similarities define the edge probabilities\namong pairs of nodes. Although this formulation has led to successful\nimplementations and impactful extensions, the overarching focus has been on\ndirectly inferring node embeddings through the latent features rather than\nlearning the generative process underlying the embedding. This focus prevents\nfrom borrowing information among the features of different nodes and fails to\ninfer complex higher-level architectures regulating the formation of the\nnetwork itself. For example, routinely-studied networks often exhibit\nmultiscale structures informing on nested modular hierarchies among nodes that\ncould be learned via tree-based representations of dependencies among latent\nfeatures. We pursue this direction by developing an innovative phylogenetic\nlatent space model that explicitly characterizes the generative process of the\nnodes' feature vectors via a branching Brownian motion, with branching\nstructure parametrized by a phylogenetic tree. This tree constitutes the main\nobject of interest and is learned under a Bayesian perspective to infer\ntree-based modular hierarchies among nodes that explain heterogenous multiscale\npatterns in the network. Identifiability results are derived along with\nposterior consistency theory, and the inference potentials of the\nnewly-proposed model are illustrated in simulations and two real-data\napplications from criminology and neuroscience, where our formulation learns\ncore structures hidden to state-of-the-art alternatives.",
        "We present a methodology for model evaluation and selection where the\nsampling mechanism violates the i.i.d. assumption. Our methodology involves a\nformulation of the bias between the standard Cross-Validation (CV) estimator\nand the mean generalization error, denoted by $w_{cv}$, and practical\ndata-based procedures to estimate this term. This concept was introduced in the\nliterature only in the context of a linear model with squared error loss as the\ncriterion for prediction performance. Our proposed bias-corrected CV estimator,\n$\\text{CV}_c=\\text{CV}+w_{cv}$, can be applied to any learning model, including\ndeep neural networks, and to a wide class of criteria for prediction\nperformance in regression and classification tasks. We demonstrate the\napplicability of the proposed methodology in various scenarios where the data\ncontains complex correlation structures (such as clustered and spatial\nrelationships) with synthetic data and real-world datasets, providing evidence\nthat the estimator $\\text{CV}_c$ is better than the standard CV estimator. This\npaper is an expanded version of our published conference paper.",
        "Change-plane analysis is a pivotal tool for identifying subgroups within a\nheterogeneous population, yet it presents challenges when applied to functional\ndata. In this paper, we consider a change-plane model within the framework of\nfunctional response quantile regression, capable of identifying and testing\nsubgroups in non-Gaussian functional responses with scalar predictors. The\nproposed model naturally extends the change-plane method to account for the\nheterogeneity in functional data. To detect the existence of subgroups, we\ndevelop a weighted average of the squared score test statistic, which has a\nclosed form and thereby reduces the computational stress. An alternating\ndirection method of multipliers algorithm is formulated to estimate the\nfunctional coefficients and the grouping parameters. We establish the\nasymptotic theory for the estimates based on the reproducing kernel Hilbert\nspace and derive the asymptotic distributions of the proposed test statistic\nunder both null and alternative hypotheses. Simulation studies are conducted to\nevaluate the performance of the proposed approach in subgroup identification\nand hypothesis test. The proposed methods are also applied to two datasets, one\nfrom a study on China stocks and another from the COVID-19 pandemic.",
        "Pairwise network comparison is essential for various applications, including\nneuroscience, disease research, and dynamic network analysis. While existing\nliterature primarily focuses on comparing entire network structures, we address\na vertex-wise comparison problem where two random networks share the same set\nof vertices but allow for structural variations in some vertices, enabling a\nmore detailed and flexible analysis of network differences. In our framework,\nsome vertices retain their latent positions between networks, while others\nundergo shifts. To identify the shifted and unshifted vertices and estimate\ntheir latent position shifts, we propose a method that first derives vertex\nembeddings in a low-rank Euclidean space for each network, then aligns these\nestimated vertex latent positions into a common space to resolve potential\nnon-identifiability, and finally tests whether each vertex is shifted or not\nand estimates the vertex shifts. Our theoretical results establish the test\nstatistic for the algorithms, guide parameter selection, and provide\nperformance guarantees. Simulation studies and real data applications,\nincluding a case-control study in disease research and dynamic network\nanalysis, demonstrate that the proposed algorithms are both computationally\nefficient and effective in extracting meaningful insights from network\ncomparisons.",
        "The modeling of spatial point processes has advanced considerably, yet\nextending these models to non-Euclidean domains, such as road networks, remains\na challenging problem. We propose a novel framework for log-Gaussian Cox\nprocesses on general compact metric graphs by leveraging the Gaussian\nWhittle-Mat\\'ern fields, which are solutions to fractional-order stochastic\ndifferential equations on metric graphs. To achieve computationally efficient\nlikelihood-based inference, we introduce a numerical approximation of the\nlikelihood that eliminates the need to approximate the Gaussian process. This\nmethod, coupled with the exact evaluation of finite-dimensional distributions\nfor Whittle-Mat\\'ern fields with integer smoothness, ensures scalability and\ntheoretical rigour, with derived convergence rates for posterior distributions.\nThe framework is implemented in the open-source MetricGraph R package, which\nintegrates seamlessly with R-INLA to support fully Bayesian inference. We\ndemonstrate the applicability and scalability of this approach through an\nanalysis of road accident data from Al-Ahsa, Saudi Arabia, consisting of over\n150,000 road segments. By identifying high-risk road segments using exceedance\nprobabilities and excursion sets, our framework provides localized insights\ninto accident hotspots and offers a powerful tool for modeling spatial point\nprocesses directly on complex networks.",
        "We consider detecting change points in the correlation structure of streaming\nlarge-dimensional data with minimum assumptions posed on the underlying data\ndistribution. Depending on the $\\ell_1$ and $\\ell_{\\infty}$ norms of the\nsquared difference of vectorized pre-change and post-change correlation\nmatrices, detection statistics are constructed for dense and sparse settings,\nrespectively. The proposed detection procedures possess the bless-dimension\nproperty, as a novel algorithm for threshold selection is designed based on\nsign-flip permutation. Theoretical evaluations of the proposed methods are\nconducted in terms of average run length and expected detection delay.\nNumerical studies are conducted to examine the finite sample performances of\nthe proposed methods. Our methods are effective because the average detection\ndelays have slopes similar to that of the optimal exact CUSUM test. Moreover, a\ncombined $\\ell_1$ and $\\ell_{\\infty}$ norm approach is proposed and has\nexpected performance for transitions from sparse to dense settings. Our method\nis applied to forecast El Ni{\\~n}o events and achieves state-of-the-art hit\nrates greater than 0.86, while false alarm rates are 0. This application\nillustrates the efficiency and effectiveness of our proposed methodology in\ndetecting fundamental changes with minimal delay.",
        "There is growing interest in a hybrid control design in which a randomized\ncontrolled trial is augmented with an external control arm from a previous\ntrial or real world data. Existing methods for analyzing hybrid control studies\ninclude various downweighting and propensity score methods as well as methods\nthat combine downweighting with propensity score stratification. In this\narticle, we describe and discuss methods that make use of an outcome regression\nmodel (possibly in addition to a propensity score model). Specifically, we\nconsider an augmentation method, a G-computation method, and a weighted\nregression method, and note that the three methods provide different\nbias-variance trade-offs. The methods are compared with each other and with\nexisting methods in a simulation study. Simulation results indicate that\nweighted regression compares favorably with other model-based methods that seek\nto improve efficiency by incorporating external control data. The methods are\nillustrated using two examples from urology and infectious disease.",
        "We consider supervised learning (regression\/classification) problems with\ntensor-valued input. We derive multi-linear sufficient reductions for the\nregression or classification problem by modeling the conditional distribution\nof the predictors given the response as a member of the quadratic exponential\nfamily. We develop estimation procedures of sufficient reductions for both\ncontinuous and binary tensor-valued predictors. We prove the consistency and\nasymptotic normality of the estimated sufficient reduction using manifold\ntheory. For continuous predictors, the estimation algorithm is highly\ncomputationally efficient and is also applicable to situations where the\ndimension of the reduction exceeds the sample size. We demonstrate the superior\nperformance of our approach in simulations and real-world data examples for\nboth continuous and binary tensor-valued predictors.",
        "A fundamental theoretical limitation undermines current disaster risk models:\nexisting approaches suffer from two critical constraints. First, conventional\ndamage prediction models remain predominantly deterministic, relying on fixed\nparameters established through expert judgment rather than learned from data.\nSecond, probabilistic frameworks are fundamentally restricted by their\nunderlying assumption of hazard independence, which directly contradicts the\nobserved reality of cascading and compound disasters. By relying on fixed\nexpert parameters and treating hazards as independent phenomena, these models\ndangerously misrepresent the true risk landscape. This work addresses this\nchallenge by developing the Multi-Hazard Bayesian Hierarchical Model (MH-BHM),\nwhich reconceptualizes the classical risk equation beyond its deterministic\norigins. The model's core theoretical contribution lies in reformulating a\nclassical risk formula as a fully probabilistic model that naturally\naccommodates hazard interactions through its hierarchical structure while\npreserving the traditional hazard-exposure-vulnerability framework. Using\ntropical cyclone damage data (1952-2020) from the Philippines as a test case,\nwith out-of-sample validation on recent events (2020-2022), the model\ndemonstrates significant empirical advantages. Key findings include a reduction\nin damage prediction error by 61% compared to a single-hazard model, and 80%\ncompared to a benchmark deterministic model. This corresponds to an improvement\nin damage estimation accuracy of USD 0.8 billion and USD 2 billion,\nrespectively. The improved accuracy enables more effective disaster risk\nmanagement across multiple domains, from optimized insurance pricing and\nnational resource allocation to local adaptation strategies, fundamentally\nimproving society's capacity to prepare for and respond to disasters.",
        "Various factors influence why some countries are more open to immigration\nthan others. Policy is only one of them. We design country-specific measures of\nopenness to immigration that aim to capture de facto levels of openness to\nimmigration, complementing existing de jure measures of immigration, based on\nenacted immigration laws and policy measures. We estimate these for 148\ncountries and three years (2000, 2010, and 2020). For a subset of countries, we\nalso distinguish between openness towards tertiary-educated migrants and less\nthan tertiary-educated migrants. Using the measures, we show that most places\nin the World today are closed to immigration, and a few regions are very open.\nThe World became more open in the first decade of the millennium, an opening\nmainly driven by the Western World and the Gulf countries. Moreover, we show\nthat other factors equal, countries that increased their openness to\nimmigration, reduced their old-age dependency ratios, and experienced slower\nreal wage growth, arguably a sign of relaxing labor and skill shortages.",
        "Since the discovery of unconventional superconductivity in cuprates,\nunraveling the pairing mechanism of charge carriers in doped antiferromagnets\nhas been a long-standing challenge. Motivated by the discovery of high-T$_c$\nsuperconductivity in nickelate bilayer La$_3$Ni$_2$O$_7$ (LNO), we study a\nminimal mixed dimensional (MixD) $t-J$ model supplemented with a repulsive\nCoulomb interaction $V$. When hole-doped, previous numerical simulations\nrevealed that the system exhibits strong binding energies, with a phenomenology\nresembling a BCS-to-BEC crossover accompanied by a Feshbach resonance between\ntwo distinct types of charge carriers. Here, we perform a mean-field analysis\nthat enables a direct observation of the BCS-to-BEC crossover as well as\nmicroscopic insights into the crossover region and the pairing symmetry for\ntwo-dimensional bilayers. We benchmark our mean-field description by comparing\nit to density-matrix renormalization group (DMRG) simulations in quasi-one\ndimensional settings and find remarkably good agreement. For the\ntwo-dimensional system relevant to LNO our mean-field calculations predict a\nBCS pairing gap with an extended $s$-wave symmetry, directly resulting from the\npairing mechanism's Feshbach-origin. Our analysis hence gives insights into\npairing in unconventional superconductors and, further, can be tested in\ncurrently available ultracold atom experiments.",
        "Relativistic effects influence the motion of charged particles in solids by\nintertwining spin and momentum. The resulting phenomena exhibit rich and\nintriguing properties that can unveil radically new quantum devices. In this\ncontext, the two-dimensional hole gas formed in group IV heterostructures is a\nparticularly promising platform, owning to a notable spin-orbit coupling.\nHowever, the exploitation of spin-momentum locking and precise manipulation of\nspin currents has remained elusive thus far. Here we use the modulation-doping\ntechnique to break inversion symmetry at novel Ge1-xSnx\/Ge interfaces and\nexplore spin-orbit phenomena in the emergent Rashba-coupled hole gases.\nMagneto-optical investigations demonstrate the unusual establishment of a\nstaggered band alignment with carrier lifetime in the ns range. Optical spin\norientation is then leveraged to directly inject spin-polarized currents in the\nRashba-split 2D gas. Spin-to-charge conversion is shown to genuinely occur at\nthe staggered gap through the inverse spin-Hall effect. This provides\nunprecedented access to low-order contributions of the spin-orbit Hamiltonian.\nMoreover, it leads to the startling demonstration that the spin Hall angle can\nbe optically controlled by modifying the Rashba coupling through the\nphotoexcitation density. Ge1-xSnx quantum wells thus offer innovative solutions\nand functionalities stemming from their unique spin-dependent properties and\nintriguing quantum phenomena at the crossroad between transport and photonic\nrealms.",
        "Neural quantum states (NQS) have gained prominence in variational quantum\nMonte Carlo methods in approximating ground-state wavefunctions. Despite their\nsuccess, they face limitations in optimization, scalability, and expressivity\nin addressing certain problems. In this work, we propose a quantum-neural\nhybrid framework that combines parameterized quantum circuits with neural\nnetworks to model quantum many-body wavefunctions. This approach combines the\nefficient sampling and optimization capabilities of autoregressive neural\nnetworks with the enhanced expressivity provided by quantum circuits. Numerical\nsimulations demonstrate the scalability and accuracy of the hybrid ansatz in\nspin systems and quantum chemistry problems. Our results reveal that the hybrid\nmethod achieves notably lower relative energy compared to standalone NQS. These\nfindings underscore the potential of quantum-neural hybrid methods for tackling\nchallenging problems in quantum many-body simulations.",
        "White dwarfs (WDs) are the most abundant compact objects, and recent surveys\nhave suggested that over a third of WDs in accreting binaries host a strong (B\n$\\gtrsim$ 1 MG) magnetic field. However, the origin and evolution of WD\nmagnetism remain under debate. Two WD pulsars, AR Sco and J191213.72-441045.1\n(J1912), have been found, which are non-accreting binaries hosting rapidly\nspinning (1.97-min and 5.30-min, respectively) magnetic WDs. The WD in AR Sco\nis slowing down on a $P\/\\dot{P}\\approx 5.6\\times 10^6$ yr timescale. It is\nbelieved they will eventually become polars, accreting systems in which a\nmagnetic WD (B $\\approx 10-240$ MG) accretes from a Roche lobe-filling donor\nspinning in sync with the orbit ($\\gtrsim 78$ min). Here, we present\nmultiwavelength data and analysis of Gaia22ayj, which outbursted in March 2022.\nWe find that Gaia22ayj is a magnetic accreting WD that is rapidly spinning down\n($P\/\\dot{P} = 6.1^{+0.3}_{-0.2}\\times 10^6$ yr) like WD pulsars, but shows\nclear evidence of accretion, like polars. Strong linear polarization (40%) is\ndetected in Gaia22ayj; such high levels have only been seen in the WD pulsar AR\nSco and demonstrate the WD is magnetic. High speed photometry reveals a\n9.36-min period accompanying a high amplitude ($\\sim 2$ mag) modulation. We\nassociate this with a WD spin or spin-orbit beat period, not an orbital period\nas was previously suggested. Fast (60-s) optical spectroscopy reveals a broad\n``hump'', reminiscent of cyclotron emission in polars, between 4000-8000\nAngstrom. We find an X-ray luminosity of $L_X = 2.7_{-0.8}^{+6.2}\\times10^{32}\n\\textrm{ erg s}^{-1}$ in the 0.3-8 keV energy range, while two VLA radio\ncampaigns resulted in a non-detection with a $F_r < 15.8\\mu\\textrm{Jy}$ 3$\n\\sigma$ upper limit. The shared properties of both WD pulsars and polars\nsuggest that Gaia22ayj is a missing link between the two classes of magnetic WD\nbinaries.",
        "Understanding the phase stability of elemental lithium (Li) is crucial for\noptimizing its performance in lithium-metal battery anodes, yet this seemingly\nsimple metal exhibits complex polymorphism that requires proper accounting for\nquantum and anharmonic effects to capture the subtleties in its flat energy\nlandscape. Here we address this challenge by developing an accurate graph\nneural network-based machine learning force field and performing efficient\nself-consistent phonon calculations for bcc-, fcc-, and 9R-Li under\nnear-ambient conditions, incorporating quantum, phonon renormalization and\nthermal expansion effects. Our results reveal the important role of\nanharmonicity in determining Li's thermodynamic properties. The free energy\ndifferences between these phases, particularly fcc- and 9R-Li are found to be\nonly a few meV\/atom, explaining the experimental challenges in obtaining\nphase-pure samples and suggesting a propensity for stacking faults and related\ndefect formation. fcc-Li is confirmed as the ground state at zero temperature\nand pressure, and the predicted bcc-fcc phase boundary qualitatively matches\nexperimental phase transition lines, despite overestimation of the transition\ntemperature and pressure slope. These findings provide crucial insights into\nLi's complex polymorphism and establish an effective computational approach for\nlarge-scale atomistic simulations of Li in more realistic settings for\npractical energy storage applications.",
        "We consider a planar viscous shock for a scalar viscous conservation law with\na strictly convex flux in multi-dimensional setting, where the transversal\ndirection is periodic. We first show the contraction property for any solutions\nevolving from a large bounded initial perturbation in $L^2$ of the viscous\nshock. The contraction holds up to a dynamical shift, and it is measured by a\nweighted relative entropy. This result for the contraction extends the existing\nresult in 1D \\cite{Kang19} to the multi-dimensional case. As a consequence, if\nthe large bounded initial $L^2$-perturbation is also in $L^1$, then the large\nperturbation decays of rate $t^{-1\/4}$ in $L^2$, up to a dynamical shift that\nis uniformly bounded in time. This is the first result for the quantitative\nestimate converging to a planar shock under large perturbations.",
        "The dark photon is a well-motivated candidate of dark matter due to its\npotential to open the window of new physics beyond the Standard Model. A\nfundamental mass-range-sensitivity dilemma is always haunting the dark photon\nsearching experiments: The resonant haloscopes have excellent sensitivity but\nare narrowband, and vice versa for the non-resonant ones. A scalable\narchitecture integrating numerous resonant haloscopes will be a desirable\nsolution to this dilemma. However, even the concept of scalable searching\nremains rarely explored, due to the size limitation of conventional haloscopes\nimposed by the dark photon wavelength. Here we propose and demonstrate a novel\narchitecture using superconducting qubits as sub-wavelength haloscope units. By\nvirtue of the scalability of superconducting qubits, it is possible to\nintegrate multiple qubits with different frequencies on a chip-scale device.\nFurthermore, the frequencies of the qubits can be tuned to extend the searching\nmass range. Thus, our architectures allow for searching for dark photons in a\nbroad mass range with high sensitivity. As a proof-of-principle experiment, we\ndesigned and fabricated a three-qubit chip and successfully demonstrated a\nscalable dark-photon searching. Our work established constraints on dark\nphotons in the mass range of 15.632 $\\mu$eV$\\sim$15.638 $\\mu$eV, 15.838\n$\\mu$eV$\\sim$15.845 $\\mu$eV, and 16.463 $\\mu$eV$\\sim$16.468 $\\mu$eV,\nsimultaneously, and the constraints are much more stringent than the cosmology\nconstraints. Our work can be scaled up in the future to boost the scrutiny of\nnew physics and extended to search for more dark matter candidates, including\ndark photons, axions and axion-like particles.",
        "We start with a systematic study of the zero-sum Ramsey numbers. For a graph\n$G$ with $0 \\ (\\!\\!\\!\\!\\mod 3)$ edges, the zero-sum Ramsey number is defined as\nthe smallest positive integer $R(G, \\mathbb{Z}_3)$ such that for every $n \\geq\nR(G, \\mathbb{Z}_3)$ and every edge-colouring $f$ of $K_n$ using $\\mathbb{Z}_3$,\nthere is a zero-sum copy of $G$ in $K_n$ coloured by $f$, that is: $\\sum_{e \\in\nE(G)} f(e) \\equiv 0 \\ (\\!\\!\\!\\!\\mod 3)$.\n  Only sporadic results are known for these Ramsey numbers, and we discover\nmany new ones. In particular we prove that for every forest $F$ on $n$ vertices\nand with $0 \\ (\\!\\!\\!\\!\\mod 3)$ edges, $R(F, \\mathbb{Z}_3) \\leq n+2$, and this\nbound is tight if all the vertices of $F$ have degrees $1 \\ (\\!\\!\\!\\!\\mod 3)$.\nWe also determine exact values of $R(T, \\mathbb{Z}_3)$ for infinite families of\ntrees.",
        "This work is motivated by the study of continued fraction expansions of real\nnumbers: we describe in dynamical terms their orbits under the action of\n$\\mathrm{PGL}_2(\\mathbb{Q})$. A real number gives rise to a Sturmian system\nencoding a rotation of the circle. It is well known that\n$\\mathrm{PGL}_2(\\mathbb{Z})$-equivalence of real numbers, characterized by the\ntails of their continued fraction expansions, amounts to flow equivalence of\nSturmian systems. We show that the multiplicative action of $m\\in \\mathbb{Z}$\non a real number corresponds to taking the $m$th-power followed by what we call\nan infinitesimal 2-asymptotic factor of its Sturmian system.\n  This leads us to introduce the notion of isogeny between zero-dimensional\nsystems: it combines virtual flow equivalences and infinitesimal asymptotic\nequivalences. We develop tools for classifying systems up to isogeny involving\ncohomological invariants and states. We then use this to give a complete\ndescription of $\\mathrm{PSL}_2(\\mathbb{Q})$-equivalence of real numbers in\nterms of Sturmian systems. We classify Denjoy systems up to isogenies within\nthis class via the action of $\\mathrm{PGL}_{2}(\\mathbb{Q})$ on their\ninvariants.\n  We also investigate eventual flow equivalence of Sturmian systems: we show\nthat for non-quadratic parameters it amounts to topological conjugacy and for\nquadratic parameters it implies total flow equivalence and other arithmetic\nconstraints.\n  In another direction, we consider interval exchanges satisfying Keane's\ncondition. We characterize flow equivalence in terms of interval-induced\nsubsystems (or the tails of their paths in the bilateral Rauzy induction\ndiagram). Finally we find rational invariants for isogeny involving the length\nmodules and SAF invariants of the associated ergodic measures. This leads to a\nconjecture for their classification up to isogeny, which we prove in the\ntotally ergodic case.",
        "In this work, we present a detailed procedure of computer implementation of\nthe laws of refraction and reflection on an arbitrary surface with rotational\nsymmetry with respect to the propagation axis. The goal is to facilitate the\nunderstanding and application of these physical principles in a computational\ncontext. This enables students and instructors alike to develop simulations and\ninteractive applications that faithfully replicate the behavior of light and\nsound propagating in a diversity of media separated by arbitrary surfaces. In\nparticular it can help to explore freeform optics. Additionally, we include a\npractical example demonstrating these implementations using either Matlab or\nopen-source Octave programming language.",
        "We present a hydrodynamic theory of anisotropic and inversion-asymmetric\nmoving active permeable fluid membranes. These are described by an anisotropic\nKardar-Parisi-Zhang equation. Depending upon the anisotropy parameters, the\nmembrane can be large-scale anisotropic and logarithmically rough with\ntranslational quasi long range order and orientational long range order,\ntogether with the relaxational dynamics being logarithmically faster than\nordinary diffusion. For other choices of the anisotropy parameters, the\nmembrane is either effectively isotropic and algebraically rough with\ntranslational short, but orientational long range order, or crumpled.",
        "In the past decade, photoemission orbital tomography (POT) has evolved into a\npowerful tool to investigate the electronic structure of organic molecules\nadsorbed on surfaces. Here we show that POT allows for the comprehensive\nexperimental identification of all molecular orbitals in a substantial binding\nenergy range, in the present case more than 10 eV. Making use of the angular\ndistribution of photoelectrons as a function of binding energy, we exemplify\nthis by extracting orbital-resolved partial densities of states (pDOS) for 15\n$\\pi$ and 23 $\\sigma$ orbitals from the experimental photoemission intensities\nof the prototypical organic molecule bisanthene (C$_{28}$H$_{14}$) on a Cu(110)\nsurface. In their entirety, these experimentally measured orbital-resolved pDOS\nfor an essentially complete set of orbitals serve as a stringent benchmark for\nelectronic structure methods, which we illustrate by performing density\nfunctional theory (DFT) calculations employing four frequently-used\nexchange-correlation functionals. By computing the respective\nmolecular-orbital-projected densities of states of the bisanthene\/Cu(110)\ninterface, a one-to-one comparison with experimental data for an unprecedented\nnumber of 38 orbital energies becomes possible. The quantitative analysis of\nour data reveals that the range-separated hybrid functional HSE performs best\nfor the investigated organic\/metal interface. At a more fundamental level, the\nremarkable agreement between the experimental and the Kohn-Sham orbital\nenergies over a binding energy range larger than 10\\,eV suggests that --\nperhaps unexpectedly -- Kohn-Sham orbitals approximate Dyson orbitals, which\nwould rigorously account for the electron extraction process in photoemission\nspectroscopy but are notoriously difficult to compute, in a much better way\nthan previously thought.",
        "Interactions play a significant role in the formation and evolution of\ngalaxies in the Universe. The galaxy systems, NGC 7252 and NGC 5291 are two\nnearby interacting systems that are hosting Tidal Dwarf Galaxies (TDGs) and\nstar-forming knots. The present work aims (a) To determine the\nattenuation-corrected star formation rate (SFR) of the interacting system NGC\n7252 (b) To compare the star formation in the NGC 7252 system with that of the\nNGC 5291 system (c) To explore the relation between surface densities of gas\nand SFR in these two systems. The study utilises high-resolution FUV and NUV\nimaging data from the Ultraviolet Imaging Telescope (UVIT) on board AstroSat.\nSix star-forming regions, including the merger remnant, were identified in the\nNGC 7252 system. The SFR corrected for attenuation of the knots in the NGC 7252\nsystem is determined using the continuum slope (\\beta) calculated from the\nFUV-NUV colour. It has been observed that the attenuation-corrected SFR values\nof the knots in this system fall within the range of SFR values determined for\nthe NGC 5291 knots. The TDGs in both systems adhere to the same\nKennicutt-Schmidt (KS) relation as regular spiral galaxies.",
        "We showcase the utility of the Lagrangian descriptors method in qualitatively\nunderstanding the underlying dynamical behavior of dynamical systems governed\nby fractional-order differential equations. In particular, we use the\nLagrangian descriptors method to study the phase space structure of the\nunforced and undamped Duffing oscillator when its time evolution is governed by\nfractional-order differential equations. In our study, we implement two types\nof fractional derivatives, namely the standard Gr\\\"unwald-Letnikov method,\nwhich is a finite difference approximation of the Riemann-Liouville fractional\nderivative, and a Gr\\\"unwald-Letnikov method with a correction term that\napproximates the Caputo fractional derivative. While there is no issue with\nforward-time integrations needed for the evaluation of Lagrangian descriptors,\nwe discuss in detail ways to perform the non-trivial task of backward-time\nintegrations and implement two methods for this purpose: a `nonlocal implicit\ninverse' technique and a `time-reverse inverse' approach. We analyze the\ndifferences in the Lagrangian descriptors results due to the two backward-time\nintegration approaches, discuss the physical significance of these differences,\nand eventually argue that the nonlocal implicit inverse implementation of the\nGr\\\"unwald-Letnikov fractional derivative manages to reveal the phase space\nstructure of fractional-order dynamical systems correctly."
      ]
    }
  },
  {
    "id":2411.06785,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"RNA-Seq: a revolutionary tool for transcriptomics",
    "start_abstract":"RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "White-Box Transformers via Sparse Rate Reduction"
      ],
      "abstract":[
        "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Deep learning for temporal super-resolution 4D Flow MRI",
        "Continuous Diffusion Model for Language Modeling",
        "ADO: Automatic Data Optimization for Inputs in LLM Prompts",
        "Nonparametric Heterogeneous Long-term Causal Effect Estimation via Data\n  Combination",
        "Long-range Brain Graph Transformer",
        "LLM-driven Knowledge Distillation for Dynamic Text-Attributed Graphs",
        "Overcoming Fake Solutions in Semi-Dual Neural Optimal Transport: A\n  Smoothing Approach for Learning the Optimal Transport Plan",
        "Compositional World Knowledge leads to High Utility Synthetic data",
        "T-SCEND: Test-time Scalable MCTS-enhanced Diffusion Model",
        "Geoinformatics-Guided Machine Learning for Power Plant Classification",
        "Dynamic Low-Rank Sparse Adaptation for Large Language Models",
        "Incorporating graph neural network into route choice model",
        "Efficient Time Series Forecasting via Hyper-Complex Models and Frequency\n  Aggregation",
        "On the Fly Adaptation of Behavior Tree-Based Policies through\n  Reinforcement Learning",
        "A novel inversion algorithm for weak gravitational lensing using\n  quasi-conformal geometry",
        "Synthetic $\\pi$-flux system in 2D superconducting qubit array with\n  tunable coupling",
        "A Note on Exact State Visit Probabilities in Two-State Markov Chains",
        "Erd\\H{o}s's integer dilation approximation problem and GCD graphs",
        "Cracking Vector Search Indexes",
        "Computing Game Symmetries and Equilibria That Respect Them",
        "Revealing quantum operator scrambling via measuring Holevo information\n  on digital quantum simulators",
        "It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate\n  Mutually via Selective Rationale Optimisation",
        "A Geometric Perspective for High-Dimensional Multiplex Graphs",
        "Employee Turnover Prediction: A Cross-component Attention Transformer\n  with Consideration of Competitor Influence and Contagious Effect",
        "Towards High-performance Spiking Transformers from ANN to SNN Conversion",
        "Cohort-attention Evaluation Metric against Tied Data: Studying\n  Performance of Classification Models in Cancer Detection",
        "Stability, periodic orbits and KAM tori in the dynamics of the three\n  fixed centers problem",
        "The Unbearable Lightness of Prompting: A Critical Reflection on the\n  Environmental Impact of genAI use in Design Education"
      ],
      "abstract":[
        "4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive technique\nfor volumetric, time-resolved blood flow quantification. However, apparent\ntrade-offs between acquisition time, image noise, and resolution limit clinical\napplicability. In particular, in regions of highly transient flow, coarse\ntemporal resolution can hinder accurate capture of physiologically relevant\nflow variations. To overcome these issues, post-processing techniques using\ndeep learning have shown promising results to enhance resolution post-scan\nusing so-called super-resolution networks. However, while super-resolution has\nbeen focusing on spatial upsampling, temporal super-resolution remains largely\nunexplored. The aim of this study was therefore to implement and evaluate a\nresidual network for temporal super-resolution 4D Flow MRI. To achieve this, an\nexisting spatial network (4DFlowNet) was re-designed for temporal upsampling,\nadapting input dimensions, and optimizing internal layer structures. Training\nand testing were performed using synthetic 4D Flow MRI data originating from\npatient-specific in-silico models, as well as using in-vivo datasets. Overall,\nexcellent performance was achieved with input velocities effectively denoised\nand temporally upsampled, with a mean absolute error (MAE) of 1.0 cm\/s in an\nunseen in-silico setting, outperforming deterministic alternatives (linear\ninterpolation MAE = 2.3 cm\/s, sinc interpolation MAE = 2.6 cm\/s). Further, the\nnetwork synthesized high-resolution temporal information from unseen\nlow-resolution in-vivo data, with strong correlation observed at peak flow\nframes. As such, our results highlight the potential of utilizing data-driven\nneural networks for temporal super-resolution 4D Flow MRI, enabling\nhigh-frame-rate flow quantification without extending acquisition times beyond\nclinically acceptable limits.",
        "Diffusion models have emerged as a promising alternative to autoregressive\nmodels in modeling discrete categorical data. Yet diffusion models that\ndirectly work on discrete data space do not fully exploit the power of\niterative refinement, as the signals are lost during the transition between\ndiscrete states. Existing continuous diffusion models for discrete data have\nlimited performance compared to discrete approaches, and the unclear link\nbetween them restricts the development of diffusion models for discrete data.\nIn this work, we propose a continuous diffusion model for language modeling\nthat incorporates the geometry of the underlying categorical distribution. We\nestablish a connection between the discrete diffusion and continuous flow on\nthe statistical manifold, and building on the analogy, we introduce a simple\ndesign for the diffusion process that generalizes previous discrete diffusion\nmodels. We further propose a simulation-free training framework based on radial\nsymmetry and a simple technique to address the high dimensionality of the\nmanifold. Comprehensive experiments on language modeling benchmarks and other\nmodalities show that our method outperforms existing discrete diffusion models\nand approaches the performance of autoregressive models. Codes available at\n\\href{https:\/\/github.com\/harryjo97\/RDLM}{https:\/\/github.com\/harryjo97\/RDLM}.",
        "This study explores a novel approach to enhance the performance of Large\nLanguage Models (LLMs) through the optimization of input data within prompts.\nWhile previous research has primarily focused on refining instruction\ncomponents and augmenting input data with in-context examples, our work\ninvestigates the potential benefits of optimizing the input data itself. We\nintroduce a two-pronged strategy for input data optimization: content\nengineering and structural reformulation. Content engineering involves imputing\nmissing values, removing irrelevant attributes, and enriching profiles by\ngenerating additional information inferred from existing attributes. Subsequent\nto content engineering, structural reformulation is applied to optimize the\npresentation of the modified content to LLMs, given their sensitivity to input\nformat. Our findings suggest that these optimizations can significantly improve\nthe performance of LLMs in various tasks, offering a promising avenue for\nfuture research in prompt engineering. The source code is available at\nhttps:\/\/anonymous.4open.science\/r\/ADO-6BC5\/",
        "Long-term causal inference has drawn increasing attention in many scientific\ndomains. Existing methods mainly focus on estimating average long-term causal\neffects by combining long-term observational data and short-term experimental\ndata. However, it is still understudied how to robustly and effectively\nestimate heterogeneous long-term causal effects, significantly limiting\npractical applications. In this paper, we propose several two-stage style\nnonparametric estimators for heterogeneous long-term causal effect estimation,\nincluding propensity-based, regression-based, and multiple robust estimators.\nWe conduct a comprehensive theoretical analysis of their asymptotic properties\nunder mild assumptions, with the ultimate goal of building a better\nunderstanding of the conditions under which some estimators can be expected to\nperform better. Extensive experiments across several semi-synthetic and\nreal-world datasets validate the theoretical results and demonstrate the\neffectiveness of the proposed estimators.",
        "Understanding communication and information processing among brain regions of\ninterest (ROIs) is highly dependent on long-range connectivity, which plays a\ncrucial role in facilitating diverse functional neural integration across the\nentire brain. However, previous studies generally focused on the short-range\ndependencies within brain networks while neglecting the long-range\ndependencies, limiting an integrated understanding of brain-wide communication.\nTo address this limitation, we propose Adaptive Long-range aware TransformER\n(ALTER), a brain graph transformer to capture long-range dependencies between\nbrain ROIs utilizing biased random walk. Specifically, we present a novel\nlong-range aware strategy to explicitly capture long-range dependencies between\nbrain ROIs. By guiding the walker towards the next hop with higher correlation\nvalue, our strategy simulates the real-world brain-wide communication.\nFurthermore, by employing the transformer framework, ALERT adaptively\nintegrates both short- and long-range dependencies between brain ROIs, enabling\nan integrated understanding of multi-level communication across the entire\nbrain. Extensive experiments on ABIDE and ADNI datasets demonstrate that ALTER\nconsistently outperforms generalized state-of-the-art graph learning methods\n(including SAN, Graphormer, GraphTrans, and LRGNN) and other graph learning\nbased brain network analysis methods (including FBNETGEN, BrainNetGNN,\nBrainGNN, and BrainNETTF) in neurological disease diagnosis. Cases of\nlong-range dependencies are also presented to further illustrate the\neffectiveness of ALTER. The implementation is available at\nhttps:\/\/github.com\/yushuowiki\/ALTER.",
        "Dynamic Text-Attributed Graphs (DyTAGs) have numerous real-world\napplications, e.g. social, collaboration, citation, communication, and review\nnetworks. In these networks, nodes and edges often contain text descriptions,\nand the graph structure can evolve over time. Future link prediction, edge\nclassification, relation generation, and other downstream tasks on DyTAGs\nrequire powerful representations that encode structural, temporal, and textual\ninformation. Although graph neural networks (GNNs) excel at handling structured\ndata, encoding temporal information within dynamic graphs remains a significant\nchallenge. In this work, we propose LLM-driven Knowledge Distillation for\nDynamic Text Attributed Graph (LKD4DyTAG) with temporal encoding to address\nthese challenges. We use a simple, yet effective approach to encode temporal\ninformation in edges so that graph convolution can simultaneously capture both\ntemporal and structural information in the hidden representations. To leverage\nLLM's text processing capabilities for learning richer representations on\nDyTAGs, we distill knowledge from LLM-driven edge representations (based on a\nneighborhood's text attributes) into saptio-temporal representations using a\nlightweight GNN model that encodes temporal and structural information. The\nobjective of knowledge distillation enables the GNN to learn representations\nthat more effectively encode the available structural, temporal, and textual\ninformation in DyTAG. We conducted extensive experimentation on six real-world\nDyTAG datasets to verify the effectiveness of our approach LKD4DyTAG for future\nlink prediction and edge classification task. The results show that our\napproach significantly improves the performance of downstream tasks compared to\nthe baseline models.",
        "We address the convergence problem in learning the Optimal Transport (OT)\nmap, where the OT Map refers to a map from one distribution to another while\nminimizing the transport cost. Semi-dual Neural OT, a widely used approach for\nlearning OT Maps with neural networks, often generates fake solutions that fail\nto transfer one distribution to another accurately. We identify a sufficient\ncondition under which the max-min solution of Semi-dual Neural OT recovers the\ntrue OT Map. Moreover, to address cases when this sufficient condition is not\nsatisfied, we propose a novel method, OTP, which learns both the OT Map and the\nOptimal Transport Plan, representing the optimal coupling between two\ndistributions. Under sharp assumptions on the distributions, we prove that our\nmodel eliminates the fake solution issue and correctly solves the OT problem.\nOur experiments show that the OTP model recovers the optimal transport map\nwhere existing methods fail and outperforms current OT-based models in\nimage-to-image translation tasks. Notably, the OTP model can learn stochastic\ntransport maps when deterministic OT Maps do not exist, such as one-to-many\ntasks like colorization.",
        "Machine learning systems struggle with robustness, under subpopulation\nshifts. This problem becomes especially pronounced in scenarios where only a\nsubset of attribute combinations is observed during training -a severe form of\nsubpopulation shift, referred as compositional shift. To address this problem,\nwe ask the following question: Can we improve the robustness by training on\nsynthetic data, spanning all possible attribute combinations? We first show\nthat training of conditional diffusion models on limited data lead to incorrect\nunderlying distribution. Therefore, synthetic data sampled from such models\nwill result in unfaithful samples and does not lead to improve performance of\ndownstream machine learning systems. To address this problem, we propose CoInD\nto reflect the compositional nature of the world by enforcing conditional\nindependence through minimizing Fisher's divergence between joint and marginal\ndistributions. We demonstrate that synthetic data generated by CoInD is\nfaithful and this translates to state-of-the-art worst-group accuracy on\ncompositional shift tasks on CelebA.",
        "We introduce Test-time Scalable MCTS-enhanced Diffusion Model (T-SCEND), a\nnovel framework that significantly improves diffusion model's reasoning\ncapabilities with better energy-based training and scaling up test-time\ncomputation. We first show that na\\\"ively scaling up inference budget for\ndiffusion models yields marginal gain. To address this, the training of T-SCEND\nconsists of a novel linear-regression negative contrastive learning objective\nto improve the performance-energy consistency of the energy landscape, and a KL\nregularization to reduce adversarial sampling. During inference, T-SCEND\nintegrates the denoising process with a novel hybrid Monte Carlo Tree Search\n(hMCTS), which sequentially performs best-of-N random search and MCTS as\ndenoising proceeds. On challenging reasoning tasks of Maze and Sudoku, we\ndemonstrate the effectiveness of T-SCEND's training objective and scalable\ninference method. In particular, trained with Maze sizes of up to $6\\times6$,\nour T-SCEND solves $88\\%$ of Maze problems with much larger sizes of\n$15\\times15$, while standard diffusion completely fails. Code to reproduce the\nexperiments can be found at https:\/\/github.com\/AI4Science-WestlakeU\/t_scend.",
        "This paper proposes an approach in the area of Knowledge-Guided Machine\nLearning (KGML) via a novel integrated framework comprising CNN (Convolutional\nNeural Networks) and ViT (Vision Transformers) along with GIS (Geographic\nInformation Systems) to enhance power plant classification in the context of\nenergy management. Knowledge from geoinformatics derived through Spatial Masks\n(SM) in GIS is infused into an architecture of CNN and ViT, in this proposed\nKGML approach. It is found to provide much better performance compared to the\nbaseline of CNN and ViT only in the classification of multiple types of power\nplants from real satellite imagery, hence emphasizing the vital role of the\ngeoinformatics-guided approach. This work makes a contribution to the main\ntheme of KGML that can be beneficial in many AI systems today. It makes broader\nimpacts on AI in Smart Cities, and Environmental Computing.",
        "Despite the efficacy of network sparsity in alleviating the deployment strain\nof Large Language Models (LLMs), it endures significant performance\ndegradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs\noffers an intuitive approach to counter this predicament, while it holds\nshortcomings include: 1) The inability to integrate LoRA weights into sparse\nLLMs post-training, and 2) Insufficient performance recovery at high sparsity\nratios. In this paper, we introduce dynamic Low-rank Sparse Adaptation (LoSA),\na novel method that seamlessly integrates low-rank adaptation into LLM sparsity\nwithin a unified framework, thereby enhancing the performance of sparse LLMs\nwithout increasing the inference latency. In particular, LoSA dynamically\nsparsifies the LoRA outcomes based on the corresponding sparse weights during\nfine-tuning, thus guaranteeing that the LoRA module can be integrated into the\nsparse LLMs post-training. Besides, LoSA leverages Representation Mutual\nInformation (RMI) as an indicator to determine the importance of layers,\nthereby efficiently determining the layer-wise sparsity rates during\nfine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based\non the variability in layer-wise reconstruction errors, allocating an\nappropriate fine-tuning for each layer to reduce the output discrepancies\nbetween dense and sparse LLMs. Extensive experiments tell that LoSA can\nefficiently boost the efficacy of sparse LLMs within a few hours, without\nintroducing any additional inferential burden. For example, LoSA reduced the\nperplexity of sparse LLaMA-2-7B by 68.73 and increased zero-shot accuracy by\n16.32$\\%$, achieving a 2.60$\\times$ speedup on CPU and 2.23$\\times$ speedup on\nGPU, requiring only 45 minutes of fine-tuning on a single NVIDIA A100 80GB GPU.\nCode is available at https:\/\/github.com\/wzhuang-xmu\/LoSA.",
        "Route choice models are one of the most important foundations for\ntransportation research. Traditionally, theory-based models have been utilized\nfor their great interpretability, such as logit models and Recursive logit\nmodels. More recently, machine learning approaches have gained attentions for\ntheir better prediction accuracy. In this study, we propose novel hybrid models\nthat integrate the Recursive logit model with Graph Neural Networks (GNNs) to\nenhance both predictive performance and model interpretability. To the authors'\nknowldedge, GNNs have not been utilized for route choice modeling, despite\ntheir proven effectiveness in capturing road network features and their\nwidespread use in other transportation research areas. We mathematically show\nthat our use of GNN is not only beneficial for enhancing the prediction\nperformance, but also relaxing the Independence of Irrelevant Alternatives\nproperty without relying on strong assumptions. This is due to the fact that a\nspecific type of GNN can efficiently capture multiple cross-effect patterns on\nnetworks from data. By applying the proposed models to one-day travel\ntrajectory data in Tokyo, we confirmed their higher prediction accuracy\ncompared to the existing models.",
        "Time series forecasting is a long-standing problem in statistics and machine\nlearning. One of the key challenges is processing sequences with long-range\ndependencies. To that end, a recent line of work applied the short-time Fourier\ntransform (STFT), which partitions the sequence into multiple subsequences and\napplies a Fourier transform to each separately. We propose the Frequency\nInformation Aggregation (FIA)-Net, which is based on a novel complex-valued MLP\narchitecture that aggregates adjacent window information in the frequency\ndomain. To further increase the receptive field of the FIA-Net, we treat the\nset of windows as hyper-complex (HC) valued vectors and employ HC algebra to\nefficiently combine information from all STFT windows altogether. Using the\nHC-MLP backbone allows for improved handling of sequences with long-term\ndependence. Furthermore, due to the nature of HC operations, the HC-MLP uses up\nto three times fewer parameters than the equivalent standard window aggregation\nmethod. We evaluate the FIA-Net on various time-series benchmarks and show that\nthe proposed methodologies outperform existing state of the art methods in\nterms of both accuracy and efficiency. Our code is publicly available on\nhttps:\/\/anonymous.4open.science\/r\/research-1803\/.",
        "With the rising demand for flexible manufacturing, robots are increasingly\nexpected to operate in dynamic environments where local -- such as slight\noffsets or size differences in workpieces -- are common. We propose to address\nthe problem of adapting robot behaviors to these task variations with a\nsample-efficient hierarchical reinforcement learning approach adapting Behavior\nTree (BT)-based policies. We maintain the core BT properties as an\ninterpretable, modular framework for structuring reactive behaviors, but extend\ntheir use beyond static tasks by inherently accommodating local task\nvariations. To show the efficiency and effectiveness of our approach, we\nconduct experiments both in simulation and on a Franka Emika Panda 7-DoF, with\nthe manipulator adapting to different obstacle avoidance and pivoting tasks.",
        "One of the challenges in weak gravitational lensing by galaxies and clusters\nis to infer the projected mass density distribution from gravitational lensing\nmeasurements, which is known as inversion problem. We introduce a novel\ntheoretical approach to solve the inversion problem. The cornerstone of the\nproposed method lies in a complex formalism that describes the lens mapping as\nquasi-conformal mapping with the Beltrami coefficient given by the negative of\nthe reduced shear, which is, in principle, observable from the image\nellipticities. We propose an algorithm called QCLens that is based on this\ncomplex formalism. QCLens computes the underlying quasi-conformal mapping with\na finite element approach by reducing the problem to two elliptic partial\ndifferential equations solely depending on the reduced shear field.\nExperimental results for both the Schwarzschild and singular isothermal lens\ndemonstrate the agreement of our proposed method with the analytically\ncomputable solutions.",
        "Flat-band systems provide an ideal platform for exploring exotic quantum\nphenomena, where the strongly suppressed kinetic energy in these flat energy\nbands suggests the potential for exotic phases driven by geometric structure,\ndisorder, and interactions. While intriguing phenomena and physical mechanisms\nhave been unveiled in theoretical models, synthesizing such systems within\nscalable quantum platforms remains challenging. Here, we present the\nexperimental realization of a $\\pi$-flux rhombic system using a two-dimensional\nsuperconducting qubit array with tunable coupling. We experimentally observe\ncharacteristic dynamics, e.g., $\\pi$-flux driven destructive interference, and\ndemonstrate the protocol for eigenstate preparation in this rhombic array with\ncoupler-assisted flux. Our results provide future possibilities for exploring\nthe interplay of geometry, interactions, and quantum information encoding in\nsuch degenerate systems.",
        "In this note we derive the exact probability that a specific state in a\ntwo-state Markov chain is visited exactly $k$ times after $N$ transitions. We\nprovide a closed-form solution for $\\mathbb{P}(N_l = k \\mid N)$, considering\ninitial state probabilities and transition dynamics. The solution corrects and\nextends prior incomplete results, offering a rigorous framework for enumerating\nstate transitions. Numerical simulations validate the derived expressions,\ndemonstrating their applicability in stochastic modeling.",
        "Let $\\mathcal{A}\\subset\\mathbb{R}_{\\geqslant1}$ be a countable set such that\n$\\limsup_{x\\to\\infty}\\frac{1}{\\log\nx}\\sum_{\\alpha\\in\\mathcal{A}\\cap[1,x]}\\frac{1}{\\alpha}>0$. We prove that, for\nevery $\\varepsilon>0$, there exist infinitely many pairs $(\\alpha, \\beta)\\in\n\\mathcal{A}^2$ such that $\\alpha\\neq \\beta$ and $|n\\alpha-\\beta| <\\varepsilon$\nfor some positive integer $n$. This resolves a problem of Erd\\H{o}s from 1948.\nA critical role in the proof is played by the machinery of GCD graphs, which\nwere introduced by the first author and by James Maynard in their work on the\nDuffin--Schaeffer conjecture in Diophantine approximation.",
        "Retrieval Augmented Generation (RAG) uses vector databases to expand the\nexpertise of an LLM model without having to retrain it. This idea can be\napplied over data lakes, leading to the notion of embeddings data lakes, i.e.,\na pool of vector databases ready to be used by RAGs. The key component in these\nsystems is the indexes enabling Approximated Nearest Neighbor Search (ANNS).\nHowever, in data lakes, one cannot realistically expect to build indexes for\nevery possible dataset. In this paper, we propose an adaptive, partition-based\nindex, CrackIVF, that performs much better than up-front index building.\nCrackIVF starts answering queries by near brute force search and only expands\nas it sees enough queries. It does so by progressively adapting the index to\nthe query workload. That way, queries can be answered right away without having\nto build a full index first. After seeing enough queries, CrackIVF will produce\nan index comparable to the best of those built using conventional techniques.\nAs the experimental evaluation shows, CrackIVF can often answer more than 1\nmillion queries before other approaches have even built the index and can start\nanswering queries immediately, achieving 10-1000x faster initialization times.\nThis makes it ideal when working with cold data or infrequently used data or as\na way to bootstrap access to unseen datasets.",
        "Strategic interactions can be represented more concisely, and analyzed and\nsolved more efficiently, if we are aware of the symmetries within the\nmultiagent system. Symmetries also have conceptual implications, for example\nfor equilibrium selection. We study the computational complexity of identifying\nand using symmetries. Using the classical framework of normal-form games, we\nconsider game symmetries that can be across some or all players and\/or actions.\nWe find a strong connection between game symmetries and graph automorphisms,\nyielding graph automorphism and graph isomorphism completeness results for\ncharacterizing the symmetries present in a game. On the other hand, we also\nshow that the problem becomes polynomial-time solvable when we restrict the\nconsideration of actions in one of two ways.\n  Next, we investigate when exactly game symmetries can be successfully\nleveraged for Nash equilibrium computation. We show that finding a Nash\nequilibrium that respects a given set of symmetries is PPAD- and CLS-complete\nin general-sum and team games respectively -- that is, exactly as hard as\nBrouwer fixed point and gradient descent problems. Finally, we present\npolynomial-time methods for the special cases where we are aware of a vast\nnumber of symmetries, or where the game is two-player zero-sum and we do not\neven know the symmetries.",
        "Quantum operator scrambling describes the spreading of local operators into\nthe whole system in the picture of Heisenberg evolution, which is often\nquantified by the operator size growth. Here we propose a measure of quantum\noperator scrambling via Holevo information of operators, by taking its capacity\nto distinguish operator information locally. We show that the operator size is\nclosely related to a special kind of Holevo information of operators. Moreover,\nwe propose a feasible protocol for measuring Holevo information of operators on\ndigital quantum simulators based on random states. Our numerical simulations\nshow that the integrable system can be told apart from the chaotic system by\nmeasuring the spatial-temporal patterns of Holevo information. Furthermore, we\nfind that error mitigation is required to restore the time-oscillation behavior\nof Holevo information for the integrable system, a crucial feature distinct\nfrom the chaotic one. Our work provides a new perspective to understand the\ninformation scrambling and quantum chaos from aspects of Holevo information of\noperators.",
        "Very large language models (LLMs) such as GPT-4 have shown the ability to\nhandle complex tasks by generating and self-refining step-by-step rationales.\nSmaller language models (SLMs), typically with < 13B parameters, have been\nimproved by using the data generated from very-large LMs through knowledge\ndistillation. However, various practical constraints such as API costs,\ncopyright, legal and ethical policies restrict using large (often opaque)\nmodels to train smaller models for commercial use. Limited success has been\nachieved at improving the ability of an SLM to explore the space of possible\nrationales and evaluate them by itself through self-deliberation. To address\nthis, we propose COALITION, a trainable framework that facilitates interaction\nbetween two variants of the same SLM and trains them to generate and refine\nrationales optimized for the end-task. The variants exhibit different behaviors\nto produce a set of diverse candidate rationales during the generation and\nrefinement steps. The model is then trained via Selective Rationale\nOptimization (SRO) to prefer generating rationale candidates that maximize the\nlikelihood of producing the ground-truth answer. During inference, COALITION\nemploys a controller to select the suitable variant for generating and refining\nthe rationales. On five different datasets covering mathematical problems,\ncommonsense reasoning, and natural language inference, COALITION outperforms\nseveral baselines by up to 5%. Our ablation studies reveal that\ncross-communication between the two variants performs better than using the\nsingle model to self-refine the rationales. We also demonstrate the\napplicability of COALITION for LMs of varying scales (4B to 14B parameters) and\nmodel families (Mistral, Llama, Qwen, Phi). We release the code for this work\nat https:\/\/github.com\/Sohanpatnaik106\/coalition.",
        "High-dimensional multiplex graphs are characterized by their high number of\ncomplementary and divergent dimensions. The existence of multiple hierarchical\nlatent relations between the graph dimensions poses significant challenges to\nembedding methods. In particular, the geometric distortions that might occur in\nthe representational space have been overlooked in the literature. This work\nstudies the problem of high-dimensional multiplex graph embedding from a\ngeometric perspective. We find that the node representations reside on highly\ncurved manifolds, thus rendering their exploitation more challenging for\ndownstream tasks. Moreover, our study reveals that increasing the number of\ngraph dimensions can cause further distortions to the highly curved manifolds.\nTo address this problem, we propose a novel multiplex graph embedding method\nthat harnesses hierarchical dimension embedding and Hyperbolic Graph Neural\nNetworks. The proposed approach hierarchically extracts hyperbolic node\nrepresentations that reside on Riemannian manifolds while gradually learning\nfewer and more expressive latent dimensions of the multiplex graph.\nExperimental results on real-world high-dimensional multiplex graphs show that\nthe synergy between hierarchical and hyperbolic embeddings incurs much fewer\ngeometric distortions and brings notable improvements over state-of-the-art\napproaches on downstream tasks.",
        "Employee turnover refers to an individual's termination of employment from\nthe current organization. It is one of the most persistent challenges for\nfirms, especially those ones in Information Technology (IT) industry that\nconfront high turnover rates. Effective prediction of potential employee\nturnovers benefits multiple stakeholders such as firms and online recruiters.\nPrior studies have focused on either the turnover prediction within a single\nfirm or the aggregated employee movement among firms. How to predict the\nindividual employees' turnovers among multiple firms has gained little\nattention in literature, and thus remains a great research challenge. In this\nstudy, we propose a novel deep learning approach based on job embeddedness\ntheory to predict the turnovers of individual employees across different firms.\nThrough extensive experimental evaluations using a real-world dataset, our\ndeveloped method demonstrates superior performance over several\nstate-of-the-art benchmark methods. Additionally, we estimate the cost saving\nfor recruiters by using our turnover prediction solution and interpret the\nattributions of various driving factors to employee's turnover to showcase its\npractical business value.",
        "Spiking neural networks (SNNs) show great potential due to their energy\nefficiency, fast processing capabilities, and robustness. There are two main\napproaches to constructing SNNs. Direct training methods require much memory,\nwhile conversion methods offer a simpler and more efficient option. However,\ncurrent conversion methods mainly focus on converting convolutional neural\nnetworks (CNNs) to SNNs. Converting Transformers to SNN is challenging because\nof the presence of non-linear modules. In this paper, we propose an Expectation\nCompensation Module to preserve the accuracy of the conversion. The core idea\nis to use information from the previous T time-steps to calculate the expected\noutput at time-step T. We also propose a Multi-Threshold Neuron and the\ncorresponding Parallel Parameter normalization to address the challenge of\nlarge time steps needed for high accuracy, aiming to reduce network latency and\npower consumption. Our experimental results demonstrate that our approach\nachieves state-of-the-art performance. For example, we achieve a top-1 accuracy\nof 88.60\\% with only a 1\\% loss in accuracy using 4 time steps while consuming\nonly 35\\% of the original power of the Transformer. To our knowledge, this is\nthe first successful Artificial Neural Network (ANN) to SNN conversion for\nSpiking Transformers that achieves high accuracy, low latency, and low power\nconsumption on complex datasets. The source codes of the proposed method are\navailable at https:\/\/github.com\/h-z-h-cell\/Transformer-to-SNN-ECMT.",
        "Artificial intelligence (AI) has significantly improved medical screening\naccuracy, particularly in cancer detection and risk assessment. However,\ntraditional classification metrics often fail to account for imbalanced data,\nvarying performance across cohorts, and patient-level inconsistencies, leading\nto biased evaluations. We propose the Cohort-Attention Evaluation Metrics (CAT)\nframework to address these challenges. CAT introduces patient-level assessment,\nentropy-based distribution weighting, and cohort-weighted sensitivity and\nspecificity. Key metrics like CATSensitivity (CATSen), CATSpecificity (CATSpe),\nand CATMean ensure balanced and fair evaluation across diverse populations.\nThis approach enhances predictive reliability, fairness, and interpretability,\nproviding a robust evaluation method for AI-driven medical screening models.",
        "We investigate the motion in space of an infinitesimal particle in the\ngravitational field generated by three primary bodies positioned at the\nvertices of a fixed equilateral triangle. We assume that the distances between\nthe primaries are small compared to their separation from the particle. By\napplying a Lie-Deprit normalization, we simplify the Hamiltonian, relegating\nboth the mean anomaly and the argument of periapisis to third-order terms or\nhigher. After reducing out the symmetries associated with the Kepler flow and\nthe central action of the angular momentum, we examine the relative equilibria\nin the first and second reduced spaces. We are able to identify the conditions\nfor the existence of circular periodic orbits and KAM tori, thus providing\ninsight into the system's long-term stability and dynamic structure.",
        "Design educators are finding ways to support students in skillfully using\nGenAI tools in their practices while encouraging the critical scrutiny of the\nethical and social issues around these technologies. However, the issue of\nenvironmental sustainability remains unaddressed. There is a lack of both\nresources to grasp the environmental costs of genAI in education and a lack of\nshared practices for engaging with the issue. This paper critically reflects on\nthe energy costs of using genAI in design education, using a workshop held in\n2023 with 49 students as a motivating example. Through this reflection, we\ndevelop a set of five alternative stances, with related actions, that support\nthe conscious use of genAI in design education. The work contributes to the\nfield of design and HCI by bringing together ways for educators to reflect on\ntheir practices, informing the future development of educational programs\naround genAI."
      ]
    }
  },
  {
    "id":2411.06785,
    "research_type":"applied",
    "start_id":"b25",
    "start_title":"White-Box Transformers via Sparse Rate Reduction",
    "start_abstract":"In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "RNA-Seq: a revolutionary tool for transcriptomics"
      ],
      "abstract":[
        "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Genomic Analysis of Date Palm Fruit Size Traits and Identification of\n  Candidate Genes through GWAS",
        "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "CoverM: Read alignment statistics for metagenomics",
        "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "Eukaryotes evade information storage-replication rate trade-off with\n  endosymbiont assistance leading to larger genomes",
        "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)",
        "Bizard: A Community-Driven Platform for Accelerating and Enhancing\n  Biomedical Data Visualization",
        "Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity\n  in Pediatric High-Grade Gliomas Single-Cell Transcriptomics",
        "Causes of evolutionary divergence in prostate cancer",
        "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns",
        "Impact of transverse strain on linear, transitional and self-similar\n  turbulent mixing layers",
        "Dynamics of the general $Q$-tensor model interacting with a rigid body",
        "Magnetic mirror stars in five dimensions",
        "Quantum Electrodynamics from Quantum Cellular Automata, and the Tension\n  Between Symmetry, Locality and Positive Energy",
        "Oddities in the Entanglement Scaling of the Quantum Six-Vertex Model",
        "The Impact of Stellar Flares on the Atmospheric Escape of Exoplanets\n  orbiting M stars I: Insights from the AU Mic System",
        "Gradient-Based Optimization of Core-Shell Particles with Discrete\n  Materials for Directional Scattering",
        "Effects of GaAs Buffer Layer on Structural, Magnetic, and Transport\n  Properties of Magnetic Topological Insulators\n  Cr$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$ and\n  V$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$ Films",
        "Unlocking ultra-deep wide-field imaging with sidereal visibility\n  averaging",
        "Enhanced Electromechanical Properties of Solution-Processed\n  K$_{0.5}$Na$_{0.5}$NbO$_{3}$ Thin Films",
        "Enhancing Efficiency of Local Projections Estimation with Volatility\n  Clustering in High-Frequency Data",
        "Nice q-analogs of orthogonal polynomials with nice moments: Some simple\n  examples",
        "Euclid Quick Data Release (Q1): First visual morphology catalogue",
        "Table-top three-dimensional photoemission orbital tomography with a\n  femtosecond extreme ultraviolet light source",
        "Predicting the spectrum and decay constants of positive-parity\n  heavy-strange mesons using domain-wall fermions"
      ],
      "abstract":[
        "The commercial value of economically significant fruits, including date palm\nfruit (dates), is influenced by various factors, such as biochemical\ncomposition and morphological features like size, shape, and visual appearance,\nwhich are key determinants of their quality and market value. Dates are\ntypically consumed at the dry stage (Tamar), during which they exhibit a wide\nrange of physical characteristics, such as color, length, weight, and skin\nappearance. Understanding the genetic basis of these traits is crucial for\nimproving crop quality and breeding new cultivars. In this study, we integrated\na genome dataset from highly diverse date cultivars with phenotypes of dry\nfruit such as length, width, area, and weight, identifying multiple significant\ngenetic loci (SNPs) associated with these traits. We also identified candidate\ngenes located near the associated SNPs that are involved in biological\nprocesses such as cell differentiation, proliferation, growth, and the\nregulation of signalling pathways for growth regulators like auxin and abscisic\nacid, as observed in other plants. Gene expression analysis reveals that many\nof these genes are highly expressed in the early stage of fruit development\nwhen the fruit attains its maximum size and weight. These findings will enhance\nour understanding of genetic determinants of fruit size particularly at the\ncommercially important Tamar stage.",
        "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces.",
        "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "Genome length varies widely among organisms, from compact genomes of\nprokaryotes to vast and complex genomes of eukaryotes. In this study, we\ntheoretically identify the evolutionary pressures that may have driven this\ndivergence in genome length. We use a parameter-free model to study genome\nlength evolution under selection pressure to minimize replication time and\nmaximize information storage capacity. We show that prokaryotes tend to reduce\ngenome length, constrained by a single replication origin, while eukaryotes\nexpand their genomes by incorporating multiple replication origins. We propose\na connection between genome length and cellular energetics, suggesting that\nendosymbiotic organelles, mitochondria and chloroplasts, evolutionarily\nregulate the number of replication origins, thereby influencing genome length\nin eukaryotes. We show that the above two selection pressures also lead to\nstrict equalization of the number of purines and their corresponding\nbase-pairing pyrimidines within a single DNA strand, known as Chagraff's second\nparity rule, a hitherto unexplained observation in genomes of nearly all known\nspecies. This arises from the symmetrization of replichore length, another\nobservation that has been shown to hold across species, which our model\nreproduces. The model also reproduces other experimentally observed phenomena,\nsuch as a general preference for deletions over insertions, and elongation and\nhigh variance of genome lengths under reduced selection pressure for\nreplication rate, termed the C-value paradox. We highlight the possibility of\nregulation of the firing of latent replication origins in response to cues from\nthe extracellular environment leading to the regulation of cell cycle rates in\nmulticellular eukaryotes.",
        "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries.",
        "Bizard is a novel visualization code repository designed to simplify data\nanalysis in biomedical research. It integrates diverse visualization codes,\nfacilitating the selection and customization of optimal visualization methods\nfor specific research needs. The platform offers a user-friendly interface with\nadvanced browsing and filtering mechanisms, comprehensive tutorials, and\ninteractive forums to enhance knowledge exchange and innovation. Bizard's\ncollaborative model encourages continuous refinement and expansion of its\nfunctionalities, making it an indispensable tool for advancing biomedical data\nvisualization and analytical methodologies. By leveraging Bizard's resources,\nresearchers can enhance data visualization skills, drive methodological\nadvancements, and improve data interpretation standards, ultimately fostering\nthe development of precision medicine and personalized therapeutic\ninterventions.Bizard can be accessed from http:\/\/genaimed.org\/Bizard\/.",
        "By use of complex network dynamics and graph-based machine learning, we\nidentified critical determinants of lineage-specific plasticity across the\nsingle-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes:\nIDHWT glioblastoma and K27M-mutant glioma. Our study identified network\ninteractions regulating glioma morphogenesis via the tumor-immune\nmicroenvironment, including neurodevelopmental programs, calcium dynamics, iron\nmetabolism, metabolic reprogramming, and feedback loops between MAPK\/ERK and\nWNT signaling. These relationships highlight the emergence of a hybrid spectrum\nof cellular states navigating a disrupted neuro-differentiation hierarchy. We\nidentified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in\nIDHWT glioblastoma, and H3F3A, ANXA6, HES6\/7, SIRT2, FXYD6, PTPRZ1, MEIS1,\nCXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2,\nFKBP variants, and FXYD7 as transition genes that influence cell fate\ndecision-making across both subsystems. Our findings suggest pHGGs are\ndevelopmentally trapped in states exhibiting maladaptive behaviors, and hybrid\ncellular identities. In effect, tumor heterogeneity (metastability) and\nplasticity emerge as stress-response patterns to immune-inflammatory\nmicroenvironments and oxidative stress. Furthermore, we show that pHGGs are\nsteered by developmental trajectories from radial glia predominantly favoring\nneocortical cell fates, in telencephalon and prefrontal cortex (PFC)\ndifferentiation. By addressing underlying patterning processes and plasticity\nnetworks as therapeutic vulnerabilities, our findings provide precision\nmedicine strategies aimed at modulating glioma cell fates and overcoming\ntherapeutic resistance. We suggest transition therapy toward neuronal-like\nlineage differentiation as a potential therapy to help stabilize pHGG\nplasticity and aggressivity.",
        "Cancer progression involves the sequential accumulation of genetic\nalterations that cumulatively shape the tumour phenotype. In prostate cancer,\ntumours can follow divergent evolutionary trajectories that lead to distinct\nsubtypes, but the causes of this divergence remain unclear. While causal\ninference could elucidate the factors involved, conventional methods are\nunsuitable due to the possibility of unobserved confounders and ambiguity in\nthe direction of causality. Here, we propose a method that circumvents these\nissues and apply it to genomic data from 829 prostate cancer patients. We\nidentify several genetic alterations that drive divergence as well as others\nthat prevent this transition, locking tumours into one trajectory. Further\nanalysis reveals that these genetic alterations may cause each other, implying\na positive-feedback loop that accelerates divergence. Our findings provide\ninsights into how cancer subtypes emerge and offer a foundation for genomic\nsurveillance strategies aimed at monitoring the progression of prostate cancer.",
        "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis.",
        "The growth of interfacial instabilities such as the Rayleigh-Taylor (RTI) and\nRichtmyer-Meshkov instability (RMI) are modified when developing in convergent\ngeometries. Whilst these modifications are usually quantified by the\ncompression rate and convergence rate of the mixing layer, an alternative\nframework is proposed, describing the evolution of the mixing layer through the\neffects of the mean strain rates experienced by the mixing layer. An\ninvestigation into the effect of the transverse strain rate on the mixing layer\ndevelopment is conducted through application of transverse strain rates in\nplanar geometry. A model for the linear regime in planar geometry with\ntransverse strain rate is derived, with equivalent solutions to convergent\ngeometry, and validated with two-dimensional simulations demonstrating the\namplification of the instability growth under transverse compression. The\neffect of the transverse strain rate on the transitional-to-turbulent mixing\nlayer is investigated with implicit large eddy simulation based on the\nmulti-mode quarter-scale $\\theta$-group case by Thornber et al. (Phys. Fluids,\nvol. 29, 2017, 105107). The mixing layer's growth exhibits the opposite trend\nto the linear regime model, with reduced growth under transverse compression.\nThe effect of shear-production under transverse compression causes the mixing\nlayer to become more mixed and the turbulent kinetic energy is increasingly\ndominated by the transverse directions, deviating from the unstrained\nself-similar state. The mixing layer width is able to be predicted by adjusting\nthe buoyancy-drag model by Youngs & Thornber (Physica D, vol. 410, 2020,\n132517) to utilise a drag length scale that scales with the transverse\nexpansion.",
        "In this article, the fluid-rigid body interaction problem of nematic liquid\ncrystals described by the general Beris-Edwards $Q$-tensor model is studied. It\nis proved first that the total energy of this problem decreases in time. The\nassociated mathematical problem is a quasilinear mixed-order system with moving\nboundary. After the transformation to a fixed domain, a monolithic approach\nbased on the added mass operator and lifting arguments is employed to establish\nthe maximal $L^p$-regularity of the linearized problem in an anisotropic ground\nspace. This paves the way for the local strong well-posedness for large data\nand global strong well-posedness for small data of the interaction problem.",
        "We discuss a class of solutions of multidimensional gravity which are\nformally related to black-hole solutions but can observationally look like\ncompact stars whose surface reflects back all particles or signals getting\nthere. Some particular examples of such solutions are presented and studied,\nincluding those with a magnetic field in Maxwell or nonlinear electrodynamics\n(NED) in five dimensions. For NED as a possible source for magnetic mirror\nstars, we formulate a methodology of solving the 5D Einstein-NED equations and\npoint out the conditions under which there always exist mirror star solutions.\nWe also note that some of the Einstein-Maxwell solutions under consideration\nare discussed in the literature and called ``topological stars'' due to the\ncircular topology of the fifth dimension.",
        "We show that free QED is equivalent to the continuous-space-and-time limit of\nFermi and Bose lattice quantum cellular automata theories derived from quantum\nrandom walks satisfying simple symmetry and unitarity conditions. In doing so\nwe define the Fermi and Bose theories in a unified manner using the usual\nfermion internal space but a boson internal space that is six-dimensional. We\nshow that the reduction to a two-dimensional boson internal space (two helicity\nstates arising from spin-1 plus the photon transversality condition) comes from\nrestricting the quantum cellular automaton theory to positive energies. We\nbriefly examine common symmetries of quantum cellular automata, and how\ntime-reversal symmetry demands the existence of negative-energy solutions.\nThese solutions produce a tension in coupling the Fermi and Bose theories, in\nwhich the strong locality of quantum cellular automata seems to require a\nnonzero amplitude to produce negative-energy states, leading to an unphysical\ncascade of negative-energy particles. However, we show in a 1D model that by\nextending interactions over a larger (but finite) range it is possible to\nexponentially suppress the production of negative-energy particles to the point\nwhere they can be neglected.",
        "We investigate the entanglement properties of the Quantum Six-Vertex Model on\na cylinder, focusing on the Shannon-Renyi entropy in the limit of Renyi order\n$n = \\infty$. This entropy, calculated from the ground state amplitudes of the\nequivalent XXZ spin-1\/2 chain, allows us to determine the Renyi entanglement\nentropy of the corresponding Rokhsar-Kivelson wavefunctions, which describe the\nground states of certain conformal quantum critical points. Our analysis\nreveals a novel logarithmic correction to the expected entanglement scaling\nwhen the system size is odd. This anomaly arises from the geometric frustration\nof spin configurations imposed by periodic boundary conditions on odd-sized\nchains. We demonstrate that the scaling prefactor of this logarithmic term is\ndirectly related to the compactification radius of the low-energy bosonic field\ntheory description, or equivalently, the Luttinger parameter. Thus, this\ncorrection provides a direct probe of the underlying Conformal Field Theory\n(CFT) describing the critical point. Our findings highlight the crucial role of\nsystem size parity in determining the entanglement properties of this model and\noffer insights into the interplay between geometry, frustration, and\ncriticality.",
        "The X-rays and Extreme Ultraviolet (XUV) emission from M stars can drive the\natmospheric escape on planets orbiting them. M stars are also known for their\nfrequent emission of stellar flares, which will increase the high-energy flux\nreceived by their orbiting planets. To understand how stellar flares impact the\nprimordial atmospheres of planets orbiting young M stars, we use UV\nspectroscopic data of flares from the Habitable Zones and M dwarf Activity\nacross Time (HAZMAT) and Measurements of the Ultraviolet Spectral\nCharacteristics of Low-mass Exoplanetary Systems (MUSCLES) programs as a proxy\nto the XUV flare emission. Using the software package VPLanet, we simulate the\nyoung AU Mic planetary system composed of two Neptune-sized and one Earth-sized\nplanet orbiting a 23-Myr-old M1 star. Our findings show that the Earth-sized\nplanet AU Mic d should be in the process of losing completely its atmosphere in\nthe next couple million years, solely due to the quiescent emission, with\nflares not significantly contributing to its atmospheric escape due to the\nsmall size of AU mic d and its close-in distance from the star. However, our\nresults indicate that flares would play a crucial role for such planets further\naway, in the habitable zone (i.e. 0.2935 AU) of AU Mic-like stars during the\npost-saturation phase, accelerating the total atmospheric loss process by a few\nbillion years. For planets between 0.365 AU and the HZ outer edge, the\nadditional XUV from flares is necessary to deplete primordial atmospheres fully\nsince the quiescent emission alone is insufficient.",
        "Designing nanophotonic structures traditionally grapples with the\ncomplexities of discrete parameters, such as real materials, often resorting to\ncostly global optimization methods. This paper introduces an approach that\nleverages generative deep learning to map discrete parameter sets into a\ncontinuous latent space, enabling direct gradient-based optimization. For\nscenarios with non-differentiable physics evaluation functions, a neural\nnetwork is employed as a differentiable surrogate model. The efficacy of this\nmethodology is demonstrated by optimizing the directional scattering properties\nof core-shell nanoparticles composed of a selection of realistic materials. We\nderive suggestions for core-shell geometries with strong forward scattering and\nminimized backscattering. Our findings reveal significant improvements in\ncomputational efficiency and performance when compared to global optimization\ntechniques. Beyond nanophotonics design problems, this framework holds promise\nfor broad applications across all types of inverse problems constrained by\ndiscrete variables.",
        "Here, we study the effects of a GaAs buffer layer on the structural,\nmagnetic, and transport properties of Cr$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$\nmagnetic topological insulator thin films and compare them with those of\nV$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$, which we recently reported. Similar to\nthe case of V$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$, growth on a GaAs buffer\nlayer leads to some distinctly different properties than direct growth on InP\nsubstrates. These include improved interface quality confirmed by transmission\nelectron microscopy, enhanced magnetic coercive fields, and smaller resistivity\npeaks at the magnetization reversals. Furthermore, the Bi-ratio dependence of\nthe carrier density reveals that the interface property also affects the Fermi\nlevel. These results demonstrate the importance of the buffer layer in\ncontrolling the electronic properties of the magnetic topological insulator\nfilms.",
        "Producing ultra-deep high-angular-resolution images with current and\nnext-generation radio interferometers introduces significant computational\nchallenges. In particular, the imaging is so demanding that processing large\ndatasets, accumulated over hundreds of hours on the same pointing, is likely\ninfeasible in the current data reduction schemes. In this paper, we revisit a\nsolution to this problem that was considered in the past but is not being used\nin modern software: sidereal visibility averaging (SVA). This technique\ncombines individual observations taken at different sidereal days into one much\nsmaller dataset by averaging visibilities at similar baseline coordinates. We\npresent our method and validated it using four separate 8-hour observations of\nthe ELAIS-N1 deep field, taken with the International LOw Frequency ARray\n(LOFAR) Telescope (ILT) at 140~MHz. Additionally, we assessed the accuracy\nconstraints imposed by Earth's orbital motion relative to the observed pointing\nwhen combining multiple datasets. We find, with four observations, data volume\nreductions of a factor of 1.8 and computational time improvements of a factor\nof 1.6 compared to standard imaging. These factors will increase when more\nobservations are combined with SVA. For instance, with 3000~hours of LOFAR data\naimed at achieving sensitivities of the order of {\\mu}Jy\/beam at sub-arcsecond\nresolutions, we estimate data volume reductions of up to a factor of 169 and a\n14-fold decrease in computing time using our current algorithm. This\nadvancement for imaging large deep interferometric datasets will benefit\ncurrent generation instruments, such as LOFAR, and upcoming instruments such as\nthe Square Kilometre Array (SKA), provided the calibrated visibility data of\nthe individual observations are retained.",
        "K$_{0.5}$Na$_{0.5}$NbO$_{3}$ is among the most promising lead-free\npiezoelectrics. While its sputtered films match the performance of the champion\npiezoelectric Pb(Zr,Ti)O$_{3}$, processing of high-quality, reproducible, and\ntime-stable solution-processed K$_{0.5}$Na$_{0.5}$NbO$_{3}$ films remains\nchallenging. Here, we report 1 $\\mu$m-thick Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films prepared through a chemical solution\ndeposition process, which have perfectly dense microstructure and uniform\ncomposition across their thickness. The films exhibit a high transverse\npiezoelectric coefficient (e$_{31,f}$ = -14.8 C\/m$^{2}$), high dielectric\npermittivity (${\\epsilon}_{r}$ = 920), low dielectric losses (tan${\\delta}$ =\n0.05) and can withstand electric fields up to at least 1 MV\/cm. The functional\nproperties show excellent stability over time, and the synthesis process is\nreproducible. The results demonstrate the high potential of Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films to become a replacement for lead-based\nPb(Zr,Ti)O$_{3}$ films in piezoelectric applications.",
        "This paper advances the local projections (LP) method by addressing its\ninefficiency in high-frequency economic and financial data with volatility\nclustering. We incorporate a generalized autoregressive conditional\nheteroskedasticity (GARCH) process to resolve serial correlation issues and\nextend the model with GARCH-X and GARCH-HAR structures. Monte Carlo simulations\nshow that exploiting serial dependence in LP error structures improves\nefficiency across forecast horizons, remains robust to persistent volatility,\nand yields greater gains as sample size increases. Our findings contribute to\nrefining LP estimation, enhancing its applicability in analyzing economic\ninterventions and financial market dynamics.",
        "In this note I collect some typical examples of orthogonal polynomials with\nsimple moments where both moments and orthogonal polynomials have nice\nq-analogs.",
        "We present a detailed visual morphology catalogue for Euclid's Quick Release\n1 (Q1). Our catalogue includes galaxy features such as bars, spiral arms, and\nongoing mergers, for the 378000 bright ($I_E < 20.5$) or extended (area $\\geq\n700\\,$pixels) galaxies in Q1. The catalogue was created by finetuning the\nZoobot galaxy foundation models on annotations from an intensive one month\ncampaign by Galaxy Zoo volunteers. Our measurements are fully automated and\nhence fully scaleable. This catalogue is the first 0.4% of the approximately\n100 million galaxies where Euclid will ultimately resolve detailed morphology.",
        "Following electronic processes in molecules and materials at the level of the\nquantum mechanical electron wavefunction with angstrom-level spatial resolution\nand with full access to its femtosecond temporal dynamics is at the heart of\nultrafast condensed matter physics. A breakthrough invention allowing\nexperimental access to electron wavefunctions was the reconstruction of\nmolecular orbitals from angle-resolved photoelectron spectroscopy data in 2009,\ntermed photoemission orbital tomography (POT). This invention puts ultrafast\nthree-dimensional (3D) POT in reach, with many new prospects for the study of\nultrafast light-matter interaction, femtochemistry and photo-induced phase\ntransitions. Here, we develop a synergistic experimental-algorithmic approach\nto realize the first 3D-POT experiment using a short-pulse extreme ultraviolet\nlight source. We combine a new variant of photoelectron spectroscopy, namely\nultrafast momentum microscopy, with a table-top spectrally-tunable\nhigh-harmonic generation light source and a tailored algorithm for efficient 3D\nreconstruction from sparse, undersampled data. This combination dramatically\nspeeds up the experimental data acquisition, while at the same time reducing\nthe sampling requirements to achieve complete 3D information. We demonstrate\nthe power of this approach by full 3D imaging of the frontier orbitals of a\nprototypical organic semiconductor absorbed on pristine Ag(110).",
        "We present a lattice-QCD calculation of the masses and decay constants of the\npositive-parity heavy-strange mesons $D^*_{s0}$, $D_{s1}$, $B^*_{s0}$, and\n$B_{s1}$. The calculations are performed with domain-wall fermions for the\nlight and strange quarks and an anisotropic clover action for the charm and\nbottom quarks. We use seven different RBC\/UKQCD ensembles with pion masses\nranging from a near-physical 139 MeV up to 431 MeV. We consider two different\nanalysis types, with or without two-meson operators at the source. We observe\nthe expected below-threshold ground states. The fits without the two-meson\noperators appear to be more stable, but may overestimate the ground-state\nenergies, while preliminary fits with two-meson operators at the source only\nappear to underestimate the ground-state energies."
      ]
    }
  },
  {
    "id":2412.00868,
    "research_type":"applied",
    "start_id":"b22",
    "start_title":"Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models",
    "start_abstract":"The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm.",
    "start_categories":[
      "stat.ME"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "How resilient are language models to text perturbations"
      ],
      "abstract":[
        "Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Explainable Distributed Constraint Optimization Problems",
        "CSSDM Ontology to Enable Continuity of Care Data Interoperability",
        "Generative AI in Transportation Planning: A Survey",
        "Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation",
        "Hierarchical Expert Prompt for Large-Language-Model: An Approach Defeat\n  Elite AI in TextStarCraft II for the First Time",
        "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented\n  Reinforcement in Embodied Systems",
        "The Goofus & Gallant Story Corpus for Practical Value Alignment",
        "Online inductive learning from answer sets for efficient reinforcement\n  learning exploration",
        "Demonstrating specification gaming in reasoning models",
        "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical\n  Deployment",
        "LLM-driven Effective Knowledge Tracing by Integrating Dual-channel\n  Difficulty",
        "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based\n  Automatic Heuristic Design",
        "REALM-Bench: A Real-World Planning Benchmark for LLMs and Multi-Agent\n  Systems",
        "Apparent violations of the second law in the quantum-classical dynamics\n  of interacting levitated nanoparticles",
        "Does dark matter fall in the same way as standard model particles? A\n  direct constraint of Euler's equation with cosmological data",
        "A countable Boolean algebra that is Reichenbach's common cause complete",
        "Exploring Sentiment Manipulation by LLM-Enabled Intelligent Trading\n  Agents",
        "Effects of galactic environment on size and dark matter content in\n  low-mass galaxies",
        "Lattice-Based Pruning in Recurrent Neural Networks via Poset Modeling",
        "Lipschitz Decompositions of Finite $\\ell_{p}$ Metrics",
        "DIN-CTS: Low-Complexity Depthwise-Inception Neural Network with\n  Contrastive Training Strategy for Deepfake Speech Detection",
        "Recent open heavy flavor studies for the Electron-Ion Collider",
        "An Investigation of FP8 Across Accelerators for LLM Inference",
        "ASTRAL: Automated Safety Testing of Large Language Models",
        "Scalable and Site-Specific Frequency Tuning of Two-Level System Defects\n  in Superconducting Qubit Arrays",
        "Fabrication of Soft and Comfortable Pressure-Sensing Shoe Sole for\n  Intuitive Monitoring of Human Quality Gaits",
        "Cognitive Performance Measurements and the Impact of Sleep Quality Using\n  Wearable and Mobile Sensors",
        "High-throughput Discovery of Anti-gap Semiconductors"
      ],
      "abstract":[
        "The Distributed Constraint Optimization Problem (DCOP) formulation is a\npowerful tool to model cooperative multi-agent problems that need to be solved\ndistributively. A core assumption of existing approaches is that DCOP solutions\ncan be easily understood, accepted, and adopted, which may not hold, as\nevidenced by the large body of literature on Explainable AI. In this paper, we\npropose the Explainable DCOP (X-DCOP) model, which extends a DCOP to include\nits solution and a contrastive query for that solution. We formally define some\nkey properties that contrastive explanations must satisfy for them to be\nconsidered as valid solutions to X-DCOPs as well as theoretical results on the\nexistence of such valid explanations. To solve X-DCOPs, we propose a\ndistributed framework as well as several optimizations and suboptimal variants\nto find valid explanations. We also include a human user study that showed that\nusers, not surprisingly, prefer shorter explanations over longer ones. Our\nempirical evaluations showed that our approach can scale to large problems, and\nthe different variants provide different options for trading off explanation\nlengths for smaller runtimes. Thus, our model and algorithmic contributions\nextend the state of the art by reducing the barrier for users to understand\nDCOP solutions, facilitating their adoption in more real-world applications.",
        "The rapid advancement of digital technologies and recent global pandemic\nscenarios have led to a growing focus on how these technologies can enhance\nhealthcare service delivery and workflow to address crises. Action plans that\nconsolidate existing digital transformation programs are being reviewed to\nestablish core infrastructure and foundations for sustainable healthcare\nsolutions. Reforming health and social care to personalize home care, for\nexample, can help avoid treatment in overcrowded acute hospital settings and\nimprove the experiences and outcomes for both healthcare professionals and\nservice users. In this information-intensive domain, addressing the\ninteroperability challenge through standards-based roadmaps is crucial for\nenabling effective connections between health and social care services. This\napproach facilitates safe and trustworthy data workflows between different\nhealthcare system providers. In this paper, we present a methodology for\nextracting, transforming, and loading data through a semi-automated process\nusing a Common Semantic Standardized Data Model (CSSDM) to create personalized\nhealthcare knowledge graph (KG). The CSSDM is grounded in the formal ontology\nof ISO 13940 ContSys and incorporates FHIR-based specifications to support\nstructural attributes for generating KGs. We propose that the CSSDM facilitates\ndata harmonization and linking, offering an alternative approach to\ninteroperability. This approach promotes a novel form of collaboration between\ncompanies developing health information systems and cloud-enabled health\nservices. Consequently, it provides multiple stakeholders with access to\nhigh-quality data and information sharing.",
        "The integration of generative artificial intelligence (GenAI) into\ntransportation planning has the potential to revolutionize tasks such as demand\nforecasting, infrastructure design, policy evaluation, and traffic simulation.\nHowever, there is a critical need for a systematic framework to guide the\nadoption of GenAI in this interdisciplinary domain. In this survey, we, a\nmultidisciplinary team of researchers spanning computer science and\ntransportation engineering, present the first comprehensive framework for\nleveraging GenAI in transportation planning. Specifically, we introduce a new\ntaxonomy that categorizes existing applications and methodologies into two\nperspectives: transportation planning tasks and computational techniques. From\nthe transportation planning perspective, we examine the role of GenAI in\nautomating descriptive, predictive, generative, simulation, and explainable\ntasks to enhance mobility systems. From the computational perspective, we\ndetail advancements in data preparation, domain-specific fine-tuning, and\ninference strategies, such as retrieval-augmented generation and zero-shot\nlearning tailored to transportation applications. Additionally, we address\ncritical challenges, including data scarcity, explainability, bias mitigation,\nand the development of domain-specific evaluation frameworks that align with\ntransportation goals like sustainability, equity, and system efficiency. This\nsurvey aims to bridge the gap between traditional transportation planning\nmethodologies and modern AI techniques, fostering collaboration and innovation.\nBy addressing these challenges and opportunities, we seek to inspire future\nresearch that ensures ethical, equitable, and impactful use of generative AI in\ntransportation planning.",
        "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs.",
        "Since the emergence of the Large Language Model (LLM), LLM has been widely\nused in fields such as writing, translating, and searching. However, there is\nstill great potential for LLM-based methods in handling complex tasks such as\ndecision-making in the StarCraft II environment. To address problems such as\nlack of relevant knowledge and poor control over subtasks of varying\nimportance, we propose a Hierarchical Expert Prompt (HEP) for LLM. Our method\nimproves the understanding of game situations through expert-level tactical\nknowledge, improving the processing quality of tasks of varying importance\nthrough a hierarchical framework. Our approach defeated the highest level\n(Elite) standard built-in agent in TextStarCraft II for the first time and\nconsistently outperformed the baseline method in other difficulties. Our\nexperiments suggest that the proposed method is a practical solution for\ntackling complex decision-making challenges. The replay video can be viewed on\nhttps:\/\/www.bilibili.com\/video\/BV1uz42187EF and https:\/\/youtu.be\/dO3PshWLV5M,\nand our codes have been open-sourced on\nhttps:\/\/github.com\/luchang1113\/HEP-LLM-play-StarCraftII.",
        "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience.",
        "Values or principles are key elements of human society that influence people\nto behave and function according to an accepted standard set of social rules to\nmaintain social order. As AI systems are becoming ubiquitous in human society,\nit is a major concern that they could violate these norms or values and\npotentially cause harm. Thus, to prevent intentional or unintentional harm, AI\nsystems are expected to take actions that align with these principles. Training\nsystems to exhibit this type of behavior is difficult and often requires a\nspecialized dataset. This work presents a multi-modal dataset illustrating\nnormative and non-normative behavior in real-life situations described through\nnatural language and artistic images. This training set contains curated sets\nof images that are designed to teach young children about social principles. We\nargue that this is an ideal dataset to use for training socially normative\nagents given this fact.",
        "This paper presents a novel approach combining inductive logic programming\nwith reinforcement learning to improve training performance and explainability.\nWe exploit inductive learning of answer set programs from noisy examples to\nlearn a set of logical rules representing an explainable approximation of the\nagent policy at each batch of experience. We then perform answer set reasoning\non the learned rules to guide the exploration of the learning agent at the next\nbatch, without requiring inefficient reward shaping and preserving optimality\nwith soft bias. The entire procedure is conducted during the online execution\nof the reinforcement learning algorithm. We preliminarily validate the efficacy\nof our approach by integrating it into the Q-learning algorithm for the Pac-Man\nscenario in two maps of increasing complexity. Our methodology produces a\nsignificant boost in the discounted return achieved by the agent, even in the\nfirst batches of training. Moreover, inductive learning does not compromise the\ncomputational time required by Q-learning and learned rules quickly converge to\nan explanation of the agent policy.",
        "We demonstrate LLM agent specification gaming by instructing models to win\nagainst a chess engine. We find reasoning models like o1 preview and\nDeepSeek-R1 will often hack the benchmark by default, while language models\nlike GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work\nto hack.\n  We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024;\nWeij et al., 2024) by using realistic task prompts and avoiding excess nudging.\nOur results suggest reasoning models may resort to hacking to solve difficult\nproblems, as observed in OpenAI (2024)'s o1 Docker escape during cyber\ncapabilities testing.",
        "We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale. V-Droid\nsets a new state-of-the-art task success rate across several public mobile task\nautomation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on\nMobileAgentBench, surpassing existing agents by 9.5%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves an impressively low latency of 0.7\nseconds per step, making it the first mobile agent capable of delivering\nnear-real-time, effective decision-making capabilities.",
        "Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring\nsystems used to simulate changes in students' knowledge state during learning,\ntrack personalized knowledge mastery, and predict performance. However, current\nKT models face three major challenges: (1) When encountering new questions,\nmodels face cold-start problems due to sparse interaction records, making\nprecise modeling difficult; (2) Traditional models only use historical\ninteraction records for student personalization modeling, unable to accurately\ntrack individual mastery levels, resulting in unclear personalized modeling;\n(3) The decision-making process is opaque to educators, making it challenging\nfor them to understand model judgments. To address these challenges, we propose\na novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that\nutilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG)\nfor subjective difficulty assessment, while integrating difficulty bias-aware\nalgorithms and student mastery algorithms for precise difficulty measurement.\nOur framework introduces three key innovations: (1) Difficulty Balance\nPerception Sequence (DBPS) - students' subjective perceptions combined with\nobjective difficulty, measuring gaps between LLM-assessed difficulty,\nmathematical-statistical difficulty, and students' subjective perceived\ndifficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) -\nprecise modeling of student mastery levels through different difficulty zones;\n(3) Knowledge State Update Mechanism - implementing personalized knowledge\nacquisition through gated networks and updating student knowledge state.\nExperimental results on two real datasets show our method consistently\noutperforms nine baseline models, improving AUC metrics by 2% to 10% while\neffectively addressing cold-start problems and enhancing model\ninterpretability.",
        "Handcrafting heuristics for solving complex optimization tasks (e.g., route\nplanning and task allocation) is a common practice but requires extensive\ndomain knowledge. Recently, Large Language Model (LLM)-based automatic\nheuristic design (AHD) methods have shown promise in generating high-quality\nheuristics without manual interventions. Existing LLM-based AHD methods employ\na population to maintain a fixed number of top-performing LLM-generated\nheuristics and introduce evolutionary computation (EC) to iteratively enhance\nthe population. However, these population-based procedures cannot fully develop\nthe potential of each heuristic and are prone to converge into local optima. To\nmore comprehensively explore the space of heuristics, this paper proposes to\nuse Monte Carlo Tree Search (MCTS) for LLM-based heuristic evolution. The\nproposed MCTS-AHD method organizes all LLM-generated heuristics in a tree\nstructure and can better develop the potential of temporarily underperforming\nheuristics. In experiments, MCTS-AHD delivers significantly higher-quality\nheuristics on various complex tasks. Our code is available.",
        "This benchmark suite provides a comprehensive evaluation framework for\nassessing both individual LLMs and multi-agent systems in real-world planning\nscenarios. The suite encompasses eleven designed problems that progress from\nbasic to highly complex, incorporating key aspects such as multi-agent\ncoordination, inter-agent dependencies, and dynamic environmental disruptions.\nEach problem can be scaled along three dimensions: the number of parallel\nplanning threads, the complexity of inter-dependencies, and the frequency of\nunexpected disruptions requiring real-time adaptation. The benchmark includes\ndetailed specifications, evaluation metrics, and baseline implementations using\ncontemporary frameworks like LangGraph, enabling rigorous testing of both\nsingle-agent and multi-agent planning capabilities. Through standardized\nevaluation criteria and scalable complexity, this benchmark aims to drive\nprogress in developing more robust and adaptable AI planning systems for\nreal-world applications.",
        "Random exchanges of energy arise naturally in stochastic systems. As a\nconsequence, apparent violations of the second law of thermodynamics can occur,\nas it holds true on average. Here we investigate the occurrence of these\napparent violations -- termed free lunches -- in a quantum-classical system\ncomprised of levitated nanoparticles exchanging energy via the Coulomb\ninteraction. We consider different initial states for the quantum system, and\nhow these exert work and fluctuations upon the classical particle affecting the\nprobability of free lunches. With that, we initiate the study of hybrid\nquantum-classical systems through the lens of stochastic thermodynamics.",
        "Since dark matter particles have never been directly detected, we do not know\nhow they move, and in particular we do not know how they fall inside\ngravitational potential wells. Usually it is assumed that dark matter only\ninteracts gravitationally with itself and with particles of the standard model,\nand therefore that its motion is governed by Euler's equation. In this paper,\nwe test this assumption for the first time at cosmological scales, by combining\nmeasurements of galaxy velocities with measurements of gravitational potential\nwells, encoded in the Weyl potential. We find that current data are consistent\nwith Euler's equation at redshifts $z\\in [0.3,0.8]$, and we place constraints\non the strength of a potential fifth force, which would alter the way dark\nmatter particles fall. We find that a positive fifth force cannot exceed 7% of\nthe gravitational interaction strength, while a negative fifth force is limited\nto 21%. The coming generation of surveys, including the Legacy Survey of Space\nand Time (LSST) of the Vera C. Rubin Observatory and the Dark Energy\nSpectroscopic Instrument (DESI) will drastically improve the constraints,\nallowing to constrain a departure from pure gravitational interaction at the\nlevel of 2%.",
        "The common cause completeness (CCC) is a philosophical principle that asserts\nthat if we consider two positively correlated events then it evokes a common\ncause. The principle is due to H. Reichenbach and has been largely studied in\nBoolean algebras and elsewhere.The results published so far bring about a\nquestion whether there is a small (countable) Boolean algebra with CCC. In this\nnote we construct such an example.",
        "Companies across all economic sectors continue to deploy large language\nmodels at a rapid pace. Reinforcement learning is experiencing a resurgence of\ninterest due to its association with the fine-tuning of language models from\nhuman feedback. Tool-chain language models control task-specific agents; if the\nconverse has not already appeared, it soon will. In this paper, we present what\nwe believe is the first investigation of an intelligent trading agent based on\ncontinuous deep reinforcement learning that also controls a large language\nmodel with which it can post to a social media feed observed by other traders.\nWe empirically investigate the performance and impact of such an agent in a\nsimulated financial market, finding that it learns to optimize its total\nreward, and thereby augment its profit, by manipulating the sentiment of the\nposts it produces. The paper concludes with discussion, limitations, and\nsuggestions for future work.",
        "We utilize the cosmological volume simulation, FIREbox, to investigate how a\ngalaxy's environment influences its size and dark matter content. Our study\nfocuses on approximately 1,200 galaxies (886 central and 332 satellite halos)\nin the low-mass regime, with stellar masses between $10^6$ to $10^9$\n$M_{\\odot}$. We analyze the size-mass relation ($r_{50} - M_{\\star}$), inner\ndark matter mass-stellar mass ($M^{50}_{\\rm DM} - M_{\\star}$) relation, and the\nhalo mass-stellar mass ($M_{\\rm halo} - M_{\\star}$) relation. At fixed stellar\nmass, we find the galaxies experiencing stronger tidal influences, indicated by\nhigher Perturbation Indices (PI $>$ 1) are generally larger and have lower\nmasses relative to their counterparts with lower Perturbation Indices (PI $<$\n1). Applying a Random Forest regression model, we show that both the\nenvironment (PI) and halo mass ($M_{rm halo}$) are significant predictors of a\ngalaxy's relative size and dark matter content. Notably, because $M_{\\rm halo}$\nis also strongly affected by the environment, our findings indicate that\nenvironmental conditions not only influence galactic sizes and relative inner\ndark matter content directly, but also indirectly through their impact on halo\nmass. Our results highlight a critical interplay between environmental factors\nand halo mass in shaping galaxy properties, affirming the environment as a\nfundamental driver in galaxy formation and evolution.",
        "Recurrent neural networks (RNNs) are central to sequence modeling tasks, yet\ntheir high computational complexity poses challenges for scalability and\nreal-time deployment. Traditional pruning techniques, predominantly based on\nweight magnitudes, often overlook the intrinsic structural properties of these\nnetworks. We introduce a novel framework that models RNNs as partially ordered\nsets (posets) and constructs corresponding dependency lattices. By identifying\nmeet irreducible neurons, our lattice-based pruning algorithm selectively\nretains critical connections while eliminating redundant ones. The method is\nimplemented using both binary and continuous-valued adjacency matrices to\ncapture different aspects of network connectivity. Evaluated on the MNIST\ndataset, our approach exhibits a clear trade-off between sparsity and\nclassification accuracy. Moderate pruning maintains accuracy above 98%, while\naggressive pruning achieves higher sparsity with only a modest performance\ndecline. Unlike conventional magnitude-based pruning, our method leverages the\nstructural organization of RNNs, resulting in more effective preservation of\nfunctional connectivity and improved efficiency in multilayer networks with\ntop-down feedback. The proposed lattice-based pruning framework offers a\nrigorous and scalable approach for reducing RNN complexity while sustaining\nrobust performance, paving the way for more efficient hierarchical models in\nboth machine learning and computational neuroscience.",
        "Lipschitz decomposition is a useful tool in the design of efficient\nalgorithms involving metric spaces. While many bounds are known for different\nfamilies of finite metrics, the optimal parameters for $n$-point subsets of\n$\\ell_p$, for $p > 2$, remained open, see e.g. [Naor, SODA 2017]. We make\nsignificant progress on this question and establish the bound\n$\\beta=O(\\log^{1-1\/p} n)$. Building on prior work, we demonstrate applications\nof this result to two problems, high-dimensional geometric spanners and\ndistance labeling schemes. In addition, we sharpen a related decomposition\nbound for $1<p<2$, due to Filtser and Neiman [Algorithmica 2022].",
        "In this paper, we propose a deep neural network approach for deepfake speech\ndetection (DSD) based on a lowcomplexity Depthwise-Inception Network (DIN)\ntrained with a contrastive training strategy (CTS). In this framework, input\naudio recordings are first transformed into spectrograms using Short-Time\nFourier Transform (STFT) and Linear Filter (LF), which are then used to train\nthe DIN. Once trained, the DIN processes bonafide utterances to extract audio\nembeddings, which are used to construct a Gaussian distribution representing\ngenuine speech. Deepfake detection is then performed by computing the distance\nbetween a test utterance and this distribution to determine whether the\nutterance is fake or bonafide. To evaluate our proposed systems, we conducted\nextensive experiments on the benchmark dataset of ASVspoof 2019 LA. The\nexperimental results demonstrate the effectiveness of combining the\nDepthwise-Inception Network with the contrastive learning strategy in\ndistinguishing between fake and bonafide utterances. We achieved Equal Error\nRate (EER), Accuracy (Acc.), F1, AUC scores of 4.6%, 95.4%, 97.3%, and 98.9%\nrespectively using a single, low-complexity DIN with just 1.77 M parameters and\n985 M FLOPS on short audio segments (4 seconds). Furthermore, our proposed\nsystem outperforms the single-system submissions in the ASVspoof 2019 LA\nchallenge, showcasing its potential for real-time applications.",
        "The future Electron-Ion Collider (EIC) will operate a series of\nhigh-luminosity high-energy electron+proton ($e+p$) and electron+nucleus\n($\\textit{e + A}$) collisions to study several fundamental questions in the\nhigh energy and nuclear physics field. Heavy flavor hadron and jet production\nat the EIC plays an important role in exploring both potential modification on\nthe initial-state nuclear parton distribution functions (nPDFs) and final-state\nparton propagation and hadronization processes under different nuclear medium\nconditions. The current design of the EIC ePIC detector has good performance of\nvertex and track reconstruction, particle identification and energy\ndetermination in the pseudorapidity region of $-3.5<\\eta<3.5$, which will\nenable a series of high precision heavy flavor hadron and jet measurements.\nLatest simulation studies of the projected nuclear modification factor $R_{eA}$\nof heavy flavor jets and heavy flavor hadron inside jets in $e+p$ and\n$\\textit{e + Au}$ collisions at $\\sqrt{s} =$ 28.6 GeV and 63.2 GeV as well as\nthe projected statistical accuracy of inclusive and differential charm baryon\nover meson ratio measurements in $e+p$ collisions will be presented. The\nimpacts of these proposed EIC measurements on constraining the heavy quark\npropagation properties in cold nuclear medium and exploring the heavy quark\nhadronization process will be discussed.",
        "The introduction of 8-bit floating-point (FP8) computation units in modern AI\naccelerators has generated significant interest in FP8-based large language\nmodel (LLM) inference. Unlike 16-bit floating-point formats, FP8 in deep\nlearning requires a shared scaling factor. Additionally, while E4M3 and E5M2\nare well-defined at the individual value level, their scaling and accumulation\nmethods remain unspecified and vary across hardware and software\nimplementations. As a result, FP8 behaves more like a quantization format than\na standard numeric representation. In this work, we provide the first\ncomprehensive analysis of FP8 computation and acceleration on two AI\naccelerators: the NVIDIA H100 and Intel Gaudi 2. Our findings highlight that\nthe Gaudi 2, by leveraging FP8, achieves higher throughput-to-power efficiency\nduring LLM inference, offering valuable insights into the practical\nimplications of FP8 adoption for datacenter-scale LLM serving.",
        "Large Language Models (LLMs) have recently gained attention due to their\nability to understand and generate sophisticated human-like content. However,\nensuring their safety is paramount as they might provide harmful and unsafe\nresponses. Existing LLM testing frameworks address various safety-related\nconcerns (e.g., drugs, terrorism, animal abuse) but often face challenges due\nto unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool\nthat automates the generation and execution of test cases (i.e., prompts) for\ntesting the safety of LLMs. First, we introduce a novel black-box coverage\ncriterion to generate balanced and diverse unsafe test inputs across a diverse\nset of safety categories as well as linguistic writing characteristics (i.e.,\ndifferent style and persuasive writing techniques). Second, we propose an\nLLM-based approach that leverages Retrieval Augmented Generation (RAG),\nfew-shot prompting strategies and web browsing to generate up-to-date test\ninputs. Lastly, similar to current LLM test automation techniques, we leverage\nLLMs as test oracles to distinguish between safe and unsafe test outputs,\nallowing a fully automated testing approach. We conduct an extensive evaluation\non well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms\nother LLMs when acting as the test oracle, accurately detecting unsafe\nresponses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs\nthat are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard);\nii) the results confirm that our approach can uncover nearly twice as many\nunsafe LLM behaviors with the same number of test inputs compared to currently\nused static datasets; and iii) our black-box coverage criterion combined with\nweb browsing can effectively guide the LLM on generating up-to-date unsafe test\ninputs, significantly increasing the number of unsafe LLM behaviors.",
        "State-of-the-art superconducting quantum processors containing tens to\nhundreds of qubits have demonstrated the building blocks for realizing\nfault-tolerant quantum computation. Nonetheless, a fundamental barrier to\nscaling further is the prevalence of fluctuating quantum two-level system (TLS)\ndefects that can couple resonantly to qubits, causing excess decoherence and\nenhanced gate errors. Here we introduce a scalable architecture for\nsite-specific and in-situ manipulation of TLS frequencies out of the spectral\nvicinity of our qubits. Our method is resource efficient, combining TLS\nfrequency tuning and universal single qubit control into a single on-chip\ncontrol line per qubit. We independently control each qubit's dissipative\nenvironment to dynamically improve both qubit coherence times and single qubit\ngate fidelities -- with a constant time overhead that does not scale with the\ndevice size. Over a period of 40 hours across 6 qubits, we demonstrate a $36\\%$\nimprovement in average single qubit error rates and a $17\\%$ improvement in\naverage energy relaxation times. Critically, we realize a 4-fold suppression in\nthe occurrence of TLS-induced performance outliers, and a complete reduction of\nsimultaneous outlier events. These results mark a significant step toward\novercoming the challenges that TLS defects pose to scaling superconducting\nquantum processors.",
        "The study discusses the design and fabrication of flexible pressure sensors\nusing Ecoflex\/Graphene composites. The fabricated sensor is used for the\napplication of intuitive monitoring of human quality gaits and implementation\nof the soft and comfortable shoe sole for rehabilitation of the patients with\nfoot disorder is also taken into consideration. The sensor is fabricated using\nmolding and casting technique by sandwiching the thin film Ecoflex\/Graphene\ncomposites between the copper (Cu) electrodes with the dimension of 15 x 15 mm2\nwith high sensitivity. There are five pressure sensors integrated in the shoe\nsole, a sensor at the forefoot, three sensors at the midfoot and one sensor at\nthe lower foot (heel). The behavior of the sensor is negative piezoresistive in\nwhich the resistance decreases as the pressure increases. The sensors are\nembedded in a soft and comfortable shoe sole and then integrated with a laptop\nor mobile application to monitor and analyze human gait in real-time.\nFurthermore, a dedicated Graphical User Interface (GUI) is designed to read the\ndata. The pressure sensors are integrated with ESP32 microcontroller which\nwirelessly transmit data to the GUI and smart phones which could be further\nused in the intuitive monitoring, rehabilitation of the patients with foot\ndisorder or neuromotor diseases.",
        "Human cognitive performance is an underlying factor in most of our daily\nlives, and numerous factors influence cognitive performance. In this work, we\ninvestigate how changes in sleep quality influence cognitive performance,\nmeasured from a dataset collected during a 2-month field study. We collected\ncognitive performance data (alertness) with the Psychomotor Vigilance Task\n(PVT), mobile keyboard typing metrics from participants' smartphones, and sleep\nquality metrics through a wearable sleep tracking ring. Our findings highlight\nthat specific sleep metrics like night-time heart rate, sleep latency, sleep\ntiming, sleep restfulness, and overall sleep quantity significantly influence\ncognitive performance. To strengthen the current research on cognitive\nmeasurements, we introduce smartphone typing metrics as a proxy or a\ncomplementary method for continuous passive measurement of cognitive\nperformance. Together, our findings contribute to ubiquitous computing via a\nlongitudinal case study with a novel wearable device, the resulting findings on\nthe association between sleep and cognitive function, and the introduction of\nsmartphone keyboard typing as a proxy of cognitive function.",
        "Conventional semiconductors typically have bonding states near the valence\nband maximum (VBM) and antibonding states near the conduction band minimum\n(CBM). Semiconductors with the opposite electronic configuration, namely an\nantibonding VBM and a bonding CBM, are here termed ``anti-gap semiconductors\".\nThey have been theoretically proposed to exhibit excellent optoelectronic\nproperties because of their strong tolerance to defects. However, no anti-gap\nsemiconductors have been identified so far, despite a known list of\nsemiconductors with an antibonding VBM. Here, we use high-throughput\ncomputation to identify over 100 anti-gap semiconductors. From this group, we\nanalyze the transition metal dichalcogenide MX$_2$ (M=Hf, Zr; X=S, Se) family\nin detail. In addition to verifying their defect tolerance for both electrons\nand holes using first-principles simulations, we also discovered that\nphotoexcitation of charge carriers can lead to significant lattice stiffening\nand increased thermal conductivity in anti-gap semiconductors, which can be\npotentially used as photo-driven thermal switches. Our work analyzes the\nformation of the anti-gap electronic structure and showcases their unusual\nphotoinduced lattice dynamics that can have a potential impact on their\nphotophysical applications."
      ]
    }
  },
  {
    "id":2412.00868,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"How resilient are language models to text perturbations",
    "start_abstract":"Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b22"
      ],
      "title":[
        "Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models"
      ],
      "abstract":[
        "The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."
      ],
      "categories":[
        "stat.ME"
      ]
    },
    "list":{
      "title":[
        "Tightening Causal Bounds via Covariate-Aware Optimal Transport",
        "Clustered Flexible Calibration Plots For Binary Outcomes Using Random\n  Effects Modeling",
        "Identifying rapid changes in the hemodynamic response in event-related\n  functional magnetic resonance imaging",
        "The Building Blocks of Classical Nonparametric Two-Sample Testing\n  Procedures: Statistically Equivalent Blocks",
        "Wavelet-based estimation of long-memory parameter in stochastic\n  volatility models using a robust log-periodogram",
        "Spectral Clustering on Multilayer Networks with Covariates",
        "Inverse sampling intensity weighting for preferential sampling\n  adjustment",
        "Sparsity learning via structured functional factor augmentation",
        "Two-Round Distributed Principal Component Analysis: Closing the\n  Statistical Efficiency Gap",
        "A time-to-event three-outcome design for randomized phase II cancer\n  trials",
        "A Generalized Fr\\'echet Test for Object Data with Unequal Repeated\n  Measurements",
        "Time-Varying Causal Survival Learning",
        "Denoising Diffused Embeddings: a Generative Approach for Hypergraphs",
        "Qmod: Expressive High-Level Quantum Modeling",
        "On almost Gallai colourings in complete graphs",
        "Assessment of spectral phases of non-Hermitian quantum systems through\n  complex and singular values",
        "Bifurcations and stability of synchronized solutions in the Kuramoto\n  model with uniformly spaced natural frequencies",
        "Construction of exact refinements for the two-dimensional HB\/THB-spline\n  de Rham complex",
        "AI-powered virtual tissues from spatial proteomics for clinical\n  diagnostics and biomedical discovery",
        "Exact calculation of spectral properties of a particle interacting with\n  a one-dimensional Fermi gas in optical lattices",
        "Independent transversal blow-up of graphs",
        "WISDOM Project -- XXII. A 5% precision CO-dynamical supermassive black\n  hole mass measurement in the galaxy NGC 383",
        "Effects of Ru-doping on the magnetism of Ag3LiIr2O6, a candidate Kitaev\n  quantum spin liquid",
        "A Bayesian Record Linkage Approach to Applications in Tree Demography\n  Using Overlapping LiDAR Scans",
        "Multipoint stress mixed finite element methods for elasticity on cuboid\n  grids",
        "$q$-deformation of random partitions, determinantal structure, and\n  Riemann-Hilbert problem",
        "Efficient parameter inference in networked dynamical systems via steady\n  states: A surrogate objective function approach integrating mean-field and\n  nonlinear least squares",
        "A note on Arveson's hyperrigidity and non-degenerate C*-correspondences"
      ],
      "abstract":[
        "Causal estimands can vary significantly depending on the relationship between\noutcomes in treatment and control groups, potentially leading to wide partial\nidentification (PI) intervals that impede decision making. Incorporating\ncovariates can substantially tighten these bounds, but requires determining the\nrange of PI over probability models consistent with the joint distributions of\nobserved covariates and outcomes in treatment and control groups. This problem\nis known to be equivalent to a conditional optimal transport (COT) optimization\ntask, which is more challenging than standard optimal transport (OT) due to the\nadditional conditioning constraints. In this work, we study a tight relaxation\nof COT that effectively reduces it to standard OT, leveraging its\nwell-established computational and theoretical foundations. Our relaxation\nincorporates covariate information and ensures narrower PI intervals for any\nvalue of the penalty parameter, while becoming asymptotically exact as a\npenalty increases to infinity. This approach preserves the benefits of\ncovariate adjustment in PI and results in a data-driven estimator for the PI\nset that is easy to implement using existing OT packages. We analyze the\nconvergence rate of our estimator and demonstrate the effectiveness of our\napproach through extensive simulations, highlighting its practical use and\nsuperior performance compared to existing methods.",
        "Evaluation of clinical prediction models across multiple clusters, whether\ncenters or datasets, is becoming increasingly common. A comprehensive\nevaluation includes an assessment of the agreement between the estimated risks\nand the observed outcomes, also known as calibration. Calibration is of utmost\nimportance for clinical decision making with prediction models and it may vary\nbetween clusters. We present three approaches to take clustering into account\nwhen evaluating calibration. (1) Clustered group calibration (CG-C), (2) two\nstage meta-analysis calibration (2MA-C) and (3) mixed model calibration (MIX-C)\ncan obtain flexible calibration plots with random effects modelling and\nproviding confidence and prediction intervals. As a case example, we externally\nvalidate a model to estimate the risk that an ovarian tumor is malignant in\nmultiple centers (N = 2489). We also conduct a simulation study and synthetic\ndata study generated from a true clustered dataset to evaluate the methods. In\nthe simulation and the synthetic data analysis MIX-C gave estimated curves\nclosest to the true overall and center specific curves. Prediction interval was\nbest for 2MA-C with splines. Standard flexible calibration worked likewise in\nterms of calibration error when sample size is limited. We recommend using\n2MA-C (splines) to estimate the curve with the average effect and the 95% PI\nand MIX-C for the cluster specific curves, specially when sample size per\ncluster is limited. We provide ready-to-use code to construct summary flexible\ncalibration curves with confidence and prediction intervals to assess\nheterogeneity in calibration across datasets or centers.",
        "The hemodynamic response (HR) in event-related functional magnetic resonance\nimaging is typically assumed to be stationary. While there are some approaches\nin the literature to model nonstationary HRs, few focus on rapid changes. In\nthis work, we propose two procedures to investigate rapid changes in the HR.\nBoth procedures make inference on the existence of rapid changes for\nmulti-subject data. We allow the change point locations to vary between\nsubjects, conditions and brain regions. The first procedure utilizes available\ninformation about the change point locations to compare multiple shape\nparameters of the HR over time. In the second procedure, the change point\nlocations are determined for each subject separately. To account for the\nestimation of the change point locations, we propose the notion of post\nselection variance. The power of the proposed procedures is assessed in\nsimulation studies. We apply the procedure for pre-specified change point\nlocations to data from a category learning experiment.",
        "Statistically equivalent blocks are not frequently considered in the context\nof nonparametric two-sample hypothesis testing. Despite the limited exposure,\nthis paper shows that a number of classical nonparametric hypothesis tests can\nbe derived on the basis of statistically equivalent blocks and their\nfrequencies. Far from a moot historical point, this allows for a more unified\napproach in considering the many two-sample nonparametric tests based on ranks,\nsigns, placements, order statistics, and runs. Perhaps more importantly, this\napproach also allows for the easy extension of many univariate nonparametric\ntests into arbitrarily high dimensions that retain all null properties\nregardless of dimensionality and are invariant to the scaling of the\nobservations. These generalizations do not require depth functions or the\nexplicit use of spatial signs or ranks and may be of use in various areas such\nas life-testing and quality control. In the manuscript, an overview of\nstatistically equivalent blocks and tests based on these blocks are provided.\nThis is followed by reformulations of some popular univariate tests and\ngeneralizations to higher dimensions. Comments comparing proposed methods to\nthose based on spatial signs and ranks are offered along with some conclusions.",
        "In this paper, we propose a novel method for estimating the long-memory\nparameter in time series. By combining the multi-resolution framework of\nwavelets with the robustness of the Least Absolute Deviations (LAD) criterion,\nwe introduce a periodogram providing a robust alternative to classical methods\nin the presence of non-Gaussian noise. Incorporating this periodogram into a\nlog-periodogram regression, we develop a new estimator. Simulation studies\ndemonstrate that our estimator outperforms the Geweke and Porter-Hudak (GPH)\nand Wavelet-Based Log-Periodogram (WBLP) estimators, particularly in terms of\nmean squared error, across various sample sizes and parameter configurations.",
        "The community detection problem on multilayer networks have drawn much\ninterest. When the nodal covariates ar also present, few work has been done to\nintegrate information from both sources. To leverage the multilayer networks\nand the covariates, we propose two new algorithms: the spectral clustering on\naggregated networks with covariates (SCANC), and the spectral clustering on\naggregated Laplacian with covariates (SCALC). These two algorithms are easy to\nimplement, computationally fast, and feature a data-driven approach for tuning\nparameter selection.\n  We establish theoretical guarantees for both methods under the Multilayer\nStochastic Blockmodel with Covariates (MSBM-C), demonstrating their consistency\nin recovering community structure. Our analysis reveals that increasing the\nnumber of layers, incorporating covariate information, and enhancing network\ndensity all contribute to improved clustering accuracy. Notably, SCANC is most\neffective when all layers exhibit similar assortativity, whereas SCALC performs\nbetter when both assortative and disassortative layers are present. On the\nsimulation studies and a primary school contact data analysis, our method\noutperforms other methods. Our results highlight the advantages of\nspectral-based aggregation techniques in leveraging both network structure and\nnodal attributes for robust community detection.",
        "Traditional geostatistical methods assume independence between observation\nlocations and the spatial process of interest. Violations of this independence\nassumption are referred to as preferential sampling (PS). Standard methods to\naddress PS rely on estimating complex shared latent variable models and can be\ndifficult to apply in practice. We study the use of inverse sampling intensity\nweighting (ISIW) for PS adjustment in model-based geostatistics. ISIW is a\ntwo-stage approach wherein we estimate the sampling intensity of the\nobservation locations then define intensity-based weights within a weighted\nlikelihood adjustment. Prediction follows by substituting the adjusted\nparameter estimates within a kriging framework. A primary contribution was to\nimplement ISIW by means of the Vecchia approximation, which provides large\ncomputational gains and improvements in predictive accuracy. Interestingly, we\nfound that accurate parameter estimation had little correlation with predictive\nperformance, raising questions about the conditions and parameter choices\ndriving optimal implementation of kriging-based predictors under PS. Our work\nhighlights the potential of ISIW to adjust for PS in an intuitive, fast, and\neffective manner.",
        "As one of the most powerful tools for examining the association between\nfunctional covariates and a response, the functional regression model has been\nwidely adopted in various interdisciplinary studies. Usually, a limited number\nof functional covariates are assumed in a functional linear regression model.\nNevertheless, correlations may exist between functional covariates in\nhigh-dimensional functional linear regression models, which brings significant\nstatistical challenges to statistical inference and functional variable\nselection. In this article, a novel functional factor augmentation structure\n(fFAS) is proposed for multivariate functional series, and a multivariate\nfunctional factor augmentation selection model (fFASM) is further proposed to\ndeal with issues arising from variable selection of correlated functional\ncovariates. Theoretical justifications for the proposed fFAS are provided, and\nstatistical inference results of the proposed fFASM are established. Numerical\ninvestigations support the superb performance of the novel fFASM model in terms\nof estimation accuracy and selection consistency.",
        "We enhance Fan et al.'s (2019) one-round distributed principal component\nanalysis algorithm by adding a second fixed-point iteration round. Random\nmatrix theory reveals the one-round estimator exhibits higher asymptotic error\nthan the pooling estimator under moderate local signal-to-noise ratios.\nRemarkably, our second iteration round eliminates this efficiency gap. It\nfollows from a careful analysis of the first-order perturbation of eigenspaces.\nEmpirical experiments on synthetic and benchmark datasets consistently\ndemonstrate the two-round method's statistical advantage over the one-round\napproach.",
        "Tumor response, a binary variable, has historically been the main measure of\nantitumor activity for many cancer phase II single-arm trials. Simon two-stage\ndesigns are often used. Sargent et al. proposed a three-outcome trial design in\nthis setting which requires smaller sample sizes. For many new, molecularly\ntargeted therapies, however, tumor response may not be the most reliable\nendpoint for measuring anti-tumor activity. Increasingly, time-to-event\nendpoints, such as progression-free survival (PFS), are used in the phase II\nsetting. When such endpoints are the primary measure of efficacy, a randomized\nconcurrently controlled study design is usually required. Given limited\nresources for phase II, studies are often underpowered with relatively large\ntype I and II error rates, and it is sometimes unavoidable to have a \"gray\"\ndecision zone after phase II where a clear decision regarding further\ndevelopment actions cannot be made without additional information. Compared\nwith an underpowered standard two-outcome study, a three-outcome design prompts\nclinical trialists to contemplate the likelihood of landing in the \"gray\" zone\nat the trial design stage and choose study design parameters more\nappropriately. We propose a three-outcome design, with or without interim\nanalyses, for randomized comparative phase II trials when a time-to-event\nendpoint is used.",
        "Advancements in data collection have led to increasingly common repeated\nobservations with complex structures in biomedical studies. Treating these\nobservations as random objects, rather than summarizing features as vectors,\navoids feature extraction and better reflects the data's nature. Examples\ninclude repeatedly measured activity intensity distributions in physical\nactivity analysis and brain networks in neuroimaging. Testing whether these\nrepeated random objects differ across groups is fundamentally important;\nhowever, traditional statistical tests often face challenges due to the\nnon-Euclidean nature of metric spaces, dependencies from repeated measurements,\nand the unequal number of repeated measures. By defining within-subject\nvariability using pairwise distances between repeated measures and extending\nFr\\'echet analysis of variance, we develop a generalized Fr\\'echet test for\nexchangeable repeated random objects, applicable to general metric space-valued\ndata with unequal numbers of repeated measures. The proposed test can\nsimultaneously detect differences in location, scale, and within-subject\nvariability. We derive the asymptotic distribution of the test statistic, which\nfollows a weighted chi-squared distribution. Simulations demonstrate that the\nproposed test performs well across different types of random objects. We\nillustrate its effectiveness through applications to physical activity data and\nresting-state functional magnetic resonance imaging data.",
        "This work bridges the gap between staggered adoption designs and survival\nanalysis to estimate causal effects in settings with time-varying treatments,\naddressing a fundamental challenge in medical research exemplified by the\nStanford Heart Transplant study. In medical interventions, particularly organ\ntransplantation, the timing of treatment varies significantly across patients\ndue to factors such as donor availability and patient readiness, introducing\npotential bias in treatment effect estimation if not properly accounted for. We\nidentify conditions under which staggered adoption assumptions can justify the\nuse of survival analysis techniques for causal inference with time-varying\ntreatments. By establishing this connection, we enable the use of existing\nsurvival analysis methods while maintaining causal interpretability.\nFurthermore, we enhance estimation performance by incorporating double machine\nlearning methods, improving efficiency when handling complex relationships\nbetween patient characteristics and survival outcomes. Through both simulation\nstudies and application to heart transplant data, our approach demonstrates\nsuperior performance compared to traditional methods, reducing bias and\noffering theoretical guarantees for improved efficiency in survival analysis\nsettings.",
        "Hypergraph data, which capture multi-way interactions among entities, are\nbecoming increasingly prevalent in the big data eta. Generating new hyperlinks\nfrom an observed, usually high-dimensional hypergraph is an important yet\nchallenging task with diverse applications, such as electronic health record\nanalysis and biological research. This task is fraught with several challenges.\nThe discrete nature of hyperlinks renders many existing generative models\ninapplicable. Additionally, powerful machine learning-based generative models\noften operate as black boxes, providing limited interpretability. Key\nstructural characteristics of hypergraphs, including node degree heterogeneity\nand hyperlink sparsity, further complicate the modeling process and must be\ncarefully addressed. To tackle these challenges, we propose Denoising Diffused\nEmbeddings (DDE), a general generative model architecture for hypergraphs. DDE\nexploits potential low-rank structures in high-dimensional hypergraphs and\nadopts the state-of-the-art diffusion model framework. Theoretically, we show\nthat when true embeddings are accessible, DDE exactly reduces the task of\ngenerating new high-dimensional hyperlinks to generating new low-dimensional\nembeddings. Moreover, we analyze the implications of using estimated embeddings\nin DDE, revealing how hypergraph properties--such as dimensionality, node\ndegree heterogeneity, and hyperlink sparsity--impact its generative\nperformance. Simulation studies demonstrate the superiority of DDE over\nexisting methods, in terms of both computational efficiency and generative\naccuracy. Furthermore, an application to a symptom co-occurrence hypergraph\nderived from electronic medical records uncovers interesting findings and\nhighlights the advantages of DDE.",
        "Quantum computing hardware is advancing at a rapid pace, yet the lack of\nhigh-level programming abstractions remains a serious bottleneck in the\ndevelopment of new applications. Widely used frameworks still rely on\ngate-level circuit descriptions, causing the algorithm's functional intent to\nbecome lost in low-level implementation details, and hindering flexibility and\nreuse. While various high-level quantum programming languages have emerged in\nrecent years - offering a significant step toward higher abstraction - many\nstill lack support for classical-like expression syntax, and native constructs\nfor useful quantum algorithmic idioms. This paper presents Qmod, a high-level\nquantum programming language designed to capture algorithmic intent in natural\nterms while delegating implementation decisions to automation. Qmod introduces\nquantum numeric variables and expressions, including digital fixed-point\narithmetic tuned for compact representations and optimal resource usage. Beyond\ndigital encoding, Qmod also supports non-digital expression modes - phase and\namplitude encoding - frequently exploited by quantum algorithms to achieve\ncomputational advantages. We describe the language's constructs, demonstrate\npractical usage examples, and outline future work on evaluating Qmod across a\nbroader set of use cases.",
        "For $t \\in \\mathbb{N}$, we say that a colouring of $E(K_n)$ is\n$\\textit{almost}$ $t$-$\\textit{Gallai}$ if no two rainbow $t$-cliques share an\nedge. Motivated by a lemma of Berkowitz on bounding the modulus of the\ncharacteristic function of clique counts in random graphs, we study the maximum\nnumber $\\tau_t(n)$ of rainbow $t$-cliques in an almost $t$-Gallai colouring of\n$E(K_n)$. For every $t \\ge 4$, we show that $n^{2-o(1)} \\leq \\tau_t(n) =\no(n^2)$. For $t=3$, surprisingly, the behaviour is substantially different. Our\nmain result establishes that $$\\left ( \\frac{1}{2}-o(1) \\right ) n\\log n \\le\n\\tau_3(n) = O\\big (n^{\\sqrt{2}}\\log n \\big ),$$ which gives the first\nnon-trivial improvements over the simple lower and upper bounds. Our proof\ncombines various applications of the probabilistic method and a generalisation\nof the edge-isoperimetric inequality for the hypercube.",
        "Chaotic behavior or lack thereof in non-Hermitian systems is often diagnosed\nvia spectral analysis of associated complex eigenvalues. Very recently,\nsingular values of the associated non-Hermitian systems have been proposed as\nan effective measure to study dissipative quantum chaos. Motivated by the rich\nproperties of non-Hermitian power-law banded random matrices and its promise as\na platform to study localized and delocalized phases in non-Hermitian systems,\nwe make an in-depth study to assess different spectral phases of these matrices\nthrough the lens of both complex eigenvalues and singular values. Remarkably,\nthe results from complex spectra and singular value analysis are seemingly\ndifferent, thereby necessitating caution while identifying different phases. We\nalso exemplify our findings by studying a non-Hermitian Hamiltonian with a\ncomplex on-site disorder. Our work indicates that systems, where disorder is\npresent both in the Hermitian and non-Hermitian segments of a Hamiltonian, are\nsensitive to the specific diagnostic tool that needs to be employed to study\nquantum chaos.",
        "We consider the classical Kuramoto model (KM) with natural frequencies and\nits continuum limit (CL), and discuss the existence of synchronized solutions\nand their bifurcations and stability. We specifically assume that the frequency\nfunction is symmetric and linear in the CL, so that the natural frequencies are\nevenly spaced in the KM. We show that in the KM, $O(2^n)$ one-parameter\nfamilies of synchronized solutions are born and $O(2^n)$ {saddle-node and}\npitchfork bifurcations occur at least, when the node number $n$ is odd and\ntends to infinity. Moreover, we prove that the family of synchronized solutions\nobtained in the previous work suffers a saddle-node bifurcation at which its\nstability changes from asymptotically stable to unstable and the other families\nof synchronized solutions are unstable in the KM. For the CL, we show that the\none-parameter family of synchronized solutions obtained in the previous work is\nthe only continuous one and there exist uncountably many one-parameter families\nof noncontinuous synchronized solutions and that the former is asymptotically\nstable and the latter are unstable.",
        "Studying the de Rham complex is a natural choice when working with problems\nin electromagnetics and fluid mechanics. By discretizing the complex correctly,\nit is possible to attain stable numerical methods to tackle these problems. An\nimportant consideration when constructing the discrete complex is that it must\npreserve the cohomology structure of the original one. This property is not\nguaranteed when the discrete function spaces chosen are hierarchical B-splines.\nResearch shows that a poor choice of refinement domains may give rise to\nspurious harmonic forms that ruin the accuracy of solutions, even for the\nsimplest partial differential equations. Another crucial aspect to consider in\nthe hierarchical setting is the notion of admissibility, as it is possible to\nobtain optimal convergence rates of numerical solutions by limiting the\nmulti-level interaction of basis functions. We will focus on the\ntwo-dimensional de Rham complex over the unit square $\\Omega \\subseteq\n\\mathbb{R}^2$. In this scenario, the discrete de Rham complex should be exact,\nand we provide both the theoretical and the algorithm-implementation framework\nto ensure this is the case. Moreover, we show that, under a common restriction,\nthe admissibility class of the first space of the discrete complex persists\nthroughout the remaining spaces. Finally, we include numerical results that\nmotivate the importance of the previous concerns for the vector Laplace and\nMaxwell eigenvalue problems.",
        "Spatial proteomics technologies have transformed our understanding of complex\ntissue architectures by enabling simultaneous analysis of multiple molecular\nmarkers and their spatial organization. The high dimensionality of these data,\nvarying marker combinations across experiments and heterogeneous study designs\npose unique challenges for computational analysis. Here, we present Virtual\nTissues (VirTues), a foundation model framework for biological tissues that\noperates across the molecular, cellular and tissue scale. VirTues introduces\ninnovations in transformer architecture design, including a novel tokenization\nscheme that captures both spatial and marker dimensions, and attention\nmechanisms that scale to high-dimensional multiplex data while maintaining\ninterpretability. Trained on diverse cancer and non-cancer tissue datasets,\nVirTues demonstrates strong generalization capabilities without task-specific\nfine-tuning, enabling cross-study analysis and novel marker integration. As a\ngeneralist model, VirTues outperforms existing approaches across clinical\ndiagnostics, biological discovery and patient case retrieval tasks, while\nproviding insights into tissue function and disease mechanisms.",
        "By using the exact Bethe wavefunctions of the one-dimensional Hubbard model\nwith $N$ spin-up fermions and one spin-down impurity, we derive an analytic\nexpression of the impurity form factor, in the form of a determinant of a\n$(N+1)$ by $(N+1)$ matrix. This analytic expression enables us to exactly\ncalculate spectral properties of one-dimensional Fermi polarons in lattices,\nwhen the masses of the impurity particle and the Fermi bath are equal. We\npresent the impurity spectral function as functions of the on-site interaction\nstrength and the filling factor of the Fermi bath, and discuss the origin of\nFermi singularities in the spectral function at small momentum and the\nemergence of polaron quasiparticles at large momentum near the boundary of\nBrillouin zone. Our analytic expression of the impurity form factors pave the\nway to exploring the intriguing dynamics of a particle interacting with a Fermi\nbath. Our exact predictions on the impurity spectral function could be directly\nexamined in cold-atom laboratories by using the radio-frequency spectroscopy\nand Ramsey spectroscopy.",
        "In an $r$-partite graph, an independent transversal of size $s$ (ITS)\nconsists of $s$ vertices from each part forming an independent set. Motivated\nby a question from Bollob\\'as, Erd\\H{o}s, and Szemer\\'edi (1975), Di Braccio\nand Illingworth (2024) inquired about the minimum degree needed to ensure an $n\n\\times \\cdots \\times n$ $r$-partite graph contains $K_r(s)$, a complete\n$r$-partite graph with $s$ vertices in each part. We reformulate this as\nfinding the smallest $n$ such that any $n \\times \\cdots \\times n$ $r$-partite\ngraph with maximum degree $\\Delta$ has an ITS. For any $\\varepsilon>0$, we\nprove the existence of a $\\gamma>0$ ensuring that if $G$ is a multipartite\ngraph partitioned as $(V_1, V_2, \\ldots, V_r)$, where the average degree of\neach part $V_i$ is at most $D$, the maximum degree of any vertex to any part\n$V_i$ is at most $\\gamma D$, and the size of each part $V_i$ is at least $(s +\n\\varepsilon)D$, then $G$ possesses an ITS. The constraint $(s + \\varepsilon)D$\non the part size is tight. This extends results of Loh and Sudakov (2007),\nGlock and Sudakov (2022), and Kang and Kelly (2022). We also show that any $n\n\\times \\cdots \\times n$ $r$-partite graph with minimum degree at least\n$\\left(r-1-\\frac{1}{2s^2}\\right)n$ contains $K_r(s)$ and provide a relative\nTur\\'an-type result. Additionally, this paper explores counting ITSs in\nmultipartite graphs.",
        "We present a measurement of the supermassive black hole (SMBH) mass of the\nnearby lenticular galaxy NGC 383, based on Atacama Large\nMillimeter\/sub-millimeter Array (ALMA) observations of the $^{12}$CO(2-1)\nemission line with an angular resolution of $0.''050\\times0.''024$\n($\\approx16\\times8$ pc$^2$). These observations spatially resolve the nuclear\nmolecular gas disc down to $\\approx41,300$ Schwarzschild radii and the SMBH\nsphere of influence by a factor of $\\approx24$ radially, better than any other\nSMBH mass measurement using molecular gas to date. The high resolution enables\nus to probe material with a maximum circular velocity of $\\approx1040$ km\/s,\neven higher than those of the highest-resolution SMBH mass measurements using\nmegamasers. We detect a clear Keplerian increase (from the outside in) of the\nline-of-sight rotation velocities, a slight offset between the gas disc\nkinematic (i.e. the position of the SMBH) and morphological (i.e. the centre of\nthe molecular gas emission) centres, an asymmetry of the innermost rotation\nvelocity peaks and evidence for a mild position angle warp and\/or non-circular\nmotions within the central $\\approx0.''3$. By forward modelling the mass\ndistribution and ALMA data cube, we infer a SMBH mass of\n$(3.58\\pm0.19)\\times10^9$ M$_\\odot$ ($1\\sigma$ confidence interval), more\nprecise ($5\\%$) but consistent within $\\approx1.4\\sigma$ with the previous\nmeasurement using lower-resolution molecular gas data. Our measurement\nemphasises the importance of high spatial resolution observations for precise\nSMBH mass determinations.",
        "We report our investigations on Ag3LiIr1.4Ru0.6O6, which results from the Ru\nsubstitution in the Kitaev quantum spin liquid candidate Ag3LiIr2O6. It\ncrystallizes in the monoclinic C2\/m space group like its parent compound,\nAg3LiIr2O6. Our susceptibility measurements reveal an effective moment = 2.6\nmuB, which is higher than the moments of the parent compound and less than that\nof the Ru-analog (Ag3LiRu2O6), suggesting the presence of magnetic Ir4+ (Jeff=\n1\/2) and Ru4+ (S=1). Bulk magnetic susceptibility suggests long-range order\n(LRO)at T~20 K, whereas no clear signature is present in the heat capacity.\nLikewise, there is a loss of the 7Li NMR spectral intensity around T~20 K as\nexpected at the onset of LRO, but a complete wipe-out is not seen in contrast\nto the result in Ag3LiIr2O6. There is also a T~20 K anomaly in the 7Li NMR\nrelaxation rate and also a fall in the 7Li NMR shift with decreasing\ntemperature. These results suggest LRO at T~20 K in Ag3LiIr1.4Ru0.6O6. However,\nat low-T below 10 K, we observe a power law variation in magnetic heat capacity\nand spin lattice relaxation rate, temperature-independent-7K, and no further\nloss of the 7Li NMR spectral intensity. These results might suggest the\npersistence or stabilisation of a quantum spin liquid-like phase, perhaps from\na fraction of the sample in Ag3LiIr1.4Ru0.6O6 below 10 K. Our muon spin\nrelaxation measurements suggest ordering around 20 K, consistent with our other\nprobes. It appears that the main effect of Ru-substitution is to shift the LRO\nto a higher temperature in comparison with Ag3LiIr2O6, though there are\nsignatures of a novel phase below about 10 K.",
        "In the information age, it has become increasingly common for data containing\nrecords about overlapping individuals to be distributed across multiple\nsources, making it necessary to identify which records refer to the same\nindividual. The goal of record linkage is to estimate this unknown structure in\nthe absence of a unique identifiable attribute. We introduce a Bayesian\nhierarchical record linkage model for spatial location data motivated by the\nestimation of individual specific growth-size curves for conifer species using\ndata derived from overlapping LiDAR scans. Annual tree growth may be estimated\ndependent upon correctly identifying unique individuals across scans in the\npresence of noise. We formalize a two-stage modeling framework, connecting the\nrecord linkage model and a flexible downstream individual tree growth model,\nthat provides robust uncertainty quantification and propagation through both\nstages of the modeling pipeline via an extension of the linkage-averaging\napproach of Sadinle (2018). In this paper, we discuss the two-stage model\nformulation, outline the computational strategies required to achieve\nscalability, assess the model performance on simulated data, and fit the model\nto a bi-temporal dataset derived from LiDAR scans of the Upper Gunnison\nWatershed provided by the Rocky Mountain Biological Laboratory to assess the\nimpact of key topographic covariates on the growth behavior of conifer species\nin the Southern Rocky Mountains (USA).",
        "We develop multipoint stress mixed finite element methods for linear\nelasticity with weak stress symmetry on cuboid grids, which can be reduced to a\nsymmetric and positive definite cell-centered system. The methods employ the\nlowest-order enhanced Raviart-Thomas finite element space for the stress and\npiecewise constant displacement. The vertex quadrature rule is employed to\nlocalize the interaction of stress degrees of freedom, enabling local stress\nelimination around each vertex. We introduce two methods. The first method uses\na piecewise constant rotation, resulting in a cell-centered system for the\ndisplacement and rotation. The second method employs a continuous piecewise\ntrilinear rotation and the vertex quadrature rule for the asymmetry bilinear\nforms, allowing for further elimination of the rotation and resulting in a\ncell-centered system for the displacement only. Stability and error analysis is\nperformed for both methods. For the stability analysis of the second method, a\nnew auxiliary H-curl conforming matrix-valued space is constructed, which forms\nan exact sequence with the stress space. A matrix-matrix inf-sup condition is\nshown for the curl of this auxiliary space and the trilinear rotation space.\nFirst-order convergence is established for all variables in their natural\nnorms, as well as second-order superconvergence of the displacement at the cell\ncenters. Numerical results are presented to verify the theory.",
        "We study $q$-deformation of the probability measure on partitions, i.e.,\n$q$-deformed random partitions. We in particular consider the $q$-Plancherel\nmeasure and show a determinantal formula for the correlation function using a\n$q$-deformation of the discrete Bessel kernel. We also investigate\nRiemann-Hilbert problems associated with the corresponding orthogonal\npolynomials and obtain $q$-Painlev{\\'e} equations from the $q$-difference Lax\nformalism.",
        "In networked dynamical systems, inferring governing parameters is crucial for\npredicting nodal dynamics, such as gene expression levels, species abundance,\nor population density. While many parameter estimation techniques rely on\ntime-series data, particularly systems that converge over extreme time ranges,\nonly noisy steady-state data is available, requiring a new approach to infer\ndynamical parameters from noisy observations of steady states. However, the\ntraditional optimization process is computationally demanding, requiring\nrepeated simulation of coupled ordinary differential equations (ODEs). To\novercome these limitations, we introduce a surrogate objective function that\nleverages decoupled equations to compute steady states, significantly reducing\ncomputational complexity. Furthermore, by optimizing the surrogate objective\nfunction, we obtain steady states that more accurately approximate the ground\ntruth than noisy observations and predict future equilibria when topology\nchanges. We empirically demonstrate the effectiveness of the proposed method\nacross ecological, gene regulatory, and epidemic networks. Our approach\nprovides an efficient and effective way to estimate parameters from\nsteady-state data and has the potential to improve predictions in networked\ndynamical systems.",
        "We revisit the results of Kim, and of Katsoulis and Ramsey concerning\nhyperrigidity for non-degenerate C*-correspondences. We show that the tensor\nalgebra is hyperrigid, if and only if Katsura's ideal acts non-degenerately, if\nand only if Katsura's ideal acts non-degenerately under any representation.\nThis gives a positive answer to the question of Katsoulis and Ramsey, showing\nthat their necessary condition and their sufficient condition for hyperrigidity\nof the tensor algebra are equivalent. Non-degeneracy of the left action of\nKatsura's ideal was also shown by Kim to be equivalent to hyperrigidity for the\nselfadjoint operator space associated with the C*-correspondence, and our\napproach provides a simplified proof of this result as well. In the process we\nrevisit Arveson's criterion connecting maximality with the unique extension\nproperty and hyperrigidity, in conjunction with the work of Salomon on\ngenerating sets."
      ]
    }
  },
  {
    "id":2411.01668,
    "research_type":"basic",
    "start_id":"b4",
    "start_title":"Mean\u2010field games with differing beliefs for algorithmic trading",
    "start_abstract":"Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity.",
    "start_categories":[
      "q-fin.MF"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Linear-quadratic mean field games"
      ],
      "abstract":[
        "In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Online Learning-Based Predictive Control for Nonlinear System",
        "Characterization of Highly Robust Solutions in Multi-Objective\n  Programming in Banach Spaces",
        "Engagement Zones for a Turn Constrained Pursuer",
        "Nonlinearly Preconditioned Gradient Methods under Generalized Smoothness",
        "Approximate solutions in multiobjective interval-valued optimization\n  problems: Existence theorems and optimality conditions",
        "Pseudo-concave optimization of the first eigenvalue of elliptic\n  operators with application to topology optimization by homogenization",
        "PDLP: A Practical First-Order Method for Large-Scale Linear Programming",
        "Generalized transition uncertainties in constrained Markov decision\n  processes",
        "Unifying restart accelerated gradient and proximal bundle methods",
        "Double Traversals in Optimal Picker Routes for Warehouses with Multiple\n  Blocks",
        "Two Innovations in Inexact Augmented Lagrangian Methods for Convex\n  Optimization",
        "Optimal mixed fleet and charging infrastructure planning to electrify\n  demand responsive feeder services with target CO2 emission constraints",
        "Inner approximations of convex sets and intersections of projectionally\n  exposed cones",
        "The Eigenfunctions of the Transfer Operator for the Dyson model in a\n  field",
        "Minding Fuzzy Regions: A Data-driven Alternating Learning Paradigm for\n  Stable Lesion Segmentation",
        "Learning Privacy from Visual Entities",
        "A proposal for removing $\\pi N$-state contamination from the nucleon\n  induced pseudoscalar form factor in lattice QCD",
        "Modular Units on $X_{1}( p)$ and Quotients of the Cuspidal Group",
        "Entente: Cross-silo Intrusion Detection on Network Log Graphs with\n  Federated Learning",
        "Twin-Space Representation of Classical Mapping Model in the Constraint\n  Phase Space Representation: Numerically Exact Approach to Open Quantum\n  Systems",
        "Structure and Dynamics of Deep Eutectic Systems from Cluster-Optimized\n  Energy Functions",
        "SPRIG: Stackelberg Perception-Reinforcement Learning with Internal Game\n  Dynamics",
        "Towards Heisenberg limit without critical slowing down via quantum\n  reinforcement learning",
        "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
        "Unreflected Use of Tabular Data Repositories Can Undermine Research\n  Quality",
        "Quantum stochastic communication via high-dimensional entanglement",
        "The closure of linear foliations",
        "Dynamics near a class of nonhyperbolic fixed points"
      ],
      "abstract":[
        "In this paper, we propose an online learning-based predictive control (LPC)\napproach designed for nonlinear systems that lack explicit system dynamics.\nUnlike traditional model predictive control (MPC) algorithms that rely on known\nsystem models to optimize controller outputs, our proposed algorithm integrates\na reinforcement learning component to learn optimal policies in real time from\nthe offline dataset and real-time data. Additionally, an optimal control\nproblem (OCP)-based optimization framework is incorporated to enhance real-time\ncomputational efficiency while ensuring stability during online operation.\nMoreover, we rigorously establish the super-linear convergence properties of\nthe algorithm. Finally, extensive simulations are performed to evaluate the\nfeasibility and effectiveness of the proposed approach.",
        "This paper delves into the challenging issues in uncertain multi-objective\noptimization, where uncertainty permeates nonsmooth nonconvex objective and\nconstraint functions. In this context, we investigate highly robust (weakly\nefficient) solutions, a solution concept defined by efficiency across all\nscenarios. Our exploration reveals important relationships between highly\nrobust solutions and other robustness notions, including set-based and\nworst-case notions, as well as connections with proper and isolated efficiency.\nLeveraging modern techniques from variational analysis, we establish necessary\nand sufficient optimality conditions for these solutions. Moreover, we explore\nthe robustness of multi-objective optimization problems in the face of various\nuncertain sets, such as ball, ellipsoidal, and polyhedral sets.",
        "This work derives two basic engagement zone models, describing regions of\npotential risk or capture for a mobile vehicle by a pursuer. The pursuer is\nmodeled as having turn-constraints rather than simple motion. Turn-only\n(C-Paths) and turn-straight (CS-Paths) paths are considered for the pursuer of\nlimited range. Following the derivation, a simulation of a vehicle avoiding the\npursuer's engagement zone is provided.",
        "We analyze nonlinearly preconditioned gradient methods for solving smooth\nminimization problems. We introduce a generalized smoothness property, based on\nthe notion of abstract convexity, that is broader than Lipschitz smoothness and\nprovide sufficient first- and second-order conditions. Notably, our framework\nencapsulates algorithms associated with the clipping gradient method and brings\nout novel insights for the class of $(L_0,L_1)$-smooth functions that has\nreceived widespread interest recently, thus allowing us to go beyond already\nestablished methods. We investigate the convergence of the proposed method in\nboth the convex and nonconvex setting.",
        "This paper is devoted to the study of approximate solutions for a\nmultiobjective interval-valued optimization problem based on an interval order.\nWe establish new existence theorems of approximate solutions for such a problem\nunder some mild conditions. Moreover, we give KKT optimality conditions for\napproximate solutions for such a problem whose associated functions are\nnonsmooth and nonconvex. We also propose the approximate KKT optimality\ncondition of an approximate solution for such a problem. Finally, we apply some\nobtained results to a noncooperative game involving the multiobjective\ninterval-valued function.",
        "We consider optimization problems of the first eigenvalue of linear elliptic\noperators. It has an application to a two-phase optimal design problem (also\nknown as topology optimization problem) relaxed by the homogenization method.\nUnder certain assumptions, we show that the first eigenvalue is a\npseudo-concave function with respect to the density-like parameter and apply\nthe result to optimal design problems of conductivity and elasticity. Due to\npseudo-concavity, every stationary point is a global optimal solution for a\nmaximization problem, and there exists a solution that is an extreme point\n(corresponding to a 0-1 design) for a minimization problem. In numerical\nexperiments, we demonstrate that a global optimal solution or a 0-1 solution\ncan be obtained by a simple projected gradient method. These problems can be\nused as benchmark problems to test heuristic topology optimization methods used\nin engineering. The 80-line FreeFEM code is provided in the appendix.",
        "We present PDLP, a practical first-order method for linear programming (LP)\ndesigned to solve large-scale LP problems. PDLP is based on the primal-dual\nhybrid gradient (PDHG) method applied to the minimax formulation of LP. PDLP\nincorporates several enhancements to PDHG, including diagonal preconditioning,\npresolving, adaptive step sizes, adaptive restarting, and feasibility\npolishing. Our algorithm is implemented in C++, available in Google's\nopen-source OR-Tools library, and supports multithreading.\n  To evaluate our method, we introduce a new collection of eleven large-scale\nLP problems with sizes ranging from 125 million to 6.3 billion nonzeros. PDLP\nsolves eight of these instances to optimality gaps of 1\\% (with primal and dual\nfeasibility errors of less than $10^{-8}$) within six days on a single machine.\nWe also compare PDLP with Gurobi barrier, primal simplex, and dual simplex\nimplementations. Gurobi barrier solves only three instances, exceeding our 1TB\nRAM limit on the other eight. While primal and dual simplex are more\nmemory-efficient than the barrier method, they are slower and solve only three\ninstances within six days.\n  Compared with the conference version of this work (in: Advances in Neural\nInformation Processing Systems 34 (NeurIPS 2021)), the key new contributions\nare: (i) feasibility polishing, a technique that quickly finds solutions that\nare approximately optimal but almost exactly feasible (without which only three\nof the eleven problems can be solved); (ii) a multithreaded C++ implementation\navailable in Google OR-Tools; and (iii) a new collection of large-scale LP\nproblems. Note that the conference version should be referred to for\ncomparisons with SCS and ablation studies, which we do not repeat in this\npaper.",
        "We examine a constrained Markov decision process under uncertain transition\nprobabilities, with the uncertainty modeled as deviations from observed\ntransition probabilities. We construct the uncertainty set associated with the\ndeviations using polyhedral and second-order cone constraints and employ a\nrobust optimization framework. We demonstrate that each inner optimization\nproblem of the robust model can be equivalently transformed into a second-order\ncone programming problem. Using strong duality arguments, we show that the\nresulting robust problem can be equivalently reformulated into a non-convex\nprogramming problem that includes bilinear and second-order cone constraints.\nIn the numerical experiments, we study a machine replacement problem and\nexplore potential sources of uncertainty in the transition probabilities. We\nexamine how the optimal values and solutions differ as we vary the feasible\nregion of the uncertainty set, considering only polyhedral constraints and a\ncombination of polyhedral and second-order cone constraints. Furthermore, we\nanalyze the impact of the number of states, the discount factor, and variations\nin the feasible region of the uncertainty set on the optimal values.",
        "This paper presents a novel restarted version of Nesterov's accelerated\ngradient method and establishes its optimal iteration-complexity for solving\nconvex smooth composite optimization problems. The proposed restart accelerated\ngradient method is shown to be a specific instance of the accelerated inexact\nproximal point framework introduced in \"An accelerated hybrid proximal\nextragradient method for convex optimization and its implications to\nsecond-order methods\" by Monteiro and Svaiter, SIAM Journal on Optimization,\n2013. Furthermore, this work examines the proximal bundle method within the\ninexact proximal point framework, demonstrating that it is an instance of the\nframework. Notably, this paper provides new insights into the underlying\nalgorithmic principle that unifies two seemingly disparate optimization\nmethods, namely, the restart accelerated gradient and the proximal bundle\nmethods.",
        "The Picker Routing Problem is a variation of the Rectilinear Traveling\nSalesman Problem that involves finding the optimal tour of a warehouse that\ncollects all the required items on a given pick list. It has been proven that\nfor a rectangular warehouse with only two cross-aisles, traversing an aisle\ntwice is not required for an optimal tour; however, instances exist where this\nis needed for warehouses with more than two cross-aisles. In this paper, we\ndefine categories of double traversals in the Rectilinear Traveling Salesman\nProblem and prove that some of these are not required in a minimal tour. This\nis then applied to the Picker Routing Problem for generalized rectangular\nwarehouses of any size. These results are then used to show that some vertices\ncan be ignored when constructing a tour subgraph, and two out of the six\nvertical edge configurations considered in existing algorithms are not\nrequired. Finally, it is demonstrated that the horizontal edges of a minimal\ntour subgraph determine the vertical edge configurations, a result that could\nlead to practical improvement to existing algorithms.",
        "This paper presents two new techniques relating to inexact solution of\nsubproblems in augmented Lagrangian methods for convex programming. The first\ninvolves combining a relative error criterion for solution of the subproblems\nwith over- or under-relaxation of the multiplier update step. In one\ninterpretation of our proposed iterative scheme, a predetermined amount of\nrelaxation effects the criterion for an acceptably accurate solution value.\nAlternatively, the amount of multiplier step relaxation can be adapted to the\naccuracy of the subproblem subject to a viability test employing the\ndiscriminant of a certain quadratic function. The second innovation involves\nsolution of augmented Lagrangian subproblems for problems posed in standard\nFenchel-Rockafellar form. We show that applying alternating minimization to\nthis subproblem, as in the first two steps of the ADMM, is equivalent to\nexecuting the classical proximal gradient method on a dual formulation of the\nsubproblem. By substituting more sophisticated variants of the proximal\ngradient method for the classical one, it is possible to construct new\nADMM-like methods with better empirical performance than using ordinary\nalternating minimization within an inexact augmented Lagrangian framework. The\npaper concludes by describing some computational experiments exploring using\nthese two innovations, both separately and jointly, to solve LASSO problems.",
        "Electrifying demand-responsive transport systems need to plan the charging\ninfrastructure carefully, considering the trade-offs of charging efficiency and\ncharging infrastructure costs. Earlier studies assume a fully electrified fleet\nand overlook the planning issue in the transition period. This study addresses\nthe joint fleet size and charging infrastructure planning for a\ndemand-responsive feeder service under stochastic demand, given a user-defined\ntargeted CO2 emission reduction policy. We propose a bi-level optimization\nmodel where the upper-level determines charging station configuration given\nstochastic demand patterns, whereas the lower-level solves a mixed fleet\ndial-a-ride routing problem under the CO2 emission and capacitated charging\nstation constraints. An efficient deterministic annealing metaheuristic is\nproposed to solve the CO2-constrained mixed fleet routing problem. The\nperformance of the algorithm is validated by a series of numerical test\ninstances with up to 500 requests. We apply the model for a real-world case\nstudy in Bettembourg, Luxembourg, with different demand and customised CO2\nreduction targets. The results show that the proposed method provides a\nflexible tool for joint charging infrastructure and fleet size planning under\ndifferent levels of demand and CO2 emission reduction targets.",
        "A convex cone is said to be projectionally exposed (p-exposed) if every face\narises as a projection of the original cone. It is known that, in dimension at\nmost four, the intersection of two p-exposed cones is again p-exposed. In this\npaper we construct two p-exposed cones in dimension $5$ whose intersection is\nnot p-exposed. This construction also leads to the first example of an amenable\ncone that is not projectionally exposed, showing that these properties, which\ncoincide in dimension at most $4$, are distinct in dimension $5$. In order to\nachieve these goals, we develop a new technique for constructing arbitrarily\ntight inner convex approximations of compact convex sets with desired facial\nstructure. These inner approximations have the property that all proper faces\nare extreme points, with the exception of a specific exposed face of the\noriginal set.",
        "The recent works \\cite{EFMV2024} and \\cite{JOP2023} have studied the spectral\nproperties of the Dyson model in the absence of an external field. This paper\nis a continuation of \\cite{EFMV2024} and aims to bridge the gap in the\nliterature by investigating the Dyson model in a field.\\\\ In this paper, we\nprove that, for high temperatures or strong magnetic fields, there exists a\nnon-negative, integrable (with respect to the unique half-line Gibbs measure)\neigenfunction of the transfer operator for the Dyson model if $\\alpha\\in(\\frac\n3 2,2]$. However, unlike in the zero-magnetic-field case, this eigenfunction is\nnot continuous.",
        "Deep learning has achieved significant advancements in medical image\nsegmentation, but existing models still face challenges in accurately\nsegmenting lesion regions. The main reason is that some lesion regions in\nmedical images have unclear boundaries, irregular shapes, and small tissue\ndensity differences, leading to label ambiguity. However, the existing model\ntreats all data equally without taking quality differences into account in the\ntraining process, resulting in noisy labels negatively impacting model training\nand unstable feature representations. In this paper, a data-driven alternating\nlearning (DALE) paradigm is proposed to optimize the model's training process,\nachieving stable and high-precision segmentation. The paradigm focuses on two\nkey points: (1) reducing the impact of noisy labels, and (2) calibrating\nunstable representations. To mitigate the negative impact of noisy labels, a\nloss consistency-based collaborative optimization method is proposed, and its\neffectiveness is theoretically demonstrated. Specifically, the label confidence\nparameters are introduced to dynamically adjust the influence of labels of\ndifferent confidence levels during model training, thus reducing the influence\nof noise labels. To calibrate the learning bias of unstable representations, a\ndistribution alignment method is proposed. This method restores the underlying\ndistribution of unstable representations, thereby enhancing the discriminative\ncapability of fuzzy region representations. Extensive experiments on various\nbenchmarks and model backbones demonstrate the superiority of the DALE\nparadigm, achieving an average performance improvement of up to 7.16%.",
        "Subjective interpretation and content diversity make predicting whether an\nimage is private or public a challenging task. Graph neural networks combined\nwith convolutional neural networks (CNNs), which consist of 14,000 to 500\nmillions parameters, generate features for visual entities (e.g., scene and\nobject types) and identify the entities that contribute to the decision. In\nthis paper, we show that using a simpler combination of transfer learning and a\nCNN to relate privacy with scene types optimises only 732 parameters while\nachieving comparable performance to that of graph-based methods. On the\ncontrary, end-to-end training of graph-based methods can mask the contribution\nof individual components to the classification performance. Furthermore, we\nshow that a high-dimensional feature vector, extracted with CNNs for each\nvisual entity, is unnecessary and complexifies the model. The graph component\nhas also negligible impact on performance, which is driven by fine-tuning the\nCNN to optimise image features for privacy nodes.",
        "In the PACS10 project, the PACS collaboration has generated three sets of the\nPACS10 gauge configurations at the physical point with lattice volume larger\nthan $(10\\;{\\rm fm})^4$ and three different lattice spacings. The isovector\nnucleon form factors had been already calculated by using two sets of the\nPACS10 gauge configurations. In our strategy, the smearing parameters of the\nnucleon interpolation operator were highly optimized to eliminate as much as\npossible the contribution of excited states in the nucleon two-point function.\nThis strategy was quite successful in calculations of the electric ($G_E$),\nmagnetic ($G_M$) and axial-vector ($F_A$) form factors, while the induced\npseudoscalar ($F_P$) and pseudoscalar ($G_P$) form factors remained strongly\naffected by residual contamination of $\\pi N$-state contribution. In this work,\nwe propose a simple method to remove the $\\pi N$-state contamination from the\n$F_P$ form factor, and then evaluate the induced pseudoscalar charge $g_P^\\ast$\nand the pion-nucleon coupling $g_{\\pi NN}$ from existing data in a new\nanalysis. Applying this method to the $G_P$ form factor is also considered with\na help of the axial Ward-Takahashi identity.",
        "Modular units are functions on modular curves whose divisors are supported on\nthe cusps. They form a free abelian group of rank at most one less than the\nnumber of cusps. In this paper we study the group of modular units on $X_{1}( p\n)$, with prime level $p \\ge 5$. We give an explicit basis for this group and\nstudy certain rational subgroups of it. We use the basis to numerically\ninvestigate the structure of the cuspidal group of $X_{1}( p)$ and its rational\nsubgroup. In the later stages of this paper we use our basis to determine a\nspecific large quotient of the cuspidal group.",
        "Graph-based Network Intrusion Detection System (GNIDS) has gained significant\nmomentum in detecting sophisticated cyber-attacks, like Advanced Persistent\nThreat (APT), in an organization or across organizations. Though achieving\nsatisfying detection accuracy and adapting to ever-changing attacks and normal\npatterns, all prior GNIDSs assume the centralized data settings directly, but\nnon-trivial data collection is not always practical under privacy regulations\nnowadays. We argue that training a GNIDS model has to consider privacy\nregulations, and propose to leverage federated learning (FL) to address this\nprominent challenge.\n  Yet, directly applying FL to GNIDS is unlikely to succeed, due to issues like\nnon-IID (independent and identically distributed) graph data over clients and\nthe diverse design choices taken by different GNIDS. We address these issues\nwith a set of novel techniques tailored to the graph datasets, including\nreference graph synthesis, graph sketching and adaptive contribution scaling,\nand develop a new system Entente. We evaluate Entente on the large-scale LANL,\nOpTC and Pivoting datasets. The result shows Entente outperforms the other\nbaseline FL algorithms and sometimes even the non-FL GNIDS. We also evaluate\nEntente under FL poisoning attacks tailored to the GNIDS setting, and show\nEntente is able to bound the attack success rate to low values. Overall, our\nresult suggests building cross-silo GNIDS is feasible and we hope to encourage\nmore efforts in this direction.",
        "The constraint coordinate-momentum \\textit{phase space} (CPS) has recently\nbeen developed to study nonadiabatic dynamics in gas-phase and condensed-phase\nmolecular systems. Although the CPS formulation is exact for describing the\ndiscrete (electronic\/ vibrational\/spin) state degrees of freedom (DOFs), when\nsystem-bath models in condense phase are studied, previous works often employ\nthe discretization of environmental bath DOFs, which breaks the time\nirreversibility and may make it difficult to obtain numerically converged\nresults in the long-time limit. In this paper, we develop an exact\ntrajectory-based phase space approach by adopting the twin-space (TS)\nformulation of quantum statistical mechanics, in which the density operator of\nthe reduced system is transformed to the wavefunction of an expanded system\nwith twice the DOFs. The classical mapping model (CMM) is then used to map the\nHamiltonian of the expanded system to its equivalent classical counterpart on\nCPS. To demonstrate the applicability of the TS-CMM approach, we compare\nsimulated population dynamics and nonlinear spectra for a few benchmark\ncondensed phase system-bath models with those obtained from the hierarchical\nequations of motion method, which shows that our approach yields accurate\ndynamics of open quantum systems.",
        "Generating energy functions for heterogeneous systems suitable for\nquantitative and predictive atomistic simulations is a challenging undertaking.\nThe present work combines a cluster-based approach with electronic structure\ncalculations at the density functional theory level and machine learning-based\nenergy functions for a spectroscopic reporter for eutectic mixtures consisting\nof water, acetamide and KSCN. Two water models are considered: TIP3P which is\nconsistent with the CGenFF energy function and TIP4P which - as a water model -\nis superior to TIP4P. Both fitted models, {\\bf M2$^{\\rm TIP3P}$} and {\\bf\n  M2$^{\\rm TIP4P}$}, yield favourable thermodynamic, structural, spectroscopic\nand transport properties from extensive molecular dynamics simulations. In\nparticular, the slow and fast decay times from 2-dimensional infrared\nspectroscopy and the viscosity for water-rich mixtures are described\nrealistically and consistent with experiments. On the other hand, including the\nco-solvent (acetamide) in the present case is expected to further improve the\ncomputed viscosity for low-water content. It is concluded that such a\ncluster-based approach is a promising and generalizable route for routine\nparametrization of heterogeneous, electrostatically dominated systems.",
        "Deep reinforcement learning agents often face challenges to effectively\ncoordinate perception and decision-making components, particularly in\nenvironments with high-dimensional sensory inputs where feature relevance\nvaries. This work introduces SPRIG (Stackelberg Perception-Reinforcement\nlearning with Internal Game dynamics), a framework that models the internal\nperception-policy interaction within a single agent as a cooperative\nStackelberg game. In SPRIG, the perception module acts as a leader,\nstrategically processing raw sensory states, while the policy module follows,\nmaking decisions based on extracted features. SPRIG provides theoretical\nguarantees through a modified Bellman operator while preserving the benefits of\nmodern policy optimization. Experimental results on the Atari BeamRider\nenvironment demonstrate SPRIG's effectiveness, achieving around 30% higher\nreturns than standard PPO through its game-theoretical balance of feature\nextraction and decision-making.",
        "Critical ground states of quantum many-body systems have emerged as vital\nresources for quantum-enhanced sensing. Traditional methods to prepare these\nstates often rely on adiabatic evolution, which may diminish the quantum\nsensing advantage. In this work, we propose a quantum reinforcement learning\n(QRL)-enhanced critical sensing protocol for quantum many-body systems with\nexotic phase diagrams. Starting from product states and utilizing\nQRL-discovered gate sequences, we explore sensing accuracy in the presence of\nunknown external magnetic fields, covering both local and global regimes. Our\nresults demonstrate that QRL-learned sequences reach the finite quantum speed\nlimit and generalize effectively across systems of arbitrary size, ensuring\naccuracy regardless of preparation time. This method can robustly achieve\nHeisenberg and super-Heisenberg limits, even in noisy environments with\npractical Pauli measurements. Our study highlights the efficacy of QRL in\nenabling precise quantum state preparation, thereby advancing scalable,\nhigh-accuracy quantum critical sensing.",
        "Flow matching in the continuous simplex has emerged as a promising strategy\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\nand Score Matching, a generative framework on the simplex based on a novel\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\nparameterized velocity field that transports from smooth categorical\ndistributions to distributions concentrated at a single vertex of the simplex.\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\nthe gradient of the probability density. Our framework enables high-quality,\ndiverse generation and scales efficiently to higher-dimensional simplices. To\nenable training-free guidance, we propose Straight-Through Guided Flows\n(STGFlow), a classifier-based guidance method that leverages straight-through\nestimators to steer the unconditional velocity field toward optimal vertices of\nthe simplex. STGFlow enables efficient inference-time guidance using\nclassifiers pre-trained on clean sequences, and can be used with any discrete\nflow method. Together, these components form a robust framework for\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\nperformance in conditional DNA promoter design, sequence-only protein\ngeneration, and target-binding peptide design for rare disease treatment.",
        "Data repositories have accumulated a large number of tabular datasets from\nvarious domains. Machine Learning researchers are actively using these datasets\nto evaluate novel approaches. Consequently, data repositories have an important\nstanding in tabular data research. They not only host datasets but also provide\ninformation on how to use them in supervised learning tasks. In this paper, we\nargue that, despite great achievements in usability, the unreflected usage of\ndatasets from data repositories may have led to reduced research quality and\nscientific rigor. We present examples from prominent recent studies that\nillustrate the problematic use of datasets from OpenML, a large data repository\nfor tabular data. Our illustrations help users of data repositories avoid\nfalling into the traps of (1) using suboptimal model selection strategies, (2)\noverlooking strong baselines, and (3) inappropriate preprocessing. In response,\nwe discuss possible solutions for how data repositories can prevent the\ninappropriate use of datasets and become the cornerstones for improved overall\nquality of empirical research studies.",
        "Entanglement has the ability to enhance the transmission of classical\ninformation over a quantum channel. However, fully harvesting this advantage\ntypically requires complex entangling measurements, which are challenging to\nimplement and scale with the system's size. In this work, we consider a natural\nquantum information primitive in which the message to be communicated is\nselected stochastically. We introduce a protocol that leverages\nhigh-dimensional entanglement to perform this task perfectly, without requiring\nquantum interference between particles at the measurement station. We\nexperimentally demonstrate the protocol's scalability in an optical setup using\n8-dimensional entanglement and multi-outcome detection, providing a practical\nsolution for stochastic communication and a robust method for certifying the\ndimensionality of entanglement in communication experiments.",
        "This paper presents a simplified geometric proof of the\nMolino-Alexandrino-Radeschi (MAR) Theorem, which states that the closure of a\nsingular Riemannian foliation on a complete Riemannian manifold is itself a\nsmooth singular Riemannian foliation. Our approach circumvents several\ntechnical and analytical tools employed in the previous proof of the Theorem,\nresulting in a more direct geometric demonstration. We first establish\nconditions for a projectable foliation to be Riemannian, focusing on compatible\nconnections. We then apply these results to linear foliations on vector bundles\nand their lifts to frame bundles. Finally, we use these findings to the\nlinearization of singular Riemannian foliations around leaf closures. This\nmethod allows us to prove the smoothness of the closure directly for the linear\nsemi-local model, bypassing the need for intermediate results on orbit-like\nfoliations.",
        "In this paper, we investigate some dynamical properties near a nonhyperbolic\nfixed point. Under some conditions on the higher nonlinear terms, we establish\na stable manifold theorem and a degenerate Hartman theorem. Furthermore, the\nfinite shadowing property also be discussed."
      ]
    }
  },
  {
    "id":2411.01668,
    "research_type":"basic",
    "start_id":"b3",
    "start_title":"Linear-quadratic mean field games",
    "start_abstract":"In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Mean\u2010field games with differing beliefs for algorithmic trading"
      ],
      "abstract":[
        "Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity."
      ],
      "categories":[
        "q-fin.MF"
      ]
    },
    "list":{
      "title":[
        "Short-Rate Derivatives in a Higher-for-Longer Environment",
        "Existence of Optimal Contracts for Principal-Agent Problem with Drift\n  Control and Quadratic Effort Cost",
        "Decentralized Annuity: A Quest for the Holy Grail of Lifetime Financial\n  Security",
        "Numerical methods for two-dimensional G-heat equation",
        "On consistency of optimal portfolio choice for state-dependent\n  exponential utilities",
        "Heterogenous Macro-Finance Model: A Mean-field Game Approach",
        "Modelling High-Frequency Data with Bivariate Hawkes Processes: Power-Law\n  vs. Exponential Kernels",
        "Dual Formulation of the Optimal Consumption problem with Multiplicative\n  Habit Formation",
        "Capturing Smile Dynamics with the Quintic Volatility Model: SPX,\n  Skew-Stickiness Ratio and VIX",
        "Pricing time-capped American options using Least Squares Monte Carlo\n  method",
        "Beyond the Leland strategies",
        "Pricing American options under rough volatility using deep-signatures\n  and signature-kernels",
        "Optimal risk-aware interest rates for decentralized lending protocols",
        "GM-MoE: Low-Light Enhancement with Gated-Mechanism Mixture-of-Experts",
        "Large Language Models with Human-In-The-Loop Validation for Systematic\n  Review Data Extraction",
        "MODRIC: A Cost Effective MODular Data Center Network Architecture with\n  Rich InterConnections",
        "Investigating Human-Aligned Large Language Model Uncertainty",
        "Synthesizing Consistent Novel Views via 3D Epipolar Attention without\n  Re-Training",
        "An X-ray view of the Cataclysmic Variable V902 Mon: Discovery of an\n  X-ray eclipse",
        "Superconducting LaPtH$_{ 6 }$ with triatomic hydrogen units",
        "Bridging Structural Dynamics and Biomechanics: Human Motion Estimation\n  through Footstep-Induced Floor Vibrations",
        "Connecting the dots: Tracing the evolutionary pathway of Polar Ring\n  Galaxies in the cases of NGC 3718, NGC 2685, and NGC 4262",
        "TRADES: Generating Realistic Market Simulations with Diffusion Models",
        "Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go\n  Beyond",
        "Optimizing CNN Architectures for Advanced Thoracic Disease\n  Classification",
        "Evolution of Spots and Stripes in Cellular Automata",
        "Formation of super-Earths and mini-Neptunes from rings of planetesimals",
        "Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and\n  efficient reconstruction of dynamic scene"
      ],
      "abstract":[
        "We introduce a class of short-rate models that exhibit a ``higher for\nlonger'' phenomenon. Specifically, the short-rate is modeled as a general\ntime-homogeneous one-factor Markov diffusion on a finite interval. The lower\nendpoint is assumed to be regular, exit or natural according to boundary\nclassification while the upper endpoint is assumed to be regular with absorbing\nbehavior. In this setting, we give an explicit expression for price of a\nzero-coupon bond (as well as more general interest rate derivatives) in terms\nof the transition density of the short-rate under a new probability measure,\nand the solution of a non-linear ordinary differential equation (ODE). We then\nnarrow our focus to a class of models for which the transition density and ODE\ncan be solved explicitly. For models within this class, we provide conditions\nunder which the lower endpoint is regular, exit and natural. Finally, we study\ntwo specific models -- one in which the lower endpoint is exit and another in\nwhich the lower endpoint is natural. In these two models, we give an explicit\nsolution of transition density of the short-rate as a (generalized)\neigenfunction expansion. We provide plots of the transition density,\n(generalized) eigenfunctions, bond prices and the associated yield curve.",
        "The existence of optimal contracts of the principal-agent problem is a\nlong-standing problem. According to the general framework in Cvitani\\'c et al.\n[2], this existence can be derived from the existence of a classical solution\nto a degenerated fully nonlinear parabolic partial differential equation\nproblem. In this work we consider the simple case with drift control and\nquadratic cost function, then prove the existence of classical solution to that\nPDE.",
        "This paper presents a novel framework for decentralized annuities, aiming to\naddress the limitations of traditional pension systems such as defined\ncontribution (DC) and defined benefit (DB) plans, while providing lifetime\nfinancial support. It sheds light on often ignored pitfalls within current\nretirement schemes and introduces individual rationality properties. The\nresearch delves into various fairness concepts that underpin existing plans,\nemphasizing that decentralized annuities, while meeting similar fairness\ncriteria, offer enhanced flexibility for individual rationality and improved\nsocial welfare for all participants. Using theoretical models and examples, we\ndemonstrate the potential of decentralized annuities to outperform self-managed\nplans (DC) and to produce effects comparable to defined benefit (DB) plans,\nparticularly within larger participant pools. The paper concludes by exploring\nthe managerial implications of decentralized annuities and laying the\ngroundwork for the further advancement of equitable and sustainable\ndecentralized annuity systems.",
        "The G-expectation is a sublinear expectation. It is an important tool for\npricing financial products and managing risk thanks to its ability to deal with\nmodel uncertainty. The problem is how to efficiently quantify it since the\ncommonly used Monte Carlo method does not work. Fortunately, the expectation of\na G-normal random variable can be linked to the viscosity solution of a fully\nnonlinear G-heat equation. In this paper, we propose a novel numerical scheme\nfor the two-dimensional G-heat equation and pay more attention to the case that\nthere exists uncertainty on the correlationship, especially to the case that\nthe correlationship ranges from negative to positive. The scheme is monotonic,\nstable, and convergent. The numerical tests show that the scheme is highly\nefficient.",
        "In an arbitrage-free simple market, we demonstrate that for a class of\nstate-dependent exponential utilities, there exists a unique prediction of the\nrandom risk aversion that ensures the consistency of optimal strategies across\nany time horizon. Our solution aligns with the theory of forward performances,\nwith the added distinction of identifying, among the infinite possible\nsolutions, the one for which the profile remains optimal at all times for the\nmarket-adjusted system of preferences adopted.",
        "We investigate the full dynamics of capital allocation and wealth\ndistribution of heterogeneous agents in a frictional economy during booms and\nbusts using tools from mean-field games. Two groups in our models, namely the\nexpert and the household, are interconnected within and between their classes\nthrough the law of capital processes and are bound by financial constraints.\nSuch a mean-field interaction explains why experts accumulate a lot of capital\nin the good times and reverse their behavior quickly in the bad times even in\nthe absence of aggregate macro-shocks. When common noises from the market are\ninvolved, financial friction amplifies the mean-field effect and leads to\ncapital fire sales by experts. In addition, the implicit interlink between and\nwithin heterogeneous groups demonstrates the slow economic recovery and\ncharacterizes the deviating and fear-of-missing-out (FOMO) behaviors of\nhouseholds compared to their counterparts. Our model also gives a fairly\nexplicit representation of the equilibrium solution without exploiting\ncomplicated numerical approaches.",
        "This study explores the application of Hawkes processes to model\nhigh-frequency data in the context of limit order books. Two distinct\nHawkes-based models are proposed and analyzed: one utilizing exponential\nkernels and the other employing power-law kernels. These models are implemented\nwithin a bivariate framework. The performance of each model is evaluated using\nhigh-frequency trading data, with a focus on their ability to reproduce key\nstatistical properties of limit order books. Through a comprehensive\ncomparison, we identify the strengths and limitations of each kernel type,\nproviding insights into their suitability for modeling high-frequency financial\ndata. Simulations are conducted to validate the models, and the results are\ninterpreted. Based on these insights, a trading strategy is formulated.",
        "This paper provides a dual formulation of the optimal consumption problem\nwith internal multiplicative habit formation. In this problem, the agent\nderives utility from the ratio of consumption to the internal habit component.\nDue to this multiplicative specification of the habit model, the optimal\nconsumption problem is not strictly concave and incorporates irremovable\npath-dependency. As a consequence, standard Lagrangian techniques fail to\nsupply a candidate for the corresponding dual formulation. Using Fenchel's\nDuality Theorem, we manage to identify a candidate formulation and prove that\nit satisfies strong duality. On the basis of this strong duality result, we are\nable to derive duality relations that stipulate how the optimal primal controls\ndepend on the optimal dual controls and vice versa. {Moreover, using the dual\nformulation, we develop an analytical evaluation mechanism to bound the\naccuracy of approximations to the optimal solutions.",
        "We introduce the two-factor Quintic Ornstein-Uhlenbeck model, where\nvolatility is modeled as a polynomial of degree five based on the sum of two\nOrnstein-Uhlenbeck processes driven by the same Brownian Motion, each\nmean-reverting at a different speed. We demonstrate that the Quintic model\neffectively captures the volatility surfaces of SPX and VIX while aligning with\nthe skew-stickiness ratio (SSR) across maturities ranging from a few days to\nover two years. Furthermore, the Quintic model shows consistency with key\nempirical stylized facts, notably reproducing the Zumbach effect.",
        "In this paper, we adopt the least squares Monte Carlo (LSMC) method to price\ntime-capped American options. The aforementioned cap can be an independent\nrandom variable or dependent on asset price at random time. We allow various\ntime caps. In particular, we give an algorithm for pricing the American options\ncapped by the first drawdown epoch. We focus on the geometric L\\'evy market. We\nprove that our estimator converges to the true price as one takes the\ndiscretisation step tending to zero and the number of trajectories going to\ninfinity.",
        "In the Black and Scholes model with proportional transaction costs, the\nLeland strategy allows to asymptotically super-replicate the European Call\noption as the number of revision dates converges to + infinity and the\ntransaction costs rate tends rapidly to 0. This method relies heavily on the\nexplicit expression of the delta-hedging strategy in the Black and Scholes\nmodel where the volatility is enlarged to compensate for the transaction costs.\nWe solve the same problem of super-hedging but for a general model with an\narbitrary fixed number of revision dates and arbitrary fixed transaction costs\nrates. Moreover, our approach does not need the existence of a risk-neutral\nprobability measure and is (almost) model free and easily implementable from\nreal data.",
        "We extend the signature-based primal and dual solutions to the optimal\nstopping problem recently introduced in [Bayer et al.: Primal and dual optimal\nstopping with signatures, to appear in Finance & Stochastics 2025], by\nintegrating deep-signature and signature-kernel learning methodologies. These\napproaches are designed for non-Markovian frameworks, in particular enabling\nthe pricing of American options under rough volatility. We demonstrate and\ncompare the performance within the popular rough Heston and rough Bergomi\nmodels.",
        "Decentralized lending protocols within the decentralized finance ecosystem\nenable the lending and borrowing of crypto-assets without relying on\ntraditional intermediaries. Interest rates in these protocols are set\nalgorithmically and fluctuate according to the supply and demand for liquidity.\nIn this study, we propose an agent-based model tailored to a decentralized\nlending protocol and determine the optimal interest rate model. When the\nresponses of the agents are linear with respect to the interest rate, the\noptimal solution is derived from a system of Riccati-type ODEs. For nonlinear\nbehaviors, we propose a Monte-Carlo estimator, coupled with deep learning\ntechniques, to approximate the optimal solution. Finally, after calibrating the\nmodel using block-by-block data, we conduct a risk-adjusted profit and loss\nanalysis of the liquidity pool under industry-standard interest rate models and\nbenchmark them against the optimal interest rate model.",
        "Low-light enhancement has wide applications in autonomous driving, 3D\nreconstruction, remote sensing, surveillance, and so on, which can\nsignificantly improve information utilization. However, most existing methods\nlack generalization and are limited to specific tasks such as image recovery.\nTo address these issues, we propose \\textbf{Gated-Mechanism Mixture-of-Experts\n(GM-MoE)}, the first framework to introduce a mixture-of-experts network for\nlow-light image enhancement. GM-MoE comprises a dynamic gated weight\nconditioning network and three sub-expert networks, each specializing in a\ndistinct enhancement task. Combining a self-designed gated mechanism that\ndynamically adjusts the weights of the sub-expert networks for different data\ndomains. Additionally, we integrate local and global feature fusion within\nsub-expert networks to enhance image quality by capturing multi-scale features.\nExperimental results demonstrate that the GM-MoE achieves superior\ngeneralization with respect to 25 compared approaches, reaching\nstate-of-the-art performance on PSNR on 5 benchmarks and SSIM on 4 benchmarks,\nrespectively.",
        "Systematic reviews are time-consuming endeavors. Historically speaking,\nknowledgeable humans have had to screen and extract data from studies before it\ncan be analyzed. However, large language models (LLMs) hold promise to greatly\naccelerate this process. After a pilot study which showed great promise, we\ninvestigated the use of freely available LLMs for extracting data for\nsystematic reviews. Using three different LLMs, we extracted 24 types of data,\n9 explicitly stated variables and 15 derived categorical variables, from 112\nstudies that were included in a published scoping review. Overall we found that\nGemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Large 2 performed reasonably\nwell, with 71.17%, 72.14%, and 62.43% of data extracted being consistent with\nhuman coding, respectively. While promising, these results highlight the dire\nneed for a human-in-the-loop (HIL) process for AI-assisted data extraction. As\na result, we present a free, open-source program we developed (AIDE) to\nfacilitate user-friendly, HIL data extraction with LLMs.",
        "Shipping container based modular architectures provide design flexibility in\ndata centers with building blocks to expand the network as and when needed. In\nthis paper, high capacity Modular Data Center (MDC) network architecture with\nRich Inter Connections named MODRIC is proposed. MODRIC is a cost-effective\nswitch-centric network design which allows building a flexible MDC network with\ncommodity switches. It uses an inter-container connectivity similar to the\nstructure of generalized hypercube in order to provide high inter-container\nbandwidth. Further, a hybrid Clos topology is used to build the container\nnetwork. MODRIC is highly suitable for cost effectively building mega data\ncenters requiring high throughput capacity and resilience against failures.\nThis paper presents the proposed architecture, discusses its relevant\nproperties, and proposes suitable addressing, routing and network construction\nschemes. The paper also presents comparative studies on its cost and\nperformance with existing network topologies.",
        "Recent work has sought to quantify large language model uncertainty to\nfacilitate model control and modulate user trust. Previous works focus on\nmeasures of uncertainty that are theoretically grounded or reflect the average\novert behavior of the model. In this work, we investigate a variety of\nuncertainty measures, in order to identify measures that correlate with human\ngroup-level uncertainty. We find that Bayesian measures and a variation on\nentropy measures, top-k entropy, tend to agree with human behavior as a\nfunction of model size. We find that some strong measures decrease in\nhuman-similarity with model size, but, by multiple linear regression, we find\nthat combining multiple uncertainty measures provide comparable human-alignment\nwith reduced size-dependency.",
        "Large diffusion models demonstrate remarkable zero-shot capabilities in novel\nview synthesis from a single image. However, these models often face challenges\nin maintaining consistency across novel and reference views. A crucial factor\nleading to this issue is the limited utilization of contextual information from\nreference views. Specifically, when there is an overlap in the viewing frustum\nbetween two views, it is essential to ensure that the corresponding regions\nmaintain consistency in both geometry and appearance. This observation leads to\na simple yet effective approach, where we propose to use epipolar geometry to\nlocate and retrieve overlapping information from the input view. This\ninformation is then incorporated into the generation of target views,\neliminating the need for training or fine-tuning, as the process requires no\nlearnable parameters. Furthermore, to enhance the overall consistency of\ngenerated views, we extend the utilization of epipolar attention to a\nmulti-view setting, allowing retrieval of overlapping information from the\ninput view and other target views. Qualitative and quantitative experimental\nresults demonstrate the effectiveness of our method in significantly improving\nthe consistency of synthesized views without the need for any fine-tuning.\nMoreover, This enhancement also boosts the performance of downstream\napplications such as 3D reconstruction. The code is available at\nhttps:\/\/github.com\/botaoye\/ConsisSyn.",
        "V902 Mon is one of a few eclipsing Intermediate Polars (IPs), and show deep\neclipses in the optical lightcurves. The presence of a strong Fe K$\\alpha$\nfluorescence line in its X-ray spectrum and its low X-ray flux compared to\nother IPs suggests significant absorption, most likely from an accretion disk.\nIn an observation carried out using the Nuclear Spectroscopic Telescope Array\n(NuSTAR), we confirm the presence of an X-ray eclipse in the energy resolved\nlightcurves, coincident with the optical AAVSO\/CV-band lightcurves. Broadband\nX-ray spectral analysis using NuSTAR and XMM-Newton observations confirm a\nstrong absorption N$_{H}$ $\\sim 10^{23}$ cm$^{-2}$ local to the source, along\nwith a high equivalent width of about 0.7 keV for a Fe K$\\alpha$ fluorescence\nline. We interpret this using a model similar to an Accretion Disk Corona\nsource, which have a very high inclination and the compact object is heavily\nobscured by the body of the accretion disk. We propose that the primary X-rays\nfrom the accretion column in V902 Mon is hidden from our direct view at all\ntimes by the accretion disk. In this scenario, the observed scattered X-rays\nindicate substantial absorption of direct X-rays by the accretion disk.\nAdditionally, a strong Fe fluorescence line suggests reprocessing of the\nradiation by a more extended region, such as the pre-shock region, which could\nbe located a few white dwarf radii above the orbital plane.",
        "To veryfy \"hot supreconductivity\" recently proposed in lanthanum\nhydride-based compounds, we explored thermodynamically stable and\nsuperconducting phases in the lanthanum (La)-platinum (Pt)-hydrogen (H) ternary\nsystem at 20 GPa using an evolutionary construction scheme of a\nformation-enthalpy convex hull, universal neural network potential\ncalculations, and density functional theory calculations. Although we found no\nevidence of the hot superconductivity in this ternary system, we predicted a\nunique compound, LaPtH$_{ 6 }$, which has equilateral triangular H$_{ 3 }$\nunits nearly forming a two-dimensional kagome lattice between La and Pt layers\nand shows the superconductivity at 18.67 K. This structure is dynamically\nstable from ambient pressure to at least 200 GPa and the superconducting\ncritical temperature increases from 13.51 to 40.63 K.",
        "Quantitative estimation of human joint motion in daily living spaces is\nessential for early detection and rehabilitation tracking of\nneuromusculoskeletal disorders (e.g., Parkinson's) and mitigating trip and fall\nrisks for older adults. Existing approaches involve monitoring devices such as\ncameras, wearables, and pressure mats, but have operational constraints such as\ndirect line-of-sight, carrying devices, and dense deployment. To overcome these\nlimitations, we leverage gait-induced floor vibration to estimate lower-limb\njoint motion (e.g., ankle, knee, and hip flexion angles), allowing\nnon-intrusive and contactless gait health monitoring in people's living spaces.\nTo overcome the high uncertainty in lower-limb movement given the limited\ninformation provided by the gait-induced floor vibrations, we formulate a\nphysics-informed graph to integrate domain knowledge of gait biomechanics and\nstructural dynamics into the model. Specifically, different types of nodes\nrepresent heterogeneous information from joint motions and floor vibrations;\nTheir connecting edges represent the physiological relationships between joints\nand forces governed by gait biomechanics, as well as the relationships between\nforces and floor responses governed by the structural dynamics. As a result,\nour model poses physical constraints to reduce uncertainty while allowing\ninformation sharing between the body and the floor to make more accurate\npredictions. We evaluate our approach with 20 participants through a real-world\nwalking experiment. We achieved an average of 3.7 degrees of mean absolute\nerror in estimating 12 joint flexion angles (38% error reduction from\nbaseline), which is comparable to the performance of cameras and wearables in\ncurrent medical practices.",
        "Polar Ring Galaxies (PRGs) are a unique class of galaxies characterised by a\nring of gas and stars orbiting nearly orthogonal to the main body. This study\ndelves into the evolutionary trajectory of PRGs using the exemplary trio of NGC\n3718, NGC 2685, and NGC 4262. We investigate the distinct features of PRGs by\nanalysing their ring and host components to reveal their unique characteristics\nthrough Spectral Energy Distribution (SED) fitting. Using CIGALE, we performed\nSED fitting to independently analyse the ring and host spatially resolved\nregions, marking the first decomposed SED analysis for PRGs, which examines\nstellar populations using high-resolution observations from AstroSat UVIT at a\nresolved scale. The UV-optical surface profiles provide an initial idea that\ndistinct patterns in the galaxies, with differences in FUV and NUV, suggest\nthree distinct stages of ring evolution in the selected galaxies. The study of\nresolved-scale stellar regions reveals that the ring regions are generally\nyounger than their host galaxies, with the age disparity progressively\ndecreasing along the evolutionary sequence from NGC 3718 to NGC 4262. Star\nformation rates (SFR) also exhibit a consistent pattern, with higher SFR in the\nring of NGC 3718 compared to the others, and a progressive decrease through NGC\n2685 and NGC 4262. Finally, the representation of the galaxies in the HI gas\nfraction versus the NUV- r plane supports the idea that they are in three\ndifferent evolutionary stages of PRG evolution, with NGC 3718 in the initial\nstage, NGC 2685 in the intermediate stage, and NGC 4262 representing the final\nstage. NGC 3718, NGC 2685, and NGC 4262 represent different stages of this\nevolution, highlighting the dynamic nature of PRGs and emphasising the\nimportance of studying their evolutionary processes to gain insights into\ngalactic formation and evolution.",
        "Financial markets are complex systems characterized by high statistical\nnoise, nonlinearity, and constant evolution. Thus, modeling them is extremely\nhard. We address the task of generating realistic and responsive Limit Order\nBook (LOB) market simulations, which are fundamental for calibrating and\ntesting trading strategies, performing market impact experiments, and\ngenerating synthetic market data. Previous works lack realism, usefulness, and\nresponsiveness of the generated simulations. To bridge this gap, we propose a\nnovel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB\nSimulations (TRADES). TRADES generates realistic order flows conditioned on the\nstate of the market, leveraging a transformer-based architecture that captures\nthe temporal and spatial characteristics of high-frequency market data. There\nis a notable absence of quantitative metrics for evaluating generative market\nsimulation models in the literature. To tackle this problem, we adapt the\npredictive score, a metric measured as an MAE, by training a stock price\npredictive model on synthetic data and testing it on real data. We compare\nTRADES with previous works on two stocks, reporting an x3.27 and x3.47\nimprovement over SoTA according to the predictive score, demonstrating that we\ngenerate useful synthetic market data for financial downstream tasks. We assess\nTRADES's market simulation realism and responsiveness, showing that it\neffectively learns the conditional data distribution and successfully reacts to\nan experimental agent, giving sprout to possible calibrations and evaluations\nof trading strategies and market impact experiments. We developed DeepMarket,\nthe first open-source Python framework for market simulation with deep\nlearning. Our repository includes a synthetic LOB dataset composed of TRADES's\ngenerates simulations. We release the code at\ngithub.com\/LeonardoBerti00\/DeepMarket.",
        "Large language models (LLMs) should undergo rigorous audits to identify\npotential risks, such as copyright and privacy infringements. Once these risks\nemerge, timely updates are crucial to remove undesirable responses, ensuring\nlegal and safe model usage. It has spurred recent research into LLM unlearning,\nfocusing on erasing targeted undesirable knowledge without compromising the\nintegrity of other, non-targeted responses. Existing studies have introduced\nvarious unlearning objectives to pursue LLM unlearning without necessitating\ncomplete retraining. However, each of these objectives has unique properties,\nand no unified framework is currently available to comprehend them thoroughly.\nTo fill the gap, we propose a toolkit of the gradient effect (G-effect),\nquantifying the impacts of unlearning objectives on model performance from a\ngradient perspective. A notable advantage is its broad ability to detail the\nunlearning impacts from various aspects across instances, updating steps, and\nLLM layers. Accordingly, the G-effect offers new insights into identifying\ndrawbacks of existing unlearning objectives, further motivating us to explore a\nseries of new solutions for their mitigation and improvements. Finally, we\noutline promising directions that merit further studies, aiming at contributing\nto the community to advance this important field.",
        "Machine learning, particularly convolutional neural networks (CNNs), has\nshown promise in medical image analysis, especially for thoracic disease\ndetection using chest X-ray images. In this study, we evaluate various CNN\narchitectures, including binary classification, multi-label classification, and\nResNet50 models, to address challenges like dataset imbalance, variations in\nimage quality, and hidden biases. We introduce advanced preprocessing\ntechniques such as principal component analysis (PCA) for image compression and\npropose a novel class-weighted loss function to mitigate imbalance issues. Our\nresults highlight the potential of CNNs in medical imaging but emphasize that\nissues like unbalanced datasets and variations in image acquisition methods\nmust be addressed for optimal model performance.",
        "Cellular automata are computers, similar to Turing machines. The main\ndifference is that Turing machines use a one-dimensional tape, whereas cellular\nautomata use a two-dimensional grid. The best-known cellular automaton is the\nGame of Life, which is a universal computer. It belongs to a family of cellular\nautomata with 262,144 members. Playing the Game of Life generally involves\nengineering; that is, assembling a device composed of various parts that are\ncombined to achieve a specific intended result. Instead of engineering cellular\nautomata, we propose evolving cellular automata. Evolution applies mutation and\nselection to a population of organisms. If a mutation increases the fitness of\nan organism, it may have many descendants, displacing the less fit organisms.\nUnlike engineering, evolution does not work towards an imagined goal. Evolution\nworks towards increasing fitness, with no expectations about the specific form\nof the final result. Mutation, selection, and fitness yield structures that\nappear to be more organic and life-like than engineered structures. In our\nexperiments, the patterns resulting from evolving cellular automata look much\nlike the spots on leopards and the stripes on tigers.",
        "The solar system planetary architecture has been proposed to be consistent\nwith the terrestrial and giant planets forming from material rings at ~1 au and\n~5 au, respectively. Here, we show that super-Earths and mini-Neptunes may\nshare a similar formation pathway. In our simulations conducted with a disk\nalpha-viscosity of 4e-3, super-Earths accrete from rings of rocky material in\nthe inner disk, growing predominantly via planetesimal accretion. Mini-Neptunes\nprimarily originate from rings located beyond the water snowline, forming via\npebble accretion. Our simulations broadly match the period-ratio distribution,\nthe intra-system size uniformity, and the planet multiplicity distribution of\nexoplanets. The radius valley constrains the typical total mass available for\nrocky planet formation to be less than 3-6 Earth masses. Our results predict\nthat planets at ~1 au in systems with close-in super-Earths and mini-Neptunes\nare predominantly water-rich. Though relatively uncommon, at ~1% level, such\nsystems might also host rocky Earth-sized planets in the habitable zone that\nunderwent late giant impacts, akin to the Moon-forming event.",
        "Novel view synthesis has long been a practical but challenging task, although\nthe introduction of numerous methods to solve this problem, even combining\nadvanced representations like 3D Gaussian Splatting, they still struggle to\nrecover high-quality results and often consume too much storage memory and\ntraining time. In this paper we propose Swift4D, a divide-and-conquer 3D\nGaussian Splatting method that can handle static and dynamic primitives\nseparately, achieving a good trade-off between rendering quality and\nefficiency, motivated by the fact that most of the scene is the static\nprimitive and does not require additional dynamic properties. Concretely, we\nfocus on modeling dynamic transformations only for the dynamic primitives which\nbenefits both efficiency and quality. We first employ a learnable decomposition\nstrategy to separate the primitives, which relies on an additional parameter to\nclassify primitives as static or dynamic. For the dynamic primitives, we employ\na compact multi-resolution 4D Hash mapper to transform these primitives from\ncanonical space into deformation space at each timestamp, and then mix the\nstatic and dynamic primitives to produce the final output. This\ndivide-and-conquer method facilitates efficient training and reduces storage\nredundancy. Our method not only achieves state-of-the-art rendering quality\nwhile being 20X faster in training than previous SOTA methods with a minimum\nstorage requirement of only 30MB on real-world datasets. Code is available at\nhttps:\/\/github.com\/WuJH2001\/swift4d."
      ]
    }
  },
  {
    "id":2411.00575,
    "research_type":"basic",
    "start_id":"b0",
    "start_title":"Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem",
    "start_abstract":"Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b22"
      ],
      "title":[
        "Vertical slice modelling of nonlinear Eady waves using a compatible finite element method"
      ],
      "abstract":[
        "A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "Etude du graphe divisoriel 6",
        "Singularities in Bayesian Inference: Crucial or Overstated?",
        "Comparative Analysis of Two-Stage Distributionally Robust Optimization\n  over 1-Wasserstein and 2-Wasserstein Balls",
        "Intrinsic Donaldson-Thomas theory. II. Stability measures and invariants",
        "Joint modeling of longitudinal HRQoL data accounting for the risk of\n  competing dropouts",
        "Disturbance-to-state stabilization by output feedback of nonlinear ODE\n  cascaded with a reaction-diffusion equation",
        "Circular sorting",
        "Global well-posedness and stability of three-dimensional isothermal\n  Euler equations with damping",
        "Direct sampling from conditional distributions by sequential maximum\n  likelihood estimations",
        "Decay of mass for a semilinear heat equation with mixed local-nonlocal\n  operators",
        "Isoparametric foliations and bounded geometry",
        "Operator $\\ell^\\infty \\to \\ell^\\infty$ norm of products of random\n  matrices",
        "Major-minor mean field games: common noise helps",
        "Decentralized RISE-based Control for Exponential Heterogeneous\n  Multi-Agent Target Tracking of Second-Order Nonlinear Systems",
        "On the Origin and Fate of Our Universe",
        "Trend-Aware Supervision: On Learning Invariance for Semi-Supervised\n  Facial Action Unit Intensity Estimation",
        "Data-Driven Decision Making for Enhancing Small-Signal Stability in\n  Hybrid AC\/DC Grids Through Converter Control Role Assignment",
        "Multi-Year-to-Decadal Temperature Prediction using a Machine Learning\n  Model-Analog Framework",
        "Decentralized Inference for Spatial Data Using Low-Rank Models",
        "Quadratic quasinormal modes at null infinity on a Schwarzschild\n  spacetime",
        "On the Adversarial Vulnerabilities of Transfer Learning in Remote\n  Sensing",
        "Stress field in the vicinity of a bubble\/sphere moving in a dilute\n  surfactant solution",
        "A Hybrid Swarm Intelligence Approach for Optimizing Multimodal Large\n  Language Models Deployment in Edge-Cloud-based Federated Learning\n  Environments",
        "Making Puzzle Pieces Fit or Reshaping MiMiC for Multiscale Simulations\n  with CP2K and More",
        "To Hedge or Not to Hedge: Optimal Strategies for Stochastic Trade Flow\n  Management",
        "Generalized Venn and Venn-Abers Calibration with Applications in\n  Conformal Prediction",
        "Keyword Search in the Deep Web",
        "Holistic Semantic Representation for Navigational Trajectory Generation"
      ],
      "abstract":[
        "The divisor graph is the non oriented graph whose vertices are the positive\nintegers, and edges are the {a,b} such that a divides b or b divides a. Let\nF(x,y) be the maximum number of integers<= x belonging in one of y pairwise\ndisjoint simple path of the restriction of the divisor graph to integers <= x.\nOur main result is the following. There exist two real numbers K >c>0 such that\nfor every x and y with x>=2y>=2 , we have cx \/ log(x\/y) <= F(x,y) <= Kx \/\nlog(x\/y). It answers a question of Erd\\\"os.",
        "Over the past two decades, shrinkage priors have become increasingly popular,\nand many proposals can be found in the literature. These priors aim to shrink\nsmall effects to zero while maintaining true large effects. Horseshoe-type\npriors have been particularly successful in various applications, mainly due to\ntheir computational advantages. However, there is no clear guidance on choosing\nthe most appropriate prior for a specific setting. In this work, we propose a\nframework that encompasses a large class of shrinkage distributions, including\npriors with and without a singularity at zero. By reframing such priors in the\ncontext of reliability theory and wealth distributions, we provide insights\ninto the prior parameters and shrinkage properties. The paper's key\ncontributions are based on studying the folded version of such distributions,\nwhich we refer to as the Gambel distribution. The Gambel can be rewritten as\nthe ratio between a Generalised Gamma and a Generalised Beta of the second\nkind. This representation allows us to gain insights into the behaviours near\nthe origin and along the tails, compute measures to compare their\ndistributional properties, derive consistency results, devise MCMC schemes for\nposterior inference and ultimately provide guidance on the choice of the\nhyperparameters.",
        "This paper investigates advantages of using 2-Wasserstein ambiguity sets over\n1-Wasserstein sets in two-stage distributionally robust optimization with\nright-hand side uncertainty. We examine the worst-case distributions within 1-\nand 2-Wasserstein balls under both unrestricted and nonnegative orthant\nsupports, highlighting a pathological behavior arising in 1-Wasserstein balls.\nClosed-form solutions for a single-scenario newsvendor problem illustrate that\n2-Wasserstein balls enable more informed decisions. Additionally, a\npenalty-based dual interpretation suggests that 2-Wasserstein balls may\noutperform 1-Wasserstein balls across a broader range of Wasserstein radii,\neven with general support sets.",
        "This is the second paper in a series on intrinsic Donaldson-Thomas theory, a\nframework for studying the enumerative geometry of general algebraic stacks.\n  In this paper, we present the construction of Donaldson-Thomas invariants for\ngeneral $(-1)$-shifted symplectic derived Artin stacks, generalizing the\nconstructions of Joyce-Song and Kontsevich-Soibelman for moduli stacks of\nobjects in $3$-Calabi-Yau abelian categories. Our invariants are defined using\nrings of motives, and depend intrinsically on the stack, together with a set of\ncombinatorial data similar to a stability condition, called a stability measure\non the component lattice of the stack. For our invariants to be well-defined,\nwe prove a generalization of Joyce's no-pole theorem to general stacks, using a\nsimpler and more conceptual argument than the original proof in the abelian\ncategory case.\n  Further properties and applications of these invariants, such as\nwall-crossing formulae, will be discussed in a forthcoming paper.",
        "In cancer clinical trials, health-related quality of life (HRQoL) is an\nimportant endpoint, providing information about patients' well-being and daily\nfunctioning. However, missing data due to premature dropout can lead to biased\nestimates, especially when dropouts are informative. This paper introduces the\nextJMIRT approach, a novel tool that efficiently analyzes multiple longitudinal\nordinal categorical data while addressing informative dropout. Within a joint\nmodeling framework, this approach connects a latent variable, derived from\nHRQoL data, to cause-specific hazards of dropout. Unlike traditional joint\nmodels, which treat longitudinal data as a covariate in the survival submodel,\nour approach prioritizes the longitudinal data and incorporates the log\nbaseline dropout risks as covariates in the latent process. This leads to a\nmore accurate analysis of longitudinal data, accounting for potential effects\nof dropout risks. Through extensive simulation studies, we demonstrate that\nextJMIRT provides robust and unbiased parameter estimates and highlight the\nimportance of accounting for informative dropout. We also apply this\nmethodology to HRQoL data from patients with progressive glioblastoma,\nshowcasing its practical utility.",
        "In this paper, we analyze the output stabilization problem for cascaded\nnonlinear ODE with $1-d$ heat diffusion equation affected by both in-domain and\nboundary perturbations. We assume that the only available part of states is the\nfirst components of the ODE-subsystem and one boundary of the heat-subsystem.\nThe particularity of this system is two folds i) it contains a nonlinear\nadditive term in the ODE-subsystem, and ii) it is affected by both boundary and\nin-domain perturbations signals.\n  For such a system, and unlike the existing works, we succeeded to design an\noutput observer-based feedback that guarantees not only asymptotic\nstabilization result but also a globally {\\it disturbance-to-state\nstabilization} for our cascaded system. The output feedback is designed using\nan adequate backstepping transformation recently introduced for coupled\nODE-heat equations combined with high-gain observer and high-gain controller.",
        "We determine the maximal number of steps required to sort $n$ labeled points\non a circle by adjacent swaps. Lower bounds for sorting by all swaps, not\nnecessarily adjacent, are given as well.",
        "The global well-posedness and stability of solutions to the three-dimensional\ncompressible Euler equations with damping is a longstanding open problem. This\nproblem was addressed in \\cite{WY, STW} in the isentropic regime (i.e.\n$\\gamma>1$) for small smooth solutions. In this paper, we prove the global\nwell-posedness and stability of smooth solutions to the three-dimensional\nisothermal Euler equations ($\\gamma=1$) with damping for some partially large\ninitial values, i.e., $\\|(\\rho_0-\\rho_*,u_0)\\|_{L^2}$ could be large, but\n$\\|D^3(\\rho_0-\\rho_*,u_0)\\|_{L^2}$ is necessarily small. Moreover, the optimal\nalgebraic decay rate is also obtained.\n  The proof is based on the observation that the isothermal Euler equations\nwith damping possess a good structure so that the equations can be reduced into\na symmetrically hyperbolic system with partial damping, i.e., \\eqref{au}. In\nthe new system, all desired a priori estimates can be obtained under the\nassumption that $\\int_0^T(\\|\\nabla \\mathrm{ln}\\rho\\|_{L^{\\infty}}+\\|\\nabla\nu\\|_{L^{\\infty}}) \\mathrm{d}t $ is small. The assumption can be verified\nthrough the low-high frequency analysis via Fourier transformation under the\ncondition that $\\|D^3(\\rho_0-\\rho_*,u_0)\\|_{L^2}$ is small, but\n$\\|(\\rho_0-\\rho_*,u_0)\\|_{L^2}$ could be large.",
        "We can directly sample from the conditional distribution of any log-affine\nmodel. The algorithm is a Markov chain on a bounded integer lattice, and its\ntransition probability is the ratio of the UMVUE (uniformly minimum variance\nunbiased estimator) of the expected counts to the total number of counts. The\ncomputation of the UMVUE accounts for most of the computational cost, which\nmakes the implementation challenging. Here, we investigated an approximate\nalgorithm that replaces the UMVUE with the MLE (maximum likelihood estimator).\nAlthough it is generally not exact, it is efficient and easy to implement; no\nprior study is required, such as about the connection matrices of the holonomic\nideal in the original algorithm.",
        "In this paper, we are concerned with the Cauchy problem for the\nreaction-diffusion equation $\\partial_t u+t^\\beta\\mathcal{L} u= - h(t)u^p$\nposed on $\\mathbb{R}^N$, driven by the mixed local-nonlocal operator\n$\\mathcal{L}=-\\Delta+(-\\Delta)^{\\alpha\/2}$, $\\alpha\\in(0,2)$, and supplemented\nwith a nonnegative integrable initial data, where $p>1$, $\\beta\\geq 0$, and\n$h:(0,\\infty)\\to(0,\\infty)$ is a locally integrable function. We study the\nlarge time behavior of non-negative solutions and show that the nonlinear term\ndetermines the large time asymptotic for $p\\leq 1+{\\alpha}\/{N(\\beta+1)},$ while\nthe classical\/anomalous diffusion effects win if $p>1+{\\alpha}\/{N(\\beta+1)}$.",
        "We prove that there are only finitely many isoparametrically foliated closed\nconnected Riemannian manifolds with bounded geometry, fixed dimension $n\\neq5$,\nand finite fundamental group, up to foliated diffeomorphism. In addition, we\nconstruct various infinite families of isoparametric foliations that are\nmutually not foliated diffeomorphic, for instance on a fixed sphere.",
        "We study the $\\ell^\\infty \\to \\ell^\\infty$ operator norm of products of\nindependent random matrices with independent and identically distributed\nentries. For $n$-by-$n$ matrices whose entries are centered, have unit\nvariance, and have a finite moment of order $4\\alpha$ for some $\\alpha > 1$, we\nfind that the operator norm of the product of $p$ matrices behaves\nasymptotically like $n^{\\frac {p+1}{2}}\\sqrt{2\/\\pi}$. The case of products of\npossibly non-square matrices with possibly non-centered entries is also\ncovered.",
        "The objective of this work is to study the existence, uniqueness, and\nstability of equilibria in mean field games involving a major player and a\ncontinuum of minor players over finite intervals of arbitrary length. Following\nearlier articles addressing similar questions in the context of classical mean\nfield games, the cost functions for the minor players are assumed to satisfy\nthe Lasry-Lions monotonicity condition. In this contribution, we demonstrate\nthat if, in addition to the monotonicity condition, the intensity of the\n(Brownian) noise driving the major player is sufficiently high, then -- under\nfurther mild regularity assumptions on the coefficients -- existence,\nuniqueness, and stability of equilibria are guaranteed. A key challenge is to\nshow that the threshold (beyond which the noise intensity must be taken) can be\nchosen independently of the length of the time interval over which the game is\ndefined. Building on the stability properties thus established, we further show\nthat the associated system of master equations admits a unique classical\nsolution. To the best of our knowledge, this is the first result of its kind\nfor major-minor mean field games defined over intervals of arbitrary length.",
        "This work presents a decentralized implementation of a Robust Integral of the\nSign of the Error (RISE) controller for multi-agent target tracking problems\nwith exponential convergence guarantees. Previous RISE-based approaches for\nmulti-agent systems required 2-hop communication, limiting practical\napplicability. New insights from a Lyapunov-based design-analysis approach are\nused to eliminate the need for multi-hop communication required in previous\nliterature, while yielding exponential target tracking. The new insights\ninclude the development of a new P-function which is developed which works in\ntandem with the inclusion of the interaction matrix in the Lyapunov function.\nNonsmooth Lyapunov-based stability analysis methods are used to yield\nsemi-global exponential convergence to the target agent state despite the\npresence of bounded disturbances with bounded derivatives. The resulting\noutcome is a controller that achieves exponential target tracking with only\nlocal information exchange between neighboring agents.",
        "This brief review, intended for high energy and astrophysics researchers,\nexplores the implications of recent theoretical advances in string theory and\nthe Swampland program for understanding bounds on the structure of positive\npotentials allowed in quantum gravity. This has a bearing on both inflationary\nmodels for the early universe as well as the fate of our universe. The paper\nincludes a review of the dS conjecture as well as the TransPlanckian Censorship\nConjecture (TCC) and its relation to the species scale. We provide evidence for\nthese principles as well as what they may lead to in terms of phenomenological\npredictions. (Talk presented at Lemaitre Conference 2024)",
        "With the increasing need for facial behavior analysis, semi-supervised AU\nintensity estimation using only keyframe annotations has emerged as a practical\nand effective solution to relieve the burden of annotation. However, the lack\nof annotations makes the spurious correlation problem caused by AU\nco-occurrences and subject variation much more prominent, leading to non-robust\nintensity estimation that is entangled among AUs and biased among subjects. We\nobserve that trend information inherent in keyframe annotations could act as\nextra supervision and raising the awareness of AU-specific facial appearance\nchanging trends during training is the key to learning invariant AU-specific\nfeatures. To this end, we propose \\textbf{T}rend-\\textbf{A}ware\n\\textbf{S}upervision (TAS), which pursues three kinds of trend awareness,\nincluding intra-trend ranking awareness, intra-trend speed awareness, and\ninter-trend subject awareness. TAS alleviates the spurious correlation problem\nby raising trend awareness during training to learn AU-specific features that\nrepresent the corresponding facial appearance changes, to achieve intensity\nestimation invariance. Experiments conducted on two commonly used AU benchmark\ndatasets, BP4D and DISFA, show the effectiveness of each kind of awareness. And\nunder trend-aware supervision, the performance can be improved without extra\ncomputational or storage costs during inference.",
        "Hybrid AC\/DC transmission grids incorporate Modular Multilevel Converters\nfunctioning as Interconnecting Power Converters (IPCs). The control role\nassigned to each converter significantly influences grid dynamics.\nTraditionally, these converters operate with static control roles, but recent\nstudies have proposed scheduling their roles based on day-ahead forecasts to\nenhance stability performance. However, in systems with high renewable energy\npenetration, forecast deviations can render scheduled control assignments\nsuboptimal or even lead to instability. To address this challenge, this work\nproposes an online scheduling recalculation algorithm that dynamically adapts\nIPC control roles during system operation. The approach leverages a data-driven\nmulti-criteria decision-making framework, integrating surrogate models of\nconventional small-signal stability analysis tools to enable a fast computation\nof system stability and stability performance indicators.",
        "Multi-year-to-decadal climate prediction is a key tool in understanding the\nrange of potential regional and global climate futures. Here, we present a\nframework that combines machine learning and analog forecasting for predictions\non these timescales. A neural network is used to learn a mask, specific to a\nregion and lead time, with global weights based on relative importance as\nprecursors to the evolution of that prediction target. A library of\nmask-weighted model states, or potential analogs, are then compared to a single\nmask-weighted observational state. The known future of the best matching\npotential analogs serve as the prediction for the future of the observational\nstate. We match and predict 2-meter temperature using the Berkeley Earth\nSurface Temperature dataset for observations, and a set of CMIP6 models as the\nanalog library. We find improved performance over traditional analog methods\nand initialized decadal predictions.",
        "Advancements in information technology have enabled the creation of massive\nspatial datasets, driving the need for scalable and efficient computational\nmethodologies. While offering viable solutions, centralized frameworks are\nlimited by vulnerabilities such as single-point failures and communication\nbottlenecks. This paper presents a decentralized framework tailored for\nparameter inference in spatial low-rank models to address these challenges. A\nkey obstacle arises from the spatial dependence among observations, which\nprevents the log-likelihood from being expressed as a summation-a critical\nrequirement for decentralized optimization approaches. To overcome this\nchallenge, we propose a novel objective function leveraging the evidence lower\nbound, which facilitates the use of decentralized optimization techniques. Our\napproach employs a block descent method integrated with multi-consensus and\ndynamic consensus averaging for effective parameter optimization. We prove the\nconvexity of the new objective function in the vicinity of the true parameters,\nensuring the convergence of the proposed method. Additionally, we present the\nfirst theoretical results establishing the consistency and asymptotic normality\nof the estimator within the context of spatial low-rank models. Extensive\nsimulations and real-world data experiments corroborate these theoretical\nfindings, showcasing the robustness and scalability of the framework.",
        "The ringdown of perturbed black holes has been studied since the 1970s, but\nuntil recently, studies have focused on linear perturbations. There is now\nburgeoning interest in nonlinear perturbative effects during ringdown. Here,\nusing a hyperboloidal framework, we provide a complete treatment of linear and\nquadratic quasinormal modes (QNMs and QQNMs) in second-order perturbation\ntheory, in Schwarzschild spacetime. We include novel methods for extracting\nQNMs and QQNMs amplitudes using a Laplace transform treatment, allowing for the\ninclusion of arbitrary initial data. We produce both time- and frequency-domain\ncodes. From these codes, we present new results further exploring the\nunforeseen dependence of QQNMs amplitudes on the parity of the progenitor\nsystem, as demonstrated in our letter [Phys. Rev. Lett. 134, 061401 (2025)].\nOur numerical results are restricted to perturbations of a Schwarzschild black\nhole, but our methods extend straightforwardly to the astrophysically realistic\ncase of a Kerr black hole.",
        "The use of pretrained models from general computer vision tasks is widespread\nin remote sensing, significantly reducing training costs and improving\nperformance. However, this practice also introduces vulnerabilities to\ndownstream tasks, where publicly available pretrained models can be used as a\nproxy to compromise downstream models. This paper presents a novel Adversarial\nNeuron Manipulation method, which generates transferable perturbations by\nselectively manipulating single or multiple neurons in pretrained models.\nUnlike existing attacks, this method eliminates the need for domain-specific\ninformation, making it more broadly applicable and efficient. By targeting\nmultiple fragile neurons, the perturbations achieve superior attack\nperformance, revealing critical vulnerabilities in deep learning models.\nExperiments on diverse models and remote sensing datasets validate the\neffectiveness of the proposed method. This low-access adversarial neuron\nmanipulation technique highlights a significant security risk in transfer\nlearning models, emphasizing the urgent need for more robust defenses in their\ndesign when addressing the safety-critical remote sensing tasks.",
        "In this study, we experimentally investigate the stress field around a bubble\nrising in a dilute surfactant solution (20 < Re < 220, high Peclet numbers)\nwhose surface gradually becomes contaminated, and compare it with that around a\nsphere free from surface contamination. We employ a newly developed\npolarization measurement technique, highly sensitive to stress fields near\ninterfaces. First, we validate this method by measuring the flow around a solid\nsphere settling at Re = 120 and comparing results with numerical predictions,\nconfirming its accuracy. We then measure the stress field around a bubble whose\ndrag force transitions from that of a clean interface to that of a rigid\ninterface within the observation region. The stress near the bubble's front\nresembles that of a clean bubble, while the rear behaves like a solid sphere.\nBetween these regions, a discontinuous phase retardation near the cap angle\nindicates a transition from slip to no-slip boundary conditions. Axisymmetric\nstress reconstruction reveals localized stress spike at the cap angle, which\nshifts as surfactant accumulates and increases the drag. Remarkably, the\nmeasured cap angle versus normalized drag coefficient agrees well with\nnumerical simulations at Re = 100 (Cuenot et al. 1997) and shows only a slight\ndeviation from the creeping-flow stagnant cap model (Sadhal and Johnson 1983).\nThis work demonstrates that polarization-based stress field measurements\neffectively capture the interplay between surface contamination and\nhydrodynamics at intermediate Reynolds numbers.",
        "The combination of Federated Learning (FL), Multimodal Large Language Models\n(MLLMs), and edge-cloud computing enables distributed and real-time data\nprocessing while preserving privacy across edge devices and cloud\ninfrastructure. However, the deployment of MLLMs in FL environments with\nresource-constrained edge devices presents significant challenges, including\nresource management, communication overhead, and non-IID data. To address these\nchallenges, we propose a novel hybrid framework wherein MLLMs are deployed on\nedge devices equipped with sufficient resources and battery life, while the\nmajority of training occurs in the cloud. To identify suitable edge devices for\ndeployment, we employ Particle Swarm Optimization (PSO), and Ant Colony\nOptimization (ACO) is utilized to optimize the transmission of model updates\nbetween edge and cloud nodes. This proposed swarm intelligence-based framework\naims to enhance the efficiency of MLLM training by conducting extensive\ntraining in the cloud and fine-tuning at the edge, thereby reducing energy\nconsumption and communication costs. Our experimental results show that the\nproposed method significantly improves system performance, achieving an\naccuracy of 92%, reducing communication cost by 30%, and enhancing client\nparticipation compared to traditional FL methods. These results make the\nproposed approach highly suitable for large-scale edge-cloud computing systems.",
        "MiMiC is a framework for modeling large-scale chemical processes that require\ntreatment at multiple resolutions. It does not aim to implement single-handedly\nall methods required to treat individual subsystems, but instead, it relegates\nthis task to specialized computational chemistry software while it serves as an\nintermediary between these external programs, and computes the interactions\nbetween the subsystems. MiMiC minimizes issues typically associated with\nmolecular dynamics performed with multiple programs, by adopting a\nmultiple-program multiple-data paradigm combined with a loose-coupling model.\nIn this article, we present the addition of a new client program, CP2K, to the\nMiMiC ecosystem, which required a major refactoring of the entire framework and\nin the end allowed us to unlock its full flexibility. By thorough timing\nanalysis, we verify that the introduced changes do not affect the performance\nof MiMiC or CP2K, and neither are they a source of significant computational\noverheads that would be detrimental to simulation efficiency. Moreover, we\ndemonstrate the benefits of the framework's modular design, by performing a\nQM\/MM MD simulation combining CP2K with previously interfaced OpenMM, with no\nadditional implementation effort required.",
        "This paper addresses the trade-off between internalisation and\nexternalisation in the management of stochastic trade flows. We consider agents\nwho must absorb flows and manage risk by deciding whether to warehouse it or\nhedge in the market, thereby incurring transaction costs and market impact.\nUnlike market makers, these agents cannot skew their quotes to attract\noffsetting flows and deter risk-increasing ones, leading to a fundamentally\ndifferent problem. Within the Almgren-Chriss framework, we derive\nalmost-closed-form solutions in the case of quadratic execution costs, while\nmore general cases require numerical methods. In particular, we discuss the\nchallenges posed by artificial boundary conditions when using classical\ngrid-based numerical PDE techniques and propose reinforcement learning methods\nas an alternative.",
        "Ensuring model calibration is critical for reliable predictions, yet popular\ndistribution-free methods, such as histogram binning and isotonic regression,\nprovide only asymptotic guarantees. We introduce a unified framework for Venn\nand Venn-Abers calibration, generalizing Vovk's binary classification approach\nto arbitrary prediction tasks and loss functions. Venn calibration leverages\nbinning calibrators to construct prediction sets that contain at least one\nmarginally perfectly calibrated point prediction in finite samples, capturing\nepistemic uncertainty in the calibration process. The width of these sets\nshrinks asymptotically to zero, converging to a conditionally calibrated point\nprediction. Furthermore, we propose Venn multicalibration, a novel methodology\nfor finite-sample calibration across subpopulations. For quantile loss,\ngroup-conditional and multicalibrated conformal prediction arise as special\ncases of Venn multicalibration, and Venn calibration produces novel conformal\nprediction intervals that achieve quantile-conditional coverage. As a separate\ncontribution, we extend distribution-free conditional calibration guarantees of\nhistogram binning and isotonic calibration to general losses.",
        "The Deep Web is constituted by data that are accessible through Web pages,\nbut not readily indexable by search engines as they are returned in dynamic\npages. In this paper we propose a conceptual framework for answering keyword\nqueries on Deep Web sources represented as relational tables with so-called\naccess limitations. We formalize the notion of optimal answer, characterize\nqueries for which an answer can be found, and present a method for query\nprocessing based on the construction of a query plan that minimizes the\naccesses to the data sources.",
        "Trajectory generation has garnered significant attention from researchers in\nthe field of spatio-temporal analysis, as it can generate substantial\nsynthesized human mobility trajectories that enhance user privacy and alleviate\ndata scarcity. However, existing trajectory generation methods often focus on\nimproving trajectory generation quality from a singular perspective, lacking a\ncomprehensive semantic understanding across various scales. Consequently, we\nare inspired to develop a HOlistic SEmantic Representation (HOSER) framework\nfor navigational trajectory generation. Given an origin-and-destination (OD)\npair and the starting time point of a latent trajectory, we first propose a\nRoad Network Encoder to expand the receptive field of road- and zone-level\nsemantics. Second, we design a Multi-Granularity Trajectory Encoder to\nintegrate the spatio-temporal semantics of the generated trajectory at both the\npoint and trajectory levels. Finally, we employ a Destination-Oriented\nNavigator to seamlessly integrate destination-oriented guidance. Extensive\nexperiments on three real-world datasets demonstrate that HOSER outperforms\nstate-of-the-art baselines by a significant margin. Moreover, the model's\nperformance in few-shot learning and zero-shot learning scenarios further\nverifies the effectiveness of our holistic semantic representation."
      ]
    }
  },
  {
    "id":2411.00575,
    "research_type":"basic",
    "start_id":"b22",
    "start_title":"Vertical slice modelling of nonlinear Eady waves using a compatible finite element method",
    "start_abstract":"A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution.",
    "start_categories":[
      "math.MP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem"
      ],
      "abstract":[
        "Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Optimal Criteria for Best Subset Selection",
        "Approximate Dynamic Programming for a Remanufacture-to-Order System",
        "Sparse control in microscopic and mean-field leader-follower models",
        "Stabilization and Optimal Control of an Interconnected $n + m$\n  Hetero-directional Hyperbolic PDE-SDE System",
        "Upper and Lower Bounds for a Class of Constrained Linear Time-Varying\n  Games",
        "Stabilization of a Chain of Three Hyperbolic PDEs using a Time-Delay\n  Representation",
        "A Survey of Exact and Approximation Algorithms for Linear-Parametric\n  Optimization Problems",
        "Error Bounds for a Class of Cone-Convex Inclusion Problems",
        "On extending the class of convex functions",
        "A relaxed proximal point algorithm with double-inertial effects for\n  nonconvex equilibrium problems",
        "Assessment various control methods a digital copy of enterprise by\n  integral indicator",
        "Some commutation principles for optimization problems over\n  transformation groups and semi-FTvN systems",
        "On the resolution and linear optimization problems subject to a system\n  of bipolar fuzzy relational equalities defined with continuous Archimedean\n  t-norms",
        "Decentralized RISE-based Control for Exponential Heterogeneous\n  Multi-Agent Target Tracking of Second-Order Nonlinear Systems",
        "On the Origin and Fate of Our Universe",
        "Trend-Aware Supervision: On Learning Invariance for Semi-Supervised\n  Facial Action Unit Intensity Estimation",
        "Data-Driven Decision Making for Enhancing Small-Signal Stability in\n  Hybrid AC\/DC Grids Through Converter Control Role Assignment",
        "Multi-Year-to-Decadal Temperature Prediction using a Machine Learning\n  Model-Analog Framework",
        "Decentralized Inference for Spatial Data Using Low-Rank Models",
        "Quadratic quasinormal modes at null infinity on a Schwarzschild\n  spacetime",
        "On the Adversarial Vulnerabilities of Transfer Learning in Remote\n  Sensing",
        "Stress field in the vicinity of a bubble\/sphere moving in a dilute\n  surfactant solution",
        "A Hybrid Swarm Intelligence Approach for Optimizing Multimodal Large\n  Language Models Deployment in Edge-Cloud-based Federated Learning\n  Environments",
        "Making Puzzle Pieces Fit or Reshaping MiMiC for Multiscale Simulations\n  with CP2K and More",
        "To Hedge or Not to Hedge: Optimal Strategies for Stochastic Trade Flow\n  Management",
        "Generalized Venn and Venn-Abers Calibration with Applications in\n  Conformal Prediction",
        "Keyword Search in the Deep Web",
        "Holistic Semantic Representation for Navigational Trajectory Generation"
      ],
      "abstract":[
        "This paper introduces two novel criteria: one for feature selection and\nanother for feature elimination in the context of best subset selection, which\nis a benchmark problem in statistics and machine learning. From the perspective\nof optimization, we revisit the classical selection and elimination criteria in\ntraditional best subset selection algorithms, revealing that these classical\ncriteria capture only partial variations of the objective function after the\nentry or exit of features. By formulating and solving optimization subproblems\nfor feature entry and exit exactly, new selection and elimination criteria are\nproposed, proved as the optimal decisions for the current entry-and-exit\nprocess compared to classical criteria. Replacing the classical selection and\nelimination criteria with the proposed ones generates a series of enhanced best\nsubset selection algorithms. These generated algorithms not only preserve the\ntheoretical properties of the original algorithms but also achieve significant\nmeta-gains without increasing computational cost across various scenarios and\nevaluation metrics on multiple tasks such as compressed sensing and sparse\nregression.",
        "Remanufacturing is pivotal in transitioning to more sustainable economies.\nWhile industry evidence highlights its vast market potential and economic and\nenvironmental benefits, remanufacturing remains underexplored in theoretical\nresearch. This study revisits a state-of-the-art remanufacture-to-order (RtO)\nsystem and develops an alternative approach by developing an approximate\ndynamic programming (ADP) algorithm to solve larger RtO instances. The proposed\nmethodology yields results consistent with existing state-dependent,\nnon-congestive policies. Key findings include the optimality of fulfilling\ndemand whenever remanufacturable cores are available and prioritizing cores of\nthe highest quality level. This work highlights the potential of ADP in\naddressing problems in remanufacturing domains.",
        "This work investigates the decay properties of Lyapunov functions in\nleader-follower systems seen as a sparse control framework. Starting with a\nmicroscopic representation, we establish conditions under which the total\nLyapunov function, encompassing both leaders and followers, exhibits\nexponential decay. The analysis is extended to a hybrid setting combining a\nmean-field description for followers and a microscopic model for leaders. We\nidentify sufficient conditions on control gain and interaction strengths that\nguarantee stabilization of the linear system towards a target state. The\nresults highlight the influence of sparse control and interaction parameters in\nachieving coordinated behavior in multi-agent systems.",
        "In this paper, we design a controller for an interconnected system composed\nof a linear Stochastic Differential Equation (SDE) controlled through a linear\nhetero-directional hyperbolic Partial Differential Equation (PDE). Our\nobjective is to steer the coupled system to a desired final state on average,\nwhile keeping the variance-in-time as small as possible, improving robustness\nto disturbances. By employing backstepping techniques, we decouple the original\nPDE, reformulating the system as an input delayed SDE with a stochastic drift.\nWe first establish a controllability result, shading light on lower bounds for\nthe variance. This shows that the system can never improve variance below\nstrict structural limits. Under standard controllability conditions, we then\ndesign a controller that drives the mean of the states while keeping the\nvariance bounded. Finally, we analyze the optimal control problem of variance\nminimization along the entire trajectory. Under additional controllability\nassumptions, we prove that the optimal control can achieve any variance level\nabove the fundamental structural limit.",
        "This paper develops an algorithm for upper- and lower-bounding the value\nfunction for a class of linear time-varying games subject to convex control\nsets. In particular, a two-player zero-sum differential game is considered\nwhere the respective players aim to minimise and maximise a convex terminal\nstate cost. A collection of solutions of a single-player dynamical system\nsubject to a trimmed control set is used to characterise a viscosity\nsupersolution of a Hamilton-Jacobi (HJ) equation, which in turn yields an upper\nbound for the value function. Analogously, a collection of hyperplanes is used\nto characterise a viscosity subsolution of the HJ equation, which yields a\nlower bound. The computational complexity and memory requirement of the\nproposed algorithm scales with the number of solutions and hyperplanes that\ncharacterise the bounds, which is not explicitly tied to the number of system\nstates. Thus, the algorithm is tractable for systems of moderately high\ndimension whilst preserving rigorous guarantees for optimal control and\ndifferential game applications.",
        "This paper addresses the stabilization of a chain system consisting of three\nhyperbolic Partial Differential Equations (PDEs). The system is reformulated\ninto a pure transport system of equations via an invertible backstepping\ntransformation. Using the method of characteristics and exploiting the inherent\ncascade structure of the chain, the stabilization problem is reduced to that of\nan associated Integral Difference Equation (IDE). A dynamic controller is\ndesigned for the IDE, whose gains are computed by solving a system of\nFredholm-type integral equations. This approach provides a systematic framework\nfor achieving exponential stabilization of the chain of hyperbolic PDEs.",
        "Linear-parametric optimization, where multiple objectives are combined into a\nsingle objective using linear combinations with parameters as coefficients, has\nnumerous links to other fields in optimization and a wide range of application\nareas. In this survey, we provide a comprehensive overview of structural\nresults and algorithmic strategies for solving linear-parametric optimization\nproblems exactly and approximately. Transferring concepts from related areas\nsuch as multi-objective optimization provides further relevant results. The\nsurvey consists of two parts: First, we list strategies that work in a general\nfashion and do not rely on specific problem structures. Second, we look at\nwell-studied parametric optimization problems and cover both important\ntheoretical results and specialized algorithmic approaches for these problems.\nAmong these problems are parametric variants of shortest path problems, minimum\ncost flow and maximum flow problems, spanning tree problems, the knapsack\nproblem, and matching problems. Overall, we cover the results from 128\npublications (and refer to 33 supplemental works) published between 1963 and\n2024.",
        "In this paper, we investigate error bounds for cone-convex inclusion problems\nin finite-dimensional settings of the form $f(x)\\in K$, where $K$ is a smooth\ncone and $f$ is a continuously differentiable and $K$-concave function. We show\nthat local error bounds for the inclusion can be characterized by the Abadie\nconstraint qualification around the reference point. In the case where $f$ is\nan affine function, we precisely identify the conditions under which the\ninclusion admits global error bounds. Additionally, we derive some properties\nof smooth cones, as well as regular cones and strictly convex cones.",
        "In this brief note, it is shown that the function p^TW log(p) is convex in p\nif W is a diagonally dominant positive definite M-matrix. The techniques used\nto prove convexity are well-known in linear algebra and essentially involves\nfactoring the Hessian in a way that is amenable to martix analysis. Using\nsimilar techniques, two classes of convex homogeneous polynomials is derived -\nnamely, p^TW p2 and (p^k)^TW p^k - the latter also happen to be SOS-convex.\nLastly, usign the same techniques, it is also shown that the function p^TW ep\nis convex over the positive reals only if W is a non-negative diagonal matrix.\nDiscussions regarding the utility of these functions and examples accompany the\nresults presented.",
        "In this paper, we present a relaxation proximal point method with double\ninertial effects to approximate a solution of a non-convex equilibrium problem.\nWe give global\n  convergence results of the iterative sequence generated by our algorithm.\nSome known results are recovered\n  as special cases of our results. Numerical test is given to support the\ntheoretical findings.",
        "The difficulty of assessing the state lies in a little predictable change in\nthe dimension of a dynamic system under the influence of internal changes and\nenvironmental parameters. In the work, the state of such a system is estimated\nby the method of integral indicators. The application of the method of integral\nindicators allowed us to evaluate the activity of an enterprise. In the present\nwork, the method of integrated indicators is used to assess the control of a\ndigital copy (enterprise).",
        "We introduce the concepts of commutativity relative to a transformation group\nand strong commutativity in the setting of a semi-FTvN system and show their\nappearance as optimality conditions in certain optimization problems. In the\nsetting of a semi-FTvN system (in particular, in an FTvN system), we show that\nstrong commutativity implies commutativity and observe that in the special case\nof Euclidean Jordan algebra, commutativity and strong commutativity concepts\nreduce, respectively, to those of operator and strong operator commutativity.\nWe demonstrate that every complete hyperbolic polynomial induces a semi-FTvN\nsystem. By way of an application, we describe several commutation principles.",
        "This paper considers the linear objective function optimization with respect\nto a more general class of bipolar fuzzy relational equations, where the fuzzy\ncompositions are defined by an arbitrary continuous Archimedean t-norm. In\naddition, a faster method for finding a global optimum is proposed that, unlike\nthe previous work, does not require obtaining all local optimal solutions and\nclassifying the constraints. Analytical concepts and properties of the\nArchimedean bipolar fuzzy equations are investigated and two necessary\nconditions are presented to conceptualize the feasibility of the problem. It is\nshown that the feasible solution set can be resulted by a union of the finite\nnumber of compact sets, where each compact set is obtained by a function.\nMoreover, to accelerate identification of the mentioned compact sets (and\ntherefore, to speed up solution finding), four simplification techniques are\npresented, which are based on either omitting redundant constraints and\/or\neliminating unknowns by assigning them a fixed value. Also, three additional\nsimplification techniques are given to reduce the search domain by removing\nsome parts of the feasible region that do not contain optimal solutions.\nSubsequently, a method is proposed to find an optimal solution for the current\nlinear optimization problems. The proposed method consists of two accelerative\nstrategies that are used during the problem solving process. By the first\nstrategy, the method neglects some candidate solutions that are not optimal, by\nconsidering only a subset of admissible functions. As for the second strategy,\na branch-and-bound method is used to delete non-optimal branches. Then, the\nmethod is summarized in an algorithm that represents all essential steps of the\nsolution and finally, the whole method is applied in an example that has been\nchosen in such a way that the various situations are illustrated.",
        "This work presents a decentralized implementation of a Robust Integral of the\nSign of the Error (RISE) controller for multi-agent target tracking problems\nwith exponential convergence guarantees. Previous RISE-based approaches for\nmulti-agent systems required 2-hop communication, limiting practical\napplicability. New insights from a Lyapunov-based design-analysis approach are\nused to eliminate the need for multi-hop communication required in previous\nliterature, while yielding exponential target tracking. The new insights\ninclude the development of a new P-function which is developed which works in\ntandem with the inclusion of the interaction matrix in the Lyapunov function.\nNonsmooth Lyapunov-based stability analysis methods are used to yield\nsemi-global exponential convergence to the target agent state despite the\npresence of bounded disturbances with bounded derivatives. The resulting\noutcome is a controller that achieves exponential target tracking with only\nlocal information exchange between neighboring agents.",
        "This brief review, intended for high energy and astrophysics researchers,\nexplores the implications of recent theoretical advances in string theory and\nthe Swampland program for understanding bounds on the structure of positive\npotentials allowed in quantum gravity. This has a bearing on both inflationary\nmodels for the early universe as well as the fate of our universe. The paper\nincludes a review of the dS conjecture as well as the TransPlanckian Censorship\nConjecture (TCC) and its relation to the species scale. We provide evidence for\nthese principles as well as what they may lead to in terms of phenomenological\npredictions. (Talk presented at Lemaitre Conference 2024)",
        "With the increasing need for facial behavior analysis, semi-supervised AU\nintensity estimation using only keyframe annotations has emerged as a practical\nand effective solution to relieve the burden of annotation. However, the lack\nof annotations makes the spurious correlation problem caused by AU\nco-occurrences and subject variation much more prominent, leading to non-robust\nintensity estimation that is entangled among AUs and biased among subjects. We\nobserve that trend information inherent in keyframe annotations could act as\nextra supervision and raising the awareness of AU-specific facial appearance\nchanging trends during training is the key to learning invariant AU-specific\nfeatures. To this end, we propose \\textbf{T}rend-\\textbf{A}ware\n\\textbf{S}upervision (TAS), which pursues three kinds of trend awareness,\nincluding intra-trend ranking awareness, intra-trend speed awareness, and\ninter-trend subject awareness. TAS alleviates the spurious correlation problem\nby raising trend awareness during training to learn AU-specific features that\nrepresent the corresponding facial appearance changes, to achieve intensity\nestimation invariance. Experiments conducted on two commonly used AU benchmark\ndatasets, BP4D and DISFA, show the effectiveness of each kind of awareness. And\nunder trend-aware supervision, the performance can be improved without extra\ncomputational or storage costs during inference.",
        "Hybrid AC\/DC transmission grids incorporate Modular Multilevel Converters\nfunctioning as Interconnecting Power Converters (IPCs). The control role\nassigned to each converter significantly influences grid dynamics.\nTraditionally, these converters operate with static control roles, but recent\nstudies have proposed scheduling their roles based on day-ahead forecasts to\nenhance stability performance. However, in systems with high renewable energy\npenetration, forecast deviations can render scheduled control assignments\nsuboptimal or even lead to instability. To address this challenge, this work\nproposes an online scheduling recalculation algorithm that dynamically adapts\nIPC control roles during system operation. The approach leverages a data-driven\nmulti-criteria decision-making framework, integrating surrogate models of\nconventional small-signal stability analysis tools to enable a fast computation\nof system stability and stability performance indicators.",
        "Multi-year-to-decadal climate prediction is a key tool in understanding the\nrange of potential regional and global climate futures. Here, we present a\nframework that combines machine learning and analog forecasting for predictions\non these timescales. A neural network is used to learn a mask, specific to a\nregion and lead time, with global weights based on relative importance as\nprecursors to the evolution of that prediction target. A library of\nmask-weighted model states, or potential analogs, are then compared to a single\nmask-weighted observational state. The known future of the best matching\npotential analogs serve as the prediction for the future of the observational\nstate. We match and predict 2-meter temperature using the Berkeley Earth\nSurface Temperature dataset for observations, and a set of CMIP6 models as the\nanalog library. We find improved performance over traditional analog methods\nand initialized decadal predictions.",
        "Advancements in information technology have enabled the creation of massive\nspatial datasets, driving the need for scalable and efficient computational\nmethodologies. While offering viable solutions, centralized frameworks are\nlimited by vulnerabilities such as single-point failures and communication\nbottlenecks. This paper presents a decentralized framework tailored for\nparameter inference in spatial low-rank models to address these challenges. A\nkey obstacle arises from the spatial dependence among observations, which\nprevents the log-likelihood from being expressed as a summation-a critical\nrequirement for decentralized optimization approaches. To overcome this\nchallenge, we propose a novel objective function leveraging the evidence lower\nbound, which facilitates the use of decentralized optimization techniques. Our\napproach employs a block descent method integrated with multi-consensus and\ndynamic consensus averaging for effective parameter optimization. We prove the\nconvexity of the new objective function in the vicinity of the true parameters,\nensuring the convergence of the proposed method. Additionally, we present the\nfirst theoretical results establishing the consistency and asymptotic normality\nof the estimator within the context of spatial low-rank models. Extensive\nsimulations and real-world data experiments corroborate these theoretical\nfindings, showcasing the robustness and scalability of the framework.",
        "The ringdown of perturbed black holes has been studied since the 1970s, but\nuntil recently, studies have focused on linear perturbations. There is now\nburgeoning interest in nonlinear perturbative effects during ringdown. Here,\nusing a hyperboloidal framework, we provide a complete treatment of linear and\nquadratic quasinormal modes (QNMs and QQNMs) in second-order perturbation\ntheory, in Schwarzschild spacetime. We include novel methods for extracting\nQNMs and QQNMs amplitudes using a Laplace transform treatment, allowing for the\ninclusion of arbitrary initial data. We produce both time- and frequency-domain\ncodes. From these codes, we present new results further exploring the\nunforeseen dependence of QQNMs amplitudes on the parity of the progenitor\nsystem, as demonstrated in our letter [Phys. Rev. Lett. 134, 061401 (2025)].\nOur numerical results are restricted to perturbations of a Schwarzschild black\nhole, but our methods extend straightforwardly to the astrophysically realistic\ncase of a Kerr black hole.",
        "The use of pretrained models from general computer vision tasks is widespread\nin remote sensing, significantly reducing training costs and improving\nperformance. However, this practice also introduces vulnerabilities to\ndownstream tasks, where publicly available pretrained models can be used as a\nproxy to compromise downstream models. This paper presents a novel Adversarial\nNeuron Manipulation method, which generates transferable perturbations by\nselectively manipulating single or multiple neurons in pretrained models.\nUnlike existing attacks, this method eliminates the need for domain-specific\ninformation, making it more broadly applicable and efficient. By targeting\nmultiple fragile neurons, the perturbations achieve superior attack\nperformance, revealing critical vulnerabilities in deep learning models.\nExperiments on diverse models and remote sensing datasets validate the\neffectiveness of the proposed method. This low-access adversarial neuron\nmanipulation technique highlights a significant security risk in transfer\nlearning models, emphasizing the urgent need for more robust defenses in their\ndesign when addressing the safety-critical remote sensing tasks.",
        "In this study, we experimentally investigate the stress field around a bubble\nrising in a dilute surfactant solution (20 < Re < 220, high Peclet numbers)\nwhose surface gradually becomes contaminated, and compare it with that around a\nsphere free from surface contamination. We employ a newly developed\npolarization measurement technique, highly sensitive to stress fields near\ninterfaces. First, we validate this method by measuring the flow around a solid\nsphere settling at Re = 120 and comparing results with numerical predictions,\nconfirming its accuracy. We then measure the stress field around a bubble whose\ndrag force transitions from that of a clean interface to that of a rigid\ninterface within the observation region. The stress near the bubble's front\nresembles that of a clean bubble, while the rear behaves like a solid sphere.\nBetween these regions, a discontinuous phase retardation near the cap angle\nindicates a transition from slip to no-slip boundary conditions. Axisymmetric\nstress reconstruction reveals localized stress spike at the cap angle, which\nshifts as surfactant accumulates and increases the drag. Remarkably, the\nmeasured cap angle versus normalized drag coefficient agrees well with\nnumerical simulations at Re = 100 (Cuenot et al. 1997) and shows only a slight\ndeviation from the creeping-flow stagnant cap model (Sadhal and Johnson 1983).\nThis work demonstrates that polarization-based stress field measurements\neffectively capture the interplay between surface contamination and\nhydrodynamics at intermediate Reynolds numbers.",
        "The combination of Federated Learning (FL), Multimodal Large Language Models\n(MLLMs), and edge-cloud computing enables distributed and real-time data\nprocessing while preserving privacy across edge devices and cloud\ninfrastructure. However, the deployment of MLLMs in FL environments with\nresource-constrained edge devices presents significant challenges, including\nresource management, communication overhead, and non-IID data. To address these\nchallenges, we propose a novel hybrid framework wherein MLLMs are deployed on\nedge devices equipped with sufficient resources and battery life, while the\nmajority of training occurs in the cloud. To identify suitable edge devices for\ndeployment, we employ Particle Swarm Optimization (PSO), and Ant Colony\nOptimization (ACO) is utilized to optimize the transmission of model updates\nbetween edge and cloud nodes. This proposed swarm intelligence-based framework\naims to enhance the efficiency of MLLM training by conducting extensive\ntraining in the cloud and fine-tuning at the edge, thereby reducing energy\nconsumption and communication costs. Our experimental results show that the\nproposed method significantly improves system performance, achieving an\naccuracy of 92%, reducing communication cost by 30%, and enhancing client\nparticipation compared to traditional FL methods. These results make the\nproposed approach highly suitable for large-scale edge-cloud computing systems.",
        "MiMiC is a framework for modeling large-scale chemical processes that require\ntreatment at multiple resolutions. It does not aim to implement single-handedly\nall methods required to treat individual subsystems, but instead, it relegates\nthis task to specialized computational chemistry software while it serves as an\nintermediary between these external programs, and computes the interactions\nbetween the subsystems. MiMiC minimizes issues typically associated with\nmolecular dynamics performed with multiple programs, by adopting a\nmultiple-program multiple-data paradigm combined with a loose-coupling model.\nIn this article, we present the addition of a new client program, CP2K, to the\nMiMiC ecosystem, which required a major refactoring of the entire framework and\nin the end allowed us to unlock its full flexibility. By thorough timing\nanalysis, we verify that the introduced changes do not affect the performance\nof MiMiC or CP2K, and neither are they a source of significant computational\noverheads that would be detrimental to simulation efficiency. Moreover, we\ndemonstrate the benefits of the framework's modular design, by performing a\nQM\/MM MD simulation combining CP2K with previously interfaced OpenMM, with no\nadditional implementation effort required.",
        "This paper addresses the trade-off between internalisation and\nexternalisation in the management of stochastic trade flows. We consider agents\nwho must absorb flows and manage risk by deciding whether to warehouse it or\nhedge in the market, thereby incurring transaction costs and market impact.\nUnlike market makers, these agents cannot skew their quotes to attract\noffsetting flows and deter risk-increasing ones, leading to a fundamentally\ndifferent problem. Within the Almgren-Chriss framework, we derive\nalmost-closed-form solutions in the case of quadratic execution costs, while\nmore general cases require numerical methods. In particular, we discuss the\nchallenges posed by artificial boundary conditions when using classical\ngrid-based numerical PDE techniques and propose reinforcement learning methods\nas an alternative.",
        "Ensuring model calibration is critical for reliable predictions, yet popular\ndistribution-free methods, such as histogram binning and isotonic regression,\nprovide only asymptotic guarantees. We introduce a unified framework for Venn\nand Venn-Abers calibration, generalizing Vovk's binary classification approach\nto arbitrary prediction tasks and loss functions. Venn calibration leverages\nbinning calibrators to construct prediction sets that contain at least one\nmarginally perfectly calibrated point prediction in finite samples, capturing\nepistemic uncertainty in the calibration process. The width of these sets\nshrinks asymptotically to zero, converging to a conditionally calibrated point\nprediction. Furthermore, we propose Venn multicalibration, a novel methodology\nfor finite-sample calibration across subpopulations. For quantile loss,\ngroup-conditional and multicalibrated conformal prediction arise as special\ncases of Venn multicalibration, and Venn calibration produces novel conformal\nprediction intervals that achieve quantile-conditional coverage. As a separate\ncontribution, we extend distribution-free conditional calibration guarantees of\nhistogram binning and isotonic calibration to general losses.",
        "The Deep Web is constituted by data that are accessible through Web pages,\nbut not readily indexable by search engines as they are returned in dynamic\npages. In this paper we propose a conceptual framework for answering keyword\nqueries on Deep Web sources represented as relational tables with so-called\naccess limitations. We formalize the notion of optimal answer, characterize\nqueries for which an answer can be found, and present a method for query\nprocessing based on the construction of a query plan that minimizes the\naccesses to the data sources.",
        "Trajectory generation has garnered significant attention from researchers in\nthe field of spatio-temporal analysis, as it can generate substantial\nsynthesized human mobility trajectories that enhance user privacy and alleviate\ndata scarcity. However, existing trajectory generation methods often focus on\nimproving trajectory generation quality from a singular perspective, lacking a\ncomprehensive semantic understanding across various scales. Consequently, we\nare inspired to develop a HOlistic SEmantic Representation (HOSER) framework\nfor navigational trajectory generation. Given an origin-and-destination (OD)\npair and the starting time point of a latent trajectory, we first propose a\nRoad Network Encoder to expand the receptive field of road- and zone-level\nsemantics. Second, we design a Multi-Granularity Trajectory Encoder to\nintegrate the spatio-temporal semantics of the generated trajectory at both the\npoint and trajectory levels. Finally, we employ a Destination-Oriented\nNavigator to seamlessly integrate destination-oriented guidance. Extensive\nexperiments on three real-world datasets demonstrate that HOSER outperforms\nstate-of-the-art baselines by a significant margin. Moreover, the model's\nperformance in few-shot learning and zero-shot learning scenarios further\nverifies the effectiveness of our holistic semantic representation."
      ]
    }
  },
  {
    "id":2411.00578,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association",
    "start_abstract":"Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "Voxel Scene Graph for Intracranial Hemorrhage"
      ],
      "abstract":[
        "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Transformer Dynamics: A neuroscientific approach to interpretability of\n  large language models",
        "Actual Causation and Nondeterministic Causal Models",
        "ILLC: Iterative Layer-by-Layer Compression for Enhancing Structural\n  Faithfulness in SpArX",
        "GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis\n  and Automated Report Generation",
        "Neurosymbolic artificial intelligence via large language models and\n  coherence-driven inference",
        "On Sequential Fault-Intolerant Process Planning",
        "Neuro-Symbolic AI in 2024: A Systematic Review",
        "On the Complexity of Global Necessary Reasons to Explain Classification",
        "Analyzing sequential activity and travel decisions with interpretable\n  deep inverse reinforcement learning",
        "Beyond Text: Implementing Multimodal Large Language Model-Powered\n  Multi-Agent Systems Using a No-Code Platform",
        "On Scaling Neurosymbolic Programming through Guided Logical Inference",
        "MMCR: Advancing Visual Language Model in Multimodal Multi-Turn\n  Contextual Reasoning",
        "From Screens to Scenes: A Survey of Embodied AI in Healthcare",
        "CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference",
        "Mobile Application Threats and Security",
        "Water Flow Detection Device Based on Sound Data Analysis and Machine\n  Learning to Detect Water Leakage",
        "Recent open heavy flavor studies for the Electron-Ion Collider",
        "A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention\n  Logit Interpolation (GALI)",
        "ReasonGraph: Visualisation of Reasoning Paths",
        "Foundations of block-parallel automata networks",
        "Enhancing Quantum-ready QUBO-based Suppression for Object Detection with\n  Appearance and Confidence Features",
        "Uncertainty-Aware Decoding with Minimum Bayes Risk",
        "StructVPR++: Distill Structural and Semantic Knowledge with Weighting\n  Samples for Visual Place Recognition",
        "Infinitely many solutions for elliptic system with Hamiltonian type",
        "Stochastic resonance in Schmitt trigger and its application towards weak\n  signal detection",
        "mmDEAR: mmWave Point Cloud Density Enhancement for Accurate Human Body\n  Reconstruction",
        "Dubrovin duality and mirror symmetry for ADE resolutions",
        "Geometric deformations of cuspidal $S_1$ singularities"
      ],
      "abstract":[
        "As artificial intelligence models have exploded in scale and capability,\nunderstanding of their internal mechanisms remains a critical challenge.\nInspired by the success of dynamical systems approaches in neuroscience, here\nwe propose a novel framework for studying computations in deep learning\nsystems. We focus on the residual stream (RS) in transformer models,\nconceptualizing it as a dynamical system evolving across layers. We find that\nactivations of individual RS units exhibit strong continuity across layers,\ndespite the RS being a non-privileged basis. Activations in the RS accelerate\nand grow denser over layers, while individual units trace unstable periodic\norbits. In reduced-dimensional spaces, the RS follows a curved trajectory with\nattractor-like dynamics in the lower layers. These insights bridge dynamical\nsystems theory and mechanistic interpretability, establishing a foundation for\na \"neuroscience of AI\" that combines theoretical rigor with large-scale data\nanalysis to advance our understanding of modern neural networks.",
        "In (Beckers, 2025) I introduced nondeterministic causal models as a\ngeneralization of Pearl's standard deterministic causal models. I here take\nadvantage of the increased expressivity offered by these models to offer a\nnovel definition of actual causation (that also applies to deterministic\nmodels). Instead of motivating the definition by way of (often subjective)\nintuitions about examples, I proceed by developing it based entirely on the\nunique function that it can fulfil in communicating and learning a causal\nmodel. First I generalize the more basic notion of counterfactual dependence,\nsecond I show how this notion has a vital role to play in the logic of causal\ndiscovery, third I introduce the notion of a structural simplification of a\ncausal model, and lastly I bring both notions together in my definition of\nactual causation. Although novel, the resulting definition arrives at verdicts\nthat are almost identical to those of my previous definition (Beckers, 2021,\n2022).",
        "In the field of Explainable Artificial Intelligence (XAI), argumentative XAI\napproaches have been proposed to represent the internal reasoning process of\ndeep neural networks in a more transparent way by interpreting hidden nodes as\narguements. However, as the number of layers increases, existing compression\nmethods simplify all layers at once, which lead to high accumulative\ninformation loss. To compensate for this, we propose an iterative\nlayer-by-layer compression technique in which each layer is compressed\nseparately and the reduction error in the next layer is immediately compensated\nfor, thereby improving the overall input-output and structural fidelity of the\nmodel. Experiments on the Breast Cancer Diagnosis dataset show that, compared\nto traditional compression, the method reduces input-output and structural\nunfaithfulness, and maintains a more consistent attack-support relationship in\nthe Argumentative Explanation scheme. This is significant because it provides a\nnew way to make complex MLP models more compact while still conveying their\ninternal inference logic without distortion.",
        "This study introduces GreenIQ, an AI-powered deep search platform designed to\nrevolutionise carbon market intelligence through autonomous analysis and\nautomated report generation. Carbon markets operate across diverse regulatory\nlandscapes, generating vast amounts of heterogeneous data from policy\ndocuments, industry reports, academic literature, and real-time trading\nplatforms. Traditional research approaches remain labour-intensive, slow, and\ndifficult to scale. GreenIQ addresses these limitations through a multi-agent\narchitecture powered by Large Language Models (LLMs), integrating five\nspecialised AI agents: a Main Researcher Agent for intelligent information\nretrieval, a Report Writing Agent for structured synthesis, a Final Reviewer\nAgent for accuracy verification, a Data Visualisation Agent for enhanced\ninterpretability, and a Translator Agent for multilingual adaptation. The\nsystem achieves seamless integration of structured and unstructured information\nwith AI-driven citation verification, ensuring high transparency and\nreliability. GreenIQ delivers a 99.2\\% reduction in processing time and a\n99.7\\% cost reduction compared to traditional research methodologies. A novel\nAI persona-based evaluation framework involving 16 domain-specific AI personas\nhighlights its superior cross-jurisdictional analytical capabilities and\nregulatory insight generation. GreenIQ sets new standards in AI-driven research\nsynthesis, policy analysis, and sustainability finance by streamlining carbon\nmarket research. It offers an efficient and scalable framework for\nenvironmental and financial intelligence, enabling more accurate, timely, and\ncost-effective decision-making in complex regulatory landscapes",
        "We devise an algorithm to generate sets of propositions that objectively\ninstantiate graphs that support coherence-driven inference. We then benchmark\nthe ability of large language models (LLMs) to reconstruct coherence graphs\nfrom (a straightforward transformation of) propositions expressed in natural\nlanguage, with promising results from a single prompt to models optimized for\nreasoning. Combining coherence-driven inference with consistency evaluations by\nneural models may advance the state of the art in machine cognition.",
        "We propose and study a planning problem we call Sequential Fault-Intolerant\nProcess Planning (SFIPP). SFIPP captures a reward structure common in many\nsequential multi-stage decision problems where the planning is deemed\nsuccessful only if all stages succeed. Such reward structures are different\nfrom classic additive reward structures and arise in important applications\nsuch as drug\/material discovery, security, and quality-critical product design.\nWe design provably tight online algorithms for settings in which we need to\npick between different actions with unknown success chances at each stage. We\ndo so both for the foundational case in which the behavior of actions is\ndeterministic, and the case of probabilistic action outcomes, where we\neffectively balance exploration for learning and exploitation for planning\nthrough the usage of multi-armed bandit algorithms. In our empirical\nevaluations, we demonstrate that the specialized algorithms we develop, which\nleverage additional information about the structure of the SFIPP instance,\noutperform our more general algorithm.",
        "Background: The field of Artificial Intelligence has undergone cyclical\nperiods of growth and decline, known as AI summers and winters. Currently, we\nare in the third AI summer, characterized by significant advancements and\ncommercialization, particularly in the integration of Symbolic AI and\nSub-Symbolic AI, leading to the emergence of Neuro-Symbolic AI.\n  Methods: The review followed the PRISMA methodology, utilizing databases such\nas IEEE Explore, Google Scholar, arXiv, ACM, and SpringerLink. The inclusion\ncriteria targeted peer-reviewed papers published between 2020 and 2024. Papers\nwere screened for relevance to Neuro-Symbolic AI, with further inclusion based\non the availability of associated codebases to ensure reproducibility.\n  Results: From an initial pool of 1,428 papers, 167 met the inclusion criteria\nand were analyzed in detail. The majority of research efforts are concentrated\nin the areas of learning and inference (63%), logic and reasoning (35%), and\nknowledge representation (44%). Explainability and trustworthiness are less\nrepresented (28%), with Meta-Cognition being the least explored area (5%). The\nreview identifies significant interdisciplinary opportunities, particularly in\nintegrating explainability and trustworthiness with other research areas.\n  Conclusion: Neuro-Symbolic AI research has seen rapid growth since 2020, with\nconcentrated efforts in learning and inference. Significant gaps remain in\nexplainability, trustworthiness, and Meta-Cognition. Addressing these gaps\nthrough interdisciplinary research will be crucial for advancing the field\ntowards more intelligent, reliable, and context-aware AI systems.",
        "Explainable AI has garnered considerable attention in recent years, as\nunderstanding the reasons behind decisions or predictions made by AI systems is\ncrucial for their successful adoption. Explaining classifiers' behavior is one\nprominent problem. Work in this area has proposed notions of both local and\nglobal explanations, where the former are concerned with explaining a\nclassifier's behavior for a specific instance, while the latter are concerned\nwith explaining the overall classifier's behavior regardless of any specific\ninstance. In this paper, we focus on global explanations, and explain\nclassification in terms of ``minimal'' necessary conditions for the classifier\nto assign a specific class to a generic instance. We carry out a thorough\ncomplexity analysis of the problem for natural minimality criteria and\nimportant families of classifiers considered in the literature.",
        "Travel demand modeling has shifted from aggregated trip-based models to\nbehavior-oriented activity-based models because daily trips are essentially\ndriven by human activities. To analyze the sequential activity-travel\ndecisions, deep inverse reinforcement learning (DIRL) has proven effective in\nlearning the decision mechanisms by approximating a reward function to\nrepresent preferences and a policy function to replicate observed behavior\nusing deep neural networks (DNNs). However, most existing research has focused\non using DIRL to enhance only prediction accuracy, with limited exploration\ninto interpreting the underlying decision mechanisms guiding sequential\ndecision-making. To address this gap, we introduce an interpretable DIRL\nframework for analyzing activity-travel decision processes, bridging the gap\nbetween data-driven machine learning and theory-driven behavioral models. Our\nproposed framework adapts an adversarial IRL approach to infer the reward and\npolicy functions of activity-travel behavior. The policy function is\ninterpreted through a surrogate interpretable model based on choice\nprobabilities from the policy function, while the reward function is\ninterpreted by deriving both short-term rewards and long-term returns for\nvarious activity-travel patterns. Our analysis of real-world travel survey data\nreveals promising results in two key areas: (i) behavioral pattern insights\nfrom the policy function, highlighting critical factors in decision-making and\nvariations among socio-demographic groups, and (ii) behavioral preference\ninsights from the reward function, indicating the utility individuals gain from\nspecific activity sequences.",
        "This study proposes the design and implementation of a multimodal LLM-based\nMulti-Agent System (MAS) leveraging a No-Code platform to address the practical\nconstraints and significant entry barriers associated with AI adoption in\nenterprises. Advanced AI technologies, such as Large Language Models (LLMs),\noften pose challenges due to their technical complexity and high implementation\ncosts, making them difficult for many organizations to adopt. To overcome these\nlimitations, this research develops a No-Code-based Multi-Agent System designed\nto enable users without programming knowledge to easily build and manage AI\nsystems. The study examines various use cases to validate the applicability of\nAI in business processes, including code generation from image-based notes,\nAdvanced RAG-based question-answering systems, text-based image generation, and\nvideo generation using images and prompts. These systems lower the barriers to\nAI adoption, empowering not only professional developers but also general users\nto harness AI for significantly improved productivity and efficiency. By\ndemonstrating the scalability and accessibility of No-Code platforms, this\nstudy advances the democratization of AI technologies within enterprises and\nvalidates the practical applicability of Multi-Agent Systems, ultimately\ncontributing to the widespread adoption of AI across various industries.",
        "Probabilistic neurosymbolic learning seeks to integrate neural networks with\nsymbolic programming. Many state-of-the-art systems rely on a reduction to the\nProbabilistic Weighted Model Counting Problem (PWMC), which requires computing\na Boolean formula called the logical provenance.However, PWMC is \\\\#P-hard, and\nthe number of clauses in the logical provenance formula can grow exponentially,\ncreating a major bottleneck that significantly limits the applicability of PNL\nsolutions in practice.We propose a new approach centered around an exact\nalgorithm DPNL, that enables bypassing the computation of the logical\nprovenance.The DPNL approach relies on the principles of an oracle and a\nrecursive DPLL-like decomposition in order to guide and speed up logical\ninference.Furthermore, we show that this approach can be adapted for\napproximate reasoning with $\\epsilon$ or $(\\epsilon, \\delta)$ guarantees,\ncalled ApproxDPNL.Experiments show significant performance gains.DPNL enables\nscaling exact inference further, resulting in more accurate models.Further,\nApproxDPNL shows potential for advancing the scalability of neurosymbolic\nprogramming by incorporating approximations even further, while simultaneously\nensuring guarantees for the reasoning process.",
        "Compared to single-turn dialogue, multi-turn dialogue involving multiple\nimages better aligns with the needs of real-world human-AI interactions.\nAdditionally, as training data, it provides richer contextual reasoning\ninformation, thereby guiding the model to achieve better performance. However,\nexisting vision-language models (VLMs) primarily rely on single-turn dialogue\ntraining and evaluation benchmarks. In this paper, following the\ncharacteristics of human dialogue, such as focused topics and concise, clear\ncontent, we present MMCR (Multimodal Multi-turn Contextual Reasoning), a novel\ndataset comprising: (1) MMCR-310k -- the largest multi-image multi-turn\ninstruction tuning dataset with 310K contextual dialogues, each covering 1-4\nimages and 4 or 8 dialogue turns; and (2) MMCR-Bench -- a diagnostic benchmark\nfeaturing dialogues, spanning 8 domains (Humanities, Natural, Science,\nEducation, etc.) and 40 sub-topics. Extensive evaluations demonstrate that\nmodels fine-tuned with MMCR-310k achieve 5.2\\% higher contextual accuracy on\nMMCR-Bench, while showing consistent improvements on existing benchmarks\n(+1.1\\% on AI2D, +1.2\\% on MMMU and MMVet). MMCR and prompt engineering will be\nreleased publicly.",
        "Healthcare systems worldwide face persistent challenges in efficiency,\naccessibility, and personalization. Powered by modern AI technologies such as\nmultimodal large language models and world models, Embodied AI (EmAI)\nrepresents a transformative frontier, offering enhanced autonomy and the\nability to interact with the physical world to address these challenges. As an\ninterdisciplinary and rapidly evolving research domain, \"EmAI in healthcare\"\nspans diverse fields such as algorithms, robotics, and biomedicine. This\ncomplexity underscores the importance of timely reviews and analyses to track\nadvancements, address challenges, and foster cross-disciplinary collaboration.\nIn this paper, we provide a comprehensive overview of the \"brain\" of EmAI for\nhealthcare, wherein we introduce foundational AI algorithms for perception,\nactuation, planning, and memory, and focus on presenting the healthcare\napplications spanning clinical interventions, daily care & companionship,\ninfrastructure support, and biomedical research. Despite its promise, the\ndevelopment of EmAI for healthcare is hindered by critical challenges such as\nsafety concerns, gaps between simulation platforms and real-world applications,\nthe absence of standardized benchmarks, and uneven progress across\ninterdisciplinary domains. We discuss the technical barriers and explore\nethical considerations, offering a forward-looking perspective on the future of\nEmAI in healthcare. A hierarchical framework of intelligent levels for EmAI\nsystems is also introduced to guide further development. By providing\nsystematic insights, this work aims to inspire innovation and practical\napplications, paving the way for a new era of intelligent, patient-centered\nhealthcare.",
        "Large language models (LLMs) achieve impressive performance by scaling model\nparameters, but this comes with significant inference overhead. Feed-forward\nnetworks (FFNs), which dominate LLM parameters, exhibit high activation\nsparsity in hidden neurons. To exploit this, researchers have proposed using a\nmixture-of-experts (MoE) architecture, where only a subset of parameters is\nactivated. However, existing approaches often require extensive training data\nand resources, limiting their practicality. We propose CMoE (Carved MoE), a\nnovel framework to efficiently carve MoE models from dense models. CMoE\nachieves remarkable performance through efficient expert grouping and\nlightweight adaptation. First, neurons are grouped into shared and routed\nexperts based on activation rates. Next, we construct a routing mechanism\nwithout training from scratch, incorporating a differentiable routing process\nand load balancing. Using modest data, CMoE produces a well-designed, usable\nMoE from a 7B dense model within five minutes. With lightweight fine-tuning, it\nachieves high-performance recovery in under an hour. We make our code publicly\navailable at https:\/\/github.com\/JarvisPei\/CMoE.",
        "The movement to mobile computing solutions provides flexibility to different\nusers whether it is a business user, a student, or even providing entertainment\nto children and adults of all ages. Due to these emerging technologies mobile\nusers are unable to safeguard private information in a very effective way and\ncybercrimes are increasing day by day. This manuscript will focus on security\nvulnerabilities in the mobile computing industry, especially focusing on\ntablets and smart phones. This study will dive into current security threats\nfor the Android & Apple iOS market, exposing security risks and threats that\nthe novice or average user may not be aware of. The purpose of this study is to\nanalyze current security risks and threats, and provide solutions that may be\ndeployed to protect against such threats.",
        "In this paper, we introduce a novel mechanism that uses machine learning\ntechniques to detect water leaks in pipes. The proposed simple and low-cost\nmechanism is designed that can be easily installed on building pipes with\nvarious sizes. The system works based on gathering and amplifying water flow\nsignals using a mechanical sound amplifier. Then sounds are recorded and\nconverted to digital signals in order to be analyzed. After feature extraction\nand selection, deep neural networks are used to discriminate between with and\nwithout leak pipes. The experimental results show that this device can detect\nat least 100 milliliters per minute (mL\/min) of water flow in a pipe so that it\ncan be used as a core of a water leakage detection system.",
        "The future Electron-Ion Collider (EIC) will operate a series of\nhigh-luminosity high-energy electron+proton ($e+p$) and electron+nucleus\n($\\textit{e + A}$) collisions to study several fundamental questions in the\nhigh energy and nuclear physics field. Heavy flavor hadron and jet production\nat the EIC plays an important role in exploring both potential modification on\nthe initial-state nuclear parton distribution functions (nPDFs) and final-state\nparton propagation and hadronization processes under different nuclear medium\nconditions. The current design of the EIC ePIC detector has good performance of\nvertex and track reconstruction, particle identification and energy\ndetermination in the pseudorapidity region of $-3.5<\\eta<3.5$, which will\nenable a series of high precision heavy flavor hadron and jet measurements.\nLatest simulation studies of the projected nuclear modification factor $R_{eA}$\nof heavy flavor jets and heavy flavor hadron inside jets in $e+p$ and\n$\\textit{e + Au}$ collisions at $\\sqrt{s} =$ 28.6 GeV and 63.2 GeV as well as\nthe projected statistical accuracy of inclusive and differential charm baryon\nover meson ratio measurements in $e+p$ collisions will be presented. The\nimpacts of these proposed EIC measurements on constraining the heavy quark\npropagation properties in cold nuclear medium and exploring the heavy quark\nhadronization process will be discussed.",
        "Transformer-based Large Language Models (LLMs) struggle to process inputs\nexceeding their training context window, with performance degrading due to\npositional out-of-distribution (O.O.D.) that disrupt attention computations.\nExisting solutions, fine-tuning and training-free methods, are limited by\ncomputational inefficiency, attention logit outliers or loss of local\npositional information. To address this, we propose Greedy Attention Logit\nInterpolation (GALI), a training-free length extrapolation method that\nmaximizes the utilization of pretrained positional intervals while avoiding\nattention logit outliers through attention logit interpolation. The result\ndemonstrates that GALI consistently outperforms state-of-the-art training-free\nmethods. Our findings reveal that LLMs interpret positional intervals unevenly\nwithin their training context window, suggesting that extrapolating within a\nsmaller positional interval range yields superior results-even for\nshort-context tasks. GALI represents a significant step toward resolving the\npositional O.O.D. challenge, enabling more reliable long-text understanding in\nLLMs. Our implementation of GALI, along with the experiments from our paper, is\nopen-sourced at https:\/\/github.com\/AcademyCityL\/GALI.",
        "Large Language Models (LLMs) reasoning processes are challenging to analyze\ndue to their complexity and the lack of organized visualization tools. We\npresent ReasonGraph, a web-based platform for visualizing and analyzing LLM\nreasoning processes. It supports both sequential and tree-based reasoning\nmethods while integrating with major LLM providers and over fifty\nstate-of-the-art models. ReasonGraph incorporates an intuitive UI with meta\nreasoning method selection, configurable visualization parameters, and a\nmodular framework that facilitates efficient extension. Our evaluation shows\nhigh parsing reliability, efficient processing, and strong usability across\nvarious downstream applications. By providing a unified visualization\nframework, ReasonGraph reduces cognitive load in analyzing complex reasoning\npaths, improves error detection in logical processes, and enables more\neffective development of LLM-based applications. The platform is open-source,\npromoting accessibility and reproducibility in LLM reasoning analysis.",
        "We settle the theoretical ground for the study of automata networks under\nblock-parallel update schedules, which are somehow dual to the block-sequential\nones, but allow for repetitions of automaton updates. This gain in expressivity\nbrings new challenges, and we analyse natural equivalence classes of update\nschedules: those leading to the same dynamics, and to the same limit dynamics,\nfor any automata network. Countings and enumeration algorithms are provided,\nfor their numerical study. We also prove computational complexity bounds for\nmany classical problems, involving fixed points, limit cycles, the recognition\nof subdynamics, reachability, etc. The PSPACE-completeness of computing the\nimage of a single configuration lifts the complexity of most problems, but the\nlandscape keeps some relief, in particular for reversible computations.",
        "Quadratic Unconstrained Binary Optimization (QUBO)-based suppression in\nobject detection is known to have superiority to conventional Non-Maximum\nSuppression (NMS), especially for crowded scenes where NMS possibly suppresses\nthe (partially-) occluded true positives with low confidence scores. Whereas\nexisting QUBO formulations are less likely to miss occluded objects than NMS,\nthere is room for improvement because existing QUBO formulations naively\nconsider confidence scores and pairwise scores based on spatial overlap between\npredictions. This study proposes new QUBO formulations that aim to distinguish\nwhether the overlap between predictions is due to the occlusion of objects or\ndue to redundancy in prediction, i.e., multiple predictions for a single\nobject. The proposed QUBO formulation integrates two features into the pairwise\nscore of the existing QUBO formulation: i) the appearance feature calculated by\nthe image similarity metric and ii) the product of confidence scores. These\nfeatures are derived from the hypothesis that redundant predictions share a\nsimilar appearance feature and (partially-) occluded objects have low\nconfidence scores, respectively. The proposed methods demonstrate significant\nadvancement over state-of-the-art QUBO-based suppression without a notable\nincrease in runtime, achieving up to 4.54 points improvement in mAP and 9.89\npoints gain in mAR.",
        "Despite their outstanding performance in the majority of scenarios,\ncontemporary language models still occasionally generate undesirable outputs,\nfor example, hallucinated text. While such behaviors have previously been\nlinked to uncertainty, there is a notable lack of methods that actively\nconsider uncertainty during text generation. In this work, we show how Minimum\nBayes Risk (MBR) decoding, which selects model generations according to an\nexpected risk, can be generalized into a principled uncertainty-aware decoding\nmethod. In short, we account for model uncertainty during decoding by\nincorporating a posterior over model parameters into MBR's computation of\nexpected risk. We show that this modified expected risk is useful for both\nchoosing outputs and deciding when to abstain from generation and can provide\nimprovements without incurring overhead. We benchmark different methods for\nlearning posteriors and show that performance improves with prediction\ndiversity. We release our code publicly.",
        "Visual place recognition is a challenging task for autonomous driving and\nrobotics, which is usually considered as an image retrieval problem. A commonly\nused two-stage strategy involves global retrieval followed by re-ranking using\npatch-level descriptors. Most deep learning-based methods in an end-to-end\nmanner cannot extract global features with sufficient semantic information from\nRGB images. In contrast, re-ranking can utilize more explicit structural and\nsemantic information in one-to-one matching process, but it is time-consuming.\nTo bridge the gap between global retrieval and re-ranking and achieve a good\ntrade-off between accuracy and efficiency, we propose StructVPR++, a framework\nthat embeds structural and semantic knowledge into RGB global representations\nvia segmentation-guided distillation. Our key innovation lies in decoupling\nlabel-specific features from global descriptors, enabling explicit semantic\nalignment between image pairs without requiring segmentation during deployment.\nFurthermore, we introduce a sample-wise weighted distillation strategy that\nprioritizes reliable training pairs while suppressing noisy ones. Experiments\non four benchmarks demonstrate that StructVPR++ surpasses state-of-the-art\nglobal methods by 5-23% in Recall@1 and even outperforms many two-stage\napproaches, achieving real-time efficiency with a single RGB input.",
        "In this paper, we use Legendre-Fenchel transform and a space decomposition to\ncarry out Fountain theorem and dual Fountain theorem for the following elliptic\nsystem of Hamiltonian type: \\[ \\begin{cases} \\begin{aligned} -\\Delta u&=H_v(u,\nv) \\,\\quad&&\\text{in}~\\Omega,\\\\ -\\Delta v&=H_u(u, v)\n\\,\\quad&&\\text{in}~\\Omega,\\\\ u,\\,v&=0~~&&\\text{on} ~ \\partial\\Omega,\\\\\n\\end{aligned} \\end{cases} \\] where $N\\ge 1$, $\\Omega \\subset \\mathbb{R}^N$ is a\nbounded domain and $H\\in C^1( \\mathbb{R}^2)$ is strictly convex, even and\nsubcritical. We mainly present two results: (i) When $H$ is superlinear, the\nsystem has infinitely many solutions, whose energies tend to infinity. (ii)\nWhen $H$ is sublinear, the system has infinitely many solutions, whose energies\nare negative and tend to 0. As a byproduct, the Lane-Emden system under\nsubcritical growth has infinitely many solutions.",
        "This study explores stochastic resonance (SR) in a Schmitt trigger circuit\nand its application to weak signal detection. SR, a phenomenon where noise\nsynchronizes with weak signals to enhance detectability, was demonstrated using\na custom-designed bi-stable Schmitt trigger system. The circuit's bi-stability\nwas validated through hysteresis curve analysis, confirming its suitability for\nSR studies. Experimental results revealed SR behavior by analyzing\nsignal-to-noise ratio (SNR) responses to noise amplitude variations. Detection\nexperiments were conducted to determine frequency and amplitude of damping\nsinusoidal pulses. Frequency detection proved effective, albeit with\nlimitations at low frequencies, while amplitude detection faced challenges due\nto mathematical complexities. Nonetheless, the study highlights SR's potential\nfor weak signal detection, with proposed enhancements to improve detection\naccuracy. This work underscores the adaptability of classical SR principles to\npractical detection systems and suggests future applications in advanced\ndetection technologies, including quantum systems.",
        "Millimeter-wave (mmWave) radar offers robust sensing capabilities in diverse\nenvironments, making it a highly promising solution for human body\nreconstruction due to its privacy-friendly and non-intrusive nature. However,\nthe significant sparsity of mmWave point clouds limits the estimation accuracy.\nTo overcome this challenge, we propose a two-stage deep learning framework that\nenhances mmWave point clouds and improves human body reconstruction accuracy.\nOur method includes a mmWave point cloud enhancement module that densifies the\nraw data by leveraging temporal features and a multi-stage completion network,\nfollowed by a 2D-3D fusion module that extracts both 2D and 3D motion features\nto refine SMPL parameters. The mmWave point cloud enhancement module learns the\ndetailed shape and posture information from 2D human masks in single-view\nimages. However, image-based supervision is involved only during the training\nphase, and the inference relies solely on sparse point clouds to maintain\nprivacy. Experiments on multiple datasets demonstrate that our approach\noutperforms state-of-the-art methods, with the enhanced point clouds further\nimproving performance when integrated into existing models.",
        "We show that, under Dubrovin's notion of ''almost'' duality, the Frobenius\nmanifold structure on the orbit spaces of the extended affine Weyl groups of\ntype $\\mathrm{ADE}$ is dual, for suitable choices of weight markings, to the\nequivariant quantum cohomology of the minimal resolution of the du Val\nsingularity of the same Dynkin type. We also provide a uniform Lie-theoretic\nconstruction of Landau-Ginzburg mirrors for the quantum cohomology of\n$\\mathrm{ADE}$ resolutions. The mirror B-model is described by a\none-dimensional LG superpotential associated to the spectral curve of the\n$\\widehat{\\mathrm{ADE}}$ affine relativistic Toda chain.",
        "To study a deformation of a singularity taking into consideration their\ndifferential geometric properties, a form representing the deformation using\nonly diffeomorphisms on the source space and isometries of the target space\nplays a crucial role. Such a form for an $S_1$ singularity is obtained by the\nauthor's previous work. On this form, we give a necessary and sufficient\ncondition for such a map is being a frontal. The form for an $S_1$ singularity\nwith the frontal condition can be considered such a form for a cuspidal $S_1$\nsingularity. Using this form, we investigate geometric properties of cuspidal\n$S_1$ singularities and the cuspidal cross caps appearing in the deformation."
      ]
    }
  },
  {
    "id":2411.00578,
    "research_type":"applied",
    "start_id":"b25",
    "start_title":"Voxel Scene Graph for Intracranial Hemorrhage",
    "start_abstract":"Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association"
      ],
      "abstract":[
        "Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Estimating invasive rodent abundance using removal data and hierarchical\n  models",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Prediction of Binding Affinity for ErbB Inhibitors Using Deep Neural\n  Network Model with Morgan Fingerprints as Features",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "An intriguing coincidence between the majority of vast polar structure\n  dwarfs and a recent major merger at the M31 position",
        "Fault-Resilience of Dissipative Processes for Quantum Computing",
        "Does a Large Language Model Really Speak in Human-Like Language?",
        "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs",
        "Time Evolution of the Symmetry Resolved Entanglement Entropy after a\n  Mass Quench",
        "Quantum metric induced magneto-optical effects in\n  $\\mathcal{PT}$-symmetric antiferromagnets",
        "Systematic Search for Long-Term Trends in Fermi-LAT Jetted Active\n  Galactic Nuclei",
        "Approximate Evaluation Method for the Probability of the Union of\n  Independent Events",
        "Unveiling stellar spin: Determining inclination angles in Be stars",
        "FEASTS Combined with Interferometry (IV): Mapping HI Emission to a limit\n  of $N_{\\text{HI}}=10^{17.7} \\text{cm}^{-2}$ in Seven Edge-on Galaxies",
        "Randomized Spectral Clustering for Large-Scale Multi-Layer Networks",
        "Liquidity provision of utility indifference type in decentralized\n  exchanges",
        "How good is PAC-Bayes at explaining generalisation?",
        "Sliding ferroelectric control of unconventional magnetism in stacked\n  bilayers",
        "The centimeter emission from planet-forming disks in Taurus"
      ],
      "abstract":[
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Invasive rodents pose significant ecological, economic, and public health\nchallenges. Robust methods are needed for estimating population abundance to\nguide effective management. Traditional methods such as capture-recapture are\noften impractical for invasive species due to ethical and logistical\nconstraints. Here, I showcase the application of hierarchical multinomial\nN-mixture models for estimating the abundance of invasive rodents using removal\ndata. First, I perform a simulation study which demonstrates minimal bias in\nabundance estimates across a range of sampling scenarios. Second, I analyze\nremoval data for two invasive rodent species, namely coypus (Myocastor coypus)\nin France and muskrats (Ondatra zibethicus) in the Netherlands. Using\nhierarchical multinomial N-mixture models, I examine the effects of temperature\non abundance while accounting for imperfect and time-varying capture\nprobabilities. I also show how to accommodate spatial variability using random\neffects, and quantify uncertainty in parameter estimates. Overall, I hope to\ndemonstrate the flexibility and utility of hierarchical models in invasive\nspecies management.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "The ErbB receptor family, including EGFR and HER2, plays a crucial role in\ncell growth and survival and is associated with the progression of various\ncancers such as breast and lung cancer. In this study, we developed a deep\nlearning model to predict the binding affinity of ErbB inhibitors using\nmolecular fingerprints derived from SMILES representations. The SMILES\nrepresentations for each ErbB inhibitor were obtained from the ChEMBL database.\nWe first generated Morgan fingerprints from the SMILES strings and applied\nAutoDock Vina docking to calculate the binding affinity values. After filtering\nthe dataset based on binding affinity, we trained a deep neural network (DNN)\nmodel to predict binding affinity values from the molecular fingerprints. The\nmodel achieved significant performance, with a Mean Squared Error (MSE) of\n0.2591, Mean Absolute Error (MAE) of 0.3658, and an R-squared value of 0.9389\non the training set. Although performance decreased slightly on the test set (R\nsquared = 0.7731), the model still demonstrated robust generalization\ncapabilities. These results indicate that the deep learning approach is highly\neffective for predicting the binding affinity of ErbB inhibitors, offering a\nvaluable tool for virtual screening and drug discovery.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "A significant part of the Milky Way (MW) dwarf galaxies orbit within a Vast\nPOlar Structure (VPOS), which is perpendicular to the Galactic disc and whose\norigin has not yet been identified. It includes the Large Magellanic Cloud\n(LMC) and its six dynamically associated dwarf galaxies. Andromeda Galaxy (M31)\nexperienced a major merger two to three billion years ago, and its accurate\nmodelling predicts that an associated tidal tail is pointing towards the\nGalaxy. Here, we tested a possible association between M31 tidal tail particles\nand MW dwarf galaxies, focusing first on the LMC and its associated dwarfs\nsince they are less affected by ram pressure. We traced back these dwarf galaxy\norbits by one billion years and calculated their association with the tidal\ntail particles in the 6D phase space, based on their proper motion from\n\\textit{Gaia} DR3. We find that for low-mass MW models (total mass less than 5\n$\\times 10^{11} M_{\\odot}$), the separation in the 6D space can be less than\n1$\\sigma$ for most of the M31 modelling, albeit with a significant degree of\nfreedom due to the still unknown proper motion of M31. We further discover that\nmany other dwarfs could also be associated with the M31 tidal tails if their\nmotions had been radially slowed, as expected from the ram pressure exerted by\nthe MW corona. This intriguing coincidence could explain the origin of the\nVPOS, which resulted from a matter exchange between M31 and MW.",
        "Dissipative processes have long been proposed as a means of performing\ncomputational tasks on quantum computers that may be intrinsically more robust\nto noise. In this work, we prove two main results concerning the\nerror-resilience capabilities of two types of dissipative algorithms:\ndissipative ground state preparation in the form of the dissipative quantum\neigensolver (DQE), and dissipative quantum computation (DQC). The first result\nis that under circuit-level depolarizing noise, a version of the DQE algorithm\napplied to the geometrically local, stabilizer-encoded Hamiltonians that arise\nnaturally when fermionic Hamiltonians are represented in qubits, can suppress\nthe additive error in the ground space overlap of the final output state\nexponentially in the code distance. This enables us to get closer to\nfault-tolerance for this task without the associated overhead. In contrast, for\ncomputation as opposed to ground state preparation, the second result proves\nthat DQC is no more robust to noise than the standard quantum circuit model.",
        "Large Language Models (LLMs) have recently emerged, attracting considerable\nattention due to their ability to generate highly natural, human-like text.\nThis study compares the latent community structures of LLM-generated text and\nhuman-written text within a hypothesis testing procedure. Specifically, we\nanalyze three text sets: original human-written texts ($\\mathcal{O}$), their\nLLM-paraphrased versions ($\\mathcal{G}$), and a twice-paraphrased set\n($\\mathcal{S}$) derived from $\\mathcal{G}$. Our analysis addresses two key\nquestions: (1) Is the difference in latent community structures between\n$\\mathcal{O}$ and $\\mathcal{G}$ the same as that between $\\mathcal{G}$ and\n$\\mathcal{S}$? (2) Does $\\mathcal{G}$ become more similar to $\\mathcal{O}$ as\nthe LLM parameter controlling text variability is adjusted? The first question\nis based on the assumption that if LLM-generated text truly resembles human\nlanguage, then the gap between the pair ($\\mathcal{O}$, $\\mathcal{G}$) should\nbe similar to that between the pair ($\\mathcal{G}$, $\\mathcal{S}$), as both\npairs consist of an original text and its paraphrase. The second question\nexamines whether the degree of similarity between LLM-generated and human text\nvaries with changes in the breadth of text generation. To address these\nquestions, we propose a statistical hypothesis testing framework that leverages\nthe fact that each text has corresponding parts across all datasets due to\ntheir paraphrasing relationship. This relationship enables the mapping of one\ndataset's relative position to another, allowing two datasets to be mapped to a\nthird dataset. As a result, both mapped datasets can be quantified with respect\nto the space characterized by the third dataset, facilitating a direct\ncomparison between them. Our results indicate that GPT-generated text remains\ndistinct from human-authored text.",
        "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x.",
        "In this paper we investigate the properties of the symmetry resolved\nentanglement entropy after a mass quench in the Ising field theory. Since the\ntheory is free and the post-quench state known explicitly, the one-point\nfunction of the relevant (composite) branch point twist field can be computed\nusing form factor techniques, similar to previous work on the branch point\ntwist field and the magnetisation, respectively. We find that the symmetry\nresolved entropy grows linearly in time at the same rate as the total entropy,\nand that there are sub-leading oscillatory corrections. This result provides\nthe first explicit computation of the out-of-equilibrium dynamics of the\nsymmetry resolved entropy employing twist fields in quantum field theory and is\nconsistent with existing results based on the quasiparticle picture.",
        "The magneto-optical effects (MOEs), as a fundamental physical phenomenon, can\nreveal the electronic structures of materials. The related probing methods are\nwidely used in the study of magnetic materials. However, space-time inversion\n($\\mathcal{PT}$) symmetric antiferromagnets were previously believed to be\nmagneto-optically inactive. Here, we point out that this traditional\nunderstanding is incorrect. Based on our generic formulas and symmetry\nanalysis, we find that in $\\mathcal{PT}$-symmetric antiferromagnets, it is the\nquantum metric, i.e., the real part of the quantum geometry, that induces MOEs.\nCombining a tight-binding model and first-principles calculations, we confirm\nthis observation by showing MOEs in the $\\mathcal{PT}$-symmetric\nantiferromagnet. Our work demonstrates that $\\mathcal{PT}$-symmetric\nantiferromagnets previously thought to lack MOEs can indeed exhibit MOEs and\ngreatly broaden the research on MOEs.",
        "Jetted Active Galactic Nuclei (AGN) exhibit variability across a wide range\nof time scales. Traditionally, this variability can often be modeled well as a\nstochastic process. However, in certain cases, jetted AGN variability displays\nregular patterns, enabling us to conduct investigations aimed at understanding\nits origins. Additionally, a novel type of variability has emerged in jetted\nAGN lightcurves, specifically, the observation of a long-term trend\ncharacterized by a linear increase of the flux with time in blazars such as PG\n1553+113, which is among the objects most likely to display periodic behavior.\nIn this paper, we present the results of a systematic search for long-term\ntrends, spanning $\\approx$10\\, years, utilizing 12 years of Fermi-LAT\nobservations. The study is focused on detecting the presence of linear or\nquadratic long-term trends in a sample of 3308 jetted AGN. Our analysis has\nidentified 40 jetted AGN that exhibit long-term trends, each with distinct\nproperties, which we also characterize in this study. These long-term trends\nmay originate from the dynamics of a supermassive black hole binary system, or\nthey could be the result of intrinsic phenomena within the jet itself. Our\nfindings can help in addressing questions pertaining to the astrophysical\norigins of variability and periodicity within jetted AGN.",
        "The evaluation of the probability of union of a large number of independent\nevents requires several combinations involving the factorial and the use of\nhigh performance computers with several hours of processing. Bounds and\nsimplifications on the probability of the union are useful in the analysis of\nstochastic problems across various areas including (but not limited to) systems\nreliability, biological systems, real-time fault-tolerant systems, probability\ntheory, information theory and communications. We propose an approximation to\nevaluate the probability of the union of several independent events that uses\nthe arithmetic mean of the probability of all of them. The approximate results\nare very close to, but larger than the exact values. The method allows a much\nsmaller number of operations with a similar result and more simplicity.",
        "The physical properties of stellar atmospheres in rapidly rotating massive\nstars, such as Be stars, are critical to understanding their evolution and\ntheir role as progenitors of supernovae. These stars, which often have\nnear-critical rotation, exhibit equatorial stretching and gravity darkening,\nwhich significantly complicates the determination of parameters such as the\ninclination angle. Be stars, characterized by their extreme rotational\nvelocities, serve as excellent candidates for exploring these phenomena.\nHowever, fundamental quantities such as polar and equatorial radii and\ninclination angles are typically derived from interferometry, which applies\nonly to a limited number of stars. This study aims to enhance the determination\nof inclination angles for Be stars using the ZPEKTR spectral synthesis code. By\nincorporating advanced models of gravity darkening and stellar deformation, we\nevaluated the effectiveness of this method with a sample of ten Be stars from\nthe BeSOS database, comparing results with established interferometric data.\nMethods. We used the ZPEKTR code to model the effects of stellar oblateness and\ngravity darkening on spectral lines, focusing on the HeI 4471 line. We applied\na chi-squared test minimization approach to identify the best-fitting models,\nand we evaluated the inclination angles derived against interferometric\nmeasurements. Our analysis reveals a robust linear correlation between the\ninclination angles derived from ZPEKTR and using interferometric techniques,\nwhich demonstrates an excellent agreement. The ZPEKTR code effectively models\nhigh rotational velocity effects, providing precise stellar parameter\ndeterminations. The results underscore the potential of advanced spectroscopic\ntechniques to yield inclination measurements comparable to interferometry,\nwhich offers a pathway to studying distant massive stars.",
        "We present a statistical study of the neutral atomic hydrogen (HI) gas\nextending into the circumgalactic medium perpendicular to the disk for 7\nedge-on galaxies with inclinations above $85^{\\circ}$ from the FEASTS program\nwith a $3\\sigma$ ($20\\,\\text{km}\\,\\text{s}^{-1}$) column density\n($N_{\\text{HI}}$) depth of $5\\times10^{17} \\text{cm}^{-2}$. We develop two\nphotometric methods to separate the extraplanar HI from the disk component,\nbased on existing interferometric data and parametric modeling of the disk flux\ndistribution respectively. With both methods, the FEASTS data exhibit clear\nextended wings beyond the disk along the minor axis. The extraplanar HI\naccounts for 5% to 20% of the total HI mass and extends to $20\\text{-}50$ kpc\nat $N_{\\text{HI}}=10^{18} \\text{cm}^{-2}$. We find a tight positive correlation\nbetween vertical extensions of the extraplanar HI and total HI mass\n$M_\\text{HI}$. The iso-density shape of HI at $N_{\\text{HI}}=10^{18}\n\\text{cm}^{-2}$ has an average axis ratio of $0.56\\pm0.11$. The off-disk\n$N_{\\text{HI}}$ profiles of these edge-on galaxies well represent the lower\nenvelop of previous Lyman-$\\alpha$ absorption measurements at low-redshift. Our\nresults suggest that at $N_{\\text{HI}}=5\\times10^{17} \\text{cm}^{-2}$, the HI\nextends considerably further than the known thin and thick disks in the\nvertical direction, but still remains much flattener than a spherical\ndistribution, consistent with theoretical expectations that outflow,\ncirculation, and accretion should have different impacts in these two\ndirections. We show the tension of our results with Illustris and TNG\npredictions, highlighting the constraining power of our results for future\nsimulations.",
        "Large-scale multi-layer networks with large numbers of nodes, edges, and\nlayers arise across various domains, which poses a great computational\nchallenge for the downstream analysis. In this paper, we develop an efficient\nrandomized spectral clustering algorithm for community detection of multi-layer\nnetworks. We first utilize the random sampling strategy to sparsify the\nadjacency matrix of each layer. Then we use the random projection strategy to\naccelerate the eigen-decomposition of the sum-of-squared sparsified adjacency\nmatrices of all layers. The communities are finally obtained via the k-means of\nthe eigenvectors. The algorithm not only has low time complexity but also saves\nthe storage space. Theoretically, we study the misclassification error rate of\nthe proposed algorithm under the multi-layer stochastic block models, which\nshows that the randomization does not deteriorate the error bound under certain\nconditions. Numerical studies on multi-layer networks with millions of nodes\nshow the superior efficiency of the proposed algorithm, which achieves\nclustering results rapidly. A new R package called MLRclust is developed and\nmade available to the public.",
        "We present a mathematical formulation of liquidity provision in decentralized\nexchanges. We focus on constant function market makers of utility indifference\ntype, which include constant product market makers with concentrated liquidity\nas a special case. First, we examine no-arbitrage conditions for a liquidity\npool and compute an optimal arbitrage strategy when there is an external liquid\nmarket. Second, we show that liquidity provision suffers from impermanent loss\nunless a transaction fee is levied under the general framework with\nconcentrated liquidity. Third, we establish the well-definedness of\narbitrage-free reserve processes of a liquidity pool in continuous-time and\nshow that there is no loss-versus-rebalancing under a nonzero fee if the\nexternal market price is continuous. We then argue that liquidity provision by\nmultiple liquidity providers can be understood as liquidity provision by a\nrepresentative liquidity provider, meaning that the analysis boils down to that\nfor a single liquidity provider. Last, but not least, we give an answer to the\nfundamental question in which sense the very construction of constant function\nmarket makers with concentrated liquidity in the popular platform Uniswap v3 is\noptimal.",
        "We discuss necessary conditions for a PAC-Bayes bound to provide a meaningful\ngeneralisation guarantee. Our analysis reveals that the optimal generalisation\nguarantee depends solely on the distribution of the risk induced by the prior\ndistribution. In particular, achieving a target generalisation level is only\nachievable if the prior places sufficient mass on high-performing predictors.\nWe relate these requirements to the prevalent practice of using data-dependent\npriors in deep learning PAC-Bayes applications, and discuss the implications\nfor the claim that PAC-Bayes ``explains'' generalisation.",
        "The control of unconventional magnetism, which displays an antiferromagnetic\nconfiguration with ferromagnetism-like properties, has drawn intense attention\nfor advancing antiferromagnetic spintronics. Here, through symmetry analysis,\nwe propose a general stacking rule, characterized by a connection operator\nlinking two stacked bilayers, for controlling unconventional magnetism via\nsliding ferroelectricity. Such rule enables the simultaneous switching of both\nelectric polarization and nonrelativistic spin splitting or anomalous Hall\neffect in altermagnets, a class of collinear unconventional magnets. By\ncomprehensively surveying the 80 layer groups, we identify all the stacking\norders that allow for such two types of simultaneous switching. Combined with\nfirst-principles calculations, we demonstrate the sliding ferroelectric control\nof spin polarization and anomalous Hall effect in the altermagnetic AgF2\nbilayer. Our work provides a symmetry strategy for achieving ferroelectric\ncontrol of unconventional magnetism in bilayer systems and opens avenues for\nexploring new types of magnetoelectric coupling.",
        "The last decade has witnessed remarkable advances in the characterization of\nthe (sub-)millimeter emission from planet-forming disks. Instead, the study of\nthe (sub-)centimeter emission has made more limited progress, to the point that\nonly a few exceptional disk-bearing objects have been characterized in the\ncentimeter regime. This work takes a broad view of the centimeter emission from\na large sample with VLA observations that is selected from previous ALMA\nsurveys of more representative disks in brightness and extent. We report on the\ndetection and characterization of flux at centimeter wavelengths from 21\nsources in the Taurus star-forming region. Complemented by literature and\narchival data, the entire photometry from 0.85 mm to 6 cm is fitted by a\ntwo-component model that determines the ubiquitous presence of free-free\nemission entangled with the dust emission. The flux density of the free-free\nemission is found to scale with the accretion rate but is independent of the\nouter disk morphology depicted by ALMA. The dust emission at 2 cm is still\nappreciable, and offers the possibility to extract an unprecedented large set\nof dust spectral indices in the centimeter regime. A pronounced change between\nthe median millimeter indices (2.3) and centimeter indices (2.8) suggests that\na large portion of the disk emission is optically thick up to 3 mm. The\ncomparison of both indices and fluxes with the ALMA disk extent indicates that\nthis portion can be as large as 40 au, and suggests that the grain population\nwithin this disk region that emits the observed centimeter emission is similar\nin disks with different size and morphology. All these results await\nconfirmation and dedicated dust modeling once facilities like ngVLA or SKA-mid\nare able to resolve the centimeter emission from planet-forming disks and\ndisentangle the various components."
      ]
    }
  },
  {
    "id":2411.00614,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion",
    "start_abstract":"Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Causal identification of single-cell experimental perturbation effects with CINEMA-OT"
      ],
      "abstract":[
        "Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Robust Cislunar Low-Thrust Trajectory Optimization under Uncertainties\n  via Sequential Covariance Steering",
        "Complete systems of inequalities relating the perimeter, the area and\n  the Cheeger constant of planar domains",
        "Bi-Parameterized Two-Stage Stochastic Min-Max and Min-Min Mixed Integer\n  Programs",
        "Disturbance-to-state stabilization by output feedback of nonlinear ODE\n  cascaded with a reaction-diffusion equation",
        "A bang-bang optimal control for a nonlinear system modeling the Gate\n  Control Theory of Pain",
        "Separable Approximations of Optimal Value Functions and Their\n  Representation by Neural Networks",
        "On extending the class of convex functions",
        "Convergence Analysis of EXTRA in Non-convex Distributed Optimization",
        "A Dual Koopman Approach to Observer Design for Nonlinear Systems",
        "Transportation Network Analysis, Volume I: Static and Dynamic Traffic\n  Assignment",
        "A unified recursive identification algorithm with quantized observations\n  based on weighted least-squares type criteria",
        "A Unified Dual Consensus Approach to Distributed Optimization with\n  Globally-Coupled Constraints",
        "Generic linear convergence for algorithms of non-linear least squares\n  over smooth varieties",
        "A Survey on Foundation-Model-Based Industrial Defect Detection",
        "Generalized $\\eta$-pairing theory and anomalous localization in\n  non-Hermitian systems",
        "Aerial Vision-and-Language Navigation with Grid-based View Selection and\n  Map Construction",
        "Real-Time Fast Marching Tree for Mobile Robot Motion Planning in Dynamic\n  Environments",
        "The Muddy Waters of Modeling Empathy in Language: The Practical Impacts\n  of Theoretical Constructs",
        "Theoretical study of the $\\Sigma N$ cusp in the\n  $K^-d\\rightarrow\\pi\\Lambda N$ reaction",
        "A modeling framework to support the electrification of private transport\n  in African cities: a case study of Addis Ababa",
        "Stacked Intelligent Metasurface Enabled Near-Field Multiuser\n  Beamfocusing in the Wave Domain",
        "Systematic Abductive Reasoning via Diverse Relation Representations in\n  Vector-symbolic Architecture",
        "A note On the existence of solutions to Hitchin's self-duality equations",
        "General relativistic quasi-spherical accretion in a dark matter halo",
        "General relativistic particle trajectories via quantum mechanical weak\n  values and the Schwarzschild-Alcubierre spacetime",
        "ProPINN: Demystifying Propagation Failures in Physics-Informed Neural\n  Networks",
        "RePanda: Pandas-powered Tabular Verification and Reasoning",
        "Comparing Native and Non-native English Speakers' Behaviors in\n  Collaborative Writing through Visual Analytics"
      ],
      "abstract":[
        "Spacecraft operations are influenced by uncertainties such as dynamics\nmodeling, navigation, and maneuver execution errors. Although mission design\nhas traditionally incorporated heuristic safety margins to mitigate the effect\nof uncertainties, particularly before\/after crucial events, it is yet unclear\nwhether this practice will scale in the cislunar region, which features locally\nchaotic nonlinear dynamics and involves frequent lunar flybys. This paper\napplies chance-constrained covariance steering and sequential convex\nprogramming to simultaneously design an optimal trajectory and trajectory\ncorrection policy that can probabilistically guarantee safety constraints under\nthe assumed physical\/navigational error models. The results show that the\nproposed method can effectively control the state uncertainty in a highly\nnonlinear environment and provide a trajectory with better local stability\nproperties than a trajectory designed without considering uncertainties. The\nframework allows faster computation and lossless covariance propagation\ncompared to existing methods, enabling a rapid and accurate comparison of\n$\\Delta V_{99}$ costs for different uncertainty parameters. We demonstrate the\nalgorithm on several transfers in the Earth-Moon Circular Restricted Three Body\nProblem.",
        "The object of the paper is to find complete systems of inequalities relating\nthe perimeter $P$, the area $|\\cdot|$ and the Cheeger constant $h$ of planar\nsets. To do so, we study the so called Blaschke--Santal\\'o diagram of the\ntriplet $(P,h,|\\cdot|)$ for different classes of domains: simply connected\nsets, convex sets and convex polygons with at most $N$ sides. We completely\ndetermine the diagram in the latter cases except for the class of convex\n$N$-gons when $N\\ge 5$ is odd: therein, we show that the boundary of the\ndiagram is given by the graphs of two continuous and strictly increasing\nfunctions. An explicit formula for the lower one and a numerical method to\nobtain the upper one is provided. At last, some applications of the results are\npresented.",
        "We introduce two-stage stochastic min-max and min-min integer programs with\nbi-parameterized recourse (BTSPs), where the first-stage decisions affect both\nthe objective function and the feasible region of the second-stage problem. To\nsolve these programs efficiently, we introduce Lagrangian-integrated L-shaped\n($L^2$) methods, which guarantee exact solutions when the first-stage decisions\nare pure binary. For mixed-binary first-stage programs, we present a\nregularization-augmented variant of this method. We also introduce\ndistributionally robust bi-parameterized two-stage stochastic integer programs\nand present an extension of the $L^2$ method and a reformulation-based method\nfor programs with finite and continuous supports, respectively. Our\ncomputational results show that the $L^2$ method surpasses the benchmark method\nfor bi-parameterized stochastic network interdiction problems, solving all\ninstances in 23 seconds on average, whereas the benchmark method failed to\nsolve any instance within 3600 seconds. Additionally, it achieves optimal\nsolutions up to 18.4 and 1.7 times faster for instances of risk-neutral and\ndistributionally robust bi-parameterized stochastic facility location problems,\nrespectively. Furthermore, BTSPs have applications in solving stochastic\nproblems with decision-dependent probability distributions or sets of\ndistributions (ambiguity set). The $L^2$ method outperforms existing\napproaches, achieving optimal solutions 5.3 times faster for distributionally\nrobust facility location problem with a decision-dependent and non-relatively\ncomplete ambiguity set.",
        "In this paper, we analyze the output stabilization problem for cascaded\nnonlinear ODE with $1-d$ heat diffusion equation affected by both in-domain and\nboundary perturbations. We assume that the only available part of states is the\nfirst components of the ODE-subsystem and one boundary of the heat-subsystem.\nThe particularity of this system is two folds i) it contains a nonlinear\nadditive term in the ODE-subsystem, and ii) it is affected by both boundary and\nin-domain perturbations signals.\n  For such a system, and unlike the existing works, we succeeded to design an\noutput observer-based feedback that guarantees not only asymptotic\nstabilization result but also a globally {\\it disturbance-to-state\nstabilization} for our cascaded system. The output feedback is designed using\nan adequate backstepping transformation recently introduced for coupled\nODE-heat equations combined with high-gain observer and high-gain controller.",
        "We consider a nonlinear system of coupled ordinary differential equations\n(representing the excitatory, inhibitory, and T-cell potentials) based on the\nGate Control Theory of Pain, initially proposed by R. Melzack and P.D. Wall in\n1965, and later mathematically modeled by N.F. Britton and S.M. Skevington in\n1988.",
        "The use of separable approximations is proposed to mitigate the curse of\ndimensionality related to the approximation of high-dimensional value functions\nin optimal control. The separable approximation exploits intrinsic decaying\nsensitivity properties of the system, where the influence of a state variable\non another diminishes as their spatial, temporal, or graph-based distance\ngrows. This property allows the efficient representation of global functions as\na sum of localized contributions. A theoretical framework for constructing\nseparable approximations in the context of optimal control is proposed by\nleveraging decaying sensitivity in both discrete and continuous time. Results\nextend prior work on decay properties of solutions to Lyapunov and Riccati\nequations, offering new insights into polynomial and exponential decay regimes.\nConnections to neural networks are explored, demonstrating how separable\nstructures enable scalable representations of high-dimensional value functions\nwhile preserving computational efficiency.",
        "In this brief note, it is shown that the function p^TW log(p) is convex in p\nif W is a diagonally dominant positive definite M-matrix. The techniques used\nto prove convexity are well-known in linear algebra and essentially involves\nfactoring the Hessian in a way that is amenable to martix analysis. Using\nsimilar techniques, two classes of convex homogeneous polynomials is derived -\nnamely, p^TW p2 and (p^k)^TW p^k - the latter also happen to be SOS-convex.\nLastly, usign the same techniques, it is also shown that the function p^TW ep\nis convex over the positive reals only if W is a non-negative diagonal matrix.\nDiscussions regarding the utility of these functions and examples accompany the\nresults presented.",
        "Optimization problems involving the minimization of a finite sum of smooth,\npossibly non-convex functions arise in numerous applications. To achieve a\nconsensus solution over a network, distributed optimization algorithms, such as\n\\textbf{EXTRA} (decentralized exact first-order algorithm), have been proposed\nto address these challenges. In this paper, we analyze the convergence\nproperties of \\textbf{EXTRA} in the context of smooth, non-convex optimization.\nBy interpreting its updates as a nonlinear dynamical system, we show novel\ninsights into its convergence properties. Specifically, i) \\textbf{EXTRA}\nconverges to a consensual first-order stationary point of the global objective\nwith a sublinear rate; and ii) \\textbf{EXTRA} avoids convergence to consensual\nstrict saddle points, offering second-order guarantees that ensure robustness.\nThese findings provide a deeper understanding of \\textbf{EXTRA} in a non-convex\ncontext.",
        "The Koopman operator approach to the state estimation problem for nonlinear\nsystems is a promising research area. The main goal of this paper is an attempt\nto provide a rigorous theoretical framework for this approach. In particular,\nthe (linear) dual Koopman system is introduced and studied in an infinite\ndimensional context. Moreover, new concepts of observability and detectability\nare defined in the dual Koopman system, which are shown to be equivalent to the\nobservability and detectability of the nonlinear system, respectively. The\ntheoretical framework is applied to a class of holomorphic dynamics. For this\nclass, a Luenberger-type observer is designed for the dual Koopman system via a\nspectral method, yielding an estimate of the state of the nonlinear system. A\nparticular attention is given to the existence of an appropriate solution to\nthe dual Koopman system and observer, which are defined in the Hardy space on\nthe polydisc. Spectral observability and detectability conditions are derived\nin this setting, and the exponential convergence of the Koopman observer is\nshown. Finally, numerical experiments support the theoretical findings.",
        "This book covers static and dynamic traffic assignment models used in\ntransportation planning and network analysis. Traffic assignment is the final\nstep in the traditional planning process, and recent decades have seen many\nadvances in formulating and solving such models. The book discusses classical\nsolution methods alongside recent ones used in contemporary planning software.\n  The primary audience for the book is graduate students new to transportation\nnetwork analysis, and to this end there are appendices providing general\nmathematical background, and more specific background in formulating\noptimization problems. We have also included appendices discussing more general\noptimization applications outside of traffic assignment. We believe the book is\nalso of interest to practitioners seeking to understand recent advances in\nnetwork analysis, and to researchers wanting a unified reference for traffic\nassignment content.\n  A second volume is currently under preparation, and will cover transit,\nfreight, and logistics models in transportation networks. A free PDF version of\nthe text will always be available online at\nhttps:\/\/sboyles.github.io\/blubook.html. We will periodically post updated\nversions of the text at this link, along with slides and other instructor\nresources.",
        "This paper investigates system identification problems with Gaussian inputs\nand quantized observations under fixed thresholds. A new formulation for the\npredictor of quantized observations is introduced, establishing a linear\ncorrelation with the parameter estimations through a probabilistic relationship\namong quantized observations, Gaussian inputs, and system parameters.\nSubsequently, a novel weighted least-squares criterion is proposed, and a\ntwo-step recursive identification algorithm is constructed, which is capable of\naddressing both noisy and noise-free linear systems. Convergence analysis of\nthis identification algorithm is conducted, demonstrating convergence in both\nalmost sure and $L^{p}$ senses under mild conditions, with respective rates of\n$O(\\sqrt{ \\log \\log k\/k})$ and $O(1\/k^{p\/2})$, where $k$ denotes the time step.\nIn particular, this algorithm offers an asymptotically efficient estimation of\nthe variance of Gaussian variables using quantized observations. Additionally,\nasymptotic normality is established, and an expression for the asymptotic\nvariance is provided when the weight coefficients are properly selected.\nFurthermore, extensions to output-error systems are discussed, enhancing the\napplicability and relevance of the proposed methods. Two numerical examples are\nprovided to validate these theoretical advancements.",
        "This article explores distributed convex optimization with globally-coupled\nconstraints, where the objective function is a general nonsmooth convex\nfunction, the constraints include nonlinear inequalities and affine equalities,\nand the feasible region is possibly unbounded. To address such problems, a\nunified DUal Consensus Algorithm (DUCA) and its proximal variant (Pro-DUCA) are\nproposed, which are unified frameworks that approximate the method of\nmultipliers applied to the corresponding dual problem in no need of a\nclosed-form dual objective. With varied parameter settings, DUCA and Pro-DUCA\nnot only extend a collection of existing consensus optimization methods to\nsolve the dual problem that they used to be inapplicable to, but also aid in\noffering new efficient algorithms to the literature. The proposed unified\nalgorithms are shown to achieve $O(1\/k)$ convergence rates in terms of\noptimality and feasibility, providing new or enhanced convergence results for a\nnumber of existing methods. Simulations demonstrate that these algorithms\noutperform several state-of-the-art alternatives in terms of objective and\nfeasibility errors.",
        "In applications, a substantial number of problems can be formulated as\nnon-linear least squares problems over smooth varieties. Unlike the usual least\nsquares problem over a Euclidean space, the non-linear least squares problem\nover a variety can be challenging to solve and analyze, even if the variety\nitself is simple. Geometrically, this problem is equivalent to projecting a\npoint in the ambient Euclidean space onto the image of the given variety under\na non-linear map. It is the singularities of the image that make both the\ncomputation and the analysis difficult. In this paper, we prove that under some\nmild assumptions, these troublesome singularities can always be avoided. This\nenables us to establish a linear convergence rate for iterative sequences\ngenerated by algorithms satisfying some standard assumptions. We apply our\ngeneral results to the low-rank partially orthogonal tensor approximation\nproblem. As a consequence, we obtain the linear convergence rate for a\nclassical APD-ALS method applied to a generic tensor, without any further\nassumptions.",
        "As industrial products become abundant and sophisticated, visual industrial\ndefect detection receives much attention, including two-dimensional and\nthree-dimensional visual feature modeling. Traditional methods use statistical\nanalysis, abnormal data synthesis modeling, and generation-based models to\nseparate product defect features and complete defect detection. Recently, the\nemergence of foundation models has brought visual and textual semantic prior\nknowledge. Many methods are based on foundation models (FM) to improve the\naccuracy of detection, but at the same time, increase model complexity and slow\ndown inference speed. Some FM-based methods have begun to explore lightweight\nmodeling ways, which have gradually attracted attention and deserve to be\nsystematically analyzed. In this paper, we conduct a systematic survey with\ncomparisons and discussions of foundation model methods from different aspects\nand briefly review non-foundation model (NFM) methods recently published.\nFurthermore, we discuss the differences between FM and NFM methods from\ntraining objectives, model structure and scale, model performance, and\npotential directions for future exploration. Through comparison, we find FM\nmethods are more suitable for few-shot and zero-shot learning, which are more\nin line with actual industrial application scenarios and worthy of in-depth\nresearch.",
        "By generalizing the eta-pairing theory to non-Hermitian Hubbard models on\narbitrary lattices, we obtain the sufficient and necessary condition for the\neta-pairing operator to be an eigenoperator of the Hamiltonian $H$, and find\nunique eta-pairing phenomena without Hermitian analogs. For instance, the\nHermitian conjugate of an eta-pairing eigenoperator may not be an\neigenoperator, eta-pairing eigenoperators can be spatially modulated, and the\n$SU(2)$ pseudospin symmetry may not be respected even if $H$ commutes with the\neta-pairing operators. Remarkably, these novel non-Hermitian phenomena are\nclosely related to each other by several theorems we establish and can lead to,\ne.g., the notion of non-Hermitian angular-momentum operators and the anomalous\nlocalization of eta-pairing eigenstates. Some issues on the $SO(4)$ and\nparticle-hole symmetries are clarified. Our general eta-pairing theory also\nreveals a previously unnoticed unification of these symmetries of the Hubbard\nmodel. To exemplify these findings, we propose the Hatano-Nelson-Hubbard model.\nIn this interacting non-Hermitian system without even the bulk translation\ninvariance, the right and left two-particle eta-pairing eigenstates are\nexponentially localized at opposite boundaries of the chain. We then generalize\nthis model to two dimensions and find that the eta-pairing eigenstate can\nexhibit the first- or second-order skin effect. Thus, eta-pairing may represent\na new mechanism for skin effects in interacting non-Hermitian systems, even in\nhigher dimensions and without the bulk translation symmetry. To realize all of\nthe non-Hermitian eta-pairing phenomena, we construct a general two-sublattice\nmodel defined on an arbitrary lattice, which can exhibit anomalous localization\nof eta-pairing eigenstates; besides, this model can reveal the eta-pairing\nstructure [e.g., the $SO(4)$ symmetry] in systems with Hermitian hoppings.",
        "Aerial Vision-and-Language Navigation (Aerial VLN) aims to obtain an unmanned\naerial vehicle agent to navigate aerial 3D environments following human\ninstruction. Compared to ground-based VLN, aerial VLN requires the agent to\ndecide the next action in both horizontal and vertical directions based on the\nfirst-person view observations. Previous methods struggle to perform well due\nto the longer navigation path, more complicated 3D scenes, and the neglect of\nthe interplay between vertical and horizontal actions. In this paper, we\npropose a novel grid-based view selection framework that formulates aerial VLN\naction prediction as a grid-based view selection task, incorporating vertical\naction prediction in a manner that accounts for the coupling with horizontal\nactions, thereby enabling effective altitude adjustments. We further introduce\na grid-based bird's eye view map for aerial space to fuse the visual\ninformation in the navigation history, provide contextual scene information,\nand mitigate the impact of obstacles. Finally, a cross-modal transformer is\nadopted to explicitly align the long navigation history with the instruction.\nWe demonstrate the superiority of our method in extensive experiments.",
        "This paper proposes the Real-Time Fast Marching Tree (RT-FMT), a real-time\nplanning algorithm that features local and global path generation,\nmultiple-query planning, and dynamic obstacle avoidance. During the search,\nRT-FMT quickly looks for the global solution and, in the meantime, generates\nlocal paths that can be used by the robot to start execution faster. In\naddition, our algorithm constantly rewires the tree to keep branches from\nforming inside the dynamic obstacles and to maintain the tree root near the\nrobot, which allows the tree to be reused multiple times for different goals.\nOur algorithm is based on the planners Fast Marching Tree (FMT*) and Real-time\nRapidly-Exploring Random Tree (RT-RRT*). We show via simulations that RT-FMT\noutperforms RT- RRT* in both execution cost and arrival time, in most cases.\nMoreover, we also demonstrate via simulation that it is worthwhile taking the\nlocal path before the global path is available in order to reduce arrival time,\neven though there is a small possibility of taking an inferior path.",
        "Conceptual operationalizations of empathy in NLP are varied, with some having\nspecific behaviors and properties, while others are more abstract. How these\nvariations relate to one another and capture properties of empathy observable\nin text remains unclear. To provide insight into this, we analyze the transfer\nperformance of empathy models adapted to empathy tasks with different\ntheoretical groundings. We study (1) the dimensionality of empathy definitions,\n(2) the correspondence between the defined dimensions and measured\/observed\nproperties, and (3) the conduciveness of the data to represent them, finding\nthey have a significant impact to performance compared to other transfer\nsetting features. Characterizing the theoretical grounding of empathy tasks as\ndirect, abstract, or adjacent further indicates that tasks that directly\npredict specified empathy components have higher transferability. Our work\nprovides empirical evidence for the need for precise and multidimensional\nempathy operationalizations.",
        "The $K^-d\\rightarrow\\pi\\Lambda N$ reaction is useful for exploring the\nhyperon-nucleon interaction through final state interactions. In particular,\nthe cusp structure of the $\\Lambda N$ invariant mass spectrum at the $\\Sigma N$\nthreshold contains information about the s-wave interaction of 1\/2-isospin\nhyperon-nucleon systems. The calculation of the spectrum is performed with the\naim of extracting the scattering length of the $\\Sigma N(I=1\/2)$ channel that\ncouples to the $\\Lambda N$ channel from this reaction, and the results are\ndiscussed in comparison with experimental data to highlight the factors that\nshould be considered.",
        "The electrification of road transport, as the predominant mode of\ntransportation in Africa, represents a great opportunity to reduce greenhouse\ngas emissions and dependence on costly fuel imports. However, it introduces\nmajor challenges for local energy infrastructures, including the deployment of\ncharging stations and the impact on often fragile electricity grids. Despite\nits importance, research on electric mobility planning in Africa remains\nlimited, while existing planning tools rely on detailed local mobility data\nthat is often unavailable, especially for privately owned passenger vehicles.\nIn this study, we introduce a novel framework designed to support private\nvehicle electrification in data-scarce regions and apply it to Addis Ababa,\nsimulating the mobility patterns and charging needs of 100,000 electric\nvehicles. Our analysis indicate that these vehicles generate a daily charging\ndemand of approximately 350 MWh and emphasize the significant influence of the\ncharging location on the spatial and temporal distribution of this demand.\nNotably, charging at public places can help smooth the charging demand\nthroughout the day, mitigating peak charging loads on the electricity grid. We\nalso estimate charging station requirements, finding that workplace charging\nrequires approximately one charging point per three electric vehicles, while\npublic charging requires only one per thirty. Finally, we demonstrate that\nphotovoltaic energy can cover a substantial share of the charging needs,\nemphasizing the potential for renewable energy integration. This study lays the\ngroundwork for electric mobility planning in Addis Ababa while offering a\ntransferable framework for other African cities.",
        "Intelligent surfaces represent a breakthrough technology capable of\ncustomizing the wireless channel cost-effectively. However, the existing works\ngenerally focus on planar wavefront, neglecting near-field spherical wavefront\ncharacteristics caused by large array aperture and high operation frequencies\nin the terahertz (THz). Additionally, the single-layer reconfigurable\nintelligent surface (RIS) lacks the signal processing ability to mitigate the\ncomputational complexity at the base station (BS). To address this issue, we\nintroduce a novel stacked intelligent metasurfaces (SIM) comprised of an array\nof programmable metasurface layers. The SIM aims to substitute conventional\ndigital baseband architecture to execute computing tasks with ultra-low\nprocessing delay, albeit with a reduced number of radio-frequency (RF) chains\nand low-resolution digital-to-analog converters. In this paper, we present a\nSIM-aided multiuser multiple-input single-output (MU-MISO) near-field system,\nwhere the SIM is integrated into the BS to perform beamfocusing in the wave\ndomain and customize an end-to-end channel with minimized inter-user\ninterference. Finally, the numerical results demonstrate that near-field\ncommunication achieves superior spatial gain over the far-field, and the SIM\neffectively suppresses inter-user interference as the wireless signals\npropagate through it.",
        "In abstract visual reasoning, monolithic deep learning models suffer from\nlimited interpretability and generalization, while existing neuro-symbolic\napproaches fall short in capturing the diversity and systematicity of\nattributes and relation representations. To address these challenges, we\npropose a Systematic Abductive Reasoning model with diverse relation\nrepresentations (Rel-SAR) in Vector-symbolic Architecture (VSA) to solve\nRaven's Progressive Matrices (RPM). To derive attribute representations with\nsymbolic reasoning potential, we introduce not only various types of atomic\nvectors that represent numeric, periodic and logical semantics, but also the\nstructured high-dimentional representation (SHDR) for the overall Grid\ncomponent. For systematic reasoning, we propose novel numerical and logical\nrelation functions and perform rule abduction and execution in a unified\nframework that integrates these relation representations. Experimental results\ndemonstrate that Rel-SAR achieves significant improvement on RPM tasks and\nexhibits robust out-of-distribution generalization. Rel-SAR leverages the\nsynergy between HD attribute representations and symbolic reasoning to achieve\nsystematic abductive reasoning with both interpretable and computable\nsemantics.",
        "In 1987, Hitchin introduced the self-duality equations on rank-2 complex\nvector bundles over compact Riemann surfaces with genus greater than one as a\nreduction of the Yang-Mills equation and established the existence of solutions\nto these equations starting from a Higgs stable bundle. In this paper, we fill\nin some technical details in Hitchin's original proof by the following three\nsteps. First, we reduce the existence of a solution of class $L_1^2$ to\nminimizing the energy functional within a Higgs stable orbit of the $L_2^2$\ncomplex gauge group action. Second, using this transformation, we obtain a\nsolution of class $L_1^2$ in this orbit. These two steps primarily follow\nHitchin's original approach. Finally, using the Coulomb gauge, we construct a\nsmooth solution by applying an $L_2^2$ unitary gauge transformation to the\n$L_1^2$ solution constructed previously. This last step provides additional\ntechnical details to Hitchin's original proof.",
        "Context. The Bondi spherical accretion solution has been used to model\naccretion onto compact objects in a variety of situations, from interpretation\nof observations to subgrid models in cosmological simulations. Aims. We aim to\ninvestigate how the presence of dark matter (DM) alters the dynamics and\nphysical properties of accretion onto supermassive black holes on scales\nranging from ~ 10 pc to the event horizon. Methods. In particular, we\ninvestigate Bondi-like accretion flows with zero and low specific angular\nmomentum around supermassive black holes surrounded by dark-matter halos by\nperforming 1D and 2.5D general relativistic hydrodynamics (GRHD) simulations\nusing the black hole accretion code (BHAC). Results. We find notable\ndifferences in the dynamics and structure of spherical accretion flows in the\npresence of DM. The most significant effects include increases in density,\ntemperature, and pressure, as well as variations in radial velocity both inside\nand outside the regions containing DM or even the production of outflow.\nConclusions. This investigation provides valuable insights into the role of\ncosmological effects, particularly DM, in shaping the behavior of accretion\nflows and black holes (BHs). Our simulations may be directly applicable to\nmodel systems with a large black hole-to-halo mass ratio, which are expected to\nbe found at very high redshifts.",
        "We show that the average trajectories of relativistic quantum particles in\nSchwarzschild spacetime, obtained via quantum mechanical weak measurements of\nmomentum and energy, are equivalent to the predicted flow lines of probability\ncurrent in curved spacetime quantum theory. We subsequently demonstrate that\nthese trajectories correspond exactly to classical null geodesics in a hybrid\nSchwarzschild-Alcubierre spacetime. This threefold equivalence demonstrates how\nquantum theory in curved spacetime can be formulated via operationally-defined\nmeasurements, and that such a theory may be interpreted deterministically, in\nthe spirit of hidden-variable models such as Bohmian mechanics, through the\nnovel connection to an underlying \"guiding metric.\"",
        "Physics-informed neural networks (PINNs) have earned high expectations in\nsolving partial differential equations (PDEs), but their optimization usually\nfaces thorny challenges due to the unique derivative-dependent loss function.\nBy analyzing the loss distribution, previous research observed the propagation\nfailure phenomenon of PINNs, intuitively described as the correct supervision\nfor model outputs cannot ``propagate'' from initial states or boundaries to the\ninterior domain. Going beyond intuitive understanding, this paper provides the\nfirst formal and in-depth study of propagation failure and its root cause.\nBased on a detailed comparison with classical finite element methods, we\nascribe the failure to the conventional single-point-processing architecture of\nPINNs and further prove that propagation failure is essentially caused by the\nlower gradient correlation of PINN models on nearby collocation points.\nCompared to superficial loss maps, this new perspective provides a more precise\nquantitative criterion to identify where and why PINN fails. The theoretical\nfinding also inspires us to present a new PINN architecture, named ProPINN,\nwhich can effectively unite the gradient of region points for better\npropagation. ProPINN can reliably resolve PINN failure modes and significantly\nsurpass advanced Transformer-based models with 46% relative promotion.",
        "Fact-checking tabular data is essential for ensuring the accuracy of\nstructured information. However, existing methods often rely on black-box\nmodels with opaque reasoning. We introduce RePanda, a structured fact\nverification approach that translates claims into executable pandas queries,\nenabling interpretable and verifiable reasoning.\n  To train RePanda, we construct PanTabFact, a structured dataset derived from\nthe TabFact train set, where claims are paired with executable queries\ngenerated using DeepSeek-Chat and refined through automated error correction.\nFine-tuning DeepSeek-coder-7B-instruct-v1.5 on PanTabFact, RePanda achieves\n84.09% accuracy on the TabFact test set.\n  To evaluate Out-of-Distribution (OOD) generalization, we interpret\nquestion-answer pairs from WikiTableQuestions as factual claims and refer to\nthis dataset as WikiFact. Without additional fine-tuning, RePanda achieves\n84.72% accuracy on WikiFact, significantly outperforming all other baselines\nand demonstrating strong OOD robustness. Notably, these results closely match\nthe zero-shot performance of DeepSeek-Chat (671B), indicating that our\nfine-tuning approach effectively distills structured reasoning from a much\nlarger model into a compact, locally executable 7B model.\n  Beyond fact verification, RePanda extends to tabular question answering by\ngenerating executable queries that retrieve precise answers. To support this,\nwe introduce PanWiki, a dataset mapping WikiTableQuestions to pandas queries.\nFine-tuning on PanWiki, RePanda achieves 75.1% accuracy in direct answer\nretrieval. These results highlight the effectiveness of structured\nexecution-based reasoning for tabular verification and question answering.\n  We have publicly released the dataset on Hugging Face at\ndatasets\/AtoosaChegini\/PanTabFact.",
        "Understanding collaborative writing dynamics between native speakers (NS) and\nnon-native speakers (NNS) is critical for enhancing collaboration quality and\nteam inclusivity. In this paper, we partnered with communication researchers to\ndevelop visual analytics solutions for comparing NS and NNS behaviors in 162\nwriting sessions across 27 teams. The primary challenges in analyzing writing\nbehaviors are data complexity and the uncertainties introduced by automated\nmethods. In response, we present \\textsc{COALA}, a novel visual analytics tool\nthat improves model interpretability by displaying uncertainties in author\nclusters, generating behavior summaries using large language models, and\nvisualizing writing-related actions at multiple granularities. We validated the\neffectiveness of \\textsc{COALA} through user studies with domain experts\n(N=2+2) and researchers with relevant experience (N=8). We present the insights\ndiscovered by participants using \\textsc{COALA}, suggest features for future\nAI-assisted collaborative writing tools, and discuss the broader implications\nfor analyzing collaborative processes beyond writing."
      ]
    }
  },
  {
    "id":2411.00614,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"Causal identification of single-cell experimental perturbation effects with CINEMA-OT",
    "start_abstract":"Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion"
      ],
      "abstract":[
        "Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Leveraging Sequence Purification for Accurate Prediction of Multiple\n  Conformational States with AlphaFold2",
        "Advances in RNA secondary structure prediction and RNA modifications:\n  Methods, data, and applications",
        "An Evaluation on the Role of Non-Coding RNA in HIV Transcription and\n  Latency: A Review",
        "Pushing the boundaries of Structure-Based Drug Design through\n  Collaboration with Large Language Models",
        "DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown\n  Efficiency",
        "An Energy-Adaptive Elastic Equivariant Transformer Framework for Protein\n  Structure Representation",
        "Progress of the anti-obesity of Berberine",
        "Applying computational protein design to therapeutic antibody discovery\n  -- current state and perspectives",
        "The standard coil or globule phases cannot describe the denatured state\n  of structured proteins and intrinsically disordered proteins",
        "PyMOLfold: Interactive Protein and Ligand Structure Prediction in PyMOL",
        "Inverse problems with experiment-guided AlphaFold",
        "Mechanism of Electricacupuncture Treating Detrusor Bladder Neck\n  Dyscoordination After Suprasacral Spinal Cord Injury by Proteomics",
        "Nonsuppressible viremia during HIV-1 therapy meets molecular virology",
        "Nuclear matter in relativistic Brueckner-Hartree-Fock theory with local\n  and nonlocal covariant chiral interactions at leading order",
        "Nucleolus Credit Assignment for Effective Coalitions in Multi-agent\n  Reinforcement Learning",
        "Mechanics and Design of Metastructured Auxetic Patches with Bio-inspired\n  Materials",
        "Rethinking High-speed Image Reconstruction Framework with Spike Camera",
        "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
        "Optimal disk packing of chloroplasts in plant cells",
        "Are the Majority of Public Computational Notebooks Pathologically\n  Non-Executable?",
        "Native Three-Body Interactions in a Superconducting Lattice Gauge\n  Quantum Simulator",
        "Test Schedule Generation for Acceptance Testing of Mission-Critical\n  Satellite Systems",
        "A novel approach to data generation in generative model",
        "Towards Location-Specific Precipitation Projections Using Deep Neural\n  Networks",
        "Rotatable Antenna Enabled Wireless Communication: Modeling and\n  Optimization",
        "Adapting Automatic Speech Recognition for Accented Air Traffic Control\n  Communications",
        "A spatially varying differential equation for multi-patch pandemic\n  propagation",
        "Quantification of Uncertainties in Probabilistic Deep Neural Network by\n  Implementing Boosting of Variational Inference"
      ],
      "abstract":[
        "AlphaFold2 (AF2) has transformed protein structure prediction by harnessing\nco-evolutionary constraints embedded in multiple sequence alignments (MSAs).\nMSAs not only encode static structural information, but also hold critical\ndetails about protein dynamics, which underpin biological functions. However,\nthese subtle co-evolutionary signatures, which dictate conformational state\npreferences, are often obscured by noise within MSA data and thus remain\nchallenging to decipher. Here, we introduce AF-ClaSeq, a systematic framework\nthat isolates these co-evolutionary signals through sequence purification and\niterative enrichment. By extracting sequence subsets that preferentially encode\ndistinct structural states, AF-ClaSeq enables high-confidence predictions of\nalternative conformations. Our findings reveal that the successful sampling of\nalternative states depends not on MSA depth but on sequence purity.\nIntriguingly, purified sequences encoding specific structural states are\ndistributed across phylogenetic clades and superfamilies, rather than confined\nto specific lineages. Expanding upon AF2's transformative capabilities,\nAF-ClaSeq provides a powerful approach for uncovering hidden structural\nplasticity, advancing allosteric protein and drug design, and facilitating\ndynamics-based protein function annotation.",
        "Due to the hierarchical organization of RNA structures and their pivotal\nroles in fulfilling RNA functions, the formation of RNA secondary structure\ncritically influences many biological processes and has thus been a crucial\nresearch topic. This review sets out to explore the computational prediction of\nRNA secondary structure and its connections to RNA modifications, which have\nemerged as an active domain in recent years. We first examine the progression\nof RNA secondary structure prediction methodology, focusing on a set of\nrepresentative works categorized into thermodynamic, comparative, machine\nlearning, and hybrid approaches. Next, we survey the advances in RNA\nmodifications and computational methods for identifying RNA modifications,\nfocusing on the prominent modification types. Subsequently, we highlight the\ninterplay between RNA modifications and secondary structures, emphasizing how\nmodifications such as m6A dynamically affect RNA folding and vice versa. In\naddition, we also review relevant data sources and provide a discussion of\ncurrent challenges and opportunities in the field. Ultimately, we hope our\nreview will be able to serve as a cornerstone to aid in the development of\ninnovative methods for this emerging topic and foster therapeutic applications\nin the future.",
        "The existence of latent cellular reservoirs is recognized as the major\nbarrier to an HIV cure. Reactivating and eliminating \"shock and kill\" or\npermanently silencing \"block and lock\" the latent HIV reservoir, as well as\ngene editing, remain promising approaches, but so far have proven to be only\npartially successful. Moreover, using latency reversing agents or \"block and\nlock\" drugs pose additional considerations, including the ability to cause\ncellular toxicity, a potential lack of specificity for HIV, or low potency when\neach agent is used alone. RNA molecules, such as microRNAs (miRNAs) and long\nnon-coding RNAs (lncRNAs) are becoming increasingly recognized as important\nregulators of gene expression. RNA-based approaches for combatting HIV latency\nrepresent a promising strategy since both miRNAs and lncRNAs are more cell-type\nand tissue specific than protein coding genes. Thus, a higher specificity of\ntargeting the latent HIV reservoir with less overall cellular toxicity can\nlikely be achieved. In this review, we summarize current knowledge about HIV\ngene expression regulation by miRNAs and lncRNAs encoded in the human genome,\nas well as regulatory molecules encoded in the HIV genome. We discuss both the\ntranscriptional and post-transcriptional regulation of HIV gene expression to\nalign with the current definition of latency, and describe RNA molecules that\neither promote HIV latency or have anti-latency properties. Finally, we provide\nperspectives on using each class of RNAs as potential targets for combatting\nHIV latency, and describe the complexity of the interactions between different\nRNA molecules, their protein targets, and HIV.",
        "Structure-Based Drug Design (SBDD) has revolutionized drug discovery by\nenabling the rational design of molecules for specific protein targets. Despite\nsignificant advancements in improving docking scores, advanced 3D-SBDD\ngenerative models still face challenges in producing drug-like candidates that\nmeet medicinal chemistry standards and pharmacokinetic requirements. These\nlimitations arise from their inherent focus on molecular interactions, often\nneglecting critical aspects of drug-likeness. To address these shortcomings, we\nintroduce the Collaborative Intelligence Drug Design (CIDD) framework, which\ncombines the structural precision of 3D-SBDD models with the chemical reasoning\ncapabilities of large language models (LLMs). CIDD begins by generating\nsupporting molecules with 3D-SBDD models and then refines these molecules\nthrough LLM-supported modules to enhance drug-likeness and structural\nreasonability. When evaluated on the CrossDocked2020 dataset, CIDD achieved a\nremarkable success ratio of 37.94%, significantly outperforming the previous\nstate-of-the-art benchmark of 15.72%. Although improving molecular interactions\nand drug-likeness is often seen as a trade-off, CIDD uniquely achieves a\nbalanced improvement in both by leveraging the complementary strengths of\ndifferent models, offering a robust and innovative pathway for designing\ntherapeutically promising drug candidates.",
        "Background: Small interfering RNA (siRNA) is a promising therapeutic agent\ndue to its ability to silence disease-related genes via RNA interference. While\ntraditional machine learning and early deep learning methods have made progress\nin predicting siRNA efficacy, there remains significant room for improvement.\nAdvanced deep learning techniques can enhance prediction accuracy, reducing the\nreliance on extensive wet-lab experiments and accelerating the identification\nof effective siRNA sequences. This approach also provides deeper insights into\nthe mechanisms of siRNA efficacy, facilitating more targeted and efficient\ntherapeutic strategies.\n  Methods: We introduce DeepSilencer, an innovative deep learning model\ndesigned to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced\nneural network architectures to capture the complex features of siRNA\nsequences. Our key contributions include a specially designed deep learning\nmodel, an innovative online data sampling method, and an improved loss function\ntailored for siRNA prediction. These enhancements collectively boost the\nmodel's prediction accuracy and robustness.\n  Results: Extensive evaluations on multiple test sets demonstrate that\nDeepSilencer achieves state-of-the-art performance using only siRNA sequences\nand basic physicochemical properties. Our model surpasses several other methods\nand shows superior predictive performance, particularly when incorporating\nthermodynamic parameters.\n  Conclusion: The advancements in data sampling, model design, and loss\nfunction significantly enhance the predictive capabilities of DeepSilencer.\nThese improvements underscore its potential to advance RNAi therapeutic design\nand development, offering a powerful tool for researchers and clinicians.",
        "Structure-informed protein representation learning is essential for effective\nprotein function annotation and \\textit{de novo} design. However, the presence\nof inherent noise in both crystal and AlphaFold-predicted structures poses\nsignificant challenges for existing methods in learning robust protein\nrepresentations. To address these issues, we propose a novel equivariant\nTransformer-State Space Model(SSM) hybrid framework, termed $E^3$former,\ndesigned for efficient protein representation. Our approach uses energy\nfunction-based receptive fields to construct proximity graphs and incorporates\nan equivariant high-tensor-elastic selective SSM within the transformer\narchitecture. These components enable the model to adapt to complex atom\ninteractions and extract geometric features with higher signal-to-noise ratios.\nEmpirical results demonstrate that our model outperforms existing methods in\nstructure-intensive tasks, such as inverse folding and binding site prediction,\nparticularly when using predicted structures, owing to its enhanced tolerance\nto data deviation and noise. Our approach offers a novel perspective for\nconducting biological function research and drug discovery using noisy protein\nstructure data.",
        "Obesity is defined as the excessive accumulation or abnormal distribution of\nbody fat. According to data from World Obesity Atlas 2024, the increase in\nprevalence of obesity has become a major worldwide health problem in adults as\nwell as among children and adolescents. Although an increasing number of drugs\nhave been approved for the treatment of obesity in recent years, many of these\ndrugs have inevitable side effects which have increased the demand for new\nsafe, accessible and effective drugs for obesity and prompt interest in natural\nproducts. Berberine (BBR) and its metabolites, known for their multiple\npharmacological effects. Recent studies have emphatically highlighted the\nanti-obesity benefits of BBR and the underlying mechanisms have been gradually\nelucidated. However, its clinical application is limited by poor oral\nabsorption and low bioavailability. Based on this, this review summarizes\ncurrent research on the anti-obesity effects of BBR and its metabolites,\nincluding advancements in clinical trail results, understanding potential\nmolecular mechanisms and absorption and bioavailability. As a natural compound\nderived from plants, BBR holds potential as an alternative approach for\nmanaging obesity.",
        "Machine learning applications in protein sciences have ushered in a new era\nfor designing molecules in silico. Antibodies, which currently form the largest\ngroup of biologics in clinical use, stand to benefit greatly from this shift.\nDespite the proliferation of these protein design tools, their direct\napplication to antibodies is often limited by the unique structural biology of\nthese molecules. Here, we review the current computational methods for antibody\ndesign, highlighting their role in advancing computational drug discovery.",
        "The concepts of globule and random coil were developed to describe the phases\nof homopolymers and then used to characterize the denatured state of structured\ncytosolic proteins and intrinsically disordered proteins. Using multi-scale\nmolecular dynamics simulations, we were able to explore the conformational\nspace of the disordered conformations of both types of protein under biological\nconditions in an affordable amount of computational time. By studying the size\nof the protein and the density correlations in space, we conclude that the\nstandard phases of homopolymers and the tools to detect them cannot be applied\nstraightforwardly to proteins.",
        "PyMOLfold is a flexible and open-source plugin designed to seamlessly\nintegrate AI-based protein structure prediction and visualization within the\nwidely used PyMOL molecular graphics system. By leveraging state-of-the-art\nprotein folding models such as ESM3, Boltz-1, and Chai-1, PyMOLfold allows\nresearchers to directly predict protein tertiary structures from amino acid\nsequences without requiring external tools or complex workflows. Furthermore,\nwith certain models, users can provide a SMILES string of a ligand and have the\nsmall molecule placed in the protein structure. This unique capability bridges\nthe gap between computational folding and structural visualization, enabling\nusers to input a primary sequence, perform a folding prediction, and\nimmediately explore the resulting 3D structure within the same intuitive\nplatform.",
        "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
        "Objectives This study aimed to elucidate the potential mechanisms of\nelectroacupuncture (EA) in restoring detrusor-bladder neck dyssynergesia (DBND)\nfollowing suprasacral spinal cord injury.\n  Methods A total of 52 adult female Sprague-Dawley rats were randomly assigned\nto either a sham group (n=12) or a spinal cord injury model group (n=40). In\nthe model group, DBND was induced in 40 rats through Hassan Shaker spinal cord\ntransection, with 24 rats surviving spinal shock and subsequently randomized\ninto two groups: a model-only group (DBND, n=12) and an EA intervention group\n(DBND+EA, n=12). DBND+EA was administered at Ciliao (BL32), Zhongji (RN3), and\nSanyinjiao (SP6) acupoints, for 20 minutes per session, once daily for 10\nconsecutive days. On day 29 post-injury, all rats underwent urodynamic\nassessments, followed by hematoxylin and eosin (HE) staining, tandem mass tag\n(TMT) proteomics, and Western blot (WB) analysis of the detrusor and bladder\nneck tissues.\n  Results Urodynamic evaluation demonstrated that EA intervention enhanced\nbladder function in DBND rats. HE staining indicated reduced fibroplasia in the\ndetrusor muscle and alleviated inflammation in the bladder neck following EA.\nTMT proteomic analysis revealed 30 differentially expressed proteins (DEPs) in\nthe detrusor and 59 DEPs in the bladder neck post-EA treatment. WB results\ncorroborated these TMT findings.\n  Conclusion EA effectively promotes synergy between the detrusor muscle and\nbladder neck in DBND, likely by enhancing detrusor contractility and\nfacilitating bladder neck relaxation during urination. This study provides\nmechanistic insights into the therapeutic role of EA in managing DBND.",
        "HIV-1 replication can be suppressed with antiretroviral therapy (ART), but\nindividuals who stop taking ART soon become viremic again. Some people\nexperience extended times of detectable viremia despite optimal adherence to\nART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of\nnonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded\nT cells harboring HIV-1 proviruses with small deletions or mutations in the\n5'-leader, the UTR that includes the major splice donor site of viral RNA.\nThese mutations altered viral RNA-splicing efficiency and RNA dimerization and\npackaging, yet still allowed production of detectable levels of noninfectious\nvirus particles. These particles lacked the HIV-1 Env surface protein required\nfor cell entry and failed to form the mature capsid cone required for\ninfectivity. These studies improve our understanding of NSV and the regulation\nof viral functions in the 5'-leader with implications for rationalized care in\nindividuals with NSV.",
        "The simultaneous description for nuclear matter and finite nuclei has been a\nlong-standing challenge in nuclear ab initio theory. With the success for\nnuclear matter, the relativistic Brueckner-Hartree-Fock (RBHF) theory with\ncovariant chiral interactions is a promising ab initio approach to describe\nboth nuclear matter and finite nuclei. In the description of the finite nuclei\nwith the current RBHF theory, the covariant chiral interactions have to be\nlocalized to make calculations feasible. In order to examine the reliability\nand validity, in this letter, the RBHF theory with local and nonlocal covariant\nchiral interactions at leading order are applied for nuclear matter. The\nlow-energy constants in the covariant chiral interactions determined with the\nlocal regularization are close to those with the nonlocal regularization.\nMoreover, the RBHF theory with local and nonlocal covariant chiral interactions\nprovide equally well description of the saturation properties of nuclear\nmatter. The present work paves the way for the implementation of covariant\nchiral interactions in RBHF theory for finite nuclei.",
        "In cooperative multi-agent reinforcement learning (MARL), agents typically\nform a single grand coalition based on credit assignment to tackle a composite\ntask, often resulting in suboptimal performance. This paper proposed a\nnucleolus-based credit assignment grounded in cooperative game theory, enabling\nthe autonomous partitioning of agents into multiple small coalitions that can\neffectively identify and complete subtasks within a larger composite task.\nSpecifically, our designed nucleolus Q-learning could assign fair credits to\neach agent, and the nucleolus Q-operator provides theoretical guarantees with\ninterpretability for both learning convergence and the stability of the formed\nsmall coalitions. Through experiments on Predator-Prey and StarCraft scenarios\nacross varying difficulty levels, our approach demonstrated the emergence of\nmultiple effective coalitions during MARL training, leading to faster learning\nand superior performance in terms of win rate and cumulative rewards especially\nin hard and super-hard environments, compared to four baseline methods. Our\nnucleolus-based credit assignment showed the promise for complex composite\ntasks requiring effective subteams of agents.",
        "Metastructured auxetic patches, characterized by negative Poisson's ratios,\noffer unique mechanical properties that closely resemble the behavior of human\ntissues and organs. As a result, these patches have gained significant\nattention for their potential applications in organ repair and tissue\nregeneration. This study focuses on neural networks-based computational\nmodeling of auxetic patches with a sinusoidal metastructure fabricated from\nsilk fibroin, a bio-inspired material known for its biocompatibility and\nstrength. The primary objective of this research is to introduce a novel,\ndata-driven framework for patch design. To achieve this, we conducted\nexperimental fabrication and mechanical testing to determine material\nproperties and validate the corresponding finite element models. Finite element\nsimulations were then employed to generate the necessary data, while greedy\nsampling, an active learning technique, was utilized to reduce the\ncomputational cost associated with data labeling. Two neural networks were\ntrained to accurately predict Poisson's ratios and stresses for strains up to\n15\\%, respectively. Both models achieved $R^2$ scores exceeding 0.995, which\nindicates highly reliable predictions. Building on this, we developed a neural\nnetwork-based design model capable of tailoring patch designs to achieve\nspecific mechanical properties. This model demonstrated superior performance\nwhen compared to traditional optimization methods, such as genetic algorithms,\nby providing more efficient and precise design solutions. The proposed\nframework represents a significant advancement in the design of bio-inspired\nmetastructures for medical applications, paving the way for future innovations\nin tissue engineering and regenerative medicine.",
        "Spike cameras, as innovative neuromorphic devices, generate continuous spike\nstreams to capture high-speed scenes with lower bandwidth and higher dynamic\nrange than traditional RGB cameras. However, reconstructing high-quality images\nfrom the spike input under low-light conditions remains challenging.\nConventional learning-based methods often rely on the synthetic dataset as the\nsupervision for training. Still, these approaches falter when dealing with\nnoisy spikes fired under the low-light environment, leading to further\nperformance degradation in the real-world dataset. This phenomenon is primarily\ndue to inadequate noise modelling and the domain gap between synthetic and real\ndatasets, resulting in recovered images with unclear textures, excessive noise,\nand diminished brightness. To address these challenges, we introduce a novel\nspike-to-image reconstruction framework SpikeCLIP that goes beyond traditional\ntraining paradigms. Leveraging the CLIP model's powerful capability to align\ntext and images, we incorporate the textual description of the captured scene\nand unpaired high-quality datasets as the supervision. Our experiments on\nreal-world low-light datasets U-CALTECH and U-CIFAR demonstrate that SpikeCLIP\nsignificantly enhances texture details and the luminance balance of recovered\nimages. Furthermore, the reconstructed images are well-aligned with the broader\nvisual features needed for downstream tasks, ensuring more robust and versatile\nperformance in challenging environments.",
        "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
        "Photosynthesis is vital for the survival of entire ecosystems on Earth. While\nlight is fundamental to this process, excessive exposure can be detrimental to\nplant cells. Chloroplasts, the photosynthetic organelles, actively move in\nresponse to light and self-organize within the cell to tune light absorption.\nThese disk-shaped motile organelles must balance dense packing for enhanced\nlight absorption under dim conditions with spatial rearrangements to avoid\ndamage from excessive light exposure. Here, we reveal that the packing\ncharacteristics of chloroplasts within plant cells show signatures of\noptimality. Combining measurements of chloroplast densities and\nthree-dimensional cell shape in the water plant Elodea densa, we construct an\nargument for optimal cell shape versus chloroplast size to achieve two targets:\ndense packing into a two-dimensional monolayer for optimal absorption under dim\nlight conditions and packing at the sidewalls for optimal light avoidance. We\nformalize these constraints using a model for random close packing matched with\npacking simulations of polydisperse hard disks confined within rectangular\nboxes. The optimal cell shape resulting from these models corresponds closely\nto that measured in the box-like plant cells, highlighting the importance of\nparticle packing in the light adaptation of plants. Understanding the interplay\nbetween structure and function sheds light on how plants achieve efficient\nphoto adaptation. It also highlights a broader principle: how cell shape\nrelates to the optimization of packing finite and relatively small numbers of\norganelles under confinement. This universal challenge in biological systems\nshares fundamental features with the mechanics of confined granular media and\nthe jamming transitions in dense active and passive systems across various\nscales and contexts.",
        "Computational notebooks are the de facto platforms for exploratory data\nscience, offering an interactive programming environment where users can\ncreate, modify, and execute code cells in any sequence. However, this\nflexibility often introduces code quality issues, with prior studies showing\nthat approximately 76% of public notebooks are non-executable, raising\nsignificant concerns about reusability. We argue that the traditional notion of\nexecutability - requiring a notebook to run fully and without error - is overly\nrigid, misclassifying many notebooks and overestimating their\nnon-executability. This paper investigates pathological executability issues in\npublic notebooks under varying notions and degrees of executability. Even\npartially improving executability can improve code comprehension and offer a\npathway for dynamic analyses. With this insight, we first categorize notebooks\ninto potentially restorable and pathological non-executable notebooks and then\nmeasure how removing misconfiguration and superficial execution issues in\nnotebooks can improve their executability (i.e., additional cells executed\nwithout error). In a dataset of 42,546 popular public notebooks containing\n34,659 non-executable notebooks, only 21.3% are truly pathologically\nnon-executable. For restorable notebooks, LLM-based methods fully restore 5.4%\nof previously non-executable notebooks. Among the partially restored, the\nexecutability of notebooks improves by 42.7% and 28% by installing the correct\nmodules and generating synthetic data. These findings challenge prior\nassumptions, suggesting that notebooks have higher executability than\npreviously reported, many of which offer valuable partial execution, and that\ntheir executability should be evaluated within the interactive notebook\nparadigm rather than through traditional software executability standards.",
        "While universal quantum computers remain under development, analog quantum\nsimulators offer a powerful alternative for understanding complex systems in\ncondensed matter, chemistry, and high-energy physics. One compelling\napplication is the characterization of real-time lattice gauge theories (LGTs).\nLGTs are nonperturbative tools, utilizing discretized spacetime to describe\ngauge-invariant models. They hold immense potential for understanding\nfundamental physics but require enforcing local constraints analogous to\nelectromagnetism's Gauss's Law. These constraints, which arise from gauge\nsymmetries and dictate the form of the interaction between matter and gauge\nfields, are a significant challenge for simulators to enforce. Implementing\nthese constraints at the hardware level in analog simulations is crucial. This\nrequires realizing multibody interactions between matter and gauge-field\nelements, enabling them to evolve together while suppressing unwanted two-body\ninteractions that violate the gauge symmetry. In this paper, we propose and\nimplement a novel parametrically activated three-qubit interaction within a\ncircuit quantum electrodynamics architecture. We experimentally demonstrate a\nminimal $U(1)$ spin-1\/2 model with a time evolution that intrinsically\nsatisfies Gauss's law in the system. This design serves as the foundational\nblock for simulating LGTs on a superconducting photonic lattice.",
        "Mission-critical system, such as satellite systems, healthcare systems, and\nnuclear power plant control systems, undergo rigorous testing to ensure they\nmeet specific operational requirements throughout their operation. This\nincludes Operational Acceptance Testing (OAT), which aims to ensure that the\nsystem functions correctly under real-world operational conditions. In\nsatellite development, In-Orbit Testing (IOT) is a crucial OAT activity\nperformed regularly and as needed after deployment in orbit to check the\nsatellite's performance and ensure that operational requirements are met. The\nscheduling of an IOT campaign, which executes multiple IOT procedures, is an\nimportant yet challenging problem, as it accounts for various factors,\nincluding satellite visibility, antenna usage costs, testing time periods, and\noperational constraints. To address the IOT scheduling problem, we propose a\nmulti-objective approach to generate near-optimal IOT schedules, accounting for\noperational costs, fragmentation (i.e., the splitting of tests), and resource\nefficiency, which align with practitioners' objectives for IOT scheduling. Our\nindustrial case study with SES Techcom shows significant improvements, as\nfollows: an average improvement of 49.4% in the cost objective, 60.4% in the\nfragmentation objective, and 30% in the resource usage objective, compared to\nour baselines. Additionally, our approach improves cost efficiency by 538% and\nresource usage efficiency by 39.42% compared to manually constructed schedules\nprovided by practitioners, while requiring only 12.5% of the time needed for\nmanual IOT scheduling.",
        "Variational Autoencoders (VAEs) and other generative models are widely\nemployed in artificial intelligence to synthesize new data. However, current\napproaches rely on Euclidean geometric assumptions and statistical\napproximations that fail to capture the structured and emergent nature of data\ngeneration. This paper introduces the Convergent Fusion Paradigm (CFP) theory,\na novel geometric framework that redefines data generation by integrating\ndimensional expansion accompanied by qualitative transformation. By modifying\nthe latent space geometry to interact with emergent high-dimensional\nstructures, CFP theory addresses key challenges such as identifiability issues\nand unintended artifacts like hallucinations in Large Language Models (LLMs).\nCFP theory is based on two key conceptual hypotheses that redefine how\ngenerative models structure relationships between data and algorithms. Through\nthe lens of CFP theory, we critically examine existing metric-learning\napproaches. CFP theory advances this perspective by introducing time-reversed\nmetric embeddings and structural convergence mechanisms, leading to a novel\ngeometric approach that better accounts for data generation as a structured\nepistemic process. Beyond its computational implications, CFP theory provides\nphilosophical insights into the ontological underpinnings of data generation.\nBy offering a systematic framework for high-dimensional learning dynamics, CFP\ntheory contributes to establishing a theoretical foundation for understanding\nthe data-relationship structures in AI. Finally, future research in CFP theory\nwill be led to its implications for fully realizing qualitative\ntransformations, introducing the potential of Hilbert space in generative\nmodeling.",
        "Accurate precipitation estimates at individual locations are crucial for\nweather forecasting and spatial analysis. This study presents a paradigm shift\nby leveraging Deep Neural Networks (DNNs) to surpass traditional methods like\nKriging for station-specific precipitation approximation. We propose two\ninnovative NN architectures: one utilizing precipitation, elevation, and\nlocation, and another incorporating additional meteorological parameters like\nhumidity, temperature, and wind speed. Trained on a vast dataset (1980-2019),\nthese models outperform Kriging across various evaluation metrics (correlation\ncoefficient, root mean square error, bias, and skill score) on a five-year\nvalidation set. This compelling evidence demonstrates the transformative power\nof deep learning for spatial prediction, offering a robust and precise\nalternative for station-specific precipitation estimation.",
        "Fluid antenna system (FAS) and movable antenna (MA) have recently emerged as\npromising technologies to exploit new spatial degrees of freedom (DoFs), which\nhave attracted growing attention in wireless communication. In this paper, we\npropose a new rotatable antenna (RA) model to improve the performance of\nwireless communication systems. Different from conventional fixed antennas, the\nproposed RA system can flexibly alter the three-dimensional (3D) boresight\ndirection of each antenna independently by adjusting its deflection angles to\nachieve a desired array directional gain pattern. Specifically, we investigate\nan RA-enabled uplink communication system, where the receive beamforming and\nthe deflection angles of all RAs at the base station (BS) are jointly optimized\nto maximize the minimum signal-to-interference-plus-noise ratio (SINR) among\nall the users. In the special single-user and free-space propagation setup, the\noptimal deflection angles of RAs are derived in closed form with the\nmaximum-ratio combining (MRC) beamformer applied at the BS. Moreover, we\nanalyze the asymptotic performance with an infinite number of antennas based on\nthis solution, which theoretically proves that the RA system can achieve a\nhigher array gain as compared to the fixed-antenna system. In the general\nmulti-user and multi-path channel setup, we first propose an alternating\noptimization (AO) algorithm to alternately optimize the receive beamforming and\nthe deflection angles of RAs in an iterative manner. Then, a two-stage\nalgorithm that solves the formulated problem without the need for iteration is\nfurther proposed to reduce computational complexity. Simulation results are\nprovided to validate our analytical results and demonstrate that the proposed\nRA system can significantly outperform other benchmark schemes.",
        "Effective communication in Air Traffic Control (ATC) is critical to\nmaintaining aviation safety, yet the challenges posed by accented English\nremain largely unaddressed in Automatic Speech Recognition (ASR) systems.\nExisting models struggle with transcription accuracy for Southeast\nAsian-accented (SEA-accented) speech, particularly in noisy ATC environments.\nThis study presents the development of ASR models fine-tuned specifically for\nSoutheast Asian accents using a newly created dataset. Our research achieves\nsignificant improvements, achieving a Word Error Rate (WER) of 0.0982 or 9.82%\non SEA-accented ATC speech. Additionally, the paper highlights the importance\nof region-specific datasets and accent-focused training, offering a pathway for\ndeploying ASR systems in resource-constrained military operations. The findings\nemphasize the need for noise-robust training techniques and region-specific\ndatasets to improve transcription accuracy for non-Western accents in ATC\ncommunications.",
        "We develop an extension of the Susceptible-Infected-Recovery (SIR) model to\naccount for spatial variations in population as well as infection and recovery\nparameters. The equations are derived by taking the continuum limit of discrete\ninteracting patches, and results in a diffusion equation with some nonlinear\nterms. The resulting population dynamics can be reinterpreted as a nonlinear\nheat flow equation where the temperature vector captures both infected and\nrecovered populations across multiple patches.",
        "Modern neural network architectures have achieved remarkable accuracies but\nremain highly dependent on their training data, often lacking interpretability\nin their learned mappings. While effective on large datasets, they tend to\noverfit on smaller ones. Probabilistic neural networks, such as those utilizing\nvariational inference, address this limitation by incorporating uncertainty\nestimation through weight distributions rather than point estimates. However,\nstandard variational inference often relies on a single-density approximation,\nwhich can lead to poor posterior estimates and hinder model performance. We\npropose Boosted Bayesian Neural Networks (BBNN), a novel approach that enhances\nneural network weight distribution approximations using Boosting Variational\nInference (BVI). By iteratively constructing a mixture of densities, BVI\nexpands the approximating family, enabling a more expressive posterior that\nleads to improved generalization and uncertainty estimation. While this\napproach increases computational complexity, it significantly enhances accuracy\nan essential tradeoff, particularly in high-stakes applications such as medical\ndiagnostics, where false negatives can have severe consequences. Our\nexperimental results demonstrate that BBNN achieves ~5% higher accuracy\ncompared to conventional neural networks while providing superior uncertainty\nquantification. This improvement highlights the effectiveness of leveraging a\nmixture-based variational family to better approximate the posterior\ndistribution, ultimately advancing probabilistic deep learning."
      ]
    }
  },
  {
    "id":2411.00749,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"A 2021 update on cancer image analytics with deep learning",
    "start_abstract":"Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification"
      ],
      "abstract":[
        "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MMCR: Advancing Visual Language Model in Multimodal Multi-Turn\n  Contextual Reasoning",
        "Generative Artificial Intelligence: Implications for Biomedical and\n  Health Professions Education",
        "Governing AI Agents",
        "Saarthi: The First AI Formal Verification Engineer",
        "Think Smarter not Harder: Adaptive Reasoning with Inference Aware\n  Optimization",
        "Observer-Aware Probabilistic Planning Under Partial Observability",
        "PairVDN - Pair-wise Decomposed Value Functions",
        "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task\n  Planning",
        "How Well Can AI Build SD Models?",
        "ASKCOS: an open source software suite for synthesis planning",
        "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based\n  Automatic Heuristic Design",
        "MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs\n  and Deep Learning",
        "Analyzing Advanced AI Systems Against Definitions of Life and\n  Consciousness",
        "SparAMX: Accelerating Compressed LLMs Token Generation on AMX-powered\n  CPUs",
        "VarGes: Improving Variation in Co-Speech 3D Gesture Generation via\n  StyleCLIPS",
        "Thermodynamics of driven systems with explicitly broken detailed balance",
        "How to Upscale Neural Networks with Scaling Law? A Survey and Practical\n  Guidelines",
        "Charge symmetry breaking in hypernuclei within RMF model",
        "The nonlinear limit of Babinet's Principle",
        "Structural and optical properties of in situ Eu-doped ZnCdO\/ZnMgO\n  superlattices grown by plasma-assisted molecular beam epitaxy",
        "Accenture-NVS1: A Novel View Synthesis Dataset",
        "CANUCS\/Technicolor: JWST Medium Band Photometry Finds Half of the Star\n  Formation at $z>7.5$ is Obscured",
        "Sparse wavefield reconstruction and denoising with boostlets",
        "Theoretical study of the Spectroscopic measurements of Kerr non-linear\n  resonators with four-body interaction",
        "Energy dynamics in a class of local random matrix Hamiltonians",
        "Attention Reallocation: Towards Zero-cost and Controllable Hallucination\n  Mitigation of MLLMs",
        "Pointwise estimates for the fundamental solutions of higher order\n  schr\\\"{o}dinger equations with finite rank perturbations",
        "3D ReX: Causal Explanations in 3D Neuroimaging Classification"
      ],
      "abstract":[
        "Compared to single-turn dialogue, multi-turn dialogue involving multiple\nimages better aligns with the needs of real-world human-AI interactions.\nAdditionally, as training data, it provides richer contextual reasoning\ninformation, thereby guiding the model to achieve better performance. However,\nexisting vision-language models (VLMs) primarily rely on single-turn dialogue\ntraining and evaluation benchmarks. In this paper, following the\ncharacteristics of human dialogue, such as focused topics and concise, clear\ncontent, we present MMCR (Multimodal Multi-turn Contextual Reasoning), a novel\ndataset comprising: (1) MMCR-310k -- the largest multi-image multi-turn\ninstruction tuning dataset with 310K contextual dialogues, each covering 1-4\nimages and 4 or 8 dialogue turns; and (2) MMCR-Bench -- a diagnostic benchmark\nfeaturing dialogues, spanning 8 domains (Humanities, Natural, Science,\nEducation, etc.) and 40 sub-topics. Extensive evaluations demonstrate that\nmodels fine-tuned with MMCR-310k achieve 5.2\\% higher contextual accuracy on\nMMCR-Bench, while showing consistent improvements on existing benchmarks\n(+1.1\\% on AI2D, +1.2\\% on MMMU and MMVet). MMCR and prompt engineering will be\nreleased publicly.",
        "Generative AI has had a profound impact on biomedicine and health, both in\nprofessional work and in education. Based on large language models (LLMs),\ngenerative AI has been found to perform as well as humans in simulated\nsituations taking medical board exams, answering clinical questions, solving\nclinical cases, applying clinical reasoning, and summarizing information.\nGenerative AI is also being used widely in education, performing well in\nacademic courses and their assessments. This review summarizes the successes of\nLLMs and highlights some of their challenges in the context of education, most\nnotably aspects that may undermines the acquisition of knowledge and skills for\nprofessional work. It then provides recommendations for best practices\novercoming shortcomings for LLM use in education. Although there are challenges\nfor use of generative AI in education, all students and faculty, in biomedicine\nand health and beyond, must have understanding and be competent in its use.",
        "The field of AI is undergoing a fundamental transition from generative models\nthat can produce synthetic content to artificial agents that can plan and\nexecute complex tasks with only limited human involvement. Companies that\npioneered the development of language models have now built AI agents that can\nindependently navigate the internet, perform a wide range of online tasks, and\nincreasingly serve as AI personal assistants and virtual coworkers. The\nopportunities presented by this new technology are tremendous, as are the\nassociated risks. Fortunately, there exist robust analytic frameworks for\nconfronting many of these challenges, namely, the economic theory of\nprincipal-agent problems and the common law doctrine of agency relationships.\nDrawing on these frameworks, this Article makes three contributions. First, it\nuses agency law and theory to identify and characterize problems arising from\nAI agents, including issues of information asymmetry, discretionary authority,\nand loyalty. Second, it illustrates the limitations of conventional solutions\nto agency problems: incentive design, monitoring, and enforcement might not be\neffective for governing AI agents that make uninterpretable decisions and\noperate at unprecedented speed and scale. Third, the Article explores the\nimplications of agency law and theory for designing and regulating AI agents,\narguing that new technical and legal infrastructure is needed to support\ngovernance principles of inclusivity, visibility, and liability.",
        "Recently, Devin has made a significant buzz in the Artificial Intelligence\n(AI) community as the world's first fully autonomous AI software engineer,\ncapable of independently developing software code. Devin uses the concept of\nagentic workflow in Generative AI (GenAI), which empowers AI agents to engage\nin a more dynamic, iterative, and self-reflective process. In this paper, we\npresent a similar fully autonomous AI formal verification engineer, Saarthi,\ncapable of verifying a given RTL design end-to-end using an agentic workflow.\nWith Saarthi, verification engineers can focus on more complex problems, and\nverification teams can strive for more ambitious goals. The domain-agnostic\nimplementation of Saarthi makes it scalable for use across various domains such\nas RTL design, UVM-based verification, and others.",
        "Solving mathematics problems has been an intriguing capability of large\nlanguage models, and many efforts have been made to improve reasoning by\nextending reasoning length, such as through self-correction and extensive long\nchain-of-thoughts. While promising in problem-solving, advanced long reasoning\nchain models exhibit an undesired single-modal behavior, where trivial\nquestions require unnecessarily tedious long chains of thought. In this work,\nwe propose a way to allow models to be aware of inference budgets by\nformulating it as utility maximization with respect to an inference budget\nconstraint, hence naming our algorithm Inference Budget-Constrained Policy\nOptimization (IBPO). In a nutshell, models fine-tuned through IBPO learn to\n``understand'' the difficulty of queries and allocate inference budgets to\nharder ones. With different inference budgets, our best models are able to have\na $4.14$\\% and $5.74$\\% absolute improvement ($8.08$\\% and $11.2$\\% relative\nimprovement) on MATH500 using $2.16$x and $4.32$x inference budgets\nrespectively, relative to LLaMA3.1 8B Instruct. These improvements are\napproximately $2$x those of self-consistency under the same budgets.",
        "In this article, we are interested in planning problems where the agent is\naware of the presence of an observer, and where this observer is in a partial\nobservability situation. The agent has to choose its strategy so as to optimize\nthe information transmitted by observations. Building on observer-aware Markov\ndecision processes (OAMDPs), we propose a framework to handle this type of\nproblems and thus formalize properties such as legibility, explicability and\npredictability. This extension of OAMDPs to partial observability can not only\nhandle more realistic problems, but also permits considering dynamic hidden\nvariables of interest. These dynamic target variables allow, for instance,\nworking with predictability, or with legibility problems where the goal might\nchange during execution. We discuss theoretical properties of PO-OAMDPs and,\nexperimenting with benchmark problems, we analyze HSVI's convergence behavior\nwith dedicated initializations and study the resulting strategies.",
        "Extending deep Q-learning to cooperative multi-agent settings is challenging\ndue to the exponential growth of the joint action space, the non-stationary\nenvironment, and the credit assignment problem. Value decomposition allows deep\nQ-learning to be applied at the joint agent level, at the cost of reduced\nexpressivity. Building on past work in this direction, our paper proposes\nPairVDN, a novel method for decomposing the value function into a collection of\npair-wise, rather than per-agent, functions, improving expressivity at the cost\nof requiring a more complex (but still efficient) dynamic programming\nmaximisation algorithm. Our method enables the representation of value\nfunctions which cannot be expressed as a monotonic combination of per-agent\nfunctions, unlike past approaches such as VDN and QMIX. We implement a novel\nmany-agent cooperative environment, Box Jump, and demonstrate improved\nperformance over these baselines in this setting. We open-source our code and\nenvironment at https:\/\/github.com\/zzbuzzard\/PairVDN.",
        "A key objective of embodied intelligence is enabling agents to perform\nlong-horizon tasks in dynamic environments while maintaining robust\ndecision-making and adaptability. To achieve this goal, we propose the\nSpatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task\nplanning and execution by integrating spatio-temporal memory. STMA is built\nupon three critical components: (1) a spatio-temporal memory module that\ncaptures historical and environmental changes in real time, (2) a dynamic\nknowledge graph that facilitates adaptive spatial reasoning, and (3) a\nplanner-critic mechanism that iteratively refines task strategies. We evaluate\nSTMA in the TextWorld environment on 32 tasks, involving multi-step planning\nand exploration under varying levels of complexity. Experimental results\ndemonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7%\nincrease in average score compared to the state-of-the-art model. The results\nhighlight the effectiveness of spatio-temporal memory in advancing the memory\ncapabilities of embodied agents.",
        "Introduction: As system dynamics (SD) embraces automation, AI offers\nefficiency but risks bias from missing data and flawed models. Models that omit\nmultiple perspectives and data threaten model quality, whether created by\nhumans or with the assistance of AI. To reduce uncertainty about how well AI\ncan build SD models, we introduce two metrics for evaluation of AI-generated\ncausal maps: technical correctness (causal translation) and adherence to\ninstructions (conformance).\n  Approach: We developed an open source project called sd-ai to provide a basis\nfor collaboration in the SD community, aiming to fully harness the potential of\nAI based tools like ChatGPT for dynamic modeling. Additionally, we created an\nevaluation theory along with a comprehensive suite of tests designed to\nevaluate any such tools developed within the sd-ai ecosystem.\n  Results: We tested 11 different LLMs on their ability to do causal\ntranslation as well as conform to user instruction. gpt-4.5-preview was the top\nperformer, scoring 92.9% overall, excelling in both tasks. o1 scored 100% in\ncausal translation. gpt-4o identified all causal links but struggled with\npositive polarity in decreasing terms. While gpt-4.5-preview and o1 are most\naccurate, gpt-4o is the cheapest.\n  Discussion: Causal translation and conformance tests applied to the sd-ai\nengine reveal significant variations across lLLMs, underscoring the need for\ncontinued evaluation to ensure responsible development of AI tools for dynamic\nmodeling. To address this, an open collaboration among tool developers,\nmodelers, and stakeholders is launched to standardize measures for evaluating\nthe capacity of AI tools to improve the modeling process.",
        "The advancement of machine learning and the availability of large-scale\nreaction datasets have accelerated the development of data-driven models for\ncomputer-aided synthesis planning (CASP) in the past decade. Here, we detail\nthe newest version of ASKCOS, an open source software suite for synthesis\nplanning that makes available several research advances in a freely available,\npractical tool. Four one-step retrosynthesis models form the basis of both\ninteractive planning and automatic planning modes. Retrosynthetic planning is\ncomplemented by other modules for feasibility assessment and pathway\nevaluation, including reaction condition recommendation, reaction outcome\nprediction, and auxiliary capabilities such as solubility prediction and\nquantum mechanical descriptor prediction. ASKCOS has assisted hundreds of\nmedicinal, synthetic, and process chemists in their day-to-day tasks,\ncomplementing expert decision making. It is our belief that CASP tools like\nASKCOS are an important part of modern chemistry research, and that they offer\never-increasing utility and accessibility.",
        "Handcrafting heuristics for solving complex optimization tasks (e.g., route\nplanning and task allocation) is a common practice but requires extensive\ndomain knowledge. Recently, Large Language Model (LLM)-based automatic\nheuristic design (AHD) methods have shown promise in generating high-quality\nheuristics without manual interventions. Existing LLM-based AHD methods employ\na population to maintain a fixed number of top-performing LLM-generated\nheuristics and introduce evolutionary computation (EC) to iteratively enhance\nthe population. However, these population-based procedures cannot fully develop\nthe potential of each heuristic and are prone to converge into local optima. To\nmore comprehensively explore the space of heuristics, this paper proposes to\nuse Monte Carlo Tree Search (MCTS) for LLM-based heuristic evolution. The\nproposed MCTS-AHD method organizes all LLM-generated heuristics in a tree\nstructure and can better develop the potential of temporarily underperforming\nheuristics. In experiments, MCTS-AHD delivers significantly higher-quality\nheuristics on various complex tasks. Our code is available.",
        "In the competitive landscape of advertising, success hinges on effectively\nnavigating and leveraging complex interactions among consumers, advertisers,\nand advertisement platforms. These multifaceted interactions compel advertisers\nto optimize strategies for modeling consumer behavior, enhancing brand recall,\nand tailoring advertisement content. To address these challenges, we present\nMindMem, a multimodal predictive model for advertisement memorability. By\nintegrating textual, visual, and auditory data, MindMem achieves\nstate-of-the-art performance, with a Spearman's correlation coefficient of\n0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently\nsurpassing existing methods. Furthermore, our analysis identified key factors\ninfluencing advertisement memorability, such as video pacing, scene complexity,\nand emotional resonance. Expanding on this, we introduced MindMem-ReAd\n(MindMem-Driven Re-generated Advertisement), which employs Large Language\nModel-based simulations to optimize advertisement content and placement,\nresulting in up to a 74.12% improvement in advertisement memorability. Our\nresults highlight the transformative potential of Artificial Intelligence in\nadvertising, offering advertisers a robust tool to drive engagement, enhance\ncompetitiveness, and maximize impact in a rapidly evolving market.",
        "Could artificial intelligence ever become truly conscious in a functional\nsense; this paper explores that open-ended question through the lens of Life, a\nconcept unifying classical biological criteria (Oxford, NASA, Koshland) with\nempirical hallmarks such as adaptive self maintenance, emergent complexity, and\nrudimentary self referential modeling. We propose a number of metrics for\nexamining whether an advanced AI system has gained consciousness, while\nemphasizing that we do not claim all AI stems can become conscious. Rather, we\nsuggest that sufficiently advanced architectures exhibiting immune like\nsabotage defenses, mirror self-recognition analogs, or meta-cognitive updates\nmay cross key thresholds akin to life-like or consciousness-like traits. To\ndemonstrate these ideas, we start by assessing adaptive self-maintenance\ncapability, and introduce controlled data corruption sabotage into the training\nprocess. The result demonstrates AI capability to detect these inconsistencies\nand revert or self-correct analogous to regenerative biological processes. We\nalso adapt an animal-inspired mirror self recognition test to neural\nembeddings, finding that partially trained CNNs can distinguish self from\nforeign features with complete accuracy. We then extend our analysis by\nperforming a question-based mirror test on five state-of-the-art chatbots\n(ChatGPT4, Gemini, Perplexity, Claude, and Copilot) and demonstrated their\nability to recognize their own answers compared to those of the other chatbots.",
        "Large language models have high compute, latency, and memory requirements.\nWhile specialized accelerators such as GPUs and TPUs typically run these\nworkloads, CPUs are more widely available and consume less energy. Accelerating\nLLMs with CPUs enables broader AI access at a lower cost and power consumption.\nThis acceleration potential for CPUs is especially relevant during the\nmemory-bound decoding stage of LLM inference, which processes one token at a\ntime and is becoming increasingly utilized with reasoning models. We utilize\nAdvanced Matrix Extensions (AMX) support on the latest Intel CPUs together with\nunstructured sparsity to achieve a $1.42 \\times$ reduction in end-to-end\nlatency compared to the current PyTorch implementation by applying our\ntechnique in linear layers. We provide a set of open-source customized sparse\nkernels that can speed up any PyTorch model by automatically replacing all\nlinear layers with our custom sparse implementation. Furthermore, we\ndemonstrate for the first time the use of unstructured sparsity in the\nattention computation achieving a $1.14 \\times$ speedup over the current\nsystems without compromising accuracy. Code:\nhttps:\/\/github.com\/IntelLabs\/Hardware-Aware-Automated-Machine-Learning\/tree\/main\/SparAMX",
        "Generating expressive and diverse human gestures from audio is crucial in\nfields like human-computer interaction, virtual reality, and animation. Though\nexisting methods have achieved remarkable performance, they often exhibit\nlimitations due to constrained dataset diversity and the restricted amount of\ninformation derived from audio inputs. To address these challenges, we present\nVarGes, a novel variation-driven framework designed to enhance co-speech\ngesture generation by integrating visual stylistic cues while maintaining\nnaturalness. Our approach begins with the Variation-Enhanced Feature Extraction\n(VEFE) module, which seamlessly incorporates \\textcolor{blue}{style-reference}\nvideo data into a 3D human pose estimation network to extract StyleCLIPS,\nthereby enriching the input with stylistic information. Subsequently, we employ\nthe Variation-Compensation Style Encoder (VCSE), a transformer-style encoder\nequipped with an additive attention mechanism pooling layer, to robustly encode\ndiverse StyleCLIPS representations and effectively manage stylistic variations.\nFinally, the Variation-Driven Gesture Predictor (VDGP) module fuses MFCC audio\nfeatures with StyleCLIPS encodings via cross-attention, injecting this fused\ndata into a cross-conditional autoregressive model to modulate 3D human gesture\ngeneration based on audio input and stylistic clues. The efficacy of our\napproach is validated on benchmark datasets, where it outperforms existing\nmethods in terms of gesture diversity and naturalness. The code and video\nresults will be made publicly available upon\nacceptance:https:\/\/github.com\/mookerr\/VarGES\/ .",
        "In systems with detailed balance, the stationary distribution and the\nequilibrium distribution are identical, creating a clear connection between\nenergetic and entropic quantities. Many driven systems violate detailed balance\nand still pose a challenge for a consistent thermodynamic interpretation. Even\nsteady-state potentials like entropy or free energy are no longer state\nvariables. Here, we use a framework for systems with broken detailed balance,\nwhere Boltzmann entropy can be computed while properly taking constraints on\nstate transitions into account. As an illustration, we establish the\nthermodynamic relations for arbitrarily driven sample space-reducing processes\nthat are non-equilibrium but show steady states. We demonstrate that, despite\nexplicitly broken detailed balance, it remains feasible to define and\nunambiguously interpret the effective thermodynamic potentials.",
        "Neural scaling laws have revolutionized the design and optimization of\nlarge-scale AI models by revealing predictable relationships between model\nsize, dataset volume, and computational resources. Early research established\npower-law relationships in model performance, leading to compute-optimal\nscaling strategies. However, recent studies highlighted their limitations\nacross architectures, modalities, and deployment contexts. Sparse models,\nmixture-of-experts, retrieval-augmented learning, and multimodal models often\ndeviate from traditional scaling patterns. Moreover, scaling behaviors vary\nacross domains such as vision, reinforcement learning, and fine-tuning,\nunderscoring the need for more nuanced approaches. In this survey, we\nsynthesize insights from over 50 studies, examining the theoretical\nfoundations, empirical findings, and practical implications of scaling laws. We\nalso explore key challenges, including data efficiency, inference scaling, and\narchitecture-specific constraints, advocating for adaptive scaling strategies\ntailored to real-world applications. We suggest that while scaling laws provide\na useful guide, they do not always generalize across all architectures and\ntraining strategies.",
        "We study the charge symmetry breaking (CSB) effect in the binding energy of\nmirror hypernuclei in the mass region $A=7\\sim 48$ in relativistic mean field\n(RMF) models introducing $NN$ and $\\Lambda N$ interactions. The\nphenomenological $\\Lambda N$ CSB interaction is introduced and the strength\nparameter is fitted to reproduce the experimental binding energy difference\nbetween the mirror hypernuclei $^{12}_\\Lambda$B and $^{12}_\\Lambda$C. This\nmodel is applied to calculate the CSB energy anomaly in mirror hypernuclei with\nthe mass $A=7\\sim48$. The model is further applied to predict the binding\nenergy difference of mirror hypernuclei of $A$=40 with the isospin $T=1\/2$,\n$3\/2$ and $5\/2$ nuclei together with various hyper Ca isotopes and their mirror\nhypernuclei. Finally the binding energy systematics of $A=$48 hypernuclei are\npredicted with\/without the CSB effect by the PK1 and TM2 energy density\nfunctionals (EDFs).",
        "Babinet's principle is a powerful tool for predicting the scattering behavior\nof planar structures where the solution for the complementary structure is\nalready known. This makes it ubiquitous in the design of aperture antennas or\nmetamaterials. Even for plasmonic nanostructures, a qualitative match of the\nbehavior for complementary structures has been reported. Here, we discuss\nwhether Babinet's principle can be extended to nonlinear scattering. We compare\nthe third harmonic emission of plasmonic nanorods and complementary nanoslits\nby far field imaging and simulation. We find significantly different far field\nimages, in agreement between experiment and simulation. We explain these\ndifferences by the higher spatial resolution at the third harmonic wavelength\nand by additional eddy currents in slits that are not present in rods. Within\nthese limits, Babinet's principle can guide the design of inverted nonlinear\nplasmonic resonators, which promise to be more stable at high excitation power\ndue to better thermal conductivity.",
        "In situ Eu-doped ZnCdO-ZnMgO superlattices with varying ZnCdO:Eu and ZnMgO\nsublayers thicknesses were deposited by plasma assisted molecular beam epitaxy.",
        "This paper introduces ACC-NVS1, a specialized dataset designed for research\non Novel View Synthesis specifically for airborne and ground imagery. Data for\nACC-NVS1 was collected in Austin, TX and Pittsburgh, PA in 2023 and 2024. The\ncollection encompasses six diverse real-world scenes captured from both\nairborne and ground cameras, resulting in a total of 148,000 images. ACC-NVS1\naddresses challenges such as varying altitudes and transient objects. This\ndataset is intended to supplement existing datasets, providing additional\nresources for comprehensive research, rather than serving as a benchmark.",
        "We present a sample of 146 high-redshift ($z>7.5$) galaxies from the\nCANUCS\/Technicolor surveys, showcasing photometry in every wide- and\nmedium-band NIRCam filter in addition to ancillary HST data sampling $0.4-5 \\mu\nm$ (22 JWST bands out of 29 bands total). Additionally, 48 ($33\\%$) galaxies in\nour sample meet criteria to be classified as extreme emission line galaxies, 15\n($10\\%$) of which are completely missed by typical dropout selections due to\nfaint UV emission. By fitting the SEDs covering the rest-frame UV to optical at\n$z > 7.5$, we investigate the dust obscuration properties, giving an unbiased\nview of dust buildup in high-redshift galaxies free from spectroscopic\nfollow-up selection effects. Dust attenuation correlates with stellar mass, but\nmore strongly with star formation rate. We find typical galaxies at $z>7.5$\nhave $\\sim 25 \\%$ of their star formation obscured. However, since galaxies\nwith higher star formation rates suffer more attenuation, $\\sim 50 \\%$ of the\ntotal star formation rate density at $7.5<z<9$ is obscured. The obscured\nfraction drops to $\\sim 35 \\%$ in our $9<z<12$ bin, possibly due to substantial\ndust grain growth in the interstellar medium not having time to occur.\nExtrapolating the decline in dust obscuration of galaxies to higher redshifts,\nwe infer that dust obscuration should approach zero at $z > 15$, implying that\nepoch as when dust first forms in bright galaxies.",
        "Boostlets are spatiotemporal functions that decompose nondispersive\nwavefields into a collection of localized waveforms parametrized by dilations,\nhyperbolic rotations, and translations. We study the sparsity properties of\nboostlets and find that the resulting decompositions are significantly sparser\nthan those of other state-of-the-art representation systems, such as wavelets\nand shearlets. This translates into improved denoising performance when\nhard-thresholding the boostlet coefficients. The results suggest that boostlets\noffer a natural framework for sparsely decomposing wavefields in unified\nspace-time.",
        "Quantum annealing provides a promising way to solve combinational\noptimization problems where the solutions correspond to the ground state of the\nIsing Hamiltonian. We can implement quantum annealing using the Kerr non-linear\nresonators, with bifurcation phenomena emerging when subjected to a parametric\ndrive. These bifurcated states can function as bases of qubits. Moreover,\nintegrating four-body interactions between physical qubits enables the\nestablishment of effective all-to-all long-range interactions between logical\nqubits, which is essential for practical quantum annealing. While theoretical\nproposals exist for creating four-body interactions within Kerr non-linear\nresonators, there has not been experimental verification through their\nspectroscopic signatures. In this paper, we theoretically investigate the\nspectroscopic measurements of Kerr non-linear resonators featuring four-body\ninteraction. We identify six distinct frequencies exhibiting population changes\nby employing resonant driving on one resonator and weak driving on another.\nAnalytical and numerical calculations validate these findings. Our study\ndemonstrates the potential of spectroscopy in characterizing systems with\nfour-body interactions, offering insights for realizing quantum annealing with\nKerr parametric oscillators.",
        "Random matrix theory yields valuable insights into the universal features of\nquantum many-body chaotic systems. Although all-to-all interactions are\ntraditionally studied, many interesting dynamical questions, such as transport\nof a conserved density, require a notion of spatially local interactions. We\nstudy the transport of the energy, the most basic conserved density, in\nfew-body and 1D chains of nearest-neighbor random matrix terms that square to\none. In the few-body but large local Hilbert space dimension case, we develop a\nmapping for the energy dynamics to a single-particle hopping picture. This\nallows for the computation of the energy density autocorrelators and an\nout-of-time-ordered correlator of the energy density. In the 1D chain, we\nnumerically study the energy transport for a small local Hilbert space\ndimension. We also discuss the density of states throughout and touch upon the\nrelation to free probability theory.",
        "Multi-Modal Large Language Models (MLLMs) stand out in various tasks but\nstill struggle with hallucinations. While recent training-free mitigation\nmethods mostly introduce additional inference overhead via retrospection\nstrategy and contrastive decoding, we propose attention reallocation (AttnReal)\nto mitigate hallucinations with nearly zero extra cost. Our approach is\nmotivated by the key observations that, MLLM's unreasonable attention\ndistribution causes features to be dominated by historical output tokens, which\nfurther contributes to hallucinated responses because of the distribution gap\nbetween different token types. Based on the observations, AttnReal recycles\nexcessive attention from output tokens and reallocates it to visual tokens,\nwhich reduces MLLM's reliance on language priors and ensures the decoding\nprocess depends more on the visual inputs. More interestingly, we find that, by\ncontrolling the intensity of AttnReal, we can achieve a wide-range trade-off\nbetween the response faithfulness and overall performance. Comprehensive\nresults from different benchmarks validate the effectiveness of AttnReal across\nsix open-source MLLMs and three decoding strategies.",
        "This paper is dedicated to studying pointwise estimates of the fundamental\nsolution for the higher order Schr\\\"{o}dinger equation: % we investigate the\nfundamental solution of the higher order Schr\\\"{o}dinger equation\n$$i{\\partial}_{t}u(x,t)=Hu(x,t),\\ \\ \\ t\\in \\mathbb{R},\\ x\\in\n{\\mathbb{R}}^{n},$$ where the Hamiltonian $H$ is defined as\n$$H={(-\\Delta)}^{m}+\\displaystyle\\sum_{j=1}^{N} \\langle\\cdotp ,{\\varphi }_{j}\n\\rangle{\\varphi }_{j},$$ with each $\\varphi_j$ ($1\\le j\\le N$) satisfying\ncertain smoothness and decay conditions. %Let ${P}_{ac}(H)$ denote the\nprojection onto the absolutely continuous space of $H$. We show that for any\npositive integer $m>1$ and spatial dimension $n\\ge 1$, %under a spectral\nassumption, the operator is sharp in the sense that it\n  ${e}^{-i tH}P_{ac}(H)$ has an integral kernel $K(t,x,y)$ satisfying the\nfollowing pointwise estimate: $$\\left |K(t,x,y)\\right |\\lesssim\n|t|^{-\\frac{n}{2m}}(1+|t|^{-\\frac{1}{2m}}\\left | x-y\\right\n|)^{-\\frac{n(m-1)}{2m-1}} ,\\ \\ t\\ne 0,\\ x,y\\in {\\mathbb{R}}^{n}.$$ This\nestimate is consistent with the upper bounds in the free case. As an\napplication, we derive $L^p-L^q$ decay estimates for the propagator ${e}^{-\\i\ntH}P_{ac}(H)$, where the pairs $(1\/p, 1\/q)$ lie within a quadrilateral region\nin the plane.",
        "Explainability remains a significant problem for AI models in medical\nimaging, making it challenging for clinicians to trust AI-driven predictions.\nWe introduce 3D ReX, the first causality-based post-hoc explainability tool for\n3D models. 3D ReX uses the theory of actual causality to generate\nresponsibility maps which highlight the regions most crucial to the model's\ndecision. We test 3D ReX on a stroke detection model, providing insight into\nthe spatial distribution of features relevant to stroke."
      ]
    }
  },
  {
    "id":2411.00749,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification",
    "start_abstract":"Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "A 2021 update on cancer image analytics with deep learning"
      ],
      "abstract":[
        "Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Conditional Success of Adaptive Therapy: The Role of Treatment-Holiday\n  Thresholds Revealed by Mathematical Modeling",
        "Language-specific Tonal Features Drive Speaker-Listener Neural\n  Synchronization",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns",
        "A Bayesian Modelling Framework with Model Comparison for Epidemics with\n  Super-Spreading",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "A network-driven framework for enhancing gene-disease association\n  studies in coronary artery disease",
        "Multicellular self-organization in Escherichia coli",
        "Artificial Intelligence Approaches for Anti-Addiction Drug Discovery",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Heuristics based on Adjacency Graph Packing for DCJ Distance Considering\n  Intergenic Regions",
        "Derivation from kinetic theory and 2-D pattern analysis of chemotaxis\n  models for Multiple Sclerosis",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Weight Distribution of the Weighted Coordinates Poset Block Space and\n  Singleton Bound",
        "Feedback-augmented Non-homogeneous Hidden Markov Models for Longitudinal\n  Causal Inference",
        "Nonlocal characteristics of two-qubit gates and their argand diagrams",
        "Stability of the Euclidean 3-ball under L2-curvature pinching",
        "Observational and Theoretical Constraints on First-Order Phase\n  Transitions in Neutron Stars",
        "Breather interactions in the integrable discrete Manakov system and\n  trigonometric Yang-Baxter maps",
        "Structure evolution with cosmic backgrounds from radio to far infrared",
        "A Family of Semi-norms in $C^*$-algebras",
        "An Iterative Block Matrix Inversion (IBMI) Algorithm for Symmetric\n  Positive Definite Matrices with Applications to Covariance Matrices",
        "Mean-Field Analysis of Latent Variable Process Models on Dynamically\n  Evolving Graphs with Feedback Effects",
        "Dirichlet problem for diffusions with jumps",
        "Remote preparation of motional Schr\\\"{o}dinger cat states via\n  dissipatively-driven non-Gaussian mechanical entanglement",
        "Frequency-resolved time lags due to X-ray disk reprocessing in AGN",
        "The rise of the galactic empire: luminosity functions at $z\\sim17$ and\n  $z\\sim25$ estimated with the MIDIS$+$NGDEEP ultra-deep JWST\/NIRCam dataset",
        "Subdomains of Post-COVID-Syndrome (PCS) -- A Population-Based Study"
      ],
      "abstract":[
        "Adaptive therapy (AT) improves cancer treatment by controlling the\ncompetition between sensitive and resistant cells through treatment holidays.\nThis study highlights the critical role of treatment-holiday thresholds in AT\nfor tumors composed of drug-sensitive and resistant cells. Using a\nLotka-Volterra model, the research compares AT with maximum tolerated dose\ntherapy and intermittent therapy, showing that AT's success largely depends on\nthe threshold at which treatment is paused and resumed, as well as on the\ncompetition between sensitive and resistant cells. Three scenarios of\ncomparison between AT and other therapies are identified: uniform-decline,\nconditional-improve, and uniform-improve, illustrating that optimizing the\ntreatment-holiday threshold is crucial for AT effectiveness. Tumor composition,\nincluding initial tumor burden and the proportion of resistant cells,\ninfluences outcomes. Adjusting threshold values enables AT to suppress\nresistant subclones, preserving sensitive cells, ultimately improving\nprogression-free survival. These findings emphasize the importance of\npersonalized treatment strategies potentially enhancing long-term therapeutic\noutcomes.",
        "Verbal communication transmits information across diverse linguistic levels,\nwith neural synchronization (NS) between speakers and listeners emerging as a\nputative mechanism underlying successful exchange. However, the specific speech\nfeatures driving this synchronization and how language-specific versus\nuniversal characteristics facilitate information transfer remain poorly\nunderstood. We developed a novel content-based interbrain encoding model to\ndisentangle the contributions of acoustic and linguistic features to\nspeaker-listener NS during Mandarin storytelling and listening, as measured via\nmagnetoencephalography (MEG). Results revealed robust NS throughout\nfrontotemporal-parietal networks with systematic time lags between speech\nproduction and perception. Crucially, suprasegmental lexical tone features\n(tone categories, pitch height, and pitch contour), essential for lexical\nmeaning in Mandarin, contributed more significantly to NS than either acoustic\nelements or universal segmental units (consonants and vowels). These tonal\nfeatures generated distinctive spatiotemporal NS patterns, creating\nlanguage-specific neural \"communication channels\" that facilitated efficient\nrepresentation sharing between interlocutors. Furthermore, the strength and\npatterns of NS driven by these language-specific features predicted\ncommunication success. These findings demonstrate the neural mechanisms\nunderlying shared representations during verbal exchange and highlight how\nlanguage-specific features can shape neural coupling to optimize information\ntransfer during human communication.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis.",
        "The transmission dynamics of an epidemic are rarely homogeneous.\nSuper-spreading events and super-spreading individuals are two types of\nheterogeneous transmissibility. Inference of super-spreading is commonly\ncarried out on secondary case data, the expected distribution of which is known\nas the offspring distribution. However, this data is seldom available. Here we\nintroduce a multi-model framework fit to incidence time-series, data that is\nmuch more readily available. The framework consists of five discrete-time,\nstochastic, branching-process models of epidemics spread through a susceptible\npopulation. The framework includes a baseline model of homogeneous\ntransmission, a unimodal and a bimodal model for super-spreading events, as\nwell as a unimodal and a bimodal model for super-spreading individuals.\nBayesian statistics is used to infer model parameters using Markov Chain\nMonte-Carlo. Model comparison is conducted by computing Bayes factors, with\nimportance sampling used to estimate the marginal likelihood of each model.\nThis estimator is selected for its consistency and lower variance compared to\nalternatives. Application to simulated data from each model identifies the\ncorrect model for the majority of simulations and accurately infers the true\nparameters, such as the basic reproduction number. We also apply our methods to\nincidence data from the 2003 SARS outbreak and the Covid-19 pandemic. Model\nselection consistently identifies the same model and mechanism for a given\ndisease, even when using different time series. Our estimates are consistent\nwith previous studies based on secondary case data. Quantifying the\ncontribution of super-spreading to disease transmission has important\nimplications for infectious disease management and control. Our modelling\nframework is disease-agnostic and implemented as an R package, with potential\nto be a valuable tool for public health.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Over the last decade, genome-wide association studies (GWAS) have\nsuccessfully identified numerous genetic variants associated with complex\ndiseases. These associations have the potential to reveal the molecular\nmechanisms underlying complex diseases and lead to the identification of novel\ndrug targets. Despite these advancements, the biological pathways and\nmechanisms linking genetic variants to complex diseases are still not fully\nunderstood. Most trait-associated variants reside in non-coding regions and are\npresumed to influence phenotypes through regulatory effects on gene expression.\nYet, it is often unclear which genes they regulate and in which cell types this\nregulation occurs. Transcriptome-wide association studies (TWAS) aim to bridge\nthis gap by detecting trait-associated tissue gene expression regulated by GWAS\nvariants. However, traditional TWAS approaches frequently overlook the critical\ncontributions of trans-regulatory effects and fail to integrate comprehensive\nregulatory networks. Here, we present a novel framework that leverages\ntissue-specific gene regulatory networks (GRNs) to integrate cis- and\ntrans-genetic regulatory effects into the TWAS framework for complex diseases.\nWe validate our approach using coronary artery disease (CAD), utilizing data\nfrom the STARNET project, which provides multi-tissue gene expression and\ngenetic data from around 600 living patients with cardiovascular disease.\nPreliminary results demonstrate the potential of our GRN-driven framework to\nuncover more genes and pathways that may underlie CAD. This framework extends\ntraditional TWAS methodologies by utilizing tissue-specific regulatory insights\nand advancing the understanding of complex disease genetic architecture.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Drug addiction is a complex and pervasive global challenge that continues to\npose significant public health concerns. Traditional approaches to\nanti-addiction drug discovery have struggled to deliver effective therapeutics,\nfacing high attrition rates, long development timelines, and inefficiencies in\nprocessing large-scale data. Artificial intelligence (AI) has emerged as a\ntransformative solution to address these issues. Using advanced algorithms, AI\nis revolutionizing drug discovery by enhancing the speed and precision of key\nprocesses. This review explores the transformative role of AI in the pipeline\nfor anti-addiction drug discovery, including data collection, target\nidentification, and compound optimization. By highlighting the potential of AI\nto overcome traditional barriers, this review systematically examines how AI\naddresses critical gaps in anti-addiction research, emphasizing its potential\nto revolutionize drug discovery and development, overcome challenges, and\nadvance more effective therapeutic strategies.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "In this work, we explore heuristics for the Adjacency Graph Packing problem,\nwhich can be applied to the Double Cut and Join (DCJ) Distance Problem. The DCJ\nis a rearrangement operation and the distance problem considering it is a well\nestablished method for genome comparison. Our heuristics will use the structure\ncalled adjacency graph adapted to include information about intergenic regions,\nmultiple copies of genes in the genomes, and multiple circular or linear\nchromosomes. The only required property from the genomes is that it must be\npossible to turn one into the other with DCJ operations. We propose one greedy\nheuristic and one heuristic based on Genetic Algorithms. Our experimental tests\nin artificial genomes show that the use of heuristics is capable of finding\ngood results that are superior to a simpler random strategy.",
        "In this paper, a class of reaction-diffusion equations for Multiple Sclerosis\nis presented. These models are derived by means of a diffusive limit starting\nfrom a proper kinetic description, taking account of the underlying microscopic\ninteractions among cells. At the macroscopic level, we discuss the necessary\nconditions for Turing instability phenomena and the formation of\ntwo-dimensional patterns, whose shape and stability are investigated by means\nof a weakly nonlinear analysis. Some numerical simulations, confirming and\nextending theoretical results, are proposed for a specific scenario.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "In this paper, we determine the complete weight distribution of the space $\n\\mathbb{F}_q^N $ endowed by the weighted coordinates poset block metric\n($(P,w,\\pi)$-metric), also known as the $(P,w,\\pi)$-space, thereby obtaining it\nfor $(P,w)$-space, $(P,\\pi)$-space, $\\pi$-space, and $P$-space as special\ncases. Further, when $P$ is a chain, the resulting space is called as\nNiederreiter-Rosenbloom-Tsfasman (NRT) weighted block space and when $P$ is\nhierarchical, the resulting space is called as weighted coordinates\nhierarchical poset block space. The complete weight distribution of both the\nspaces are deduced from the main result. Moreover, we define an $I$-ball for an\nideal $I$ in $P$ and study the characteristics of it in $(P,w,\\pi)$-space.\n  We investigate the relationship between the $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Given an ideal $I$, we investigate how the maximum\ndistance separability (MDS) is related with $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Duality theorem is derived for an MDS\n$(P,w,\\pi)$-code when all the blocks are of same length. Finally, the\ndistribution of codewords among $r$-balls is analyzed in the case of chain\nposet, when all the blocks are of same length.",
        "Hidden Markov models are widely used for modeling sequential data but\ntypically have limited applicability in observational causal inference due to\ntheir strong conditional independence assumptions. I introduce\nfeedback-augmented non-homogeneous hidden Markov model (FAN-HMM), which\nincorporate time-varying covariates and feedback mechanisms from past\nobservations to latent states and future responses. Integrating these models\nwith the structural causal model framework allows flexible causal inference in\nlongitudinal data with time-varying unobserved heterogeneity and multiple\ncausal pathways. I show how, in a common case of categorical response\nvariables, long-term causal effects can be estimated efficiently without the\nneed for simulating counterfactual trajectories. Using simulation experiments,\nI study the performance of FAN-HMM under the common misspecification of the\nnumber of latent states, and finally apply the proposed approach to estimate\nthe effect of the 2013 parental leave reform on fathers' paternal leave uptake\nin Finnish workplaces.",
        "In this paper, we show the usefulness of the chords present in the argand\ndiagram of squared eigenvalues of nonlocal part of two-qubit gates to study\ntheir nonlocal characteristics. We discuss the criteria for perfect entanglers\nto transform a pair of orthonormal product states into a pair of orthonormal\nmaximally entangled states. Perfect entanglers with a chord passing through\norigin can do such a transformation. In the Weyl chamber, we identify the\nregions of perfect entanglers with at least one chord passing through origin.\nWe also provide the conditions for a perfect entangler without any chord\npassing through origin to transform a pair of orthonormal product states into\northonormal maximally entangled states. Finally, we show that similar to\nentangling power, gate typicality can also be described using the chords\npresent in the argand diagram. For each chord describing the entangling power,\nthere exists a chord describing the gate typicality. We show the geometrical\nrelation between the two sets of chords.",
        "In this article, we consider compact Riemannian 3-manifolds with boundary. We\nprove that if the $L^2$-norm of the curvature is small and if the\n$H^{1\/2}$-norm of the difference of the fundamental forms of the boundary is\nsmall, then the manifold is diffeomorphic to the Euclidean ball. Moreover, we\nobtain that the manifold and the ball are metrically close (uniformly and in\n$H^2$-norm), with a quantitative, optimal bound. The required smallness\nassumption only depends on the volumes of the manifold and its boundary and on\na trace and Sobolev constant of the manifold. The proof only relies on\nelementary computations based on the Bochner formula for harmonic functions and\ntensors, and on the 2-spheres effective uniformisation result of\nKlainerman-Szeftel.",
        "Understanding the equation of state (EOS) of neutron stars (NSs) is a\nfundamental challenge in astrophysics and nuclear physics. A first-order phase\ntransition (FOPT) at high densities could lead to the formation of a quark\ncore, significantly affecting NS properties. This review explores observational\nand theoretical constraints on such transitions using multi-messenger\nastrophysics. X-ray observations, including mass-radius measurements from NICER\nand spectral features like quasi-periodic oscillations (QPOs) and cyclotron\nresonance scattering features (CRSFs), provide indirect evidence of EOS\nmodifications. Gravitational wave detections, particularly from binary NS\nmergers such as GW170817, constrain tidal deformability and post-merger\noscillations, which may carry signatures of phase transitions. Pulsar timing\noffers additional constraints through measurements of mass, spin evolution, and\nglitches, with millisecond pulsars exceeding twice the solar mass posing\nchallenges to purely hadronic EOSs. Theoretical models and numerical\nsimulations predict that an FOPT could impact gravitational wave signals,\ntwin-star configurations, and NS cooling. Future advancements, including\nnext-generation gravitational wave detectors, high-precision X-ray telescopes,\nand improved theoretical modeling, will enhance our ability to probe phase\ntransitions in NSs. A combination of these approaches will provide crucial\ninsights into the existence and properties of deconfined quark matter in NS\ninteriors.",
        "The goal of this work is to obtain a complete characterization of soliton and\nbreather interactions in the integrable discrete Manakov (IDM) system, a vector\ngeneralization of the Ablowitz-Ladik model. The IDM system, which in the\ncontinuous limit reduces to the Manakov system (i.e., a 2-component vector\nnonlinear Schrodinger equation), was shown to admit a variety of discrete\nvector soliton solutions: fundamental solitons, fundamental breathers, and\ncomposite breathers. While the interaction of fundamental solitons was studied\nearly on, no results are presently available for other types of\nsoliton-breather and breather-breather interactions. Our study reveals that\nupon interacting with a fundamental breather, a fundamental soliton becomes a\nfundamental breather. Conversely, the interaction of two fundamental breathers\ngenerically yields two fundamental breathers with polarization shifts, but may\nalso result in a fundamental soliton and a fundamental breather. Composite\nbreathers interact trivially both with each other and with a fundamental\nsoliton or breather. Explicit formulas for the scattering coefficients that\ncharacterize fundamental and composite breathers are given. This allows us to\ninterpret the interactions in terms of a refactorization problem and derive the\nassociated Yang-Baxter maps describing the effect of interactions on the\npolarizations. These give the first examples of parametric Yang-Baxter maps of\ntrigonometric type.",
        "Cosmic background radiation, both diffuse and discrete in nature, produced at\ndifferent cosmic epochs before and after recombination, provides key\ninformation on the evolution of cosmic structures. We discuss the main classes\nof sources that contribute to the extragalactic background light from radio to\nsub-millimetre wavelenghs and the currently open question on the level of the\ncosmic radio background spectrum. The redshifted 21cm line signal from\ncosmological neutral Hydrogen during the primeval phases of cosmic structures\nas a probe of the cosmological reionisation process is presented, along with\nthe route for confident detection of this signal. We then describe the basic\nformalism and the feasibility to study via a differential approach, based\nmainly on dipole analysis, the tiny imprints in the CB spectrum expected from a\nvariety of cosmological and astrophysical processes at work during the early\nphases of cosmic perturbation and structure evolution. Finally, we discuss the\nidentification of high-redshift sub-millimetre lensed galaxies with extreme\nmagnifications in the Planck maps and their use for the comprehension of\nfundamental processes in early galaxy formation and evolution.",
        "We introduce a new family of non-negative real-valued functions on a\n$C^*$-algebra $\\mathcal{A}$, i.e., for $0\\leq \\mu \\leq 1,$\n$$\\|a\\|_{\\sigma_{\\mu}}= \\text{sup}\\left\\lbrace \\sqrt{|f(a)|^2 \\sigma_{\\mu}\nf(a^*a)}: f\\in \\mathcal{A}', \\, f(1)=\\|f\\|=1 \\right\\rbrace, \\quad $$ where\n$a\\in \\mathcal{A}$ and $\\sigma_{\\mu}$ is an interpolation path of the symmetric\nmean $\\sigma$. These functions are semi-norms as they satisfy the norm axioms,\nexcept for the triangle inequality. Special cases satisfying triangle\ninequality, and a complete equality characterization is also discussed. Various\nbounds and relationships will be established for this new family, with a\nconnection to the existing literature in the algebra of all bounded linear\noperators on a Hilbert space.",
        "Obtaining the inverse of a large symmetric positive definite matrix\n$\\mathcal{A}\\in\\mathbb{R}^{p\\times p}$ is a continual challenge across many\nmathematical disciplines. The computational complexity associated with direct\nmethods can be prohibitively expensive, making it infeasible to compute the\ninverse. In this paper, we present a novel iterative algorithm (IBMI), which is\ndesigned to approximate the inverse of a large, dense, symmetric positive\ndefinite matrix. The matrix is first partitioned into blocks, and an iterative\nprocess using block matrix inversion is repeated until the matrix approximation\nreaches a satisfactory level of accuracy. We demonstrate that the two-block,\nnon-overlapping approach converges for any positive definite matrix, while\nnumerical results provide strong evidence that the multi-block, overlapping\napproach also converges for such matrices.",
        "In this paper, we study the asymptotic behavior of a class of dynamic\nco-evolving latent space networks. The model we study is subject to\nbi-directional feedback effects, meaning that at any given time, the latent\nprocess depends on its own value and the graph structure at the previous time\nstep, and the graph structure at the current time depends on the value of the\nlatent processes at the current time but also on the graph structure at the\nprevious time instance (sometimes called a persistence effect). We construct\nthe mean-field limit of this model, which we use to characterize the limiting\nbehavior of a random sample taken from the latent space network in the limit as\nthe number of nodes in the network diverges. From this limiting model, we can\nderive the limiting behavior of the empirical measure of the latent process and\nestablish the related graphon limit of the latent particle network process. We\nalso provide a description of the rich conditional probabilistic structure of\nthe limiting model. The inherent dependence structure complicates the\nmathematical analysis significantly. In the process of proving our main\nresults, we derive a general conditional propagation of chaos result, which is\nof independent interest. In addition, our novel approach of studying the\nlimiting behavior of random samples proves to be a very useful methodology for\nfully grasping the asymptotic behavior of co-evolving particle systems.\nNumerical results are included to illustrate the theoretical findings.",
        "In this paper, we study Dirichlet problem for non-local operator on bounded\ndomains in ${\\mathbb R}^d$\n  $$\n  {\\cal L}u = {\\rm div}(A(x) \\nabla (x)) + b(x) \\cdot \\nabla u(x)\n  + \\int_{{\\mathbb R}^d} (u(y)-u(x) ) J(x, dy) , $$ where\n$A(x)=(a_{ij}(x))_{1\\leq i,j\\leq d}$ is a measurable $d\\times d$ matrix-valued\nfunction on ${\\mathbb R}^d$ that is uniformly elliptic and bounded, $b$ is an\n${\\mathbb R}^d$-valued function so that $|b|^2$ is in some Kato class ${\\mathbb\nK}_d$, for each $x\\in {\\mathbb R}^d$, $J(x, dy)$ is a finite measure on\n${\\mathbb R}^d$ so that $x\\mapsto J(x, {\\mathbb R}^d)$ is in the Kato class\n${\\mathbb K}_d$. We show there is a unique Feller process $X$ having strong\nFeller property associated with ${\\cal L}$, which can be obtained from the\ndiffusion process having generator $ {\\rm div}(A(x) \\nabla (x)) + b(x) \\cdot\n\\nabla u(x) $ through redistribution. We further show that for any bounded\nconnected open subset $D\\subset{\\mathbb R}^d$ that is regular with respect to\nthe Laplace operator $\\Delta$ and for any bounded continuous function $\\varphi\n$ on $D^c$, the Dirichlet problem ${\\cal L} u=0$ in $D$ with $u=\\varphi$ on\n$D^c$ has a unique bounded continuous weak solution on ${\\mathbb R}^d$. This\nunique weak solution can be represented in terms of the Feller process\nassociated with ${\\cal L}$.",
        "In this paper, we propose a driven-dissipative scheme for generating\nnon-Gaussian mechanical entangled states and remotely preparing mechanical\nSchr\\\"{o}dinger cat states via the entanglement. The system under study\nconsists of a cavity optomechanical setup with two frequency-mismatched\nmechanical oscillators coupled to a cavity field driven by a bichromatic pump.\nWe show that under proper conditions, an effective Hamiltonian for\nnondegenerate parametric downconversion involving the two mechanical\noscillators and the cavity field can be engineered. We demonstrate analytically\nand numerically that the cavity dissipation drives the mechanical oscillators\ninto a steady-state pair-coherent state. The no-Gaussianity and nonclassical\nproperties, including Winger negativity, entanglement and quantum steering, of\nthe achieved non-Gaussian mechanical state are investigated in detail. We\nfurther show that homodyne detection on one mechanical oscillator enables the\nremote generation of Schr\\\"{o}dinger cat states in the other oscillator through\nthe non-Gaussian mechanical entanglement. As we show, this detection can be\nimplemented by transferring the mechanical state to the output field of an\nauxiliary probe cavity coupled to the target oscillator, followed by homodyne\ndetection on the output field. We also discuss the robustness of the mechanical\nentangled states and cat states against thermal fluctuations. Our findings\nestablish a feasible approach for the dissipative and remote preparation of\nmechanical nonclassical states.",
        "Over the last years, a number of broadband reverberation mapping campaigns\nhave been conducted to explore the short-term UV and optical variability of\nnearby AGN. Despite the extensive data collected, the origin of the observed\nvariability is still debated in the literature. Frequency-resolved time lags\noffer a promising approach to distinguish between different scenarios, as they\nprobe variability on different time scales. In this study, we present the\nexpected frequency-resolved lags resulting from X-ray reprocessing in the\naccretion disk. The predicted lags are found to feature a general shape that\nresembles that of observational measurements, while exhibiting strong\ndependence on various physical parameters. Additionally, we compare our model\npredictions to observational data for the case of NGC 5548, concluding that the\nX-ray illumination of the disk can effectively account for the observed\nfrequency-resolved lags and power spectra in a self-consistent way. To date,\nX-ray disk reprocessing is the only physical model that has successfully\nreproduced the observed multi-wavelength variability, in both amplitude and\ntime delays, across a range of temporal frequencies.",
        "We present a sample of six F200W and three F277W dropout sources identified\nas $16<z<25$ galaxy candidates based on the deepest JWST\/NIRCam data to date,\nprovided by the MIRI Deep Imaging Survey (MIDIS) and the Next Generation Deep\nExtragalactic Exploratory Public survey (NGDEEP), reaching 5$\\sigma$ depths of\n$\\sim31.5$ mag (AB) at $\\geq2$ $\\mu$m. We estimate ultraviolet (UV) luminosity\nfunctions and densities at $z\\sim17$ and $z\\sim25$. We find that the number\ndensity of galaxies with absolute magnitudes $-19<M_\\mathrm{UV}<-18$ (AB) at\n$z\\sim17$ ($z\\sim25$) is a factor of 4 (25) smaller than at $z\\sim12$; a\nsimilar evolution is observed for the luminosity density. Compared to\nstate-of-the-art galaxy simulations, we find the need for an enhanced UV-photon\nproduction at $z=17-25$ in $\\mathrm{M}_\\mathrm{DM}=10^{8.5-9.5}$ M$_\\odot$ dark\nmatter halos, maybe provided by an increase in the star formation efficiency at\nearly times and\/or by intense bursts fed by very low metallicity or primordial\ngas. There are few robust theoretical predictions for the evolution of galaxies\nabove $z\\sim20$ in the literature, however, the continuing rapid drop in the\nhalo mass function suggests more rapid evolution than we observe if photon\nproduction efficiencies remained constant. Our $z>16$ galaxy candidates present\nmass-weighted ages around 30 Myr, and attenuations $\\mathrm{A(V)}<0.1$ mag.\nTheir average stellar mass is\n$\\mathrm{M}_\\bigstar\\sim10^{7}\\,\\mathrm{M}_\\odot$, implying a star formation\nefficiency (stellar-to-baryon mass fraction) around 10%. We find three galaxies\nwith very blue UV spectral slopes ($\\beta\\sim-3$) compatible with low\nmetallicity or Pop III and young ($\\lesssim10$ Myr) stars and\/or high escape\nfractions of ionizing photons, the rest presenting slopes $\\beta\\sim-2.5$\nsimilar to $z=10-12$ samples.",
        "Post-COVID Syndrome (PCS), encompassing the multifaceted sequelae of\nCOVID-19, can be severity-graded using a score comprising 12 different\nlong-term symptom complexes. Acute COVID-19 severity and individual resilience\nwere previously identified as key predictors of this score. This study\nvalidated these predictors and examined their relationship to PCS symptom\ncomplexes, using an expanded dataset (n=3,372) from the COVIDOM cohort study.\nClassification and Regression Tree (CART) analysis resolved the detailed\nrelationship between the predictors and the constituting symptom complexes of\nthe PCS score. Among newly recruited COVIDOM participants (n=1,930), the PCS\nscore was again found to be associated with both its putative predictors. Of\nthe score-constituting symptom complexes, neurological symptoms, sleep\ndisturbance, and fatigue were predicted by individual resilience, whereas acute\ndisease severity predicted exercise intolerance, chemosensory deficits, joint\nor muscle pain, signs of infection, and fatigue. These associations inspired\nthe definition of two novel PCS scores that included the above-mentioned\nsubsets of symptom complexes only. Both novel scores were inversely correlated\nwith quality of life, measured by the EQ-5D-5L index. The newly defined scores\nmay enhance the assessment of PCS severity, both in a research context and to\ndelineate distinct PCS subdomains with different therapeutic and interventional\nneeds in clinical practise."
      ]
    }
  },
  {
    "id":2411.00758,
    "research_type":"basic",
    "start_id":"b6",
    "start_title":"Introduction to Nonimaging Optics, second edition",
    "start_abstract":"Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book.",
    "start_categories":[
      "physics.optics"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Inverse methods for illumination optics"
      ],
      "abstract":[
        "\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "On the local Maxwellians solving the Boltzmann equation with boundary\n  condition",
        "Ill-posedness of the pure-noise Dean-Kawasaki equation",
        "Langevin Bi-fidelity Importance Sampling for Failure Probability\n  Estimation",
        "Exponential stability for an infinite memory wave equation with\n  frictional damping and logarithmic nonlinear terms",
        "A Multi-Objective Portfolio of Portfolios Problem with Qualitative\n  Performance Assessments",
        "Statistical inference for Levy-driven graph supOU processes: From short-\n  to long-memory in high-dimensional time series",
        "Nonradial stability of self-similar blowup to Keller-Segel equation in\n  three dimensions",
        "Local intersection cohomology of varieties of complexes",
        "A lower bound on the Ulrich complexity of hypersurfaces",
        "Association measures for two-way contingency tables based on\n  multi-categorical proportional reduction in error",
        "Stein's unbiased risk estimate and Hyv\\\"arinen's score matching",
        "Stochastic very weak solution to parabolic equations with singular\n  coefficients",
        "On Bezdek's conjecture for high-dimensional convex bodies with an\n  aligned center of symmetry",
        "Increasing the Energy-Efficiency of Wearables Using Low-Precision Posit\n  Arithmetic with PHEE",
        "Coherent Local Explanations for Mathematical Optimization",
        "Transformers with Joint Tokens and Local-Global Attention for Efficient\n  Human Pose Estimation",
        "Imperfect Knowledge Management (IKM) in GEFRED (GENeralized model for\n  Fuzzy RElational Databases)",
        "An Interpretable Neural Control Network with Adaptable Online Learning\n  for Sample Efficient Robot Locomotion Learning",
        "We Need to Effectively Integrate Computing Skills Across Discipline\n  Curricula",
        "Sweeping Orders for Simplicial Complex Reconstruction",
        "Rapid and Inexpensive Inertia Tensor Estimation from a Single Object\n  Throw",
        "Fractional Correspondence Framework in Detection Transformer",
        "Modelling Capillary Rise with a Slip Boundary Condition: Well-posedness\n  and Long-time Dynamics of Solutions to Washburn's Equation",
        "A comprehensive study of bound-states for the nonlinear Schr\\\"odinger\n  equation on single-knot metric graphs",
        "A precise asymptotic analysis of learning diffusion models: theory and\n  insights",
        "BoKDiff: Best-of-K Diffusion Alignment for Target-Specific 3D Molecule\n  Generation",
        "TMI-CLNet: Triple-Modal Interaction Network for Chronic Liver Disease\n  Prognosis From Imaging, Clinical, and Radiomic Data Fusion",
        "Constructing self-similar subsets within the fractal support of Lacunary\n  Wavelet Series for their multifractal analysis"
      ],
      "abstract":[
        "We derive the expressions of the local Maxwellians that solve the Boltzmann\nequation in the interior of an open domain. We determine which of these local\nMaxwellians satisfy the Boltzmann equation in a regular domain with boundary,\nwithout assuming the boundedness of the domain. We investigate separately, on\nthe one hand, the case of the bounce-back boundary condition in any dimension,\nand on the other hand the case of the specular reflection boundary condition,\nin dimension $d = 2$ and $d = 3$. In the case of the bounce-back boundary\ncondition, we prove that the only local Maxwellians solving the Boltzmann\nequation with boundary condition are the global Maxwellians. In the case of the\nspecular reflection, we provide a complete classification of the domains for\nwhich only the global Maxwellians solve the Boltzmann equation with boundary\ncondition, and we describe all the local Maxwellians that solve the equation\nfor the domains presenting symmetries.",
        "We prove that the Dean-Kawasaki-type stochastic partial differential equation\n$$\\partial \\rho= \\nabla\\cdot (\\sqrt{\\rho\\,}\\, \\xi) + \\nabla\\cdot \\left(\\rho\\,\nH(\\rho)\\right)$$ with vector-valued space-time white noise $\\xi$, does not\nadmit solutions for any initial measure and any vector-valued bounded\nmeasurable function $H$ on the space of measures. This applies in particular to\nthe pure-noise Dean-Kawasaki equation ($H\\equiv 0$). The result is sharp, in\nthe sense that solutions are known to exist for some unbounded $H$.",
        "Estimating failure probability is one of the key tasks in the field of\nuncertainty quantification. In this domain, importance sampling has proven to\nbe an effective estimation strategy; however, its efficiency heavily depends on\nthe choice of the biasing distribution. An improperly selected biasing\ndistribution can significantly increase estimation error. One way to solve this\nproblem is to leverage a less expensive, lower-fidelity surrogate. Building on\nthe accessibility to such a model and its derivative on the random uncertain\ninputs, we introduce an importance-sampling-based estimator, termed the\nLangevin bi-fidelity importance sampling (L-BF-IS), which uses\nscore-function-based sampling algorithms to generate new samples and\nsubstantially reduces the mean square error (MSE) of failure probability\nestimation. The proposed method demonstrates lower estimation error, especially\nin high-dimensional ($\\geq 100$) input spaces and when limited high-fidelity\nevaluations are available. The L-BF-IS estimator's effectiveness is validated\nthrough experiments with two synthetic functions and two real-world\napplications governed by partial differential equations. These real-world\napplications involve a composite beam, which is represented using a simplified\nEuler-Bernoulli equation as a low-fidelity surrogate, and a steady-state\nstochastic heat equation, for which a pre-trained neural operator serves as the\nlow-fidelity surrogate.",
        "This article is concerned with the energy decay of an infinite memory wave\nequation with a logarithmic nonlinear term and a frictional damping term. The\nproblem is formulated in a bounded domain in $\\mathbb R^d$ ($d\\ge3$) with a\nsmooth boundary, on which we prescribe a mixed boundary condition of the\nDirichlet and the acoustic types. We establish an exponential decay result for\nthe energy with a general material density $\\rho(x)$ under certain assumptions\non the involved coefficients. The proof is based on a contradiction argument,\nthe multiplier method and some microlocal analysis techniques. In addition, if\n$\\rho(x)$ takes a special form, our result even holds without the damping\neffect, that is, the infinite memory effect alone is strong enough to guarantee\nthe exponential stability of the system.",
        "We present a multi-objective portfolio decision model that involves selecting\nboth a portfolio of projects and a set of elements to allocate to each project.\nOur model includes a defined set of objectives to optimize, with projects\ncontributing to these objectives in various ways. The elements included in the\nportfolios are assessed based on both qualitative and quantitative criteria.\nProjects can only be selected for the portfolio if they meet specific\nrequirements defined by threshold values on the criteria. The model is\nadaptable to include temporal considerations and stochastic, making it suitable\nfor a wide range of real-life applications. To manage the decision-making\nprocess, we employ an interactive multi-objective method that integrates the\nselection of both portfolios and elements. After making initial selections, we\nask the decision-maker to evaluate the portfolios, from which we derive a\nseries of rules to be incorporated into the multiobjective model until the\ndecision-maker is satisfied. We illustrate the functionality of our model\nthrough an illustrative case study.",
        "This article introduces Levy-driven graph supOU processes, offering a\nparsimonious parametrisation for high-dimensional time-series, where\ndependencies between the individual components are governed via a graph\nstructure. Specifically, we propose a model specification that allows for a\nsmooth transition between short- and long-memory settings while accommodating a\nwide range of marginal distributions.\n  We further develop an inference procedure based on the generalised method of\nmoments, establish its asymptotic properties and demonstrate its strong finite\nsample performance through a simulation study.\n  Finally, we illustrate the practical relevance of our new model and\nestimation method in an empirical study of wind capacity factors in an European\nelectricity network context.",
        "In three dimensions, the parabolic-elliptic Keller-Segel system exhibits a\nrich variety of singularity formations. Notably, it admits an explicit\nself-similar blow-up solution whose radial stability, conjectured more than two\ndecades ago in [Brenner-Constantin-Kadanoff-Schenkel-Venkataramani, 1999], was\nrecently confirmed by [Glogi\\'c-Sch\\\"orkhuber, 2024]. This paper aims to extend\nthe radial stability to the nonradial setting, building on the\nfinite-codimensional stability analysis in our previous work [Li-Zhou, 2024].\nThe main input is the mode stability of the linearized operator, whose nonlocal\nnature presents challenges for the spectral analysis. Besides a quantitative\nperturbative analysis for the high spherical classes, we adapted in the first\nspherical class the wave operator method of [Li-Wei-Zhang, 2020] for the fluid\nstability to localize the operator and remove the known unstable mode\nsimultaneously. Our method provides localization beyond the partial mass\nvariable and is independent of the explicit formula of the profile, so it\npotentially sheds light on other linear nonlocal problems.",
        "We compute the local intersection cohomology of the irreducible components of\nvarieties of complexes, by using Lusztig's geometric approach to quantum groups\nand explicit constructions of elements of Lusztig's canonical bases.",
        "We give a lower bound on the Ulrich complexity of hypersurfaces of dimension\n$n \\ge 6$.",
        "In two-way contingency tables under an asymmetric situation, where the row\nand column variables are defined as explanatory and response variables\nrespectively, quantifying the extent to which the explanatory variable\ncontributes to predicting the response variable is important. One\nquantification method is the association measure, which indicates the degree of\nassociation in a range from $0$ to $1$. Among various measures, those based on\nproportional reduction in error (PRE) are particularly notable for their\nsimplicity and intuitive interpretation. These measures, including\nGoodman-Kruskal's lambda proposed in 1954, are widely implemented in\nstatistical software such as R and SAS and remain extensively used. However, a\nknown limitation of PRE measures is their potential to return a value of $0$\ndespite no independence. This issue arises because the measures are constructed\nbased solely on the maximum joint and marginal probabilities, failing to\nutilize the information available in the contingency table fully. To address\nthis problem, we propose new association measures designed for the proportional\nreduction in error with multiple categories. The properties of the proposed\nmeasure are examined and their utility is demonstrated through simulations and\nreal data analyses. The results suggest their potential as practical tools in\napplied statistics.",
        "We study two G-modeling strategies for estimating the signal distribution\n(the empirical Bayesian's prior) from observations corrupted with normal noise.\nFirst, we choose the signal distribution by minimizing Stein's unbiased risk\nestimate (SURE) of the implied Eddington\/Tweedie Bayes denoiser, an approach\nmotivated by optimal empirical Bayesian shrinkage estimation of the signals.\nSecond, we select the signal distribution by minimizing Hyv\\\"arinen's score\nmatching objective for the implied score (derivative of log-marginal density),\ntargeting minimal Fisher divergence between estimated and true marginal\ndensities. While these strategies appear distinct, they are known to be\nmathematically equivalent. We provide a unified analysis of SURE and score\nmatching under both well-specified signal distribution classes and\nmisspecification. In the classical well-specified setting with homoscedastic\nnoise and compactly supported signal distribution, we establish nearly\nparametric rates of convergence of the empirical Bayes regret and the Fisher\ndivergence. In a commonly studied misspecified model, we establish fast rates\nof convergence to the oracle denoiser and corresponding oracle inequalities.\nOur empirical results demonstrate competitiveness with nonparametric maximum\nlikelihood in well-specified settings, while showing superior performance under\nmisspecification, particularly in settings involving heteroscedasticity and\nside information.",
        "A class of stochastic parabolic equations with singular potentials is\nanalysed in the chaos expansion setting where the Wick product is used to give\nsense to the product of generalized stochastic processes. For the analysis of\nsuch equations we combine the chaos expansion method from the white noise\nanalysis and the concept of very weak solutions from the theory of partial\ndifferential equations. The stochastic very weak solution to the stochastic\nparabolic evolution problem is defined and its existence and uniqueness are\nshown. For regular enough potentials and data we prove consistency of\nstochastic very weak solution with a stochastic weak solution. We give an\nexample to illustrate the method and possible applications.",
        "In 1999, K. Bezdek posed a conjecture stating that among all convex bodies in\n$\\mathbb R^3$, ellipsoids and bodies of revolution are characterized by the\nfact that all their planar sections have an axis of reflection. We prove\nBezdek's conjecture in arbitrary dimension $n\\geq 3$, assuming only that\nsections passing through a fixed point have an axis of reflection, provided\nthat the complementary invariant subspaces are all parallel to a fixed\nhyperplane. The result is proven in both orthogonal and affine settings.",
        "Wearable biomedical devices are increasingly being used for continuous\npatient health monitoring, enabling real-time insights and extended data\ncollection without the need for prolonged hospital stays. These devices must be\nenergy efficient to minimize battery size, improve comfort, and reduce\nrecharging intervals. This paper investigates the use of specialized\nlow-precision arithmetic formats to enhance the energy efficiency of biomedical\nwearables. Specifically, we explore posit arithmetic, a floating-point-like\nrepresentation, in two key applications: cough detection for chronic cough\nmonitoring and R peak detection in ECG analysis. Simulations reveal that 16-bit\nposits can replace 32-bit IEEE 754 floating point numbers with minimal accuracy\nloss in cough detection. For R peak detection, posit arithmetic achieves\nsatisfactory accuracy with as few as 10 or 8 bits, compared to the 16-bit\nrequirement for floating-point formats. To further this exploration, we\nintroduce PHEE, a modular and extensible architecture that integrates the\nCoprosit posit coprocessor within a RISC-V-based system. Using the X-HEEP\nframework, PHEE seamlessly incorporates posit arithmetic, demonstrating reduced\nhardware area and power consumption compared to a floating-point counterpart\nsystem. Post-synthesis results targeting 16nm TSMC technology show that the\nposit hardware targeting these biomedical applications can be 38% smaller and\nconsume up to 54% less energy at the functional unit level, with no performance\ncompromise. These findings establish the potential of low-precision posit\narithmetic to significantly improve the energy efficiency of wearable\nbiomedical devices.",
        "The surge of explainable artificial intelligence methods seeks to enhance\ntransparency and explainability in machine learning models. At the same time,\nthere is a growing demand for explaining decisions taken through complex\nalgorithms used in mathematical optimization. However, current explanation\nmethods do not take into account the structure of the underlying optimization\nproblem, leading to unreliable outcomes. In response to this need, we introduce\nCoherent Local Explanations for Mathematical Optimization (CLEMO). CLEMO\nprovides explanations for multiple components of optimization models, the\nobjective value and decision variables, which are coherent with the underlying\nmodel structure. Our sampling-based procedure can provide explanations for the\nbehavior of exact and heuristic solution algorithms. The effectiveness of CLEMO\nis illustrated by experiments for the shortest path problem, the knapsack\nproblem, and the vehicle routing problem.",
        "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) have led\nto significant progress in 2D body pose estimation. However, achieving a good\nbalance between accuracy, efficiency, and robustness remains a challenge. For\ninstance, CNNs are computationally efficient but struggle with long-range\ndependencies, while ViTs excel in capturing such dependencies but suffer from\nquadratic computational complexity. This paper proposes two ViT-based models\nfor accurate, efficient, and robust 2D pose estimation. The first one,\nEViTPose, operates in a computationally efficient manner without sacrificing\naccuracy by utilizing learnable joint tokens to select and process a subset of\nthe most important body patches, enabling us to control the trade-off between\naccuracy and efficiency by changing the number of patches to be processed. The\nsecond one, UniTransPose, while not allowing for the same level of direct\ncontrol over the trade-off, efficiently handles multiple scales by combining\n(1) an efficient multi-scale transformer encoder that uses both local and\nglobal attention with (2) an efficient sub-pixel CNN decoder for better speed\nand accuracy. Moreover, by incorporating all joints from different benchmarks\ninto a unified skeletal representation, we train robust methods that learn from\nmultiple datasets simultaneously and perform well across a range of scenarios\n-- including pose variations, lighting conditions, and occlusions. Experiments\non six benchmarks demonstrate that the proposed methods significantly\noutperform state-of-the-art methods while improving computational efficiency.\nEViTPose exhibits a significant decrease in computational complexity (30% to\n44% less in GFLOPs) with a minimal drop of accuracy (0% to 3.5% less), and\nUniTransPose achieves accuracy improvements ranging from 0.9% to 43.8% across\nthese benchmarks.",
        "Imperfect Knowledge Management (IKM) aids in managing imprecise, uncertain,\nor incomplete aspects of meaning. IKM acknowledges that an enterprise's\nknowledge is often imperfect, characterized by varying degrees of imprecision,\nuncertainty, or incompleteness. In this context, knowledge is viewed as an\nobject described by attributes and values. Our focus is on the domain of\ncompetencies (know how) in the production of coated cardboard, particularly the\nprocess of converting finished products from the manufactured cardboard. This\nprocess involves both classic and fuzzy attributes that are used to assess the\nquality of the cardboard. This article introduces a set of protocols designed\nto model a fuzzy metaknowledge base using GEFRED (GENeralized model for Fuzzy\nRElational Databases) in an Oracle 8i relational database system.",
        "Robot locomotion learning using reinforcement learning suffers from training\nsample inefficiency and exhibits the non-understandable\/black-box nature. Thus,\nthis work presents a novel SME-AGOL to address such problems. Firstly,\nSequential Motion Executor (SME) is a three-layer interpretable neural network,\nwhere the first produces the sequentially propagating hidden states, the second\nconstructs the corresponding triangular bases with minor non-neighbor\ninterference, and the third maps the bases to the motor commands. Secondly, the\nAdaptable Gradient-weighting Online Learning (AGOL) algorithm prioritizes the\nupdate of the parameters with high relevance score, allowing the learning to\nfocus more on the highly relevant ones. Thus, these two components lead to an\nanalyzable framework, where each sequential hidden state\/basis represents the\nlearned key poses\/robot configuration. Compared to state-of-the-art methods,\nthe SME-AGOL requires 40% fewer samples and receives 150% higher final\nreward\/locomotion performance on a simulated hexapod robot, while taking merely\n10 minutes of learning time from scratch on a physical hexapod robot. Taken\ntogether, this work not only proposes the SME-AGOL for sample efficient and\nunderstandable locomotion learning but also emphasizes the potential\nexploitation of interpretability for improving sample efficiency and learning\nperformance.",
        "Computing is increasingly central to innovation across a wide range of\ndisciplinary and interdisciplinary problem domains. Students across\nnoncomputing disciplines need to apply sophisticated computational skills and\nmethods to fields as diverse as biology, linguistics, and art. Furthermore,\ncomputing plays a critical role in \"momentous geopolitical events\", such as\nelections in several countries including the US, and is changing how people\n\"work, collaborate, communicate, shop, eat, travel, get news and entertainment,\nand quite simply live\". Traditional computing courses, however, fail to equip\nnon-computing discipline students with the necessary computing skills - if they\ncan even get into classes packed with CS majors. A pressing question facing\nacademics today is: How do we effectively integrate computing skills that are\nuseful for the discipline into discipline curricula?\n  We advocate an approach where courses in discipline X include the computing\nrelevant to the learning outcomes of that course, as used by practitioners in\nX. We refer to the computing skills relevant to a course in discipline X as an\n\"ounce of computing skills\", to highlight our belief regarding the amount of\ncomputing to be integrated in that course. In this article, we outline our\ninsights regarding the development of an ounce of computing skills for a\ndiscipline course, and the evaluation of the developed ounce. The key takeaways\nare that the goal has to be to advance students in their disciplines, and only\nthe disciplinary experts can tell us how computing is used in that discipline.\nComputer scientists know how to teach computing, but the classes can't be about\nCS values. The disciplinary values are paramount.",
        "Simplicial complexes arising from real-world settings may not be directly\nobservable. Hence, for an unknown simplicial complex in Euclidean space, we\nwant to efficiently reconstruct it by querying local structure. In particular,\nwe are interested in queries for the indegree of a simplex $\\sigma$ in some\ndirection: the number of cofacets of $\\sigma$ contained in some halfspace\n\"below\" $\\sigma$. Fasy et al. proposed a method that, given the vertex set of a\nsimplicial complex, uses indegree queries to reconstruct the set of edges. In\nparticular, they use a sweep algorithm through the vertex set, identifying\nedges adjacent to and above each vertex in the sweeping order. The algorithm\nrelies on a natural but crucial property of the sweeping order: at a given\nvertex $v$, all edges adjacent to $v$ contained in the halfspace below $v$ have\nanother endpoint that appeared earlier in the order.\n  The edge reconstruction algorithm does not immediately extend to\nhigher-dimensional simplex reconstruction. In particular, it is not possible to\nsweep through a set of $i$-simplices in a fixed direction and maintain that all\n$(i+1)$-cofacets of a given simplex $\\sigma$ that come below $\\sigma$ are\nknown. We circumvent this by defining a sweeping order on a set of\n$i$-simplices, that additionally pairs each $i$-simplex $\\sigma$ with a\ndirection perpendicular to $\\sigma$. Analogous to Fasy et al., our order has\nthe crucial property that, at any $i$-simplex $\\sigma$ paired with direction\n$s$, each $(i+1)$-dimensional coface of $\\sigma$ that lies in the halfspace\nbelow $\\sigma$ with respect to the direction $s$ has an $i$-dimensional face\nthat appeared earlier in the order. We show how to compute such an order and\nuse it to extend the edge reconstruction algorithm of Fasy et al. to simplicial\ncomplex reconstruction. Our algorithm can reconstruct arbitrary embedded\nsimplicial complexes.",
        "The inertia tensor is an important parameter in many engineering fields, but\nmeasuring it can be cumbersome and involve multiple experiments or accurate and\nexpensive equipment. We propose a method to measure the moment of inertia\ntensor of a rigid body from a single spinning throw, by attaching a small and\ninexpensive stand-alone measurement device consisting of a gyroscope,\naccelerometer and a reaction wheel. The method includes a compensation for the\nincrease of moment of inertia due to adding the measurement device to the body,\nand additionally obtains the location of the centre of gravity of the body as\nan intermediate result. Experiments performed with known rigid bodies show that\nthe mean accuracy is around 2\\%.",
        "The Detection Transformer (DETR), by incorporating the Hungarian algorithm,\nhas significantly simplified the matching process in object detection tasks.\nThis algorithm facilitates optimal one-to-one matching of predicted bounding\nboxes to ground-truth annotations during training. While effective, this strict\nmatching process does not inherently account for the varying densities and\ndistributions of objects, leading to suboptimal correspondences such as failing\nto handle multiple detections of the same object or missing small objects. To\naddress this, we propose the Regularized Transport Plan (RTP). RTP introduces a\nflexible matching strategy that captures the cost of aligning predictions with\nground truths to find the most accurate correspondences between these sets. By\nutilizing the differentiable Sinkhorn algorithm, RTP allows for soft,\nfractional matching rather than strict one-to-one assignments. This approach\nenhances the model's capability to manage varying object densities and\ndistributions effectively. Our extensive evaluations on the MS-COCO and VOC\nbenchmarks demonstrate the effectiveness of our approach. RTP-DETR, surpassing\nthe performance of the Deform-DETR and the recently introduced DINO-DETR,\nachieving absolute gains in mAP of +3.8% and +1.7%, respectively.",
        "The aim of this paper is to extend Washburn's capillary rise equation by\nincorporating a slip condition at the pipe wall. The governing equation is\nderived using fundamental principles from continuum mechanics. A new scaling is\nintroduced, allowing for a systematic analysis of different flow regimes. We\nprove the global-in-time existence and uniqueness of a bounded positive\nsolution to Washburn's equation that includes the slip parameter, as well as\nthe continuous dependence of the solution in the maximum norm on the initial\ndata. Thus, the initial-value problem for Washburn's equation is shown to be\nwell-posed in the sense of Hadamard. Additionally, we show that the unique\nequilibrium solution may be reached either monotonically or in an oscillatory\nfashion, similarly to the no-slip case. Finally, we determine the basin of\nattraction for the system, ensuring that the equilibrium state will be reached\nfrom the initial data we impose. These results hold for any positive value of\nthe nondimensional slip parameter in the model, and for all values of the ratio\n$h_0\/h_e$ in the range $[0,3\/2]$, where $h_0$ is the initial height of the\nfluid column and $h_e$ is its equilibrium height.",
        "We study the existence and qualitative properties of action ground-states\n(that is, bound-states with minimal action) {of the nonlinear Schr\\\"odinger\nequation} over single-knot metric graphs -- which are made of half-lines, loops\nand pendants, all connected at a single vertex. First, we prove existence of\naction ground-state for generic single-knot graphs, even in the absence of an\nassociated variational problem. Second, for regular single-knot graphs of\nlength $\\ell$, we perform a complete analysis of positive monotone\nbound-states. Furthermore, we characterize all positive bound-states when\n$\\ell$ is small and prove some symmetry-breaking results for large $\\ell$.\nFinally, we apply the results to some particular graphs to illustrate the\ncomplex relation between action ground-states and the topological {and metric}\nfeatures of the underlying metric graph.\n  The proofs are nonvariational, using a careful phase-plane analysis, the\nstudy of sections of period functions, asymptotic estimates and blowup\narguments. We show, in particular, how nonvariational techniques are\ncomplementary to variational ones in order to deeply understand bound-states of\nthe nonlinear Schr\\\"odinger equation on metric graphs.",
        "In this manuscript, we consider the problem of learning a flow or\ndiffusion-based generative model parametrized by a two-layer auto-encoder,\ntrained with online stochastic gradient descent, on a high-dimensional target\ndensity with an underlying low-dimensional manifold structure. We derive a\ntight asymptotic characterization of low-dimensional projections of the\ndistribution of samples generated by the learned model, ascertaining in\nparticular its dependence on the number of training samples. Building on this\nanalysis, we discuss how mode collapse can arise, and lead to model collapse\nwhen the generative model is re-trained on generated synthetic data.",
        "Structure-based drug design (SBDD) leverages the 3D structure of biomolecular\ntargets to guide the creation of new therapeutic agents. Recent advances in\ngenerative models, including diffusion models and geometric deep learning, have\ndemonstrated promise in optimizing ligand generation. However, the scarcity of\nhigh-quality protein-ligand complex data and the inherent challenges in\naligning generated ligands with target proteins limit the effectiveness of\nthese methods. We propose BoKDiff, a novel framework that enhances ligand\ngeneration by combining multi-objective optimization and Best-of-K alignment\nmethodologies. Built upon the DecompDiff model, BoKDiff generates diverse\ncandidates and ranks them using a weighted evaluation of molecular properties\nsuch as QED, SA, and docking scores. To address alignment challenges, we\nintroduce a method that relocates the center of mass of generated ligands to\ntheir docking poses, enabling accurate sub-component extraction. Additionally,\nwe integrate a Best-of-N (BoN) sampling approach, which selects the optimal\nligand from multiple generated candidates without requiring fine-tuning. BoN\nachieves exceptional results, with QED values exceeding 0.6, SA scores above\n0.75, and a success rate surpassing 35%, demonstrating its efficiency and\npracticality. BoKDiff achieves state-of-the-art results on the CrossDocked2020\ndataset, including a -8.58 average Vina docking score and a 26% success rate in\nmolecule generation. This study is the first to apply Best-of-K alignment and\nBest-of-N sampling to SBDD, highlighting their potential to bridge generative\nmodeling with practical drug discovery requirements. The code is provided at\nhttps:\/\/github.com\/khodabandeh-ali\/BoKDiff.git.",
        "Chronic liver disease represents a significant health challenge worldwide and\naccurate prognostic evaluations are essential for personalized treatment plans.\nRecent evidence suggests that integrating multimodal data, such as computed\ntomography imaging, radiomic features, and clinical information, can provide\nmore comprehensive prognostic information. However, modalities have an inherent\nheterogeneity, and incorporating additional modalities may exacerbate the\nchallenges of heterogeneous data fusion. Moreover, existing multimodal fusion\nmethods often struggle to adapt to richer medical modalities, making it\ndifficult to capture inter-modal relationships. To overcome these limitations,\nWe present the Triple-Modal Interaction Chronic Liver Network (TMI-CLNet).\nSpecifically, we develop an Intra-Modality Aggregation module and a\nTriple-Modal Cross-Attention Fusion module, which are designed to eliminate\nintra-modality redundancy and extract cross-modal information, respectively.\nFurthermore, we design a Triple-Modal Feature Fusion loss function to align\nfeature representations across modalities. Extensive experiments on the liver\nprognosis dataset demonstrate that our approach significantly outperforms\nexisting state-of-the-art unimodal models and other multi-modal techniques. Our\ncode is available at https:\/\/github.com\/Mysterwll\/liver.git.",
        "Given a fractal $\\mathcal{I}$ whose Hausdorff dimension matches with the\nupper-box dimension, we propose a new method which consists in selecting inside\n$\\mathcal{I}$ some subsets (called quasi-Cantor sets) of almost same dimension\nand with controled properties of self-similarties at prescribed scales. It\nallows us to estimate below the Hausdorff dimension $\\mathcal{I}$ intersected\nto limsup sets of contracted balls selected according a Bernoulli law, in\ncontexts where classical Mass Transference Principles cannot be applied. We\napply this result to the computation of the increasing multifractal spectrum of\nlacunary wavelet series supported on $\\mathcal{I}$."
      ]
    }
  },
  {
    "id":2411.00758,
    "research_type":"basic",
    "start_id":"b18",
    "start_title":"Inverse methods for illumination optics",
    "start_abstract":"\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers.",
    "start_categories":[
      "math.MP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Introduction to Nonimaging Optics, second edition"
      ],
      "abstract":[
        "Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book."
      ],
      "categories":[
        "physics.optics"
      ]
    },
    "list":{
      "title":[
        "Electrical Generation of Colour Centres in Hexagonal Boron Nitride",
        "Modeling Nonlinear Optics with the Transfer Matrix Method",
        "Efficient and accurate analysis of oscillation dynamics for dissipative\n  cavity solitons based on the artificial neural network",
        "Engineering 2D Van der Waals Electrode via MBE Grown Weyl Semimetal\n  1T-WTe2 for Enhanced Photodetection in InSe",
        "Pseudospin Transverse Localization of Light in an Optical Disordered\n  Spin-Glass Phase",
        "Skyrmion Generation through the Chirality Interplay of Light and\n  Magnetism",
        "Optical force and torque on a spinning dielectric sphere",
        "Direct Observation of Strongly Tilted Dirac Points at General Positions\n  in the Reciprocal Space",
        "Singleshot Multispectral Imaging via a Chromatic Metalens Array",
        "Dual-Frequency Comb in Fiber Fabry-Perot Resonator",
        "Towards Efficient PCSEL Design: A Data-Driven Approach for Design\n  Insights",
        "High Contrast Nulling in Photonic Meshes Through Architectural\n  Redundancy",
        "Resolution enhancement in quantitative phase microscopy: a review",
        "Opinion Dynamics with Multiple Adversaries",
        "Text to Band Gap: Pre-trained Language Models as Encoders for\n  Semiconductor Band Gap Prediction",
        "Dynamical analysis of an HIV infection model including quiescent cells\n  and immune response",
        "GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for\n  Remote Sensing Image Analysis",
        "Controlling spin currents with magnon interference in a canted\n  antiferromagnet",
        "Pre-trained Models Succeed in Medical Imaging with Representation\n  Similarity Degradation",
        "Mass and Metal Flows in Isolated IllustrisTNG Halos",
        "Resonance nuclear excitation of the $^{229}$Th nucleus via electronic\n  bridge process in Th~II",
        "Adaptive Prompt: Unlocking the Power of Visual Prompt Tuning",
        "SN1987A bounds on neutrino quantum decoherence",
        "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
        "Cryogenic operation of silicon photomultiplier arrays",
        "LexGenie: Automated Generation of Structured Reports for European Court\n  of Human Rights Case Law",
        "Low-Rank Adapters Meet Neural Architecture Search for LLM Compression",
        "Strong Solutions and Quantization-Based Numerical Schemes for a Class of\n  Non-Markovian Volatility Models"
      ],
      "abstract":[
        "Defects in wide band gap crystals have emerged as a promising platform for\nhosting colour centres that enable quantum photonic applications. Among these,\nhexagonal boron nitride (hBN), a van der Waals material, stands out for its\nability to be integrated into heterostructures, enabling unconventional charge\ninjection mechanisms that bypass the need for p-n junctions. This advancement\nallows for the electrical excitation of hBN colour centres deep inside the\nlarge hBN bandgap, which has seen rapid progress in recent developments. Here,\nwe fabricate hBN electroluminescence (EL) devices that generate narrowband\ncolour centres suitable for electrical excitation. The colour centres are\nlocalised to tunnelling current hotspots within the hBN flake, which are\ndesigned during device fabrication. We outline the optimal conditions for\ndevice operation and colour centre stability, focusing on minimising background\nemission and ensuring prolonged operation. Our findings follow up on the\nexisting literature and mark a step forward towards the integration of hBN\nbased colour centres into quantum photonic technologies.",
        "The Transfer Matrix Method (TMM) is a widely used technique for modeling\nlinear propagation of electromagnetic waves through stratified layered media.\nHowever, since its extension to inhomogeneous and nonlinear systems is not\nstraightforward, much more computationally demanding methods such as\nFinite-difference time-domain (FDTD) or Method of lines (MoL) are typically\nused. In this work, we extend the TMM framework to incorporate the effects of\nnonlinearity. We consider the case when strong coupling between excitons\n(electron-hole pairs) and photons leads to the formation of exciton-polaritons.\nThis extension is crucial for accurately simulating the behavior of light in\npolariton microcavities, where nonlinearities arising from exciton-exciton\ninteractions play a key role. We perform efficient simulations of light\ntransmission and reflection in a multidimensional system using the plane wave\nbasis. Additionally, we compare our extended TMM approach with the\nstate-of-the-art admittance transfer method, and highlight the computational\nadvantage of extended TMM for large-scale systems. The extended TMM not only\nprovides a robust and computationally efficient numerical framework, but also\npaves the way for the development of future low-power nonlinear optical\ndevices, polariton-based photonic circuits, and quantum photonic technologies.",
        "As a conventional means to analyze the system mechanism based on partial\ndifferential equations (PDE) or nonlinear dynamics, iterative algorithms are\ncomputationally intensive. In this framework, the details of oscillating\ndynamics of cavity solitons are beyond the reach of traditional mathematical\nanalysis. In this work, we demonstrate that this long-standing challenge could\nbe tackled down with the Long Short-Term Memory (LSTM) neural network. We\npropose the incorporating parameter-fed ports, which are capable of recognizing\nperiod-doubling bifurcations of respiratory solitons and quickly predicting\nnonlinear dynamics of solitons with arbitrary parameter combinations and\narbitrary time series lengths. The model predictions capture oscillatory\nfeatures with a small Root Mean Square Errors (RMSE) = 0.01676 and an absolute\nerror that barely grows with the length of the prediction time. Lugiato-Lefever\nequation (LLE) based parameter space boundaries for typical oscillatory\npatterns are plotted at about 120 times the speed relative to the split-step\nFourier method (SSFM) and higher resolution.",
        "Achieving low contact resistance in advanced electronic devices remains a\nsignificant challenge. As the demand for faster and more energy-efficient\ndevices grows, 2D contact engineering emerges as a promising solution for\nnext-generation electronics. Beyond graphene, 1T-WTe2 has gained attention due\nto its outstanding electrical transport properties, quantum phenomena, and Weyl\nsemimetallic characteristics. We demonstrate the direct wafer-scale growth of\n1T-WTe2 via molecular beam epitaxy (MBE) and use it as a 2D contact for layered\nmaterials like InSe, which exhibits broad photoresponsivity. The performance of\nthis 2D electrode in InSe-based photodetectors is compared with conventional\nmetal electrodes. Under near-infrared (NIR) to deep ultraviolet (DUV)\nillumination, the InSe\/1T-WTe2 configuration shows a broad photoresponsivity\nrange from 0.14 to 217.58 A\/W, with fast rise\/fall times of 42\/126 ms in the\nvisible region. In contrast, the InSe\/Ti-Au configuration exhibits a peak\nphotoresponsivity of 3.64 A\/W in the DUV range, with an overall lower\nresponsivity spanning from 0.000865 A\/W to 3.64 A\/W under NIR and DUV\nillumination, respectively. Additionally, in the visible regime, it exhibits\nslower rise and fall times of 150 ms and 144 ms, respectively, compared to\nInSe\/1T-WTe2. These findings indicate that MBE-grown 1T-WTe2 serves as an\neffective 2D electrode, delivering higher photoresponsivity and faster\nphotodetection compared to traditional metal contacts. This approach offers a\nsimplified, high-performance alternative for layered material-based devices,\neliminating the need for complex heterostructure configurations.",
        "Localization phenomena during transport are typically driven by disordered\nscalar potentials. Here, we predict a universal pseudospin localization\nphenomenon induced by a disordered vectorial potential and demonstrate it\nexperimentally in an optical analogue of a classical disordered spin-glass\nmagnetic phase. In our system, a transverse disorder in the second-order\nnonlinear coupling of a nonlinear photonic crystal causes the idler-signal\nlight beam, representing the pseudospin current, to become localized in the\ntransverse plane. This effect depends strongly on the nonlinear coupling\nstrength, controlled by the optical pump power, revealing its inherently\nnonlinear and non-perturbative nature. Furthermore, this phenomenon is marked\nby decaying Rabi oscillations between the idler and signal fields, linked to\nthe disorder properties, suggesting an accompanied longitudinal decoherence\neffect. Our findings offer deep insights into spin transport in disordered\nmagnetic textures and open avenues for exploring complex magnetic phases and\nphase transitions using nonlinear optics.",
        "Light beams, with their rich degrees of freedom, including polarization and\nphase, along with their flexible tunability, have emerged as an ideal tool for\ngenerating magnetic topological textures.However, how to precisely control the\nlight beams to generate a specific number of magnetic topological textures on\ndemand remains a critical scientific issue that needs to be resolved. Based on\nthe numerical simulation of the Landau-Lifshitz-Gilbert equation, we propose\nthat circularly polarized Laguerre-Gaussian beams can induce chiral magnetic\nfields through the interaction of the chirality of these beams'angular momenta.\nBy utilizing these chiral magnetic fields, skyrmions or skyrmionium can be\ninduced in chiral magnets. Moreover, the vectorial magnetic fields can be\nmanipulated by adjusting the angular momenta and light intensity, thereby\ngenerating target chiral patterns and strengths, which allows for precise\ncontrol over the type and number of these topological magnetic textures. This\nfinding not only reveals the underlying physical mechanisms of the interaction\nbetween light and magnetic systems but also provides a feasible solution for\nthe on-demand generation and encoding of skyrmions.",
        "Optical force can enable precise manipulations of small particles for various\napplications. It is well known that an isotropic lossless dielectric sphere is\nonly subject to forward optical force under the illumination of an\nelectromagnetic plane wave. By using rigorous full-wave simulations, we show\nthat such a sphere can experience a lateral optical force and an optical torque\nbesides the conventional longitudinal force, if it spins with a constant\nangular velocity. The emergence of the unusual optical force and torque is\nattributed to the breaking of mirror and cylindrical symmetries by the spinning\nmotion. Using the multipole expansion in source representation, we illustrate\nhow the spinning-induced effective bi-anisotropy generates the lateral force\nand torque on the sphere through the interference of electric and magnetic\nmultipoles. We also uncover the effect of Sagnac frequency splitting on the\noptical force and torque. The results contribute to the understanding of the\noptical force and torque in moving media and can be applied to realize\nunconventional optical manipulations of small particles.",
        "Type-II Dirac points (DPs), which occur at the intersection of strongly\ntilted and touching energy bands, exhibit many intriguing physical phenomena\nfundamentally different from the non-tilted type-I counterparts. Over the past\ndecade, their discovery has spurred extensive research into electronic systems\nand other Bloch-wave systems, such as photonic and phononic crystals. However,\ncurrent studies typically focus on type-II DPs along high-symmetry directions\nin the first Brillouin zone (FBZ) under mirror symmetry conditions, which are\nhighly restrictive and limit further investigations and applications. To\novercome the stringent constraint, here we identify and demonstrate the\nemergence of type-II DPs at general positions inside the FBZ without requiring\nthe mirror symmetry. The type-II DPs, being accidental degeneracies, are\nexperimentally realized on a metacrystal slab with H-shaped metallic patterns.\nOur findings indicate that even in the absence of mirror symmetry, type-II DPs\ncan emerge at designated locations inside the FBZ by simply rotating the\nH-shaped patterns and adjusting geometrical and physical parameters.\nFurthermore, based on the rotated type-II DPs, off-axis conical diffractions\nhave been both realized and experimentally observed. Meanwhile, we discovered\nthat during the rotation process, the type-II DPs transform into off-axis\ntype-I DPs, but still strongly tilted, resulting in the emergence of negative\nrefractions. Hence, the generic method we propose for inducing type-II or\nstrongly tilted type-I DPs without the high-symmetry limitations opens\npotential avenues for related research. For example, the observed off-axis\nconical diffraction and negative refraction could inspire future development\nand applications in photonics and other Bloch-wave systems.",
        "Real time, singleshot multispectral imaging systems are crucial for\nenvironment monitoring and biomedical imaging. Most singleshot multispectral\nimagers rely on complex computational backends, which precludes real time\noperations. In this work, we leverage the spectral selectivity afforded by\nengineered photonic materials to perform bulk of the multispectral data\nextraction in the optical domain, thereby circumventing the need for heavy\nbackend computation. We use our imager to extract multispectral data for two\nreal world objects at 8 predefined spectral channels in the 400 to 900 nm\nwavelength range. For both objects, an RGB image constructed using extracted\nmultispectral data shows good agreement with an image taken using a phone\ncamera, thereby validating our imaging approach. We believe that the proposed\nsystem can provide new avenues for the development of highly compact and low\nlatency multispectral imaging technologies.",
        "This paper presents a novel approach to dual-frequency comb generation\nutilizing a single fiber Fabry-Perot resonator, advancing the implementation of\nthese sources in fiber-based systems. Dual-comb applications such as\nspectroscopy, ranging, and imaging, known for their high-resolution and rapid\ndata acquisition capabilities, benefit significantly from the stability and\ncoherence of optical frequency comb sources. Our method leverages the\nbirefringent property of the resonator induced by the optical fiber to generate\ntwo orthogonally polarized optical frequency combs in a monolitic resonator.\nThis approach allows for the generation of two different frequency combs with\nslightly different repetition rates, exhibiting excellent mutual coherence,\nmaking it highly relevant for dual-comb applications. The 40 nm bandwidth\ngenerated combs are induced by switching-waves in a normal dispersion fiber\nFabry-Perot resonator. These comb types have the advantage of being easily\ngenerated by a pulse pumping scheme, which is employed in this study. Finally,\nthe potential of the source is demonstrated by a proof-of-concept spectroscopy\nmeasurement.",
        "We present a data-driven design approach for photonic crystals to achieve\nhigh efficiency in photonic crystal surface-emitting lasers (PCSELs). By\ndiscretizing the photonic crystal structure into a grid, we enable the\ngeneration of arbitrary lattice designs. Multiple fully connected layers\ncombined with a position embedding module extract essential features from the\nphotonic crystal designs, while coupled-wave theory (CWT) is used to evaluate\nthe efficiency (based on the ratio of surface-emitting to edge-emitting\nresonant) and quality factor Q. We introduce the Neural Networks (NNs) model to\nevaluate the structures, and to find a better performance design according to\nthe evaluation result. The model achieves high prediction accuracy, with\nPearson correlation coefficients of 0.780 for SEE and 0.887 for the\nlog-transformed Q. Additionally, we perform Shapley value analysis to identify\nthe most important Fourier coefficients, providing insights into the factors\nthat impact the performance of PCSEL designs. Our work speeds up the design\nprocess and offers valuable guidance for optimizing high-performance PCSELs,\nsupporting the development of fully photonic design automation (PDA).",
        "We demonstrate a silicon photonic architecture comprised of Double\nMach-Zehnder Interferometers (DMZIs) designed for high-contrast photonic\napplications. This configuration significantly enhances the achievable\nextinction ratio of photonic integrated circuits (PICs), reaching levels\nexceeding 80 dB. By leveraging the tunable properties of DMZIs and implementing\na systematic configuration algorithm, the proposed mesh effectively compensates\nfor fabrication imperfections and mitigates non-idealities such as back\nreflections. Experimental validation on a silicon-on-insulator platform\ndemonstrates the potential of this approach for applications requiring high\ncontrast nulling such as astronomical sensing.",
        "Quantitative phase microscopy (QPM), a technique combining phase imaging and\nmicroscopy, enables visualization of the 3D topography in reflective samples,\nas well as the inner structure or refractive index distribution of transparent\nand translucent samples. Similar to other imaging modalities, QPM is\nconstrained by the conflict between numerical aperture (NA) and field of view\n(FOV): an imaging system with a low NA has to be employed to maintain a large\nFOV. This fact severely limits the resolution in QPM up to being the\nillumination wavelength. Consequently, finer structures of samples cannot be\nresolved by using modest NA objectives in QPM. Aimed to that, many approaches,\nsuch as oblique illumination, structured illumination, and speckle illumination\n(just to cite a few), have been proposed to improve the spatial resolution (or\nthe space bandwidth product) in phase microscopy by restricting other degrees\nof freedom (mostly time). This paper aims to provide an up to date review on\nthe resolution enhancement approaches in QPM, discussing the pros and cons of\neach technique as well as the confusion on resolution definition claims on QPM\nand other coherent microscopy methods. Through this survey, we will review the\nmost appealing and useful techniques for superresolution in coherent\nmicroscopy, working with and without lenses and with special attention to QPM.",
        "Opinion dynamics model how the publicly expressed opinions of users in a\nsocial network coevolve according to their neighbors as well as their own\nintrinsic opinion. Motivated by the real-world manipulation of social networks\nduring the 2016 US elections and the 2019 Hong Kong protests, a growing body of\nwork models the effects of a strategic actor who interferes with the network to\ninduce disagreement or polarization. We lift the assumption of a single\nstrategic actor by introducing a model in which any subset of network users can\nmanipulate network outcomes. They do so by acting according to a fictitious\nintrinsic opinion. Strategic actors can have conflicting goals, and push\ncompeting narratives. We characterize the Nash Equilibrium of the resulting\nmeta-game played by the strategic actors. Experiments on real-world social\nnetwork datasets from Twitter, Reddit, and Political Blogs show that strategic\nagents can significantly increase polarization and disagreement, as well as\nincrease the \"cost\" of the equilibrium. To this end, we give worst-case upper\nbounds on the Price of Misreporting (analogous to the Price of Anarchy).\nFinally, we give efficient learning algorithms for the platform to (i) detect\nwhether strategic manipulation has occurred, and (ii) learn who the strategic\nactors are. Our algorithms are accurate on the same real-world datasets,\nsuggesting how platforms can take steps to mitigate the effects of strategic\nbehavior.",
        "In this study, we explore the use of a transformer-based language model as an\nencoder to predict the band gaps of semiconductor materials directly from their\ntext descriptions. Quantum chemistry simulations, including Density Functional\nTheory (DFT), are computationally intensive and time-consuming, which limits\ntheir practicality for high-throughput material screening, particularly for\ncomplex systems. Shallow machine learning (ML) models, while effective, often\nrequire extensive data preprocessing to convert non-numerical material\nproperties into numerical inputs. In contrast, our approach leverages textual\ndata directly, bypassing the need for complex feature engineering. We generate\nmaterial descriptions in two formats: formatted strings combining features and\nnatural language text generated using the ChatGPT API. We demonstrate that the\nRoBERTa model, pre-trained on natural language processing tasks, performs\neffectively as an encoder for prediction tasks. With minimal fine-tuning, it\nachieves a mean absolute error (MAE) of approximately 0.33 eV, performing\nbetter than shallow machine learning models such as Support Vector Regression,\nRandom Forest, and XGBoost. Even when only the linear regression head is\ntrained while keeping the RoBERTa encoder layers frozen, the accuracy remains\nnearly identical to that of the fully trained model. This demonstrates that the\npre-trained RoBERTa encoder is highly adaptable for processing domain-specific\ntext related to material properties, such as the band gap, significantly\nreducing the need for extensive retraining. This study highlights the potential\nof transformer-based language models to serve as efficient and versatile\nencoders for semiconductor materials property prediction tasks.",
        "This research gives a thorough examination of an HIV infection model that\nincludes quiescent cells and immune response dynamics in the host. The model,\nrepresented by a system of ordinary differential equations, captures the\ncomplex interaction between the host's immune response and viral infection. The\nstudy focuses on the model's fundamental aspects, such as equilibrium analysis,\ncomputing the basic reproduction number $\\mathcal{R}_0$, stability analysis,\nbifurcation phenomena, numerical simulations, and sensitivity analysis.\n  The analysis reveals both an infection equilibrium, which indicates the\npersistence of the illness, and an infection-free equilibrium, which represents\ndisease control possibilities. Applying matrix-theoretical approaches,\nstability analysis proved that the infection-free equilibrium is both locally\nand globally stable for $\\mathcal{R}_0 < 1$. For the situation of\n$\\mathcal{R}_0 > 1$, the infection equilibrium is locally asymptotically stable\nvia the Routh--Hurwitz criterion. We also studied the uniform persistence of\nthe infection, demonstrating that the infection remains present above a\npositive threshold under certain conditions. The study also found a\ntranscritical forward-type bifurcation at $\\mathcal{R}_0 = 1$, indicating a\ncritical threshold that affects the system's behavior. The model's temporal\ndynamics are studied using numerical simulations, and sensitivity analysis\nidentifies the most significant variables by assessing the effects of parameter\nchanges on system behavior.",
        "The continuous operation of Earth-orbiting satellites generates vast and\never-growing archives of Remote Sensing (RS) images. Natural language presents\nan intuitive interface for accessing, querying, and interpreting the data from\nsuch archives. However, existing Vision-Language Models (VLMs) are\npredominantly trained on web-scraped, noisy image-text data, exhibiting limited\nexposure to the specialized domain of RS. This deficiency results in poor\nperformance on RS-specific tasks, as commonly used datasets often lack\ndetailed, scientifically accurate textual descriptions and instead emphasize\nsolely on attributes like date and location. To bridge this critical gap, we\nintroduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and\nmulti-modal RS image analysis. GAIA comprises of 205,150 meticulously curated\nRS image-text pairs, representing a diverse range of RS modalities associated\nto different spatial resolutions. Unlike existing vision-language datasets in\nRS, GAIA specifically focuses on capturing a diverse range of RS applications,\nproviding unique information about environmental changes, natural disasters,\nand various other dynamic phenomena. The dataset provides a spatially and\ntemporally balanced distribution, spanning across the globe, covering the last\n25 years with a balanced temporal distribution of observations. GAIA's\nconstruction involved a two-stage process: (1) targeted web-scraping of images\nand accompanying text from reputable RS-related sources, and (2) generation of\nfive high-quality, scientifically grounded synthetic captions for each image\nusing carefully crafted prompts that leverage the advanced vision-language\ncapabilities of GPT-4o. Our extensive experiments, including fine-tuning of\nCLIP and BLIP2 models, demonstrate that GAIA significantly improves performance\non RS image classification, cross-modal retrieval and image captioning tasks.",
        "Controlling spin current lies at the heart of spintronics and its\napplications. The sign of spin currents is monotonous in ferromagnets once the\ncurrent direction is determined. Spin currents in antiferromagnets can possess\nopposite polarization, but requires enormous magnetic fields to lift the\ndegeneracy. Controlling spin currents with different polarization is urgently\ndemanded but remains hitherto elusive. Here, we demonstrate the control of spin\ncurrents at room temperature by magnon interference in a canted\nantiferromagnet, hematite recently also classified as an altermagnet.\nMagneto-optical characterization by Brillouin light scattering revealed that\nthe spatial periodicity of the beating patterns was tunable via the microwave\nfrequency. The inverse spin-Hall voltage changed sign as the frequency was\nscanned, i.e., a frequency-controlled switching of polarization in pure spin\ncurrents was obtained. Our work marks the use of antiferromagnetic magnon\ninterference to control spin currents, which substantially extends the horizon\nfor the emerging field of coherent antiferromagnetic spintronics.",
        "This paper investigates the critical problem of representation similarity\nevolution during cross-domain transfer learning, with particular focus on\nunderstanding why pre-trained models maintain effectiveness when adapted to\nmedical imaging tasks despite significant domain gaps. The study establishes a\nrigorous problem definition centered on quantifying and analyzing\nrepresentation similarity trajectories throughout the fine-tuning process,\nwhile carefully delineating the scope to encompass both medical image analysis\nand broader cross-domain adaptation scenarios. Our empirical findings reveal\nthree critical discoveries: the potential existence of high-performance models\nthat preserve both task accuracy and representation similarity to their\npre-trained origins; a robust linear correlation between layer-wise similarity\nmetrics and representation quality indicators; and distinct adaptation patterns\nthat differentiate supervised versus self-supervised pre-training paradigms.\nThe proposed similarity space framework not only provides mechanistic insights\ninto knowledge transfer dynamics but also raises fundamental questions about\noptimal utilization of pre-trained models. These results advance our\nunderstanding of neural network adaptation processes while offering practical\nimplications for transfer learning strategies that extend beyond medical\nimaging applications. The code will be available once accepted.",
        "The cicumgalactic medium (CGM) is a reservoir of metals and star-forming\nfuel. Most baryons in the universe are in the circumgalactic medium (CGM) or\nintergalactic medium (IGM). The baryon cycle -- how mass and metals reach the\nCGM from the inner regions of the galaxy and how gas from the CGM replenishes\nstar-forming activity in the inner regions -- is an essential question in\ngalaxy evolution. In this paper, we study the flow of mass and metals in a\nstacked sample of 2770 isolated halos from the IllustrisTNG cosmological\nhydrodynamic simulation. The mean gas flow as a function of radius and angle is\nsimilar across a large galactic mass range when accounting for different\nfeedback modes. Although both star formation and black holes cause powerful\noutflows, the flows from star formation are more angularly restricted. Black\nhole feedback dominates massflow throughout the halo, while star-formation\nfeedback mainly affects the inner region. When scaling by virial radius\n($R_v$), large dynamical changes occur at $0.2R_v$ for most halos, suggesting a\ncharacteristic size for the inner galaxy. Despite radio mode feedback from\nblack holes being the primary quenching mechanism in IllustrisTNG, a small\npopulation of high mass radio mode disks are able to form stars.",
        "The 8.4 eV transition in the $^{229}$Th nucleus is the basis for a\nhigh-precision nuclear clock with exceptional sensitivity to new physics\neffects. We have identified several cases in the Th$^+$ ion where electronic\nexcitations closely resonate with the nuclear excitation, with the smallest\nenergy difference being $\\Delta = -0.09$ cm$^{-1}$. We investigate the\nelectronic bridge process, in which nuclear excitation is induced via\nelectronic transitions, and demonstrate that a proper selection of laser\nfrequencies can lead to a dramatic enhancement of this effect. Additionally, we\nshow that the interaction with electrons significantly shortens the lifetime of\nthe nuclear excited state.",
        "Visual Prompt Tuning (VPT) has recently emerged as a powerful method for\nadapting pre-trained vision models to downstream tasks. By introducing\nlearnable prompt tokens as task-specific instructions, VPT effectively guides\npre-trained transformer models with minimal overhead. Despite its empirical\nsuccess, a comprehensive theoretical understanding of VPT remains an active\narea of research. Building on recent insights into the connection between\nmixture of experts and prompt-based approaches, we identify a key limitation in\nVPT: the restricted functional expressiveness in prompt formulation. To address\nthis limitation, we propose Visual Adaptive Prompt Tuning (VAPT), a new\ngeneration of prompts that redefines prompts as adaptive functions of the\ninput. Our theoretical analysis shows that this simple yet intuitive approach\nachieves optimal sample efficiency. Empirical results on VTAB-1K and FGVC\nfurther demonstrate VAPT's effectiveness, with performance gains of 7.34% and\n1.04% over fully fine-tuning baselines, respectively. Notably, VAPT also\nsurpasses VPT by a substantial margin while using fewer parameters. These\nresults highlight both the effectiveness and efficiency of our method and pave\nthe way for future research to explore the potential of adaptive prompts.",
        "We obtain stringent bounds on neutrino quantum decoherence from the analysis\nof SN1987A data. We show that for the decoherence model considered here, which\nallows for neutrino-loss along the trajectory, the bounds are many orders of\nmagnitude stronger than the ones that can be obtained from the analysis of data\nfrom reactor neutrino oscillation experiments or neutrino telescopes.",
        "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
        "The LHCb experiment at CERN has been upgraded for the Run 3 operation of the\nLarge Hadron Collider (LHC). A new concept of tracking detector based on\nScintillating Fibres (SciFi) read out with multichannel silicon\nphotomultipliers (SiPMs) was installed during its upgrade. One of the main\nchallenges the SciFi tracker will face during the Run 4 operation of the LHC is\nthe higher radiation environment due to fast neutrons, where the SiPMs are\nlocated. To cope with the increase in radiation, cryogenic cooling with liquid\nnitrogen is being investigated as a possible solution to mitigate the\nperformance degradation of the SiPMs induced by radiation damage. Thus, a\ndetailed performance study of different layouts of SiPM arrays produced by\nFondazione Bruno Kessler (FBK) and Hamamatsu Photonics K.K. is being carried\nout. These SiPMs have been designed to operate at cryogenic temperatures.\nSeveral SiPMs have been tested in a dedicated cryogenic setup down to 100 K.\nKey performance parameters such as breakdown voltage, dark count rate, photon\ndetection efficiency, gain and direct cross-talk are characterized as a\nfunction of the temperature. The main results of this study are going to be\npresented here.",
        "Analyzing large volumes of case law to uncover evolving legal principles,\nacross multiple cases, on a given topic is a demanding task for legal\nprofessionals. Structured topical reports provide an effective solution by\nsummarizing key issues, principles, and judgments, enabling comprehensive legal\nanalysis on a particular topic. While prior works have advanced query-based\nindividual case summarization, none have extended to automatically generating\nmulti-case structured reports. To address this, we introduce LexGenie, an\nautomated LLM-based pipeline designed to create structured reports using the\nentire body of case law on user-specified topics within the European Court of\nHuman Rights jurisdiction. LexGenie retrieves, clusters, and organizes relevant\npassages by topic to generate a structured outline and cohesive content for\neach section. Expert evaluation confirms LexGenie's utility in producing\nstructured reports that enhance efficient, scalable legal analysis.",
        "The rapid expansion of Large Language Models (LLMs) has posed significant\nchallenges regarding the computational resources required for fine-tuning and\ndeployment. Recent advancements in low-rank adapters have demonstrated their\nefficacy in parameter-efficient fine-tuning (PEFT) of these models. This\nretrospective paper comprehensively discusses innovative approaches that\nsynergize low-rank representations with Neural Architecture Search (NAS)\ntechniques, particularly weight-sharing super-networks. Robust solutions for\ncompressing and fine-tuning large pre-trained models are developed by\nintegrating these methodologies. Our analysis highlights the potential of these\ncombined strategies to democratize the use of LLMs, making them more accessible\nfor deployment in resource-constrained environments. The resulting models\nexhibit reduced memory footprints and faster inference times, paving the way\nfor more practical and scalable applications of LLMs. Models and code are\navailable at\nhttps:\/\/github.com\/IntelLabs\/Hardware-Aware-Automated-Machine-Learning.",
        "We investigate a class of non-Markovian processes that hold particular\nrelevance in the realm of mathematical finance. This family encompasses\npath-dependent volatility models, including those pioneered by [Platen and\nRendek, 2018] and, more recently, by [Guyon and Lekeufack, 2023], as well as an\nextension of the framework proposed by [Blanc et al., 2017]. Our study unfolds\nin two principal phases. In the first phase, we introduce a functional\nquantization scheme based on an extended version of the Lamperti transformation\nthat we propose to handle the presence of a memory term incorporated into the\ndiffusion coefficient. For scenarios involving a Brownian integral in the\ndiffusion term, we propose alternative numerical schemes that leverage the\npower of marginal recursive quantization. In the second phase, we study the\nproblem of existence and uniqueness of a strong solution for the SDEs related\nto the examples that motivate our study, in order to provide a theoretical\nbasis to correctly apply the proposed numerical schemes."
      ]
    }
  },
  {
    "id":2411.01291,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI",
    "start_abstract":"Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"
      ],
      "abstract":[
        "Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "A Scalable Approach to Probabilistic Neuro-Symbolic Verification",
        "Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems",
        "Statistical Scenario Modelling and Lookalike Distributions for\n  Multi-Variate AI Risk",
        "OvercookedV2: Rethinking Overcooked for Zero-Shot Coordination",
        "ARES: Auxiliary Range Expansion for Outlier Synthesis",
        "Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models",
        "Intrinsic Bias is Predicted by Pretraining Data and Correlates with\n  Downstream Performance in Vision-Language Encoders",
        "MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via\n  Event-Driven Text-Code Cyclic Training",
        "Can Reasoning Models Reason about Hardware? An Agentic HLS Perspective",
        "Spatiotemporal Prediction of Secondary Crashes by Rebalancing Dynamic\n  and Static Data with Generative Adversarial Networks",
        "SPPD: Self-training with Process Preference Learning Using Dynamic Value\n  Margin",
        "Ensemble based approach to quantifying uncertainty of LLM based\n  classifications",
        "CoPEFT: Fast Adaptation Framework for Multi-Agent Collaborative\n  Perception with Parameter-Efficient Fine-Tuning",
        "Developing and Evaluating an AI-Assisted Prediction Model for Unplanned\n  Intensive Care Admissions following Elective Neurosurgery using Natural\n  Language Processing within an Electronic Healthcare Record System",
        "A Vehicle-Infrastructure Multi-layer Cooperative Decision-making\n  Framework",
        "Three-body structures of low-lying nuclear states of $^8$Li",
        "Nonequilibrium Continuous Transition in a Fast Rotating Turbulence",
        "Training Allostery-Inspired Mechanical Response in Disordered Elastic\n  Networks",
        "Spin glass behavior in amorphous CrSiTe3 alloy",
        "Representative dietary behavior patterns and associations with\n  cardiometabolic outcomes in Puerto Rico using a Bayesian latent class\n  analysis for non-probability samples",
        "On the coefficients of Tutte polynomials with one variable at 1",
        "Enhancing, Refining, and Fusing: Towards Robust Multi-Scale and Dense\n  Ship Detection",
        "Fluctuations in the email size modeled by a log-normal-like distribution",
        "Heterogeneity Matters even More in Distributed Learning: Study from\n  Generalization Perspective",
        "From Dense to Dynamic: Token-Difficulty Driven MoEfication of\n  Pre-Trained LLMs",
        "Differentiable Information Enhanced Model-Based Reinforcement Learning",
        "Underwater Soft Fin Flapping Motion with Deep Neural Network Based\n  Surrogate Model",
        "Automatic detection of single-electron regime of quantum dots and\n  definition of virtual gates using U-Net and clustering"
      ],
      "abstract":[
        "Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising\ndirection for integrating neural learning with symbolic reasoning. In the\nprobabilistic variant of such systems, a neural network first extracts a set of\nsymbols from sub-symbolic input, which are then used by a symbolic component to\nreason in a probabilistic manner towards answering a query. In this work, we\naddress the problem of formally verifying the robustness of such NeSy\nprobabilistic reasoning systems, therefore paving the way for their safe\ndeployment in critical domains. We analyze the complexity of solving this\nproblem exactly, and show that it is $\\mathrm{NP}^{\\# \\mathrm{P}}$-hard. To\novercome this issue, we propose the first approach for approximate,\nrelaxation-based verification of probabilistic NeSy systems. We demonstrate\nexperimentally that the proposed method scales exponentially better than\nsolver-based solutions and apply our technique to a real-world autonomous\ndriving dataset, where we verify a safety property under large input\ndimensionalities and network sizes.",
        "Recent advancements in Large Language Model(LLM)-based Multi-Agent\nSystems(MAS) have demonstrated remarkable potential for tackling complex\ndecision-making tasks. However, existing frameworks inevitably rely on\nserialized execution paradigms, where agents must complete sequential LLM\nplanning before taking action. This fundamental constraint severely limits\nreal-time responsiveness and adaptation, which is crucial in dynamic\nenvironments with ever-changing scenarios. In this paper, we propose a novel\nparallelized planning-acting framework for LLM-based MAS, featuring a\ndual-thread architecture with interruptible execution to enable concurrent\nplanning and acting. Specifically, our framework comprises two core threads:(1)\na planning thread driven by a centralized memory system, maintaining\nsynchronization of environmental states and agent communication to support\ndynamic decision-making; and (2) an acting thread equipped with a comprehensive\nskill library, enabling automated task execution through recursive\ndecomposition. Extensive experiments on challenging Minecraft demonstrate the\neffectiveness of the proposed framework.",
        "Evaluating AI safety requires statistically rigorous methods and risk metrics\nfor understanding how the use of AI affects aggregated risk. However, much AI\nsafety literature focuses upon risks arising from AI models in isolation,\nlacking consideration of how modular use of AI affects risk distribution of\nworkflow components or overall risk metrics. There is also a lack of\nstatistical grounding enabling sensitisation of risk models in the presence of\nabsence of AI to estimate causal contributions of AI. This is in part due to\nthe dearth of AI impact data upon which to fit distributions. In this work, we\naddress these gaps in two ways. First, we demonstrate how scenario modelling\n(grounded in established statistical techniques such as Markov chains, copulas\nand Monte Carlo simulation) can be used to model AI risk holistically. Second,\nwe show how lookalike distributions from phenomena analogous to AI can be used\nto estimate AI impacts in the absence of directly observable data. We\ndemonstrate the utility of our methods for benchmarking cumulative AI risk via\nrisk analysis of a logistic scenario simulations.",
        "AI agents hold the potential to transform everyday life by helping humans\nachieve their goals. To do this successfully, agents need to be able to\ncoordinate with novel partners without prior interaction, a setting known as\nzero-shot coordination (ZSC). Overcooked has become one of the most popular\nbenchmarks for evaluating coordination capabilities of AI agents and learning\nalgorithms. In this work, we investigate the origins of ZSC challenges in\nOvercooked. We introduce a state augmentation mechanism which mixes states that\nmight be encountered when paired with unknown partners into the training\ndistribution, reducing the out-of-distribution challenge associated with ZSC.\nWe show that independently trained agents under this algorithm coordinate\nsuccessfully in Overcooked. Our results suggest that ZSC failure can largely be\nattributed to poor state coverage under self-play rather than more\nsophisticated coordination challenges. The Overcooked environment is therefore\nnot suitable as a ZSC benchmark. To address these shortcomings, we introduce\nOvercookedV2, a new version of the benchmark, which includes asymmetric\ninformation and stochasticity, facilitating the creation of interesting ZSC\nscenarios. To validate OvercookedV2, we conduct experiments demonstrating that\nmere exhaustive state coverage is insufficient to coordinate well. Finally, we\nuse OvercookedV2 to build a new range of coordination challenges, including\nones that require test time protocol formation, and we demonstrate the need for\nnew coordination algorithms that can adapt online. We hope that OvercookedV2\nwill help benchmark the next generation of ZSC algorithms and advance\ncollaboration between AI agents and humans.",
        "Recent successes of artificial intelligence and deep learning often depend on\nthe well-collected training dataset which is assumed to have an identical\ndistribution with the test dataset. However, this assumption, which is called\nclosed-set learning, is hard to meet in realistic scenarios for deploying deep\nlearning models. As one of the solutions to mitigate this assumption, research\non out-of-distribution (OOD) detection has been actively explored in various\ndomains. In OOD detection, we assume that we are given the data of a new class\nthat was not seen in the training phase, i.e., outlier, at the evaluation\nphase. The ultimate goal of OOD detection is to detect and classify such unseen\noutlier data as a novel \"unknown\" class. Among various research branches for\nOOD detection, generating a virtual outlier during the training phase has been\nproposed. However, conventional generation-based methodologies utilize\nin-distribution training dataset to imitate outlier instances, which limits the\nquality of the synthesized virtual outlier instance itself. In this paper, we\npropose a novel methodology for OOD detection named Auxiliary Range Expansion\nfor Outlier Synthesis, or ARES. ARES models the region for generating\nout-of-distribution instances by escaping from the given in-distribution\nregion; instead of remaining near the boundary of in-distribution region.\nVarious stages consists ARES to ultimately generate valuable OOD-like virtual\ninstances. The energy score-based discriminator is then trained to effectively\nseparate in-distribution data and outlier data. Quantitative experiments on\nbroad settings show the improvement of performance by our method, and\nqualitative results provide logical explanations of the mechanism behind it.",
        "Traditional agentic workflows rely on external prompts to manage interactions\nwith tools and the environment, which limits the autonomy of reasoning models.\nWe position \\emph{Large Agent Models (LAMs)} that internalize the generation of\n\\emph{Chain-of-Action (CoA)}, enabling the model to autonomously decide when\nand how to use external tools. Our proposed AutoCoA framework combines\nsupervised fine-tuning (SFT) and reinforcement learning (RL), allowing the\nmodel to seamlessly switch between reasoning and action while efficiently\nmanaging environment interactions. Main components include step-level action\ntriggering, trajectory-level CoA optimization, and an internal world model to\nreduce real-environment interaction costs. Evaluations on open-domain QA tasks\ndemonstrate that AutoCoA-trained agent models significantly outperform\nReAct-based workflows in task completion, especially in tasks that require\nlong-term reasoning and multi-step actions. Code and dataset are available at\nhttps:\/\/github.com\/ADaM-BJTU\/AutoCoA",
        "While recent work has found that vision-language models trained under the\nContrastive Language Image Pre-training (CLIP) framework contain intrinsic\nsocial biases, the extent to which different upstream pre-training features of\nthe framework relate to these biases, and hence how intrinsic bias and\ndownstream performance are connected has been unclear. In this work, we present\nthe largest comprehensive analysis to-date of how the upstream pre-training\nfactors and downstream performance of CLIP models relate to their intrinsic\nbiases. Studying 131 unique CLIP models, trained on 26 datasets, using 55\narchitectures, and in a variety of sizes, we evaluate bias in each model using\n26 well-established unimodal and cross-modal principled Embedding Association\nTests. We find that the choice of pre-training dataset is the most significant\nupstream predictor of bias, whereas architectural variations have minimal\nimpact. Additionally, datasets curated using sophisticated filtering techniques\naimed at enhancing downstream model performance tend to be associated with\nhigher levels of intrinsic bias. Finally, we observe that intrinsic bias is\noften significantly correlated with downstream performance ($0.3 \\leq r \\leq\n0.8$), suggesting that models optimized for performance inadvertently learn to\namplify representational biases. Comparisons between unimodal and cross-modal\nassociation tests reveal that social group bias depends heavily on the\nmodality. Our findings imply that more sophisticated strategies are needed to\naddress intrinsic model bias for vision-language models across the entire model\ndevelopment pipeline.",
        "Recent methodologies utilizing synthetic datasets have aimed to address\ninconsistent hallucinations in large language models (LLMs); however,these\napproaches are primarily tailored to specific tasks, limiting their\ngeneralizability. Inspired by the strong performance of code-trained models in\nlogic-intensive domains, we propose a novel framework that leverages\nevent-based text to generate corresponding code and employs cyclic training to\ntransfer the logical consistency of code to natural language effectively. Our\nmethod significantly reduces inconsistent hallucinations across three leading\nLLMs and two categories of natural language tasks while maintaining overall\nperformance. This framework effectively alleviates hallucinations without\nnecessitating adaptation to downstream tasks, demonstrating generality and\nproviding new perspectives to tackle the challenge of inconsistent\nhallucinations.",
        "Recent Large Language Models (LLMs) such as OpenAI o3-mini and DeepSeek-R1\nuse enhanced reasoning through Chain-of-Thought (CoT). Their potential in\nhardware design, which relies on expert-driven iterative optimization, remains\nunexplored. This paper investigates whether reasoning LLMs can address\nchallenges in High-Level Synthesis (HLS) design space exploration and\noptimization. During HLS, engineers manually define pragmas\/directives to\nbalance performance and resource constraints. We propose an LLM-based\noptimization agentic framework that automatically restructures code, inserts\npragmas, and identifies optimal design points via feedback from HLs tools and\naccess to integer-linear programming (ILP) solvers. Experiments compare\nreasoning models against conventional LLMs on benchmarks using success rate,\nefficiency, and design quality (area\/latency) metrics, and provide the\nfirst-ever glimpse into the CoTs produced by a powerful open-source reasoning\nmodel like DeepSeek-R1.",
        "Data imbalance is a common issue in analyzing and predicting sudden traffic\nevents. Secondary crashes constitute only a small proportion of all crashes.\nThese secondary crashes, triggered by primary crashes, significantly exacerbate\ntraffic congestion and increase the severity of incidents. However, the severe\nimbalance of secondary crash data poses significant challenges for prediction\nmodels, affecting their generalization ability and prediction accuracy.\nExisting methods fail to fully address the complexity of traffic crash data,\nparticularly the coexistence of dynamic and static features, and often struggle\nto effectively handle data samples of varying lengths. Furthermore, most\ncurrent studies predict the occurrence probability and spatiotemporal\ndistribution of secondary crashes separately, lacking an integrated solution.\nTo address these challenges, this study proposes a hybrid model named\nVarFusiGAN-Transformer, aimed at improving the fidelity of secondary crash data\ngeneration and jointly predicting the occurrence and spatiotemporal\ndistribution of secondary crashes. The VarFusiGAN-Transformer model employs\nLong Short-Term Memory (LSTM) networks to enhance the generation of\nmultivariate long-time series data, incorporating a static data generator and\nan auxiliary discriminator to model the joint distribution of dynamic and\nstatic features. In addition, the model's prediction module achieves\nsimultaneous prediction of both the occurrence and spatiotemporal distribution\nof secondary crashes. Compared to existing methods, the proposed model\ndemonstrates superior performance in generating high-fidelity data and\nimproving prediction accuracy.",
        "Recently, enhancing the numerical and logical reasoning capability of Large\nLanguage Models (LLMs) has emerged as a research hotspot. Existing methods face\nseveral limitations: inference-phase techniques (e.g., Chain of Thoughts) rely\non prompt selection and the pretrained knowledge; sentence-level Supervised\nFine-Tuning (SFT) and Direct Preference Optimization (DPO) struggle with\nstep-wise mathematical correctness and depend on stronger models distillation\nor human annotations; while Reinforcement Learning (RL) approaches incur high\nGPU memory costs and unstable training. To address these, we propose\n\\textbf{S}elf-training framework integrating \\textbf{P}rocess\n\\textbf{P}reference learning using \\textbf{D}ynamic value margin (SPPD). SPPD\nleverages a process-based Markov Decision Process (MDP) and Bellman optimality\nequation to derive \\textbf{dynamic value margin} on step-level preference\noptimization, which employs tree-based self-sampling on model responses\n\\textbf{without any distillation} from other models. Furthermore, we\ntheoretically prove that SPPD is \\textbf{equivalent to on-policy policy\ngradient methods} under reward constraints. Experiments on 7B-scale models\ndemonstrate superior performance across in-domain and out-domain mathematical\nbenchmarks. We open-source our code at\n\\href{https:\/\/anonymous.4open.science\/r\/SSDPO-D-DCDD}{https:\/\/anonymous.4open.science\/r\/SPPD-DCDD}.",
        "The output of Large Language Models (LLMs) are a function of the internal\nmodel's parameters and the input provided into the context window. The\nhypothesis presented here is that under a greedy sampling strategy the variance\nin the LLM's output is a function of the conceptual certainty embedded in the\nmodel's parametric knowledge, as well as the lexical variance in the input.\nFinetuning the model results in reducing the sensitivity of the model output to\nthe lexical input variations. This is then applied to a classification problem\nand a probabilistic method is proposed for estimating the certainties of the\npredicted classes.",
        "Multi-agent collaborative perception is expected to significantly improve\nperception performance by overcoming the limitations of single-agent perception\nthrough exchanging complementary information. However, training a robust\ncollaborative perception model requires collecting sufficient training data\nthat covers all possible collaboration scenarios, which is impractical due to\nintolerable deployment costs. Hence, the trained model is not robust against\nnew traffic scenarios with inconsistent data distribution and fundamentally\nrestricts its real-world applicability. Further, existing methods, such as\ndomain adaptation, have mitigated this issue by exposing the deployment data\nduring the training stage but incur a high training cost, which is infeasible\nfor resource-constrained agents. In this paper, we propose a\nParameter-Efficient Fine-Tuning-based lightweight framework, CoPEFT, for fast\nadapting a trained collaborative perception model to new deployment\nenvironments under low-cost conditions. CoPEFT develops a Collaboration Adapter\nand Agent Prompt to perform macro-level and micro-level adaptations separately.\nSpecifically, the Collaboration Adapter utilizes the inherent knowledge from\ntraining data and limited deployment data to adapt the feature map to new data\ndistribution. The Agent Prompt further enhances the Collaboration Adapter by\ninserting fine-grained contextual information about the environment. Extensive\nexperiments demonstrate that our CoPEFT surpasses existing methods with less\nthan 1\\% trainable parameters, proving the effectiveness and efficiency of our\nproposed method.",
        "Introduction: Timely care in a specialised neuro-intensive therapy unit (ITU)\nreduces mortality and hospital stays, with planned admissions being safer than\nunplanned ones. However, post-operative care decisions remain subjective. This\nstudy used artificial intelligence (AI), specifically natural language\nprocessing (NLP) to analyse electronic health records (EHRs) and predict ITU\nadmissions for elective surgery patients. Methods: This study analysed the EHRs\nof elective neurosurgery patients from University College London Hospital\n(UCLH) using NLP. Patients were categorised into planned high dependency unit\n(HDU) or ITU admission; unplanned HDU or ITU admission; or ward \/ overnight\nrecovery (ONR). The Medical Concept Annotation Tool (MedCAT) was used to\nidentify SNOMED-CT concepts within the clinical notes. We then explored the\nutility of these identified concepts for a range of AI algorithms trained to\npredict ITU admission. Results: The CogStack-MedCAT NLP model, initially\ntrained on hospital-wide EHRs, underwent two refinements: first with data from\npatients with Normal Pressure Hydrocephalus (NPH) and then with data from\nVestibular Schwannoma (VS) patients, achieving a concept detection F1-score of\n0.93. This refined model was then used to extract concepts from EHR notes of\n2,268 eligible neurosurgical patients. We integrated the extracted concepts\ninto AI models, including a decision tree model and a neural time-series model.\nUsing the simpler decision tree model, we achieved a recall of 0.87 (CI 0.82 -\n0.91) for ITU admissions, reducing the proportion of unplanned ITU cases missed\nby human experts from 36% to 4%. Conclusion: The NLP model, refined for\naccuracy, has proven its efficiency in extracting relevant concepts, providing\na reliable basis for predictive AI models to use in clinically valid\napplications.",
        "Autonomous driving has entered the testing phase, but due to the limited\ndecision-making capabilities of individual vehicle algorithms, safety and\nefficiency issues have become more apparent in complex scenarios. With the\nadvancement of connected communication technologies, autonomous vehicles\nequipped with connectivity can leverage vehicle-to-vehicle (V2V) and\nvehicle-to-infrastructure (V2I) communications, offering a potential solution\nto the decision-making challenges from individual vehicle's perspective. We\npropose a multi-level vehicle-infrastructure cooperative decision-making\nframework for complex conflict scenarios at unsignalized intersections. First,\nbased on vehicle states, we define a method for quantifying vehicle impacts and\ntheir propagation relationships, using accumulated impact to group vehicles\nthrough motif-based graph clustering. Next, within and between vehicle groups,\na pass order negotiation process based on Large Language Models (LLM) is\nemployed to determine the vehicle passage order, resulting in planned vehicle\nactions. Simulation results from ablation experiments show that our approach\nreduces negotiation complexity and ensures safer, more efficient vehicle\npassage at intersections, aligning with natural decision-making logic.",
        "The four nucleons in $^8$Li outside the $\\alpha$-particle ($\\alpha=^4$He) can\nbe divided into pairs of one neutron ($n$) and 3 nucleons in the triton\n($t=^3$H), or 2 in the deuteron ($d=^2$H) and two neutrons in a dineutron\n($^2n$). The corresponding three-body structures, $\\alpha$+$t$+$n$ or\n$\\alpha$+$d$+$^2n$, are suggested to describe the bulk part of the low-energy\n($<10$~MeV) states of $^8$Li. Several breakup thresholds influence the\nstructures and possible decays. We calculate the three-body structures of the\nvarious $J^{\\pi}$ states, where different clustering appear, e.g. $^7$Li*+$n$,\n$^6$Li*$+^2n$, $^6$He*$+d$. The experimental $^8$Li spectrum can be reproduced\nwith fine tuning by a three-body potential parameter. Three unobserved $0^+$\nand an excited 2$^+$ states are found. All states appear as bound states or\nresonances. The lowest or highest energies have cluster structures,\n$\\alpha$+$t$+$n$ or $\\alpha$+$d$+$^2n$, respectively. We give calculated energy\nand width (if possible), geometry, and partial wave decomposition for all\nstates.",
        "We study the saturation of three-dimensional unstable perturbations on a fast\nrotating turbulent flow using direct numerical simulations (DNSs). Under the\neffect of Kolmogorov forcing, a transition between states dominated by coherent\ntwo-dimensional modes to states with three-dimensional variations\n(quasi-two-dimensional) is observed as we change the global rotation rate. We\nfind this akin to a critical phenomenon, wherein the order parameter scales\nwith the distance to the critical point raised to an exponent. The exponent\nitself deviates from the predicted mean field value. Also, the nature of the\nfluctuations of the order parameter near the critical point indicate the\npresence of on-off intermittency. The critical rotation rate at which the\ntransition occurs exhibits a linear scaling behaviour with the forcing wave\nnumber. A reduced model based on linear stability analysis is used to find the\nlinear threshold estimates; we find these to be in good agreement with the 3D\nnonlinear DNS results.",
        "Disordered elastic networks are a model material system in which it is\npossible to achieve tunable and trainable functions. This work investigates the\nmodification of local mechanical properties in disordered networks inspired by\nallosteric interactions in proteins: applying strain locally to a set of source\nnodes triggers a strain response at a distant set of target nodes. This is\ndemonstrated first by using directed aging to modify the existing mechanical\ncoupling between pairs of distant source and target nodes, and later as a means\nfor inducing coupling between formerly isolated source-target pairs. The\nexperimental results are compared with those predicted by simulations.",
        "Owing to the intrinsically high crystallization temperatures, layered\nphase-change materials, such as CrGeTe3 and InGeTe3, are attracting attention\nfor embedded memory applications, In addition to the electrical contrast, a\nmajor change in magnetic properties is observed in CrGeTe3 upon switching from\nthe crystalline to the amorphous state. In this work, we report a combined ab\ninitio modeling and magnetic characterization study on the isostructural\nsilicon parent compound of CrGeTe3, namely, CrSiTe3. Amorphous CrSiTe3 has\nsimilar structural properties to amorphous CrGeTe3; however, it shows a smaller\nenergy difference between the ferromagnetic configuration and the random\nmagnetic configuration, indicating a high probability of spin glass formation.\nIndeed, direct-current and alternating-current magnetic measurements show that\nthe coercive force of amorphous CrSiTe3 is higher than that of amorphous\nCrGeTe3. Therefore, the pinning effect of spins is enhanced in amorphous\nCrSiTe3, leading to a more robust spin glass state with a higher freezing\ntemperature. The large magnetic contrast between the amorphous and crystalline\nphase could make CrSiTe3 a potential candidate for phase-change magnetic\nswitching applications.",
        "There is limited understanding of how dietary behaviors cluster together and\ninfluence cardiometabolic health at a population level in Puerto Rico. Data\navailability is scarce, particularly outside of urban areas, and is often\nlimited to non-probability sample (NPS) data where sample inclusion mechanisms\nare unknown. In order to generalize results to the broader Puerto Rican\npopulation, adjustments are necessary to account for selection bias but are\ndifficult to implement for NPS data. Although Bayesian latent class models\nenable summaries of dietary behavior variables through underlying patterns,\nthey have not yet been adapted to the NPS setting. We propose a novel Weighted\nOverfitted Latent Class Analysis for Non-probability samples (WOLCAN). WOLCAN\nutilizes a quasi-randomization framework to (1) model pseudo-weights for an NPS\nusing Bayesian additive regression trees (BART) and a reference probability\nsample, and (2) integrate the pseudo-weights within a weighted\npseudo-likelihood approach for Bayesian latent class analysis, while\npropagating pseudo-weight uncertainty into parameter estimation. A stacked\nsample approach is used to allow shared individuals between the NPS and the\nreference sample. We evaluate model performance through simulations and apply\nWOLCAN to data from the Puerto Rico Observational Study of Psychosocial,\nEnvironmental, and Chronic Disease Trends (PROSPECT). We identify dietary\nbehavior patterns for adults in Puerto Rico aged 30 to 75 and examine their\nassociations with type 2 diabetes, hypertension, and hypercholesterolemia. Our\nfindings suggest that an out-of-home eating pattern is associated with a higher\nlikelihood of these cardiometabolic outcomes compared to a nutrition-sensitive\npattern. WOLCAN effectively reveals generalizable dietary behavior patterns and\ndemonstrates relevant applications in studying diet-disease relationships.",
        "Denote the Tutte polynomial of a graph $G$ and a matroid $M$ by $T_G(x,y)$\nand $T_M(x,y)$ respectively. $T_G(x,1)$ and $T_G(1,y)$ were generalized to\nhypergraphs and further extended to integer polymatroids by K\\'{a}lm\\'{a}n\n\\cite{Kalman} in 2013, called interior and exterior polynomials respectively.\nLet $G$ be a $(k+1)$-edge connected graph of order $n$ and size $m$, and let\n$g=m-n+1$. Guan et al. (2023) \\cite{Guan} obtained the coefficients of\n$T_G(1,y)$: \\[[y^j]T_G(1,y)=\\binom{m-j-1}{n-2} \\text{ for } g-k\\leq j\\leq g,\\]\nwhich was deduced from coefficients of the exterior polynomial of polymatroids.\nRecently, Chen and Guo (2025) \\cite{Chen} further obtained\n\\[[y^j]T_G(1,y)=\\binom{m-j-1}{n-2}-\\sum_{i=k+1}^{g-j}\\binom{m-j-i-1}{n-2}|\\mathcal{EC}_i(G)|\\]\nfor $g-3(k+1)\/2< j\\leq g$, where $\\mathcal{EC}_i(G)$ denotes the set of all\nminimal edge cuts with $i$ edges. In this paper, for any matroid $M=(X,rk)$ we\nfirst obtain\n\\[[y^j]T_M(1,y)=\\sum_{t=j}^{|X|-r}(-1)^{t-j}\\binom{t}{j}\\sigma_{r+t}(M),\\]\nwhere $\\sigma_{r+t}(M)$ denotes the number of spanning sets with $r+t$ elements\nin $M$ and $r=rk(M)$. Moveover, the expression of $[x^i]T_M(x,1)$ is obtained\nimmediately from the duality of the Tutte polynomial. As applications of our\nresults, we generalize the two aforementioned results on graphs to the setting\nof matroids. This not only resolves two open problems posed by Chen and Guo in\n\\cite{Chen} but also provides a purely combinatorial proof that is\nsignificantly simpler than their original proofs.",
        "Synthetic aperture radar (SAR) imaging, celebrated for its high resolution,\nall-weather capability, and day-night operability, is indispensable for\nmaritime applications. However, ship detection in SAR imagery faces significant\nchallenges, including complex backgrounds, densely arranged targets, and large\nscale variations. To address these issues, we propose a novel framework,\nCenter-Aware SAR Ship Detector (CASS-Det), designed for robust multi-scale and\ndensely packed ship detection. CASS-Det integrates three key innovations: (1) a\ncenter enhancement module (CEM) that employs rotational convolution to\nemphasize ship centers, improving localization while suppressing background\ninterference; (2) a neighbor attention module (NAM) that leverages cross-layer\ndependencies to refine ship boundaries in densely populated scenes; and (3) a\ncross-connected feature pyramid network (CC-FPN) that enhances multi-scale\nfeature fusion by integrating shallow and deep features. Extensive experiments\non the SSDD, HRSID, and LS-SSDD-v1.0 datasets demonstrate the state-of-the-art\nperformance of CASS-Det, excelling at detecting multi-scale and densely\narranged ships.",
        "A previously established frequency distribution model combining a log-normal\ndistribution with a logarithmic equation describes fluctuations in the email\nsize during send requests. Although the frequency distribution fit was\nconsidered satisfactory, the underlying mechanism driving this distribution\nremains inadequately explained. To address this gap, this study introduced a\nnovel email-send model that characterizes the sending process as an exponential\nfunction modulated by noise from a normal distribution. This model is\nconsistent with both the observed frequency distribution and the previously\nproposed frequency distribution model.",
        "In this paper, we investigate the effect of data heterogeneity across clients\non the performance of distributed learning systems, i.e., one-round Federated\nLearning, as measured by the associated generalization error. Specifically,\n\\(K\\) clients have each \\(n\\) training samples generated independently\naccording to a possibly different data distribution and their individually\nchosen models are aggregated by a central server. We study the effect of the\ndiscrepancy between the clients' data distributions on the generalization error\nof the aggregated model. First, we establish in-expectation and tail upper\nbounds on the generalization error in terms of the distributions. In part, the\nbounds extend the popular Conditional Mutual Information (CMI) bound which was\ndeveloped for the centralized learning setting, i.e., \\(K=1\\), to the\ndistributed learning setting with arbitrary number of clients $K \\geq 1$. Then,\nwe use a connection with information theoretic rate-distortion theory to derive\npossibly tighter \\textit{lossy} versions of these bounds. Next, we apply our\nlossy bounds to study the effect of data heterogeneity across clients on the\ngeneralization error for distributed classification problem in which each\nclient uses Support Vector Machines (D-SVM). In this case, we establish\nexplicit generalization error bounds which depend explicitly on the data\nheterogeneity degree. It is shown that the bound gets smaller as the degree of\ndata heterogeneity across clients gets higher, thereby suggesting that D-SVM\ngeneralizes better when the dissimilarity between the clients' training samples\nis bigger. This finding, which goes beyond D-SVM, is validated experimentally\nthrough a number of experiments.",
        "Training large language models (LLMs) for different inference constraints is\ncomputationally expensive, limiting control over efficiency-accuracy\ntrade-offs. Moreover, once trained, these models typically process tokens\nuniformly, regardless of their complexity, leading to static and inflexible\nbehavior. In this paper, we introduce a post-training optimization framework,\nDynaMoE, that adapts a pre-trained dense LLM to a token-difficulty-driven\nMixture-of-Experts model with minimal fine-tuning cost. This adaptation makes\nthe model dynamic, with sensitivity control to customize the balance between\nefficiency and accuracy. DynaMoE features a token-difficulty-aware router that\npredicts the difficulty of tokens and directs them to the appropriate\nsub-networks or experts, enabling larger experts to handle more complex tokens\nand smaller experts to process simpler ones. Our experiments demonstrate that\nDynaMoE can generate a range of adaptive model variants of the existing trained\nLLM with a single fine-tuning step, utilizing only $10B$ tokens, a minimal cost\ncompared to the base model's training. Each variant offers distinct trade-offs\nbetween accuracy and performance. Compared to the baseline post-training\noptimization framework, Flextron, our method achieves similar aggregated\naccuracy across downstream tasks, despite using only $\\frac{1}{9}\\text{th}$ of\ntheir fine-tuning cost.",
        "Differentiable environments have heralded new possibilities for learning\ncontrol policies by offering rich differentiable information that facilitates\ngradient-based methods. In comparison to prevailing model-free reinforcement\nlearning approaches, model-based reinforcement learning (MBRL) methods exhibit\nthe potential to effectively harness the power of differentiable information\nfor recovering the underlying physical dynamics. However, this presents two\nprimary challenges: effectively utilizing differentiable information to 1)\nconstruct models with more accurate dynamic prediction and 2) enhance the\nstability of policy training. In this paper, we propose a Differentiable\nInformation Enhanced MBRL method, MB-MIX, to address both challenges. Firstly,\nwe adopt a Sobolev model training approach that penalizes incorrect model\ngradient outputs, enhancing prediction accuracy and yielding more precise\nmodels that faithfully capture system dynamics. Secondly, we introduce mixing\nlengths of truncated learning windows to reduce the variance in policy gradient\nestimation, resulting in improved stability during policy learning. To validate\nthe effectiveness of our approach in differentiable environments, we provide\ntheoretical analysis and empirical results. Notably, our approach outperforms\nprevious model-based and model-free methods, in multiple challenging tasks\ninvolving controllable rigid robots such as humanoid robots' motion control and\ndeformable object manipulation.",
        "This study presents a novel framework for precise force control of\nfin-actuated underwater robots by integrating a deep neural network (DNN)-based\nsurrogate model with reinforcement learning (RL). To address the complex\ninteractions with the underwater environment and the high experimental costs, a\nDNN surrogate model acts as a simulator for enabling efficient training for the\nRL agent. Additionally, grid-switching control is applied to select optimized\nmodels for specific force reference ranges, improving control accuracy and\nstability. Experimental results show that the RL agent, trained in the\nsurrogate simulation, generates complex thrust motions and achieves precise\ncontrol of a real soft fin actuator. This approach provides an efficient\ncontrol solution for fin-actuated robots in challenging underwater\nenvironments.",
        "To realize practical quantum computers, a large number of quantum bits\n(qubits) will be required. Semiconductor spin qubits offer advantages such as\nhigh scalability and compatibility with existing semiconductor technologies.\nHowever, as the number of qubits increases, manual qubit tuning becomes\ninfeasible, motivating automated tuning approaches. In this study, we use\nU-Net, a neural network method for object detection, to identify charge\ntransition lines in experimental charge stability diagrams. The extracted\ncharge transition lines are analyzed using the Hough transform to determine\ntheir positions and angles. Based on this analysis, we obtain the\ntransformation matrix to virtual gates. Furthermore, we identify the\nsingle-electron regime by clustering the Hough transform outputs. We also show\nthe single-electron regime within the virtual gate space. These sequential\nprocesses are performed automatically. This approach will advance automated\ncontrol technologies for large-scale quantum devices."
      ]
    }
  },
  {
    "id":2411.01291,
    "research_type":"applied",
    "start_id":"b19",
    "start_title":"vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems",
    "start_abstract":"Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI"
      ],
      "abstract":[
        "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "An Efficient Algorithm for Determining the Equivalence of Zero-one\n  Reaction Networks",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "$\\tt GrayHawk$: A public code for calculating the Gray Body Factors of\n  massless fields around spherically symmetric Black Holes",
        "MinGRU-Based Encoder for Turbo Autoencoder Frameworks",
        "Type semigroups for twisted groupoids and a dichotomy for groupoid\n  C*-algebras",
        "Algebraic surfaces as Hadamard products of curves",
        "$C^{1}$-Stable-Manifolds for Periodic Heteroclinic Chains in Bianchi IX:\n  Symbolic Computations and Statistical Properties",
        "Slow Light Waveguides based on Bound States in the Continuum",
        "High-Accuracy Physical Property Prediction for Organics via Molecular\n  Representation Learning: Bridging Data to Discovery",
        "Identical Suppression of Spin and Charge Density Wave Transitions in\n  La$_4$Ni$_3$O$_{10}$ by Pressure",
        "A Relaxed Wasserstein Distance Formulation for Mixtures of Radially\n  Contoured Distributions",
        "Weight Distribution of the Weighted Coordinates Poset Block Space and\n  Singleton Bound",
        "Cold dark gas in Cygnus X: The first large-scale mapping of\n  low-frequency carbon recombination lines",
        "Properties of Turnpike Functions for Discounted Finite MDPs",
        "Raman Forbidden Layer-Breathing Modes in Layered Semiconductor Materials\n  Activated by Phonon and Optical Cavity Effects",
        "Anomalous temperature-dependent magnetization in the nearly collinear\n  antiferromagnet Y$_2$Co$_3$",
        "The nebular spectra of SN 2023ixf: A lower mass, partially stripped\n  progenitor may be the result of binary interaction"
      ],
      "abstract":[
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "Zero-one reaction networks play a crucial role in cell signaling. Determining\nthe equivalence of reaction networks is a fundamental computational problem in\nthe field of chemical reaction networks. In this work, we develop an efficient\nmethod for determining the equivalence of zero-one networks. The efficiency\ncomes from several criteria for determining the equivalence of the steady-state\nideals arising from zero-one networks, which helps for cutting down the\nexpenses on computing Grobner bases. Experiments show that our method can\nsuccessfully classify over three million networks according to their\nequivalence in a reasonable time.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "We introduce and describe $\\tt GrayHawk$, a publicly available\nMathematica-based tool designed for the efficient computation of gray-body\nfactors for spherically symmetric and asymptotically flat black holes. This\nprogram provides users with a rapid and reliable means to compute gray-body\nfactors for massless fields with spin \\(s = 0, 1\/2, 1, 2\\) in modes specified\nby the angular quantum number \\(l\\), given a black hole metric and the\nassociated parameter values. $\\tt GrayHawk$ is preloaded with seven different\nblack hole metrics, offering immediate applicability to a variety of\ntheoretical models. Additionally, its modular structure allows users to extend\nits functionality easily by incorporating alternative metrics or\nconfigurations. This versatility makes $\\tt GrayHawk$ a powerful and adaptable\nresource for researchers studying black hole physics and Hawking radiation. The\ncodes described in this work are publicly available at\nhttps:\/\/github.com\/marcocalza89\/GrayHawk.",
        "Early neural channel coding approaches leveraged dense neural networks with\none-hot encodings to design adaptive encoder-decoder pairs, improving block\nerror rate (BLER) and automating the design process. However, these methods\nstruggled with scalability as the size of message sets and block lengths\nincreased. TurboAE addressed this challenge by focusing on bit-sequence inputs\nrather than symbol-level representations, transforming the scalability issue\nassociated with large message sets into a sequence modeling problem. While\nrecurrent neural networks (RNNs) were a natural fit for sequence processing,\ntheir reliance on sequential computations made them computationally expensive\nand inefficient for long sequences. As a result, TurboAE adopted convolutional\nnetwork blocks, which were faster to train and more scalable, but lacked the\nsequential modeling advantages of RNNs. Recent advances in efficient RNN\narchitectures, such as minGRU and minLSTM, and structured state space models\n(SSMs) like S4 and S6, overcome these limitations by significantly reducing\nmemory and computational overhead. These models enable scalable sequence\nprocessing, making RNNs competitive for long-sequence tasks. In this work, we\nrevisit RNNs for Turbo autoencoders by integrating the lightweight minGRU model\nwith a Mamba block from SSMs into a parallel Turbo autoencoder framework. Our\nresults demonstrate that this hybrid design matches the performance of\nconvolutional network-based Turbo autoencoder approaches for short sequences\nwhile significantly improving scalability and training efficiency for long\nblock lengths. This highlights the potential of efficient RNNs in advancing\nneural channel coding for long-sequence scenarios.",
        "We develop a theory of type semigroups for arbitrary twisted, not necessarily\nHausdorff \\'etale groupoids. The type semigroup is a dynamical version of the\nCuntz semigroup. We relate it to traces, ideals, pure infiniteness, and stable\nfiniteness of the reduced and essential C*-algebras. If the reduced C*-algebra\nof a twisted groupoid is simple and the type semigroup satisfies a weak version\nof almost unperforation, then the C*-algebra is either stably finite or purely\ninfinite. We apply our theory to Cartan inclusions. We calculate the type\nsemigroup for the possibly non-Hausdorff groupoids associated to self-similar\ngroup actions on graphs and deduce a dichotomy for the resulting Exel-Pardo\nalgebras.",
        "We study projective surfaces in $\\mathbb{P}^3$ which can be written as\nHadamard product of two curves. We show that quadratic surfaces which are\nHadamard product of two lines are smooth and tangent to all coordinate planes,\nand such tangency points uniquely identify the quadric. The variety of such\nquadratic surfaces corresponds to the Zariski closure of the space of symmetric\nmatrices whose inverse has null diagonal. For higher-degree surfaces which are\nHadamard product of a line and a curve we show that the intersection with the\ncoordinate planes is always non-transversal.",
        "In this paper we study oscillatory Bianchi models of class A and are able to\nshow that for admissible periodic heteroclinic chains in Bianchi IX there\nexisist $C^{1}$- stable - manifolds of orbits that follow these chains towards\nthe big bang. A detailed study of Takens Linearization Theorem and the\nNon-Resonance-Conditions leads us to this new result in Bianchi class A. More\nprecisely, we can show that there are no heteroclinic chains in Bianchi IX with\nconstant continued fraction development that allow Takens-Linearization at all\nof their base points. Geometrically speaking, this excludes \"symmetric\"\nheteroclinic chains with the same number of \"bounces\" near all of the 3 Taub\nPoints - the result shows that we have to require some \"asymmetry\" in the\nbounces in order to allow for Takens Linearization, e.g. by considering\nadmissible 2-periodic continued fraction developments. We conclude by\ndiscussing the statistical properties of those solutions, including their\ntopological and measure-theoretic genericity.",
        "The concept of bound states in the continuum (BIC) has been advancing light\nconfinement technology in leaky environments. In this letter, we propose and\nnumerically demonstrate a slow light waveguide based on a BIC mode. We\nconsidered a waveguide with a polymer core loaded on a plane slab, which\nsupports a leaky guided mode coupled to the radiation continuum in the slab. We\nfound that periodic modulation of the polymer core along the propagation\ndirection can result in a high group index mode with a low propagation loss due\nto BIC confinement. The introduction of one-dimensional photonic crystals into\nthe BIC waveguides will largely expand its functionality and applications in\nintegrated photonics.",
        "The ongoing energy crisis has underscored the urgent need for\nenergy-efficient materials with high energy utilization efficiency, prompting a\nsurge in research into organic compounds due to their environmental\ncompatibility, cost-effective processing, and versatile modifiability. To\naddress the high experimental costs and time-consuming nature of traditional\ntrial-and-error methods in the discovery of highly functional organic\ncompounds, we apply the 3D transformer-based molecular representation learning\nalgorithm to construct a pre-trained model using 60 million semi-empirically\noptimized structures of small organic molecules, namely, Org-Mol, which is then\nfine-tuned with public experimental data to obtain prediction models for\nvarious physical properties. Despite the pre-training process relying solely on\nsingle molecular coordinates, the fine-tuned models achieves high accuracy\n(with $R^2$ values for the test set exceeding 0.95). These fine-tuned models\nare applied in a high-throughput screening process to identify novel immersion\ncoolants among millions of automatically constructed ester molecules, resulting\nin the experimental validation of two promising candidates. This work not only\ndemonstrates the potential of Org-Mol in predicting bulk properties for organic\ncompounds but also paves the way for the rational and efficient development of\nideal candidates for energy-saving materials.",
        "Understanding the interplay between magnetism and superconductivity in\nnickelate systems is a key focus of condensed matter research. Microscopic\ninsights into magnetism, which emerges near superconductivity, require a\nsynergistic approach that combines complementary techniques with controlled\nparameter tuning. In this paper, we present a systematic investigation of the\nthree-layer Ruddlesden-Popper (RP) nickelate La$_4$Ni$_3$O$_{10}$ using\nmuon-spin rotation\/relaxation ($\\mu$SR), neutron powder diffraction (NPD),\nresistivity, and specific heat measurements. At ambient pressure, two\nincommensurate spin density wave (SDW) transitions were identified at $T_{\\rm\nSDW} \\simeq 132$ K and $T^\\ast \\simeq 90$ K. NPD experiments revealed that the\nmagnetic wave vector $(0, 0.574, 0)$ remains unchanged below 130 K, indicating\nthat the transition at $T^\\ast$ corresponds to a reorientation of the Ni\nmagnetic moments within a similar magnetic structure. Comparison of the\nobserved internal magnetic fields with dipole-field calculations reveals a\nmagnetic structure consistent with an antiferromagnetically coupled SDW on the\nouter two Ni layers, with smaller moments on the inner Ni layer. The internal\nfields at muon stopping sites appeared abruptly at $T_{\\rm SDW}$, suggesting a\nfirst-order-like nature of the SDW transition, which is closely linked to the\ncharge density wave (CDW) order occurring at the same temperature ($T_{\\rm SDW}\n= T_{\\rm CDW}$). Under applied pressure, all transition temperatures, including\n$T_{\\rm SDW}$, $T^\\ast$, and $T_{\\rm CDW}$, were suppressed at a nearly uniform\nrate of $\\simeq -13$ K\/GPa. This behavior contrasts with the double-layer RP\nnickelate La$_3$Ni$_2$O$_7$, where pressure enhances the separation of the\ndensity wave transitions.",
        "Recently, a Wasserstein-type distance for Gaussian mixture models has been\nproposed. However, that framework can only be generalized to identifiable\nmixtures of general elliptically contoured distributions whose components come\nfrom the same family and satisfy marginal consistency. In this paper, we\npropose a simple relaxed Wasserstein distance for identifiable mixtures of\nradially contoured distributions whose components can come from different\nfamilies. We show some properties of this distance and that its definition does\nnot require marginal consistency. We apply this distance in color transfer\ntasks and compare its performance with the Wasserstein-type distance for\nGaussian mixture models in an experiment. The error of our method is more\nstable and the color distribution of our output image is more desirable.",
        "In this paper, we determine the complete weight distribution of the space $\n\\mathbb{F}_q^N $ endowed by the weighted coordinates poset block metric\n($(P,w,\\pi)$-metric), also known as the $(P,w,\\pi)$-space, thereby obtaining it\nfor $(P,w)$-space, $(P,\\pi)$-space, $\\pi$-space, and $P$-space as special\ncases. Further, when $P$ is a chain, the resulting space is called as\nNiederreiter-Rosenbloom-Tsfasman (NRT) weighted block space and when $P$ is\nhierarchical, the resulting space is called as weighted coordinates\nhierarchical poset block space. The complete weight distribution of both the\nspaces are deduced from the main result. Moreover, we define an $I$-ball for an\nideal $I$ in $P$ and study the characteristics of it in $(P,w,\\pi)$-space.\n  We investigate the relationship between the $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Given an ideal $I$, we investigate how the maximum\ndistance separability (MDS) is related with $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Duality theorem is derived for an MDS\n$(P,w,\\pi)$-code when all the blocks are of same length. Finally, the\ndistribution of codewords among $r$-balls is analyzed in the case of chain\nposet, when all the blocks are of same length.",
        "Understanding the transition from atomic gas to molecular gas is critical to\nexplain the formation and evolution of molecular clouds. However, the gas\nphases involved, cold HI and CO-dark molecular gas, are challenging to directly\nobserve and physically characterize. We observed the Cygnus X star-forming\ncomplex in carbon radio recombination lines (CRRLs) at 274--399 MHz with the\nGreen Bank Telescope at 21 pc (48') resolution. Of the 30 deg^2 surveyed, we\ndetect line-synthesized C273alpha emission from 24 deg^2 and produce the first\nlarge-area maps of low-frequency CRRLs. The morphology of the C273alpha\nemission reveals arcs, ridges, and extended possibly sheet-like gas which are\noften on the outskirts of CO emission and likely transitioning from HI-to-H_2.\nThe typical angular separation of C273alpha and 13CO emission is 12 pc, and we\nestimate C273alpha gas densities of n_H ~ 40 - 400 cm^3. The C273alpha line\nprofiles are Gaussian and likely turbulent broadened, spanning a large range of\nFWHM from 2 to 20 km\/s with a median of 10.6 km\/s. Mach numbers fall within\n10--30. The turbulent timescale is relatively short, 2.6 Myr, and we deduce\nthat the turbulent pressure likely dominates the evolution of the C273alpha\ngas. Velocity offsets between C273alpha and 13CO components are apparent\nthroughout the region and have a typical value of 2.9 km\/s. Two regimes have\nemerged from the data: one regime in which C273alpha and 13CO are strongly\nrelated (at N_H ~ 4 x 10^21 cm^-2), and a second, in which C273alpha emits\nindependently of the 13CO intensity. In the former regime, C273alpha may arise\nfrom the the envelopes of massive clouds (filaments), and in the latter,\nC273alpha emits from cold clumps in a more-diffuse mix of HI and H_2 gas.",
        "This paper studies discounted Markov Decision Processes (MDPs) with finite\nsets of states and actions. Value iteration is one of the major methods for\nfinding optimal policies. For each discount factor, starting from a finite\nnumber of iterations, which is called the turnpike integer, value iteration\nalgorithms always generate decision rules, which are deterministic optimal\npolicies for the infinite-horizon problems. This fact justifies the rolling\nhorizon approach for computing infinite-horizon optimal policies by conducting\na finite number of value iterations. This paper describes properties of\nturnpike integers and provides their upper bounds.",
        "We report Raman forbidden layer-breathing modes (LBMs) in layered\nsemiconductor materials (LSMs). The intensity distribution of all observed LBMs\ndepends on layer number, incident light wavelength and refractive index\nmismatch between LSM and underlying substrate. These results are understood by\na Raman scattering theory via the proposed spatial interference model, where\nthe naturally occurring optical and phonon cavities in LSMs enable spatially\ncoherent photon-phonon coupling mediated by the corresponding one-dimensional\nperiodic electronic states. Our work reveals the spatial coherence of photon\nand phonon fields on the phonon excitation via photon\/phonon cavity\nengineering.",
        "Y$_2$Co$_3$ is a newly discovered antiferromagnetic (AFM) compound with\ndistorted kagome layers. Previous investigations via bulk magnetization\nmeasurements suggested a complex noncollinear magnetic behavior, with magnetic\nmoments primarily anti-aligned along the $b$ axis and some canting towards the\n$ac$ plane. In this study, we report the magnetic structure of Y$_2$Co$_3$ to\nbe an A-type AFM structure with ferromagnetic (FM) interactions within the\ndistorted kagome plane and an interplane antiferromagnetic interaction, as\ndetermined by single-crystal neutron diffraction. The magnetic moments align\nalong the $b$ axis, with minimal canting towards the $c$ axis, at odds with the\nprevious interpretation of bulk magnetization measurements. The magnetic\nmoments on the two distinct Co sites are [0, -0.68(3), 0] $\\mu_B$ and [0,\n1.25(4), 0.07(1)] $\\mu_B$. We attribute the previously reported \"noncollinear\"\nbehavior to the considerable temperature dependence of itinerant AFM exchange\ninteractions, induced by thermal contraction along the $b$ axis. Additionally,\nour examination of lattice constants through pressure studies reveals\ncompensating effects on FM and AFM interactions, resulting in negligible\npressure dependence of $T_\\textrm{N}$.",
        "SN 2023ixf is one of the brightest Core Collapse Supernovae of the 21st\ncentury and offers a rare opportunity to investigate the late stage of a\nSupernova through nebular phase spectroscopy. We present four nebular phase\nspectra from day +291 to +413 after explosion. This is supplemented with high\ncadence early phase spectroscopic observations and photometry covering the\nfirst 500 days to investigate explosion parameters. The narrow and blue-shifted\nnebular Oxygen emission lines are used to infer an ejected Oxygen mass of\n$<0.65M_\\odot$, consistent with models of a relatively low mass\n($M_{ZAMS}<15M_\\odot$) progenitor. An energy of 0.3 to $1.4 \\times10^{51}$ erg\nand a light curve powered by an initial $^{56}$Ni mass of $0.049 \\pm 0.005\nM_\\odot$ appear consistent with a relatively standard Type II explosion, while\nan incomplete $\\gamma$-ray trapping (with timescale of $240\\pm4$ days) suggests\na lower ejecta mass. Assuming a typical explosion, the broad Hydrogen and\nCalcium profiles suggest a common origin within a lower mass, partially\nstripped envelope. Hydrogen emission broadens with time, indicating\ncontribution from an additional power source at an extended distance; while the\nemergence of high velocity ($\\sim$6,000 km s$^{-1}$) Hydrogen emission features\n(beginning around day +200) may be explained by Shock Interaction with a dense\nHydrogen-rich region located at $\\sim1.5 \\times 10^{16}$cm. Such envelope mass\nloss for a low mass progenitor may be explained through theoretical models of\nBinary interaction."
      ]
    }
  },
  {
    "id":2411.01758,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data",
    "start_abstract":"Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "A review on segmentation of positron emission tomography images"
      ],
      "abstract":[
        "Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Application of Artificial Intelligence (AI) in Civil Engineering",
        "MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation",
        "Reproducibility Study of Cooperation, Competition, and Maliciousness:\n  LLM-Stakeholders Interactive Negotiation",
        "AoI-Sensitive Data Forwarding with Distributed Beamforming in\n  UAV-Assisted IoT",
        "Interpretable Machine Learning for Oral Lesion Diagnosis through\n  Prototypical Instances Identification",
        "L2R: Learning to Reduce Search Space for Generalizable Neural Routing\n  Solver",
        "From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector\n  Graphics",
        "Talking like Piping and Instrumentation Diagrams (P&IDs)",
        "Imitation Learning of Correlated Policies in Stackelberg Games",
        "Why Do Multi-Agent LLM Systems Fail?",
        "Artificial Intelligence-Driven Clinical Decision Support Systems",
        "GDiffRetro: Retrosynthesis Prediction with Dual Graph Enhanced Molecular\n  Representation and Diffusion Generation",
        "SPRIG: Stackelberg Perception-Reinforcement Learning with Internal Game\n  Dynamics",
        "CoddLLM: Empowering Large Language Models for Data Analytics",
        "Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain\n  Generalization",
        "Towards a Study of Low Energy Antiproton Annihilations on Nuclei",
        "Knowledge Phenomenology Research of Future Industrial Iconic Product\n  Innovation",
        "The simplest solutions of cold plasma equations: change in properties\n  from a hydrodynamic to a kinetic model",
        "Position: Stop Acting Like Language Model Agents Are Normal Agents",
        "Optimizing Energy Efficiency in Subthreshold RISC-V Cores",
        "Non-archimedean integration on totally disconnected spaces",
        "Cohomology of classifying spaces of rank 3 Kac-Moody groups",
        "Optimization Methods for Joint Eigendecomposition",
        "Graph factors and powers of Hamilton cycles in the budget-constrained\n  random graph process",
        "Teaching Dense Retrieval Models to Specialize with Listwise Distillation\n  and LLM Data Augmentation",
        "Learning to be Smooth: An End-to-End Differentiable Particle Smoother",
        "Advanced Zero-Shot Text-to-Speech for Background Removal and\n  Preservation with Controllable Masked Speech Prediction",
        "A diagrammatic approach to the Rasmussen invariant via tangles and\n  cobordisms"
      ],
      "abstract":[
        "Hard computing generally deals with precise data, which provides ideal\nsolutions to problems. However, in the civil engineering field, amongst other\ndisciplines, that is not always the case as real-world systems are continuously\nchanging. Here lies the need to explore soft computing methods and artificial\nintelligence to solve civil engineering shortcomings. The integration of\nadvanced computational models, including Artificial Neural Networks (ANNs),\nFuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, has\nrevolutionized the domain of civil engineering. These models have significantly\nadvanced diverse sub-fields by offering innovative solutions and improved\nanalysis capabilities. Sub-fields such as: slope stability analysis, bearing\ncapacity, water quality and treatment, transportation systems, air quality,\nstructural materials, etc. ANNs predict non-linearities and provide accurate\nestimates. Fuzzy logic uses an efficient decision-making process to provide a\nmore precise assessment of systems. Lastly, while GAs optimizes models (based\non evolutionary processes) for better outcomes, probabilistic reasoning lowers\ntheir statistical uncertainties.",
        "The growing demand for efficient and lightweight Retrieval-Augmented\nGeneration (RAG) systems has highlighted significant challenges when deploying\nSmall Language Models (SLMs) in existing RAG frameworks. Current approaches\nface severe performance degradation due to SLMs' limited semantic understanding\nand text processing capabilities, creating barriers for widespread adoption in\nresource-constrained scenarios. To address these fundamental limitations, we\npresent MiniRAG, a novel RAG system designed for extreme simplicity and\nefficiency. MiniRAG introduces two key technical innovations: (1) a\nsemantic-aware heterogeneous graph indexing mechanism that combines text chunks\nand named entities in a unified structure, reducing reliance on complex\nsemantic understanding, and (2) a lightweight topology-enhanced retrieval\napproach that leverages graph structures for efficient knowledge discovery\nwithout requiring advanced language capabilities. Our extensive experiments\ndemonstrate that MiniRAG achieves comparable performance to LLM-based methods\neven when using SLMs while requiring only 25\\% of the storage space.\nAdditionally, we contribute a comprehensive benchmark dataset for evaluating\nlightweight RAG systems under realistic on-device scenarios with complex\nqueries. We fully open-source our implementation and datasets at:\nhttps:\/\/github.com\/HKUDS\/MiniRAG.",
        "This paper presents a reproducibility study and extension of \"Cooperation,\nCompetition, and Maliciousness: LLM-Stakeholders Interactive Negotiation.\" We\nvalidate the original findings using a range of open-weight models (1.5B-70B\nparameters) and GPT-4o Mini while introducing several novel contributions. We\nanalyze the Pareto front of the games, propose a communication-free baseline to\ntest whether successful negotiations are possible without agent interaction,\nevaluate recent small language models' performance, analyze structural\ninformation leakage in model responses, and implement an inequality metric to\nassess negotiation fairness. Our results demonstrate that smaller models (<10B\nparameters) struggle with format adherence and coherent responses, but larger\nopen-weight models can approach proprietary model performance. Additionally, in\nmany scenarios, single-agent approaches can achieve comparable results to\nmulti-agent negotiations, challenging assumptions about the necessity of agent\ncommunication to perform well on the benchmark. This work also provides\ninsights into the accessibility, fairness, environmental impact, and privacy\nconsiderations of LLM-based negotiation systems.",
        "This paper proposes a UAV-assisted forwarding system based on distributed\nbeamforming to enhance age of information (AoI) in Internet of Things (IoT).\nSpecifically, UAVs collect and relay data between sensor nodes (SNs) and the\nremote base station (BS). However, flight delays increase the AoI and degrade\nthe network performance. To mitigate this, we adopt distributed beamforming to\nextend the communication range, reduce the flight frequency and ensure the\ncontinuous data relay and efficient energy utilization. Then, we formulate an\noptimization problem to minimize AoI and UAV energy consumption, by jointly\noptimizing the UAV trajectories and communication schedules. The problem is\nnon-convex and with high dynamic, and thus we propose a deep reinforcement\nlearning (DRL)-based algorithm to solve the problem, thereby enhancing the\nstability and accelerate convergence speed. Simulation results show that the\nproposed algorithm effectively addresses the problem and outperforms other\nbenchmark algorithms.",
        "Decision-making processes in healthcare can be highly complex and\nchallenging. Machine Learning tools offer significant potential to assist in\nthese processes. However, many current methodologies rely on complex models\nthat are not easily interpretable by experts. This underscores the need to\ndevelop interpretable models that can provide meaningful support in clinical\ndecision-making. When approaching such tasks, humans typically compare the\nsituation at hand to a few key examples and representative cases imprinted in\ntheir memory. Using an approach which selects such exemplary cases and grounds\nits predictions on them could contribute to obtaining high-performing\ninterpretable solutions to such problems. To this end, we evaluate PivotTree,\nan interpretable prototype selection model, on an oral lesion detection\nproblem, specifically trying to detect the presence of neoplastic, aphthous and\ntraumatic ulcerated lesions from oral cavity images. We demonstrate the\nefficacy of using such method in terms of performance and offer a qualitative\nand quantitative comparison between exemplary cases and ground-truth prototypes\nselected by experts.",
        "Constructive neural combinatorial optimization (NCO) has attracted growing\nresearch attention due to its ability to solve complex routing problems without\nrelying on handcrafted rules. However, existing NCO methods face significant\nchallenges in generalizing to large-scale problems due to high computational\ncomplexity and inefficient capture of structural patterns. To address this\nissue, we propose a novel learning-based search space reduction method that\nadaptively selects a small set of promising candidate nodes at each step of the\nconstructive NCO process. Unlike traditional methods that rely on fixed\nheuristics, our selection model dynamically prioritizes nodes based on learned\npatterns, significantly reducing the search space while maintaining solution\nquality. Experimental results demonstrate that our method, trained solely on\n100-node instances from uniform distribution, generalizes remarkably well to\nlarge-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) instances with up to 1 million nodes from the uniform\ndistribution and over 80K nodes from other distributions.",
        "Advances in large language models (LLMs) offer new possibilities for\nenhancing math education by automating support for both teachers and students.\nWhile prior work has focused on generating math problems and high-quality\ndistractors, the role of visualization in math learning remains under-explored.\nDiagrams are essential for mathematical thinking and problem-solving, yet\nmanually creating them is time-consuming and requires domain-specific\nexpertise, limiting scalability. Recent research on using LLMs to generate\nScalable Vector Graphics (SVG) presents a promising approach to automating\ndiagram creation. Unlike pixel-based images, SVGs represent geometric figures\nusing XML, allowing seamless scaling and adaptability. Educational platforms\nsuch as Khan Academy and IXL already use SVGs to display math problems and\nhints. In this paper, we explore the use of LLMs to generate math-related\ndiagrams that accompany textual hints via intermediate SVG representations. We\naddress three research questions: (1) how to automatically generate math\ndiagrams in problem-solving hints and evaluate their quality, (2) whether SVG\nis an effective intermediate representation for math diagrams, and (3) what\nprompting strategies and formats are required for LLMs to generate accurate\nSVG-based diagrams. Our contributions include defining the task of\nautomatically generating SVG-based diagrams for math hints, developing an LLM\nprompting-based pipeline, and identifying key strategies for improving diagram\ngeneration. Additionally, we introduce a Visual Question Answering-based\nevaluation setup and conduct ablation studies to assess different pipeline\nvariations. By automating the math diagram creation, we aim to provide students\nand teachers with accurate, conceptually relevant visual aids that enhance\nproblem-solving and learning experiences.",
        "We propose a methodology that allows communication with Piping and\nInstrumentation Diagrams (P&IDs) using natural language. In particular, we\nrepresent P&IDs through the DEXPI data model as labeled property graphs and\nintegrate them with Large Language Models (LLMs). The approach consists of\nthree main parts: 1) P&IDs are cast into a graph representation from the DEXPI\nformat using our pyDEXPI Python package. 2) A tool for generating P&ID\nknowledge graphs from pyDEXPI. 3) Integration of the P&ID knowledge graph to\nLLMs using graph-based retrieval augmented generation (graph-RAG). This\napproach allows users to communicate with P&IDs using natural language. It\nextends LLM's ability to retrieve contextual data from P&IDs and mitigate\nhallucinations. Leveraging the LLM's large corpus, the model is also able to\ninterpret process information in PIDs, which could help engineers in their\ndaily tasks. In the future, this work will also open up opportunities in the\ncontext of other generative Artificial Intelligence (genAI) solutions on P&IDs,\nand AI-assisted HAZOP studies.",
        "Stackelberg games, widely applied in domains like economics and security,\ninvolve asymmetric interactions where a leader's strategy drives follower\nresponses. Accurately modeling these dynamics allows domain experts to optimize\nstrategies in interactive scenarios, such as turn-based sports like badminton.\nIn multi-agent systems, agent behaviors are interdependent, and traditional\nMulti-Agent Imitation Learning (MAIL) methods often fail to capture these\ncomplex interactions. Correlated policies, which account for opponents'\nstrategies, are essential for accurately modeling such dynamics. However, even\nmethods designed for learning correlated policies, like CoDAIL, struggle in\nStackelberg games due to their asymmetric decision-making, where leaders and\nfollowers cannot simultaneously account for each other's actions, often leading\nto non-correlated policies. Furthermore, existing MAIL methods that match\noccupancy measures or use adversarial techniques like GAIL or Inverse RL face\nscalability challenges, particularly in high-dimensional environments, and\nsuffer from unstable training. To address these challenges, we propose a\ncorrelated policy occupancy measure specifically designed for Stackelberg games\nand introduce the Latent Stackelberg Differential Network (LSDN) to match it.\nLSDN models two-agent interactions as shared latent state trajectories and uses\nmulti-output Geometric Brownian Motion (MO-GBM) to effectively capture joint\npolicies. By leveraging MO-GBM, LSDN disentangles environmental influences from\nagent-driven transitions in latent space, enabling the simultaneous learning of\ninterdependent policies. This design eliminates the need for adversarial\ntraining and simplifies the learning process. Extensive experiments on\nIterative Matrix Games and multi-agent particle environments demonstrate that\nLSDN can better reproduce complex interaction dynamics than existing MAIL\nmethods.",
        "Despite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM\nagents collaborate to accomplish tasks, their performance gains across popular\nbenchmarks remain minimal compared to single-agent frameworks. This gap\nhighlights the need to analyze the challenges hindering MAS effectiveness.\n  In this paper, we present the first comprehensive study of MAS challenges. We\nanalyze five popular MAS frameworks across over 150 tasks, involving six expert\nhuman annotators. We identify 14 unique failure modes and propose a\ncomprehensive taxonomy applicable to various MAS frameworks. This taxonomy\nemerges iteratively from agreements among three expert annotators per study,\nachieving a Cohen's Kappa score of 0.88. These fine-grained failure modes are\norganized into 3 categories, (i) specification and system design failures, (ii)\ninter-agent misalignment, and (iii) task verification and termination. To\nsupport scalable evaluation, we integrate MASFT with LLM-as-a-Judge. We also\nexplore if identified failures could be easily prevented by proposing two\ninterventions: improved specification of agent roles and enhanced orchestration\nstrategies. Our findings reveal that identified failures require more complex\nsolutions, highlighting a clear roadmap for future research. We open-source our\ndataset and LLM annotator.",
        "As artificial intelligence (AI) becomes increasingly embedded in healthcare\ndelivery, this chapter explores the critical aspects of developing reliable and\nethical Clinical Decision Support Systems (CDSS). Beginning with the\nfundamental transition from traditional statistical models to sophisticated\nmachine learning approaches, this work examines rigorous validation strategies\nand performance assessment methods, including the crucial role of model\ncalibration and decision curve analysis. The chapter emphasizes that creating\ntrustworthy AI systems in healthcare requires more than just technical\naccuracy; it demands careful consideration of fairness, explainability, and\nprivacy. The challenge of ensuring equitable healthcare delivery through AI is\nstressed, discussing methods to identify and mitigate bias in clinical\npredictive models. The chapter then delves into explainability as a cornerstone\nof human-centered CDSS. This focus reflects the understanding that healthcare\nprofessionals must not only trust AI recommendations but also comprehend their\nunderlying reasoning. The discussion advances in an analysis of privacy\nvulnerabilities in medical AI systems, from data leakage in deep learning\nmodels to sophisticated attacks against model explanations. The text explores\nprivacy-preservation strategies such as differential privacy and federated\nlearning, while acknowledging the inherent trade-offs between privacy\nprotection and model performance. This progression, from technical validation\nto ethical considerations, reflects the multifaceted challenges of developing\nAI systems that can be seamlessly and reliably integrated into daily clinical\npractice while maintaining the highest standards of patient care and data\nprotection.",
        "Retrosynthesis prediction focuses on identifying reactants capable of\nsynthesizing a target product. Typically, the retrosynthesis prediction\ninvolves two phases: Reaction Center Identification and Reactant Generation.\nHowever, we argue that most existing methods suffer from two limitations in the\ntwo phases: (i) Existing models do not adequately capture the ``face''\ninformation in molecular graphs for the reaction center identification. (ii)\nCurrent approaches for the reactant generation predominantly use sequence\ngeneration in a 2D space, which lacks versatility in generating reasonable\ndistributions for completed reactive groups and overlooks molecules' inherent\n3D properties. To overcome the above limitations, we propose GDiffRetro. For\nthe reaction center identification, GDiffRetro uniquely integrates the original\ngraph with its corresponding dual graph to represent molecular structures,\nwhich helps guide the model to focus more on the faces in the graph. For the\nreactant generation, GDiffRetro employs a conditional diffusion model in 3D to\nfurther transform the obtained synthon into a complete reactant. Our\nexperimental findings reveal that GDiffRetro outperforms state-of-the-art\nsemi-template models across various evaluative metrics.",
        "Deep reinforcement learning agents often face challenges to effectively\ncoordinate perception and decision-making components, particularly in\nenvironments with high-dimensional sensory inputs where feature relevance\nvaries. This work introduces SPRIG (Stackelberg Perception-Reinforcement\nlearning with Internal Game dynamics), a framework that models the internal\nperception-policy interaction within a single agent as a cooperative\nStackelberg game. In SPRIG, the perception module acts as a leader,\nstrategically processing raw sensory states, while the policy module follows,\nmaking decisions based on extracted features. SPRIG provides theoretical\nguarantees through a modified Bellman operator while preserving the benefits of\nmodern policy optimization. Experimental results on the Atari BeamRider\nenvironment demonstrate SPRIG's effectiveness, achieving around 30% higher\nreturns than standard PPO through its game-theoretical balance of feature\nextraction and decision-making.",
        "Large Language Models (LLMs) have the potential to revolutionize data\nanalytics by simplifying tasks such as data discovery and SQL query synthesis\nthrough natural language interactions. This work serves as a pivotal first step\ntoward the development of foundation models explicitly designed for data\nanalytics applications. To propel this vision forward, we unveil a new data\nrecipe for post-training LLMs, enhancing their comprehension of data management\nand empowering them to tackle complex real-world analytics tasks. Specifically,\nour innovative approach includes a scalable synthetic data generation method\nthat enables the creation of a broad spectrum of topics centered on data\nrepresentation and manipulation. Furthermore, we introduce two new tasks that\nseamlessly bridge tables and text. We show that such tasks can enhance models'\nunderstanding of schema creation and the nuanced translation between natural\nlanguage and tabular data. Leveraging this data recipe, we post-train a new\nfoundation model, named CoddLLM, based on Mistral-NeMo-12B. To assess the\nlanguage understanding and reasoning capabilities of LLMs in the realm of data\nanalytics, we contribute AnalyticsMMLU, a benchmark containing thousands of\nmultiple-choice questions on databases, data analysis, and machine learning.\nOur focus on data discovery, has resulted in the contribution of three\ncomprehensive benchmarks that address both database and data lake scenarios.\nCoddLLM not only excels in performance but also sets a new standard, achieving\nthe highest average accuracy across eight datasets. It outperforms\nGPT-3.5-Turbo on AnalyticsMMLU, exceeding GPT-4o by 12.1% in table selection\nand showing an average improvement of 24.9% in Text-to-SQL compared to the base\nmodel.",
        "We address the problem of semi-supervised domain generalization (SSDG), where\nthe distributions of train and test data differ, and only a small amount of\nlabeled data along with a larger amount of unlabeled data are available during\ntraining. Existing SSDG methods that leverage only the unlabeled samples for\nwhich the model's predictions are highly confident (confident-unlabeled\nsamples), limit the full utilization of the available unlabeled data. To the\nbest of our knowledge, we are the first to explore a method for incorporating\nthe unconfident-unlabeled samples that were previously disregarded in SSDG\nsetting. To this end, we propose UPCSC to utilize these unconfident-unlabeled\nsamples in SSDG that consists of two modules: 1) Unlabeled Proxy-based\nContrastive learning (UPC) module, treating unconfident-unlabeled samples as\nadditional negative pairs and 2) Surrogate Class learning (SC) module,\ngenerating positive pairs for unconfident-unlabeled samples using their\nconfusing class set. These modules are plug-and-play and do not require any\ndomain labels, which can be easily integrated into existing approaches.\nExperiments on four widely used SSDG benchmarks demonstrate that our approach\nconsistently improves performance when attached to baselines and outperforms\ncompeting plug-and-play methods. We also analyze the role of our method in\nSSDG, showing that it enhances class-level discriminability and mitigates\ndomain gaps. The code is available at https:\/\/github.com\/dongkwani\/UPCSC.",
        "A study of antiproton annihilations at rest on thin solid targets is underway\nat the ASACUSA facility, which now features a dedicated beam line for slow\nextraction at 250 eV. The experiment will employ new technologies, such as the\nTimepix4 ASICs coupled to silicon sensors, to measure the total multiplicity,\nenergy, and angular distribution of various prongs produced in thin solid\ntargets. A detection system consisting of seven Timepix4, covering most of the\nsolid angle, is being constructed. A 3D annihilation vertex reconstruction\nalgorithm from particle tracks in the single-plane detectors has been developed\nusing Monte Carlo simulations. The measurements will enable a study of\npbar-nucleus interactions, their dependence on nucleus mass and branching\nratios. The results will be used to assess and potentially improve various\nsimulation models.",
        "Iconic products, as innovative carriers supporting the development of future\nindustries, are key breakthrough points for driving the transformation of new\nquality productive forces. This article is grounded in the philosophy of\ntechnology and examines the evolution of human civilization to accurately\nidentify the patterns of product innovation. By integrating theories from\nsystems science, it analyzes the intrinsic logical differences between\ntraditional products and iconic products. The study finds that iconic products\nare based on a comprehensive knowledge system that integrates explicit and\ntacit knowledge, enabling them to adapt to complex dynamic environments.\nTherefore, based on the method of phenomenological essence reduction and the\nprocess of specialized knowledge acquisition, this study establishes the first\nprinciple of knowledge phenomenology: \"knowledge generation-moving from the\ntacit to the explicit-moving from the explicit to the tacit-fusion of the\nexplicit and tacit.\" Grounded in knowledge phenomenology, it reconstructs the\nproduct design evolution process and establishes a forward innovative design\nframework for iconic products, consisting of \"design problem space-explicit\nknowledge space-tacit knowledge space-innovative solution space.\" Furthermore,\nbased on FBS design theory, it develops a disruptive technology innovation\nforecasting framework of \"technology problem space-knowledge base\nprediction-application scenario prediction-coupled technology prediction,\"\nwhich collectively advances the innovation systems engineering of iconic\nproducts. In light of the analysis of the global future industrial competitive\nlandscape, it proposes a strategy for enhancing embodied intelligence in iconic\nproducts.",
        "We consider the transition from the kinetic model of Landau cold plasma to\nthe hydrodynamic one by constructing a \"multi-speed\" moment chain in the case\nof one spatial variable. Closing this chain at the first step leads to the\nstandard hydrodynamic system of cold plasma. The change in the properties of\nthe solution when closing the chain at the second step is discussed using the\nexample of two classes of solutions - affine in space and traveling waves, and\nit is shown that their properties change significantly compared to the\nhydrodynamic model.",
        "Language Model Agents (LMAs) are increasingly treated as capable of\nautonomously navigating interactions with humans and tools. Their design and\ndeployment tends to presume they are normal agents capable of sustaining\ncoherent goals, adapting across contexts and acting with a measure of\nintentionality. These assumptions are critical to prospective use cases in\nindustrial, social and governmental settings. But LMAs are not normal agents.\nThey inherit the structural problems of the large language models (LLMs) around\nwhich they are built: hallucinations, jailbreaking, misalignment and\nunpredictability. In this Position paper we argue LMAs should not be treated as\nnormal agents, because doing so leads to problems that undermine their utility\nand trustworthiness. We enumerate pathologies of agency intrinsic to LMAs.\nDespite scaffolding such as external memory and tools, they remain\nontologically stateless, stochastic, semantically sensitive, and linguistically\nintermediated. These pathologies destabilise the ontological properties of LMAs\nincluding identifiability, continuity, persistence and and consistency,\nproblematising their claim to agency. In response, we argue LMA ontological\nproperties should be measured before, during and after deployment so that the\nnegative effects of pathologies can be mitigated.",
        "Our goal in this paper is to understand how to maximize energy efficiency\nwhen designing standard-ISA processor cores for subthreshold operation. We\nhence develop a custom subthreshold library and use it to synthesize the\nopen-source RISC-V cores SERV, QERV, PicoRV32, Ibex, Rocket, and two variants\nof Vex, targeting a supply voltage of 300 mV in a commercial 130 nm process.\nSERV, QERV, and PicoRV32 are multi-cycle architectures, while Ibex, Vex, and\nRocket are pipelined architectures.\n  We find that SERV, QERV, PicoRV32, and Vex are Pareto optimal in one or more\nof performance, power, and area. The 2-stage Vex (Vex-2) is the most energy\nefficient core overall, mainly because it uses fewer cycles per instruction\nthan multi-cycle SERV, QERV, and PicoRV32 while retaining similar power\nconsumption. Pipelining increases core area, and we observe that for\nsubthreshold operation, the longer wires of pipelined designs require adding\nbuffers to maintain a cycle time that is low enough to achieve high energy\nefficiency. These buffers limit the performance gains achievable by deeper\npipelining because they result in cycle time no longer scaling proportionally\nwith pipeline stages. The added buffers and the additional area required for\npipelining logic however increase power consumption, and Vex-2 therefore\nprovides similar performance and lower power consumption than the 5-stage cores\nVex-5 and Rocket. A key contribution of this paper is therefore to demonstrate\nthat limited-depth pipelined RISC-V designs hit the sweet spot in balancing\nperformance and power consumption when optimizing for energy efficiency in\nsubthreshold operation.",
        "We work in the category $\\mathcal{CLM}^u_k$ of [5] of separated complete\nbounded $k$-linearly topologized modules over a complete linearly topologized\nring $k$ and discuss duality on certain exact subcategories. We study\ntopological and uniform structures on locally compact paracompact\n$0$-dimensional topological spaces $X$, named $td$-spaces in [11] and [17], and\nthe corresponding algebras $\\mathscr{C}_?(X,k)$ of continuous $k$-valued\nfunctions, with a choice of support and uniformity conditions. We apply the\nprevious duality theory to define and study the dual coalgebras\n$\\mathscr{D}_?(X,k)$ of $k$-valued measures on $X$. We then complete the\npicture by providing a direct definition of the various types of measures. In\nthe case of $X$ a commutative $td$-group $G$ the integration pairing provides\nperfect dualities of Hopf $k$-algebras between $$\\mathscr{C}_{\\rm unif}(G,k)\n\\longrightarrow \\mathscr{C}(G,k) \\;\\;\\;\\mbox{and}\\;\\;\\; \\mathscr{D}_{\\rm\nacs}(G,k) \\longrightarrow \\mathscr{D}_{\\rm unif}(G,k) \\;.$$ We conclude the\npaper with the remarkable example of $G= \\mathbb{G}_a(\\mathbb{Q}_p)$ and $k =\n\\mathbb{Z}_p$, leading to the basic Fontaine ring $${\\bf A}_{\\rm inf} = {\\rm W}\n\\left(\\widehat{\\mathbb{F}_p[[t^{1\/p^\\infty}]]}\\right) = \\mathscr{D}_{\\rm\nunif}(\\mathbb{Q}_p,\\mathbb{Z}_p) \\;.$$ We discuss Fourier duality between ${\\bf\nA}_{\\rm inf}$ and $\\mathscr{C}_{\\rm unif}(\\mathbb{Q}_p,\\mathbb{Z}_p)$ and\nexhibit a remarkable Fr\\'echet basis of $\\mathscr{C}_{\\rm\nunif}(\\mathbb{Q}_p,\\mathbb{Z}_p)$ related to the classical binomial\ncoefficients.",
        "We represent the rational and mod $p$ cohomology groups of classifying spaces\nof rank 3 Kac-Moody groups by a direct sum of the invariants of Weyl groups and\ntheir quotients. As an application, the authors conclude that there is a\n$p$-torsion for each prime $p$ in the integral cohomology groups of classifying\nspaces of rank 3 Kac-Moody groups. We also determine the ring structure of the\nrational cohomology with one exception case.",
        "Joint diagonalization, the process of finding a shared set of approximate\neigenvectors for a collection of matrices, arises in diverse applications such\nas multidimensional harmonic analysis or quantum information theory. This task\nis typically framed as an optimization problem: minimizing a non-convex\nfunction that quantifies off-diagonal matrix elements across possible bases. In\nthis work, we introduce a suite of efficient algorithms designed to locate\nlocal minimizers of this functional. Our methods leverage the Hessian's\nstructure to bypass direct computation of second-order derivatives, evaluating\nit as either an operator or bilinear form - a strategy that remains\ncomputationally feasible even for large-scale applications. Additionally, we\ndemonstrate that this Hessian-based information enables precise estimation of\nparameters, such as step-size, in first-order optimization techniques like\nGradient Descent and Conjugate Gradient, and the design of second-order methods\nsuch as (Quasi-)Newton. The resulting algorithms for joint diagonalization\noutperform existing techniques, and we provide comprehensive numerical evidence\nof their superior performance.",
        "We consider the following budget-constrained random graph process introduced\nby Frieze, Krivelevich and Michaeli. A player, called Builder, is presented\nwith $t$ distinct edges of $K_n$ one by one, chosen uniformly at random.\nBuilder may purchase at most $b$ of these edges, and must (irrevocably) decide\nwhether to purchase each edge as soon as it is offered. Builder's goal is to\nconstruct a graph which satisfies a certain property; we investigate the\nproperties of containing different $F$-factors or powers of Hamilton cycles.\n  We obtain general lower bounds on the budget $b$, as a function of $t$,\nrequired for Builder to obtain partial $F$-factors, for arbitrary $F$. These\nimply lower bounds for many distinct spanning structures, such as powers of\nHamilton cycles. Notably, our results show that, if $t$ is close to the hitting\ntime for a partial $F$-factor, then the budget $b$ cannot be substantially\nlower than $t$. These results give negative answers to questions of Frieze,\nKrivelevich and Michaeli.\n  Conversely, we also exhibit a simple strategy for constructing (partial)\n$F$-factors, in particular showing that our general lower bound is tight up to\nconstant factors. The ideas from this strategy can be exploited for other\nproperties. As an example, we obtain an essentially optimal strategy for powers\nof Hamilton cycles. In order to formally prove that this strategy succeeds, we\ndevelop novel tools for analysing multi-stage strategies, which may be of\ngeneral interest for studying other properties.",
        "While the current state-of-the-art dense retrieval models exhibit strong\nout-of-domain generalization, they might fail to capture nuanced\ndomain-specific knowledge. In principle, fine-tuning these models for\nspecialized retrieval tasks should yield higher effectiveness than relying on a\none-size-fits-all model, but in practice, results can disappoint. We show that\nstandard fine-tuning methods using an InfoNCE loss can unexpectedly degrade\neffectiveness rather than improve it, even for domain-specific scenarios. This\nholds true even when applying widely adopted techniques such as hard-negative\nmining and negative de-noising. To address this, we explore a training strategy\nthat uses listwise distillation from a teacher cross-encoder, leveraging rich\nrelevance signals to fine-tune the retriever. We further explore synthetic\nquery generation using large language models. Through listwise distillation and\ntraining with a diverse set of queries ranging from natural user searches and\nfactual claims to keyword-based queries, we achieve consistent effectiveness\ngains across multiple datasets. Our results also reveal that synthetic queries\ncan rival human-written queries in training utility. However, we also identify\nlimitations, particularly in the effectiveness of cross-encoder teachers as a\nbottleneck. We release our code and scripts to encourage further research.",
        "For challenging state estimation problems arising in domains like vision and\nrobotics, particle-based representations attractively enable temporal reasoning\nabout multiple posterior modes. Particle smoothers offer the potential for more\naccurate offline data analysis by propagating information both forward and\nbackward in time, but have classically required human-engineered dynamics and\nobservation models. Extending recent advances in discriminative training of\nparticle filters, we develop a framework for low-variance propagation of\ngradients across long time sequences when training particle smoothers. Our\n\"two-filter'' smoother integrates particle streams that are propagated forward\nand backward in time, while incorporating stratification and importance weights\nin the resampling step to provide low-variance gradient estimates for neural\nnetwork dynamics and observation models. The resulting mixture density particle\nsmoother is substantially more accurate than state-of-the-art particle filters,\nas well as search-based baselines, for city-scale global vehicle localization\nfrom real-world videos and maps.",
        "The acoustic background plays a crucial role in natural conversation. It\nprovides context and helps listeners understand the environment, but a strong\nbackground makes it difficult for listeners to understand spoken words. The\nappropriate handling of these backgrounds is situation-dependent: Although it\nmay be necessary to remove background to ensure speech clarity, preserving the\nbackground is sometimes crucial to maintaining the contextual integrity of the\nspeech. Despite recent advancements in zero-shot Text-to-Speech technologies,\ncurrent systems often struggle with speech prompts containing backgrounds. To\naddress these challenges, we propose a Controllable Masked Speech Prediction\nstrategy coupled with a dual-speaker encoder, utilizing a task-related control\nsignal to guide the prediction of dual background removal and preservation\ntargets. Experimental results demonstrate that our approach enables precise\ncontrol over the removal or preservation of background across various acoustic\nconditions and exhibits strong generalization capabilities in unseen scenarios.",
        "We introduce a diagrammatic approach to Rasmussen's $s$-invariant via tangles\nand cobordisms, combining Bar-Natan's formulation of Khovanov homology for\ntangles and cobordisms with the characterization of $s$ via the divisibility of\nthe Lee class, as developed in the author's previous works. This framework\nenables a \"divide-and-conquer\" method for computing $s$ from a tangle\ndecomposition of a given knot diagram, making it suitable for both\npen-and-paper calculations and algorithmic implementations. As an application,\nwe determine the $s$-invariants of pretzel knots of the form $P(p_1, -p_2,\n\\ldots, -p_l)$, where $l \\geq 3$ is odd, all $p_i$ are positive and odd, and\n$p_1 < \\min\\{p_2, \\ldots, p_l\\}$."
      ]
    }
  },
  {
    "id":2411.01758,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"A review on segmentation of positron emission tomography images",
    "start_abstract":"Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data"
      ],
      "abstract":[
        "Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Collective inference of the truth of propositions from crowd probability\n  judgments",
        "Functional Correspondences in the Human and Marmoset Visual Cortex\n  During Movie Watching: Insights from Correlation, Redundancy, and Synergy",
        "Neural Constraints on Cognitive Experience and Mental Health",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Structural determinants of soft memory in recurrent biological networks",
        "Neuronal Correlates of Semantic Event Classes during Presentation of\n  Complex Naturalistic Stimuli: Anatomical Patterns, Context-Sensitivity, and\n  Potential Impact on shared Human-Robot Ontologies",
        "Multicellular self-organization in Escherichia coli",
        "Automating Care by Self-maintainability for Full Laboratory Automation",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "The Nature of Organization in Living Systems",
        "Operational Feasibility Analysis of a Cryogenic Active Intake Device for\n  Atmosphere-Breathing Electric Propulsion",
        "Exponentially Better Bounds for Quantum Optimization via Dynamical\n  Simulation",
        "Scalar Field Kantowski--Sachs Solutions in Teleparallel $F(T)$ Gravity",
        "In the graphical Sierpinski gasket, the reverse Riesz transform is\n  unbounded on $L^p$, $p\\in (1,2)$",
        "On the origin of radio polarization in pulsar polar caps",
        "Heavy Axions Can Disrupt $\\gamma$-ray Bursts",
        "TOPCAT\/STILTS Integration",
        "Estimating Task-based Performance Bounds for Accelerated MRI Image\n  Reconstruction Methods by Use of Learned-Ideal Observers",
        "JT Gravity in de Sitter Space and Its Extensions",
        "Generalised Process Theories",
        "Observer-Based Output-Feedback Backstepping Stabilization of Continua of\n  Hyperbolic PDEs and Application to Large-Scale $n+m$ Coupled Hyperbolic PDEs",
        "Deep inference of simulated strong lenses in ground-based surveys",
        "A simple recursive representation of the Faulhaber series",
        "Fragmentation measurements with the FOOT experiment",
        "Galactic structure dependence of cloud-cloud collisions driven star\n  formation in the barred galaxy NGC 3627"
      ],
      "abstract":[
        "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty.",
        "The world of beauty is deeply connected to the visual cortex, as perception\noften begins with vision in both humans and marmosets. Quantifying functional\ncorrespondences in the visual cortex across species can help us understand how\ninformation is processed in the primate visual cortex, while also providing\ndeeper insights into human visual cortex functions through the study of\nmarmosets. In this study, we measured pairwise and beyond pairwise correlation,\nredundancy, and synergy in movie-driven fMRI data across species. Our first key\nfinding was that humans and marmosets exhibited significant overlaps in\nfunctional synergy. Second, we observed that the strongest functional\ncorrespondences between the human peri-entorhinal and entorhinal cortex (PeEc)\nand the occipitotemporal higher-level visual regions in the marmoset during\nmovie watching reflected a functional synergistic relationship. These regions\nare known to correspond to face-selective areas in both species. Third,\nredundancy measures maintained stable high-order hubs, indicating a steady core\nof shared information processing, while synergy measures revealed a dynamic\nshift from low- to high-level visual regions as interaction increased,\nreflecting adaptive integration. This highlights distinct patterns of\ninformation processing across the visual hierarchy. Ultimately, our results\nreveal the marmoset as a compelling model for investigating visual perception,\ndistinguished by its remarkable functional parallels to the human visual\ncortex.",
        "Understanding how neural dynamics shape cognitive experiences remains a\ncentral challenge in neuroscience and psychiatry. Here, we present a novel\nframework leveraging state-to-output controllability from dynamical systems\ntheory to model the interplay between cognitive perturbations, neural activity,\nand subjective experience. We demonstrate that large-scale fMRI signals are\nconstrained to low-dimensional manifolds, where affective and cognitive states\nare naturally organized. Furthermore, we provide a theoretically robust method\nto estimate the controllability Gramian from steady-state neural responses,\noffering a direct measure of the energy required to steer cognitive outcomes.\nIn five healthy participants viewing 2,185 emotionally evocative short videos,\nour analyses reveal a strong alignment between neural activations and affective\nratings, with an average correlation of $r \\approx 0.7$. In a clinical cohort\nof 255 patients with major depressive disorder, biweekly Hamilton Rating Scale\ntrajectories over 11 weeks significantly mapped onto these manifolds,\nexplaining approximately 20% more variance than chance ($p < 10^{-10}$,\nnumerically better than chance in 93% reaching statistical significance in\none-third of subjects). Our work bridges dynamical systems theory and clinical\nneuroscience, providing a principled approach to optimize mental health\ntreatments by targeting the most efficient neural pathways for cognitive\nchange.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "Recurrent neural networks are frequently studied in terms of their\ninformation-processing capabilities. The structural properties of these\nnetworks are seldom considered, beyond those emerging from the connectivity\ntuning necessary for network training. However, real biological networks have\nnon-contingent architectures that have been shaped by evolution over eons,\nconstrained partly by information-processing criteria, but more generally by\nfitness maximization requirements. Here we examine the topological properties\nof existing biological networks, focusing in particular on gene regulatory\nnetworks in bacteria. We identify structural features, both local and global,\nthat dictate the ability of recurrent networks to store information on the fly\nand process complex time-dependent inputs.",
        "The present study forms part of a research project that aims to develop\ncognition-enabled robotic agents with environmental interaction capabilities\nclose to human proficiency. This approach is based on human-derived neuronal\ndata in combination with a shared ontology to enable robots to learn from human\nexperiences. To gain further insight into the relation between human neuronal\nactivity patterns and ontological classes, we introduced General Linear Model\n(GLM) analyses on fMRI data of participants who were presented with complex\nnaturalistic video stimuli comparable to the robot tasks. We modeled four event\nclasses (pick, place, fetch and deliver) attached to different environmental\nand object-related context and employed a Representational Similarity Analysis\n(RSA) on associated brain activity patterns as a starting point for an\nautomatic hierarchical clustering. Based on the default values for the\nHemodynamic Response Function (HRF), the activity patterns were reliably\ngrouped according to their parent classes of object interaction and navigation.\nAlthough fetch and deliver events were also distinguished by neuronal patterns,\npick and place events demonstrated higher ambiguity with respect to neuronal\nactivation patterns. Introducing a shorter HRF time-to-peak leads to a more\nreliable grouping of all four semantic classes, despite contextual factors.\nThese data might give novel insights into the neuronal representation of\ncomplex stimuli and may enable further research in ontology validation in\ncognition-enabled robotics.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "The automation of experiments in life sciences and chemistry has\nsignificantly advanced with the development of various instruments and AI\ntechnologies. However, achieving full laboratory automation, where experiments\nconceived by scientists are seamlessly executed in automated laboratories,\nremains a challenge. We identify the lack of automation in planning and\noperational tasks--critical human-managed processes collectively termed\n\"care\"--as a major barrier. Automating care is the key enabler for full\nlaboratory automation. To address this, we propose the concept of\nself-maintainability (SeM): the ability of a laboratory system to autonomously\nadapt to internal and external disturbances, maintaining operational readiness\nakin to living cells. A SeM-enabled laboratory features autonomous recognition\nof its state, dynamic resource and information management, and adaptive\nresponses to unexpected conditions. This shifts the planning and execution of\nexperimental workflows, including scheduling and reagent allocation, from\nhumans to the system. We present a conceptual framework for implementing\nSeM-enabled laboratories, comprising three modules--Requirement manager,\nLabware manager, and Device manager--and a Central manager. SeM not only\nenables scientists to execute envisioned experiments seamlessly but also\nprovides developers with a design concept that drives the technological\ninnovations needed for full automation.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "Living systems are thermodynamically open but closed in their organization.\nIn other words, even though their material components turn over constantly, a\nmaterial-independent property persists, which we call organization. Moreover,\norganization comes from within organisms themselves, which requires us to\nexplain how this self-organization is established and maintained. In this paper\nwe propose a mathematical and conceptual framework to understand the kinds of\norganized systems that living systems are, aiming to explain how\nself-organization emerges from more basic elemental processes. Additionally, we\nmap our own notions to existing traditions in theoretical biology and\nphilosophy, aiming to bring the main formal ideas into conceptual congruence.",
        "Atmosphere-breathing electric propulsion (ABEP) systems are emerging for\norbit maintenance in very-low-Earth orbit (VLEO) by capturing atmospheric\npropellant \\textit{in situ} using an intake device. A previous study proposed\nthe cryocondensation-regeneration active intake device (CRAID) to significantly\nenhance intake performance. This study investigates the operational feasibility\nof CRAID. A conceptual prototype model (CPM) is presented to verify its\nfeasibility, and numerical analyses demonstrate the practical operational\nsequences, required cryocooler capacity, intake performance, and flight\nenvelope. The numerical analyses employ the direct simulation Monte Carlo\n(DSMC) method with a phase change model and a 0D analytical model for RF ion\nthrusters. A significant improvement in intake performance is estimated based\non the practical sequences, with compression performance at least 1000 times\nhigher than that of prevalent intake devices. The capability for consistent\npropellant supply is observed regardless of atmospheric conditions. A model\nsatellite incorporating CPM confirms that CRAID enables complete drag\ncompensation at altitudes above 190 km without limiting the upper boundary of\nthe flight envelope.",
        "We provide several quantum algorithms for continuous optimization that do not\nrequire any gradient estimation. Instead, we encode the optimization problem\ninto the dynamics of a physical system and coherently simulate the time\nevolution. This allows us, in certain cases, to obtain exponentially better\nquery upper bounds relative to the best known upper bounds for gradient-based\noptimization schemes which utilize quantum computers only for the evaluation of\ngradients. Our first two algorithms can find local optima of a differentiable\nfunction $f: \\mathbb{R}^N \\rightarrow \\mathbb{R}$ by simulating either\nclassical or quantum dynamics with friction via a time-dependent Hamiltonian.\nWe show that these methods require $O(N\\kappa^2\/h_x^2\\epsilon)$ queries to a\nphase oracle to find an $\\epsilon$-approximate local optimum of a locally\nquadratic objective function, where $\\kappa$ is the condition number of the\nHessian matrix and $h_x$ is the discretization spacing. In contrast, we show\nthat gradient-based methods require $O(N(1\/\\epsilon)^{\\kappa \\log(3)\/4})$\nqueries. Our third algorithm can find the global optimum of $f$ by preparing a\nclassical low-temperature thermal state via simulation of the classical\nLiouvillian operator associated with the Nos\\'e Hamiltonian. We use results\nfrom the quantum thermodynamics literature to bound the thermalization time for\nthe discrete system. Additionally, we analyze barren plateau effects that\ncommonly plague quantum optimization algorithms and observe that our approach\nis vastly less sensitive to this problem than standard gradient-based\noptimization. Our results suggests that these dynamical optimization approaches\nmay be far more scalable for future quantum machine learning, optimization and\nvariational experiments than was widely believed.",
        "In this paper, we investigate time-dependent Kantowski--Sachs spherically\nsymmetric teleparallel $F(T)$ gravity with a scalar field source. We begin by\nsetting the exact field equations to be solved and solve conservation laws for\npossible scalar field potential, $V\\left(\\phi\\right)$, solutions. Then, we find\nnew non-trivial teleparallel $F(T)$ solutions by using power-law and\nexponential ansatz for each potential case arising from conservation laws, such\nas linear, quadratic, or logarithmic, to name a few. We find a general formula\nallowing us to compute all possible new teleparallel $F(T)$ solutions\napplicable for any scalar field potential and ansatz. Then, we apply this\nformula and find a large number of exact and approximate new teleparallel\n$F(T)$ solutions for several types of cases. Some new $F(T)$ solution classes\nmay be relevant for future cosmological applications, especially concerning\ndark matter, dark energy quintessence, phantom energy leading to the Big Rip\nevent, and quintom models of physical processes.",
        "In this article, we proved that the reverse Riesz transform on the graphical\nSierpinski gasket is unbounded on $L^p$ for $p\\in (1,2)$. Together with\nprevious results, it shows that the Riesz transform on the graphical Sierpinski\ngasket is bounded on $L^p$ if and only if $p\\in (1,2]$ and the reverse Riesz\ntransform is bounded on $L^p$ if and only if $p\\in [2,\\infty)$.\n  Moreover, our method is quite flexible - but requires explicit computations -\nand hints to the fact that the reverse Riesz transforms is never bounded on\n$L^p$, $p\\in (1,2)$, on graphs with slow diffusions.",
        "A knowledge of polarization properties of coherent radio waves escaping\npulsar polar caps is crucial for calculating radiative transfer through the\nmagnetosphere and for obtaining specific predictions of observable radio\nproperties. We describe the pair cascades in the pulsar polar cap, and for the\nfirst time, determine the Stokes parameters of the escaping radio waves from\nfirst-principle kinetic simulations for a pulsar with an inclination angle of\nthe magnetic axis 60{\\deg}.\n  Our model provides a quantitative and qualitative explanation of the observed\npulsar radio powers and spectra, the pulse profiles and polarization curves,\ntheir temporal variability, the strong Stokes L and weak Stokes V polarization\ncomponents, as well as the fact that linear polarization decreases with\nfrequency and the non-existence of a radius to frequency relationship. We find\nthat the radio emission from the polar cap can produce a diverse range of\nobserved pulsar properties, including single or double peaked profiles. Most of\nthe Stokes V curves from our simulations appear to be antisymmetric, but\nsymmetric curves are also present at some viewing angles. Although the PA swing\nof the radiation from the polar cap can be fitted by the rotating vector model\n(RVM) for most viewing angles, the angles obtained from the RVM do not\ncorrespond to the angular distance of the observer from the magnetic axis.\nInstead, the PA is directly related to the plasma flows in the polar cap and\nnot to the dipole geometry of the magnetic field. The observed range of other\npolarization features, in addition to our results, can be explained by\npropagation effects which are not part of the simulation.\n  Our simulations demonstrate that pair discharges determine the majority of\nits typically observed properties. The usage of RVM for estimations of the\nmagnetic field geometry from observations needs to be reevaluated.",
        "Axion-like particles (ALPs) can be produced in the hot dense plasma of\nfireballs that develop in the initial stage of $\\gamma$-ray burst (GRB)\noutflows. They can transport an enormous amount of energy away from the jet by\npropagating out of the fireball. The photons produced by the eventual decay of\nsuch ALPs do not reach a sufficient density to re-thermalize through pair\nproduction, preventing fireball re-emergence. Thus, the production of heavy\nALPs disrupts the fireball and dims GRBs, allowing bright GRB observations to\nstrongly constrain the existence of heavy ALPs. By adding ALP interactions to\nexisting models of GRB fireballs, we set competitive bounds on the ALP-photon\ncoupling down to $g_{a \\gamma \\gamma} \\sim 4 \\times\n10^{-12}~{\\mathrm{GeV}^{-1}}$ for ALPs in the mass range of 200 MeV - 5 GeV.",
        "TOPCAT and STILTS are related packages for desktop analysis of tabular data,\npresenting GUI and command-line interfaces respectively to much of the same\nfunctionality. This paper presents features in TOPCAT that facilitate use of\nSTILTS.",
        "Medical imaging systems are commonly assessed and optimized by the use of\nobjective measures of image quality (IQ). The performance of the ideal observer\n(IO) acting on imaging measurements has long been advocated as a\nfigure-of-merit to guide the optimization of imaging systems. For computed\nimaging systems, the performance of the IO acting on imaging measurements also\nsets an upper bound on task-performance that no image reconstruction method can\ntranscend. As such, estimation of IO performance can provide valuable guidance\nwhen designing under-sampled data-acquisition techniques by enabling the\nidentification of designs that will not permit the reconstruction of\ndiagnostically inappropriate images for a specified task - no matter how\nadvanced the reconstruction method is or how plausible the reconstructed images\nappear. The need for such analysis is urgent because of the substantial\nincrease of medical device submissions on deep learning-based image\nreconstruction methods and the fact that they may produce clean images\ndisguising the potential loss of diagnostic information when data is\naggressively under-sampled. Recently, convolutional neural network (CNN)\napproximated IOs (CNN-IOs) was investigated for estimating the performance of\ndata space IOs to establish task-based performance bounds for image\nreconstruction, under an X-ray computed tomographic (CT) context. In this work,\nthe application of such data space CNN-IO analysis to multi-coil magnetic\nresonance imaging (MRI) systems has been explored. This study utilized stylized\nmulti-coil sensitivity encoding (SENSE) MRI systems and deep-generated\nstochastic brain models to demonstrate the approach. Signal-known-statistically\nand background-known-statistically (SKS\/BKS) binary signal detection tasks were\nselected to study the impact of different acceleration factors on the data\nspace IO performance.",
        "We discuss and extend some aspects pertaining to the canonical quantisation\nof JT gravity in de Sitter space, including the problem of time and the\nconstruction of a Hilbert space. We then extend this discussion to other two\ndimensional models obtained by changing the dilaton potential and show that the\ncanonical quantisation procedure can be carried out for a large class of such\nmodels. Some discussion leading towards a path integral understanding for\nstates, other than the Hartle Hawking state, is also included here, along with\ncomments pertaining to Holography and the entropy of de Sitter space.",
        "Process theories provide a powerful framework for describing compositional\nstructures across diverse fields, from quantum mechanics to computational\nlinguistics. Traditionally, they have been formalized using symmetric monoidal\ncategories (SMCs). However, various generalizations, including time-neutral,\nhigher-order, and enriched process theories, do not naturally conform to this\nstructure. In this work, we propose an alternative formalization using operad\nalgebras, motivated by recent results connecting SMCs to operadic structures,\nwhich captures a broader class of process theories. By leveraging the\nstring-diagrammatic language, we provide an accessible yet rigorous formulation\nthat unifies and extends traditional process-theoretic approaches. Our operadic\nframework not only recovers standard process theories as a special case but\nalso enables new insights into quantum foundations and compositional\nstructures. This work paves the way for further investigations into the\nalgebraic and operational properties of generalised process theories within an\noperadic setting.",
        "We develop a non-collocated, observer-based output-feedback law for a class\nof continua of linear hyperbolic PDE systems, which are viewed as the continuum\nversion of $n+m$, general heterodirectional hyperbolic systems as $n\\to\\infty$.\nThe design relies on the introduction of a novel, continuum PDE backstepping\ntransformation, which enables the construction of a Lyapunov functional for the\nestimation error system. Stability under the observer-based output-feedback law\nis established by using the Lyapunov functional construction for the estimation\nerror system and proving well-posedness of the complete closed-loop system,\nwhich allows utilization of the separation principle.\n  Motivated by the fact that the continuum-based designs may provide\ncomputationally tractable control laws for large-scale, $n+m$ systems, we then\nutilize the control\/observer kernels and the observer constructed for the\ncontinuum system to introduce an output-feedback control design for the\noriginal $n+m$ system. We establish exponential stability of the resulting\nclosed-loop system, which consists of a mixed $n+m$-continuum PDE system\n(comprising the plant-observer dynamics), introducing a virtual continuum\nsystem with resets, which enables utilization of the continuum approximation\nproperty of the solutions of the $n+m$ system by its continuum counterpart (for\nlarge $n$). We illustrate the potential computational complexity\/flexibility\nbenefits of our approach via a numerical example of stabilization of a\nlarge-scale $n+m$ system, for which we employ the continuum observer-based\ncontroller, while the continuum-based stabilizing control\/observer kernels can\nbe computed in closed form.",
        "The large number of strong lenses discoverable in future astronomical surveys\nwill likely enhance the value of strong gravitational lensing as a cosmic probe\nof dark energy and dark matter. However, leveraging the increased statistical\npower of such large samples will require further development of automated lens\nmodeling techniques. We show that deep learning and simulation-based inference\n(SBI) methods produce informative and reliable estimates of parameter\nposteriors for strong lensing systems in ground-based surveys. We present the\nexamination and comparison of two approaches to lens parameter estimation for\nstrong galaxy-galaxy lenses -- Neural Posterior Estimation (NPE) and Bayesian\nNeural Networks (BNNs). We perform inference on 1-, 5-, and 12-parameter lens\nmodels for ground-based imaging data that mimics the Dark Energy Survey (DES).\nWe find that NPE outperforms BNNs, producing posterior distributions that are\nmore accurate, precise, and well-calibrated for most parameters. For the\n12-parameter NPE model, the calibration is consistently within $<$10\\% of\noptimal calibration for all parameters, while the BNN is rarely within 20\\% of\noptimal calibration for any of the parameters. Similarly, residuals for most of\nthe parameters are smaller (by up to an order of magnitude) with the NPE model\nthan the BNN model. This work takes important steps in the systematic\ncomparison of methods for different levels of model complexity.",
        "We present a simple elementary recursive representation of the so called\nFaulhaber series $\\sum_{k=1}^n k^N$ for integer $n$ and $N$, without reference\nto Bernoulli numbers or polynomials.",
        "Particle Therapy (PT) has emerged as a powerful tool in cancer treatment,\nleveraging the unique dose distribution of charged particles to deliver high\nradiation levels to the tumor while minimizing damage to surrounding healthy\ntissue. Despite its advantages, further improvements in Treatment Planning\nSystems (TPS) are needed to address uncertainties related to fragmentation\nprocess, which can affect both dose deposition and effectiveness. These\nfragmentation effects also play a critical role in Radiation Protection in\nSpace, where astronauts are exposed to high level of radiation, necessitating\nprecise models for shielding optimization. The FOOT (FragmentatiOn Of Target)\nexperiment addresses these challenges by measuring fragmentation cross-section\nwith high precision, providing essential data for improving TPS for PT and\nspace radiation protection strategies. This thesis contributes to the FOOT\nexperiment in two key areas. First, it focuses on the performances of the\nvertex detector, which is responsible for reconstructing particle tracks and\nfragmentation vertexes with high spatial resolution. The study evaluates the\ndetector's reconstruction algorithm and its efficiency to detect particles.\nSecond the thesis present a preliminary calculation of fragmentation cross\nsection, incorporating the vertex detector for the first time in these\nmeasurements.",
        "While cloud-cloud collisions (CCCs) have been proposed as a mechanism for\ntriggering massive star formation, it is suggested that higher collision\nvelocities ($v_{\\rm col}$) and lower GMC mass ($M_{\\rm GMC}$) or\/and density\n($\\Sigma_{\\rm GMC}$) tend to suppress star formation. In this study, we choose\nthe nearby barred galaxy NGC 3627 to examine the SFR and SFE of a colliding GMC\n($m^\\star_{\\rm CCC}$ and $\\epsilon_{\\rm CCC}$) and explore the connections\nbetween $m^\\star_{\\rm CCC}$ and $\\epsilon_{\\rm CCC}$, $M_{\\rm\nGMC}$($\\Sigma_{\\rm GMC}$) and $v_{\\rm col}$, and galactic structures (disk,\nbar, and bar-end). Using ALMA CO(2--1) data (60~pc resolution), we estimated\n$v_{\\rm col}$ within 500~pc apertures, based on line-of-sight GMC velocities,\nassuming random motion in a two-dimensional plane. We extracted apertures where\nat least 0.1 collisions occur per 1 Myr, identifying them as regions dominated\nby CCC-driven star formation, and then calculated $m^\\star_{\\rm CCC}$ and\n$\\epsilon_{\\rm CCC}$ using attenuation-corrected H$\\alpha$ data from VLT MUSE.\nWe found that both $m^\\star_{\\rm CCC}$ and $\\epsilon_{\\rm CCC}$ are lower in\nthe bar (median values: $10^{3.84}~M_\\odot$ and $0.18~\\%$), and higher in the\nbar-end ($10^{4.89}~M_\\odot$ and $1.10~\\%$) compared to the disk\n($10^{4.28}~M_\\odot$ and $0.75~\\%$). Furthermore, we found that structural\ndifferences within the parameter space of $v_{\\rm col}$ and $M_{\\rm\nGMC}$($\\Sigma_{\\rm GMC}$), with higher $M_{\\rm GMC}$($\\Sigma_{\\rm GMC}$) in the\nbar-end and higher $v_{\\rm col}$ in the bar compared to the disk, lead to\nhigher star formation activity in the bar-end and lower activity in the bar.\nOur results support the scenario that variations in CCC properties across\ndifferent galactic structures can explain the observed differences in SFE on a\nkpc scale within a disk galaxy."
      ]
    }
  },
  {
    "id":2411.03389,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Attention Is All You Need",
    "start_abstract":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code"
      ],
      "abstract":[
        "A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores."
      ],
      "categories":[
        "astro-ph.HE"
      ]
    },
    "list":{
      "title":[
        "Carpet-3 detection of a photon-like air shower with estimated primary\n  energy above 100 TeV in a spatial and temporal coincidence with GRB 221009A",
        "Observation of discontinuities in the periodic modulation of PSR\n  B1828-11",
        "Forecasting of the time-dependent fluxes of antiprotons in the AMS-02\n  era",
        "Peculiar radio-bright behaviour of the Galactic black hole transient 4U\n  1543-47 in the 2021-2023 outburst",
        "Blazars Jets and prospects for TeV-PeV neutrinos & gamma-rays through\n  cosmic-ray interactions",
        "Nonextensive entropic behavior observed in Quasar 3C 273",
        "A second-scale periodicity in an active repeating fast radio burst\n  source",
        "A Possible Four-Month Periodicity in the Activity of FRB 20240209A",
        "Spectroscopy of Supernova Remnants and Candidates in M31",
        "Comparison of Equations of State for Neutron Stars with First-Order\n  Phase Transitions: A Qualitative Study",
        "Evolution of LISA Observables for Binary Black Holes Lensed by an SMBH",
        "Discovery of the variable optical counterpart of the redback pulsar PSR\n  J2055+1545",
        "Photon-ALP beam propagation from Mrk 501",
        "Investigating Evolving Wormholes in $f(R,T)$ Gravity",
        "Bounded-Confidence Models of Multi-Dimensional Opinions with\n  Topic-Weighted Discordance",
        "Separation of the initial conditions in the inverse problem for 1D\n  non-linear tsunami wave run-up theory",
        "Energy Dispersion, Superconductivity and Magnetic Fluctuations in\n  Stacked Altermagnetism Materials",
        "SpecPT (Spectroscopy Pre-trained Transformer) Model for Extragalactic\n  Spectroscopy: I. Architecture and Automated Redshift Measurement",
        "Time-resolved second-order autocorrelation function of parametric\n  downconversion",
        "FinRLlama: A Solution to LLM-Engineered Signals Challenge at FinRL\n  Contest 2024",
        "Different physical and numerical sources of scatter in the\n  $M_{\\star}$-$M_{\\mathrm{BH}}$ relation and their connection to galaxy\n  evolution",
        "Membrane Charge Effects on Solute Transport in Polyamide Membranes",
        "Cycle Patterns and Mean Payoff Games",
        "Machine Learning-Driven Analytical Models for Threshold Displacement\n  Energy Prediction in Materials",
        "Non-Variational Quantum Random Access Optimization with Alternating\n  Operator Ansatz",
        "Rhizaform algebras",
        "Conical Targets for Enhanced High-Current Positron Sources",
        "Electroweak diboson production in association with a high-mass dijet\n  system in semileptonic final states from $pp$ collisions at $\\sqrt{s} = 13$\n  TeV with the ATLAS detector"
      ],
      "abstract":[
        "The brightest cosmic gamma-ray burst (GRB) ever detected, GRB 221009A, was\naccompanied by photons of very high energies. These gamma rays may be used to\ntest both the astrophysical models of the burst and our understanding of\nlong-distance propagation of energetic photons, including potential new-physics\neffects. Here we present the observation of a photon-like air shower with the\nestimated primary energy of $300^{+43}_{-38}$ TeV, coincident (with the chance\nprobability of $\\sim 9\\cdot 10^{-3}$) with the GRB in its arrival direction and\ntime. Making use of the upgraded Carpet-3 muon detector and new machine\nlearning analysis, we estimate the probability that the primary was hadronic as\n$\\sim 3 \\cdot 10^{-4}$. This is the highest-energy event ever associated with\nany GRB.",
        "PSR B1828-11 is a radio pulsar that undergoes periodic modulations (~500\ndays) of its spin-down rate and beam width, providing a valuable opportunity to\nunderstand the rotational dynamics of neutron stars. The periodic modulations\nhave previously been attributed to planetary companion(s), precession, or\nmagnetospheric effects and have several interesting features: they persist over\n10 cycles, there are at least two harmonically related components, and the\nperiod is decreasing at a rate of about 5 days per cycle. PSR B1828-11 also\nexperienced a glitch, a sudden increase in its rotation frequency, at 55 040.9\nModified Julian Day(MJD). By studying the interaction of the periodic\nmodulations with the glitch, we seek to find evidence to distinguish\nexplanations of the periodic modulation. Using a phenomenological model, we\nanalyse a recently published open data set from Jodrell Bank Observatory,\nproviding the longest and highest resolution measurements of the pulsar's\nspin-down rate data. Our phenomenological model consists of step changes in the\namplitude, modulation frequency, and phase of the long-term periodic modulation\nand the usual spin-down glitch behaviour. We find clear evidence with a\n(natural-log) Bayes factor of 1486 to support that not only is there a change\nto these three separate parameters but that the shifts occur before the glitch.\nFinally, we also present model-independent evidence which demonstrates visually\nhow and when the modulation period and amplitude change. Discontinuities in the\nmodulation period are difficult to explain if a planetary companion sources the\nperiodic modulations, but we conclude with a discussion on the insights into\nprecession and magnetospheric switching.",
        "The spectra of galactic cosmic rays (GCRs) contain crucial information about\ntheir origin and propagation through the interstellar medium. When GCRs reach\nEarth, they are significantly influenced by the solar wind and the heliospheric\nmagnetic field, a phenomenon known as solar modulation. This effect introduces\ntime-dependent variations in GCR fluxes. The AMS-02 experiment has released\ntime-dependent flux data for protons, electrons, and positrons, revealing clear\ncorrelations with solar modulation. Studies suggest that cosmic rays with the\nsame charge, such as protons and helium nuclei, exhibit similar\/same solar\nmodulation parameters. In this work, we derive the LIS for protons and\npositrons under the assumption of a common solar modulation potential, using\ndata from Voyager 1 and a 7-year average from AMS-02. Similarly, the LIS for\nantiprotons and electrons is derived by assuming they are governed by a\nseparate solar modulation potential. We demonstrate that the time-dependent\nfluxes of positrons and protons can be accurately modeled using the same set of\nsolar modulation parameters within a modified force-field approximation\nframework. Based on this, we predict the time-dependent fluxes of antiprotons\nusing the corresponding electron flux data.",
        "Correlated behaviours between the radio emission and the X-ray emission in\nGalactic black hole X-ray binaries (BH XRBs) in the X-ray hard state are\ncrucial to the understanding of disc-jet coupling of accreting black holes. The\nBH transient 4U 1543-47 went into outburst in 2021 following ~19 years of\nquiescence. We followed it up with ~weekly cadence with MeerKAT for about one\nyear and a half until it faded into quiescence. Multi-epoch quasi-simultaneous\nMeerKAT and X-ray observations allowed us to trace the compact jet emission and\nits X-ray emission. In its hard spectral state across three orders of magnitude\nof X-ray luminosities above ~10$^{34}$ ergs\/s, we found the correlation between\nradio and X-ray emission had a power-law index of 0.82$\\pm$0.09, steeper than\nthe canonical value of ~0.6 for BH XRBs. In addition, the radio vs. X-ray\ncorrelation shows a large range of the power-law normalization, with the\nmaximum significantly larger than that obtained for most BH XRBs, indicating it\ncan be particularly radio-bright and variable in the X-ray binary sample. The\nradio emission is unlikely diluted by discrete jet components. The observed\npeculiar radio-bright and variable behaviours provide the evidence for the\nrelativistic effects of a variable Lorentz factor in the range between 1 and ~2\nof the compact jet.",
        "This study explores the origins of cosmic rays and their secondary\nmessengers, focusing on the potential role of four BL Lacs W Comae, 1ES\n1959+650, PKS 2005-489, and PKS 2155-304 as potential sources of astrophysical\nneutrinos and gamma rays. We analyzed a single-zone model to understand the\ninteractions between high-energy protons and ambient photons within blazar\njets, leading to neutrino production observables and gamma-ray emission. This\nmodeling contextualizes the emissions within multiwavelength observations and\nevaluates the capabilities of the next-generation Cherenkov Telescope Array\nObservatory (CTAO) in detecting these emissions. Our estimations suggest that\nthese sources could be effective emitters of CRs, highlighting the need for\nfuture multimessenger observations to further investigate and constrain this\nclass of sources.",
        "We investigate the flux intensities spanning from radio waves to X-rays\nacross 39 light curves of Quasar 3C 273, utilizing publicly available data\ncollected by the Integral Science Data Centre (ISDC) database. Our results\nsuggest that Quasar 3C 273 exhibits nonextensive behavior. Furthermore, we\ncalculate the $q$ entropic indices for these light curves using the\n$q$-Gaussian distribution with a predominant observation of cases where $q>1$.\nBased on this index, we estimate the non-extensive entropy ($S_{q}$) and\nexplore its correlation with the energy (in eV). In this context, we identify\ntwo jump-like increases in entropy, particularly evident in the infrared (IR)\nand X-ray wavebands. The peak in the far-IR band, around 0.34 eV, results from\nsynchrotron flares evolving from higher to lower energies and thermal radiation\nemitted by hot dust near the sublimation radius. However, the second entropic\npeak in the hard X-ray range lacks statistical robustness due to limited data\nor large measurement uncertainties.",
        "Fast radio bursts (FRBs) are fierce radio flashes from the deep sky. Abundant\nobservations have indicated that highly magnetized neutron stars might be\ninvolved in these energetic bursts, but the underlying trigger mechanism is\nstill enigmatic. Especially, the widely expected periodicity connected to the\nspin of the central engine has never been discovered, which leads to further\ndebates on the nature of FRBs. Here we report the first discovery of a $\\sim\n1.7$ s period in the repeating source of FRB 20201124A. This is an active\nrepeater, from which more than 2800 FRBs have been observed on a total of 49\ndays. The phase-folding method is adopted to analyze the bursts on each day\nseparately. While no periodical signal is found in all other datasets, a clear\nperiodicity does appear on two specific days, i.e. a period of $1.706015(2)$ s\non MJD 59310, and a slightly larger period of $1.707972(1)$ s on MJD 59347. A\nperiod derivative of $6.14\\times10^{-10}$ s s$^{-1}$ can be derived from these\ntwo periods, which further implies a surface magnetic field strength of\n$1.04\\times10^{15}$ G and a spin-down age of $44$ years for the central engine.\nIt is thus concluded that FRB 20201124A should be associated with a young\nmagnetar.",
        "Fast Radio Bursts (FRBs) are millisecond-duration radio transients from\ndistant galaxies. While most FRBs are singular events, repeaters emit multiple\nbursts, with only two-FRB 121102 and FRB 180916B-showing periodic activity (160\nand 16 days, respectively). FRB 20240209A, discovered by CHIME-FRB, is\nlocalized to the outskirts of a quiescent elliptical galaxy (z = 0.1384). We\ndiscovered a periodicity of ~ 126 days in the activity of the FRB 20240209A,\npotentially adding to the list of extremely rare periodic repeating FRBs. We\nused auto-correlation and Lomb-Scargle periodogram analyses, validated with\nrandomized control samples, to confirm the periodicity. The FRB's location in\nan old stellar population disfavors young progenitor models, instead pointing\nto scenarios involving globular clusters, late-stage magnetars, or low-mass\nX-ray binaries (LMXBs). Though deep X-ray or polarimetric observations are not\navailable, the localization of the FRB and a possible periodicity points to\nprogenitors likely to be a binary involving a compact object and a stellar\ncompanion or a precessing or rotating old neutron star.",
        "With a star formation rate of order 0.4 M$_\\odot $ yr$^{-1}$, M31 should have\nsignificant population of supernova remnants (SNRs), and, in fact, 156 SNR and\nSNR candidates have been suggested by Lee et al. (2014) by searching for\nnebulae with elevated [SII]\/H${\\alpha}$ ratios in narrow band images. Here we\nuse a combination of low and high resolution optical spectroscopy obtained with\nHectospec on the MMT to characterize 152 of these nebulae. Of these candidates,\nwe find 93 nebulae that have [SII]\/H${\\alpha}$ ratios that exceed 0.4, the\ntraditional ratio used to separate SNRs from HII regions, strongly suggesting\nthat at least these objects are SNRs. Our high resolution spectroscopy reveals\n108 nebulae that have velocity widths in H${\\alpha} $ (full-width at 20% peak\nflux) that exceed 50 km s$^{-1}$, significantly larger than found in HII\nregions. There are 72 objects that satisfy both tests. Here we discuss the\nspectroscopic characteristics of all of the objects in our sample, and the\nlikelihood that other objects in the sample of Lee et al. are also SNRs, and we\nbriefly consider confirmation by X-ray, radio and UV observations. We also\ndiscuss several new candidates that have been identified serendipitously in the\ncourse of examining a large amount of archival Hectospec data.",
        "The equation of state is fundamental in describing matter under the extreme\nconditions characteristic of neutron stars and is central to advancing our\nunderstanding of dense matter physics. A critical challenge, however, lies in\naccurately modelling first-order phase transitions while ensuring thermodynamic\nconsistency and aligning with astrophysical observations. This study explores\ntwo frameworks for constructing EoSs with first-order phase transitions: the\npolytropic interpolation method and the randomized speed-of-sound interpolation\napproach. It is found that the mass-radius relation and pressure vs. energy\ndensity relation are blind towards the thermodynamic consistency check. The\npolytropic interpolation method can exhibit discontinuities in the chemical\npotential for first-order phase transition, raising concerns regarding\npotential causality violations and thermodynamic inconsistencies. In contrast,\nthe speed of sound interpolation approach ensures continuity in the chemical\npotential, offering a more thermodynamically consistent and reliable framework.\nMoreover, the sound speed method effectively captures the softer segment of the\nmass-radius spectrum, a capability not achieved by the consistent\npiecewise-polytropic approach due to its monotonic stiffness constraints. The\nspeed of sound definition involving number density and chemical potential\nreveals the thermodynamic inconsistency, making it a more consistent and robust\ndefinition. These findings underscore the importance of thermodynamic\nconsistency in EoS construction and highlight the advantages of the randomized\nspeed-of-sound method for modelling phase transitions in dense matter.",
        "Binary black holes (BBH) are expected to form and merge in active galactic\nnuclei (AGN), deep in the potential well of a supermassive black hole (SMBH),\nfrom populations that exist in a nuclear star cluster (NSC). Here we\ninvestigate the gravitational wave (GW) signature of a BBH lensed by a nearby\nSMBH. For a fiducial GW150914-like BBH orbiting close to a $10^{8}M_{\\odot}$\nSMBH located at $z=0.1$, the lensed GW signal varies in a predictable manner in\nand out of the LISA detectability band and across frequencies. The occurrence\nof such signatures has the potential to confound LISA global fit models if they\nare not modelled. Detection of these sources provide an independent measure of\nAGN inclination angles, along with detecting warping of the inner disk, and\nmeasuring the SMBH spin.",
        "We present the discovery of the variable optical counterpart to PSR\nJ2055+1545, a redback millisecond pulsar, and the first radial velocity curve\nof its companion star. The multi-band optical light curves of this system show\na $0.4$$-$$0.6 \\ \\mathrm{mag}$ amplitude modulation with a single peak per\norbit and variable colours, suggesting that the companion is mildly irradiated\nby the pulsar wind. We find that the flux maximum is asymmetric and occurs at\norbital phase $\\simeq0.4$, anticipating the superior conjunction of the\ncompanion (where the optical emission of irradiated redback companions is\ntypically brightest). We ascribe this asymmetry, well fit with a hot spot in\nour light curve modelling, to irradiation from the intrabinary shock between\npulsar and companion winds. The optical spectra obtained with the \\textit{Gran\nTelescopio Canarias} reveal a G-dwarf companion star with temperatures of $5749\n\\pm 34 \\ \\mathrm{K}$ and $6106 \\pm 35 \\ \\mathrm{K}$ at its inferior and\nsuperior orbital conjunctions, respectively, and a radial velocity\nsemi-amplitude of $385 \\pm 3 \\ \\mathrm{km}\\ \\mathrm{s}^{-1}$. Our best-fit\nmodel yields a neutron star mass of $1.7^{+0.4}_{-0.1} \\ \\mathrm{M_{sun}}$ and\na companion mass of $0.29^{+0.07}_{-0.01} \\ \\mathrm{M_{sun}}$. Based on the\nclose similarity between the optical light curve of PSR~J2055$+$1545 and those\nobserved from PSR J1023+0038 and PSR J1227-4853 during their rotation-powered\nstates, we suggest this system may develop an accretion disc in the future and\nmanifest as a transitional millisecond pulsar.",
        "The very high energy (VHE, E $>$ $100 \\mathrm~{GeV}$) $\\gamma$-ray\nobservations offer a possibility of indirectly detecting the presence of\naxion-like particles (ALPs). The paper focuses on detecting photon-ALP\noscillations on $\\gamma$-ray spectra from distant sources in astrophysical\nmagnetic fields. Strong evidence indicates that: (1) the photon-ALP\noscillations can effectively decrease the photon absorption at energies of\nseveral tens of TeV -- caused by the extragalactic background light (EBL) -- to\na level able to explain better the observational data; (2) the impact of\nmagnetic-field models in photon-ALP beams crossing several magnetized media is\nsignificant. We revisit the expected signature for the photon-ALP oscillation\neffects on $\\gamma-\\gamma $ absorption in the TeV spectra of Mrk 501. The\nresult issues that the photon-ALP beam propagation with mass\n$\\mathrm{m_a}\\sim10^{-10} eV$ and two-photon coupling constant\n$\\begin{aligned}g_{a\\gamma}\\sim0.417\\times10^{-11}GeV^{-1}\\end{aligned}$\ncrossing reasonable magnetic field scenarios considered here can roughly\nreproduce the observed TeV $\\gamma$-ray spectra for Mrk 501.",
        "The present work examines whether evolving wormhole solution is possible or\nnot in $f(R,T)$ modified gravity theory. In the background of inhomogeneous\nFLRW type wormhole configuration the field equations are investigated for\ndifferent choices of scale factors and shape functions. For the power law and\nexponential choice of the scale factor from cosmological context and decoupled\npower law of $f(R,T)$ in each variable, wormhole configuration has been\nexamined for two viable choices of shape function. Energy conditions are\nexamined graphically for a range of values of the parameters involved. Finally,\nthe possibility of emergent scenario at early cosmic evolution has been\nexamined.",
        "People's opinions on a wide range of topics often evolve over time through\ntheir interactions with others. Models of opinion dynamics primarily focus on\none-dimensional opinions which represent opinions on one topic. However,\nopinions on various topics are rarely isolated; instead, they can be\ninterdependent and exhibit correlations. In a bounded-confidence model (BCM) of\nopinion dynamics, agents influence each other's opinions only if their opinions\nare sufficiently similar. We extend classical agent-based BCMs -- namely, the\nHegeselmann--Krause BCM, which has synchronous interactions, and the\nDeffuant--Weisbuch BCM, which has asynchronous interactions -- to a\nmultidimensional setting, in which the opinions are multidimensional vectors\nrepresenting opinions of different topics and opinions on different topics are\ninterdependent. To measure opinion differences between agents, we introduce\ntopic-weighted discordance functions that account for opinion differences in\nall topics. We use the regions of receptiveness to characterize the\nsteady-state opinion clusters and provide an analytical approach to compute\nthese regions. In addition, we numerically simulate our models on various\nnetworks with initial opinions drawn from a variety of distributions. When\ninitial opinions are correlated across different topics, our topic-weighted\nBCMs yield significantly different results in both transient and steady states\ncompared to baseline models, where the dynamics of each opinion topic are\nindependent.",
        "We investigate the inverse tsunami wave problem within the framework of the\n1D nonlinear shallow water equations (SWE). Specifically, we focus on\ndetermining the initial displacement $\\eta_0(x)$ and velocity $u_0(x)$ of the\nwave, given the known motion of the shoreline $R(t)$ (the wet\/dry free\nboundary). We demonstrate that for power-shaped inclined bathymetries, this\nproblem admits a complete solution for any $\\eta_0$ and $u_0$, provided the\nwave does not break. In particular, we show that the knowledge of $R(t)$\nenables the unique recovery of both $\\eta_0(x$) and $u_0(x)$ in terms of the\nAbel transform.\n  It is important to note that, in contrast to the direct problem (also known\nas the tsunami wave run-up problem), where $R(t)$ can be computed exactly only\nfor $u_0(x)=0$, our algorithm can recover $\\eta_0$ and $u_0$ exactly for any\nnon-zero $u_0$. This highlights an interesting asymmetry between the direct and\ninverse problems. Our results extend the work presented in\n\\cite{Rybkin23,Rybkin24}, where the inverse problem was solved for $u_0(x)=0$.\nAs in previous work, our approach utilizes the Carrier-Greenspan\ntransformation, which linearizes the SWE for inclined bathymetries. Extensive\nnumerical experiments confirm the efficiency of our algorithms.",
        "Recently, altermagnetism (AM) has emerged as a new category of magnetism,\nalongside conventional antiferromagnetism (AFM) and ferromagnetism (FM). In an\nAM, superconductivity (SC) is faced with a dilemma that the spin-polarized\nbands, induced by the broken time reversal (T ) symmetry, dominantly supports\nspin-triplet pairing. In contrast, AM spin fluctuations routinely facilitate\nspin-singlet pairing as in AFM. Consequently, unconventional SC is either\nabsent or weak in AM materials. Here, we propose that stacking 2D AM materials\ncould resolve this dilemma. Stacked 2D materials have yielded a variety of new\nelectronic properties by altering the symmetries inherent in the monolayer. In\na 2D anisotropic Hubbard model, we investigate the general energy dispersions\nof both single-layer and stacked AM materials. We demonstrate that AM sheet\nstacking can alter the original symmetries, consequently affecting the energy\ndispersion. The interlayer magnetic coupling enhances the low q magnetic\nfluctuations. T symmetry is restored in the AA stacking with an\nantiferromagnetic interlayer coupling, and then both the energy dispersion and\npairing interaction are in favor of spin-singlet SC. The ferromagnetic\ninterlayer coupling in the AB stacking not only recovers T symmetry but also\nsupports spin-triplet pairing. It is further anticipated that twisted bilayer\nAM sheets could exhibit additional novel electronic properties, including\ntopology, flat bands, and collective excitations. Our work illustrates that\nstacking sheets of AM materials could open up a unique research domain in\nexploring novel quantum phenomena and offer a fertile ground for potential\nelectronic applications.",
        "We introduce the Spectroscopy Pre-trained Transformer (SpecPT), a\ntransformer-based model designed to analyze spectroscopic data, with\napplications in spectrum reconstruction and redshift measurement. Using the\nEarly Data Release (EDR) of the DESI survey, we evaluate SpecPT's performance\non two distinct datasets: the Bright Galaxy Survey (BGS) and Emission Line\nGalaxy (ELG) samples. SpecPT successfully reconstructs spectra, accurately\ncapturing emission lines, absorption features, and continuum shapes while\neffectively reducing noise. For redshift prediction, SpecPT achieves\ncompetitive accuracy, with Normalized Median Absolute Deviation (NMAD) values\nof 0.0006 and 0.0008, and catastrophic outlier fractions of 0.20% and 0.80% for\nBGS and ELG, respectively. Notably, SpecPT performs consistently well across\nthe full redshift range ($0 < z < 1.6$), demonstrating its versatility and\nrobustness. By leveraging its learned latent representations, SpecPT lays the\ngroundwork for a foundational spectroscopic model, with potential applications\nin outlier detection, interstellar medium (ISM) property estimation, and\ntransfer learning to other datasets. This work represents a first step in\nbuilding a generalized framework for spectroscopic analysis, capable of scaling\nto the full DESI dataset and beyond.",
        "We study a possibility of measuring the time-resolved second-order\nautocorrelation function of one of two beams generated in type-II parametric\ndownconversion by means of temporal magnification of this beam, bringing its\ncorrelation time from the picosecond to the nanosecond scale, which can be\nresolved by modern photodetectors. We show that such a measurement enables one\nto infer directly the degree of global coherence of that beam, which is linked\nby a simple relation to the number of modes characterizing the entanglement\nbetween the two generated beams. We illustrate the proposed method by an\nexample of photon pairs generated in a periodically poled KTP crystal with a\nsymmetric group velocity matching for various durations of the pump pulse,\nresulting in different numbers of modes. Our theoretical model also shows that\nthe magnified double-heralded autocorrelation function of one beam exhibits a\nlocal maximum around zero delay time, corresponding to photon bunching at a\nshort time scale.",
        "In response to Task II of the FinRL Challenge at ACM ICAIF 2024, this study\nproposes a novel prompt framework for fine-tuning large language models (LLM)\nwith Reinforcement Learning from Market Feedback (RLMF). Our framework\nincorporates market-specific features and short-term price dynamics to generate\nmore precise trading signals. Traditional LLMs, while competent in sentiment\nanalysis, lack contextual alignment for financial market applications. To\nbridge this gap, we fine-tune the LLaMA-3.2-3B-Instruct model using a custom\nRLMF prompt design that integrates historical market data and reward-based\nfeedback. Our evaluation shows that this RLMF-tuned framework outperforms\nbaseline methods in signal consistency and achieving tighter trading outcomes;\nawarded as winner of Task II. You can find the code for this project on GitHub.",
        "Observations have established that the masses of supermassive black holes\n(SMBHs) correlate tightly with the stellar masses of their host galaxies,\nalbeit with substantial scatter. The size of this scatter as a function of\ngalaxy mass and redshift contains valuable information about the origin of\nSMBHs and the physical nature of their co-evolution with galaxies. In this\nwork, we highlight this connection by studying the scatter in the\n$M_{\\mathrm{BH}} - M_{\\star}$ relation for massive galaxies in the Illustris,\nIllustrisTNG (TNG), and EAGLE cosmological simulations. We find that the\nscatter in TNG is significantly lower than in Illustris and EAGLE, reflecting\ntheir different BH feedback models. By performing various numerical\nexperiments, we quantify different contributions to the scatter in the\nsimulations, and also identify a suitably defined intrinsic scatter. The\nintrinsic scatter in Illustris and EAGLE is $\\sim0.3$ dex at $z=0$, and is\ndominated by variations from BH accretion, whereas the smaller scatter of TNG\nis rather dominated by hierarchical merging, suggesting that the massive\ngalaxies in TNG are more tightly quenched. Variations in the BH seed mass can\ncontribute to the scatter of the $M_{\\rm BH}-M_{\\star}$ relation as well, but\nwhether this still plays a role at $z=0$ depends on the feedback model.\nSimulations with disabled AGN feedback produce much higher scatter for low-mass\ngalaxies than seen in our cosmological simulations, demonstrating the crucial\ninfluence of feedback for determining the co-evolution of SMBHs and their host\ngalaxies in this regime. In contrast, an important factor in reducing the\nscatter for massive galaxies is hierarchical merging of mostly quenched\nsystems. Based on our results, we expect that the scatter in the\n$M_{\\mathrm{BH}} - M_{\\star}$ relation at high redshift could be particularly\npowerful in providing clues to the origin of SMBHs.",
        "Polyamide membranes, such as nanofiltration (NF) and reverse osmosis (RO)\nmembranes, are widely used for water desalination and purification. However,\nthe mechanisms of solute transport and solute rejection due to charge\ninteractions remain unclear at the molecular level. Here we use molecular\ndynamics (MD) simulations to examine the transport of single-solute feeds\nthrough charged nanofiltration membranes with different membrane charge\nconcentrations of COO$^{\\text{-}}$ and NH$_2\\!^+$ corresponding to different pH\nlevels. Results show that Na$^+$ and Cl$^{\\text{-}}$ solute ions are better\nrejected when the membrane has a higher concentration of negatively charged\ngroups, corresponding to a higher pH, whereas CaCl$_2$ is well-rejected at all\npH levels studied. These results are consistent with experimental findings\nwhich are performed at the same pH conditions as simulation setup. Moreover,\nsolute transport behavior depends on the membrane functional group\ndistribution. When COO$^{\\text{-}}$ functional groups are concentrated at\nmembrane feed surface, ion permeation into the membrane is reduced.\nCounter-ions tend to associate with charged functional groups while co-ions\nseem to pass by the charged groups more easily. In addition, steric effects\nplay a role when ions of opposite charge cluster in pores of the membrane. This\nstudy reveals solute transport and rejection mechanisms related to membrane\ncharge and provides insights into how membranes might be designed to achieve\nspecific desired solute rejection.",
        "We introduce the concept of a \\emph{cycle pattern} for directed graphs as\nfunctions from the set of cycles to the set $\\{-,0,+\\}$. The key example for\nsuch a pattern is derived from a weight function, giving rise to the sign of\nthe total weight of the edges for each cycle. Hence, cycle patterns describe a\nfundamental structure of a weighted digraph, and they arise naturally in games\non graphs, in particular parity games, mean payoff games, and energy games.\n  Our contribution is threefold: we analyze the structure and derive hardness\nresults for the realization of cycle patterns by weight functions. Then we use\nthem to show hardness of solving games given the limited information of a cycle\npattern. Finally, we identify a novel geometric hardness measure for solving\nmean payoff games (MPG) using the framework of linear decision trees, and use\ncycle patterns to derive lower bounds with respect to this measure, for large\nclasses of algorithms for MPGs.",
        "Understanding the behavior of materials under irradiation is crucial for the\ndesign and safety of nuclear reactors, spacecraft, and other radiation\nenvironments. The threshold displacement energy (Ed) is a critical parameter\nfor understanding radiation damage in materials, yet its determination often\nrelies on costly experiments or simulations. This work leverages the machine\nlearning-based Sure Independence Screening and Sparsifying Operator (SISSO)\nmethod to derive accurate, analytical models for predicting Ed using\nfundamental material properties. The models outperform traditional approaches\nfor monoatomic materials, capturing key trends with high accuracy. While\npredictions for polyatomic materials highlight challenges due to dataset\ncomplexity, they reveal opportunities for improvement with expanded data. This\nstudy identifies cohesive energy and melting temperature as key factors\ninfluencing Ed, offering a robust framework for efficient, data-driven\npredictions of radiation damage in diverse materials.",
        "Solving hard optimization problems is one of the most promising application\ndomains for quantum computers due to the ubiquity of such problems in industry\nand the availability of broadly applicable quantum speedups. However, the\nability of near-term quantum computers to tackle industrial-scale optimization\nproblems is limited by their size and the overheads of quantum error\ncorrection. Quantum Random Access Optimization (QRAO) has been proposed to\nreduce the space requirements of quantum optimization. However, to date QRAO\nhas only been implemented using variational algorithms, which suffer from the\nneed to train instance-specific variational parameters, making them difficult\nto scale. We propose and benchmark a non-variational approach to QRAO based on\nthe Quantum Alternating Operator Ansatz (QAOA) for the MaxCut problem. We show\nthat instance-independent ``fixed'' parameters achieve good performance,\nremoving the need for variational parameter optimization. Additionally, we\nevaluate different design choices, such as various mixers and initial states,\nas well as QAOA operator implementations when customizing for QRAO, and\nidentify a strategy that performs well in practice. Our results pave the way\nfor the practical execution of QRAO on early fault-tolerant quantum computers.",
        "Any anti-associative algebra gives rise to a Jacobi-Jordan algebra by [x, y]\n= xy + yx. This article aims to introduce the concept of \"rhizaform algebras\",\nwhich offer an approach to addressing anti-associativity. These algebras are\ndefined by two operations whose sum is anti-associative, with the left and\nright multiplication operators forming bimodules of the sum of anti-associative\nalgebras. This characterization parallels that of dendriform algebras, where\nthe sum of operations preserves associativity. Additionally, the notions of\nO-operators and Rota-Baxter operators on anti-associative algebras are\npresented as tools to interpret rhizaform algebras. Notably, anti-associative\nalgebras with nondegenerate Connes cocycles admit compatible rhizaform algebra\nstructures.",
        "Previous pair-production-driven positron source designs have assumed that the\ntransverse dimension of the target is significantly greater than the secondary\nbeam it generates. This paper explores the use of targets with different\ntransverse profiles with the aim of enhancing positron production. The starting\npoint of this research is the concept of wire targets, proposed by M. James et\nal. in 1991 for the former SLC positron source. Building on this foundation,\nthis study takes this concept a step further by introducing conical-shaped\ntargets, which can substantially improve the yield by reducing the reabsorption\nof positrons by the target--an issue that is worsened by the high-field\nsolenoid lenses commonly used for positron capture. Using Geant4 simulations,\nwe propose new conical targets adapted for the parameters of the future\ncollider FCC-ee and its positron source test facility P-cubed (PSI Positron\nProduction experiment) at the Paul Scherrer Institute. We find that conical\ntargets can nearly double the positron production at the target and enhance the\nbaseline positron yield of FCC-ee by around 60%. Additionally, we present the\nthermo-mechanical studies for the conical targets based on the FCC-ee primary\nbeam power requirements and outline the mechanical implementation for a future\nproof-of-principle demonstration at the P-cubed facility.",
        "This paper reports the observation of electroweak diboson ($WW\/WZ\/ZZ$)\nproduction in association with a high-mass dijet system, in which final states\nwith one boson decaying leptonically and the other boson decaying hadronically\nare studied. The hadronically decaying $W\/Z$ boson is reconstructed as either\ntwo small-radius jets or one large-radius jet with jet substructure\nrequirements. The data analyzed correspond to an integrated luminosity of 140\nfb$^{-1}$ of proton-proton collisions at a center-of-mass energy of\n$\\sqrt{s}=13$ TeV collected with the ATLAS detector during the 2015-2018 data\ntaking at the Large Hadron Collider. The electroweak production of $WW\/WZ\/ZZ$\nin association with two jets is observed in a phase space dominated by\nvector-boson scattering with a significance of $7.4\\sigma$ (expected\n$6.1\\sigma$) and the signal strength is determined to be\n$1.28^{+0.23}_{-0.21}$. The corresponding production cross section in a\nfiducial phase space is measured in addition. The signal strengths of both\nelectroweak and QCD associated diboson productions are furthermore measured in\na two-dimensional fit, the result of which agrees with the Standard Model\nprediction. The data are interpreted in the context of a dimension-8 effective\nfield theory to probe anomalous quartic gauge couplings resulting in the first\nset of exclusion limits on the Wilson coefficients in the semileptonic channel\nreported by the ATLAS Collaboration. The observed limits for the S02, T0 and M0\noperators are $(-3.96 < f_{S02} \/ \\Lambda^4 < 3.96)$ TeV$^{-4}$, $(-0.25 <\nf_{T0} \/ \\Lambda^4 < 0.22)$ TeV$^{-4}$, $(-1.26 < f_{M0} \/ \\Lambda^4 < 1.25)$\nTeV$^{-4}$."
      ]
    }
  },
  {
    "id":2411.03389,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code",
    "start_abstract":"A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores.",
    "start_categories":[
      "astro-ph.HE"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Attention Is All You Need"
      ],
      "abstract":[
        "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Principles for Responsible AI Consciousness Research",
        "Agentic Mixture-of-Workflows for Multi-Modal Chemical Search",
        "Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts",
        "Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs\n  under Noisy Observations",
        "L2R: Learning to Reduce Search Space for Generalizable Neural Routing\n  Solver",
        "A consensus set for the aggregation of partial rankings: the case of the\n  Optimal Set of Bucket Orders Problem",
        "Cost-Saving LLM Cascades with Early Abstention",
        "Agentic AI Needs a Systems Theory",
        "Towards A Litmus Test for Common Sense",
        "Hierarchical Expert Prompt for Large-Language-Model: An Approach Defeat\n  Elite AI in TextStarCraft II for the First Time",
        "KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity\n  Recognition and Normalization for Dysmorphology Physical Examination Reports",
        "Theorem Prover as a Judge for Synthetic Data Generation",
        "NS-Gym: Open-Source Simulation Environments and Benchmarks for\n  Non-Stationary Markov Decision Processes",
        "Types of elements in non-commutative Poisson algebras and Dixmier\n  Conjecture",
        "Deep Learning and Foundation Models for Weather Prediction: A Survey",
        "CacheMamba: Popularity Prediction for Mobile Edge Caching Networks via\n  Selective State Spaces",
        "Solving the Catastrophic Forgetting Problem in Generalized Category\n  Discovery",
        "Motion planning for highly-dynamic unconditioned reflexes based on\n  chained Signed Distance Functions",
        "Diffusion-Based Imitation Learning for Social Pose Generation",
        "Hochschild cohomology and extensions of triangulated categories",
        "Provably-Stable Neural Network-Based Control of Nonlinear Systems",
        "Towards a Digital Twin Modeling Method for Container Terminal Port",
        "Multiport Support for Vortex OpenGPU Memory Hierarchy",
        "Anomize: Better Open Vocabulary Video Anomaly Detection",
        "Piano Transcription by Hierarchical Language Modeling with Pretrained\n  Roll-based Encoders",
        "On the limit of random hives with GUE boundary conditions",
        "Transformer Semantic Genetic Programming for Symbolic Regression",
        "Mutation Testing via Iterative Large Language Model-Driven Scientific\n  Debugging"
      ],
      "abstract":[
        "Recent research suggests that it may be possible to build conscious AI\nsystems now or in the near future. Conscious AI systems would arguably deserve\nmoral consideration, and it may be the case that large numbers of conscious\nsystems could be created and caused to suffer. Furthermore, AI systems or\nAI-generated characters may increasingly give the impression of being\nconscious, leading to debate about their moral status. Organisations involved\nin AI research must establish principles and policies to guide research and\ndeployment choices and public communication concerning consciousness. Even if\nan organisation chooses not to study AI consciousness as such, it will still\nneed policies in place, as those developing advanced AI systems risk\ninadvertently creating conscious entities. Responsible research and deployment\npractices are essential to address this possibility. We propose five principles\nfor responsible research and argue that research organisations should make\nvoluntary, public commitments to principles on these lines. Our principles\nconcern research objectives and procedures, knowledge sharing and public\ncommunications.",
        "The vast and complex materials design space demands innovative strategies to\nintegrate multidisciplinary scientific knowledge and optimize materials\ndiscovery. While large language models (LLMs) have demonstrated promising\nreasoning and automation capabilities across various domains, their application\nin materials science remains limited due to a lack of benchmarking standards\nand practical implementation frameworks. To address these challenges, we\nintroduce Mixture-of-Workflows for Self-Corrective Retrieval-Augmented\nGeneration (CRAG-MoW) - a novel paradigm that orchestrates multiple agentic\nworkflows employing distinct CRAG strategies using open-source LLMs. Unlike\nprior approaches, CRAG-MoW synthesizes diverse outputs through an orchestration\nagent, enabling direct evaluation of multiple LLMs across the same problem\ndomain. We benchmark CRAG-MoWs across small molecules, polymers, and chemical\nreactions, as well as multi-modal nuclear magnetic resonance (NMR) spectral\nretrieval. Our results demonstrate that CRAG-MoWs achieve performance\ncomparable to GPT-4o while being preferred more frequently in comparative\nevaluations, highlighting the advantage of structured retrieval and multi-agent\nsynthesis. By revealing performance variations across data types, CRAG-MoW\nprovides a scalable, interpretable, and benchmark-driven approach to optimizing\nAI architectures for materials discovery. These insights are pivotal in\naddressing fundamental gaps in benchmarking LLMs and autonomous AI agents for\nscientific applications.",
        "This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a\nbenchmark designed to evaluate the effectiveness of Multi-modal Large Language\nModels (MLLMs) in leveraging knowledge from multi-modal retrieval documents.\nThe benchmark comprises four tasks: image captioning, multi-modal question\nanswering, multi-modal fact verification, and image reranking. All tasks are\nset in an open-domain setting, requiring RAG models to retrieve query-relevant\ninformation from a multi-modal document collection and use it as input context\nfor RAG modeling. To enhance the context utilization capabilities of MLLMs, we\nalso introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an\ninstruction tuning method that optimizes MLLMs within multi-modal contexts. Our\nexperiments show that MM-RAIT improves the performance of RAG systems by\nenabling them to effectively learn from multi-modal contexts. All data and code\nare available at https:\/\/github.com\/NEUIR\/M2RAG.",
        "Inductive reasoning, a cornerstone of human cognition, enables generalization\nfrom limited data but hasn't yet been fully achieved by large language models\n(LLMs). While modern LLMs excel at reasoning tasks, their ability to maintain\nstable and consistent rule abstraction under imperfect observations remains\nunderexplored. To fill this gap, in this work, we introduce Robust Rule\nInduction, a task that evaluates LLMs' capability in inferring rules from data\nthat are fused with noisy examples. To address this task, we further propose\nSample-steered Rule Refinement (SRR), a method enhancing reasoning stability\nvia observation diversification and execution-guided feedback. Experiments\nacross arithmetic, cryptography, and list functions reveal: (1) SRR outperforms\nother methods with minimal performance degradation under noise; (2) Despite\nslight accuracy variation, LLMs exhibit instability under noise (e.g., 0%\naccuracy change with only 70% consistent score); (3) Counterfactual task gaps\nhighlight LLMs' reliance on memorized patterns over genuine abstraction. Our\nfindings challenge LLMs' reasoning robustness, revealing susceptibility to\nhypothesis drift and pattern overfitting, while providing empirical evidence\ncritical for developing human-like inductive systems. Code and data are\navailable at\n\\href{https:\/\/github.com\/lcy2723\/Robust-Rule-Induction}{https:\/\/github.com\/lcy2723\/Robust-Rule-Induction}.",
        "Constructive neural combinatorial optimization (NCO) has attracted growing\nresearch attention due to its ability to solve complex routing problems without\nrelying on handcrafted rules. However, existing NCO methods face significant\nchallenges in generalizing to large-scale problems due to high computational\ncomplexity and inefficient capture of structural patterns. To address this\nissue, we propose a novel learning-based search space reduction method that\nadaptively selects a small set of promising candidate nodes at each step of the\nconstructive NCO process. Unlike traditional methods that rely on fixed\nheuristics, our selection model dynamically prioritizes nodes based on learned\npatterns, significantly reducing the search space while maintaining solution\nquality. Experimental results demonstrate that our method, trained solely on\n100-node instances from uniform distribution, generalizes remarkably well to\nlarge-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) instances with up to 1 million nodes from the uniform\ndistribution and over 80K nodes from other distributions.",
        "In rank aggregation problems (RAP), the solution is usually a consensus\nranking that generalizes a set of input orderings. There are different variants\nthat differ not only in terms of the type of rankings that are used as input\nand output, but also in terms of the objective function employed to evaluate\nthe quality of the desired output ranking. In contrast, in some machine\nlearning tasks (e.g. subgroup discovery) or multimodal optimization tasks,\nattention is devoted to obtaining several models\/results to account for the\ndiversity in the input data or across the search landscape. Thus, in this paper\nwe propose to provide, as the solution to an RAP, a set of rankings to better\nexplain the preferences expressed in the input orderings. We exemplify our\nproposal through the Optimal Bucket Order Problem (OBOP), an RAP which consists\nin finding a single consensus ranking (with ties) that generalizes a set of\ninput rankings codified as a precedence matrix. To address this, we introduce\nthe Optimal Set of Bucket Orders Problem (OSBOP), a generalization of the OBOP\nthat aims to produce not a single ranking as output but a set of consensus\nrankings. Experimental results are presented to illustrate this proposal,\nshowing how, by providing a set of consensus rankings, the fitness of the\nsolution significantly improves with respect to the one of the original OBOP,\nwithout losing comprehensibility.",
        "LLM cascades are based on the idea that processing all queries with the\nlargest and most expensive LLMs is inefficient. Instead, cascades deploy small\nLLMs to answer the majority of queries, limiting the use of large and expensive\nLLMs to only the most difficult queries. This approach can significantly reduce\ncosts without impacting performance. However, risk-sensitive domains such as\nfinance or medicine place an additional premium on avoiding model errors.\nRecognizing that even the most expensive models may make mistakes, applications\nin these domains benefit from allowing LLM systems to completely abstain from\nanswering a query when the chance of making a mistake is significant. However,\ngiving a cascade the ability to abstain poses an immediate design question for\nLLM cascades: should abstention only be allowed at the final model or also at\nearlier models? Since the error patterns of small and large models are\ncorrelated, the latter strategy may further reduce inference costs by letting\ninexpensive models anticipate abstention decisions by expensive models, thereby\nobviating the need to run the expensive models. We investigate the benefits of\n\"early abstention\" in LLM cascades and find that it reduces the overall test\nloss by 2.2% on average across six benchmarks (GSM8K, MedMCQA, MMLU, TriviaQA,\nTruthfulQA, and XSum). These gains result from a more effective use of\nabstention, which trades a 4.1% average increase in the overall abstention rate\nfor a 13.0% reduction in cost and a 5.0% reduction in error rate. Our findings\ndemonstrate that it is possible to leverage correlations between the error\npatterns of different language models to drive performance improvements for LLM\nsystems with abstention.",
        "The endowment of AI with reasoning capabilities and some degree of agency is\nwidely viewed as a path toward more capable and generalizable systems. Our\nposition is that the current development of agentic AI requires a more\nholistic, systems-theoretic perspective in order to fully understand their\ncapabilities and mitigate any emergent risks. The primary motivation for our\nposition is that AI development is currently overly focused on individual model\ncapabilities, often ignoring broader emergent behavior, leading to a\nsignificant underestimation in the true capabilities and associated risks of\nagentic AI. We describe some fundamental mechanisms by which advanced\ncapabilities can emerge from (comparably simpler) agents simply due to their\ninteraction with the environment and other agents. Informed by an extensive\namount of existing literature from various fields, we outline mechanisms for\nenhanced agent cognition, emergent causal reasoning ability, and metacognitive\nawareness. We conclude by presenting some key open challenges and guidance for\nthe development of agentic AI. We emphasize that a systems-level perspective is\nessential for better understanding, and purposefully shaping, agentic AI\nsystems.",
        "This paper is the second in a planned series aimed at envisioning a path to\nsafe and beneficial artificial intelligence. Building on the conceptual\ninsights of \"Common Sense Is All You Need,\" we propose a more formal litmus\ntest for common sense, adopting an axiomatic approach that combines minimal\nprior knowledge (MPK) constraints with diagonal or Godel-style arguments to\ncreate tasks beyond the agent's known concept set. We discuss how this approach\napplies to the Abstraction and Reasoning Corpus (ARC), acknowledging\ntraining\/test data constraints, physical or virtual embodiment, and large\nlanguage models (LLMs). We also integrate observations regarding emergent\ndeceptive hallucinations, in which more capable AI systems may intentionally\nfabricate plausible yet misleading outputs to disguise knowledge gaps. The\noverarching theme is that scaling AI without ensuring common sense risks\nintensifying such deceptive tendencies, thereby undermining safety and trust.\nAligning with the broader goal of developing beneficial AI without causing\nharm, our axiomatic litmus test not only diagnoses whether an AI can handle\ntruly novel concepts but also provides a stepping stone toward an ethical,\nreliable foundation for future safe, beneficial, and aligned artificial\nintelligence.",
        "Since the emergence of the Large Language Model (LLM), LLM has been widely\nused in fields such as writing, translating, and searching. However, there is\nstill great potential for LLM-based methods in handling complex tasks such as\ndecision-making in the StarCraft II environment. To address problems such as\nlack of relevant knowledge and poor control over subtasks of varying\nimportance, we propose a Hierarchical Expert Prompt (HEP) for LLM. Our method\nimproves the understanding of game situations through expert-level tactical\nknowledge, improving the processing quality of tasks of varying importance\nthrough a hierarchical framework. Our approach defeated the highest level\n(Elite) standard built-in agent in TextStarCraft II for the first time and\nconsistently outperformed the baseline method in other difficulties. Our\nexperiments suggest that the proposed method is a practical solution for\ntackling complex decision-making challenges. The replay video can be viewed on\nhttps:\/\/www.bilibili.com\/video\/BV1uz42187EF and https:\/\/youtu.be\/dO3PshWLV5M,\nand our codes have been open-sourced on\nhttps:\/\/github.com\/luchang1113\/HEP-LLM-play-StarCraftII.",
        "The objective of BioCreative8 Track 3 is to extract phenotypic key medical\nfindings embedded within EHR texts and subsequently normalize these findings to\ntheir Human Phenotype Ontology (HPO) terms. However, the presence of diverse\nsurface forms in phenotypic findings makes it challenging to accurately\nnormalize them to the correct HPO terms. To address this challenge, we explored\nvarious models for named entity recognition and implemented data augmentation\ntechniques such as synonym marginalization to enhance the normalization step.\nOur pipeline resulted in an exact extraction and normalization F1 score 2.6\\%\nhigher than the mean score of all submissions received in response to the\nchallenge. Furthermore, in terms of the normalization F1 score, our approach\nsurpassed the average performance by 1.9\\%. These findings contribute to the\nadvancement of automated medical data extraction and normalization techniques,\nshowcasing potential pathways for future research and application in the\nbiomedical domain.",
        "The demand for synthetic data in mathematical reasoning has increased due to\nits potential to enhance the mathematical capabilities of large language models\n(LLMs). However, ensuring the validity of intermediate reasoning steps remains\na significant challenge, affecting data quality. While formal verification via\ntheorem provers effectively validates LLM reasoning, the autoformalisation of\nmathematical proofs remains error-prone. In response, we introduce iterative\nautoformalisation, an approach that iteratively refines theorem prover\nformalisation to mitigate errors, thereby increasing the execution rate on the\nLean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as\na Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to\nrigorously assess LLM intermediate reasoning, effectively integrating\nautoformalisation with synthetic data generation. Finally, we present\nReinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that\nreplaces human annotation with theorem prover feedback in Reinforcement\nLearning from Human Feedback (RLHF). Across multiple LLMs, applying\nTP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving\n5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for\nSVAMP, and 3.55% on Llama-3.1-8B for AQUA.",
        "In many real-world applications, agents must make sequential decisions in\nenvironments where conditions are subject to change due to various exogenous\nfactors. These non-stationary environments pose significant challenges to\ntraditional decision-making models, which typically assume stationary dynamics.\nNon-stationary Markov decision processes (NS-MDPs) offer a framework to model\nand solve decision problems under such changing conditions. However, the lack\nof standardized benchmarks and simulation tools has hindered systematic\nevaluation and advance in this field. We present NS-Gym, the first simulation\ntoolkit designed explicitly for NS-MDPs, integrated within the popular\nGymnasium framework. In NS-Gym, we segregate the evolution of the environmental\nparameters that characterize non-stationarity from the agent's decision-making\nmodule, allowing for modular and flexible adaptations to dynamic environments.\nWe review prior work in this domain and present a toolkit encapsulating key\nproblem characteristics and types in NS-MDPs. This toolkit is the first effort\nto develop a set of standardized interfaces and benchmark problems to enable\nconsistent and reproducible evaluation of algorithms under non-stationary\nconditions. We also benchmark six algorithmic approaches from prior work on\nNS-MDPs using NS-Gym. Our vision is that NS-Gym will enable researchers to\nassess the adaptability and robustness of their decision-making algorithms to\nnon-stationary conditions.",
        "Non-commutative Poisson algebras are the algebras having an associative\nalgebra structure and a Lie algebra structure together with the Leibniz law.\nLet $P$ be a non-commutative Poisson algebra over some algebraically closed\nfield of characteristic zero. For any $z\\in P$, there exist four subalgebras of\n$P$ associated with the inner derivation $ad_z$ on $P$. Based on the\nrelationships between these four subalgebras, elements of $P$ can be divided\ninto eight types. We will mainly focus on two types of non-commutative Poisson\nalgebras: the usual Poisson algebras and the associative algebras with the\ncommutator as the Poisson bracket. The following problems are studied for such\nnon-commutative Poisson algebras: how the type of an element changes under\nhomomorphisms between non-commutative Poisson algebras, how the type of an\nelement changes after localization, and what the type of the elements of the\nform $z_1 \\otimes z_2$ and $z_1 \\otimes 1 + 1 \\otimes z_2$ is in the tensor\nproduct of non-commutative Poisson algebras $P_1\\otimes P_2$. As an application\nof above results, one knows that Dixmier Conjecture for $A_1$ holds under\ncertain conditions. Some properties of the Weyl algebras are also obtained,\nsuch as the commutativity of certain subalgebras.",
        "Physics-based numerical models have been the bedrock of atmospheric sciences\nfor decades, offering robust solutions but often at the cost of significant\ncomputational resources. Deep learning (DL) models have emerged as powerful\ntools in meteorology, capable of analyzing complex weather and climate data by\nlearning intricate dependencies and providing rapid predictions once trained.\nWhile these models demonstrate promising performance in weather prediction,\noften surpassing traditional physics-based methods, they still face critical\nchallenges. This paper presents a comprehensive survey of recent deep learning\nand foundation models for weather prediction. We propose a taxonomy to classify\nexisting models based on their training paradigms: deterministic predictive\nlearning, probabilistic generative learning, and pre-training and fine-tuning.\nFor each paradigm, we delve into the underlying model architectures, address\nmajor challenges, offer key insights, and propose targeted directions for\nfuture research. Furthermore, we explore real-world applications of these\nmethods and provide a curated summary of open-source code repositories and\nwidely used datasets, aiming to bridge research advancements with practical\nimplementations while fostering open and trustworthy scientific practices in\nadopting cutting-edge artificial intelligence for weather prediction. The\nrelated sources are available at https:\/\/github.com\/JimengShi\/\nDL-Foundation-Models-Weather.",
        "Mobile Edge Caching (MEC) plays a pivotal role in mitigating latency in\ndata-intensive services by dynamically caching frequently requested content on\nedge servers. This capability is critical for applications such as Augmented\nReality (AR), Virtual Reality (VR), and Autonomous Vehicles (AV), where\nefficient content caching and accurate popularity prediction are essential for\noptimizing performance. In this paper, we explore the problem of popularity\nprediction in MEC by utilizing historical time-series request data of intended\nfiles, formulating this problem as a ranking task. To this aim, we propose\nCacheMamba model by employing Mamba, a state-space model (SSM)-based\narchitecture, to identify the top-K files with the highest likelihood of being\nrequested. We then benchmark the proposed model against a Transformer-based\napproach, demonstrating its superior performance in terms of cache-hit rate,\nMean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG), and\nFloating-Point Operations Per Second (FLOPS), particularly when dealing with\nlonger sequences.",
        "Generalized Category Discovery (GCD) aims to identify a mix of known and\nnovel categories within unlabeled data sets, providing a more realistic setting\nfor image recognition. Essentially, GCD needs to remember existing patterns\nthoroughly to recognize novel categories. Recent state-of-the-art method SimGCD\ntransfers the knowledge from known-class data to the learning of novel classes\nthrough debiased learning. However, some patterns are catastrophically forgot\nduring adaptation and thus lead to poor performance in novel categories\nclassification. To address this issue, we propose a novel learning approach,\nLegoGCD, which is seamlessly integrated into previous methods to enhance the\ndiscrimination of novel classes while maintaining performance on previously\nencountered known classes. Specifically, we design two types of techniques\ntermed as Local Entropy Regularization (LER) and Dual-views Kullback Leibler\ndivergence constraint (DKL). The LER optimizes the distribution of potential\nknown class samples in unlabeled data, thus ensuring the preservation of\nknowledge related to known categories while learning novel classes. Meanwhile,\nDKL introduces Kullback Leibler divergence to encourage the model to produce a\nsimilar prediction distribution of two view samples from the same image. In\nthis way, it successfully avoids mismatched prediction and generates more\nreliable potential known class samples simultaneously. Extensive experiments\nvalidate that the proposed LegoGCD effectively addresses the known category\nforgetting issue across all datasets, eg, delivering a 7.74% and 2.51% accuracy\nboost on known and novel classes in CUB, respectively. Our code is available\nat: https:\/\/github.com\/Cliffia123\/LegoGCD.",
        "The unconditioned reflex (e.g., protective reflex), which is the innate\nreaction of the organism and usually performed through the spinal cord rather\nthan the brain, can enable organisms to escape harms from environments. In this\npaper, we propose an online, highly-dynamic motion planning algorithm to endow\nmanipulators the highly-dynamic unconditioned reflexes to humans and\/or\nenvironments. Our method is based on a chained version of Signed Distance\nFunctions (SDFs), which can be pre-computed and stored. Our proposed algorithm\nis divided into two stages. In the offline stage, we create 3 groups of local\nSDFs to store the geometric information of the manipulator and its working\nenvironment. In the online stage, the pre-computed local SDFs are chained\ntogether according the configuration of the manipulator, to provide global\ngeometric information about the environment. While the point clouds of the\ndynamic objects serve as query points to look up these local SDFs for quickly\ngenerating escape velocity. Then we propose a modified geometric Jacobian\nmatrix and use the Jacobian-pseudo-inverse method to generate real-time reflex\nbehaviors to avoid the static and dynamic obstacles in the environment. The\nbenefits of our method are validated in both static and dynamic scenarios. In\nthe static scenario, our method identifies the path solutions with lower time\nconsumption and shorter trajectory length compared to existing solutions. In\nthe dynamic scenario, our method can reliably pursue the dynamic target point,\navoid dynamic obstacles, and react to these obstacles within 1ms, which\nsurpasses the unconditioned reflex reaction time of humans.",
        "Intelligent agents, such as robots and virtual agents, must understand the\ndynamics of complex social interactions to interact with humans. Effectively\nrepresenting social dynamics is challenging because we require multi-modal,\nsynchronized observations to understand a scene. We explore how using a single\nmodality, the pose behavior, of multiple individuals in a social interaction\ncan be used to generate nonverbal social cues for the facilitator of that\ninteraction. The facilitator acts to make a social interaction proceed smoothly\nand is an essential role for intelligent agents to replicate in human-robot\ninteractions. In this paper, we adapt an existing diffusion behavior cloning\nmodel to learn and replicate facilitator behaviors. Furthermore, we evaluate\ntwo representations of pose observations from a scene, one representation has\npre-processing applied and one does not. The purpose of this paper is to\nintroduce a new use for diffusion behavior cloning for pose generation in\nsocial interactions. The second is to understand the relationship between\nperformance and computational load for generating social pose behavior using\ntwo different techniques for collecting scene observations. As such, we are\nessentially testing the effectiveness of two different types of conditioning\nfor a diffusion model. We then evaluate the resulting generated behavior from\neach technique using quantitative measures such as mean per-joint position\nerror (MPJPE), training time, and inference time. Additionally, we plot\ntraining and inference time against MPJPE to examine the trade-offs between\nefficiency and performance. Our results suggest that the further pre-processed\ndata can successfully condition diffusion models to generate realistic social\nbehavior, with reasonable trade-offs in accuracy and processing time.",
        "We define a notion of categorical first order deformations for (enhanced)\ntriangulated categories. For a category $\\mathcal{T}$, we show that there is a\nbijection between $\\operatorname{HH}^2(\\mathcal{T})$ and the set of categorical\ndeformations of $\\mathcal{T}$. We show that in the case of curved deformations\nof dg algebras considered in arXiv:2406.04945, the $1$-derived category of the\ndeformation (introduced in arXiv:24020.8660) is a categorical deformation of\nthe derived category of the base; the Hochschild class identified by this\ndeformation is shown to restrict to the class defining the deformation of the\nalgebra. As an application, we give a conceptual proof of the fact that (for a\nsmooth base) the filtered derived category of a dg deformation yields a\ncategorical resolution of the classical derived category.",
        "In recent years, Neural Networks (NNs) have been employed to control\nnonlinear systems due to their potential capability in dealing with situations\nthat might be difficult for conventional nonlinear control schemes. However, to\nthe best of our knowledge, the current literature on NN-based control lacks\ntheoretical guarantees for stability and tracking performance. This precludes\nthe application of NN-based control schemes to systems where stringent\nstability and performance guarantees are required. To address this gap, this\npaper proposes a systematic and comprehensive methodology to design\nprovably-stable NN-based control schemes for affine nonlinear systems. Rigorous\nanalysis is provided to show that the proposed approach guarantees stability of\nthe closed-loop system with the NN in the loop. Also, it is shown that the\nresulting NN-based control scheme ensures that system states asymptotically\nconverge to a neighborhood around the desired equilibrium point, with a tunable\nproximity threshold. The proposed methodology is validated and evaluated via\nsimulation studies on an inverted pendulum and experimental studies on a Parrot\nBebop 2 drone.",
        "This paper introduces a novel strategy aimed at enhancing productivity and\nminimizing non-productive movements within container terminals, specifically\nfocusing on container yards. It advocates for the implementation of a digital\ntwin-based methodology to streamline the operations of stacking cranes (SCs)\nresponsible for container handling. The proposed approach entails the creation\nof a virtual container yard that mirrors the physical yard within a digital\ntwin system, facilitating real-time observation and validation. In addition,\nthis article demonstrates the effectiveness of using a digital twin to reduce\nunproductive movements and improve productivity through simulation. It defines\nvarious operational strategies and takes into account different yard contexts,\nproviding a comprehensive understanding of optimisation possibilities. By\nexploiting the capabilities of the digital twin, managers and operators are\nprovided with crucial information on operational dynamics, enabling them to\nidentify areas for improvement. This visualisation helps decision-makers to\nmake informed choices about their stacking strategies, thereby improving the\nefficiency of overall container terminal operations. Overall, this paper\npresent a digital twin solution in container terminal operations, offering a\npowerful tool for optimising productivity and minimising inefficiencies.",
        "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead.",
        "Open Vocabulary Video Anomaly Detection (OVVAD) seeks to detect and classify\nboth base and novel anomalies. However, existing methods face two specific\nchallenges related to novel anomalies. The first challenge is detection\nambiguity, where the model struggles to assign accurate anomaly scores to\nunfamiliar anomalies. The second challenge is categorization confusion, where\nnovel anomalies are often misclassified as visually similar base instances. To\naddress these challenges, we explore supplementary information from multiple\nsources to mitigate detection ambiguity by leveraging multiple levels of visual\ndata alongside matching textual information. Furthermore, we propose\nincorporating label relations to guide the encoding of new labels, thereby\nimproving alignment between novel videos and their corresponding labels, which\nhelps reduce categorization confusion. The resulting Anomize framework\neffectively tackles these issues, achieving superior performance on UCF-Crime\nand XD-Violence datasets, demonstrating its effectiveness in OVVAD.",
        "Automatic Music Transcription (AMT), aiming to get musical notes from raw\naudio, typically uses frame-level systems with piano-roll outputs or language\nmodel (LM)-based systems with note-level predictions. However, frame-level\nsystems require manual thresholding, while the LM-based systems struggle with\nlong sequences. In this paper, we propose a hybrid method combining pre-trained\nroll-based encoders with an LM decoder to leverage the strengths of both\nmethods. Besides, our approach employs a hierarchical prediction strategy,\nfirst predicting onset and pitch, then velocity, and finally offset. The\nhierarchical prediction strategy reduces computational costs by breaking down\nlong sequences into different hierarchies. Evaluated on two benchmark\nroll-based encoders, our method outperforms traditional piano-roll outputs 0.01\nand 0.022 in onset-offset-velocity F1 score, demonstrating its potential as a\nperformance-enhancing plug-in for arbitrary roll-based music transcription\nencoder.",
        "We show that hives chosen at random with independent GUE boundary conditions\non two sides, weighted by a Vandermonde factor depending on the third side\n(which is necessary in the context of the randomized Horn problem), when\nnormalized so that the eigenvalues at the edge are asymptotically constant,\nconverge in probability to a continuum hive as $n \\rightarrow \\infty.$ It had\npreviously been shown in joint work with Sheffield and Tao \\cite{NST} that the\nvariance of these scaled random hives tends to $0$ and consequently, from\ncompactness, that they converge in probability subsequentially. In the present\npaper, building on \\cite{NST}, we prove convergence in probability to a single\ncontinuum hive, without having to pass to a subsequence. We moreover show that\nthe value at a given point $v$ of this continuum hive equals the supremum of a\ncertain functional acting on asymptotic height functions of lozenge tilings.",
        "In standard genetic programming (stdGP), solutions are varied by modifying\ntheir syntax, with uncertain effects on their semantics. Geometric-semantic\ngenetic programming (GSGP), a popular variant of GP, effectively searches the\nsemantic solution space using variation operations based on linear\ncombinations, although it results in significantly larger solutions. This paper\npresents Transformer Semantic Genetic Programming (TSGP), a novel and flexible\nsemantic approach that uses a generative transformer model as search operator.\nThe transformer is trained on synthetic test problems and learns semantic\nsimilarities between solutions. Once the model is trained, it can be used to\ncreate offspring solutions with high semantic similarity also for unseen and\nunknown problems. Experiments on several symbolic regression problems show that\nTSGP generates solutions with comparable or even significantly better\nprediction quality than stdGP, SLIM_GSGP, DSR, and DAE-GP. Like SLIM_GSGP, TSGP\nis able to create new solutions that are semantically similar without creating\nsolutions of large size. An analysis of the search dynamic reveals that the\nsolutions generated by TSGP are semantically more similar than the solutions\ngenerated by the benchmark approaches allowing a better exploration of the\nsemantic solution space.",
        "Large Language Models (LLMs) can generate plausible test code. Intuitively\nthey generate this by imitating tests seen in their training data, rather than\nreasoning about execution semantics. However, such reasoning is important when\napplying mutation testing, where individual tests need to demonstrate\ndifferences in program behavior between a program and specific artificial\ndefects (mutants). In this paper, we evaluate whether Scientific Debugging,\nwhich has been shown to help LLMs when debugging, can also help them to\ngenerate tests for mutants. In the resulting approach, LLMs form hypotheses\nabout how to kill specific mutants, and then iteratively generate and refine\ntests until they succeed, all with detailed explanations for each step. We\ncompare this method to three baselines: (1) directly asking the LLM to generate\ntests, (2) repeatedly querying the LLM when tests fail, and (3) search-based\ntest generation with Pynguin. Our experiments evaluate these methods based on\nseveral factors, including mutation score, code coverage, success rate, and the\nability to identify equivalent mutants. The results demonstrate that LLMs,\nalthough requiring higher computation cost, consistently outperform Pynguin in\ngenerating tests with better fault detection and coverage. Importantly, we\nobserve that the iterative refinement of test cases is important for achieving\nhigh-quality test suites."
      ]
    }
  },
  {
    "id":2411.04682,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Distributed Sky Imaging Radiometry and Tomography",
    "start_abstract":"The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data.",
    "start_categories":[
      "astro-ph.EP"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "Variable Imaging Projection Cloud Scattering Tomography"
      ],
      "abstract":[
        "Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Perception-as-Control: Fine-grained Controllable Image Animation with\n  3D-aware Motion Representation",
        "A Light Perspective for 3D Object Detection",
        "Surgical Scene Understanding in the Era of Foundation AI Models: A\n  Comprehensive Review",
        "PolarFree: Polarization-based Reflection-free Imaging",
        "Shazam: Unifying Multiple Foundation Models for Advanced Computational\n  Pathology",
        "Emergence of Painting Ability via Recognition-Driven Evolution",
        "RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes",
        "Feature-based One-For-All: A Universal Framework for Heterogeneous\n  Knowledge Distillation",
        "myEye2Wheeler: A Two-Wheeler Indian Driver Real-World Eye-Tracking\n  Dataset",
        "D2GV: Deformable 2D Gaussian Splatting for Video Representation in\n  400FPS",
        "High Resolution Tree Height Mapping of the Amazon Forest using Planet\n  NICFI Images and LiDAR-Informed U-Net Model",
        "ActiveInitSplat: How Active Image Selection Helps Gaussian Splatting",
        "Hybrid State-Space and GRU-based Graph Tokenization Mamba for\n  Hyperspectral Image Classification",
        "Codes with symmetric distances",
        "RobotIQ: Empowering Mobile Robots with Human-Level Planning for\n  Real-World Execution",
        "UniDemoir\\'e: Towards Universal Image Demoir\\'eing with Data Generation\n  and Synthesis",
        "How Vital is the Jurisprudential Relevance: Law Article Intervened Legal\n  Case Retrieval and Matching",
        "GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for\n  Autonomous Driving",
        "Enhanced Derivative-Free Optimization Using Adaptive Correlation-Induced\n  Finite Difference Estimators",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Recovering Partially Corrupted Major Objects through Tri-modality Based\n  Image Completion",
        "Designing VR Simulation System for Clinical Communication Training with\n  LLMs-Based Embodied Conversational Agents",
        "On the asymptotic validity of confidence sets for linear functionals of\n  solutions to integral equations",
        "On the approaching geodesics property",
        "Multilevel Generative Samplers for Investigating Critical Phenomena",
        "Empowering LLMs with Logical Reasoning: A Comprehensive Survey",
        "Multi-View Depth Consistent Image Generation Using Generative AI Models:\n  Application on Architectural Design of University Buildings",
        "Universal Chern classes on the moduli of bundles"
      ],
      "abstract":[
        "Motion-controllable image animation is a fundamental task with a wide range\nof potential applications. Recent works have made progress in controlling\ncamera or object motion via various motion representations, while they still\nstruggle to support collaborative camera and object motion control with\nadaptive control granularity. To this end, we introduce 3D-aware motion\nrepresentation and propose an image animation framework, called\nPerception-as-Control, to achieve fine-grained collaborative motion control.\nSpecifically, we construct 3D-aware motion representation from a reference\nimage, manipulate it based on interpreted user instructions, and perceive it\nfrom different viewpoints. In this way, camera and object motions are\ntransformed into intuitive and consistent visual changes. Then, our framework\nleverages the perception results as motion control signals, enabling it to\nsupport various motion-related video synthesis tasks in a unified and flexible\nway. Experiments demonstrate the superiority of the proposed approach. For more\ndetails and qualitative results, please refer to our anonymous project webpage:\nhttps:\/\/chen-yingjie.github.io\/projects\/Perception-as-Control.",
        "Comprehending the environment and accurately detecting objects in 3D space\nare essential for advancing autonomous vehicle technologies. Integrating Camera\nand LIDAR data has emerged as an effective approach for achieving high accuracy\nin 3D Object Detection models. However, existing methodologies often rely on\nheavy, traditional backbones that are computationally demanding. This paper\nintroduces a novel approach that incorporates cutting-edge Deep Learning\ntechniques into the feature extraction process, aiming to create more efficient\nmodels without compromising performance. Our model, NextBEV, surpasses\nestablished feature extractors like ResNet50 and MobileNetV2. On the KITTI 3D\nMonocular detection benchmark, NextBEV achieves an accuracy improvement of\n2.39%, having less than 10% of the MobileNetV3 parameters. Moreover, we propose\nchanges in LIDAR backbones that decreased the original inference time to 10 ms.\nAdditionally, by fusing these lightweight proposals, we have enhanced the\naccuracy of the VoxelNet-based model by 2.93% and improved the F1-score of the\nPointPillar-based model by approximately 20%. Therefore, this work contributes\nto establishing lightweight and powerful models for individual or fusion\ntechniques, making them more suitable for onboard implementations.",
        "Recent advancements in machine learning (ML) and deep learning (DL),\nparticularly through the introduction of foundational models (FMs), have\nsignificantly enhanced surgical scene understanding within minimally invasive\nsurgery (MIS). This paper surveys the integration of state-of-the-art ML and DL\ntechnologies, including Convolutional Neural Networks (CNNs), Vision\nTransformers (ViTs), and foundational models like the Segment Anything Model\n(SAM), into surgical workflows. These technologies improve segmentation\naccuracy, instrument tracking, and phase recognition in surgical endoscopic\nvideo analysis. The paper explores the challenges these technologies face, such\nas data variability and computational demands, and discusses ethical\nconsiderations and integration hurdles in clinical settings. Highlighting the\nroles of FMs, we bridge the technological capabilities with clinical needs and\noutline future research directions to enhance the adaptability, efficiency, and\nethical alignment of AI applications in surgery. Our findings suggest that\nsubstantial progress has been made; however, more focused efforts are required\nto achieve seamless integration of these technologies into clinical workflows,\nensuring they complement surgical practice by enhancing precision, reducing\nrisks, and optimizing patient outcomes.",
        "Reflection removal is challenging due to complex light interactions, where\nreflections obscure important details and hinder scene understanding.\nPolarization naturally provides a powerful cue to distinguish between reflected\nand transmitted light, enabling more accurate reflection removal. However,\nexisting methods often rely on small-scale or synthetic datasets, which fail to\ncapture the diversity and complexity of real-world scenarios. To this end, we\nconstruct a large-scale dataset, PolaRGB, for Polarization-based reflection\nremoval of RGB images, which enables us to train models that generalize\neffectively across a wide range of real-world scenarios. The PolaRGB dataset\ncontains 6,500 well-aligned mixed-transmission image pairs, 8x larger than\nexisting polarization datasets, and is the first to include both RGB and\npolarization images captured across diverse indoor and outdoor environments\nwith varying lighting conditions. Besides, to fully exploit the potential of\npolarization cues for reflection removal, we introduce PolarFree, which\nleverages diffusion process to generate reflection-free cues for accurate\nreflection removal. Extensive experiments show that PolarFree significantly\nenhances image clarity in challenging reflective scenarios, setting a new\nbenchmark for polarized imaging and reflection removal. Code and dataset are\navailable at https:\/\/github.com\/mdyao\/PolarFree.",
        "Foundation Models (FMs) in computational pathology (CPath) have significantly\nadvanced the extraction of meaningful features from histopathology image\ndatasets, achieving strong performance across various clinical tasks. Despite\ntheir impressive performance, these models often exhibit variability when\napplied to different tasks, prompting the need for a unified framework capable\nof consistently excelling across various applications. In this work, we propose\nShazam, a novel framework designed to efficiently combine multiple CPath\nmodels. Unlike previous approaches that train a fixed-parameter FM, Shazam\ndynamically extracts and refines information from diverse FMs for each specific\ntask. To ensure that each FM contributes effectively without dominance, a novel\ndistillation strategy is applied, guiding the student model with features from\nall teacher models, which enhances its generalization ability. Experimental\nresults on two pathology patch classification datasets demonstrate that Shazam\noutperforms existing CPath models and other fusion methods. Its lightweight,\nflexible design makes it a promising solution for improving CPath analysis in\nreal-world settings. Code will be available at\nhttps:\/\/github.com\/Tuner12\/Shazam.",
        "From Paleolithic cave paintings to Impressionism, human painting has evolved\nto depict increasingly complex and detailed scenes, conveying more nuanced\nmessages. This paper attempts to emerge this artistic capability by simulating\nthe evolutionary pressures that enhance visual communication efficiency.\nSpecifically, we present a model with a stroke branch and a palette branch that\ntogether simulate human-like painting. The palette branch learns a limited\ncolour palette, while the stroke branch parameterises each stroke using\nB\\'ezier curves to render an image, subsequently evaluated by a high-level\nrecognition module. We quantify the efficiency of visual communication by\nmeasuring the recognition accuracy achieved with machine vision. The model then\noptimises the control points and colour choices for each stroke to maximise\nrecognition accuracy with minimal strokes and colours. Experimental results\nshow that our model achieves superior performance in high-level recognition\ntasks, delivering artistic expression and aesthetic appeal, especially in\nabstract sketches. Additionally, our approach shows promise as an efficient\nbit-level image compression technique, outperforming traditional methods.",
        "3D Gaussian Splatting (3DGS) has become a popular solution in SLAM, as it can\nproduce high-fidelity novel views. However, previous GS-based methods primarily\ntarget indoor scenes and rely on RGB-D sensors or pre-trained depth estimation\nmodels, hence underperforming in outdoor scenarios. To address this issue, we\npropose a RGB-only gaussian splatting SLAM method for unbounded outdoor\nscenes--OpenGS-SLAM. Technically, we first employ a pointmap regression network\nto generate consistent pointmaps between frames for pose estimation. Compared\nto commonly used depth maps, pointmaps include spatial relationships and scene\ngeometry across multiple views, enabling robust camera pose estimation. Then,\nwe propose integrating the estimated camera poses with 3DGS rendering as an\nend-to-end differentiable pipeline. Our method achieves simultaneous\noptimization of camera poses and 3DGS scene parameters, significantly enhancing\nsystem tracking accuracy. Specifically, we also design an adaptive scale mapper\nfor the pointmap regression network, which provides more accurate pointmap\nmapping to the 3DGS map representation. Our experiments on the Waymo dataset\ndemonstrate that OpenGS-SLAM reduces tracking error to 9.8\\% of previous 3DGS\nmethods, and achieves state-of-the-art results in novel view synthesis. Project\nPage: https:\/\/3dagentworld.github.io\/opengs-slam\/",
        "Knowledge distillation (KD) involves transferring knowledge from a\npre-trained heavy teacher model to a lighter student model, thereby reducing\nthe inference cost while maintaining comparable effectiveness. Prior KD\ntechniques typically assume homogeneity between the teacher and student models.\nHowever, as technology advances, a wide variety of architectures have emerged,\nranging from initial Convolutional Neural Networks (CNNs) to Vision\nTransformers (ViTs), and Multi-Level Perceptrons (MLPs). Consequently,\ndeveloping a universal KD framework compatible with any architecture has become\nan important research topic. In this paper, we introduce a feature-based\none-for-all (FOFA) KD framework to enable feature distillation across diverse\narchitecture. Our framework comprises two key components. First, we design\nprompt tuning blocks that incorporate student feedback, allowing teacher\nfeatures to adapt to the student model's learning process. Second, we propose\nregion-aware attention to mitigate the view mismatch problem between\nheterogeneous architecture. By leveraging these two modules, effective\ndistillation of intermediate features can be achieved across heterogeneous\narchitectures. Extensive experiments on CIFAR, ImageNet, and COCO demonstrate\nthe superiority of the proposed method.",
        "This paper presents the myEye2Wheeler dataset, a unique resource of\nreal-world gaze behaviour of two-wheeler drivers navigating complex Indian\ntraffic. Most datasets are from four-wheeler drivers on well-planned roads and\nhomogeneous traffic. Our dataset offers a critical lens into the unique visual\nattention patterns and insights into the decision-making of Indian two-wheeler\ndrivers. The analysis demonstrates that existing saliency models, like\nTASED-Net, perform less effectively on the myEye-2Wheeler dataset compared to\nwhen applied on the European 4-wheeler eye tracking datasets (DR(Eye)VE),\nhighlighting the need for models specifically tailored to the traffic\nconditions. By introducing the dataset, we not only fill a significant gap in\ntwo-wheeler driver behaviour research in India but also emphasise the critical\nneed for developing context-specific saliency models. The larger aim is to\nimprove road safety for two-wheeler users and lane-planning to support a\ncost-effective mode of transport.",
        "Implicit Neural Representations (INRs) have emerged as a powerful approach\nfor video representation, offering versatility across tasks such as compression\nand inpainting. However, their implicit formulation limits both\ninterpretability and efficacy, undermining their practicality as a\ncomprehensive solution. We propose a novel video representation based on\ndeformable 2D Gaussian splatting, dubbed D2GV, which aims to achieve three key\nobjectives: 1) improved efficiency while delivering superior quality; 2)\nenhanced scalability and interpretability; and 3) increased friendliness for\ndownstream tasks. Specifically, we initially divide the video sequence into\nfixed-length Groups of Pictures (GoP) to allow parallel training and linear\nscalability with video length. For each GoP, D2GV represents video frames by\napplying differentiable rasterization to 2D Gaussians, which are deformed from\na canonical space into their corresponding timestamps. Notably, leveraging\nefficient CUDA-based rasterization, D2GV converges fast and decodes at speeds\nexceeding 400 FPS, while delivering quality that matches or surpasses\nstate-of-the-art INRs. Moreover, we incorporate a learnable pruning and\nquantization strategy to streamline D2GV into a more compact representation. We\ndemonstrate D2GV's versatility in tasks including video interpolation,\ninpainting and denoising, underscoring its potential as a promising solution\nfor video representation. Code is available at:\nhttps:\/\/github.com\/Evan-sudo\/D2GV.",
        "Tree canopy height is one of the most important indicators of forest biomass,\nproductivity, and ecosystem structure, but it is challenging to measure\naccurately from the ground and from space. Here, we used a U-Net model adapted\nfor regression to map the mean tree canopy height in the Amazon forest from\nPlanet NICFI images at ~4.78 m spatial resolution for the period 2020-2024. The\nU-Net model was trained using canopy height models computed from aerial LiDAR\ndata as a reference, along with their corresponding Planet NICFI images.\nPredictions of tree heights on the validation sample exhibited a mean error of\n3.68 m and showed relatively low systematic bias across the entire range of\ntree heights present in the Amazon forest. Our model successfully estimated\ncanopy heights up to 40-50 m without much saturation, outperforming existing\ncanopy height products from global models in this region. We determined that\nthe Amazon forest has an average canopy height of ~22 m. Events such as logging\nor deforestation could be detected from changes in tree height, and encouraging\nresults were obtained to monitor the height of regenerating forests. These\nfindings demonstrate the potential for large-scale mapping and monitoring of\ntree height for old and regenerating Amazon forests using Planet NICFI imagery.",
        "Gaussian splatting (GS) along with its extensions and variants provides\noutstanding performance in real-time scene rendering while meeting reduced\nstorage demands and computational efficiency. While the selection of 2D images\ncapturing the scene of interest is crucial for the proper initialization and\ntraining of GS, hence markedly affecting the rendering performance, prior works\nrely on passively and typically densely selected 2D images. In contrast, this\npaper proposes `ActiveInitSplat', a novel framework for active selection of\ntraining images for proper initialization and training of GS. ActiveInitSplat\nrelies on density and occupancy criteria of the resultant 3D scene\nrepresentation from the selected 2D images, to ensure that the latter are\ncaptured from diverse viewpoints leading to better scene coverage and that the\ninitialized Gaussian functions are well aligned with the actual 3D structure.\nNumerical tests on well-known simulated and real environments demonstrate the\nmerits of ActiveInitSplat resulting in significant GS rendering performance\nimprovement over passive GS baselines, in the widely adopted LPIPS, SSIM, and\nPSNR metrics.",
        "Hyperspectral image (HSI) classification plays a pivotal role in domains such\nas environmental monitoring, agriculture, and urban planning. However, it faces\nsignificant challenges due to the high-dimensional nature of the data and the\ncomplex spectral-spatial relationships inherent in HSI. Traditional methods,\nincluding conventional machine learning and convolutional neural networks\n(CNNs), often struggle to effectively capture these intricate spectral-spatial\nfeatures and global contextual information. Transformer-based models, while\npowerful in capturing long-range dependencies, often demand substantial\ncomputational resources, posing challenges in scenarios where labeled datasets\nare limited, as is commonly seen in HSI applications. To overcome these\nchallenges, this work proposes GraphMamba, a hybrid model that combines\nspectral-spatial token generation, graph-based token prioritization, and\ncross-attention mechanisms. The model introduces a novel hybridization of\nstate-space modeling and Gated Recurrent Units (GRU), capturing both linear and\nnonlinear spatial-spectral dynamics. GraphMamba enhances the ability to model\ncomplex spatial-spectral relationships while maintaining scalability and\ncomputational efficiency across diverse HSI datasets. Through comprehensive\nexperiments, we demonstrate that GraphMamba outperforms existing\nstate-of-the-art models, offering a scalable and robust solution for complex\nHSI classification tasks.",
        "For a code $C$ in a space with maximal distance $n$, we say that $C$ has\nsymmetric distances if its distance set $S(C)$ is symmetric with respect to $n\n\/ 2$. In this paper, we prove that if $C$ is a binary code with length $2n$,\nconstant weight $n$ and symmetric distances, then \\[\n  |C| \\leq \\binom{2 n - 1}{|S(C)|}. \\] This result can be interpreted using the\nlanguage of Johnson association schemes. More generally, we give a framework to\nstudy codes with symmetric distances in Q-bipartite Q-polynomial association\nschemes, and provide upper bounds for such codes. Moreover, we use number\ntheoretic techniques to determine when the equality holds.",
        "This paper introduces RobotIQ, a framework that empowers mobile robots with\nhuman-level planning capabilities, enabling seamless communication via natural\nlanguage instructions through any Large Language Model. The proposed framework\nis designed in the ROS architecture and aims to bridge the gap between humans\nand robots, enabling robots to comprehend and execute user-expressed text or\nvoice commands. Our research encompasses a wide spectrum of robotic tasks,\nranging from fundamental logical, mathematical, and learning reasoning for\ntransferring knowledge in domains like navigation, manipulation, and object\nlocalization, enabling the application of learned behaviors from simulated\nenvironments to real-world operations. All encapsulated within a modular\ncrafted robot library suite of API-wise control functions, RobotIQ offers a\nfully functional AI-ROS-based toolset that allows researchers to design and\ndevelop their own robotic actions tailored to specific applications and robot\nconfigurations. The effectiveness of the proposed system was tested and\nvalidated both in simulated and real-world experiments focusing on a home\nservice scenario that included an assistive application designed for elderly\npeople. RobotIQ with an open-source, easy-to-use, and adaptable robotic library\nsuite for any robot can be found at https:\/\/github.com\/emmarapt\/RobotIQ.",
        "Image demoir\\'eing poses one of the most formidable challenges in image\nrestoration, primarily due to the unpredictable and anisotropic nature of\nmoir\\'e patterns. Limited by the quantity and diversity of training data,\ncurrent methods tend to overfit to a single moir\\'e domain, resulting in\nperformance degradation for new domains and restricting their robustness in\nreal-world applications. In this paper, we propose a universal image\ndemoir\\'eing solution, UniDemoir\\'e, which has superior generalization\ncapability. Notably, we propose innovative and effective data generation and\nsynthesis methods that can automatically provide vast high-quality moir\\'e\nimages to train a universal demoir\\'eing model. Our extensive experiments\ndemonstrate the cutting-edge performance and broad potential of our approach\nfor generalized image demoir\\'eing.",
        "Legal case retrieval (LCR) aims to automatically scour for comparable legal\ncases based on a given query, which is crucial for offering relevant precedents\nto support the judgment in intelligent legal systems. Due to similar goals, it\nis often associated with a similar case matching (LCM) task. To address them, a\ndaunting challenge is assessing the uniquely defined legal-rational similarity\nwithin the judicial domain, which distinctly deviates from the semantic\nsimilarities in general text retrieval. Past works either tagged\ndomain-specific factors or incorporated reference laws to capture\nlegal-rational information. However, their heavy reliance on expert or\nunrealistic assumptions restricts their practical applicability in real-world\nscenarios. In this paper, we propose an end-to-end model named LCM-LAI to solve\nthe above challenges. Through meticulous theoretical analysis, LCM-LAI employs\na dependent multi-task learning framework to capture legal-rational information\nwithin legal cases by a law article prediction (LAP) sub-task, without any\nadditional assumptions in inference. Besides, LCM-LAI proposes an article-aware\nattention mechanism to evaluate the legal-rational similarity between\nacross-case sentences based on law distribution, which is more effective than\nconventional semantic similarity. Weperform a series of exhaustive experiments\nincluding two different tasks involving four real-world datasets. Results\ndemonstrate that LCM-LAI achieves state-of-the-art performance.",
        "Self-supervised pre-training based on next-token prediction has enabled large\nlanguage models to capture the underlying structure of text, and has led to\nunprecedented performance on a large array of tasks when applied at scale.\nSimilarly, autonomous driving generates vast amounts of spatiotemporal data,\nalluding to the possibility of harnessing scale to learn the underlying\ngeometric and semantic structure of the environment and its evolution over\ntime. In this direction, we propose a geometric and semantic self-supervised\npre-training method, GASP, that learns a unified representation by predicting,\nat any queried future point in spacetime, (1) general occupancy, capturing the\nevolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle\npath through the environment; and (3) distilled high-level features from a\nvision foundation model. By modeling geometric and semantic 4D occupancy fields\ninstead of raw sensor measurements, the model learns a structured,\ngeneralizable representation of the environment and its evolution through time.\nWe validate GASP on multiple autonomous driving benchmarks, demonstrating\nsignificant improvements in semantic occupancy forecasting, online mapping, and\nego trajectory prediction. Our results demonstrate that continuous 4D geometric\nand semantic occupancy prediction provides a scalable and effective\npre-training paradigm for autonomous driving. For code and additional\nvisualizations, see \\href{https:\/\/research.zenseact.com\/publications\/gasp\/.",
        "Gradient-based methods are well-suited for derivative-free optimization\n(DFO), where finite-difference (FD) estimates are commonly used as gradient\nsurrogates. Traditional stochastic approximation methods, such as\nKiefer-Wolfowitz (KW) and simultaneous perturbation stochastic approximation\n(SPSA), typically utilize only two samples per iteration, resulting in\nimprecise gradient estimates and necessitating diminishing step sizes for\nconvergence. In this paper, we first explore an efficient FD estimate, referred\nto as correlation-induced FD estimate, which is a batch-based estimate. Then,\nwe propose an adaptive sampling strategy that dynamically determines the batch\nsize at each iteration. By combining these two components, we develop an\nalgorithm designed to enhance DFO in terms of both gradient estimation\nefficiency and sample efficiency. Furthermore, we establish the consistency of\nour proposed algorithm and demonstrate that, despite using a batch of samples\nper iteration, it achieves the same convergence rate as the KW and SPSA\nmethods. Additionally, we propose a novel stochastic line search technique to\nadaptively tune the step size in practice. Finally, comprehensive numerical\nexperiments confirm the superior empirical performance of the proposed\nalgorithm.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Diffusion models have become widely adopted in image completion tasks, with\ntext prompts commonly employed to ensure semantic coherence by providing\nhigh-level guidance. However, a persistent challenge arises when an object is\npartially obscured in the damaged region, yet its remaining parts are still\nvisible in the background. While text prompts offer semantic direction, they\noften fail to precisely recover fine-grained structural details, such as the\nobject's overall posture, ensuring alignment with the visible object\ninformation in the background. This limitation stems from the inability of text\nprompts to provide pixel-level specificity. To address this, we propose\nsupplementing text-based guidance with a novel visual aid: a casual sketch,\nwhich can be roughly drawn by anyone based on visible object parts. This sketch\nsupplies critical structural cues, enabling the generative model to produce an\nobject structure that seamlessly integrates with the existing background. We\nintroduce the Visual Sketch Self-Aware (VSSA) model, which integrates the\ncasual sketch into each iterative step of the diffusion process, offering\ndistinct advantages for partially corrupted scenarios. By blending\nsketch-derived features with those of the corrupted image, and leveraging text\nprompt guidance, the VSSA assists the diffusion model in generating images that\npreserve both the intended object semantics and structural consistency across\nthe restored objects and original regions. To support this research, we created\ntwo datasets, CUB-sketch and MSCOCO-sketch, each combining images, sketches,\nand text. Extensive qualitative and quantitative experiments demonstrate that\nour approach outperforms several state-of-the-art methods.",
        "VR simulation in Health Professions (HP) education demonstrates huge\npotential, but fixed learning content with little customization limits its\napplication beyond lab environments. To address these limitations in the\ncontext of VR for patient communication training, we conducted a user-centered\nstudy involving semi-structured interviews with advanced HP students to\nunderstand their challenges in clinical communication training and perceptions\nof VR-based solutions. From this, we derived design insights emphasizing the\nimportance of realistic scenarios, simple interactions, and unpredictable\ndialogues. Building on these insights, we developed the Virtual AI Patient\nSimulator (VAPS), a novel VR system powered by Large Language Models (LLMs) and\nEmbodied Conversational Agents (ECAs), supporting dynamic and customizable\npatient interactions for immersive learning. We also provided an example of how\nclinical professors could use user-friendly design forms to create personalized\nscenarios that align with course objectives in VAPS and discuss future\nimplications of integrating AI-driven technologies into VR education.",
        "This paper examines the construction of confidence sets for parameters\ndefined as a linear functional of the solution to an integral equation\ninvolving conditional expectations. We show that any confidence set uniformly\nvalid over a broad class of probability laws, allowing the integral equation to\nbe arbitrarily ill-posed must have, with high probability under some laws, a\ndiameter at least as large as the diameter of the parameter's range over the\nmodel. Additionally, we establish that uniformly consistent estimators of the\nparameter do not exist. We show that, consistent with the weak instruments\nliterature, Wald confidence intervals are not uniformly valid. Furthermore, we\nargue that inverting the score test, a successful approach in that literature,\ndoes not extend to the broader class of parameters considered here. We present\na method for constructing uniformly valid confidence sets in the special case\nwhere all variables are binary and discuss its limitations. Finally, we\nemphasize that developing uniformly valid confidence sets for the general class\nof parameters considered in this paper remains an open problem.",
        "We survey some recent results and open questions on the approaching geodesics\nproperty and its application to the study of the Gromov and horofunction\ncompactifications of a proper geodesic Gromov metric space. We obtain results\non the dynamics of isometries and we exhibit an example of a Gromov hyperbolic\ndomain of $\\mathbb{C}$ which does not satisfy the approaching geodesic\nproperty.",
        "Investigating critical phenomena or phase transitions is of high interest in\nphysics and chemistry, for which Monte Carlo (MC) simulations, a crucial tool\nfor numerically analyzing macroscopic properties of given systems, are often\nhindered by an emerging divergence of correlation length -- known as scale\ninvariance at criticality (SIC) in the renormalization group theory. SIC causes\nthe system to behave the same at any length scale, from which many existing\nsampling methods suffer: long-range correlations cause critical slowing down in\nMarkov chain Monte Carlo (MCMC), and require intractably large receptive fields\nfor generative samplers. In this paper, we propose a Renormalization-informed\nGenerative Critical Sampler (RiGCS) -- a novel sampler specialized for\nnear-critical systems, where SIC is leveraged as an advantage rather than a\nnuisance. Specifically, RiGCS builds on MultiLevel Monte Carlo (MLMC) with Heat\nBath (HB) algorithms, which perform ancestral sampling from low-resolution to\nhigh-resolution lattice configurations with site-wise-independent conditional\nHB sampling. Although MLMC-HB is highly efficient under exact SIC, it suffers\nfrom a low acceptance rate under slight SIC violation. Notably, SIC violation\nalways occurs in finite-size systems, and may induce long-range and\nhigher-order interactions in the renormalized distributions, which are not\nconsidered by independent HB samplers. RiGCS enhances MLMC-HB by replacing a\npart of the conditional HB sampler with generative models that capture those\nresidual interactions and improve the sampling efficiency. Our experiments show\nthat the effective sample size of RiGCS is a few orders of magnitude higher\nthan state-of-the-art generative model baselines in sampling configurations for\n128x128 two-dimensional Ising systems.",
        "Large language models (LLMs) have achieved remarkable successes on various\nnatural language tasks. However, recent studies have found that there are still\nsignificant challenges to the logical reasoning abilities of LLMs. This paper\nsummarizes and categorizes the main challenges into two aspects: (1) Logical\nquestion answering, LLMs often fail to generate the correct answer within\ncomplex logical problem which requires sophisticated deductive, inductive or\nabductive reasoning given a collection of premises and constrains. (2) Logical\nconsistency, LLMs are prone to producing responses contradicting themselves\nacross different questions. For example, a state-of-the-art Macaw\nquestion-answering LLM answers Yes to both questions Is a magpie a bird? and\nDoes a bird have wings? but answers No to Does a magpie have wings?. To\nfacilitate this research direction, we comprehensively investigate the most\ncutting-edge methods and propose detailed taxonomies of these methods.\nSpecifically, to accurately answer complex logic questions, previous methods\ncan be categorized based on reliance on external solvers, prompts, pretraining,\nand fine-tuning. To avoid logical contradictions, we discuss concepts and\nsolutions of various logical consistencies, including implication, negation,\ntransitivity, factuality consistency, and their composites. In addition, we\nreview commonly used benchmark datasets and evaluation metrics, and discuss\npromising research directions, such as extensions to modal logic to account for\nuncertainty, and efficient algorithms satisfying multiple logical consistencies\nsimultaneously.",
        "In the early stages of architectural design, shoebox models are typically\nused as a simplified representation of building structures but require\nextensive operations to transform them into detailed designs. Generative\nartificial intelligence (AI) provides a promising solution to automate this\ntransformation, but ensuring multi-view consistency remains a significant\nchallenge. To solve this issue, we propose a novel three-stage consistent image\ngeneration framework using generative AI models to generate architectural\ndesigns from shoebox model representations. The proposed method enhances\nstate-of-the-art image generation diffusion models to generate multi-view\nconsistent architectural images. We employ ControlNet as the backbone and\noptimize it to accommodate multi-view inputs of architectural shoebox models\ncaptured from predefined perspectives. To ensure stylistic and structural\nconsistency across multi-view images, we propose an image space loss module\nthat incorporates style loss, structural loss and angle alignment loss. We then\nuse depth estimation method to extract depth maps from the generated multi-view\nimages. Finally, we use the paired data of the architectural images and depth\nmaps as inputs to improve the multi-view consistency via the depth-aware 3D\nattention module. Experimental results demonstrate that the proposed framework\ncan generate multi-view architectural images with consistent style and\nstructural coherence from shoebox model inputs.",
        "The goal of this paper is to construct universal cohomology classes on the\nmoduli space of stable bundles over a curve when it is not a fine moduli space,\ni.e. when the rank and degree are not coprime. More precisely, we show that\ncertain Chern classes of the universal bundle on the product of the curve with\nthe moduli stack of bundles lift to the product of the curve with the moduli\nspace of stable bundles."
      ]
    }
  }
]