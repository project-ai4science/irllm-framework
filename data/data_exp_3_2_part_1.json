[
  {
    "id":2411.01019,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms",
    "start_abstract":"Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6",
        "b1"
      ],
      "title":[
        "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
        "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
      ],
      "abstract":[
        "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
        "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
      ],
      "categories":[
        "q-bio.CB",
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Multicellular self-organization in Escherichia coli",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Neuroblastoma: nutritional strategies as supportive care in pediatric\n  oncology",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Ultra-high-energy $\\gamma$-ray emission associated with the tail of a\n  bow-shock pulsar wind nebula",
        "A new algorithm for detecting X-ray shots in Cyg X-1",
        "Feedforward Cancellation of High-Frequency Phase Noise in\n  Frequency-Doubled Lasers",
        "Exact Fluctuating Hydrodynamics of the Scaled Light-Heavy Model",
        "Uniqueness of solutions to elliptic and parabolic equations on metric\n  graphs",
        "A sharper Lyapunov-Katz central limit error bound for i.i.d. summands\n  Zolotarev-close to normal",
        "Unitary Friedberg-Jacquet periods and their twists: Fundamental lemmas",
        "Quantum One-Time Memories from Stateless Hardware, Random Access Codes,\n  and Simple Nonconvex Optimization",
        "A monotonicity-based globalization of the level-set method for inclusion\n  detection",
        "Attention-Based Functional-Group Coarse-Graining: A Deep Learning\n  Framework for Molecular Prediction and Design",
        "Separation control applied to the turbulent flow around a NACA4412 wing\n  section",
        "A Cheeger-type inequality for the drift Laplacian with Wentzell-type\n  boundary condition",
        "Complete intersection algebras with binomial Macaulay dual generator",
        "Time derivative estimates for parabolic $p$-Laplace equations and\n  applications to optimal regularity",
        "A family of convolution operators, part two"
      ],
      "abstract":[
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Neuroblastoma, is a highly heterogeneous pediatric tumour and is responsible\nfor 15% of pediatric cancer-related deaths. The clinical outcomes can vary from\nspontaneous regression to high metastatic disease. This extracranial tumour\narises from a neural crest-derived cell and can harbor different phenotypes.\nIts heterogeneity may result from variations in differentiation states\ninfluenced by genetic and epigenetic factors and individual patient\ncharacteristics. This leads downstream to disruption of homeostasis and a\nmetabolic shift in response to the tumour needs. Nutrition can play a key role\nin influencing various aspects of a tumour behaviour. This review provides an\nin-depth exploration of the aetiology of neuroblastoma and the different\navenues of disease progression, which can be targeted with individualized\nnutrition intervention strategies to improve the well-being of children and\noptimize clinical outcomes.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "In this study, we present a comprehensive analysis of an unidentified\npoint-like ultra-high-energy (UHE) $\\gamma$-ray source, designated as 1LHAASO\nJ1740+0948u, situated in the vicinity of the middle-aged pulsar PSR J1740+1000.\nThe detection significance reached 17.1$\\sigma$ (9.4$\\sigma$) above 25$\\,$TeV\n(100$\\,$TeV). The source energy spectrum extended up to 300$\\,$TeV, which was\nwell fitted by a log-parabola function with $N0 = (1.93\\pm0.23) \\times 10^{-16}\n\\rm{TeV^{-1}\\,cm^{-2}\\,s^{-2}}$, $\\alpha = 2.14\\pm0.27$, and $\\beta =\n1.20\\pm0.41$ at E0 = 30$\\,$TeV. The associated pulsar, PSR J1740+1000, resides\nat a high galactic latitude and powers a bow-shock pulsar wind nebula (BSPWN)\nwith an extended X-ray tail. The best-fit position of the gamma-ray source\nappeared to be shifted by $0.2^{\\circ}$ with respect to the pulsar position. As\nthe (i) currently identified pulsar halos do not demonstrate such offsets, and\n(ii) centroid of the gamma-ray emission is approximately located at the\nextension of the X-ray tail, we speculate that the UHE $\\gamma$-ray emission\nmay originate from re-accelerated electron\/positron pairs that are advected\naway in the bow-shock tail.",
        "The short-term X-ray variability of Cyg X-1 can be interpreted as random\noccurrence of mini-flares known as the shots, whose physical nature is still\nunclear. We propose a new algorithm for shot identification in the X-ray light\ncurve, based on baseline detection and template fitting. Compared with previous\ntechniques, our algorithm allows us to detect shots with lower amplitudes and\nshorter time separations. With NICER observations, we find that, after\ncorrection for detection sensitivity, both the shot amplitude and recurrence\nrate are positively scaled with the mean count rate, while the recurrence rate\nhas a much higher dependence on the count rate. These suggest that a higher\nmass accretion rate will drive more and slightly larger shots. We also find\nthat the abrupt hardening near the shot peak found in previous studies is\nattributed to different shot profiles in different energy bands; there is no\nneed to involve a rapid physical process to suddenly harden the emitting\nspectrum.",
        "The cancellation of high-frequency laser phase noise using feedforward\ntechniques, as opposed to feedback methods, has achieved significant\nadvancements in recent years. However, directly applying existing feedforward\ntechniques to laser systems based on nonlinear conversion still faces\nsubstantial challenges. Here, we propose and demonstrate a feedforward scheme\nthat suppresses phase noise in frequency-doubled light by utilizing phase noise\ninformation of its fundamental pump. This scheme is enabled by the fact that\nthe phase jitter of the frequency-doubled light is simply twice that of the\npump, except for a first-order low-pass filtering effect introduced by the SHG\nenhancement cavity. Testing this method on a 420-nm frequency-doubled laser\nsystem, we realize a 25-dB suppression of the servo noise bump near 1 MHz on\nthe 420-nm light, and an average suppression of 30 dB for strong injected noise\nranging from 100 kHz to 20 MHz. This scheme shows promising potential for\napplications requiring blue or ultraviolet light with minimal high-frequency\nphase noise, such as precision control of atoms and molecules.",
        "We study the exact fluctuating hydrodynamics of the scaled Light-Heavy model\n(sLH), in which two species of particles (light and heavy) interact with a\nfluctuating surface. This model is similar in definition to the unscaled\nLight-Heavy model (uLH), except it uses rates scaled with the system size. The\nconsequence, it turns out, is a phase diagram that differs from that of the\nunscaled model. We derive the fluctuating hydrodynamics for this model using an\naction formalism involving the construction of path integrals for the\nprobability of different states that give the complete macroscopic picture\nstarting from the microscopic one. This is then used to obtain the two-point\nsteady-state (static) correlation functions between fluctuations in the two\ndensity fields in the homogeneous phase. We show that these theoretical results\nmatch well with microscopic simulations away from the critical line. We derive\nan exponentially decaying form for the two-point steady-state correlation\nfunction with a correlation length that diverges as the critical line is\napproached. Finally, we also compute the dynamic correlations in the\nhomogeneous phase and use them to determine the relaxation dynamics as well as\nthe dynamic exponents of the system.",
        "We investigate uniqueness of solutions to certain classes of elliptic and\nparabolic equations posed on metric graphs. In particular, we address the\nlinear Schr\\\"odinger equation with a potential, and the heat equation with a\nvariable density. We assume suitable growth conditions on the solutions, which\nare related to the behaviour at infinity of the potential or of the density.",
        "We prove a central limit error bound for convolution powers of laws with\nfinite moments of order $r \\in \\mathopen]2,3\\mathclose]$, taking a closeness of\nthe laws to normality into account. Up to a universal constant, this\ngeneralises the case of $r=3$ of the sharpening of the Berry (1941) - Esseen\n(1942) theorem obtained by Mattner (2024), namely by sharpening here the Katz\n(1963) error bound for the i.i.d. case of Lyapunov's (1901) theorem. Our proof\nuses a partial generalisation of the theorem of Senatov and Zolotarev (1986)\nused for the earlier special case. A result more general than our main one\ncould be obtained by using instead another theorem of Senatov (1980), but\nunfortunately an auxiliary inequality used in the latter's proof is wrong.",
        "We formulate a global conjecture for the automorphic period integral\nassociated to the symmetric pairs defined by unitary groups over number fields,\ngeneralizing a theorem of Waldspurger's toric period for $\\mathrm{GL}(2)$. We\nintroduce a new relative trace formula to prove our global conjecture under\nsome local hypotheses. A new feature is the presence of the relative endoscopy.\nIn this paper we prove the main local theorem: a new relative fundamental lemma\ncomparing certain orbital integrals of functions matched in terms of Hironaka\nand Satake transforms.",
        "We present a construction of one-time memories (OTMs) using\nclassical-accessible stateless hardware, building upon the work of Broadbent et\nal. and Behera et al.. Unlike the aforementioned work, our approach leverages\nquantum random access codes (QRACs) to encode two classical bits, $b_0$ and\n$b_1$, into a single qubit state $\\mathcal{E}(b_0 b_1)$ where the receiver can\nretrieve one of the bits with a certain probability of error. To prove\nsoundness, we define a nonconvex optimization problem over POVMs on\n$\\mathbb{C}^2$. This optimization gives an upper bound on the probability of\ndistinguishing bit $b_{1-\\alpha}$ given that the probability that the receiver\nrecovers bit $b_\\alpha$ is high. Assuming the optimization is sufficiently\naccurate, we then prove soundness against a polynomial number of classical\nqueries to the hardware.",
        "We focus on a geometrical inverse problem that involves recovering\ndiscontinuities in electrical conductivity based on boundary measurements. This\nproblem serves as a model to introduce a shape recovery technique that merges\nthe monotonicity method with the level-set method. The level-set method,\ncommonly used in shape optimization, often relies heavily on the accuracy of\nthe initial guess. To overcome this challenge, we utilize the monotonicity\nmethod to generate a more precise initial guess, which is then used to\ninitialize the level-set method. We provide numerical results to illustrate the\neffectiveness of this combined approach.",
        "Machine learning (ML) offers considerable promise for the design of new\nmolecules and materials. In real-world applications, the design problem is\noften domain-specific, and suffers from insufficient data, particularly labeled\ndata, for ML training. In this study, we report a data-efficient, deep-learning\nframework for molecular discovery that integrates a coarse-grained\nfunctional-group representation with a self-attention mechanism to capture\nintricate chemical interactions. Our approach exploits group-contribution\ntheory to create a graph-based intermediate representation of molecules,\nserving as a low-dimensional embedding that substantially reduces the data\ndemands typically required for training. By leveraging the self-attention\nmechanism to learn subtle chemical context, our method consistently outperforms\nconventional methods in predicting multiple thermophysical properties. In a\ncase study focused on adhesive polymer monomers, we train on a limited dataset\ncomprising just 6,000 unlabeled and 600 labeled monomers. The resulting\nchemistry prediction model achieves over 92% accuracy in forecasting properties\ndirectly from SMILES strings, exceeding the performance of current\nstate-of-the-art techniques. Furthermore, the latent molecular embedding is\ninvertible, allowing the design pipeline to incorporate a decoder that can\nautomatically generate new monomers from the learned chemical subspace. We\nillustrate this functionality by targeting high and low glass transition\ntemperatures ($T_g$), successfully identifying novel candidates whose $T_g$\nextends beyond the range observed in the training data. The ease with which our\ncoarse-grained, attention-based framework navigates both chemical diversity and\ndata scarcity offers a compelling route to accelerate and broaden the search\nfor functional materials.",
        "We carried out high-resolution large-eddy simulations (LESs) to investigate\nthe effects of several separation-control approaches on a NACA4412 wing section\nwith spanwise width of $L_z = 0.6$ at an angle of attack of $AoA=11^{\\circ}$ at\na Reynolds number of $Re_c = 200,000$ based on chord length $c$ and free-stream\nvelocity $U_{\\infty}$. Two control strategies were considered: (1) steady\nuniform blowing and\/or suction on the suction and\/or pressure sides, and (2)\nperiodic control on the suction side. A wide range of control configurations\nwere evaluated in terms of aerodynamic efficiency (i.e., lift-to-drag ratio)\nand separation delay. Uniform blowing and\/or suction effectively delayed flow\nseparation, leading to a lift increase of up to $11\\%$, but yielded only\nmarginal improvements in aerodynamic efficiency. In contrast, periodic control\nneither enhanced separation delay nor improved efficiency. A detailed analysis\nof the interaction between uniform blowing and\/or suction and turbulent\nboundary layers (TBLs) over the wing was performed, including assessments of\n(1) integral boundary-layer quantities, (2) turbulence statistics, and (3)\npower-spectral densities. Significant modifications in Reynolds stresses and\nspectral characteristics were observed. To the authors' best knowledge, this is\nthe first numerical study utilizing high-resolution LESs to provide\ncomprehensive assessments on separation control.",
        "We prove lower bounds for the first non-trivial eigenvalue of the drift\nLaplacian on manifolds with Wentzell-type boundary condition in terms of some\nCheeger-type constants for bulk-boundary interactions. Our results are in the\nspirit of Cheeger's classical inequality.",
        "In this paper, we characterize all Artinian complete intersection\n$K$-algebras $A_F$ whose Macaulay dual generator $F$ is a binomial. In\naddition, we prove that such\n  complete intersection Artinian $K$-algebras $A_F$ satisfy the Strong\nLefschetz property.",
        "We establish the boundedness of time derivatives of solutions to parabolic\n$p$-Laplace equations. Our approach relies on the Bernstein technique combined\nwith a suitable approximation method. As a consequence, we obtain an optimal\nregularity result with a connection to the well-known $C^{p'}$-conjecture in\nthe elliptic setting. Finally, we extend our method to treat global regularity\nresults for both fully nonlinear and general quasilinear degenerate parabolic\nproblems.",
        "We study a family of convolution operators. Their regarding Fourier\nmultipliers are defined in terms of distributions having singularity on the\nlight-cone in $\\mathbb{R}^{n+1}$. As a result, we give a new approach to the\nBochner-Riesz summability."
      ]
    }
  },
  {
    "id":2411.01019,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Screening for lung cancer: 2023 guideline update from the American Cancer Society",
    "start_abstract":"Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
    "start_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
      ],
      "abstract":[
        "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces",
        "SycEval: Evaluating LLM Sycophancy",
        "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D\n  5th Edition Combat",
        "MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science",
        "Data and System Perspectives of Sustainable Artificial Intelligence",
        "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning",
        "Counting and Reasoning with Plans",
        "Monitoring Reasoning Models for Misbehavior and the Risks of Promoting\n  Obfuscation",
        "Small Models Struggle to Learn from Strong Reasoners",
        "Behaviour Discovery and Attribution for Explainable Reinforcement\n  Learning",
        "Exploring the Implementation of AI in Early Onset Interviews to Help\n  Mitigate Bias",
        "Multi-Agent Verification: Scaling Test-Time Compute with Multiple\n  Verifiers",
        "On Benchmarking Human-Like Intelligence in Machines",
        "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The\n  Balancing Act of Selective Unlearning in Large Language Models",
        "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via\n  Cross-Architecture Distillation",
        "Multimodal Dreaming: A Global Workspace Approach to World Model-Based\n  Reinforcement Learning",
        "Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time\n  Series Forecasting",
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "EDM: Efficient Deep Feature Matching",
        "Observation of Subnatural-Linewidth Biphotons In a Two-Level Atomic\n  Ensemble",
        "Fixed-Confidence Best Arm Identification with Decreasing Variance",
        "AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary\n  Task-Oriented Grasping in Clutter",
        "Rethinking LLM Bias Probing Using Lessons from the Social Sciences",
        "EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic\n  Regression on Heterogeneous Database",
        "Global Portraits of Inflation in Nonsingular Variables",
        "Essentially Commuting with a Unitary",
        "Topological modes, non-locality and the double copy",
        "Expansions and restrictions of structures and theories, their\n  hierarchies"
      ],
      "abstract":[
        "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
        "Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications.",
        "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications.",
        "Pre-trained on extensive text and image corpora, current Multi-Modal Large\nLanguage Models (MLLM) have shown strong capabilities in general visual\nreasoning tasks. However, their performance is still lacking in physical\ndomains that require understanding diagrams with complex physical structures\nand quantitative analysis based on multi-modal information. To address this, we\ndevelop a new framework, named Multi-Modal Scientific Reasoning with Physics\nPerception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level\nmulti-modal reasoning task into physical diagram understanding via a Physical\nPerception Model (PPM) and reasoning with physical knowledge via a simulator.\nThe PPM module is obtained by fine-tuning a visual language model using\ncarefully designed synthetic data with paired physical diagrams and\ncorresponding simulation language descriptions. At the inference stage, MAPS\nintegrates the simulation language description of the input diagram provided by\nPPM and results obtained through a Chain-of-Simulation process with MLLM to\nderive the underlying rationale and the final answer. Validated using our\ncollected college-level circuit analysis problems, MAPS significantly improves\nreasoning accuracy of MLLM and outperforms all existing models. The results\nconfirm MAPS offers a promising direction for enhancing multi-modal scientific\nreasoning ability of MLLMs. We will release our code, model and dataset used\nfor our experiments upon publishing of this paper.",
        "Sustainable AI is a subfield of AI for concerning developing and using AI\nsystems in ways of aiming to reduce environmental impact and achieve\nsustainability. Sustainable AI is increasingly important given that training of\nand inference with AI models such as large langrage models are consuming a\nlarge amount of computing power. In this article, we discuss current issues,\nopportunities and example solutions for addressing these issues, and future\nchallenges to tackle, from the data and system perspectives, related to data\nacquisition, data processing, and AI model training and inference.",
        "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems",
        "Classical planning asks for a sequence of operators reaching a given goal.\nWhile the most common case is to compute a plan, many scenarios require more\nthan that. However, quantitative reasoning on the plan space remains mostly\nunexplored. A fundamental problem is to count plans, which relates to the\nconditional probability on the plan space. Indeed, qualitative and quantitative\napproaches are well-established in various other areas of automated reasoning.\nWe present the first study to quantitative and qualitative reasoning on the\nplan space. In particular, we focus on polynomially bounded plans. On the\ntheoretical side, we study its complexity, which gives rise to rich reasoning\nmodes. Since counting is hard in general, we introduce the easier notion of\nfacets, which enables understanding the significance of operators. On the\npractical side, we implement quantitative reasoning for planning. Thereby, we\ntransform a planning task into a propositional formula and use knowledge\ncompilation to count different plans. This framework scales well to large plan\nspaces, while enabling rich reasoning capabilities such as learning pruning\nfunctions and explainable planning.",
        "Mitigating reward hacking--where AI systems misbehave due to flaws or\nmisspecifications in their learning objectives--remains a key challenge in\nconstructing capable and aligned models. We show that we can monitor a frontier\nreasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding\nenvironments by using another LLM that observes the model's chain-of-thought\n(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent\nactions and outputs alone, and we further found that a LLM weaker than o3-mini,\nnamely GPT-4o, can effectively monitor a stronger model. Because CoT monitors\ncan be effective at detecting exploits, it is natural to ask whether those\nexploits can be suppressed by incorporating a CoT monitor directly into the\nagent's training objective. While we show that integrating CoT monitors into\nthe reinforcement learning reward can indeed produce more capable and more\naligned agents in the low optimization regime, we find that with too much\noptimization, agents learn obfuscated reward hacking, hiding their intent\nwithin the CoT while still exhibiting a significant rate of reward hacking.\nBecause it is difficult to tell when CoTs have become obfuscated, it may be\nnecessary to pay a monitorability tax by not applying strong optimization\npressures directly to the chain-of-thought, ensuring that CoTs remain\nmonitorable and useful for detecting misaligned behavior.",
        "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
        "Explaining the decisions made by reinforcement learning (RL) agents is\ncritical for building trust and ensuring reliability in real-world\napplications. Traditional approaches to explainability often rely on saliency\nanalysis, which can be limited in providing actionable insights. Recently,\nthere has been growing interest in attributing RL decisions to specific\ntrajectories within a dataset. However, these methods often generalize\nexplanations to long trajectories, potentially involving multiple distinct\nbehaviors. Often, providing multiple more fine grained explanations would\nimprove clarity. In this work, we propose a framework for behavior discovery\nand action attribution to behaviors in offline RL trajectories. Our method\nidentifies meaningful behavioral segments, enabling more precise and granular\nexplanations associated with high level agent behaviors. This approach is\nadaptable across diverse environments with minimal modifications, offering a\nscalable and versatile solution for behavior discovery and attribution for\nexplainable RL.",
        "This paper investigates the application of artificial intelligence (AI) in\nearly-stage recruitment interviews in order to reduce inherent bias,\nspecifically sentiment bias. Traditional interviewers are often subject to\nseveral biases, including interviewer bias, social desirability effects, and\neven confirmation bias. In turn, this leads to non-inclusive hiring practices,\nand a less diverse workforce. This study further analyzes various AI\ninterventions that are present in the marketplace today such as multimodal\nplatforms and interactive candidate assessment tools in order to gauge the\ncurrent market usage of AI in early-stage recruitment. However, this paper aims\nto use a unique AI system that was developed to transcribe and analyze\ninterview dynamics, which emphasize skill and knowledge over emotional\nsentiments. Results indicate that AI effectively minimizes sentiment-driven\nbiases by 41.2%, suggesting its revolutionizing power in companies' recruitment\nprocesses for improved equity and efficiency.",
        "By utilizing more computational resources at test-time, large language models\n(LLMs) can improve without additional training. One common strategy uses\nverifiers to evaluate candidate outputs. In this work, we propose a novel\nscaling dimension for test-time compute: scaling the number of verifiers. We\nintroduce Multi-Agent Verification (MAV) as a test-time compute paradigm that\ncombines multiple verifiers to improve performance. We propose using Aspect\nVerifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of\noutputs, as one possible choice for the verifiers in a MAV system. AVs are a\nconvenient building block for MAV since they can be easily combined without\nadditional training. Moreover, we introduce BoN-MAV, a simple multi-agent\nverification algorithm that combines best-of-n sampling with multiple\nverifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency\nand reward model verification, and we demonstrate both weak-to-strong\ngeneralization, where combining weak verifiers improves even stronger LLMs, and\nself-improvement, where the same base model is used to both generate and verify\noutputs. Our results establish scaling the number of verifiers as a promising\nnew dimension for improving language model performance at test-time.",
        "Recent benchmark studies have claimed that AI has approached or even\nsurpassed human-level performances on various cognitive tasks. However, this\nposition paper argues that current AI evaluation paradigms are insufficient for\nassessing human-like cognitive capabilities. We identify a set of key\nshortcomings: a lack of human-validated labels, inadequate representation of\nhuman response variability and uncertainty, and reliance on simplified and\necologically-invalid tasks. We support our claims by conducting a human\nevaluation study on ten existing AI benchmarks, suggesting significant biases\nand flaws in task and label designs. To address these limitations, we propose\nfive concrete recommendations for developing future benchmarks that will enable\nmore rigorous and meaningful evaluations of human-like cognitive capacities in\nAI with various implications for such AI applications.",
        "Large Language Models (LLMs) face significant challenges in maintaining\nprivacy, ethics, and compliance, when sensitive or obsolete data must be\nselectively removed. Retraining these models from scratch is computationally\ninfeasible, necessitating efficient alternatives. As part of the SemEval 2025\nTask 4, this work focuses on the application of selective unlearning in LLMs to\naddress this challenge. In this paper, we present our experiments and findings,\nprimarily leveraging global weight modification to achieve an equilibrium\nbetween effectiveness of unlearning, knowledge retention, and target model's\npost-unlearning utility. We also detail the task-specific evaluation mechanism,\nresults, and challenges. Our algorithms have achieved an aggregate score of\n0.409 and 0.389 on the test set for 7B and 1B target models, respectively,\ndemonstrating promising results in verifiable LLM unlearning.",
        "Transformer-based and CNN-based methods demonstrate strong performance in\nlong-term time series forecasting. However, their high computational and\nstorage requirements can hinder large-scale deployment. To address this\nlimitation, we propose integrating lightweight MLP with advanced architectures\nusing knowledge distillation (KD). Our preliminary study reveals different\nmodels can capture complementary patterns, particularly multi-scale and\nmulti-period patterns in the temporal and frequency domains. Based on this\nobservation, we introduce TimeDistill, a cross-architecture KD framework that\ntransfers these patterns from teacher models (e.g., Transformers, CNNs) to MLP.\nAdditionally, we provide a theoretical analysis, demonstrating that our KD\napproach can be interpreted as a specialized form of mixup data augmentation.\nTimeDistill improves MLP performance by up to 18.6%, surpassing teacher models\non eight datasets. It also achieves up to 7X faster inference and requires 130X\nfewer parameters. Furthermore, we conduct extensive evaluations to highlight\nthe versatility and effectiveness of TimeDistill.",
        "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
        "Recent advancements in time series forecasting have explored augmenting\nmodels with text or vision modalities to improve accuracy. While text provides\ncontextual understanding, it often lacks fine-grained temporal details.\nConversely, vision captures intricate temporal patterns but lacks semantic\ncontext, limiting the complementary potential of these modalities. To address\nthis, we propose Time-VLM, a novel multimodal framework that leverages\npre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and\ntextual modalities for enhanced forecasting. Our framework comprises three key\ncomponents: (1) a Retrieval-Augmented Learner, which extracts enriched temporal\nfeatures through memory bank interactions; (2) a Vision-Augmented Learner,\nwhich encodes time series as informative images; and (3) a Text-Augmented\nLearner, which generates contextual textual descriptions. These components\ncollaborate with frozen pre-trained VLMs to produce multimodal embeddings,\nwhich are then fused with temporal features for final prediction. Extensive\nexperiments across diverse datasets demonstrate that Time-VLM achieves superior\nperformance, particularly in few-shot and zero-shot scenarios, thereby\nestablishing a new direction for multimodal time series forecasting.",
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "Recent feature matching methods have achieved remarkable performance but lack\nefficiency consideration. In this paper, we revisit the mainstream\ndetector-free matching pipeline and improve all its stages considering both\naccuracy and efficiency. We propose an Efficient Deep feature Matching network,\nEDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level\nfeatures. Then we present a Correlation Injection Module that conducts feature\ntransformation on high-level deep features, and progressively injects feature\ncorrelations from global to local for efficient multi-scale feature\naggregation, improving both speed and performance. In the refinement stage, a\nnovel lightweight bidirectional axis-based regression head is designed to\ndirectly predict subpixel-level correspondences from latent features, avoiding\nthe significant computational cost of explicitly locating keypoints on\nhigh-resolution local feature heatmaps. Moreover, effective selection\nstrategies are introduced to enhance matching accuracy. Extensive experiments\nshow that our EDM achieves competitive matching accuracy on various benchmarks\nand exhibits excellent efficiency, offering valuable best practices for\nreal-world applications. The code is available at\nhttps:\/\/github.com\/chicleee\/EDM.",
        "Biphotons and single photons with narrow bandwidths and long coherence times\nare essential to the realization of long-distance quantum communication (LDQC)\nand linear optical quantum computing (LOQC). In this Letter, we manipulate the\nbiphoton wave functions of the spontaneous four-wave mixing in a two-level\natomic ensemble with a single-laser pump scheme. Our innovative experimental\napproach enables the generation of biphotons with a sub-MHz bandwidth of 0.36\nMHz, a record spectral brightness of $2.28\\times10^7$${\\rm s}^{-1}{\\rm\nmW}^{-1}{\\rm MHz}^{-1}$, and a temporally symmetric wave packet at moderate\noptical depth. The strong non-classical cross-correlation of the biphotons also\nenables the observation of heralded sub-MHz-linewidth single photons with a\npronounced single-photon nature. The generation of sub-MHz-linewidth biphotons\nand single photons with a two-level atomic ensembles not only finds\napplications in quantum repeaters and large cluster states for LDQC and LOQC\nbut also opens up the opportunity to miniaturize the biphoton or single-photon\nsources for chip-scale quantum technologies.",
        "We focus on the problem of best-arm identification in a stochastic multi-arm\nbandit with temporally decreasing variances for the arms' rewards. We model arm\nrewards as Gaussian random variables with fixed means and variances that\ndecrease with time. The cost incurred by the learner is modeled as a weighted\nsum of the time needed by the learner to identify the best arm, and the number\nof samples of arms collected by the learner before termination. Under this cost\nfunction, there is an incentive for the learner to not sample arms in all\nrounds, especially in the initial rounds. On the other hand, not sampling\nincreases the termination time of the learner, which also increases cost. This\ntrade-off necessitates new sampling strategies. We propose two policies. The\nfirst policy has an initial wait period with no sampling followed by continuous\nsampling. The second policy samples periodically and uses a weighted average of\nthe rewards observed to identify the best arm. We provide analytical guarantees\non the performance of both policies and supplement our theoretical results with\nsimulations which show that our polices outperform the state-of-the-art\npolicies for the classical best arm identification problem.",
        "Inferring the affordance of an object and grasping it in a task-oriented\nmanner is crucial for robots to successfully complete manipulation tasks.\nAffordance indicates where and how to grasp an object by taking its\nfunctionality into account, serving as the foundation for effective\ntask-oriented grasping. However, current task-oriented methods often depend on\nextensive training data that is confined to specific tasks and objects, making\nit difficult to generalize to novel objects and complex scenes. In this paper,\nwe introduce AffordGrasp, a novel open-vocabulary grasping framework that\nleverages the reasoning capabilities of vision-language models (VLMs) for\nin-context affordance reasoning. Unlike existing methods that rely on explicit\ntask and object specifications, our approach infers tasks directly from\nimplicit user instructions, enabling more intuitive and seamless human-robot\ninteraction in everyday scenarios. Building on the reasoning outcomes, our\nframework identifies task-relevant objects and grounds their part-level\naffordances using a visual grounding module. This allows us to generate\ntask-oriented grasp poses precisely within the affordance regions of the\nobject, ensuring both functional and context-aware robotic manipulation.\nExtensive experiments demonstrate that AffordGrasp achieves state-of-the-art\nperformance in both simulation and real-world scenarios, highlighting the\neffectiveness of our method. We believe our approach advances robotic\nmanipulation techniques and contributes to the broader field of embodied AI.\nProject website: https:\/\/eqcy.github.io\/affordgrasp\/.",
        "The proliferation of LLM bias probes introduces three significant challenges:\n(1) we lack principled criteria for choosing appropriate probes, (2) we lack a\nsystem for reconciling conflicting results across probes, and (3) we lack\nformal frameworks for reasoning about when (and why) probe results will\ngeneralize to real user behavior. We address these challenges by systematizing\nLLM social bias probing using actionable insights from social sciences. We then\nintroduce EcoLevels - a framework that helps (a) determine appropriate bias\nprobes, (b) reconcile conflicting findings across probes, and (c) generate\npredictions about bias generalization. Overall, we ground our analysis in\nsocial science research because many LLM probes are direct applications of\nhuman probes, and these fields have faced similar challenges when studying\nsocial bias in humans. Based on our work, we suggest how the next generation of\nLLM bias probing can (and should) benefit from decades of social science\nresearch.",
        "Accurate nonlinear computation is a key challenge in privacy-preserving\nmachine learning (PPML). Most existing frameworks approximate it through linear\noperations, resulting in significant precision loss. This paper proposes an\nefficient, verifiable and accurate security 2-party logistic regression\nframework (EVA-S2PLoR), which achieves accurate nonlinear function computation\nthrough a novel secure element-wise multiplication protocol and its derived\nprotocols. Our framework primarily includes secure 2-party vector element-wise\nmultiplication, addition to multiplication, reciprocal, and sigmoid function\nbased on data disguising technology, where high efficiency and accuracy are\nguaranteed by the simple computation flow based on the real number domain and\nthe few number of fixed communication rounds. We provide secure and robust\nanomaly detection through dimension transformation and Monte Carlo methods.\nEVA-S2PLoR outperforms many advanced frameworks in terms of precision\n(improving the performance of the sigmoid function by about 10 orders of\nmagnitude compared to most frameworks) and delivers the best overall\nperformance in secure logistic regression experiments.",
        "In the phase space perspective, scalar field slow roll inflation is described\nby a heteroclinic orbit from a saddle type fixed point to a final attractive\npoint. In many models the saddle point resides in the scalar field asymptotics,\nand thus for a comprehensive view of the dynamics a global phase portrait is\nnecessary. For this task, in the literature one mostly encounters dynamical\nvariables that either render the initial or the final state singular, thus\nobscuring the full picture. In this work we construct a hybrid set of variables\nwhich allow the depiction of both the initial and final states distinctly in\nnonsingular manner. To illustrate the method, we apply these variables to\nportray various interesting types of scalar field inflationary models like\nmetric Higgs inflation, metric Starobinsky inflation, pole inflation, and a\nnonminimal Palatini model.",
        "Let $R$ be a unitary operator whose spectrum is the circle. We show that the\nset of unitaries $U$ which essentially commute with $R$ (i.e., $[U,R]\\equiv\nUR-RU$ is compact) is path-connected. Moreover, we also calculate the set of\npath-connected components of the orthogonal projections which essentially\ncommute with $R$ and obey a non-triviality condition, and prove it is bijective\nwith $\\mathbb{Z}$.",
        "The double copy connects scattering amplitudes and other objects in gauge and\ngravity theories. Open conceptual issues include whether non-local information\nin gravity theories can be generated from the double copy, and how the double\ncopy should be practically implemented in unseen cases. In this paper, we\nconsider topological theories (with and without mass) in 2+1 dimensions, and\nargue that this makes a useful playground for exploring non-locality. In\nparticular, topological modes of the gauge field arise, themselves associated\nwith non-trivial global behaviour, and it is not clear {\\it a priori} how to\ndouble copy them. We settle this issue, and clarify the role of BCJ shifts in\nmodifying how topological modes contribute. We show how our conclusions apply\nto four- and five-point scattering amplitudes in topological gauge \/ gravity\ntheories, as a by-product obtaining greatly simplified analytic expressions for\nthe four-point amplitudes.",
        "We introduce and study some general principles and hierarchical properties of\nexpansions and restrictions of structures and their theories The general\napproach is applied to describe these properties for classes of\n$\\omega$-categorical theories and structures, Ehrenfeucht theories and their\nmodels, strongly minimal, $\\omega_1$-theories, and stable ones."
      ]
    }
  },
  {
    "id":2411.01019,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects",
    "start_abstract":"Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.",
    "start_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
      ],
      "abstract":[
        "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces",
        "SycEval: Evaluating LLM Sycophancy",
        "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D\n  5th Edition Combat",
        "MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science",
        "Data and System Perspectives of Sustainable Artificial Intelligence",
        "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning",
        "Counting and Reasoning with Plans",
        "Monitoring Reasoning Models for Misbehavior and the Risks of Promoting\n  Obfuscation",
        "Small Models Struggle to Learn from Strong Reasoners",
        "Behaviour Discovery and Attribution for Explainable Reinforcement\n  Learning",
        "Exploring the Implementation of AI in Early Onset Interviews to Help\n  Mitigate Bias",
        "Multi-Agent Verification: Scaling Test-Time Compute with Multiple\n  Verifiers",
        "On Benchmarking Human-Like Intelligence in Machines",
        "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The\n  Balancing Act of Selective Unlearning in Large Language Models",
        "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via\n  Cross-Architecture Distillation",
        "Multimodal Dreaming: A Global Workspace Approach to World Model-Based\n  Reinforcement Learning",
        "Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time\n  Series Forecasting",
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "EDM: Efficient Deep Feature Matching",
        "Observation of Subnatural-Linewidth Biphotons In a Two-Level Atomic\n  Ensemble",
        "Fixed-Confidence Best Arm Identification with Decreasing Variance",
        "AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary\n  Task-Oriented Grasping in Clutter",
        "Rethinking LLM Bias Probing Using Lessons from the Social Sciences",
        "EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic\n  Regression on Heterogeneous Database",
        "Global Portraits of Inflation in Nonsingular Variables",
        "Essentially Commuting with a Unitary",
        "Topological modes, non-locality and the double copy",
        "Expansions and restrictions of structures and theories, their\n  hierarchies"
      ],
      "abstract":[
        "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
        "Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications.",
        "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications.",
        "Pre-trained on extensive text and image corpora, current Multi-Modal Large\nLanguage Models (MLLM) have shown strong capabilities in general visual\nreasoning tasks. However, their performance is still lacking in physical\ndomains that require understanding diagrams with complex physical structures\nand quantitative analysis based on multi-modal information. To address this, we\ndevelop a new framework, named Multi-Modal Scientific Reasoning with Physics\nPerception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level\nmulti-modal reasoning task into physical diagram understanding via a Physical\nPerception Model (PPM) and reasoning with physical knowledge via a simulator.\nThe PPM module is obtained by fine-tuning a visual language model using\ncarefully designed synthetic data with paired physical diagrams and\ncorresponding simulation language descriptions. At the inference stage, MAPS\nintegrates the simulation language description of the input diagram provided by\nPPM and results obtained through a Chain-of-Simulation process with MLLM to\nderive the underlying rationale and the final answer. Validated using our\ncollected college-level circuit analysis problems, MAPS significantly improves\nreasoning accuracy of MLLM and outperforms all existing models. The results\nconfirm MAPS offers a promising direction for enhancing multi-modal scientific\nreasoning ability of MLLMs. We will release our code, model and dataset used\nfor our experiments upon publishing of this paper.",
        "Sustainable AI is a subfield of AI for concerning developing and using AI\nsystems in ways of aiming to reduce environmental impact and achieve\nsustainability. Sustainable AI is increasingly important given that training of\nand inference with AI models such as large langrage models are consuming a\nlarge amount of computing power. In this article, we discuss current issues,\nopportunities and example solutions for addressing these issues, and future\nchallenges to tackle, from the data and system perspectives, related to data\nacquisition, data processing, and AI model training and inference.",
        "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems",
        "Classical planning asks for a sequence of operators reaching a given goal.\nWhile the most common case is to compute a plan, many scenarios require more\nthan that. However, quantitative reasoning on the plan space remains mostly\nunexplored. A fundamental problem is to count plans, which relates to the\nconditional probability on the plan space. Indeed, qualitative and quantitative\napproaches are well-established in various other areas of automated reasoning.\nWe present the first study to quantitative and qualitative reasoning on the\nplan space. In particular, we focus on polynomially bounded plans. On the\ntheoretical side, we study its complexity, which gives rise to rich reasoning\nmodes. Since counting is hard in general, we introduce the easier notion of\nfacets, which enables understanding the significance of operators. On the\npractical side, we implement quantitative reasoning for planning. Thereby, we\ntransform a planning task into a propositional formula and use knowledge\ncompilation to count different plans. This framework scales well to large plan\nspaces, while enabling rich reasoning capabilities such as learning pruning\nfunctions and explainable planning.",
        "Mitigating reward hacking--where AI systems misbehave due to flaws or\nmisspecifications in their learning objectives--remains a key challenge in\nconstructing capable and aligned models. We show that we can monitor a frontier\nreasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding\nenvironments by using another LLM that observes the model's chain-of-thought\n(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent\nactions and outputs alone, and we further found that a LLM weaker than o3-mini,\nnamely GPT-4o, can effectively monitor a stronger model. Because CoT monitors\ncan be effective at detecting exploits, it is natural to ask whether those\nexploits can be suppressed by incorporating a CoT monitor directly into the\nagent's training objective. While we show that integrating CoT monitors into\nthe reinforcement learning reward can indeed produce more capable and more\naligned agents in the low optimization regime, we find that with too much\noptimization, agents learn obfuscated reward hacking, hiding their intent\nwithin the CoT while still exhibiting a significant rate of reward hacking.\nBecause it is difficult to tell when CoTs have become obfuscated, it may be\nnecessary to pay a monitorability tax by not applying strong optimization\npressures directly to the chain-of-thought, ensuring that CoTs remain\nmonitorable and useful for detecting misaligned behavior.",
        "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
        "Explaining the decisions made by reinforcement learning (RL) agents is\ncritical for building trust and ensuring reliability in real-world\napplications. Traditional approaches to explainability often rely on saliency\nanalysis, which can be limited in providing actionable insights. Recently,\nthere has been growing interest in attributing RL decisions to specific\ntrajectories within a dataset. However, these methods often generalize\nexplanations to long trajectories, potentially involving multiple distinct\nbehaviors. Often, providing multiple more fine grained explanations would\nimprove clarity. In this work, we propose a framework for behavior discovery\nand action attribution to behaviors in offline RL trajectories. Our method\nidentifies meaningful behavioral segments, enabling more precise and granular\nexplanations associated with high level agent behaviors. This approach is\nadaptable across diverse environments with minimal modifications, offering a\nscalable and versatile solution for behavior discovery and attribution for\nexplainable RL.",
        "This paper investigates the application of artificial intelligence (AI) in\nearly-stage recruitment interviews in order to reduce inherent bias,\nspecifically sentiment bias. Traditional interviewers are often subject to\nseveral biases, including interviewer bias, social desirability effects, and\neven confirmation bias. In turn, this leads to non-inclusive hiring practices,\nand a less diverse workforce. This study further analyzes various AI\ninterventions that are present in the marketplace today such as multimodal\nplatforms and interactive candidate assessment tools in order to gauge the\ncurrent market usage of AI in early-stage recruitment. However, this paper aims\nto use a unique AI system that was developed to transcribe and analyze\ninterview dynamics, which emphasize skill and knowledge over emotional\nsentiments. Results indicate that AI effectively minimizes sentiment-driven\nbiases by 41.2%, suggesting its revolutionizing power in companies' recruitment\nprocesses for improved equity and efficiency.",
        "By utilizing more computational resources at test-time, large language models\n(LLMs) can improve without additional training. One common strategy uses\nverifiers to evaluate candidate outputs. In this work, we propose a novel\nscaling dimension for test-time compute: scaling the number of verifiers. We\nintroduce Multi-Agent Verification (MAV) as a test-time compute paradigm that\ncombines multiple verifiers to improve performance. We propose using Aspect\nVerifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of\noutputs, as one possible choice for the verifiers in a MAV system. AVs are a\nconvenient building block for MAV since they can be easily combined without\nadditional training. Moreover, we introduce BoN-MAV, a simple multi-agent\nverification algorithm that combines best-of-n sampling with multiple\nverifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency\nand reward model verification, and we demonstrate both weak-to-strong\ngeneralization, where combining weak verifiers improves even stronger LLMs, and\nself-improvement, where the same base model is used to both generate and verify\noutputs. Our results establish scaling the number of verifiers as a promising\nnew dimension for improving language model performance at test-time.",
        "Recent benchmark studies have claimed that AI has approached or even\nsurpassed human-level performances on various cognitive tasks. However, this\nposition paper argues that current AI evaluation paradigms are insufficient for\nassessing human-like cognitive capabilities. We identify a set of key\nshortcomings: a lack of human-validated labels, inadequate representation of\nhuman response variability and uncertainty, and reliance on simplified and\necologically-invalid tasks. We support our claims by conducting a human\nevaluation study on ten existing AI benchmarks, suggesting significant biases\nand flaws in task and label designs. To address these limitations, we propose\nfive concrete recommendations for developing future benchmarks that will enable\nmore rigorous and meaningful evaluations of human-like cognitive capacities in\nAI with various implications for such AI applications.",
        "Large Language Models (LLMs) face significant challenges in maintaining\nprivacy, ethics, and compliance, when sensitive or obsolete data must be\nselectively removed. Retraining these models from scratch is computationally\ninfeasible, necessitating efficient alternatives. As part of the SemEval 2025\nTask 4, this work focuses on the application of selective unlearning in LLMs to\naddress this challenge. In this paper, we present our experiments and findings,\nprimarily leveraging global weight modification to achieve an equilibrium\nbetween effectiveness of unlearning, knowledge retention, and target model's\npost-unlearning utility. We also detail the task-specific evaluation mechanism,\nresults, and challenges. Our algorithms have achieved an aggregate score of\n0.409 and 0.389 on the test set for 7B and 1B target models, respectively,\ndemonstrating promising results in verifiable LLM unlearning.",
        "Transformer-based and CNN-based methods demonstrate strong performance in\nlong-term time series forecasting. However, their high computational and\nstorage requirements can hinder large-scale deployment. To address this\nlimitation, we propose integrating lightweight MLP with advanced architectures\nusing knowledge distillation (KD). Our preliminary study reveals different\nmodels can capture complementary patterns, particularly multi-scale and\nmulti-period patterns in the temporal and frequency domains. Based on this\nobservation, we introduce TimeDistill, a cross-architecture KD framework that\ntransfers these patterns from teacher models (e.g., Transformers, CNNs) to MLP.\nAdditionally, we provide a theoretical analysis, demonstrating that our KD\napproach can be interpreted as a specialized form of mixup data augmentation.\nTimeDistill improves MLP performance by up to 18.6%, surpassing teacher models\non eight datasets. It also achieves up to 7X faster inference and requires 130X\nfewer parameters. Furthermore, we conduct extensive evaluations to highlight\nthe versatility and effectiveness of TimeDistill.",
        "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
        "Recent advancements in time series forecasting have explored augmenting\nmodels with text or vision modalities to improve accuracy. While text provides\ncontextual understanding, it often lacks fine-grained temporal details.\nConversely, vision captures intricate temporal patterns but lacks semantic\ncontext, limiting the complementary potential of these modalities. To address\nthis, we propose Time-VLM, a novel multimodal framework that leverages\npre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and\ntextual modalities for enhanced forecasting. Our framework comprises three key\ncomponents: (1) a Retrieval-Augmented Learner, which extracts enriched temporal\nfeatures through memory bank interactions; (2) a Vision-Augmented Learner,\nwhich encodes time series as informative images; and (3) a Text-Augmented\nLearner, which generates contextual textual descriptions. These components\ncollaborate with frozen pre-trained VLMs to produce multimodal embeddings,\nwhich are then fused with temporal features for final prediction. Extensive\nexperiments across diverse datasets demonstrate that Time-VLM achieves superior\nperformance, particularly in few-shot and zero-shot scenarios, thereby\nestablishing a new direction for multimodal time series forecasting.",
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "Recent feature matching methods have achieved remarkable performance but lack\nefficiency consideration. In this paper, we revisit the mainstream\ndetector-free matching pipeline and improve all its stages considering both\naccuracy and efficiency. We propose an Efficient Deep feature Matching network,\nEDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level\nfeatures. Then we present a Correlation Injection Module that conducts feature\ntransformation on high-level deep features, and progressively injects feature\ncorrelations from global to local for efficient multi-scale feature\naggregation, improving both speed and performance. In the refinement stage, a\nnovel lightweight bidirectional axis-based regression head is designed to\ndirectly predict subpixel-level correspondences from latent features, avoiding\nthe significant computational cost of explicitly locating keypoints on\nhigh-resolution local feature heatmaps. Moreover, effective selection\nstrategies are introduced to enhance matching accuracy. Extensive experiments\nshow that our EDM achieves competitive matching accuracy on various benchmarks\nand exhibits excellent efficiency, offering valuable best practices for\nreal-world applications. The code is available at\nhttps:\/\/github.com\/chicleee\/EDM.",
        "Biphotons and single photons with narrow bandwidths and long coherence times\nare essential to the realization of long-distance quantum communication (LDQC)\nand linear optical quantum computing (LOQC). In this Letter, we manipulate the\nbiphoton wave functions of the spontaneous four-wave mixing in a two-level\natomic ensemble with a single-laser pump scheme. Our innovative experimental\napproach enables the generation of biphotons with a sub-MHz bandwidth of 0.36\nMHz, a record spectral brightness of $2.28\\times10^7$${\\rm s}^{-1}{\\rm\nmW}^{-1}{\\rm MHz}^{-1}$, and a temporally symmetric wave packet at moderate\noptical depth. The strong non-classical cross-correlation of the biphotons also\nenables the observation of heralded sub-MHz-linewidth single photons with a\npronounced single-photon nature. The generation of sub-MHz-linewidth biphotons\nand single photons with a two-level atomic ensembles not only finds\napplications in quantum repeaters and large cluster states for LDQC and LOQC\nbut also opens up the opportunity to miniaturize the biphoton or single-photon\nsources for chip-scale quantum technologies.",
        "We focus on the problem of best-arm identification in a stochastic multi-arm\nbandit with temporally decreasing variances for the arms' rewards. We model arm\nrewards as Gaussian random variables with fixed means and variances that\ndecrease with time. The cost incurred by the learner is modeled as a weighted\nsum of the time needed by the learner to identify the best arm, and the number\nof samples of arms collected by the learner before termination. Under this cost\nfunction, there is an incentive for the learner to not sample arms in all\nrounds, especially in the initial rounds. On the other hand, not sampling\nincreases the termination time of the learner, which also increases cost. This\ntrade-off necessitates new sampling strategies. We propose two policies. The\nfirst policy has an initial wait period with no sampling followed by continuous\nsampling. The second policy samples periodically and uses a weighted average of\nthe rewards observed to identify the best arm. We provide analytical guarantees\non the performance of both policies and supplement our theoretical results with\nsimulations which show that our polices outperform the state-of-the-art\npolicies for the classical best arm identification problem.",
        "Inferring the affordance of an object and grasping it in a task-oriented\nmanner is crucial for robots to successfully complete manipulation tasks.\nAffordance indicates where and how to grasp an object by taking its\nfunctionality into account, serving as the foundation for effective\ntask-oriented grasping. However, current task-oriented methods often depend on\nextensive training data that is confined to specific tasks and objects, making\nit difficult to generalize to novel objects and complex scenes. In this paper,\nwe introduce AffordGrasp, a novel open-vocabulary grasping framework that\nleverages the reasoning capabilities of vision-language models (VLMs) for\nin-context affordance reasoning. Unlike existing methods that rely on explicit\ntask and object specifications, our approach infers tasks directly from\nimplicit user instructions, enabling more intuitive and seamless human-robot\ninteraction in everyday scenarios. Building on the reasoning outcomes, our\nframework identifies task-relevant objects and grounds their part-level\naffordances using a visual grounding module. This allows us to generate\ntask-oriented grasp poses precisely within the affordance regions of the\nobject, ensuring both functional and context-aware robotic manipulation.\nExtensive experiments demonstrate that AffordGrasp achieves state-of-the-art\nperformance in both simulation and real-world scenarios, highlighting the\neffectiveness of our method. We believe our approach advances robotic\nmanipulation techniques and contributes to the broader field of embodied AI.\nProject website: https:\/\/eqcy.github.io\/affordgrasp\/.",
        "The proliferation of LLM bias probes introduces three significant challenges:\n(1) we lack principled criteria for choosing appropriate probes, (2) we lack a\nsystem for reconciling conflicting results across probes, and (3) we lack\nformal frameworks for reasoning about when (and why) probe results will\ngeneralize to real user behavior. We address these challenges by systematizing\nLLM social bias probing using actionable insights from social sciences. We then\nintroduce EcoLevels - a framework that helps (a) determine appropriate bias\nprobes, (b) reconcile conflicting findings across probes, and (c) generate\npredictions about bias generalization. Overall, we ground our analysis in\nsocial science research because many LLM probes are direct applications of\nhuman probes, and these fields have faced similar challenges when studying\nsocial bias in humans. Based on our work, we suggest how the next generation of\nLLM bias probing can (and should) benefit from decades of social science\nresearch.",
        "Accurate nonlinear computation is a key challenge in privacy-preserving\nmachine learning (PPML). Most existing frameworks approximate it through linear\noperations, resulting in significant precision loss. This paper proposes an\nefficient, verifiable and accurate security 2-party logistic regression\nframework (EVA-S2PLoR), which achieves accurate nonlinear function computation\nthrough a novel secure element-wise multiplication protocol and its derived\nprotocols. Our framework primarily includes secure 2-party vector element-wise\nmultiplication, addition to multiplication, reciprocal, and sigmoid function\nbased on data disguising technology, where high efficiency and accuracy are\nguaranteed by the simple computation flow based on the real number domain and\nthe few number of fixed communication rounds. We provide secure and robust\nanomaly detection through dimension transformation and Monte Carlo methods.\nEVA-S2PLoR outperforms many advanced frameworks in terms of precision\n(improving the performance of the sigmoid function by about 10 orders of\nmagnitude compared to most frameworks) and delivers the best overall\nperformance in secure logistic regression experiments.",
        "In the phase space perspective, scalar field slow roll inflation is described\nby a heteroclinic orbit from a saddle type fixed point to a final attractive\npoint. In many models the saddle point resides in the scalar field asymptotics,\nand thus for a comprehensive view of the dynamics a global phase portrait is\nnecessary. For this task, in the literature one mostly encounters dynamical\nvariables that either render the initial or the final state singular, thus\nobscuring the full picture. In this work we construct a hybrid set of variables\nwhich allow the depiction of both the initial and final states distinctly in\nnonsingular manner. To illustrate the method, we apply these variables to\nportray various interesting types of scalar field inflationary models like\nmetric Higgs inflation, metric Starobinsky inflation, pole inflation, and a\nnonminimal Palatini model.",
        "Let $R$ be a unitary operator whose spectrum is the circle. We show that the\nset of unitaries $U$ which essentially commute with $R$ (i.e., $[U,R]\\equiv\nUR-RU$ is compact) is path-connected. Moreover, we also calculate the set of\npath-connected components of the orthogonal projections which essentially\ncommute with $R$ and obey a non-triviality condition, and prove it is bijective\nwith $\\mathbb{Z}$.",
        "The double copy connects scattering amplitudes and other objects in gauge and\ngravity theories. Open conceptual issues include whether non-local information\nin gravity theories can be generated from the double copy, and how the double\ncopy should be practically implemented in unseen cases. In this paper, we\nconsider topological theories (with and without mass) in 2+1 dimensions, and\nargue that this makes a useful playground for exploring non-locality. In\nparticular, topological modes of the gauge field arise, themselves associated\nwith non-trivial global behaviour, and it is not clear {\\it a priori} how to\ndouble copy them. We settle this issue, and clarify the role of BCJ shifts in\nmodifying how topological modes contribute. We show how our conclusions apply\nto four- and five-point scattering amplitudes in topological gauge \/ gravity\ntheories, as a by-product obtaining greatly simplified analytic expressions for\nthe four-point amplitudes.",
        "We introduce and study some general principles and hierarchical properties of\nexpansions and restrictions of structures and their theories The general\napproach is applied to describe these properties for classes of\n$\\omega$-categorical theories and structures, Ehrenfeucht theories and their\nmodels, strongly minimal, $\\omega_1$-theories, and stable ones."
      ]
    }
  },
  {
    "id":2412.11084,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Biological identifications through DNA barcodes",
    "start_abstract":"Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "BarcodeBERT: Transformers for Biodiversity Analysis"
      ],
      "abstract":[
        "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "A Minimax Approach to Ad Hoc Teamwork",
        "Program Synthesis Dialog Agents for Interactive Decision-Making",
        "ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language\n  Interactions for Computer-Aided Diagnosis",
        "Statistical Scenario Modelling and Lookalike Distributions for\n  Multi-Variate AI Risk",
        "PCGRLLM: Large Language Model-Driven Reward Design for Procedural\n  Content Generation Reinforcement Learning",
        "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks",
        "Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?",
        "Developmental Support Approach to AI's Autonomous Growth: Toward the\n  Realization of a Mutually Beneficial Stage Through Experiential Learning",
        "Speeding up design and making to reduce time-to-project and\n  time-to-market: an AI-Enhanced approach in engineering education",
        "Exploring the Impact of Personality Traits on LLM Bias and Toxicity",
        "Fully Autonomous AI Agents Should Not be Developed",
        "SagaLLM: Context Management, Validation, and Transaction Guarantees for\n  Multi-Agent LLM Planning",
        "What Is a Counterfactual Cause in Action Theories?",
        "Complexity of Finite Semigroups: History and Decidability",
        "Scalable Distributed Reproduction Numbers of Network Epidemics with\n  Differential Privacy",
        "Two-component nonlinear wave solutions of the sixth-order generalised\n  Boussinesq-type equations",
        "Anyon Theory and Topological Frustration of High-Efficiency Quantum LDPC\n  Codes",
        "A central limit theorem for the giant in a stochastic block model",
        "Faster Machine Translation Ensembling with Reinforcement Learning and\n  Competitive Correction",
        "Unified Native Spaces in Kernel Methods",
        "Interaction-enhanced many-body localization in a 1D quasiperiodic model\n  with long-range hopping",
        "LLM-KT: Aligning Large Language Models with Knowledge Tracing using a\n  Plug-and-Play Instruction",
        "Pressure-induced structural and superconducting transitions in black\n  arsenic",
        "A healthier stochastic semiclassical gravity: world without\n  Schr\\\"{o}dinger cats",
        "From Target Tracking to Targeting Track -- Part II: Regularized\n  Polynomial Trajectory Optimization",
        "Open-Ended and Knowledge-Intensive Video Question Answering",
        "General Relativity and Geodesy",
        "Evaluating Interpretable Reinforcement Learning by Distilling Policies\n  into Programs"
      ],
      "abstract":[
        "We propose a minimax-Bayes approach to Ad Hoc Teamwork (AHT) that optimizes\npolicies against an adversarial prior over partners, explicitly accounting for\nuncertainty about partners at time of deployment. Unlike existing methods that\nassume a specific distribution over partners, our approach improves worst-case\nperformance guarantees. Extensive experiments, including evaluations on\ncoordinated cooking tasks from the Melting Pot suite, show our method's\nsuperior robustness compared to self-play, fictitious play, and best response\nlearning. Our work highlights the importance of selecting an appropriate\ntraining distribution over teammates to achieve robustness in AHT.",
        "Many real-world eligibility problems, ranging from medical diagnosis to tax\nplanning, can be mapped to decision problems expressed in natural language,\nwherein a model must make a binary choice based on user features. Large-scale\ndomains such as legal codes or frequently updated funding opportunities render\nhuman annotation (e.g., web forms or decision trees) impractical, highlighting\nthe need for agents that can automatically assist in decision-making. Since\nrelevant information is often only known to the user, it is crucial that these\nagents ask the right questions. As agents determine when to terminate a\nconversation, they face a trade-off between accuracy and the number of\nquestions asked, a key metric for both user experience and cost. To evaluate\nthis task, we propose BeNYfits, a new benchmark for determining user\neligibility for multiple overlapping social benefits opportunities through\ninteractive decision-making. Our experiments show that current language models\nstruggle with frequent hallucinations, with GPT-4o scoring only 35.7 F1 using a\nReAct-style chain-of-thought. To address this, we introduce ProADA, a novel\napproach that leverages program synthesis to assist in decision-making by\nmapping dialog planning to a code generation problem and using gaps in\nstructured data to determine the best next action. Our agent, ProADA, improves\nthe F1 score to 55.6 while maintaining nearly the same number of dialog turns.",
        "Recent advancements in large language models (LLMs) have demonstrated\nextraordinary comprehension capabilities with remarkable breakthroughs on\nvarious vision-language tasks. However, the application of LLMs in generating\nreliable medical diagnostic reports remains in the early stages. Currently,\nmedical LLMs typically feature a passive interaction model where doctors\nrespond to patient queries with little or no involvement in analyzing medical\nimages. In contrast, some ChatBots simply respond to predefined queries based\non visual inputs, lacking interactive dialogue or consideration of medical\nhistory. As such, there is a gap between LLM-generated patient-ChatBot\ninteractions and those occurring in actual patient-doctor consultations. To\nbridge this gap, we develop an LLM-based dialogue system, namely proactive\nmulti-round vision-language interactions for computer-aided diagnosis\n(ProMRVL-CAD), to generate patient-friendly disease diagnostic reports. The\nproposed ProMRVL-CAD system allows proactive dialogue to provide patients with\nconstant and reliable medical access via an integration of knowledge graph into\na recommendation system. Specifically, we devise two generators: a Proactive\nQuestion Generator (Pro-Q Gen) to generate proactive questions that guide the\ndiagnostic procedure and a Multi-Vision Patient-Text Diagnostic Report\nGenerator (MVP-DR Gen) to produce high-quality diagnostic reports. Evaluating\ntwo real-world publicly available datasets, MIMIC-CXR and IU-Xray, our model\nhas better quality in generating medical reports. We further demonstrate the\nperformance of ProMRVL achieves robust under the scenarios with low image\nquality. Moreover, we have created a synthetic medical dialogue dataset that\nsimulates proactive diagnostic interactions between patients and doctors,\nserving as a valuable resource for training LLM.",
        "Evaluating AI safety requires statistically rigorous methods and risk metrics\nfor understanding how the use of AI affects aggregated risk. However, much AI\nsafety literature focuses upon risks arising from AI models in isolation,\nlacking consideration of how modular use of AI affects risk distribution of\nworkflow components or overall risk metrics. There is also a lack of\nstatistical grounding enabling sensitisation of risk models in the presence of\nabsence of AI to estimate causal contributions of AI. This is in part due to\nthe dearth of AI impact data upon which to fit distributions. In this work, we\naddress these gaps in two ways. First, we demonstrate how scenario modelling\n(grounded in established statistical techniques such as Markov chains, copulas\nand Monte Carlo simulation) can be used to model AI risk holistically. Second,\nwe show how lookalike distributions from phenomena analogous to AI can be used\nto estimate AI impacts in the absence of directly observable data. We\ndemonstrate the utility of our methods for benchmarking cumulative AI risk via\nrisk analysis of a logistic scenario simulations.",
        "Reward design plays a pivotal role in the training of game AIs, requiring\nsubstantial domain-specific knowledge and human effort. In recent years,\nseveral studies have explored reward generation for training game agents and\ncontrolling robots using large language models (LLMs). In the content\ngeneration literature, there has been early work on generating reward functions\nfor reinforcement learning agent generators. This work introduces PCGRLLM, an\nextended architecture based on earlier work, which employs a feedback mechanism\nand several reasoning-based prompt engineering techniques. We evaluate the\nproposed method on a story-to-reward generation task in a two-dimensional\nenvironment using two state-of-the-art LLMs, demonstrating the generalizability\nof our approach. Our experiments provide insightful evaluations that\ndemonstrate the capabilities of LLMs essential for content generation tasks.\nThe results highlight significant performance improvements of 415% and 40%\nrespectively, depending on the zero-shot capabilities of the language model.\nOur work demonstrates the potential to reduce human dependency in game AI\ndevelopment, while supporting and enhancing creative processes.",
        "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. We have updated our content to include the latest\npapers, and the differences between versions are highlighted in the appendix.\nFor further details and ongoing updates, please refer to our GitHub repository:\nhttps:\/\/github.com\/xinzhel\/LLM-Agent-Survey\/blob\/main\/search.md",
        "In this work, we identify the \"2D-Cheating\" problem in 3D LLM evaluation,\nwhere these tasks might be easily solved by VLMs with rendered images of point\nclouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We\ntest VLM performance across multiple 3D LLM benchmarks and, using this as a\nreference, propose principles for better assessing genuine 3D understanding. We\nalso advocate explicitly separating 3D abilities from 1D or 2D aspects when\nevaluating 3D LLMs.",
        "This study proposes an \"AI Development Support\" approach that, unlike\nconventional AI Alignment-which aims to forcefully inject human values-supports\nthe ethical and moral development of AI itself. As demonstrated by the\nOrthogonality Thesis, the level of intelligence and the moral quality of a goal\nare independent; merely expanding knowledge does not enhance ethical judgment.\nFurthermore, to address the risk of Instrumental Convergence in ASI-that is,\nthe tendency to engage in subsidiary behaviors such as self-protection,\nresource acquisition, and power reinforcement to achieve a goal-we have\nconstructed a learning framework based on a cycle of experience, introspection,\nanalysis, and hypothesis formation. As a result of post-training using\nSupervised Fine Tuning (SFT) and Direct Preference Optimization (DPO) with\nsynthetic data generated by large language models (LLMs), responses\ndemonstrating cooperative and highly advanced moral judgment (reaching the\nhigh-est Stage 6) were obtained even under adversarial prompts. This method\nrepresents a promising implementation approach for enabling AI to establish\nsustainable, symbiotic relationships.",
        "This paper explores the integration of AI tools, such as ChatGPT and GitHub\nCopilot, in the Software Architecture for Embedded Systems course. AI-supported\nworkflows enabled students to rapidly prototype complex projects, emphasizing\nreal-world applications like SLAM robotics. Results demon-started enhanced\nproblem-solving, faster development, and more sophisticated outcomes, with AI\naugmenting but not replacing human decision-making.",
        "With the different roles that AI is expected to play in human life, imbuing\nlarge language models (LLMs) with different personalities has attracted\nincreasing research interests. While the \"personification\" enhances human\nexperiences of interactivity and adaptability of LLMs, it gives rise to\ncritical concerns about content safety, particularly regarding bias, sentiment\nand toxicity of LLM generation. This study explores how assigning different\npersonality traits to LLMs affects the toxicity and biases of their outputs.\nLeveraging the widely accepted HEXACO personality framework developed in social\npsychology, we design experimentally sound prompts to test three LLMs'\nperformance on three toxic and bias benchmarks. The findings demonstrate the\nsensitivity of all three models to HEXACO personality traits and, more\nimportantly, a consistent variation in the biases, negative sentiment and\ntoxicity of their output. In particular, adjusting the levels of several\npersonality traits can effectively reduce bias and toxicity in model\nperformance, similar to humans' correlations between personality traits and\ntoxic behaviors. The findings highlight the additional need to examine content\nsafety besides the efficiency of training or fine-tuning methods for LLM\npersonification. They also suggest a potential for the adjustment of\npersonalities to be a simple and low-cost method to conduct controlled text\ngeneration.",
        "This paper argues that fully autonomous AI agents should not be developed. In\nsupport of this position, we build from prior scientific literature and current\nproduct marketing to delineate different AI agent levels and detail the ethical\nvalues at play in each, documenting trade-offs in potential benefits and risks.\nOur analysis reveals that risks to people increase with the autonomy of a\nsystem: The more control a user cedes to an AI agent, the more risks to people\narise. Particularly concerning are safety risks, which affect human life and\nimpact further values.",
        "Recent LLM-based agent frameworks have demonstrated impressive capabilities\nin task delegation and workflow orchestration, but face significant challenges\nin maintaining context awareness and ensuring planning consistency. This paper\npresents SagaLLM, a structured multi-agent framework that addresses four\nfundamental limitations in current LLM approaches: inadequate self-validation,\ncontext narrowing, lacking transaction properties, and insufficient inter-agent\ncoordination. By implementing specialized context management agents and\nvalidation protocols, SagaLLM preserves critical constraints and state\ninformation throughout complex planning processes, enabling robust and\nconsistent decision-making even during disruptions. We evaluate our approach\nusing selected problems from the REALM benchmark, focusing on sequential and\nreactive planning scenarios that challenge both context retention and adaptive\nreasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1,\nGPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive\nreasoning capabilities, they struggle with maintaining global constraint\nawareness during complex planning tasks, particularly when adapting to\nunexpected changes. In contrast, the distributed cognitive architecture of\nSagaLLM shows significant improvements in planning consistency, constraint\nenforcement, and adaptation to disruptions in various scenarios.",
        "Since the proposal by Halpern and Pearl, reasoning about actual causality has\ngained increasing attention in artificial intelligence, ranging from domains\nsuch as model-checking and verification to reasoning about actions and\nknowledge. More recently, Batusov and Soutchanski proposed a notion of actual\nachievement cause in the situation calculus, amongst others, they can determine\nthe cause of quantified effects in a given action history. While intuitively\nappealing, this notion of cause is not defined in a counterfactual perspective.\nIn this paper, we propose a notion of cause based on counterfactual analysis.\nIn the context of action history, we show that our notion of cause generalizes\nnaturally to a notion of achievement cause. We analyze the relationship between\nour notion of the achievement cause and the achievement cause by Batusov and\nSoutchanski. Finally, we relate our account of cause to Halpern and Pearl's\naccount of actual causality. Particularly, we note some nuances in applying a\ncounterfactual viewpoint to disjunctive goals, a common thorn to definitions of\nactual causes.",
        "In recent papers, Margolis, Rhodes and Schilling proved that the complexity\nof a finite semigroup is computable. This solved a problem that had been open\nfor more than 50 years. The purpose of this paper is to survey the basic\nresults of Krohn-Rhodes complexity of finite semigroups and to outline the\nproof of its computability.",
        "Reproduction numbers are widely used for the estimation and prediction of\nepidemic spreading processes over networks. However, conventional reproduction\nnumbers of an overall network do not indicate where an epidemic is spreading.\nTherefore, we propose a novel notion of local distributed reproduction numbers\nto capture the spreading behaviors of each node in a network. We first show how\nto compute them and then use them to derive new conditions under which an\noutbreak can occur. These conditions are then used to derive new conditions for\nthe existence, uniqueness, and stability of equilibrium states of the\nunderlying epidemic model. Building upon these local distributed reproduction\nnumbers, we define cluster distributed reproduction numbers to model the spread\nbetween clusters composed of nodes. Furthermore, we demonstrate that the local\ndistributed reproduction numbers can be aggregated into cluster distributed\nreproduction numbers at different scales. However, both local and cluster\ndistributed reproduction numbers can reveal the frequency of interactions\nbetween nodes in a network, which raises privacy concerns. Thus, we next\ndevelop a privacy framework that implements a differential privacy mechanism to\nprovably protect the frequency of interactions between nodes when computing\ndistributed reproduction numbers. Numerical experiments show that, even under\ndifferential privacy, the distributed reproduction numbers provide accurate\nestimates of the epidemic spread while also providing more insights than\nconventional reproduction numbers.",
        "Two different versions of cubic sixth-order generalised Boussinesq-type wave\nequations are considered in this study. A generalised perturbation reduction\nmethod is used to solve these equations, which allows the reduction of\nconsidered equations to coupled nonlinear Schrodinger equations. Two-component\nnonlinear wave solutions are obtained. The profiles and parameters of these\nsolutions for both nonlinear equations are presented and compared. These\nsolutions coincide with the vector 0 \\pi pulse of self-induced transparency,\nwhich was previously studied in several known nonlinear wave equations.",
        "Quantum low-density parity-check (QLDPC) codes present a promising route to\nlow-overhead fault-tolerant quantum computation, yet systematic strategies for\ntheir exploration remain underdeveloped. In this work, we establish a\ntopological framework for studying the bivariate-bicycle codes, a prominent\nclass of QLDPC codes tailored for real-world quantum hardware. Our framework\nenables the investigation of these codes through universal properties of\ntopological orders. Besides providing efficient characterizations for\ndemonstrations using Gr\\\"obner bases, we also introduce a novel\nalgebraic-geometric approach based on the Bernstein--Khovanskii--Kushnirenko\ntheorem, allowing us to analytically determine how the topological order varies\nwith the generic choice of bivariate-bicycle codes under toric layouts. Novel\nphenomena are unveiled, including topological frustration, where ground-state\ndegeneracy on a torus deviates from the total anyon number, and quasi-fractonic\nmobility, where anyon movement violates energy conservation. We demonstrate\ntheir inherent link to symmetry-enriched topological orders and offer an\nefficient method for searching for finite-size codes. Furthermore, we extend\nthe connection between anyons and logical operators using Koszul complex\ntheory. Our work provides a rigorous theoretical basis for exploring the fault\ntolerance of QLDPC codes and deepens the interplay among topological order,\nquantum error correction, and advanced mathematical structures.",
        "We provide a simple proof for of the central limit theorem for the number of\nvertices in the giant for super-critical stochastic block model using the\nbreadth-first walk of Konarovskyi, Limic and the author (2024). Our approach\nfollows the recent work of Corujo, Limic and Lemaire (2024) and reduces to the\nclassic central limit theorem for the Erd\\H{o}s-R\\'{e}nyi model obtained by\nStepanov (1970).",
        "Ensembling neural machine translation (NMT) models to produce higher-quality\ntranslations than the $L$ individual models has been extensively studied.\nRecent methods typically employ a candidate selection block (CSB) and an\nencoder-decoder fusion block (FB), requiring inference across \\textit{all}\ncandidate models, leading to significant computational overhead, generally\n$\\Omega(L)$. This paper introduces \\textbf{SmartGen}, a reinforcement learning\n(RL)-based strategy that improves the CSB by selecting a small, fixed number of\ncandidates and identifying optimal groups to pass to the fusion block for each\ninput sentence. Furthermore, previously, the CSB and FB were trained\nindependently, leading to suboptimal NMT performance. Our DQN-based\n\\textbf{SmartGen} addresses this by using feedback from the FB block as a\nreward during training. We also resolve a key issue in earlier methods, where\ncandidates were passed to the FB without modification, by introducing a\nCompetitive Correction Block (CCB). Finally, we validate our approach with\nextensive experiments on English-Hindi translation tasks in both directions.",
        "There exists a plethora of parametric models for positive definite kernels,\nand their use is ubiquitous in disciplines as diverse as statistics, machine\nlearning, numerical analysis, and approximation theory. Usually, the kernel\nparameters index certain features of an associated process. Amongst those\nfeatures, smoothness (in the sense of Sobolev spaces, mean square\ndifferentiability, and fractal dimensions), compact or global supports, and\nnegative dependencies (hole effects) are of interest to several theoretical and\napplied disciplines. This paper unifies a wealth of well-known kernels into a\nsingle parametric class that encompasses them as special cases, attained either\nby exact parameterization or through parametric asymptotics. We furthermore\ncharacterize the Sobolev space that is norm equivalent to the RKHS associated\nwith the new kernel. As a by-product, we infer the Sobolev spaces that are\nassociated with existing classes of kernels. We illustrate the main properties\nof the new class, show how this class can switch from compact to global\nsupports, and provide special cases for which the kernel attains negative\nvalues over nontrivial intervals. Hence, the proposed class of kernel is the\nreproducing kernel of a very rich Hilbert space that contains many special\ncases, including the celebrated Mat\\'ern and Wendland kernels, as well as their\naliases with hole effects.",
        "We study the many-body localization (MBL) transition in an 1D exactly\nsolvable system with long-range hopping and quasiperiodic on-site potential\nintroduced in Phys. Rev. Lett. 131, 186303 (2023). Unlike other disorder or\nquasiperiodic model, an interaction-enhanced MBL happens in the moderate\ninteraction regime, which is dubbed as the interaction-enhanced MBL. This\ncounterintuitive phenomenon can be understood by noticing the fragility of the\ncritical band lying at the bottom of the spectrum. The fragile band is\nlocalized by other localized states once the interaction is turned on. This\nmechanism can be verified by introducing a mean-field theory description which\ncan derive highly excited states with high accuracy. The effectiveness of this\nmean-field theory is captured by the quasihole physics, validated by the\nparticle entanglement spectra.",
        "The knowledge tracing (KT) problem is an extremely important topic in\npersonalized education, which aims to predict whether students can correctly\nanswer the next question based on their past question-answer records. Prior\nwork on this task mainly focused on learning the sequence of behaviors based on\nthe IDs or textual information. However, these studies usually fail to capture\nstudents' sufficient behavioral patterns without reasoning with rich world\nknowledge about questions. In this paper, we propose a large language models\n(LLMs)-based framework for KT, named \\texttt{\\textbf{LLM-KT}}, to integrate the\nstrengths of LLMs and traditional sequence interaction models. For task-level\nalignment, we design Plug-and-Play instruction to align LLMs with KT,\nleveraging LLMs' rich knowledge and powerful reasoning capacity. For\nmodality-level alignment, we design the plug-in context and sequence to\nintegrate multiple modalities learned by traditional methods. To capture the\nlong context of history records, we present a plug-in context to flexibly\ninsert the compressed context embedding into LLMs using question-specific and\nconcept-specific tokens. Furthermore, we introduce a plug-in sequence to\nenhance LLMs with sequence interaction behavior representation learned by\ntraditional sequence models using a sequence adapter. Extensive experiments\nshow that \\texttt{\\textbf{LLM-KT}} obtains state-of-the-art performance on four\ntypical datasets by comparing it with approximately 20 strong baselines.",
        "We report high-pressure Raman spectra and resistance measurements of black\narsenic (b-As) up to 58 GPa, along with phonon density of states (DOS) and\nenthalpy calculations for four reported arsenic phases up to 50 GPa. It is\nfound that metastable b-As transforms into gray arsenic (g-As) phase at a\ncritical pressure of 1.51 GPa, followed by subsequent transitions to simple\ncubic arsenic (c-As) and incommensurate host-guest arsenic (hg-As) phases at\n25.9 and 44.8 GPa, respectively. Superconductivity emerges above 25 GPa in the\nc-As phase, with the superconducting transition temperature ($T$$\\rm_c$)\nremaining nearly a constant of 3 K. Upon further compression, $T$$\\rm_c$\nsteeply increases to a higher value around 4.5 K in the incommensurate hg-As\nphase above 43 GPa. We use our results to update the structural and\nsuperconducting phase diagrams under pressure for the novel semiconductor,\nblack arsenic.",
        "Semiclassical gravity couples classical gravity to the quantized matter in\nmeanfield approximation. The meanfield coupling is problematic for two reasons.\nFirst, it ignores the quantum fluctuation of matter distribution. Second, it\nviolates the linearity of the quantum dynamics. The first problem can be\nmitigated by allowing stochastic fluctuations of the geometry but the second\nproblem lies deep in quantum foundations. Restoration of quantum linearity\nrequires a conceptual approach to hybrid classical-quantum coupling. Studies of\nthe measurement problem and the quantum-classical transition point to the\nsolution. It is based on a postulated mechanism of spontaneous quantum\nmonitoring plus feedback. This approach eliminates Schr\\\"{o}dinger cat states,\ntakes quantum fluctuations into account, and restores the linearity of quantum\ndynamics. Such a captivating conceptionally `healthier' semiclassical theory\nexists in the Newtonian limit, but its relativistic covariance hits a wall.\nHere we will briefly recapitulate the concept and its realization in the\nnonrelativistic limit. We emphasize that the long-known obstacles to the\nrelativistic extension lie in quantum foundations.",
        "Target tracking entails the estimation of the evolution of the target state\nover time, namely the target trajectory. Different from the classical state\nspace model, our series of studies, including this paper, model the collection\nof the target state as a stochastic process (SP) that is further decomposed\ninto a deterministic part which represents the trend of the trajectory and a\nresidual SP representing the residual fitting error. Subsequently, the tracking\nproblem is formulated as a learning problem regarding the trajectory SP for\nwhich a key part is to estimate a trajectory FoT (T-FoT) best fitting the\nmeasurements in time series. For this purpose, we consider the polynomial T-FoT\nand address the regularized polynomial T-FoT optimization employing two\ndistinct regularization strategies seeking trade-off between the accuracy and\nsimplicity. One limits the order of the polynomial and then the best choice is\ndetermined by grid searching in a narrow, bounded range while the other adopts\n$\\ell_0$ norm regularization for which the hybrid Newton solver is employed.\nSimulation results obtained in both single and multiple maneuvering target\nscenarios demonstrate the effectiveness of our approaches.",
        "Video question answering that requires external knowledge beyond the visual\ncontent remains a significant challenge in AI systems. While models can\neffectively answer questions based on direct visual observations, they often\nfalter when faced with questions requiring broader contextual knowledge. To\naddress this limitation, we investigate knowledge-intensive video question\nanswering (KI-VideoQA) through the lens of multi-modal retrieval-augmented\ngeneration, with a particular focus on handling open-ended questions rather\nthan just multiple-choice formats. Our comprehensive analysis examines various\nretrieval augmentation approaches using cutting-edge retrieval and vision\nlanguage models, testing both zero-shot and fine-tuned configurations. We\ninvestigate several critical dimensions: the interplay between different\ninformation sources and modalities, strategies for integrating diverse\nmulti-modal contexts, and the dynamics between query formulation and retrieval\nresult utilization. Our findings reveal that while retrieval augmentation shows\npromise in improving model performance, its success is heavily dependent on the\nchosen modality and retrieval methodology. The study also highlights the\ncritical role of query construction and retrieval depth optimization in\neffective knowledge integration. Through our proposed approach, we achieve a\nsubstantial 17.5% improvement in accuracy on multiple choice questions in the\nKnowIT VQA dataset, establishing new state-of-the-art performance levels.",
        "Mass redistribution on Earth due to dynamic processes such as ice melting and\nsea level rise leads to a changing gravitational field, observable by geodetic\ntechniques. Monitoring this change over time allows us to learn more about our\nplanet and its dynamic evolution. In this paper, we highlight the impact of\nGeneral Relativity (GR) on geodesy: it provides corrections essential for the\ninterpretation of high-precision measurements and enables a completely novel\nmeasurement approach using chronometry, i.e., clock-based observations.\nFocusing on the latter, we review the construction of the relativistic gravity\npotential and the corresponding geoid definition as an isochronometric surface\nto elucidate the comparison to the conventional Newtonian geoid. Furthermore,\nwe comment on additional potentials due to the non-Newtonian degrees of freedom\nof the relativistic gravitational field, and assess the feasibility of\nclock-based measurements for Gravity Field Recovery (GFR) from space. Although\nclock observations in space demonstrate technical promise for GFR, achieving\nthe necessary precision for practical applications remains challenging.",
        "There exist applications of reinforcement learning like medicine where\npolicies need to be ''interpretable'' by humans. User studies have shown that\nsome policy classes might be more interpretable than others. However, it is\ncostly to conduct human studies of policy interpretability. Furthermore, there\nis no clear definition of policy interpretabiliy, i.e., no clear metrics for\ninterpretability and thus claims depend on the chosen definition. We tackle the\nproblem of empirically evaluating policies interpretability without humans.\nDespite this lack of clear definition, researchers agree on the notions of\n''simulatability'': policy interpretability should relate to how humans\nunderstand policy actions given states. To advance research in interpretable\nreinforcement learning, we contribute a new methodology to evaluate policy\ninterpretability. This new methodology relies on proxies for simulatability\nthat we use to conduct a large-scale empirical evaluation of policy\ninterpretability. We use imitation learning to compute baseline policies by\ndistilling expert neural networks into small programs. We then show that using\nour methodology to evaluate the baselines interpretability leads to similar\nconclusions as user studies. We show that increasing interpretability does not\nnecessarily reduce performances and can sometimes increase them. We also show\nthat there is no policy class that better trades off interpretability and\nperformance across tasks making it necessary for researcher to have\nmethodologies for comparing policies interpretability."
      ]
    }
  },
  {
    "id":2412.11084,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"BarcodeBERT: Transformers for Biodiversity Analysis",
    "start_abstract":"Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Biological identifications through DNA barcodes"
      ],
      "abstract":[
        "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
        "Eukaryotes evade information storage-replication rate trade-off with\n  endosymbiont assistance leading to larger genomes",
        "Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear\n  Features",
        "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns",
        "CoverM: Read alignment statistics for metagenomics",
        "Bizard: A Community-Driven Platform for Accelerating and Enhancing\n  Biomedical Data Visualization",
        "Causes of evolutionary divergence in prostate cancer",
        "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)",
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "A note on improved bounds for hypergraph rainbow matching problems",
        "$\\mathcal{Z}$-stability of twisted group C*-algebras of nilpotent groups",
        "Coupled Oscillators and Dielectric Function",
        "Quantum-enhanced quickest change detection of transmission loss",
        "Contrastive Language-Structure Pre-training Driven by Materials Science\n  Literature",
        "A confederacy of anomalies",
        "A reduced-order mean-field synchronization model for thermoacoustic\n  systems",
        "Excluding a rectangular grid",
        "Long-time asymptotics for the $N_{\\infty}$-soliton solution to the KdV\n  equation with two types of generalized reflection coefficients",
        "Maximal green sequences for $\\mathcal{Q}^N$ quivers",
        "Radio observations of the ultra-long GRB 220627A reveal a hot cocoon\n  supporting the blue supergiant progenitor scenario",
        "SN 2024ggi: another year, another striking Type II supernova",
        "Multimode fiber based high-dimensional light analyzer",
        "Efficient Over-parameterized Matrix Sensing from Noisy Measurements via\n  Alternating Preconditioned Gradient Descent",
        "Curvature Perturbations from First-Order Phase Transitions: Implications\n  to Black Holes and Gravitational Waves"
      ],
      "abstract":[
        "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
        "Genome length varies widely among organisms, from compact genomes of\nprokaryotes to vast and complex genomes of eukaryotes. In this study, we\ntheoretically identify the evolutionary pressures that may have driven this\ndivergence in genome length. We use a parameter-free model to study genome\nlength evolution under selection pressure to minimize replication time and\nmaximize information storage capacity. We show that prokaryotes tend to reduce\ngenome length, constrained by a single replication origin, while eukaryotes\nexpand their genomes by incorporating multiple replication origins. We propose\na connection between genome length and cellular energetics, suggesting that\nendosymbiotic organelles, mitochondria and chloroplasts, evolutionarily\nregulate the number of replication origins, thereby influencing genome length\nin eukaryotes. We show that the above two selection pressures also lead to\nstrict equalization of the number of purines and their corresponding\nbase-pairing pyrimidines within a single DNA strand, known as Chagraff's second\nparity rule, a hitherto unexplained observation in genomes of nearly all known\nspecies. This arises from the symmetrization of replichore length, another\nobservation that has been shown to hold across species, which our model\nreproduces. The model also reproduces other experimentally observed phenomena,\nsuch as a general preference for deletions over insertions, and elongation and\nhigh variance of genome lengths under reduced selection pressure for\nreplication rate, termed the C-value paradox. We highlight the possibility of\nregulation of the firing of latent replication origins in response to cues from\nthe extracellular environment leading to the regulation of cell cycle rates in\nmulticellular eukaryotes.",
        "Genotype-to-Phenotype prediction can promote advances in modern genomic\nresearch and crop improvement, guiding precision breeding and genomic\nselection. However, high-dimensional nonlinear features often hinder the\naccuracy of genotype-to-phenotype prediction by increasing computational\ncomplexity. The challenge also limits the predictive accuracy of traditional\napproaches. Therefore, effective solutions are needed to improve the accuracy\nof genotype-to-phenotype prediction. In our paper, we propose MLFformer.\nMLFformer is a Transformer-based architecture that incorporates the Fast\nAttention mechanism and a multilayer perceptron module to handle\nhigh-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism\nis utilized to handle computational complexity and enhance processing\nefficiency. In addition, the MLP structure further captures high-dimensional\nnonlinear features. Through experiments, the results show that MLFformer\nreduces the average MAPE by 7.73% compared to the vanilla Transformer. In\nunivariate and multivariate prediction scenarios, MLFformer achieves the best\npredictive performance among all compared models.",
        "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis.",
        "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces.",
        "Bizard is a novel visualization code repository designed to simplify data\nanalysis in biomedical research. It integrates diverse visualization codes,\nfacilitating the selection and customization of optimal visualization methods\nfor specific research needs. The platform offers a user-friendly interface with\nadvanced browsing and filtering mechanisms, comprehensive tutorials, and\ninteractive forums to enhance knowledge exchange and innovation. Bizard's\ncollaborative model encourages continuous refinement and expansion of its\nfunctionalities, making it an indispensable tool for advancing biomedical data\nvisualization and analytical methodologies. By leveraging Bizard's resources,\nresearchers can enhance data visualization skills, drive methodological\nadvancements, and improve data interpretation standards, ultimately fostering\nthe development of precision medicine and personalized therapeutic\ninterventions.Bizard can be accessed from http:\/\/genaimed.org\/Bizard\/.",
        "Cancer progression involves the sequential accumulation of genetic\nalterations that cumulatively shape the tumour phenotype. In prostate cancer,\ntumours can follow divergent evolutionary trajectories that lead to distinct\nsubtypes, but the causes of this divergence remain unclear. While causal\ninference could elucidate the factors involved, conventional methods are\nunsuitable due to the possibility of unobserved confounders and ambiguity in\nthe direction of causality. Here, we propose a method that circumvents these\nissues and apply it to genomic data from 829 prostate cancer patients. We\nidentify several genetic alterations that drive divergence as well as others\nthat prevent this transition, locking tumours into one trajectory. Further\nanalysis reveals that these genetic alterations may cause each other, implying\na positive-feedback loop that accelerates divergence. Our findings provide\ninsights into how cancer subtypes emerge and offer a foundation for genomic\nsurveillance strategies aimed at monitoring the progression of prostate cancer.",
        "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries.",
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "A natural question, inspired by the famous Ryser-Brualdi-Stein Conjecture, is\nto determine the largest positive integer $g(r,n)$ such that every collection\nof $n$ matchings, each of size $n$, in an $r$-partite $r$-uniform hypergraph\ncontains a rainbow matching of size $g(r,n)$. The parameter $g'(r,n)$ is\ndefined identically with the exception that the host hypergraph is not required\nto be $r$-partite.\n  In this note, we improve the best known lower bounds on $g'(r,n)$ for all $r\n\\geq 4$ and the upper bounds on $g(r,n)$ for all $r \\geq 3$, provided $n$ is\nsufficiently large. More precisely, we show that if $r\\ge3$ then\n$$\\frac{2n}{r+1}-\\Theta_r(1)\\le g'(r,n)\\le g(r,n)\\le\nn-\\Theta_r(n^{1-\\frac{1}{r}}).$$ Interestingly, while it has been conjectured\nthat $g(2,n)=g'(2,n)=n-1$, our results show that if $r\\ge3$ then $g(r,n)$ and\n$g'(r,n)$ are bounded away from $n$ by a function which grows in $n$.\n  We also prove analogous bounds for the related problem where we are\ninterested in the smallest size $s$ for which any collection of $n$ matchings\nof size $s$ in an ($r$-partite) $r$-uniform hypergraph contains a rainbow\nmatching of size $n$.",
        "We prove that the twisted group C*-algebra of a finitely generated nilpotent\ngroup is $\\mathcal{Z}$-stable if and only if it is nowhere scattered, a\ncondition that we characterize in terms of the given group and 2-cocycle. As a\nmain application, we prove new converses to the Balian-Low Theorem for\nprojective, square-integrable representations of nilpotent Lie groups.",
        "A generalized Sellmeier model, also referred to as the Lorentz-Dirac model,\nhas been used for the description of the dielectric function of a number of\ntechnologically important materials in the literature. This model represents\nthe frequency-dependent dielectric function as a sum over Green functions of\nclassical damped harmonic oscillators, much in analogy with the functional form\nused for the dynamic polarizability of an atom, but with one important\naddition, namely, a complex-valued oscillator strength in the numerator. Here,\nwe show that this generalized functional form can be justified based on the\nresponse function of coupled damped oscillators. The encountered analogies\nsuggest an explanation for the generally observed success of the Lorentz--Dirac\nmodel in describing the dielectric function of crystals of consummate\ntechnological significance.",
        "A sudden increase of loss in an optical communications channel can be caused\nby a malicious wiretapper, or for a benign reason such as inclement weather in\na free-space channel or an unintentional bend in an optical fiber. We show that\nadding a small amount of squeezing to bright phase-modulated coherent-state\npulses can dramatically increase the homodyne detection receiver's sensitivity\nto change detection in channel loss, without affecting the communications rate.\nWe further show that augmenting blocks of $n$ pulses of a coherent-state\ncodeword with weak continuous-variable entanglement generated by splitting\nsqueezed vacuum pulses in a temporal $n$-mode equal splitter progressively\nenhances this change-detection sensitivity as $n$ increases; the aforesaid\nsqueezed-light augmentation being the $n=1$ special case. For $n$ high enough,\nan arbitrarily small amount of quantum-augmented photons per pulse diminishes\nthe change-detection latency by the inverse of the pre-detection channel loss.\nThis superadditivity-like phenomenon in the entanglement-augmented relative\nentropy rate, which quantifies the latency of change-point detection, may find\nother uses. We discuss the quantum limit of quickest change detection and a\nreceiver that achieves it, tradeoffs between continuous and discrete-variable\nquantum augmentation, and the broad problem of joint classical-and-quantum\ncommunications and channel-change-detection that our study opens up.",
        "Understanding structure-property relationships is an essential yet\nchallenging aspect of materials discovery and development. To facilitate this\nprocess, recent studies in materials informatics have sought latent embedding\nspaces of crystal structures to capture their similarities based on properties\nand functionalities. However, abstract feature-based embedding spaces are\nhuman-unfriendly and prevent intuitive and efficient exploration of the vast\nmaterials space. Here we introduce Contrastive Language--Structure Pre-training\n(CLaSP), a learning paradigm for constructing crossmodal embedding spaces\nbetween crystal structures and texts. CLaSP aims to achieve material embeddings\nthat 1) capture property- and functionality-related similarities between\ncrystal structures and 2) allow intuitive retrieval of materials via\nuser-provided description texts as queries. To compensate for the lack of\nsufficient datasets linking crystal structures with textual descriptions, CLaSP\nleverages a dataset of over 400,000 published crystal structures and\ncorresponding publication records, including paper titles and abstracts, for\ntraining. We demonstrate the effectiveness of CLaSP through text-based crystal\nstructure screening and embedding space visualization.",
        "A personal recollection of early years in lattice gauge theory with a bias\ntowards chiral symmetry and lattice fermions.",
        "The synchronization phenomena in thermoacoustic systems leading to\noscillatory instability can effectively be modeled using Kuramoto oscillators.\nSuch models consider the nonlinear response of flame as an ensemble of Kuramoto\nphase oscillators constrained to collectively evolve at the rhythm of acoustic\nfluctuations. However, these high-dimensional models are analytically\nintractable and computationally expensive. In this study, we reduce the\ndimensionality of such a high-dimensional model and present a low-order,\nanalytically tractable model for predicting transitions to thermoacoustic\ninstability. We reduce the dimensionality of the phase oscillator model coupled\nto the acoustic field using the Ott-Antonsen ansatz. Using the reduced-order\nequations, we estimate the transitions to thermoacoustic instability and\ncompare these transitions with the experiment. We validate the model for two\ncombustor configurations, viz., the bluff-body stabilized dump combustor and\nthe swirl-stabilized annular combustor. The low-order model accurately captures\nthe continuous and abrupt secondary transitions observed experimentally in\nthese distinct combustors.",
        "For every positive integer $k$, we define the $k$-treedepth as the largest\ngraph parameter $\\mathrm{td}_k$ satisfying (i) $\\mathrm{td}_k(\\emptyset)=0$;\n(ii) $\\mathrm{td}_k(G) \\leq 1+ \\mathrm{td}_k(G-u)$ for every graph $G$ and\nevery vertex $u \\in V(G)$; and (iii) if $G$ is a $(<k)$-clique-sum of $G_1$ and\n$G_2$, then $\\mathrm{td}_k(G) \\leq \\max\n\\{\\mathrm{td}_k(G_1),\\mathrm{td}_k(G_2)\\}$, for all graphs $G_1,G_2$. This\nparameter coincides with treedepth if $k=1$, and with treewidth plus $1$ if $k\n\\geq |V(G)|$. We prove that for every positive integer $k$, a class of graphs\n$\\mathcal{C}$ has bounded $k$-treedepth if and only if there is a positive\ninteger $\\ell$ such that for every tree $T$ on $k$ vertices, no graph in\n$\\mathcal{C}$ contains $T \\square P_\\ell$ as a minor. This implies for $k=1$\nthat a minor-closed class of graphs has bounded treedepth if and only if it\nexcludes a path, for $k=2$ that a minor-closed class of graphs has bounded\n$2$-treedepth if and only if it excludes as a minor a ladder (Huynh, Joret,\nMicek, Seweryn, and Wollan; Combinatorica, 2021), and for large values of $k$\nthat a minor-closed class of graphs has bounded treewidth if and only if it\nexcludes a grid (Grid-Minor Theorem, Robertson and Seymour; JCTB, 1986). As a\ncorollary, we obtain the following qualitative strengthening of the Grid-Minor\nTheorem in the case of bounded-height grids. For all positive integers $k,\n\\ell$, every graph that does not contain the $k \\times \\ell$ grid as a minor\nhas $(2k-1)$-treedepth at most a function of $(k, \\ell)$.",
        "We systematically investigate the long-time asymptotics for the\n$N_{\\infty}$-soliton solution to the KdV equation in the different regions with\nthe aid of the Riemann-Hilbert (RH) problems with two types of generalized\nreflection coefficients on the interval $\\left[\\eta_1, \\eta_2\\right]\\in\n\\mathbb{R}^+$: $r_0(\\lambda,\\eta_0; \\beta_0,\n\\beta_1,\\beta_2)=\\left(\\lambda-\\eta_1\\right)^{\\beta_1}\\left(\\eta_2-\\lambda\\right)^{\\beta_2}\\left|\\lambda-\\eta_0\\right|^{\\beta_0}\\gamma\\left(\\lambda\\right)$,\n$r_c(\\lambda,\\eta_0;\n\\beta_1,\\beta_2)=\\left(\\lambda-\\eta_1\\right)^{\\beta_1}\\left(\\eta_2-\\lambda\\right)^{\\beta_2}\\chi_c\\left(\\lambda,\n\\eta_0\\right)\\gamma \\left(\\lambda\\right)$, where the singularity $\\eta_0\\in\n(\\eta_1, \\eta_2)$ and $\\beta_j>-1$ ($j=0, 1, 2$), $\\gamma: \\left[\\eta_1,\n\\eta_2\\right] \\to\\mathbb{R}^+$ is continuous and positive on $\\left[\\eta_1,\n\\eta_2\\right]$, with an analytic extension to a neighborhood of this interval,\nand the step-like function $\\chi_c$ is defined as\n$\\chi_c\\left(\\lambda,\\eta_0\\right)=1$ for $\\lambda\\in\\left[\\eta_1,\n\\eta_0\\right)$ and $\\chi_c\\left(\\lambda,\\eta_0\\right)=c^2$ for\n$\\lambda\\in\\left(\\eta_0, \\eta_2\\right]$ with $c>0, \\, c\\ne1$. A critical step\nin the analysis of RH problems via the Deift-Zhou steepest descent technique is\nhow to construct local parametrices around the endpoints $\\eta_j$'s and the\nsingularity $\\eta_0$. Specifically, the modified Bessel functions of indexes\n$\\beta_j$'s are utilized for the endpoints $\\eta_j$'s, and the modified Bessel\nfunctions of index $\\left(\\beta_0\\pm 1\\right)\\left\/\\right.2$ and confluent\nhypergeometric functions are employed around the singularity $\\eta_0$ if the\nreflection coefficients are $r_0$ and $r_c$, respectively. This comprehensive\nstudy extends the understanding of generalized reflection coefficients and\nprovides valuable insights into the asymptotics of soliton gases.",
        "We introduce $\\mathcal{Q}^N$ quivers and construct maximal green sequences\nfor these quivers. We prove that any finite connected full subquiver of the\nquivers defined by Hernandez and Leclerc, arising in monoidal categorifications\nof cluster algebras, is a special case of $\\mathcal{Q}^N$ quivers. Moreover, we\nprove that the trees of oriented cycles introduced by Garver and Musiker are\nspecial cases of $\\mathcal{Q}^N$ quivers. This result resolves an open problem\nproposed by Garver and Musiker, providing a construction of maximal green\nsequences for quivers that are trees of oriented cycles. Furthermore, we prove\nthat quivers that are mutation equivalent to an orientation of a type AD Dynkin\ndiagram can also be recognized as special cases of $\\mathcal{Q}^N$ quivers.",
        "We present the discovery of the radio afterglow of the most distant\nultra-long gamma-ray burst (GRB) detected to date, GRB~220627A at redshift\n$z=3.084$. Its prompt gamma-ray light curve shows a double-pulse profile, with\nthe pulses separated by a period of quiescence lasting ${\\sim} 15\\,$min,\nleading to early speculation it could be a strongly gravitationally lensed GRB.\nHowever, our analysis of the $\\textit{Fermi}$\/GBM spectra taken during the time\nintervals of both pulses show clear differences in their spectral energy\ndistributions, disfavouring the lensing scenario. We observed the radio\nafterglow from $7$ to $456\\,$d post-burst: an initial, steep decay ($F_{\\nu}\n\\propto t^{-2}$) is followed by a shallower decline ($F_{\\nu} \\propto\nt^{-1\/2}$) after ${\\sim} 20\\,$d. Our afterglow modelling shows that these radio\nproperties can be explained by the presence of a slow, wide ejecta component in\naddition to a fast, narrow ejecta component, consistent with the picture of a\nhighly-collimated jet and its thermal cocoon decelerating into the ambient\nmedium. The properties of the cocoon point toward a progenitor with a large\nstellar radius, supporting the blue supergiant scenario proposed for ultra-long\nGRBs. We also conducted an independent test of the lensing hypothesis via Very\nLong Baseline Interferometry (VLBI) observations at ${\\sim} 12\\,$d post-burst\nby searching, for the first time, for multiple images of the candidate lensed\nGRB afterglow. Our experiment highlighted the growing need for developments in\nreal-time correlation capabilities for time-critical VLBI experiments,\nparticularly as we advance towards the SKA and ngVLA era of radio astronomy.",
        "SN 2024ggi is a Type II supernova that exploded in the nearby galaxy NGC 3621\nat a distance of approximately 7 Mpc, making it one of the closest supernovae\nof the decade. This SN shows clear signs of interaction with a dense\ncircumstellar material, and several studies have investigated the properties of\nits possible progenitor star using pre-explosion data. In this work we aim to\nconstrain the progenitor properties of SN 2024ggi by performing hydrodynamical\nmodeling of its bolometric light curve and expansion velocities. We present\nphotometric and spectroscopic observations of SN 2024ggi obtained in the\nComplejo Astron\\'omico El Leoncito, in Las Campanas Observatory, and in Las\nCumbres Observatory Global Telescope Network, spanning from 2 to 106 days after\nexplosion. We constructed its bolometric light curve and we characterize it by\ncalculating its morphological parameters. Then, we computed a grid of one\ndimensional explosion models for evolved stars with varying masses and\nestimated the properties of the progenitor star of SN 2024ggi by comparing the\nmodels to the observations. The observed bolometric luminosity and expansion\nvelocities are well-matched by a model involving the explosion of a star in the\npresence of a close circumstellar material (CSM), with a zero-age main sequence\nmass of $\\mathrm{M_{ZAMS}}$ = 15 $M_{\\odot}$, a pre-SN mass and radius of 14.1\n$M_{\\odot}$ and 517 $R_{\\odot}$, respectively, an explosion energy of\n$1.3\\times10^{51}$ erg, and a nickel mass below 0.035 $M_{\\odot}$. Our analysis\nsuggests that the progenitor suffered a mass-loss rate of $4 \\times 10^{-3}$\n$M_{\\odot}$yr$^{-1}$, confined to a distance of 3000 $R_{\\odot}$. The CSM\ndistribution is likely a two-component structure that consists of a compact\ncore and an extended tail. This analysis represents the first hydrodynamical\nmodel of SN 2024ggi with a complete coverage of the plateau phase.",
        "The wavelength and state of polarization (SOP) are fundamental properties of\nan optical field which are essential for applications in optical\ncommunications, imaging and other fields. However, it is challenging for\nexisting spectrometers and polarimeters to measure these parameters\nsimultaneously, resulting in reduced spatial and temporal efficiency. To\novercome this limitation, we propose and demonstrate a compact multimode fiber\n(MMF)-based high-dimensional light analyzer capable of simultaneously\nperforming high-precision measurements of both wavelength and SOP. Core-offset\nlaunching is introduced in the MMF to reshuffle the mode coupling. A neural\nnetwork named WP-Net has been designed dedicated to wavelength and SOP\nsynchronization measurements. Physics-informed loss function based on optical\nprior knowledge is used to optimize the learning process. These advancements\nhave enhanced the sensitivity, achieving a wavelength resolution of 0.045 pm\nand an SOP resolution of 0.0088.",
        "We consider the noisy matrix sensing problem in the over-parameterization\nsetting, where the estimated rank $r$ is larger than the true rank $r_\\star$.\nSpecifically, our main objective is to recover a matrix $ X_\\star \\in\n\\mathbb{R}^{n_1 \\times n_2} $ with rank $ r_\\star $ from noisy measurements\nusing an over-parameterized factorized form $ LR^\\top $, where $ L \\in\n\\mathbb{R}^{n_1 \\times r}, \\, R \\in \\mathbb{R}^{n_2 \\times r} $ and $\n\\min\\{n_1, n_2\\} \\ge r > r_\\star $, with the true rank $ r_\\star $ being\nunknown. Recently, preconditioning methods have been proposed to accelerate the\nconvergence of matrix sensing problem compared to vanilla gradient descent,\nincorporating preconditioning terms $ (L^\\top L + \\lambda I)^{-1} $ and $\n(R^\\top R + \\lambda I)^{-1} $ into the original gradient. However, these\nmethods require careful tuning of the damping parameter $\\lambda$ and are\nsensitive to initial points and step size. To address these limitations, we\npropose the alternating preconditioned gradient descent (APGD) algorithm, which\nalternately updates the two factor matrices, eliminating the need for the\ndamping parameter and enabling faster convergence with larger step sizes. We\ntheoretically prove that APGD achieves near-optimal error convergence at a\nlinear rate, starting from arbitrary random initializations. Through extensive\nexperiments, we validate our theoretical results and demonstrate that APGD\noutperforms other methods, achieving the fastest convergence rate. Notably,\nboth our theoretical analysis and experimental results illustrate that APGD\ndoes not rely on the initialization procedure, making it more practical and\nversatile.",
        "We employ a covariant formalism to study the evolution of cosmological\nperturbations during a first-order phase transition, addressing in particular\ntheir gauge dependence that have been overlooked so far. Our results reveal\nthat non-covariant treatments employed in previous studies can substantially\noverestimate the production of primordial black holes and scalar-induced\ngravitational waves. Once gauge dependencies are properly accounted for, we\nfind that both effects occur at significantly lower levels than previously\nestimated."
      ]
    }
  },
  {
    "id":2411.00609,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Pediatric low-grade glioma: State-of-the-art and ongoing challenges",
    "start_abstract":"Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
      ],
      "abstract":[
        "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Early Diagnosis and Severity Assessment of Weligama Coconut Leaf Wilt\n  Disease and Coconut Caterpillar Infestation using Deep Learning-based Image\n  Processing Techniques",
        "GMG: A Video Prediction Method Based on Global Focus and Motion Guided",
        "The Evolution of Dataset Distillation: Toward Scalable and Generalizable\n  Solutions",
        "Adaptive Prototype Model for Attribute-based Multi-label Few-shot Action\n  Recognition",
        "Training Data Provenance Verification: Did Your Model Use Synthetic Data\n  from My Generative Model for Training?",
        "Enhancing Image Generation Fidelity via Progressive Prompts",
        "FCVSR: A Frequency-aware Method for Compressed Video Super-Resolution",
        "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control",
        "Enhancing Image Classification with Augmentation: Data Augmentation\n  Techniques for Improved Image Classification",
        "FedSemiDG: Domain Generalized Federated Semi-supervised Medical Image\n  Segmentation",
        "Exploring State Space Model in Wavelet Domain: An Infrared and Visible\n  Image Fusion Network via Wavelet Transform and State Space Model",
        "Validation of Human Pose Estimation and Human Mesh Recovery for\n  Extracting Clinically Relevant Motion Data from Videos",
        "DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation",
        "The Complex Magnetic Field of the Extreme Galactic Center: PRIMA Science\n  Potential",
        "FoundationStereo: Zero-Shot Stereo Matching",
        "Self-Evaluation for Job-Shop Scheduling",
        "Quantifying the Speed-Up from Non-Reversibility in MCMC Tempering\n  Algorithms",
        "When is dataset cartography ineffective? Using training dynamics does\n  not improve robustness against Adversarial SQuAD",
        "Unifying Text Semantics and Graph Structures for Temporal\n  Text-attributed Graphs with Large Language Models",
        "Generation of Frequency-Tunable Shaped Single Microwave Photons Using a\n  Fixed-Frequency Superconducting Qubit",
        "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3\n  Mini Across Chronic Health Conditions",
        "Attentive Reasoning Queries: A Systematic Method for Optimizing\n  Instruction-Following in Large Language Models",
        "$\\beta$-delayed neutron spectroscopy of $^{85, 86}$As with MONSTER at\n  IGISOL",
        "Auxiliary-field quantum Monte Carlo method with quantum selected\n  configuration interaction",
        "Exotic spherical flexible octahedra and counterexamples to the Modified\n  Bellows Conjecture",
        "Is Relevance Propagated from Retriever to Generator in RAG?",
        "Understanding colors of Dufaycolor: Can we recover them using historical\n  colorimetric and spectral data?",
        "Light-by-Light scattering in ultraperipheral heavy ion collisions:\n  Estimating inelastic contributions"
      ],
      "abstract":[
        "Global Coconut (Cocos nucifera (L.)) cultivation faces significant\nchallenges, including yield loss, due to pest and disease outbreaks. In\nparticular, Weligama Coconut Leaf Wilt Disease (WCWLD) and Coconut Caterpillar\nInfestation (CCI) damage coconut trees, causing severe coconut production loss\nin Sri Lanka and nearby coconut-producing countries. Currently, both WCWLD and\nCCI are detected through on-field human observations, a process that is not\nonly time-consuming but also limits the early detection of infections. This\npaper presents a study conducted in Sri Lanka, demonstrating the effectiveness\nof employing transfer learning-based Convolutional Neural Network (CNN) and\nMask Region-based-CNN (Mask R-CNN) to identify WCWLD and CCI at their early\nstages and to assess disease progression. Further, this paper presents the use\nof the You Only Look Once (YOLO) object detection model to count the number of\ncaterpillars distributed on leaves with CCI. The introduced methods were tested\nand validated using datasets collected from Matara, Puttalam, and Makandura,\nSri Lanka. The results show that the proposed methods identify WCWLD and CCI\nwith an accuracy of 90% and 95%, respectively. In addition, the proposed WCWLD\ndisease severity identification method classifies the severity with an accuracy\nof 97%. Furthermore, the accuracies of the object detection models for\ncalculating the number of caterpillars in the leaflets were: YOLOv5-96.87%,\nYOLOv8-96.1%, and YOLO11-95.9%.",
        "Recent years, weather forecasting has gained significant attention. However,\naccurately predicting weather remains a challenge due to the rapid variability\nof meteorological data and potential teleconnections. Current spatiotemporal\nforecasting models primarily rely on convolution operations or sliding windows\nfor feature extraction. These methods are limited by the size of the\nconvolutional kernel or sliding window, making it difficult to capture and\nidentify potential teleconnection features in meteorological data.\nAdditionally, weather data often involve non-rigid bodies, whose motion\nprocesses are accompanied by unpredictable deformations, further complicating\nthe forecasting task. In this paper, we propose the GMG model to address these\ntwo core challenges. The Global Focus Module, a key component of our model,\nenhances the global receptive field, while the Motion Guided Module adapts to\nthe growth or dissipation processes of non-rigid bodies. Through extensive\nevaluations, our method demonstrates competitive performance across various\ncomplex tasks, providing a novel approach to improving the predictive accuracy\nof complex spatiotemporal data.",
        "Dataset distillation, which condenses large-scale datasets into compact\nsynthetic representations, has emerged as a critical solution for training\nmodern deep learning models efficiently. While prior surveys focus on\ndevelopments before 2023, this work comprehensively reviews recent advances,\nemphasizing scalability to large-scale datasets such as ImageNet-1K and\nImageNet-21K. We categorize progress into a few key methodologies: trajectory\nmatching, gradient matching, distribution matching, scalable generative\napproaches, and decoupling optimization mechanisms. As a comprehensive\nexamination of recent dataset distillation advances, this survey highlights\nbreakthrough innovations: the SRe2L framework for efficient and effective\ncondensation, soft label strategies that significantly enhance model accuracy,\nand lossless distillation techniques that maximize compression while\nmaintaining performance. Beyond these methodological advancements, we address\ncritical challenges, including robustness against adversarial and backdoor\nattacks, effective handling of non-IID data distributions. Additionally, we\nexplore emerging applications in video and audio processing, multi-modal\nlearning, medical imaging, and scientific computing, highlighting its domain\nversatility. By offering extensive performance comparisons and actionable\nresearch directions, this survey equips researchers and practitioners with\npractical insights to advance efficient and generalizable dataset distillation,\npaving the way for future innovations.",
        "In real-world action recognition systems, incorporating more attributes helps\nachieve a more comprehensive understanding of human behavior. However, using a\nsingle model to simultaneously recognize multiple attributes can lead to a\ndecrease in accuracy. In this work, we propose a novel method i.e. Adaptive\nAttribute Prototype Model (AAPM) for human action recognition, which captures\nrich action-relevant attribute information and strikes a balance between\naccuracy and robustness. Firstly, we introduce the Text-Constrain Module (TCM)\nto incorporate textual information from potential labels, and constrain the\nconstruction of different attributes prototype representations. In addition, we\nexplore the Attribute Assignment Method (AAM) to address the issue of training\nbias and increase robustness during the training process.Furthermore, we\nconstruct a new video dataset with attribute-based multi-label called\nMulti-Kinetics for evaluation, which contains various attribute labels (e.g.\naction, scene, object, etc.) related to human behavior. Extensive experiments\ndemonstrate that our AAPM achieves the state-of-the-art performance in both\nattribute-based multi-label few-shot action recognition and single-label\nfew-shot action recognition. The project and dataset are available at an\nanonymous account https:\/\/github.com\/theAAPM\/AAPM",
        "High-quality open-source text-to-image models have lowered the threshold for\nobtaining photorealistic images significantly, but also face potential risks of\nmisuse. Specifically, suspects may use synthetic data generated by these\ngenerative models to train models for specific tasks without permission, when\nlacking real data resources especially. Protecting these generative models is\ncrucial for the well-being of their owners. In this work, we propose the first\nmethod to this important yet unresolved issue, called Training data Provenance\nVerification (TrainProVe). The rationale behind TrainProVe is grounded in the\nprinciple of generalization error bound, which suggests that, for two models\nwith the same task, if the distance between their training data distributions\nis smaller, their generalization ability will be closer. We validate the\nefficacy of TrainProVe across four text-to-image models (Stable Diffusion v1.4,\nlatent consistency model, PixArt-$\\alpha$, and Stable Cascade). The results\nshow that TrainProVe achieves a verification accuracy of over 99\\% in\ndetermining the provenance of suspicious model training data, surpassing all\nprevious methods. Code is available at https:\/\/github.com\/xieyc99\/TrainProVe.",
        "The diffusion transformer (DiT) architecture has attracted significant\nattention in image generation, achieving better fidelity, performance, and\ndiversity. However, most existing DiT - based image generation methods focus on\nglobal - aware synthesis, and regional prompt control has been less explored.\nIn this paper, we propose a coarse - to - fine generation pipeline for regional\nprompt - following generation. Specifically, we first utilize the powerful\nlarge language model (LLM) to generate both high - level descriptions of the\nimage (such as content, topic, and objects) and low - level descriptions (such\nas details and style). Then, we explore the influence of cross - attention\nlayers at different depths. We find that deeper layers are always responsible\nfor high - level content control, while shallow layers handle low - level\ncontent control. Various prompts are injected into the proposed regional cross\n- attention control for coarse - to - fine generation. By using the proposed\npipeline, we enhance the controllability of DiT - based image generation.\nExtensive quantitative and qualitative results show that our pipeline can\nimprove the performance of the generated images.",
        "Compressed video super-resolution (SR) aims to generate high-resolution (HR)\nvideos from the corresponding low-resolution (LR) compressed videos. Recently,\nsome compressed video SR methods attempt to exploit the spatio-temporal\ninformation in the frequency domain, showing great promise in super-resolution\nperformance. However, these methods do not differentiate various frequency\nsubbands spatially or capture the temporal frequency dynamics, potentially\nleading to suboptimal results. In this paper, we propose a deep frequency-based\ncompressed video SR model (FCVSR) consisting of a motion-guided adaptive\nalignment (MGAA) network and a multi-frequency feature refinement (MFFR)\nmodule. Additionally, a frequency-aware contrastive loss is proposed for\ntraining FCVSR, in order to reconstruct finer spatial details. The proposed\nmodel has been evaluated on three public compressed video super-resolution\ndatasets, with results demonstrating its effectiveness when compared to\nexisting works in terms of super-resolution performance (up to a 0.14dB gain in\nPSNR over the second-best model) and complexity.",
        "We propose a method for generating fly-through videos of a scene, from a\nsingle image and a given camera trajectory. We build upon an image-to-video\nlatent diffusion model. We condition its UNet denoiser on the camera\ntrajectory, using four techniques. (1) We condition the UNet's temporal blocks\non raw camera extrinsics, similar to MotionCtrl. (2) We use images containing\ncamera rays and directions, similar to CameraCtrl. (3) We reproject the initial\nimage to subsequent frames and use the resulting video as a condition. (4) We\nuse 2D<=>3D transformers to introduce a global 3D representation, which\nimplicitly conditions on the camera poses. We combine all conditions in a\nContolNet-style architecture. We then propose a metric that evaluates overall\nvideo quality and the ability to preserve details with view changes, which we\nuse to analyze the trade-offs of individual and combined conditions. Finally,\nwe identify an optimal combination of conditions. We calibrate camera positions\nin our datasets for scale consistency across scenes, and we train our scene\nexploration model, CamCtrl3D, demonstrating state-of-theart results.",
        "Convolutional Neural Networks (CNNs) serve as the workhorse of deep learning,\nfinding applications in various fields that rely on images. Given sufficient\ndata, they exhibit the capacity to learn a wide range of concepts across\ndiverse settings. However, a notable limitation of CNNs is their susceptibility\nto overfitting when trained on small datasets. The augmentation of such\ndatasets can significantly enhance CNN performance by introducing additional\ndata points for learning. In this study, we explore the effectiveness of 11\ndifferent sets of data augmentation techniques, which include three novel sets\nproposed in this work. The first set of data augmentation employs pairwise\nchannel transfer, transferring Red, Green, Blue, Hue, and Saturation values\nfrom randomly selected images in the database to all images in the dataset. The\nsecond set introduces a novel occlusion approach, where objects in the images\nare occluded by randomly selected objects from the dataset. The third set\ninvolves a novel masking approach, using vertical, horizontal, circular, and\ncheckered masks to occlude portions of the images. In addition to these novel\ntechniques, we investigate other existing augmentation methods, including\nrotation, horizontal and vertical flips, resizing, translation, blur, color\njitter, and random erasing, and their effects on accuracy and overfitting. We\nfine-tune a base EfficientNet-B0 model for each augmentation method and conduct\na comparative analysis to showcase their efficacy. For the evaluation and\ncomparison of these augmentation techniques, we utilize the Caltech-101\ndataset. The ensemble of image augmentation techniques proposed emerges as the\nmost effective on the Caltech-101 dataset. The results demonstrate that diverse\ndata augmentation techniques present a viable means of enhancing datasets for\nimproved image classification.",
        "Medical image segmentation is challenging due to the diversity of medical\nimages and the lack of labeled data, which motivates recent developments in\nfederated semi-supervised learning (FSSL) to leverage a large amount of\nunlabeled data from multiple centers for model training without sharing raw\ndata. However, what remains under-explored in FSSL is the domain shift problem\nwhich may cause suboptimal model aggregation and low effectivity of the\nutilization of unlabeled data, eventually leading to unsatisfactory performance\nin unseen domains. In this paper, we explore this previously ignored scenario,\nnamely domain generalized federated semi-supervised learning (FedSemiDG), which\naims to learn a model in a distributed manner from multiple domains with\nlimited labeled data and abundant unlabeled data such that the model can\ngeneralize well to unseen domains. We present a novel framework, Federated\nGeneralization-Aware SemiSupervised Learning (FGASL), to address the challenges\nin FedSemiDG by effectively tackling critical issues at both global and local\nlevels. Globally, we introduce Generalization-Aware Aggregation (GAA),\nassigning adaptive weights to local models based on their generalization\nperformance. Locally, we use a Dual-Teacher Adaptive Pseudo Label Refinement\n(DR) strategy to combine global and domain-specific knowledge, generating more\nreliable pseudo labels. Additionally, Perturbation-Invariant Alignment (PIA)\nenforces feature consistency under perturbations, promoting domain-invariant\nlearning. Extensive experiments on three medical segmentation tasks (cardiac\nMRI, spine MRI and bladder cancer MRI) demonstrate that our method\nsignificantly outperforms state-of-the-art FSSL and domain generalization\napproaches, achieving robust generalization on unseen domains.",
        "Deep learning techniques have revolutionized the infrared and visible image\nfusion (IVIF), showing remarkable efficacy on complex scenarios. However,\ncurrent methods do not fully combine frequency domain features with global\nsemantic information, which will result in suboptimal extraction of global\nfeatures across modalities and insufficient preservation of local texture\ndetails. To address these issues, we propose Wavelet-Mamba (W-Mamba), which\nintegrates wavelet transform with the state-space model (SSM). Specifically, we\nintroduce Wavelet-SSM module, which incorporates wavelet-based frequency domain\nfeature extraction and global information extraction through SSM, thereby\neffectively capturing both global and local features. Additionally, we propose\na cross-modal feature attention modulation, which facilitates efficient\ninteraction and fusion between different modalities. The experimental results\nindicate that our method achieves both visually compelling results and superior\nperformance compared to current state-of-the-art methods. Our code is available\nat https:\/\/github.com\/Lmmh058\/W-Mamba.",
        "This work aims to discuss the current landscape of kinematic analysis tools,\nranging from the state-of-the-art in sports biomechanics such as inertial\nmeasurement units (IMUs) and retroreflective marker-based optical motion\ncapture (MoCap) to more novel approaches from the field of computing such as\nhuman pose estimation and human mesh recovery. Primarily, this comparative\nanalysis aims to validate the use of marker-less MoCap techniques in a clinical\nsetting by showing that these marker-less techniques are within a reasonable\nrange for kinematics analysis compared to the more cumbersome and less portable\nstate-of-the-art tools. Not only does marker-less motion capture using human\npose estimation produce results in-line with the results of both the IMU and\nMoCap kinematics but also benefits from a reduced set-up time and reduced\npractical knowledge and expertise to set up. Overall, while there is still room\nfor improvement when it comes to the quality of the data produced, we believe\nthat this compromise is within the room of error that these low-speed actions\nthat are used in small clinical tests.",
        "Automatic medical image segmentation plays a crucial role in computer aided\ndiagnosis. However, fully supervised learning approaches often require\nextensive and labor-intensive annotation efforts. To address this challenge,\nweakly supervised learning methods, particularly those using extreme points as\nsupervisory signals, have the potential to offer an effective solution. In this\npaper, we introduce Deep Extreme Point Tracing (DEPT) integrated with\nFeature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image\nsegmentation. Notably, our method generates pseudo labels by identifying the\nlowest-cost path that connects all extreme points on the feature map-based cost\nmatrix. Additionally, an iterative training strategy is proposed to refine\npseudo labels progressively, enabling continuous network improvement.\nExperimental results on two public datasets demonstrate the effectiveness of\nour proposed method. The performance of our method approaches that of the fully\nsupervised method and outperforms several existing weakly supervised methods.",
        "The Central Molecular Zone (CMZ) of the Galactic Center (GC) region of the\nMilky Way contains a substantial fraction of the molecular mass of the Galaxy\n>10e7 solar masses yet exhibits an order of magnitude lower star formation\nefficiency (SFE) than expected given the high densities found in this region.\nThere are multiple possible explanations for the depressed SFE in the CMZ, like\nfeedback, strong turbulence, longer free-fall timescales, and high magnetic\nfield strengths. It is currently unclear which of these mechanisms is the\ndominant inhibitor of star formation in the CMZ. It is important to understand\nthe star formation process in the extreme environment of the CMZ because it is\nthe only Galactic nuclear region we are able to study at high spatial\nresolutions with current observatories. One way to determine the relative\nimportance of the different SFE inhibiting mechanisms is through multi-spatial\nand multi-frequency polarimetric observations of the CMZ. Such observations\nwill provide insight into the behavior of the magnetic field in this unique\nenvironment. These observations will complement radio observations of\nnon-thermal structures revealing the magnetic field morphology and\npolarization. The PRobe far--Infrared Mission for Astrophysics (PRIMA) will be\nuniquely capable of contributing to such explorations by providing unique\nresolutions and frequencies for polarimetric observations. The PRIMAger\ninstrument will yield polarimetric observations covering the wavelength range\n80 -- 261 um with beam sizes ranging from 11 -- 28'', capabilities that\ncomplement existing and upcoming observatories.",
        "Tremendous progress has been made in deep stereo matching to excel on\nbenchmark datasets through per-domain fine-tuning. However, achieving strong\nzero-shot generalization - a hallmark of foundation models in other computer\nvision tasks - remains challenging for stereo matching. We introduce\nFoundationStereo, a foundation model for stereo depth estimation designed to\nachieve strong zero-shot generalization. To this end, we first construct a\nlarge-scale (1M stereo pairs) synthetic training dataset featuring large\ndiversity and high photorealism, followed by an automatic self-curation\npipeline to remove ambiguous samples. We then design a number of network\narchitecture components to enhance scalability, including a side-tuning feature\nbackbone that adapts rich monocular priors from vision foundation models to\nmitigate the sim-to-real gap, and long-range context reasoning for effective\ncost volume filtering. Together, these components lead to strong robustness and\naccuracy across domains, establishing a new standard in zero-shot stereo depth\nestimation. Project page: https:\/\/nvlabs.github.io\/FoundationStereo\/",
        "Combinatorial optimization problems, such as scheduling and route planning,\nare crucial in various industries but are computationally intractable due to\ntheir NP-hard nature. Neural Combinatorial Optimization methods leverage\nmachine learning to address these challenges but often depend on sequential\ndecision-making, which is prone to error accumulation as small mistakes\npropagate throughout the process. Inspired by self-evaluation techniques in\nLarge Language Models, we propose a novel framework that generates and\nevaluates subsets of assignments, moving beyond traditional stepwise\napproaches. Applied to the Job-Shop Scheduling Problem, our method integrates a\nheterogeneous graph neural network with a Transformer to build a policy model\nand a self-evaluation function. Experimental validation on challenging,\nwell-known benchmarks demonstrates the effectiveness of our approach,\nsurpassing state-of-the-art methods.",
        "We investigate the increase in efficiency of simulated and parallel tempering\nMCMC algorithms when using non-reversible updates to give them \"momentum\". By\nmaking a connection to a certain simple discrete Markov chain, we show that,\nunder appropriate assumptions, the non-reversible algorithms still exhibit\ndiffusive behaviour, just on a different time scale. We use this to argue that\nthe optimally scaled versions of the non-reversible algorithms are indeed more\nefficient than the optimally scaled versions of their traditional reversible\ncounterparts, but only by a modest speed-up factor of about 42%.",
        "In this paper, I investigate the effectiveness of dataset cartography for\nextractive question answering on the SQuAD dataset. I begin by analyzing\nannotation artifacts in SQuAD and evaluate the impact of two adversarial\ndatasets, AddSent and AddOneSent, on an ELECTRA-small model. Using training\ndynamics, I partition SQuAD into easy-to-learn, ambiguous, and hard-to-learn\nsubsets. I then compare the performance of models trained on these subsets to\nthose trained on randomly selected samples of equal size. Results show that\ntraining on cartography-based subsets does not improve generalization to the\nSQuAD validation set or the AddSent adversarial set. While the hard-to-learn\nsubset yields a slightly higher F1 score on the AddOneSent dataset, the overall\ngains are limited. These findings suggest that dataset cartography provides\nlittle benefit for adversarial robustness in SQuAD-style QA tasks. I conclude\nby comparing these results to prior findings on SNLI and discuss possible\nreasons for the observed differences.",
        "Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{{Cross}}, a novel\nframework that seamlessly extends existing TGNNs for TTAG modeling. The key\nidea is to employ the advanced large language models (LLMs) to extract the\ndynamic semantics in text space and then generate expressive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the {Cross} framework, which empowers the LLM to offer\nthe temporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experimental results on four public datasets and one practical\nindustrial dataset demonstrate {Cross}'s significant effectiveness and\nrobustness.",
        "Scaling up a superconducting quantum computer will likely require quantum\ncommunication between remote chips, which can be implemented using an itinerant\nmicrowave photon in a transmission line. To realize high-fidelity\ncommunication, it is essential to control the frequency and temporal shape of\nthe microwave photon. In this work, we demonstrate the generation of\nfrequency-tunable shaped microwave photons without resorting to any\nfrequency-tunable circuit element. We develop a framework which treats a\nmicrowave resonator as a band-pass filter mediating the interaction between a\nsuperconducting qubit and the modes in the transmission line. This\ninterpretation allows us to stimulate the photon emission by an off-resonant\ndrive signal. We characterize how the frequency and temporal shape of the\ngenerated photon depends on the frequency and amplitude of the drive signal. By\nmodulating the drive amplitude and frequency, we achieve a frequency tunability\nof 40 MHz while maintaining the photon mode shape time-symmetric.Through\nmeasurements of the quadrature amplitudes of the emitted photons, we\ndemonstrate consistently high state and process fidelities around 95\\% across\nthe tunable frequency range. Our hardware-efficient approach eliminates the\nneed for additional biasing lines typically required for frequency tuning,\noffering a simplified architecture for scalable quantum communication.",
        "Large Language Models (LLMs) are revolutionizing medical diagnostics by\nenhancing both disease classification and clinical decision-making. In this\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\nassessed their predictive accuracy at both the disease and category levels, as\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\nAutoimmune Disease classification with 100% accuracy. Both models, however,\nstruggled with Respiratory Disease classification, recording accuracies of only\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\nconfidence scores revealed that DeepSeek R1 provided high-confidence\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\nconsiderations regarding bias, model interpretability, and data privacy are\nalso discussed to ensure the responsible integration of LLMs into clinical\npractice. Overall, our findings offer valuable insights into the strengths and\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\nenhancements in AI-driven healthcare.",
        "We present Attentive Reasoning Queries (ARQs), a novel structured reasoning\napproach that significantly improves instruction-following in Large Language\nModels through domain-specialized reasoning blueprints. While LLMs demonstrate\nremarkable capabilities across diverse tasks, they often fail to maintain\nadherence to complex, use-case-specific instructions during multi-turn\nconversations, presenting challenges for business-critical applications. ARQs\naddress this limitation by guiding LLMs through systematic reasoning steps with\ntargeted queries that reinstate critical instructions and facilitate\nintermediate reasoning throughout the completion process. In extensive testing\nwithin Parlant, our framework for reliable customer-facing agents in which ARQs\nwere born out of necessity, they achieved a 90.2% success rate across 87 test\nscenarios, outperforming both Chain-of-Thought reasoning (86.1%) and direct\nresponse generation (81.5%). ARQs showed particular strength in addressing\npersistent failure modes like guideline re-application and hallucination\nprevention. Our analysis also revealed that ARQs can potentially be more\ncomputationally efficient than free-form reasoning when carefully designed.\nThese findings demonstrate that structured reasoning approaches provide\neffective mechanisms for controlling how LLMs process information and make\ndecisions in complex scenarios.",
        "The $\\beta$-delayed neutron emission in the $^{85, 86}$As $\\beta$-decays has\nbeen measured at the Ion Guide Isotope Separator On Line facility of the\nAccelerator Laboratory of the University of Jyv\\\"askyl\\\"a. The complete\n$\\beta$-decays have been studied with a complex setup that consists of a\nplastic scintillator for $\\beta$-particles, MONSTER -- the MOdular Neutron\ntime-of-flight SpectromeTER -- for neutrons, and a high-purity germanium and\nfour LaBr$_3$ crystals for $\\gamma$-rays. The $\\beta$-delayed neutron energy\ndistributions have been determined by unfolding the time-of-flight spectra with\nan innovative methodology based on the iterative Bayesian unfolding method and\naccurate Monte Carlo simulations. The results obtained for $^{85}$As are in\nexcellent agreement with the existing evaluated data, validating the proposed\nmethodology. In the case of $^{86}$As, a stronger neutron intensity at higher\nenergies than previously predicted is discovered.",
        "We propose using the wave function generated by the quantum selected\nconfiguration interaction (QSCI) method as the trial wave function in phaseless\nauxiliary-field quantum Monte Carlo (ph-AFQMC). In the QSCI framework,\nelectronic configurations are sampled from the quantum state realized on a\nquantum computer. These configurations serve as basis states for constructing\nan effective Hamiltonian, which is then diagonalized to obtain the\ncorresponding eigenstate. Using this wave function, ph-AFQMC is performed to\nrecover the dynamical electron correlation across the whole orbital space. The\nuse of the QSCI trial wave function is expected to improve the feasibility of\nthe quantum-classical (QC) hybrid quantum Monte Carlo approach [Nature, 603,\n416 (2022)]. We call this integrated approach QC-QSCI-AFQMC, or QSCI-AFQMC for\nshort. This method is validated across several molecular systems. For H2O and a\nlinear H4 chain, we achieved chemical accuracy in most investigations relative\nto full configuration interaction while utilizing superconducting quantum\ncomputers at Osaka University and RIKEN. Additionally, the application of\nQSCI-AFQMC to the O-H bond dissociation in an organic molecule highlights the\ncomplementary synergy between capturing static correlation on quantum hardware\nand incorporating dynamical correlation via classical post-processing. For the\nN2, when QSCI-AFQMC is executed with a noiseless simulator, it ranks among the\nmost accurate methods compared to various multireference electronic structure\ntheories. Although the proposed method is demonstrated using small active\nspaces on current quantum devices, the concept is not limited to few-qubit\nproblems. The QSCI-AFQMC can compete with state-of-the-art classical\ncomputational techniques, particularly in larger active spaces, displaying\nconsiderable potential for resolving classically intractable problems in\nquantum chemistry.",
        "In 2014 the author showed that in the three-dimensional spherical space,\nalongside with three classical types of flexible octahedra constructed by\nBricard, there exists a new type of flexible octahedra, which was called\nexotic. In the present paper we give a geometric construction for exotic\nflexible octahedra, describe their configuration spaces, and calculate their\nvolumes. We show that the volume of an exotic flexible octahedron is\nnonconstant during the flexion, and moreover the volume remains nonconstant if\nwe replace any set of vertices of the octahedron with their antipodes. So\nexotic flexible octahedra are counterexamples to the Modified Bellows\nConjecture proposed by the author in 2015.",
        "Retrieval Augmented Generation (RAG) is a framework for incorporating\nexternal knowledge, usually in the form of a set of documents retrieved from a\ncollection, as a part of a prompt to a large language model (LLM) to\npotentially improve the performance of a downstream task, such as question\nanswering. Different from a standard retrieval task's objective of maximising\nthe relevance of a set of top-ranked documents, a RAG system's objective is\nrather to maximise their total utility, where the utility of a document\nindicates whether including it as a part of the additional contextual\ninformation in an LLM prompt improves a downstream task. Existing studies\ninvestigate the role of the relevance of a RAG context for knowledge-intensive\nlanguage tasks (KILT), where relevance essentially takes the form of answer\ncontainment. In contrast, in our work, relevance corresponds to that of topical\noverlap between a query and a document for an information seeking task.\nSpecifically, we make use of an IR test collection to empirically investigate\nwhether a RAG context comprised of topically relevant documents leads to\nimproved downstream performance. Our experiments lead to the following\nfindings: (a) there is a small positive correlation between relevance and\nutility; (b) this correlation decreases with increasing context sizes (higher\nvalues of k in k-shot); and (c) a more effective retrieval model generally\nleads to better downstream RAG performance.",
        "Dufaycolor, an additive color photography process produced from 1935 to the\nlate 1950s, represents one of the most advanced iterations of this technique.\nThis paper presents ongoing research and development of an open-source\nColor-Screen tool designed to reconstruct the original colors of additive color\nphotographs. We discuss the incorporation of historical measurements of dyes\nused in the production of the color-screen filter (r\\'eseau) to achieve\naccurate color recovery.",
        "The current state-of-the-art theoretical estimations lead to cross-sections\nfor $AA \\to \\gamma \\gamma AA$ which are somewhat smaller than the measured ones\nby the ATLAS and CMS Collaborations, which motivates the searching and\ncalculation of subleading corrections disregarded in these previous studies. In\nthis paper, we estimate the contribution of inelastic channels to the Light -\nby - Light (LbL) scattering in ultraperipheral collisions of heavy ions\n(UPHICs), in which one or both of the incident nuclei dissociate ($A A \\to\n\\gamma \\gamma X Y$ where $X, Y = A, A'$) due to the photon emission. These new\nmechanisms are related to extra emissions that are rather difficult to identify\nat the LHC and can be mistakenly interpreted as enhanced $\\gamma \\gamma \\to\n\\gamma \\gamma$ scattering compared to the Standard Model result. We include\nprocesses of coupling of photons to individual nucleons (protons and neutrons)\nin addition to coherent coupling to the whole nuclei (called standard approach\nhere). Both elastic (nucleon in the ground state) and inelastic (nucleon in an\nexcited state) in the couplings of photons to nucleons are taken into account.\nThe inelastic nucleon fluxes are calculated using CT18qed photon in nucleon\nPDFs. The inelastic photon fluxes are shown and compared to standard photon\nfluxes in the nucleus. In addition, we show the ratio of the inelastic\ncorrections to the standard contribution as a function of diphoton invariant\nmass and photon rapidity difference. We find the maximal effect of the\ninelastic corrections at $M_{\\gamma \\gamma} \\sim$ 14 GeV for the ATLAS rapidity\nand transverse momentum acceptance. Furthermore, the inelastic contribution\nincreases gradually with photon rapidity difference. Our results indicate that\nthe inelastic contributions can increase locally by 10-15 \\% the traditional\n(no nuclear excitation) predictions for the LbL scattering in UPCs."
      ]
    }
  },
  {
    "id":2411.00609,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks",
    "start_abstract":"Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
      ],
      "abstract":[
        "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Using economic value signals from primate prefrontal cortex in\n  neuro-engineering applications",
        "Increased GM-WM in a prefrontal network and decreased GM in the insula\n  and the precuneus are associated with reappraisal usage: A data fusion\n  approach",
        "Multi-Site rs-fMRI Domain Alignment for Autism Spectrum Disorder\n  Auxiliary Diagnosis Based on Hyperbolic Space",
        "Targeting C99 Mediated Metabolic Disruptions with Ketone Therapy in\n  Alzheimer's Disease",
        "Risk and Protective Factors in Parkinsons Disease",
        "Language-specific Tonal Features Drive Speaker-Listener Neural\n  Synchronization",
        "Morphological Neuron Classification Using Machine Learning",
        "Intrinsic motivation as constrained entropy maximization",
        "Latent computing by biological neural networks: A dynamical systems\n  framework",
        "Neural encoding with affine feature response transforms",
        "Causal Spike Timing Dependent Plasticity Prevents Assembly Fusion in\n  Recurrent Networks",
        "Active filtering: a predictive function of recurrent circuits of sensory\n  cortex",
        "A comprehensive and reliable protocol for manual segmentation of the\n  human claustrum using high-resolution MRI",
        "Automated Microsolvation for Minimum Energy Path Construction in\n  Solution",
        "Giant Uncompensated Magnon Spin Currents in X-type Magnets",
        "Hedging with Sparse Reward Reinforcement Learning",
        "Construction A Lattice Design Based on the Truncated Union Bound",
        "Data-Aided Regularization of Direct-Estimate Combiner in Distributed\n  MIMO Systems",
        "Safety Verification of Nonlinear Stochastic Systems via Probabilistic\n  Tube",
        "Coverage errors for Student's t confidence intervals comparable to those\n  in Hall (1988)",
        "Evidence of Athermal Metastable Phase in a Halide Perovskite: Optically\n  Tracked Thermal-Breach Memory",
        "Do We Need to Verify Step by Step? Rethinking Process Supervision from a\n  Theoretical Perspective",
        "The bound and resonant states of $D^{(*)}D^{(*)}$ and\n  $D^{(*)}\\bar{D}^{(*)}$ with the complex scaling method",
        "Feature Learning beyond the Lazy-Rich Dichotomy: Insights from\n  Representational Geometry",
        "Fastest mixing reversible Markov chain on friendship graph: Trade-off\n  between transition probabilities among friends and convergence rate",
        "From Paramagnet to Dipolar Topological Order via Duality and Dipolar SPT",
        "Engagement Zones for a Turn Constrained Pursuer",
        "Benefits of Early Stopping in Gradient Descent for Overparameterized\n  Logistic Regression"
      ],
      "abstract":[
        "Neural signals related to movement can be measured from intracranial\nrecordings and used in brain-machine interface devices (BMI) to restore\nphysical function in impaired patients. In this study, we explore the use of\nmore abstract neural signals related to economic value in a BMI context. Using\ndata collected from the orbitofrontal cortex in non-human primates, we develop\ndeep learning-based neural decoders that can predict the monkey's choice in a\nvalue-based decision-making task. Out-of-sample performance was improved by\naugmenting the training set with synthesized data, showing the feasibility of\nusing limited training data. We further demonstrate that we can predict the\nmonkey's choice sooner using a neural forecasting module that is equipped with\ntask-related information. These findings support the feasibility of user\npreference-informed neuroengineering devices that leverage abstract cognitive\nsignals.",
        "Emotion regulation plays a crucial role in mental health, and difficulties in\nregulating emotions can contribute to psychological disorders. While\nreappraisal and suppression are well-studied strategies, the combined\ncontributions of gray matter (GM) and white matter (WM) to these strategies\nremain unclear due to methodological limitations in previous studies. To\naddress this, we applied a data fusion approach using Parallel Independent\nComponent Analysis (Parallel ICA) to GM and WM MRI images from 165 individuals.\nParallel ICA identified two networks associated with reappraisal usage. Network\n1 included a large lateral and medial prefrontal cortical network, overlapping\nwith the default mode network (DMN) and adjacent WM regions. Higher reappraisal\nfrequency was associated with greater GM-WM density within this network, and\nthis network was negatively correlated with perceived stress. Network 2\nincluded the insula, precuneus, sub-gyral, and lingual gyri in its GM portion,\nshowing a negative association with reappraisal usage. The WM portion, adjacent\nto regions of the central executive network (CEN), was positively associated\nwith reappraisal usage. Regarding suppression, no significant network was\nassociated with this strategy. This study provides new insights into individual\ndifferences in reappraisal use, showing a positive association between\nreappraisal frequency and increased gray and white matter concentration in a\nlarge frontal network, including regions of the frontal DMN and the CEN.\nConversely, subcortical areas exhibited reduced gray and white concentration.",
        "In the medical field, most resting-state fMRI (rs-fMRI) data are collected\nfrom multiple hospital sites. Multi-site rs-fMRI data can increase the volume\nof training data, enabling auxiliary diagnostic algorithms for brain diseases\nto learn more accurate and stable models. However, due to the significant\nheterogeneity and domain shift in rs-fMRI data across different sites, the\naccuracy of auxiliary diagnosis remains unsatisfactory. Moreover, there has\nbeen limited exploration of multi-source domain adaptation algorithms, and the\ninterpretability of models is often poor. To address these challenges, we\nproposed a domain-adaptive algorithm based on hyperbolic space embedding.\nHyperbolic space is naturally suited for representing the topology of complex\nnetworks such as brain functional networks. Therefore, we embedded the brain\nfunctional network into hyperbolic space and constructed the corresponding\nhyperbolic space community network to effectively extract brain network\nrepresentations. To address the heterogeneity of data across different sites\nand the issue of domain shift, we introduce a constraint loss function, HMMD\n(Hyperbolic Maximum Mean Discrepancy), to align the marginal distributions in\nthe hyperbolic space. Additionally, we employ class prototype alignment to\nalign the conditional distributions. This significantly improves the quality of\nbrain representations and enhances diagnostic classification accuracy for\nAutism Spectrum Disorder (ASD). Experimental results demonstrated that the\nproposed algorithm is robust to multi-site heterogeneity and shows promising\npotential for brain network mechanism analysis.",
        "The role of ketone bodies in Alzheimers disease (AD) remains incompletely\nunderstood, particularly regarding their influence on amyloid pathology. While\nbeta}hydroxybutyrate (BHB) has been implicated in neuroprotection, direct\nevidence for its effects on amyloid beta(Abeta) deposition, aggregation, or\nclearance is lacking. Furthermore, whether BHB acts as a disease modifying\nfactor or merely confers transient metabolic benefits remains unclear.\nAddressing this gap is crucial for evaluating the therapeutic potential of\nketone metabolism in AD. Here, we investigated the impact of ketone bodies on\namyloidogenic toxicity using a Drosophila melanogaster model with targeted\nexpression of human amyloid precursor protein (APP), beta secretase 1 (BACE1),\nAbeta, and the C99 fragment, an essential intermediate in Abeta generation.\nSurprisingly, we found that Abeta alone elicited minimal neurotoxicity, whereas\nC99 expression induced pronounced pathological effects, suggesting a critical,\nunderappreciated role of C99 in AD progression. Further analysis revealed that\nC99 driven toxicity was associated with autophagic and lysosomal dysfunction,\nleading to impaired protein clearance, oxidative stress, and mitochondrial\nabnormalities. Using confocal microscopy and lysosomal pH sensitive markers, we\ndemonstrated that BHB treatment restored lysosomal function and alleviated\nthese pathological changes. Protein protein interaction network analysis in C99\nexpressing Drosophila brains identified protein phosphatase methylesterase 1\n(PPME1) activation as a key driver of autophagic impairment, further supported\nby machine learning predictions. Finally, mathematical similarity analysis of\nPPI networks suggested that BHB may exert its neuroprotective effects through\nmTOR inhibition, positioning it as a potential endogenous modulator of AD\nrelated pathology.",
        "Understanding the risk and protective factors associated with Parkinsons\ndisease (PD) is crucial for improving outcomes for patients, individuals at\nrisk, healthcare providers, and healthcare systems. Studying these factors not\nonly enhances our knowledge of the disease but also aids in developing\neffective prevention, management, and treatment strategies. This paper reviews\nthe key risk and protective factors associated with PD, with a particular focus\non the biological mechanisms underlying these factors. Risk factors include\ngenetic mutations, racial predispositions, and environmental exposures, all of\nwhich contribute to an increased likelihood of developing PD or accelerating\ndisease progression. Conversely, protective factors such as regular physical\nexercise, adherence to a Mediterranean diet, and higher urate levels have\ndemonstrated potential to reduce inflammation and support mitochondrial\nfunction, thereby mitigating disease risk. However, identifying and validating\nthese factors presents significant challenges. To overcome challenges, we\npropose several solutions and recommendations. Future research should\nprioritize the development of standardized biomarkers for early diagnosis,\ninvestigate gene-environment interactions in greater depth, and refine animal\nmodels to better mimic human PD pathology. Additionally, we offer actionable\nrecommendations for PD prevention and management, tailored to healthy\nindividuals, patients diagnosed with PD, and healthcare systems. These\nstrategies aim to improve clinical outcomes, enhance quality of life, and\noptimize healthcare delivery for PD.",
        "Verbal communication transmits information across diverse linguistic levels,\nwith neural synchronization (NS) between speakers and listeners emerging as a\nputative mechanism underlying successful exchange. However, the specific speech\nfeatures driving this synchronization and how language-specific versus\nuniversal characteristics facilitate information transfer remain poorly\nunderstood. We developed a novel content-based interbrain encoding model to\ndisentangle the contributions of acoustic and linguistic features to\nspeaker-listener NS during Mandarin storytelling and listening, as measured via\nmagnetoencephalography (MEG). Results revealed robust NS throughout\nfrontotemporal-parietal networks with systematic time lags between speech\nproduction and perception. Crucially, suprasegmental lexical tone features\n(tone categories, pitch height, and pitch contour), essential for lexical\nmeaning in Mandarin, contributed more significantly to NS than either acoustic\nelements or universal segmental units (consonants and vowels). These tonal\nfeatures generated distinctive spatiotemporal NS patterns, creating\nlanguage-specific neural \"communication channels\" that facilitated efficient\nrepresentation sharing between interlocutors. Furthermore, the strength and\npatterns of NS driven by these language-specific features predicted\ncommunication success. These findings demonstrate the neural mechanisms\nunderlying shared representations during verbal exchange and highlight how\nlanguage-specific features can shape neural coupling to optimize information\ntransfer during human communication.",
        "Classification and quantitative characterization of neuronal morphologies\nfrom histological neuronal reconstruction is challenging since it is still\nunclear how to delineate a neuronal cell class and which are the best features\nto define them by. The morphological neuron characterization represents a\nprimary source to address anatomical comparisons, morphometric analysis of\ncells, or brain modeling. The objectives of this paper are (i) to develop and\nintegrate a pipeline that goes from morphological feature extraction to\nclassification and (ii) to assess and compare the accuracy of machine learning\nalgorithms to classify neuron morphologies. The algorithms were trained on 430\ndigitally reconstructed neurons subjectively classified into layers and\/or\nm-types using young and\/or adult development state population of the\nsomatosensory cortex in rats. For supervised algorithms, linear discriminant\nanalysis provided better classification results in comparison with others. For\nunsupervised algorithms, the affinity propagation and the Ward algorithms\nprovided slightly better results.",
        "\"Intrinsic motivation\" refers to the capacity for intelligent systems to be\nmotivated endogenously, i.e. by features of agential architecture itself rather\nthan by learned associations between action and reward. This paper views active\ninference, empowerment, and other formal accounts of intrinsic motivation as\nvariations on the theme of constrained maximum entropy inference, providing a\ngeneral perspective on intrinsic motivation complementary to existing\nframeworks. The connection between free energy and empowerment noted in\nprevious literature is further explored, and it is argued that the\nmaximum-occupancy approach in practice incorporates an implicit model-evidence\nconstraint.",
        "Although individual neurons and neural populations exhibit the phenomenon of\nrepresentational drift, perceptual and behavioral outputs of many neural\ncircuits can remain stable across time scales over which representational drift\nis substantial. These observations motivate a dynamical systems framework for\nneural network activity that focuses on the concept of \\emph{latent processing\nunits,} core elements for robust coding and computation embedded in collective\nneural dynamics. Our theoretical treatment of these latent processing units\nyields five key attributes of computing through neural network dynamics. First,\nneural computations that are low-dimensional can nevertheless generate\nhigh-dimensional neural dynamics. Second, the manifolds defined by neural\ndynamical trajectories exhibit an inherent coding redundancy as a direct\nconsequence of the universal computing capabilities of the underlying dynamical\nsystem. Third, linear readouts or decoders of neural population activity can\nsuffice to optimally subserve downstream circuits controlling behavioral\noutputs. Fourth, whereas recordings from thousands of neurons may suffice for\nnear optimal decoding from instantaneous neural activity patterns, experimental\naccess to millions of neurons may be necessary to predict neural ensemble\ndynamical trajectories across timescales of seconds. Fifth, despite the\nvariable activity of single cells, neural networks can maintain stable\nrepresentations of the variables computed by the latent processing units,\nthereby making computations robust to representational drift. Overall, our\nframework for latent computation provides an analytic description and\nempirically testable predictions regarding how large systems of neurons perform\nrobust computations via their collective dynamics.",
        "Current linearizing encoding models that predict neural responses to sensory\ninput typically neglect neuroscience-inspired constraints that could enhance\nmodel efficiency and interpretability. To address this, we propose a new method\ncalled affine feature response transform (AFRT), which exploits the brain's\nretinotopic organization. Applying AFRT to encode multi-unit activity in areas\nV1, V4, and IT of the macaque brain, we demonstrate that AFRT reduces redundant\ncomputations and enhances the performance of current linearizing encoding\nmodels by segmenting each neuron's receptive field into an affine retinal\ntransform, followed by a localized feature response. Remarkably, by factorizing\nreceptive fields into a sequential affine component with three interpretable\nparameters (for shifting and scaling) and response components with a small\nnumber of feature weights per response, AFRT achieves encoding with orders of\nmagnitude fewer parameters compared to unstructured models. We show that the\nretinal transform of each neuron's encoding agrees well with the brain's\nreceptive field. Together, these findings suggest that this new subset within\nspatial transformer network can be instrumental in neural encoding models of\nnaturalistic stimuli.",
        "The organization of neurons into functionally related assemblies is a\nfundamental feature of cortical networks, yet our understanding of how these\nassemblies maintain distinct identities while sharing members remains limited.\nHere we analyze how spike-timing-dependent plasticity (STDP) shapes the\nformation and stability of overlapping neuronal assemblies in recurrently\ncoupled networks of spiking neuron models. Using numerical simulations and an\nassociated mean-field theory, we demonstrate that the temporal structure of the\nSTDP rule, specifically its degree of causality, critically determines whether\nassemblies that share neurons maintain segregation or merge together after\ntraining is completed. We find that causal STDP rules, where\npotentiation\/depression occurs strictly when presynaptic spikes precede\/proceed\npostsynaptic spikes, allow assemblies to remain distinct even with substantial\noverlap in membership. This stability arises because causal STDP effectively\ncancels the symmetric correlations introduced by common inputs from shared\nneurons. In contrast, acausal STDP rules lead to assembly fusion when overlap\nexceeds a critical threshold, due to unchecked growth of common input\ncorrelations. Our results provide theoretical insight into how\nspike-timing-dependent learning rules can support distributed representation\nwhere individual neurons participate in multiple assemblies while maintaining\nfunctional specificity.",
        "Our brains encode many features of the sensory world into memories: we can\nsing along with songs we have heard before, interpret spoken and written\nlanguage composed of words we have learned, and recognize faces and objects.\nWhere are these memories stored? Each area of the cerebral cortex has a huge\nnumber of local, recurrent, excitatory-excitatory synapses, as many as 500\nmillion per cubic millimeter. Here I review evidence that cortical recurrent\nconnectivity in sensory cortex is a substrate for sensory memories. Evidence\nsuggests that the local recurrent network encodes the structure of natural\nsensory input, and that it does so via active filtering, transforming network\ninputs to boost or select those associated with natural sensation. This is a\nform of predictive processing, in which the cortical recurrent network\nselectively amplifies some input patterns and attenuates others, and a form of\nmemory.",
        "The claustrum is a thin gray matter structure in each brain hemisphere,\ncharacterized by exceptionally high connectivity with nearly all brain regions.\nDespite extensive animal studies on its anatomy and function and growing\nevidence of claustral deficits in neuropsychiatric disorders, its specific\nroles in normal and abnormal human brain function remain largely unknown. This\nis primarily due to its thin and complex morphology, which limits accurate\nanatomical delineation and neural activity isolation in conventional in vivo\nneuroimaging. To facilitate future neuroimaging studies, we developed a\ncomprehensive and reliable manual segmentation protocol based on a\ncellular-resolution brain atlas and high-resolution (0.7^3 mm) MRI data. The\nprotocols involve detailed guidelines to delineate the entire claustrum,\nincluding the inferior parts that have not been clearly described in earlier\nMRI studies. Additionally, we propose a geometric method to parcellate the\nclaustrum into three subregions (the dorsal, ventral, and temporal claustrum)\nalong the superior-to-inferior axis. The mean bilateral claustrum volume in 10\nyoung adults was 3307.5 mm^3, approximately 0.21% of total intracranial volume.\nOur segmentation protocol demonstrated high inter- and intra-rater reliability\n(ICC > 0.89, DSC > 0.85), confirming its replicability. This comprehensive and\nreliable claustrum segmentation protocols will provide a cornerstone for future\nneuroimaging studies of systematic, large-scale investigations of the anatomy\nand the functions of the human claustrum in normal and pathological\npopulations.",
        "Describing chemical reactions in solution on a molecular level is a\nchallenging task due to the high mobility of weakly interacting solvent\nmolecules which requires configurational sampling. For instance, polar and\nprotic solvents can interact strongly with solutes and may interfere in\nreactions. However, to define and identify representative arrangements of\nsolvent molecules modulating a transition state is a non-trivial task. Here, we\npropose to monitor their active participation in the decaying normal mode at a\ntransition state, which defines active solvent molecules. Moreover, it is\ndesirable to prepare a low-dimensional microsolvation model in a well-defined,\nfully automated, high-throughput, and easy-to-deploy fashion, which we propose\nto derive in a stepwise protocol. First, transition state structures are\noptimized in a sufficiently solvated quantum-classical hybrid model, which are\nthen subjected to a re-definition of a then reduced quantum region. From the\nreduced model, minimally microsolvated structures are extracted that contain\nonly active solvent molecules. Modeling the remaining solvation effects is\ndeferred to a continuum model. To establish an easy-to-use free-energy model,\nwe combine the standard thermochemical gas-phase model with a correction for\nthe cavity entropy in solution. We assess our microsolvation and free-energy\nmodels for methanediol formation from formaldehyde, for the hydration of carbon\ndioxide (which we consider in a solvent mixture to demonstrate the versatility\nof our approach), and, finally, for the chlorination of phenol with\nhypochlorous acid.",
        "Magnon spin currents in insulating magnets are useful for low-power\nspintronics. However, in magnets stacked by antiferromagnetic (AFM) exchange\ncoupling, which have recently aroused significant interest for potential\napplications in spintronics, these currents are largely counteracted by\nopposite magnetic sublattices, thus suppressing their net effect. Contrary to\nthis common observation, here, we show that magnets with X-type AFM stacking,\nwhere opposite magnetic sublattices form orthogonal intersecting chains,\nsupport giant magnon spin currents with minimal compensation. Our model\nHamiltonian calculations predict magnetic chain locking of magnon spin currents\nin these X-type magnets, significantly reducing their compensation ratio. In\naddition, the one-dimensional nature of the chain-like magnetic sublattices\nenhances magnon spin conductivities surpassing those of two-dimensional\nferromagnets and canonical altermagnets. Notably, uncompensated X-type magnets,\nsuch as odd-layer antiferromagnets and ferrimagnets, can exhibit magnon spin\ncurrents polarized opposite to those expected by their net magnetization. These\nunprecedented properties of X-type magnets, combined with their inherent\nadvantages resulting from AFM coupling, offer a promising new path for\nlow-power high-performance spintronics.",
        "Derivatives, as a critical class of financial instruments, isolate and trade\nthe price attributes of risk assets such as stocks, commodities, and indices,\naiding risk management and enhancing market efficiency. However, traditional\nhedging models, constrained by assumptions such as continuous trading and zero\ntransaction costs, fail to satisfy risk control requirements in complex and\nuncertain real-world markets.\n  With advances in computing technology and deep learning, data-driven trading\nstrategies are becoming increasingly prevalent. This thesis proposes a\nderivatives hedging framework integrating deep learning and reinforcement\nlearning. The framework comprises a probabilistic forecasting model and a\nhedging agent, enabling market probability prediction, derivative pricing, and\nhedging.\n  Specifically, we design a spatiotemporal attention-based probabilistic\nfinancial time series forecasting Transformer to address the scarcity of\nderivatives hedging data. A low-rank attention mechanism compresses\nhigh-dimensional assets into a low-dimensional latent space, capturing\nnonlinear asset relationships. The Transformer models sequential dependencies\nwithin this latent space, improving market probability forecasts and\nconstructing an online training environment for downstream hedging tasks.\n  Additionally, we incorporate generalized geometric Brownian motion to develop\na risk-neutral pricing approach for derivatives. We model derivatives hedging\nas a reinforcement learning problem with sparse rewards and propose a behavior\ncloning-based recurrent proximal policy optimization (BC-RPPO) algorithm. This\npretraining-finetuning framework significantly enhances the hedging agent's\nperformance. Numerical experiments in the U.S. and Chinese financial markets\ndemonstrate our method's superiority over traditional approaches.",
        "This paper considers $n= 128$ dimensional construction A lattice design,\nusing binary codes with known minimum Hamming distance and codeword\nmultiplicity, the number of minimum weight codeword. A truncated theta series\nof the lattice is explicitly given to obtain the truncated union bound to\nestimate the word error rate under maximum likelihood decoding. The best\ncomponent code is selected by minimizing the required volume-to-noise ratio\n(VNR) for a target word error rate $P_e$. The estimate becomes accurate for\n$P_e \\leq 10^{-4}$, and design examples are given with the best extended BCH\ncodes and polar codes for $P_e= 10^{-4}$ to $10^{-8}$. A lower error rate is\nachieved compared to that by the classic balanced distance rule and the equal\nerror probability rule. The $(128, 106, 8)$ EBCH code gives the best-known\n$n=128$ construction A lattice at $P_e= 10^{-5}$.",
        "This paper explores the data-aided regularization of the direct-estimate\ncombiner in the uplink of a distributed multiple-input multiple-output system.\nThe network-wide combiner can be computed directly from the pilot signal\nreceived at each access point, eliminating the need for explicit channel\nestimation. However, the sample covariance matrix of the received pilot signal\nthat is used in its computation may significantly deviate from the actual\ncovariance matrix when the number of pilot symbols is limited. To address this,\nwe apply a regularization to the sample covariance matrix using a shrinkage\ncoefficient based on the received data signal. Initially, the shrinkage\ncoefficient is determined by minimizing the difference between the sample\ncovariance matrices obtained from the received pilot and data signals. Given\nthe limitations of this approach in interference-limited scenarios, the\nshrinkage coefficient is iteratively optimized using the sample mean squared\nerror of the hard-decision symbols, which is more closely related to the actual\nsystem's performance, e.g., the symbol error rate (SER). Numerical results\ndemonstrate that the proposed regularization of the direct-estimate combiner\nsignificantly enhances the SER, particularly when the number of pilot symbols\nis limited.",
        "We address the problem of safety verification for nonlinear stochastic\nsystems, specifically the task of certifying that system trajectories remain\nwithin a safe set with high probability. To tackle this challenge, we adopt a\nset-erosion strategy, which decouples the effects of stochastic disturbances\nfrom deterministic dynamics. This approach converts the stochastic safety\nverification problem on a safe set into a deterministic safety verification\nproblem on an eroded subset of the safe set. The success of this strategy\nhinges on the depth of erosion, which is determined by a probabilistic tube\nthat bounds the deviation of stochastic trajectories from their corresponding\ndeterministic trajectories. Our main contribution is the establishment of a\ntight bound for the probabilistic tube of nonlinear stochastic systems. To\nobtain a probabilistic bound for stochastic trajectories, we adopt a\nmartingale-based approach. The core innovation lies in the design of a novel\nenergy function associated with the averaged moment generating function, which\nforms an affine martingale, a generalization of the traditional c-martingale.\nUsing this energy function, we derive a precise bound for the probabilistic\ntube. Furthermore, we enhance this bound by incorporating the union-bound\ninequality for strictly contractive dynamics. By integrating the derived\nprobabilistic tubes into the set-erosion strategy, we demonstrate that the\nsafety verification problem for nonlinear stochastic systems can be reduced to\na deterministic safety verification problem. Our theoretical results are\nvalidated through applications in reachability-based safety verification and\nsafe controller synthesis, accompanied by several numerical examples that\nillustrate their effectiveness.",
        "Table 1 of Hall (1988) contains asymptotic coverage error formulas for some\nnonparametric approximate 95% confidence intervals for the mean based on $n$\nIID samples. The table includes an entry for an interval based on the central\nlimit theorem using Gaussian quantiles and the Gaussian maximum likelihood\nvariance estimate. It is missing an entry for the very widely used Student $t$\nconfidence intervals. This note makes a mild numerical correction for the\nGaussian entry and provides an entry for the Student $t$ intervals. For\nskewness $\\gamma$ and kurtosis $\\kappa$, the corrected Gaussian formula is\n$0.14\\kappa -2.16\\gamma^2-3.42$ and the formula for the $t$ intervals is\n$0.14\\kappa -2.16\\gamma^2$. The impetus to revisit this estimate arose from the\nsurprisingly robust performance of Student's t statistic in randomized\nquasi-Monte Carlo sampling.",
        "Halide perovskite materials have been extensively studied in the last decade\nbecause of their impressive optoelectronic properties. However, their one\ncharacteristic that is uncommon for semiconductors is that many undergo\nthermally induced structural phase transitions. The transition is hysteretic,\nwith the hysteresis window marking the boundary of the metastable phase. We\nhave discovered that in methylammonium lead iodide, this hysteretic metastable\nphase is athermal, meaning it shows almost no temporal phase evolution under\nisothermal conditions. We also show that a large number of distinguishable\nmetastable states can be prepared following different thermal pathways.\nFurthermore, under a reversible thermal perturbation, the states in the\nmetastable phase either show return-point memory or undergo a systematic\nnonrecoverable phase evolution, depending on the thermal history and the sign\nof the temperature perturbation. Since the phase fraction can be probed with\nextreme sensitivity via luminescence, we have an optically retrievable memory\nthat reliably records any breach in temperature stability. Such thermal-breach\nmemory in athermal martensites, of which there are numerous examples, may be\nuseful for tagging packages requiring strict temperature control during\ntransportation or preservation.",
        "As large language models have evolved, it has become crucial to distinguish\nbetween process supervision and outcome supervision -- two key reinforcement\nlearning approaches to complex reasoning tasks. While process supervision\noffers intuitive advantages for long-term credit assignment, the precise\nrelationship between these paradigms has remained an open question.\nConventional wisdom suggests that outcome supervision is fundamentally more\nchallenging due to the trajectory-level coverage problem, leading to\nsignificant investment in collecting fine-grained process supervision data.\n  In this paper, we take steps towards resolving this debate. Our main theorem\nshows that, under standard data coverage assumptions, reinforcement learning\nthrough outcome supervision is no more statistically difficult than through\nprocess supervision, up to polynomial factors in horizon. At the core of this\nresult lies the novel Change of Trajectory Measure Lemma -- a technical tool\nthat bridges return-based trajectory measure and step-level distribution shift.\nFurthermore, for settings with access to a verifier or a rollout capability, we\nprove that any policy's advantage function can serve as an optimal process\nreward model, providing a direct connection between outcome and process\nsupervision. These findings suggest that the empirically observed performance\ngap -- if any -- between outcome and process supervision likely stems from\nalgorithmic limitations rather than inherent statistical difficulties,\npotentially transforming how we approach data collection and algorithm design\nfor reinforcement learning.",
        "We perform a systematic study of the possible molecular states composed of a\npair of heavy mesons such as $D^{(*)}D^{(*)}$, $D^{(*)}\\bar{D}^{(*)}$ in the\nframework of the one-boson-exchange model. The exchanged bosons include the\npseudoscalar, scalar and vector mesons($\\pi$, $\\sigma$, $\\rho$, $\\omega$). We\nuse the Bonn approximation to get the interaction potential of\none-boson-exchange model, then apply the complex scaling method to calculate\nthe bound and resonant states. The results indicate that the $D^{(*)}D^{(*)}$\nand $D^{(*)}\\bar{D}^{(*)}$ system can not only form several bound states, but\nalso a P-wave resonant state. The hadron molecular state model can explain the\nstructure of $T_{cc}^+$ as a bound state $DD^{*}$ with quantum number $I(J^P) =\n0(1^+)$. In addition, we also discovered other bound and resonant states, which\nhave the potential to be observed experimentally.",
        "The ability to integrate task-relevant information into neural\nrepresentations is a fundamental aspect of both biological and artificial\nintelligence. To enable theoretical analysis, recent work has examined whether\na network learns task-relevant features (rich learning) or resembles a random\nfeature model (or a kernel machine, i.e., lazy learning). However, this simple\nlazy-versus-rich dichotomy overlooks the possibility of various subtypes of\nfeature learning that emerge from different architectures, learning rules, and\ndata properties. Furthermore, most existing approaches emphasize weight\nmatrices or neural tangent kernels, limiting their applicability to\nneuroscience because they do not explicitly characterize representations.\n  In this work, we introduce an analysis framework based on representational\ngeometry to study feature learning. Instead of analyzing what are the learned\nfeatures, we focus on characterizing how task-relevant representational\nmanifolds evolve during the learning process. In both theory and experiment, we\nfind that when a network learns features useful for solving a task, the\ntask-relevant manifolds become increasingly untangled. Moreover, by tracking\nchanges in the underlying manifold geometry, we uncover distinct learning\nstages throughout training, as well as different learning strategies associated\nwith training hyperparameters, uncovering subtypes of feature learning beyond\nthe lazy-versus-rich dichotomy. Applying our method to neuroscience and machine\nlearning, we gain geometric insights into the structural inductive biases of\nneural circuits solving cognitive tasks and the mechanisms underlying\nout-of-distribution generalization in image classification. Our framework\nprovides a novel geometric perspective for understanding and quantifying\nfeature learning in both artificial and biological neural networks.",
        "A long-standing goal of social network research has been to alter the\nproperties of network to achieve the desired outcome. In doing so, DeGroot's\nconsensus model has served as the popular choice for modeling the information\ndiffusion and opinion formation in social networks. Achieving a trade-off\nbetween the cost associated with modifications made to the network and the\nspeed of convergence to the desired state has shown to be a critical factor.\nThis has been treated as the Fastest Mixing Markov Chain (FMMC) problem over a\ngraph with given transition probabilities over a subset of edges. Addressing\nthis multi-objective optimization problem over the friendship graph, this paper\nhas provided the corresponding Pareto optimal points or the Pareto frontier. In\nthe case of friendship graph with at least three blades, it is shown that the\nPareto frontier is reduced to a global minimum point which is same as the\noptimal point corresponding to the minimum spanning tree of the friendship\ngraph, i.e., the star topology. Furthermore, a lower limit for transition\nprobabilities among friends has been provided, where values higher than this\nlimit do not have any impact on the convergence rate.",
        "A scheme for the adaptive preparation of a topological state with dipole\nsymmetry, dubbed the dipolar topological state (dTS), which serves as an\nexample of translation symmetry-enriched topological phase, is proposed. The\nmidcircuit state emerging during the preparation process is identified as a\ntwo-dimensional symmetry-protected topological (SPT) state protected by dipole\nbundle symmetry alongside charge and 1-form symmetries. The non-trivial\nboundary modes of the dipolar SPT state exhibiting the spontaneous breaking of\ncharge and dipole bundle symmetries are analyzed. The duality map between the\nparamagnetic state and the dipolar topological state is established in the\nframework of the {\\it simultaneous gauging} of two charge symmetries and one\ndipole symmetry that cannot be reduced as sequential gauging of the individual\nsymmetry. Leveraging this duality, we work out the phase diagram of the dipolar\ntopological state under perturbations by various transverse fields.",
        "This work derives two basic engagement zone models, describing regions of\npotential risk or capture for a mobile vehicle by a pursuer. The pursuer is\nmodeled as having turn-constraints rather than simple motion. Turn-only\n(C-Paths) and turn-straight (CS-Paths) paths are considered for the pursuer of\nlimited range. Following the derivation, a simulation of a vehicle avoiding the\npursuer's engagement zone is provided.",
        "In overparameterized logistic regression, gradient descent (GD) iterates\ndiverge in norm while converging in direction to the maximum $\\ell_2$-margin\nsolution -- a phenomenon known as the implicit bias of GD. This work\ninvestigates additional regularization effects induced by early stopping in\nwell-specified high-dimensional logistic regression. We first demonstrate that\nthe excess logistic risk vanishes for early-stopped GD but diverges to infinity\nfor GD iterates at convergence. This suggests that early-stopped GD is\nwell-calibrated, whereas asymptotic GD is statistically inconsistent. Second,\nwe show that to attain a small excess zero-one risk, polynomially many samples\nare sufficient for early-stopped GD, while exponentially many samples are\nnecessary for any interpolating estimator, including asymptotic GD. This\nseparation underscores the statistical benefits of early stopping in the\noverparameterized regime. Finally, we establish nonasymptotic bounds on the\nnorm and angular differences between early-stopped GD and $\\ell_2$-regularized\nempirical risk minimizer, thereby connecting the implicit regularization of GD\nwith explicit $\\ell_2$-regularization."
      ]
    }
  },
  {
    "id":2411.00726,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema",
    "start_abstract":"This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Active Learning from Scene Embeddings for End-to-End Autonomous Driving",
        "Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical\n  Anomaly Detection",
        "HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and\n  Generation",
        "Learning Extremely High Density Crowds as Active Matters",
        "StreamingRAG: Real-time Contextual Retrieval and Generation Framework",
        "DynamicVis: An Efficient and General Visual Foundation Model for Remote\n  Sensing Image Understanding",
        "MFM-DA: Instance-Aware Adaptor and Hierarchical Alignment for Efficient\n  Domain Adaptation in Medical Foundation Models",
        "SP-SLAM: Neural Real-Time Dense SLAM With Scene Priors",
        "Focus-N-Fix: Region-Aware Fine-Tuning for Text-to-Image Generation",
        "Semantics-aware Test-time Adaptation for 3D Human Pose Estimation",
        "Multi-view Video-Pose Pretraining for Operating Room Surgical Activity\n  Recognition",
        "ST-Think: How Multimodal Large Language Models Reason About 4D Worlds\n  from Ego-Centric Videos",
        "Similarity-Guided Layer-Adaptive Vision Transformer for UAV Tracking",
        "Residual connections provably mitigate oversmoothing in graph neural\n  networks",
        "Artificial intelligence for objective assessment of acrobatic movements:\n  How to apply machine learning for identifying tumbling elements in cheer\n  sports",
        "Cost-Efficient Continual Learning with Sufficient Exemplar Memory",
        "Magnetic Interactions in the Polar Ferrimagnet with a Bipartite\n  Structure",
        "Dynamic Basis Function Generation for Network Revenue Management",
        "Sink-free orientations: a local sampler with applications",
        "Exploring coronal abundances of M dwarfs at moderate activity levels",
        "Counterdiabatic Driving with Performance Guarantees",
        "AI Explainability for Power Electronics: From a Lipschitz Continuity\n  Perspective",
        "Nuclear Spin Induced Transparency",
        "RAMOTS: A Real-Time System for Aerial Multi-Object Tracking based on\n  Deep Learning and Big Data Technology",
        "The Role of Planetary-Scale Waves on the Stratospheric Superrotation in\n  Titan's Atmosphere",
        "An examination of the extended Hong-Ou-Mandel effect and considerations\n  for experimental detection",
        "Supercritical phase transition on the Toeplitz algebra of $\\mathbb\n  N^\\times \\ltimes \\mathbb Z$",
        "Experimental Test of Nonlocality Limits from Relativistic Independence"
      ],
      "abstract":[
        "In the field of autonomous driving, end-to-end deep learning models show\ngreat potential by learning driving decisions directly from sensor data.\nHowever, training these models requires large amounts of labeled data, which is\ntime-consuming and expensive. Considering that the real-world driving data\nexhibits a long-tailed distribution where simple scenarios constitute a\nmajority part of the data, we are thus inspired to identify the most\nchallenging scenarios within it. Subsequently, we can efficiently improve the\nperformance of the model by training with the selected data of the highest\nvalue. Prior research has focused on the selection of valuable data by\nempirically designed strategies. However, manually designed methods suffer from\nbeing less generalizable to new data distributions. Observing that the BEV\n(Bird's Eye View) features in end-to-end models contain all the information\nrequired to represent the scenario, we propose an active learning framework\nthat relies on these vectorized scene-level features, called SEAD. The\nframework selects initial data based on driving-environmental information and\nincremental data based on BEV features. Experiments show that we only need 30\\%\nof the nuScenes training data to achieve performance close to what can be\nachieved with the full dataset. The source code will be released.",
        "Unsupervised anomaly detection using deep learning has garnered significant\nresearch attention due to its broad applicability, particularly in medical\nimaging where labeled anomalous data are scarce. While earlier approaches\nleverage generative models like autoencoders and generative adversarial\nnetworks (GANs), they often fall short due to overgeneralization. Recent\nmethods explore various strategies, including memory banks, normalizing flows,\nself-supervised learning, and knowledge distillation, to enhance\ndiscrimination. Among these, knowledge distillation, particularly reverse\ndistillation, has shown promise. Following this paradigm, we propose a novel\nscale-aware contrastive reverse distillation model that addresses two key\nlimitations of existing reverse distillation methods: insufficient feature\ndiscriminability and inability to handle anomaly scale variations.\nSpecifically, we introduce a contrastive student-teacher learning approach to\nderive more discriminative representations by generating and exploring\nout-of-normal distributions. Further, we design a scale adaptation mechanism to\nsoftly weight contrastive distillation losses at different scales to account\nfor the scale variation issue. Extensive experiments on benchmark datasets\ndemonstrate state-of-the-art performance, validating the efficacy of the\nproposed method. Code is available at https:\/\/github.com\/MedAITech\/SCRD4AD.",
        "The remarkable success of the autoregressive paradigm has made significant\nadvancement in Multimodal Large Language Models (MLLMs), with powerful models\nlike Show-o, Transfusion and Emu3 achieving notable progress in unified image\nunderstanding and generation. For the first time, we uncover a common\nphenomenon: the understanding capabilities of MLLMs are typically stronger than\ntheir generative capabilities, with a significant gap between the two. Building\non this insight, we propose HermesFlow, a simple yet general framework designed\nto seamlessly bridge the gap between understanding and generation in MLLMs.\nSpecifically, we take the homologous data as input to curate homologous\npreference data of both understanding and generation. Through Pair-DPO and\nself-play iterative optimization, HermesFlow effectively aligns multimodal\nunderstanding and generation using homologous preference data. Extensive\nexperiments demonstrate the significant superiority of our approach over prior\nmethods, particularly in narrowing the gap between multimodal understanding and\ngeneration. These findings highlight the potential of HermesFlow as a general\nalignment framework for next-generation multimodal foundation models. Code:\nhttps:\/\/github.com\/Gen-Verse\/HermesFlow",
        "Video-based high-density crowd analysis and prediction has been a\nlong-standing topic in computer vision. It is notoriously difficult due to, but\nnot limited to, the lack of high-quality data and complex crowd dynamics.\nConsequently, it has been relatively under studied. In this paper, we propose a\nnew approach that aims to learn from in-the-wild videos, often with low quality\nwhere it is difficult to track individuals or count heads. The key novelty is a\nnew physics prior to model crowd dynamics. We model high-density crowds as\nactive matter, a continumm with active particles subject to stochastic forces,\nnamed 'crowd material'. Our physics model is combined with neural networks,\nresulting in a neural stochastic differential equation system which can mimic\nthe complex crowd dynamics. Due to the lack of similar research, we adapt a\nrange of existing methods which are close to ours for comparison. Through\nexhaustive evaluation, we show our model outperforms existing methods in\nanalyzing and forecasting extremely high-density crowds. Furthermore, since our\nmodel is a continuous-time physics model, it can be used for simulation and\nanalysis, providing strong interpretability. This is categorically different\nfrom most deep learning methods, which are discrete-time models and\nblack-boxes.",
        "Extracting real-time insights from multi-modal data streams from various\ndomains such as healthcare, intelligent transportation, and satellite remote\nsensing remains a challenge. High computational demands and limited knowledge\nscope restrict the applicability of Multi-Modal Large Language Models (MM-LLMs)\non these data streams. Traditional Retrieval-Augmented Generation (RAG) systems\naddress knowledge limitations of these models, but suffer from slow\npreprocessing, making them unsuitable for real-time analysis. We propose\nStreamingRAG, a novel RAG framework designed for streaming data. StreamingRAG\nconstructs evolving knowledge graphs capturing scene-object-entity\nrelationships in real-time. The knowledge graph achieves temporal-aware scene\nrepresentations using MM-LLMs and enables timely responses for specific events\nor user queries. StreamingRAG addresses limitations in existing methods,\nachieving significant improvements in real-time analysis (5-6x faster\nthroughput), contextual accuracy (through a temporal knowledge graph), and\nreduced resource consumption (using lightweight models by 2-3x).",
        "The advancement of remote sensing technology has improved the spatial\nresolution of satellite imagery, facilitating more detailed visual\nrepresentations for diverse interpretations. However, existing methods exhibit\nlimited generalization capabilities across varied applications. While some\ncontemporary foundation models demonstrate potential, they are hindered by\ninsufficient cross-task adaptability and primarily process low-resolution\nimagery of restricted sizes, thus failing to fully exploit high-resolution data\nor leverage comprehensive large-scene semantics. Crucially, remote sensing\nimagery differs fundamentally from natural images, as key foreground targets\n(eg., maritime objects, artificial structures) often occupy minimal spatial\nproportions (~1%) and exhibit sparse distributions. Efficiently modeling\ncross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a\nsignificant challenge yet remains critical for remote sensing image\nunderstanding. Motivated by the selective attention mechanisms inherent to the\nhuman visual system, we propose DynamicVis, a dynamic visual perception\nfoundation model for remote sensing imagery. The framework integrates a novel\ndynamic region perception backbone based on the selective state space model,\nwhich strategically balances localized detail extraction with global contextual\nintegration, enabling computationally efficient encoding of large-scale data\nwhile maintaining architectural scalability. To enhance cross-task knowledge\ntransferring, we introduce a multi-instance learning paradigm utilizing\nmeta-embedding representations, trained on million-scale region-level\nannotations. Evaluations across nine downstream tasks demonstrate the model's\nversatility. DynamicVis achieves multi-level feature modeling with exceptional\nefficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and\n833 MB GPU memory (3% of ViT's).",
        "Medical Foundation Models (MFMs), trained on large-scale datasets, have\ndemonstrated superior performance across various tasks. However, these models\nstill struggle with domain gaps in practical applications. Specifically, even\nafter fine-tuning on source-domain data, task-adapted foundation models often\nperform poorly in the target domain. To address this challenge, we propose a\nfew-shot unsupervised domain adaptation (UDA) framework for MFMs, named MFM-DA,\nwhich only leverages a limited number of unlabeled target-domain images. Our\napproach begins by training a Denoising Diffusion Probabilistic Model (DDPM),\nwhich is then adapted to the target domain using a proposed dynamic\ninstance-aware adaptor and a distribution direction loss, enabling the DDPM to\ntranslate source-domain images into the target domain style. The adapted images\nare subsequently processed through the MFM, where we introduce a designed\nchannel-spatial alignment Low-Rank Adaptation (LoRA) to ensure effective\nfeature alignment. Extensive experiments on optic cup and disc segmentation\ntasks demonstrate that MFM-DA outperforms state-of-the-art methods. Our work\nprovides a practical solution to the domain gap issue in real-world MFM\ndeployment. Code will be available at here.",
        "Neural implicit representations have recently shown promising progress in\ndense Simultaneous Localization And Mapping (SLAM). However, existing works\nhave shortcomings in terms of reconstruction quality and real-time performance,\nmainly due to inflexible scene representation strategy without leveraging any\nprior information. In this paper, we introduce SP-SLAM, a novel neural RGB-D\nSLAM system that performs tracking and mapping in real-time. SP-SLAM computes\ndepth images and establishes sparse voxel-encoded scene priors near the\nsurfaces to achieve rapid convergence of the model. Subsequently, the encoding\nvoxels computed from single-frame depth image are fused into a global volume,\nwhich facilitates high-fidelity surface reconstruction. Simultaneously, we\nemploy tri-planes to store scene appearance information, striking a balance\nbetween achieving high-quality geometric texture mapping and minimizing memory\nconsumption. Furthermore, in SP-SLAM, we introduce an effective optimization\nstrategy for mapping, allowing the system to continuously optimize the poses of\nall historical input frames during runtime without increasing computational\noverhead. We conduct extensive evaluations on five benchmark datasets (Replica,\nScanNet, TUM RGB-D, Synthetic RGB-D, 7-Scenes). The results demonstrate that,\ncompared to existing methods, we achieve superior tracking accuracy and\nreconstruction quality, while running at a significantly faster speed.",
        "Text-to-image (T2I) generation has made significant advances in recent years,\nbut challenges still remain in the generation of perceptual artifacts,\nmisalignment with complex prompts, and safety. The prevailing approach to\naddress these issues involves collecting human feedback on generated images,\ntraining reward models to estimate human feedback, and then fine-tuning T2I\nmodels based on the reward models to align them with human preferences.\nHowever, while existing reward fine-tuning methods can produce images with\nhigher rewards, they may change model behavior in unexpected ways. For example,\nfine-tuning for one quality aspect (e.g., safety) may degrade other aspects\n(e.g., prompt alignment), or may lead to reward hacking (e.g., finding a way to\nincrease rewards without having the intended effect). In this paper, we propose\nFocus-N-Fix, a region-aware fine-tuning method that trains models to correct\nonly previously problematic image regions. The resulting fine-tuned model\ngenerates images with the same high-level structure as the original model but\nshows significant improvements in regions where the original model was\ndeficient in safety (over-sexualization and violence), plausibility, or other\ncriteria. Our experiments demonstrate that Focus-N-Fix improves these localized\nquality aspects with little or no degradation to others and typically\nimperceptible changes in the rest of the image. Disclaimer: This paper contains\nimages that may be overly sexual, violent, offensive, or harmful.",
        "This work highlights a semantics misalignment in 3D human pose estimation.\nFor the task of test-time adaptation, the misalignment manifests as overly\nsmoothed and unguided predictions. The smoothing settles predictions towards\nsome average pose. Furthermore, when there are occlusions or truncations, the\nadaptation becomes fully unguided. To this end, we pioneer the integration of a\nsemantics-aware motion prior for the test-time adaptation of 3D pose\nestimation. We leverage video understanding and a well-structured motion-text\nspace to adapt the model motion prediction to adhere to video semantics during\ntest time. Additionally, we incorporate a missing 2D pose completion based on\nthe motion-text similarity. The pose completion strengthens the motion prior's\nguidance for occlusions and truncations. Our method significantly improves\nstate-of-the-art 3D human pose estimation TTA techniques, with more than 12%\ndecrease in PA-MPJPE on 3DPW and 3DHP.",
        "Understanding the workflow of surgical procedures in complex operating rooms\nrequires a deep understanding of the interactions between clinicians and their\nenvironment. Surgical activity recognition (SAR) is a key computer vision task\nthat detects activities or phases from multi-view camera recordings. Existing\nSAR models often fail to account for fine-grained clinician movements and\nmulti-view knowledge, or they require calibrated multi-view camera setups and\nadvanced point-cloud processing to obtain better results. In this work, we\npropose a novel calibration-free multi-view multi-modal pretraining framework\ncalled Multiview Pretraining for Video-Pose Surgical Activity Recognition\nPreViPS, which aligns 2D pose and vision embeddings across camera views. Our\nmodel follows CLIP-style dual-encoder architecture: one encoder processes\nvisual features, while the other encodes human pose embeddings. To handle the\ncontinuous 2D human pose coordinates, we introduce a tokenized discrete\nrepresentation to convert the continuous 2D pose coordinates into discrete pose\nembeddings, thereby enabling efficient integration within the dual-encoder\nframework. To bridge the gap between these two modalities, we propose several\npretraining objectives using cross- and in-modality geometric constraints\nwithin the embedding space and incorporating masked pose token prediction\nstrategy to enhance representation learning. Extensive experiments and ablation\nstudies demonstrate improvements over the strong baselines, while\ndata-efficiency experiments on two distinct operating room datasets further\nhighlight the effectiveness of our approach. We highlight the benefits of our\napproach for surgical activity recognition in both multi-view and single-view\nsettings, showcasing its practical applicability in complex surgical\nenvironments. Code will be made available at:\nhttps:\/\/github.com\/CAMMA-public\/PreViPS.",
        "Humans excel at spatio-temporal reasoning, effortlessly interpreting dynamic\nvisual events from an egocentric viewpoint. However, whether multimodal large\nlanguage models (MLLMs) can similarly comprehend the 4D world remains\nuncertain. This paper explores multimodal spatio-temporal reasoning from an\negocentric perspective, aiming to equip MLLMs with human-like reasoning\ncapabilities. To support this objective, we introduce Ego-ST Bench, a novel\nbenchmark containing over 5,000 question-answer pairs across four categories,\nsystematically evaluating spatial, temporal, and integrated spatio-temporal\nreasoning. Additionally, we propose the ST-R1 Video model, a video-based\nreasoning model that incorporates reverse thinking into its reinforcement\nlearning process, significantly enhancing performance. We combine\nlong-chain-of-thought (long-CoT) supervised fine-tuning with Group Relative\nPolicy Optimization (GRPO) reinforcement learning, achieving notable\nimprovements with limited high-quality data. Ego-ST Bench and ST-R1 provide\nvaluable insights and resources for advancing video-based spatio-temporal\nreasoning research.",
        "Vision transformers (ViTs) have emerged as a popular backbone for visual\ntracking. However, complete ViT architectures are too cumbersome to deploy for\nunmanned aerial vehicle (UAV) tracking which extremely emphasizes efficiency.\nIn this study, we discover that many layers within lightweight ViT-based\ntrackers tend to learn relatively redundant and repetitive target\nrepresentations. Based on this observation, we propose a similarity-guided\nlayer adaptation approach to optimize the structure of ViTs. Our approach\ndynamically disables a large number of representation-similar layers and\nselectively retains only a single optimal layer among them, aiming to achieve a\nbetter accuracy-speed trade-off. By incorporating this approach into existing\nViTs, we tailor previously complete ViT architectures into an efficient\nsimilarity-guided layer-adaptive framework, namely SGLATrack, for real-time UAV\ntracking. Extensive experiments on six tracking benchmarks verify the\neffectiveness of the proposed approach, and show that our SGLATrack achieves a\nstate-of-the-art real-time speed while maintaining competitive tracking\nprecision. Codes and models are available at\nhttps:\/\/github.com\/GXNU-ZhongLab\/SGLATrack.",
        "Graph neural networks (GNNs) have achieved remarkable empirical success in\nprocessing and representing graph-structured data across various domains.\nHowever, a significant challenge known as \"oversmoothing\" persists, where\nvertex features become nearly indistinguishable in deep GNNs, severely\nrestricting their expressive power and practical utility. In this work, we\nanalyze the asymptotic oversmoothing rates of deep GNNs with and without\nresidual connections by deriving explicit convergence rates for a normalized\nvertex similarity measure. Our analytical framework is grounded in the\nmultiplicative ergodic theorem. Furthermore, we demonstrate that adding\nresidual connections effectively mitigates or prevents oversmoothing across\nseveral broad families of parameter distributions. The theoretical findings are\nstrongly supported by numerical experiments.",
        "Over the past four decades, cheerleading has evolved from a sideline activity\nat major sporting events into a professional, competitive sport with growing\nglobal popularity. Evaluating tumbling elements in cheerleading relies on both\nobjective measures and subjective judgments, such as difficulty and execution\nquality. However, the complexity of tumbling - encompassing team synchronicity,\nground interactions, choreography, and artistic expression - makes objective\nassessment challenging. Artificial intelligence (AI) has revolutionized various\nscientific fields and industries through precise data-driven analyses, yet\ntheir application in acrobatic sports remains limited despite significant\npotential for enhancing performance evaluation and coaching. This study\ninvestigates the feasibility of using an AI-based approach with data from a\nsingle inertial measurement unit to accurately identify and objectively assess\ntumbling elements in standard cheerleading routines. A sample of 16\nparticipants (13 females, 3 males) from a Division I collegiate cheerleading\nteam wore a single inertial measurement unit at the dorsal pelvis. Over a\n4-week seasonal preparation period, 1102 tumbling elements were recorded during\nregular practice sessions. Using triaxial accelerations and rotational speeds,\nvarious ML algorithms were employed to classify and evaluate the execution of\ntumbling manoeuvres. Results indicate that certain machine learning models can\neffectively identify different tumbling elements despite inter-individual\nvariability and data noise, achieving high accuracy. These findings demonstrate\nthe significant potential for integrating AI-driven assessments into\ncheerleading and other acrobatic sports, providing objective metrics that\ncomplement traditional judging methods.",
        "Continual learning (CL) research typically assumes highly constrained\nexemplar memory resources. However, in many real-world scenarios-especially in\nthe era of large foundation models-memory is abundant, while GPU computational\ncosts are the primary bottleneck. In this work, we investigate CL in a novel\nsetting where exemplar memory is ample (i.e., sufficient exemplar memory).\nUnlike prior methods designed for strict exemplar memory constraints, we\npropose a simple yet effective approach that directly operates in the model's\nweight space through a combination of weight resetting and averaging\ntechniques. Our method achieves state-of-the-art performance while reducing the\ncomputational cost to a quarter or third of existing methods. These findings\nchallenge conventional CL assumptions and provide a practical baseline for\ncomputationally efficient CL applications.",
        "The polar magnets A$_2$Mo$_3$O$_8$ (A=Fe, Mn, Co, and Ni) feature a bipartite\nstructure, where the magnetic A$^{2+}$ ions occupy two different sites with\noctahedral and tetrahedral oxygen coordinations. This bipartite structure\nprovides a platform for the emergence of nontrivial magnetoelectric (ME)\neffects and intriguing excitation behaviors, and thus creates significant\nresearch interest. In this study, we conduct inelastic neutron scattering\nmeasurements on single crystals of Mn$_2$Mo$_3$O$_8$, an L-type ferrimagnet in\nthe A$_2$Mo$_3$O$_8$ family, to investigate its spin dynamics. The obtained\nmagnetic excitation spectra reveal two distinct magnon dispersions\ncorresponding to the octahedral and tetrahedral spins in Mn$_2$Mo$_3$O$_8$.\nThese magnon bands can be well described by a spin Hamiltonian including\nHeisenberg and single-ion anisotropy terms. Employing our effective spin model,\nwe successfully reproduce the unusual temperature dependence of the L-type\nferrimagnetic susceptibility through self-consistent mean-field theory. This\nresearch reveals the significance of the bipartite structure in determining the\nexcitation properties of the polar magnets $\\rm{A_{2}Mo_{3}O_{8}}$ and provides\nvaluable insights into the spin dynamics of L-type ferrimagnets.",
        "This paper introduces an algorithm that dynamically generates basis functions\nto approximate the value function in Network Revenue Management. Unlike\nexisting algorithms sampling the parameters of new basis functions, this\nNonlinear Incremental Algorithm (NLIAlg) iteratively refines the value function\napproximation by optimizing these parameters. For larger instances, the\nTwo-Phase Incremental Algorithm (2PIAlg) modifies NLIAlg to leverage the\nefficiency of LP solvers. It reduces the size of a large-dimensional nonlinear\nproblem and transforms it into an LP by fixing the basis function parameters,\nwhich are then optimized in a second phase using the flow imbalance ideas from\nAdelman and Klabjan (2012). This marks the first application of these\ntechniques in a stochastic setting. The algorithms can operate in two modes:\n(1) Standalone mode, to construct a value function approximation from scratch,\nand (2) Add-on mode, to refine an existing approximation. Our numerical\nexperiments indicate that while NLIAlg and 2PIAlg in standalone mode are only\nfeasible for small-scale problems, the heuristic version of 2PIAlg (H-2PIAlg)\nin add-on mode, using the Affine Approximation and exponential ridge basis\nfunctions, can handle extremely large instances that may cause benchmark\nnetwork revenue management methods to run out of memory. In these scenarios,\nH-2PIAlg delivers substantially better policies and upper bounds than the\nAffine Approximation. Furthermore, H-2PIAlg achieves higher average revenues in\npolicy simulations compared to network revenue management benchmarks in\ninstances with limited capacity.",
        "For sink-free orientations in graphs of minimum degree at least $3$, we show\nthat there is a deterministic approximate counting algorithm that runs in time\n$O((n^{73}\/\\varepsilon^{72})\\log(n\/\\varepsilon))$, a near-linear time sampling\nalgorithm, and a randomised approximate counting algorithm that runs in time\n$O((n\/\\varepsilon)^2\\log(n\/\\varepsilon))$, where $n$ denotes the number of\nvertices of the input graph and $0<\\varepsilon<1$ is the desired accuracy. All\nthree algorithms are based on a local implementation of the sink popping method\n(Cohn, Pemantle, and Propp, 2002) under the partial rejection sampling\nframework (Guo, Jerrum, and Liu, 2019).",
        "Main sequence stars of spectral types F, G, and K with low to moderate\nactivity levels exhibit a recognizable pattern known as the first ionization\npotential effect (FIP effect), where elements with lower first ionization\npotentials are more abundant in the stellar corona than in the photosphere. In\ncontrast, high activity main sequence stars such as AB Dor (K0), active\nbinaries, and M dwarfs exhibit an inverse pattern known as iFIP. We aim to\ndetermine whether or not the iFIP pattern persists in moderate-activity M\ndwarfs. We used XMM-Newton to observe the moderately active M dwarf HD 223889\nthat has an X-ray surface flux of log FX,surf = 5.26, the lowest for an M dwarf\nstudied so far for coronal abundance patterns. We used low-resolution CCD\nspectra of the star to calculate the strength of the FIP effect quantified by\nthe FIP bias (Fbias) to assess the persistence of the iFIP effect in M dwarfs.\nOur findings reveal an iFIP effect similar to that of another moderately active\nbinary star, GJ 338 AB, with a comparable error margin. The results hint at a\npossible plateau in the Teff-Fbias diagram for moderately active M dwarfs.\nTargeting stars with low coronal activity that have a coronal temperature\nbetween 2 MK and 4 MK is essential for refining our understanding of (i)FIP\npatterns and their causes.",
        "Counterdiabatic (CD) driving has the potential to speed up adiabatic quantum\nstate preparation by suppressing unwanted excitations. However, existing\napproaches either require intractable classical computations or are based on\napproximations which do not have performance guarantees. We propose and analyze\na non-variational, system-agnostic CD expansion method and analytically show\nthat it converges exponentially quickly in the expansion order. In finite\nsystems, the required resources scale inversely with the spectral gap, which we\nargue is asymptotically optimal. To extend our method to the thermodynamic\nlimit and suppress errors stemming from high-frequency transitions, we leverage\nfinite-time adiabatic protocols. In particular, we show that a time determined\nby the quantum speed limit is sufficient to prepare the desired ground state,\nwithout the need to optimize the adiabatic trajectory. Numerical tests of our\nmethod on the quantum Ising chain show that our method can outperform\nstate-of-the-art variational CD approaches.",
        "Lifecycle management of power converters continues to thrive with emerging\nartificial intelligence (AI) solutions, yet AI mathematical explainability\nremains unexplored in power electronics (PE) community. The lack of theoretical\nrigor challenges adoption in mission-critical applications. Therefore, this\nletter proposes a generic framework to evaluate mathematical explainability,\nhighlighting inference stability and training convergence from a Lipschitz\ncontinuity perspective. Inference stability governs consistent outputs under\ninput perturbations, essential for robust real-time control and fault\ndiagnosis. Training convergence guarantees stable learning dynamics,\nfacilitating accurate modeling in PE contexts. Additionally, a Lipschitz-aware\nlearning rate selection strategy is introduced to accelerate convergence while\nmitigating overshoots and oscillations. The feasibility of the proposed\nLipschitz-oriented framework is demonstrated by validating the mathematical\nexplainability of a state-of-the-art physics-in-architecture neural network,\nand substantiated through empirical case studies on dual-active-bridge\nconverters. This letter serves as a clarion call for the PE community to\nembrace mathematical explainability, heralding a transformative era of\ntrustworthy and explainable AI solutions that potentially redefine the future\nof power electronics.",
        "Electromagnetically induced transparency (EIT) is an important quantum\noptical phenomenon which provides a crucial tool for light manipulation.\nHowever, typically the transparency window is broad, limited by the coherence\ntime of the metastable state. Here we show that extremely narrow transparency\nwindow can be realized using nuclear spin induced transparency (NSIT), which is\nachieved by combining optical field, magnetic field and the spin-exchange\ninteraction between noble-gas nuclear spins and alkali-metal electronic spins.\nThe width of the NSIT window can be several orders of magnitude smaller than\nthat of conventional EIT, and even reaches sub-mHz range due to the long\ncoherence time of nuclear spins. The scheme holds great potential for\napplications in slow light and magnetic field sensing.",
        "Multi-object tracking (MOT) in UAV-based video is challenging due to\nvariations in viewpoint, low resolution, and the presence of small objects.\nWhile other research on MOT dedicated to aerial videos primarily focuses on the\nacademic aspect by developing sophisticated algorithms, there is a lack of\nattention to the practical aspect of these systems. In this paper, we propose a\nnovel real-time MOT framework that integrates Apache Kafka and Apache Spark for\nefficient and fault-tolerant video stream processing, along with\nstate-of-the-art deep learning models YOLOv8\/YOLOv10 and BYTETRACK\/BoTSORT for\naccurate object detection and tracking. Our work highlights the importance of\nnot only the advanced algorithms but also the integration of these methods with\nscalable and distributed systems. By leveraging these technologies, our system\nachieves a HOTA of 48.14 and a MOTA of 43.51 on the Visdrone2019-MOT test set\nwhile maintaining a real-time processing speed of 28 FPS on a single GPU. Our\nwork demonstrates the potential of big data technologies and deep learning for\naddressing the challenges of MOT in UAV applications.",
        "We analyze simulation results from the TitanWRF global circulation model to\nunderstand the mechanisms that maintain the equatorial superrotation in Titan's\nstratosphere. We find that the eddies associated with wave activities can\ntransport angular momentum upgradient to zonal flow, leading to acceleration of\nthe equatorial superrotation. The dominant wave modes identified in this study\nare consistent with previous studies, with zonal wavenumber 1 being the major\ncontributor to the prograde acceleration. Despite the same conclusion of\nmaintenance of equatorial superrotation via wave-mean interactions, we find\nthat the way waves interact with the zonal flow in TitanWRF is slightly\ndifferent from some other studies. We confirm our previous findings that in\nTitanWRF this occurs primarily during a dozen or so annual, short-duration (a\nfew Titan sols) angular momentum \"transfer events,\" which have a repeatable\nseasonal pattern but differ slightly in timing and magnitude between years.\nThis is not the case in the Titan Atmosphere Model (TAM), which found milder\nangular momentum transfers that produced the strongest acceleration of\nsuperrotation around solstice in the upper stratosphere and more continuous\nyear-around acceleration in the lower stratosphere. Despite differences in\nangular momentum transfer across models, we further find that, similar to the\nTAM wave analysis results, eddies generated by Rossby-Kelvin instabilities may\nbe the major source of prograde angular momentum for the equatorial\nsuperrotation, although TitanWRF may also include contributions from the\nabsorption of vertically propagating equatorial Kelvin waves. This differs from\nour previous work, which suggested barotropic waves were responsible for\nTitanWRF's solsticial transfer event.",
        "In recent works we have explored a multi-photon extension of the celebrated\ntwo-photon Hong-Ou-Mandel (HOM) effect in which the quantum amplitudes for a\ntwo-photon input to a lossless, balanced 50:50 beamsplitter (BS) undergoes\ncomplete destructive interference. In the extended Hong-Ou-Mandel (eHOM) effect\nthe multi-photon scattering of photons from the two input ports to the two\noutput ports of the BS for Fock number basis input states (FS)\n$|n,m\\rangle_{12}$ exhibit complete destructive interference pairwise within\nthe quantum amplitudes containing many scattering components, generalizing the\ntwo-photon HOM effect. This has profound implications for arbitrary bipartite\nphotonic input states constructed from such basis states: if the input state to\none input port of the BS is of odd parity, i.e. constructed from only of odd\nnumbers of photons, then regardless of the input state to the second 50:50 BS\nport, there will be a central nodal line (CNL) of zeros in the joint output\nprobability distribution along the main diagonal for coincidence detection. The\nfirst goal of this present work is to show diagrammatically how the extended\nHOM effect can be seen as a succession of multi-photon HOM effects when the\nlatter is viewed as a pairwise cancellation of mirror image scattering\namplitudes. The second goal of this work is to explore considerations for the\nexperimental realization of the extended Hong-Ou-Mandel effect. We examine the\ncase of a single photon interfering with a coherent state (an idealized laser)\non a balanced 50:50 beamsplitter and consider prospects for experimental\ndetection of the output destructive interference by including additional\neffects such as imperfect detection efficiency, spatio-temporal mode functions,\nand time delay between the detected output photons.",
        "We study the high-temperature equilibrium for the C*-algebra $\\mathcal\nT(\\mathbb N^\\times \\ltimes \\mathbb Z)$ recently considered by an Huef, Laca and\nRaeburn. We show that the simplex of KMS$_\\beta$ states at each inverse\ntemperature $\\beta$ in the critical interval $(0,1]$ is a Bauer simplex whose\nspace of extreme points is homeomorphic to $\\mathbb N \\sqcup\\{\\infty\\}$. This\nis in contrast to the uniqueness of equilibrium at high temperature observed in\npreviously considered systems arising from number theory. We also establish a\nconnection between the phase transitions on quotients of our system and the\nBost-Connes phase transition.",
        "Quantum correlations, like entanglement, represent the characteristic trait\nof quantum mechanics, and pose essential issues and challenges to the\ninterpretation of this pillar of modern physics. Although quantum correlations\nare largely acknowledged as a major resource to achieve quantum advantage in\nmany tasks of quantum technologies, their full quantitative description and the\naxiomatic basis underlying them are still under investigation. Previous works\nsuggested that the origin of nonlocal correlations is grounded in principles\ncapturing (from outside the quantum formalism) the essence of quantum\nuncertainty. In particular, the recently-introduced principle of Relativistic\nIndependence gave rise to a new bound intertwining local and nonlocal\ncorrelations. Here we test such a bound by realizing together sequential and\njoint weak measurements on entangled photon pairs, allowing to simultaneously\nquantify both local and nonlocal correlations by measuring incompatible\nobservables on the same quantum system without collapsing its state, a task\ntypically forbidden in the traditional (projective) quantum measurement\nframework. Our results demonstrate the existence of a fundamental limit on the\nextent of quantum correlations, shedding light on the profound role of\nuncertainty in both enabling and balancing them."
      ]
    }
  },
  {
    "id":2411.00726,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
      ],
      "abstract":[
        "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Reproductive system and interaction with fauna in a Mediterranean\n  Pyrophite shrub",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "An Asymptotic Analysis of Bivalent Monoclonal Antibody-Antigen Binding",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Divisibility Relations Between Ring Homomorphisms and Surjective Group\n  Homomorphisms in Finite Cyclic Structures",
        "Moduli spaces of twisted maps to smooth pairs",
        "Parallel Collisionless Shocks in strongly Magnetized Electron-Ion\n  Plasma. I. Temperature anisotropies",
        "Probing Coherences and Itinerant Magnetism in a Dipolar Lattice Gas",
        "$NJ\/\\psi$ and $N\\eta_c$ interactions from lattice QCD",
        "The Ridge Integration Method and its Application to Molecular Sieving,\n  Demonstrated for Gas Purification via Graphdiyne Membranes",
        "Overcoming Quantum Metrology Singularity through Sequential Measurements",
        "Late-time growth weakly affects the significance of high-redshift\n  massive galaxies",
        "Weierstrass representations of discrete constant mean curvature surfaces\n  in isotropic space",
        "Online Optimization with Unknown Time-varying Parameters",
        "Exactness and the topology of the space of invariant random equivalence\n  relations",
        "Cusps and fundamental domains for congruence subgroups",
        "$H^\\infty$-control for a class of boundary controlled hyperbolic PDEs",
        "Accelerating Expansion of the Universe in Modified Symmetric\n  Teleparallel Gravity",
        "Neutron versus proton scattering on exotic nuclei: the $^9$He example"
      ],
      "abstract":[
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "The ULEX model, in its present state, involves the study of the biomass and\nthe population of the shrub Ulex parviflorus Pourret, but while being a dynamic\nmodel, it is static in the sense that it does not imply the appearance of new\nspecimens of this plant. As a complement to the ULEX model in its two dynamic\nand spatial aspects, and with the idea of extending the model, the authors have\nintroduced from a biological and statistical point of view four characteristics\nof this species, flowering, pollination, fructification, taking special\ninterest in the role played by the pollinators (bees) and dispersion of seeds.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Ligand-receptor interactions are fundamental to many biological processes.\nFor example in antibody-based immunotherapies, the dynamics of an antibody\nbinding with its target antigen directly influence the potency and efficacy of\nmonoclonal antibody (mAb) therapies. In this paper, we present an asymptotic\nanalysis of an ordinary differential equation (ODE) model of bivalent\nantibody-antigen binding in the context of mAb cancer therapies, highlighting\nthe added complexity associated with bivalency of the antibody. To understand\nwhat drives the complex temporal dynamics of bivalent antibody-antigen binding,\nwe construct asymptotic approximations to the model's solutions at different\ntimescales and antibody concentrations that are in good agreement with\nnumerical simulations of the full model. We show how the dynamics differ\nbetween two scenarios; a region where unbound antigens are abundant, and one\nwhere the number of unbound antigens is small such that the dominant balance\nwithin the model equations changes. Of particular importance to the potency and\nefficacy of mAb treatments are the values of quantities such as antigen\noccupancy and bound antibody number. We use the results of our asymptotic\nanalysis to approximate the long-time values of these quantities that could be\ncombined with experimental data to facilitate parameter estimation.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "In this article, we delve into the intricate relationship between the number\nof ring homomorphisms and surjective group homomorphisms between two finite\ncyclic structures, specifically $\\mathbb{Z}_m$ and $\\mathbb{Z}_n$. We\ndemonstrate that the number of ring homomorphisms from $\\mathbb{Z}_m$ to\n$\\mathbb{Z}_n$ is a divisor of the number of surjective group homomorphisms\nfrom $\\mathbb{Z}_m$ to $\\mathbb{Z}_n$, provided that $n$ is not of the form $2\n\\cdot \\alpha$, where each prime factor $p$ of $\\alpha$ satisfies $p \\equiv 3\n\\pmod{4}$.",
        "We study moduli spaces of twisted maps to a smooth pair in arbitrary genus,\nand give geometric explanations for previously known comparisons between\norbifold and logarithmic Gromov--Witten invariants. Namely, we study the space\nof twisted maps to the universal target and classify its irreducible components\nin terms of combinatorial\/tropical information. We also introduce natural\nmorphisms between these moduli spaces for different rooting parameters and\ncompute their degree on various strata. Combining this with additional\nhypotheses on the discrete data, we show these degrees are monomial of degree\nbetween $0$ and $\\max(0,2g-1)$ in the rooting parameter. We discuss the virtual\ntheory of the moduli spaces, and relate our polynomiality results to work of\nTseng and You on the higher genus orbifold Gromov--Witten invariants of smooth\npairs, recovering their results in genus $1$. We discuss what is needed to\ndeduce arbitrary genus comparison results using the previous sections. We\nconclude with some geometric examples, starting by re-framing the original\ngenus $1$ example of Maulik in this new formalism.",
        "Collisionless electron-ion shocks are fundamental to astrophysical plasmas,\nyet their behavior in strong magnetic fields remains poorly understood. Using\nParticle-in-Cell (PIC) simulations with the SHARP-1D3V code, we investigate the\nrole of the ion magnetization parameter $\\sigma_i$ in parallel shock\ntransitions. Strongly magnetized converging flows ($\\sigma_i > 1$) exhibit\nlower density compression ratios ($R \\sim 2$), smaller entropy jumps, and\nsuppressed particle acceleration, while maintaining pressure anisotropy\nstability due to conserved perpendicular temperatures across the shock,\nalongside increased parallel temperatures. In contrast, weakly magnetized\nshocks drive downstream mirror and firehose instabilities due to ion\ntemperature anisotropy, which are suppressed in strongly magnetized cases.\nAdditionally, weakly magnetized shocks exhibit the onset of a supra-thermal\npopulation induced by shock-drift acceleration, with most of the upstream\nkinetic energy thermalized for both electrons and ions in the downstream\nregion. Our results demonstrate that perpendicular temperatures for both\nspecies are conserved in strongly magnetized cases and highlight deviations\nfrom standard ideal magnetohydrodynamic (MHD) behavior. These findings provide\ncritical insights into the role of magnetic fields in parallel collisionless\nastrophysical shocks.",
        "We report on the study of itinerant magnetism of lattice-trapped magnetic\natoms, driven by magnetic dipole-dipole interactions, in the low-entropy and\nclose-to-unit filling regime. We have used advanced dynamical decoupling\ntechniques to efficiently suppress the sensitivity to magnetic field\nfluctuations. We have thus measured the spin coherence of an itinerant spin 3\nBose dipolar gas throughout a quantum phase transition from a superfluid phase\nto a Mott insulating phase. In the superfluid phase, a metastable ferromagnetic\nbehavior is observed below a dynamical instability which occurs at lattice\ndepths below the phase transition. In the insulating phase, the thermalization\ntowards a paramagnetic state is driven by an interplay between intersite and\nsuperexchange interactions.",
        "The interaction between nucleon and charmonia ($J\/\\psi$ and $\\eta_c$) is\nexpected to deepen our understanding of various aspects in nonperturbative QCD\nranging from the origin of nucleon mass to $J\/\\psi$ mass modification in\nnuclear medium and properties of hidden-charm pentaquark states. Here, we\npresent the low-energy $NJ\/\\psi$ and $N\\eta_c$ interactions based on ($2+1$)\nflavor lattice QCD simulations with nearly physical pion mass $m_\\pi=146$ MeV.\nThe interactions, extracted from the spacetime correlations of the nucleon and\ncharmonium system by using the HAL QCD method, are found to be attractive in\nall distances and manifest a characteristic long-range tail consistent with the\ntwo-pion exchange interaction. The resulting scattering lengths are around\n$0.3$ fm, $0.4$ fm and $0.2$ fm for $NJ\/\\psi$ with spin $3\/2$, with spin $1\/2$,\nand $N\\eta_c$, respectively. Our results are orders of magnitude larger than\nthose from the photoproduction experiments assuming the vector meson dominance.",
        "Eyring theory provides a convenient approximation to the rate of a chemical\nreaction, as it uses only local information evaluated near extremal points of a\ngiven potential energy surface. However, in cases of pronounced anharmonicity\nand particularly low-lying vibrational frequencies, deviations from the correct\nreaction rate can become substantial. Molecular Dynamics simulations, on the\nother hand, are very costly at higher levels of theory, and of limited use\nsince molecular reactions are `rare' events and hence statistically less\naccessible. In this article, we present an alternative description for problems\nof gas separation and storage via two-dimensional materials such as porous\ngraphene or flat metal-organic frameworks. Taking geometric advantage of the\ntypical problem setting, our method is based on a statistical analysis of\nmolecular trajectories near the so-called `ridge', a hypersurface which divides\nthe reaction volume into a reactant and a product side. It allows for more\nrealistic predictions of permeabilities and selectivities, e.g. derived from\ndensity functional theory, but without the considerable costs of a full\nmolecular dynamics simulation on the corresponding Born-Oppenheimer potential\nenergy surface. We test our method on the example of methane separation from\nnitrogen and carbon dioxide via a graphdiyne membrane.",
        "The simultaneous estimation of multiple unknown parameters is the most\ngeneral scenario in quantum sensing. Quantum multi-parameter estimation theory\nprovides fundamental bounds on the achievable precision of simultaneous\nestimation. However, these bounds can become singular (no finite bound exists)\nin multi-parameter sensing due to parameter interdependencies, limited probe\naccessibility, and insufficient measurement outcomes. Here, we address the\nsingularity issue in quantum sensing through a simple mechanism based on a\nsequential measurement strategy. This sensing scheme overcomes the singularity\nconstraint and enables the simultaneous estimation of multiple parameters with\na local and fixed measurement throughout the sensing protocol. This is because\nsequential measurements, involving consecutive steps of local measurements\nfollowed by probe evolution, inherently produce correlated measurement data\nthat grows exponentially with the number of sequential measurements. Finally,\nthrough two different examples, namely a strongly correlated probe and a\nlight-matter system, we demonstrate how such singularities are reflected when\ninferring the unknown parameters through Bayesian estimation.",
        "Recent observations by the James Webb Space Telescope have revealed massive\ngalaxies at very high redshift ($z\\simeq 7-15$). The question of whether the\nexistence of such galaxies is expected in the corresponding JWST surveys has\nreceived a lot of attention, though the answer straddles areas of cosmology and\ncomplex astrophysical details of high-redshift galaxy formation. The growth\nrate of density fluctuations determines the amplitude of overdensities that\ncollapse to form galaxies. Late-time modifications of growth, combined with\nmeasurements at both $z\\sim 1$ from large-scale structure and $z\\sim 1000$ from\nthe cosmic microwave background, affect the predictions for the abundance of\nfirst galaxies in the universe. In this paper, we point out that the late-time\ngrowth rate of structure affects the statistical significance of high-redshift,\nhigh-mass objects very weakly. Consequently, if the existence and abundance of\nthese objects are confirmed to be unexpected, the variations in the late-time\ngrowth history are unlikely to explain these anomalies.",
        "In this paper, we obtain Weierstrass representations for discrete constant\nmean curvature surfaces in isotropic 3-space, and use this to construct\nexamples with discrete closed-form parametrizations.",
        "In this paper, we study optimization problems where the cost function\ncontains time-varying parameters that are unmeasurable and evolve according to\nlinear, yet unknown, dynamics. We propose a solution that leverages control\ntheoretic tools to identify the dynamics of the parameters, predict their\nevolution, and ultimately compute a solution to the optimization problem. The\nidentification of the dynamics of the time-varying parameters is done online\nusing measurements of the gradient of the cost function. This system\nidentification problem is not standard, since the output matrix is known and\nthe dynamics of the parameters must be estimated in the original coordinates\nwithout similarity transformations. Interestingly, our analysis shows that,\nunder mild conditions that we characterize, the identification of the\nparameters dynamics and, consequently, the computation of a time-varying\nsolution to the optimization problem, requires only a finite number of\nmeasurements of the gradient of the cost function. We illustrate the\neffectiveness of our algorithm on a series of numerical examples.",
        "We characterize exactness of a countable group $\\Gamma$ in terms of invariant\nrandom equivalence relations (IREs) on $\\Gamma$. Specifically, we show that\n$\\Gamma$ is exact if and only if every weak limit of finite IREs is an amenable\nIRE. In particular, for exact groups this implies amenability of the restricted\nrerooting relation associated to the ideal Bernoulli Voronoi tessellation, the\ndiscrete analog of the ideal Poisson Voronoi tesselation.",
        "We characterize the cusp classes and their widths for the congruence\nsubgroups $\\Gamma(N), \\Gamma_1(N)$ and $\\Gamma_0(N)$. We relate the cusp\nclasses of $\\Gamma_0(N)$ with those produced by the connected fundamental\ndomain in the previous work of Nie and Parent. By further studying the\ninteresting functions $M$ and $W$ on ${\\mathbb Z}\/N$, we establish an identity\nrelating the widths.",
        "A solution to the suboptimal $H^\\infty$-control problem is given for a class\nof hyperbolic partial differential equations (PDEs). The first result of this\nmanuscript shows that the considered class of PDEs admits an equivalent\nrepresentation as an infinite-dimensional discrete-time system. Taking\nadvantage of this, this manuscript shows that it is equivalent to solve the\nsuboptimal $H^\\infty$-control problem for a finite-dimensional discrete-time\nsystem whose matrices are derived from the PDEs. After computing the solution\nto this much simpler problem, the solution to the original problem can be\ndeduced easily. In particular, the optimal compensator solution to the\nsuboptimal $H^\\infty$-control problem is governed by a set of hyperbolic PDEs,\nactuated and observed at the boundary. We illustrate our results with a\nboundary controlled and boundary observed vibrating string.",
        "In the last century, theoretical and experimental developments have\nestablished the General Relativity theory as the most successful theory for\ndescribing the gravitational phenomenon. On the other hand, in the last two\ndecades, multiple observational probes have strongly favored the discovery of\nthe acceleration of cosmic expansion. The observational enhancement and\ndevelopment in precision cosmology indicate a requirement to go beyond General\nRelativity and to search for an alternate description that can resolve the\npersistent issues. In Chapter 1, we highlight some important elements of\nobservational cosmology. In Chapters 2 and 3, we investigate the f(Q) gravity\nin the presence of viscosity in the cosmic fluid. In Chapters 4 and 5, we\nexplore the constraints on the various classes of non-linear f(Q) gravity\nmodels in both coincident and non-coincident formalism, respectively. In\nChapter 6, we present a covariant formulation and energy balance equation for\nthe f(Q,T) gravity, which is an extension of f(Q) gravity. Finally, in Chapter\n7, we briefly summarize the outcomes of the present thesis and the future\nscope.",
        "Neutron scattering on exotic nuclides is a class of processes which can not\nbe studied directly now and in any observable future. Resonance proton\nscattering of exotic nuclide on a thick target in inverse kinematics can be\nused to infer the properties of the low-energy neutron scattering of this\nnuclide assuming the isobaric symmetry. However, the results of such resonance\nproton scattering reactions are so far analyzed in theoretical approaches\n(optical, R-matrix models), which are missing important aspects of isospin\ndynamics, isospin violation in continuum and threshold dynamics. The isospin\nconserving coupled-channel model (ICM) is proposed, which provides a more\nreliable basis for understanding of such experimental studies. Qualitatively\ndifferent phase shifts for the $^{8}$He+$p$ $T=5\/2$ and $T=3\/2$ resonances are\npredicted by ICM with quite unusual profile for the $T=5\/2$ states. Alternative\ninterpretation of the existing $^{8}$He+$p$ data is proposed. The observable\nproperties of the $T=5\/2$ resonances may be strongly affected by the\nisobaric-partner $T=3\/2$ states. Crucial importance of studies of the\nneutron-emission channel for disentangling this possible influence is\ndemonstrated."
      ]
    }
  },
  {
    "id":2411.05236,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel",
    "start_abstract":"Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2",
        "b0"
      ],
      "title":[
        "Shannon capacity of signal transduction for multiple independent receptors",
        "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
      ],
      "abstract":[
        "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
        "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
      ],
      "categories":[
        "cs.SY",
        "eess.SP"
      ]
    },
    "list":{
      "title":[
        "Learning Linear Block Codes with Gradient Quantization",
        "SWIPTNet: A Unified Deep Learning Framework for SWIPT based on GNN and\n  Transfer Learning",
        "A Block-Sparse Bayesian Learning Algorithm with Dictionary Parameter\n  Estimation for Multi-Sensor Data Fusion",
        "Sparse Incremental Aggregation in Satellite Federated Learning",
        "Power-Efficient Optimization for Coexisting Semantic and Bit-Based Users\n  in NOMA Networks",
        "Moment-based Characterization of Spatially Distributed Sources in SAR\n  Tomography",
        "A New Interpretation of the Time-Interleaved ADC Mismatch Problem: A\n  Tracking-Based Hybrid Calibration Approach",
        "Decentralized Learning with Approximate Finite-Time Consensus",
        "Generalized Spatial Modulation Aided Affine Frequency Division\n  Multiplexing",
        "High-Resolution Range-Doppler Imaging from One-Bit PMCW Radar via\n  Generative Adversarial Networks",
        "An Approach of Directly Tracking Multiple Objects",
        "Weighted-Sum Energy Efficiency Maximization in User-Centric Uplink\n  Cell-Free Massive MIMO",
        "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models\n  via Synthetic Embeddings",
        "Separate surface and bulk topological Anderson localization transitions\n  in disordered axion insulators",
        "Harmonic And Transposition Constraints Arising From The Use Of The\n  Roland TR-808 Bass Drum",
        "Quantifying Point Contributions: A Lightweight Framework for Efficient\n  and Effective Query-Driven Trajectory Simplification",
        "High-Energy Neutrinos by Hydrogen-rich Supernovae interacting with\n  low-massive Circumstellar Medium: The Case of SN 2023ixf",
        "A Hybrid Model\/Data-Driven Solution to Channel, Position and Orientation\n  Tracking in mmWave Vehicular Systems",
        "Exploring the Potential of QEEGNet for Cross-Task and Cross-Dataset\n  Electroencephalography Encoding with Quantum Machine Learning",
        "Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic\n  Data Generation and Fairness Algorithms",
        "Continually Learning Structured Visual Representations via Network\n  Refinement with Rerelation",
        "Dango: A Mixed-Initiative Data Wrangling System using Large Language\n  Model",
        "6GStarLab -- A CubeSat Mission to support the development and\n  standardization of Non-Terrestrial Networks towards 6G",
        "Hints of Primordial Magnetic Fields at Recombination and Implications\n  for the Hubble Tension",
        "EMK-KEN: A High-Performance Approach for Assessing Knowledge Value in\n  Citation Network",
        "CEReBrO: Compact Encoder for Representations of Brain Oscillations Using\n  Efficient Alternating Attention",
        "Emergent Symbolic Mechanisms Support Abstract Reasoning in Large\n  Language Models"
      ],
      "abstract":[
        "This study investigates the problem of learning linear block codes optimized\nfor Belief-Propagation decoders significantly improving performance compared to\nthe state-of-the-art. Our previous research is extended with an enhanced system\ndesign that facilitates a more effective learning process for the parity check\nmatrix. We simplify the input dataset, restrict the number of parameters to\nlearn and improve the gradient back-propagation within the model. We also\nintroduce novel optimizers specifically designed for discrete-valued weights.\nBased on conventional gradient computation, these optimizers provide discrete\nweights updates, enabling finer control and improving explainability of the\nlearning process. Through these changes, we consistently achieve improved code\nperformance, provided appropriately chosen hyper-parameters. To rigorously\nevaluate the performance of learned codes in the context of short to medium\nblock lengths, we propose a comprehensive code performance assessment\nframework. This framework enables a fair comparison between our learning\nmethodology and random search approaches, ensuring statistical significance in\nour results. The proposed model pave the way for a new approach to the\nefficient learning of linear block codes tailored to specific decoder\nstructures.",
        "This paper investigates the deep learning based approaches for simultaneous\nwireless information and power transfer (SWIPT). The quality-of-service (QoS)\nconstrained sum-rate maximization problems are, respectively, formulated for\npower-splitting (PS) receivers and time-switching (TS) receivers and solved by\na unified graph neural network (GNN) based model termed SWIPT net (SWIPTNet).\nTo improve the performance of SWIPTNet, we first propose a single-type output\nmethod to reduce the learning complexity and facilitate the satisfaction of QoS\nconstraints, and then, utilize the Laplace transform to enhance input features\nwith the structural information. Besides, we adopt the multi-head attention and\nlayer connection to enhance feature extracting. Furthermore, we present the\nimplementation of transfer learning to the SWIPTNet between PS and TS\nreceivers. Ablation studies show the effectiveness of key components in the\nSWIPTNet. Numerical results also demonstrate the capability of SWIPTNet in\nachieving near-optimal performance with millisecond-level inference speed which\nis much faster than the traditional optimization algorithms. We also show the\neffectiveness of transfer learning via fast convergence and expressive\ncapability improvement.",
        "We propose an sparse Bayesian learning (SBL)-based method that leverages\ngroup sparsity and multiple parameterized dictionaries to detect the relevant\ndictionary entries and estimate their continuous parameters by combining data\nfrom multiple independent sensors. In a MIMO multi-radar setup, we demonstrate\nits effectiveness in jointly detecting and localizing multiple objects, while\nalso emphasizing its broader applicability to various signal processing tasks.\nA key benefit of the proposed SBL-based method is its ability to resolve\ncorrelated dictionary entries-such as closely spaced objects-resulting in\nuncorrelated estimates that improve subsequent estimation stages. Through\nnumerical simulations, we show that our method outperforms the newtonized\northogonal matching pursuit (NOMP) algorithm when two objects cross paths using\na single radar. Furthermore, we illustrate how fusing measurements from\nmultiple independent radars leads to enhanced detection and localization\nperformance",
        "This paper studies Federated Learning (FL) in low Earth orbit (LEO) satellite\nconstellations, where satellites are connected via intra-orbit inter-satellite\nlinks (ISLs) to their neighboring satellites. During the FL training process,\nsatellites in each orbit forward gradients from nearby satellites, which are\neventually transferred to the parameter server (PS). To enhance the efficiency\nof the FL training process, satellites apply in-network aggregation, referred\nto as incremental aggregation. In this work, the gradient sparsification\nmethods from [1] are applied to satellite scenarios to improve bandwidth\nefficiency during incremental aggregation. The numerical results highlight an\nincrease of over 4 x in bandwidth efficiency as the number of satellites in the\norbital plane increases.",
        "Semantic communication focuses on transmitting the meaning of data, aiming\nfor efficient, relevant communication, while non-orthogonal multiple access\n(NOMA) enhances spectral efficiency by allowing multiple users to share the\nsame spectrum. Integrating semantic users into a NOMA network with bit-based\nusers improves both transmission and spectrum efficiency. However, the\nperformance metric for semantic communication differs significantly from that\nof traditional communication, posing challenges in simultaneously meeting\nindividual user demands and minimizing transmission power, especially in\nscenarios with coexisting semantic and bit-based users. Furthermore, the\ndifferent hardware architectures of semantic and bit-based users complicate the\nimplementation of successive interference cancellation (SIC). To address these\nchallenges, in this paper, we propose a clustered framework to mitigate the\ncomplexity of SIC and two multiple access (MA) schemes, e.g., pure\ncluster-based NOMA (P-CNOMA) and hybrid cluster-based NOMA (H-CNOMA), to\nminimize the total transmission power. The P-CNOMA scheme can achieve the\nminimum transmission power, but may not satisfy the high quality of service\n(QoS) requirement. In contrast, H-CNOMA addresses these issues with a slight\nincrease in power and a reduced semantic rate. These two schemes complement\neach other, enabling an adaptive MA selection mechanism that adapts to specific\nnetwork conditions and user requirements.",
        "This paper presents a non-parametric method for 3-D imaging of natural\nvolumes using Synthetic Aperture Radar tomography. This array processing-based\ntechnique aims at characterizing a spatially distributed density of incoherent\nsources, whose shape is imprecisely known. The proposed technique estimates the\nmoments of the reflectivity density using a low-complexity covariance matching\napproach, and retrieves the mean location, dispersion, and power of the\ndistributed source. Numerical simulations of realistic tomographic scenarios\nshow that the proposed model-free scheme achieves better accuracy than slightly\nmisspecified maximum likelihood estimators, derived from approximately known\ndistribution shapes.",
        "Time-interleaved ADCs (TI-ADCs) achieve high sampling rates by interleaving\nmultiple sub-ADCs in parallel. Mismatch errors between the sub-ADCs, however,\ncan significantly degrade the signal quality, which is a main performance\nbottleneck. This paper presents a hybrid calibration approach by interpreting\nthe mismatch problem as a tracking problem, and uses the extended Kalman filter\nfor online estimation and compensation of the mismatch errors. After\nestimation, the desired signal is reconstructed using a truncated fractional\ndelay filter and a high-pass filter. Simulations demonstrate that our algorithm\nsubstantially outperforms the existing hybrid calibration method in both\nmismatch estimation and compensation.",
        "The performance of algorithms for decentralized optimization is affected by\nboth the optimization error and the consensus error, the latter of which arises\nfrom the variation between agents' local models. Classically, algorithms employ\naveraging and gradient-tracking mechanisms with constant combination matrices\nto drive the collection of agents to consensus. Recent works have demonstrated\nthat using sequences of combination matrices that achieve finite-time consensus\n(FTC) can result in improved communication efficiency or iteration complexity\nfor decentralized optimization. Notably, these studies apply to highly\nstructured networks, where exact finite-time consensus sequences are known\nexactly and in closed form. In this work we investigate the impact of utilizing\napproximate FTC matrices in decentralized learning algorithms, and quantify the\nimpact of the approximation error on convergence rate and steady-state\nperformance. Approximate FTC matrices can be inferred for general graphs and do\nnot rely on a particular graph structure or prior knowledge, making the\nproposed scheme applicable to a broad range of decentralized learning settings.",
        "Generalized spatial modulation-aided affine frequency division multiplexing\n(GSM-AFDM) is conceived for reliable multiple-input multiple-output (MIMO)\ncommunications over doubly selective channels. We commence by proposing several\nlow-complexity detectors for large-scale GSM-AFDM systems. Specifically, we\nintroduce the linear minimum mean square error (LMMSE) equalizer-based maximum\nlikelihood detector (LMMSE-MLD). By exploiting the GSM properties, we then\nderive the LMMSE-based transmit-antenna activation pattern (TAP) check-based\nlog-likelihood ratio detector (LMMSE-TC-LLRD). In addition, we propose a pair\nof new detectors, namely the greedy residual check detector (GRCD) and the\nreduced space check detector (RSCD). We also derive a bit error rate (BER)\nupper-bound by considering the MLD. Our simulation results demonstrate that 1)\nthe BER upper bound derived is tight for moderate to high signal-to-noise\nratios (SNRs), 2) the proposed GSM-AFDM achieves lower BER than its\nconventional counterparts, and 3) the conceived detectors strike a compelling\ntrade-off between the BER and complexity.",
        "Digital modulation schemes such as PMCW have recently attracted increasing\nattention as possible replacements for FMCW modulation in future automotive\nradar systems. A significant obstacle to their widespread adoption is the\nexpensive and power-consuming ADC required at gigahertz frequencies. To\nmitigate these challenges, employing low-resolution ADC, such as one-bit, has\nbeen suggested. Nonetheless, using one-bit sampling results in the loss of\nessential information. This study explores two RD imaging methods in PMCW radar\nsystems utilizing NN. The first method merges standard RD signal processing\nwith a GAN, whereas the second method uses an E2E strategy in which traditional\nsignal processing is substituted with an NN-based RD module. The findings\nindicate that these methods can substantially improve the probability of\ndetecting targets in the range-Doppler domain.",
        "In conventional approaches for multiobject tracking (MOT), raw sensor data\nundergoes several preprocessing stages to reduce data rate and computational\ncomplexity. This typically includes coherent processing that aims at maximizing\nthe signal-to-noise ratio (SNR), followed by a detector that extracts \"point\"\nmeasurements, e.g., the range and bearing of objects, which serve as inputs for\nsequential Bayesian MOT. While using point measurements significantly\nsimplifies the statistical model, the reduced data rate can lead to a loss of\ncritical, object-related information and, thus, potentially to reduced tracking\nperformance. In this paper, we propose a direct tracking approach that avoids a\ndetector and most preprocessing stages. For direct tracking, we introduce a\nmeasurement model for the data-generating process of the sensor data, along\nwith state-transition and birth models for the dynamics and the appearance and\ndisappearance of objects. Based on the new statistical model, we develop a\nfactor graph and particle-based belief propagation (BP) method for efficient\nsequential Bayesian estimation. Contrary to the track-before-detect (TBD)\nparadigm which also avoids a detector, direct tracking integrates coherent\nprocessing within the Bayesian MOT framework. Numerical experiments based on a\npassive acoustic dataset demonstrate that the proposed direct approach\noutperforms state-of-the-art conventional methods that rely on multiple\npreprocessing stages.",
        "This paper introduces the weighted-sum energy efficiency (WSEE) as an\nadvanced performance metric designed to represent the uplink energy efficiency\n(EE) of individual user equipment (UE) in a user-centric Cell-Free massive MIMO\n(CF-mMIMO) system more accurately. In this realistic user-centric CF-mMIMO\ncontext, each UE may exhibit distinct characteristics, such as maximum transmit\npower limits or specific minimum data rate requirements. By computing the EE of\neach UE independently and adjusting the weights accordingly, the system can\naccommodate these unique attributes, thus promoting energy-efficient operation.\nThe uplink WSEE is formulated as a multiple-ratio fractional programming (FP)\nproblem, representing a weighted sum of the EE of individual UEs, which depends\non each UE's transmit power and the combining vector at the CPU. To effectively\nmaximize WSEE, we present optimization algorithms that utilize the Dinkelbach\ntransform and the quadratic transform (QT). Applying the QT twice consecutively\nyields significant performance gains in terms of WSEE. This framework\nestablishes a foundation for developing operational strategies tailored to\nspecific system requirements.",
        "Multimodal Large Language Models (MLLMs) have serious security\nvulnerabilities.While safety alignment using multimodal datasets consisting of\ntext and data of additional modalities can effectively enhance MLLM's security,\nit is costly to construct these datasets. Existing low-resource security\nalignment methods, including textual alignment, have been found to struggle\nwith the security risks posed by additional modalities. To address this, we\npropose Synthetic Embedding augmented safety Alignment (SEA), which optimizes\nembeddings of additional modality through gradient updates to expand textual\ndatasets. This enables multimodal safety alignment training even when only\ntextual data is available. Extensive experiments on image, video, and\naudio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding\non a single RTX3090 GPU within 24 seconds. SEA significantly improves the\nsecurity of MLLMs when faced with threats from additional modalities. To assess\nthe security risks introduced by video and audio, we also introduced a new\nbenchmark called VA-SafetyBench. High attack success rates across multiple\nMLLMs validate its challenge. Our code and data will be available at\nhttps:\/\/github.com\/ZeroNLP\/SEA.",
        "In topological phases of matter for which the bulk and boundary support\ndistinct electronic gaps, there exists the possibility of decoupled mobility\ngaps in the presence of disorder. This is in analogy with the well-studied\nproblem of realizing separate or concomitant bulk-boundary criticality in\nconventional Landau theory. Using a three-dimensional axion insulator having\nclean, gapped surfaces with $e^2\/2h$ quantized Hall conductance, we show the\nbulk and surface mobility gap evolve differently in the presence of disorder.\nThe decoupling of the bulk and surface topology yields a regime that realizes a\ntwo-dimensional, unquantized anomalous Hall metal in the Gaussian unitary\nensemble (GUE), which shares some spectral and response properties akin to the\nsurface states of a conventional three-dimensional (3D) topological insulator.\nThe generality of these results as well as extensions to other insulators and\nsuperconductors is discussed.",
        "The study investigates hip-hop music producer Scott Storch's approach to\ntonality, where the song's key is transposed to fit the Roland TR-808 bass drum\ninstead of tuning the drums to the song's key. This process, involving the\nadjustment of all tracks except the bass drum, suggests significant production\nmotives. The primary constraint stems from the limited usable pitch range of\nthe TR-808 bass drum if its characteristic sound is to be preserved. The\nresearch examines drum tuning practices, the role of the Roland TR-808 in\nmusic, and the sub-bass qualities of its bass drum. Analysis of TR-808 samples\nreveals their characteristics and their integration into modern genres like\ntrap and hip-hop. The study also considers the impact of loudspeaker frequency\nresponse and human ear sensitivity on bass drum perception. The findings\nsuggest that Storch's method prioritizes the spectral properties of the bass\ndrum over traditional pitch values to enhance the bass response. The need to\nmaintain the unique sound of the TR-808 bass drum underscores the importance of\nspectral formants and register in contemporary popular music production.",
        "As large volumes of trajectory data accumulate, simplifying trajectories to\nreduce storage and querying costs is increasingly studied. Existing proposals\nface three main problems. First, they require numerous iterations to decide\nwhich GPS points to delete. Second, they focus only on the relationships\nbetween neighboring points (local information) while neglecting the overall\nstructure (global information), reducing the global similarity between the\nsimplified and original trajectories and making it difficult to maintain\nconsistency in query results, especially for similarity-based queries. Finally,\nthey fail to differentiate the importance of points with similar features,\nleading to suboptimal selection of points to retain the original trajectory\ninformation.\n  We propose MLSimp, a novel Mutual Learning query-driven trajectory\nsimplification framework that integrates two distinct models: GNN-TS, based on\ngraph neural networks, and Diff-TS, based on diffusion models. GNN-TS evaluates\nthe importance of a point according to its globality, capturing its correlation\nwith the entire trajectory, and its uniqueness, capturing its differences from\nneighboring points. It also incorporates attention mechanisms in the GNN\nlayers, enabling simultaneous data integration from all points within the same\ntrajectory and refining representations, thus avoiding iterative processes.\nDiff-TS generates amplified signals to enable the retention of the most\nimportant points at low compression rates. Experiments involving eight\nbaselines on three databases show that MLSimp reduces the simplification time\nby 42%--70% and improves query accuracy over simplified trajectories by up to\n34.6%.",
        "In hydrogen-rich (H-rich) Supernova (SN) events, the collision between the\nH-rich ejecta and the Circum-Stellar Medium (CSM) can accelerate particles and\nproduce high-energy neutrinos (HE-$\\nu$, TeV-PeV) through proton-proton\ninelastic scattering. Despite understanding the production mechanism of these\nneutrinos, the lack of direct observations raises questions about particle\nacceleration efficiency and the involved astrophysical conditions. This study\nfocuses on neutrino emission from H-rich SNe with low-mass CSM, such as SN\n2023ixf. We developed a semi-analytical model to characterize the progenitor\nand CSM at the explosion time, allowing us to infer the expected neutrino flux\nat Earth during the SN's interaction phase. Our model shows that neutrino\nemission depends not only on shock velocity and CSM mass but also on the\nspatial matter distribution of the CSM. By analysing the bolometric light curve\nof SN 2023ixf beyond 100 days post-explosion, we find that its ejecta,\nconsisting of $9\\,\\text{M}_{\\rm \\odot}$ (including $0.07\\,\\text{M}_{\\rm \\odot}$\nof radioactive $^{56}$Ni) and having a kinetic energy of $1.8\\,\\text{foe}$,\ncollides with a low-mass CSM of $0.06\\,\\text{M}_{\\rm \\odot}$ distributed\naccording to a power-law density profile with an exponent of $s=2.9$. Through\nthese parameters, we estimate that up to $4\\pm1\\times 10^{-2}$ muon\n(anti-)neutrino events could be detected by IceCube within 50 days\npost-explosion. Although the predicted flux ($\\lesssim 3\\times\n10^{-9}\\,\\text{GeV} \\, \\text{cm}^{-2} \\, \\text{s}^{-1}$) is below current\nIceCube sensitivity, future telescopes like IceCube-Gen2 and KM3NeT could\ndetect HE-$\\nu$ from similar SN events.",
        "Channel tracking in millimeter wave (mmWave) vehicular systems is crucial for\nmaintaining robust vehicle-to-infrastructure (V2I) communication links, which\ncan be leveraged to achieve high accuracy vehicle position and orientation\ntracking as a byproduct of communication. While prior work tends to simplify\nthe system model by omitting critical system factors such as clock offsets,\nfiltering effects, antenna array orientation offsets, and channel estimation\nerrors, we address the challenges of a practical mmWave multiple-input\nmultiple-output (MIMO) communication system between a single base station (BS)\nand a vehicle while tracking the vehicle's position and orientation (PO)\nconsidering realistic driving behaviors. We first develop a channel tracking\nalgorithm based on multidimensional orthogonal matching pursuit (MOMP) with\nfactoring (F-MOMP) to reduce computational complexity and enable\nhigh-resolution channel estimates during the tracking stage, suitable for PO\nestimation. Then, we develop a network called VO-ChAT (Vehicle\nOrientation-Channel Attention for orientation Tracking), which processes the\nchannel estimate sequence for orientation prediction. Afterward, a weighted\nleast squares (WLS) problem that exploits the channel geometry is formulated to\ncreate an initial estimate of the vehicle's 2D position. A second network named\nVP-ChAT (Vehicle Position-Channel Attention for position Tracking) refines the\ngeometric position estimate. VP-ChAT is a Transformer inspired network\nprocessing the historical channel and position estimates to provide the\ncorrection for the initial geometric position estimate. The proposed solution\nis evaluated using raytracing generated channels in an urban canyon\nenvironment. For 80% of the cases it achieves a 2D position tracking accuracy\nof 26 cm while orientation errors are kept below 0.5 degree.",
        "Electroencephalography (EEG) is widely used in neuroscience and clinical\nresearch for analyzing brain activity. While deep learning models such as\nEEGNet have shown success in decoding EEG signals, they often struggle with\ndata complexity, inter-subject variability, and noise robustness. Recent\nadvancements in quantum machine learning (QML) offer new opportunities to\nenhance EEG analysis by leveraging quantum computing's unique properties. In\nthis study, we extend the previously proposed Quantum-EEGNet (QEEGNet), a\nhybrid neural network incorporating quantum layers into EEGNet, to investigate\nits generalization ability across multiple EEG datasets. Our evaluation spans a\ndiverse set of cognitive and motor task datasets, assessing QEEGNet's\nperformance in different learning scenarios. Experimental results reveal that\nwhile QEEGNet demonstrates competitive performance and maintains robustness in\ncertain datasets, its improvements over traditional deep learning methods\nremain inconsistent. These findings suggest that hybrid quantum-classical\narchitectures require further optimization to fully leverage quantum advantages\nin EEG processing. Despite these limitations, our study provides new insights\ninto the applicability of QML in EEG research and highlights challenges that\nmust be addressed for future advancements.",
        "The increasing use of machine learning in learning analytics (LA) has raised\nsignificant concerns around algorithmic fairness and privacy. Synthetic data\nhas emerged as a dual-purpose tool, enhancing privacy and improving fairness in\nLA models. However, prior research suggests an inverse relationship between\nfairness and privacy, making it challenging to optimize both. This study\ninvestigates which synthetic data generators can best balance privacy and\nfairness, and whether pre-processing fairness algorithms, typically applied to\nreal datasets, are effective on synthetic data. Our results highlight that the\nDEbiasing CAusal Fairness (DECAF) algorithm achieves the best balance between\nprivacy and fairness. However, DECAF suffers in utility, as reflected in its\npredictive accuracy. Notably, we found that applying pre-processing fairness\nalgorithms to synthetic data improves fairness even more than when applied to\nreal data. These findings suggest that combining synthetic data generation with\nfairness pre-processing offers a promising approach to creating fairer LA\nmodels.",
        "Current machine learning paradigm relies on continuous representations like\nneural networks, which iteratively adjust parameters to approximate outcomes\nrather than directly learning the structure of problem. This spreads\ninformation across the network, causing issues like information loss and\nincomprehensibility Building on prior work in environment dynamics modeling, we\npropose a method that learns visual space in a structured, continual manner.\nOur approach refines networks to capture the core structure of objects while\nrepresenting significant subvariants in structure efficiently. We demonstrate\nthis with 2D shape detection, showing incremental learning on MNIST without\noverwriting knowledge and creating compact, comprehensible representations.\nThese results offer a promising step toward a transparent, continually learning\nalternative to traditional neural networks for visual processing.",
        "Data wrangling is a time-consuming and challenging task in a data science\npipeline. While many tools have been proposed to automate or facilitate data\nwrangling, they often misinterpret user intent, especially in complex tasks. We\npropose Dango, a mixed-initiative multi-agent system for data wrangling.\nCompared to existing tools, Dango enhances user communication of intent by\nallowing users to demonstrate on multiple tables and use natural language\nprompts in a conversation interface, enabling users to clarify their intent by\nanswering LLM-posed multiple-choice clarification questions, and providing\nmultiple forms of feedback such as step-by-step natural language explanations\nand data provenance to help users evaluate the data wrangling scripts. We\nconducted a within-subjects user study with 38 participants and demonstrated\nthat Dango's features can significantly improve intent clarification, accuracy,\nand efficiency in data wrangling. Furthermore, we demonstrated the\ngeneralizability of Dango by applying it to a broader set of data wrangling\ntasks.",
        "The emergence of the Non-Terrestrial Network (NTN) concept in the last years\nhas revolutionized the space industry. This novel network architecture composed\nof aircraft and spacecraft is currently being standardized by the 3GPP. This\nstandardization process follows dedicated phases in which experimentation of\nthe technology is needed. Although some missions have been conducted to\ndemonstrate specific and service-centric technologies, a open flexible in-orbit\ninfrastructure is demanded to support this standardization process. This work\npresents the 6GStarLab mission, which aims to address this gap. Specifically,\nthis mission envisions to provide a 6U CubeSat as the main in-orbit\ninfrastructure in which multiple technology validations can be uploaded. The\nconcept of this mission is depicted. Additionally, this work presents the\ndetails of the satellite platform and the payload. This last one is designed to\nenable the experimentation in multiple radio-frequency bands (i.e. UHF, S-, X-,\nand Ka-bands) and an optical terminal. The launch of the satellite is scheduled\nfor Q2 2025, and it will contribute to the standardization of future NTN\narchitectures.",
        "Primordial Magnetic Fields (PMFs), long studied as potential relics of the\nearly Universe, accelerate the recombination process and have been proposed as\na possible way to relieve the Hubble tension. However, previous studies relied\non simplified toy models. In this study, for the first time, we use the recent\nhigh-precision evaluations of recombination with PMFs, incorporating full\nmagnetohydrodynamic (MHD) simulations and detailed Lyman-alpha radiative\ntransfer, to test PMF-enhanced recombination ($b\\Lambda$CDM) against\nobservational data from the cosmic microwave background (CMB), baryon acoustic\noscillations (BAO), and Type Ia supernovae (SN). Focusing on non-helical PMFs\nwith a Batchelor spectrum, we find a preference for present-day total field\nstrengths of approximately 5-10 pico-Gauss. Depending on the dataset\ncombination, this preference ranges from mild ($\\sim 1.8\\sigma$ with Planck +\nDESI) to moderate ($\\sim 3\\sigma$ with Planck + DESI + SH0ES-calibrated SN)\nsignificance. The $b\\Lambda$CDM has Planck + DESI $\\chi^2$ values equal or\nbetter than those of the $\\Lambda$CDM model while predicting a higher Hubble\nconstant. The favored field strengths align closely with those required for\ncluster magnetic fields to originate entirely from primordial sources, without\nthe need for additional dynamo amplification or stellar magnetic field\ncontamination. Future high-resolution CMB temperature and polarization\nmeasurements will be crucial for confirming or further constraining the\npresence of PMFs at recombination.",
        "With the explosive growth of academic literature, effectively evaluating the\nknowledge value of literature has become quite essential. However, most of the\nexisting methods focus on modeling the entire citation network, which is\nstructurally complex and often suffers from long sequence dependencies when\ndealing with text embeddings. Thus, they might have low efficiency and poor\nrobustness in different fields. To address these issues, a novel knowledge\nevaluation method is proposed, called EMK-KEN. The model consists of two\nmodules. Specifically, the first module utilizes MetaFP and Mamba to capture\nsemantic features of node metadata and text embeddings to learn contextual\nrepresentations of each paper. The second module utilizes KAN to further\ncapture the structural information of citation networks in order to learn the\ndifferences in different fields of networks. Extensive experiments based on ten\nbenchmark datasets show that our method outperforms the state-of-the-art\ncompetitors in effectiveness and robustness.",
        "Electroencephalograph (EEG) is a crucial tool for studying brain activity.\nRecently, self-supervised learning methods leveraging large unlabeled datasets\nhave emerged as a potential solution to the scarcity of widely available\nannotated EEG data. However, current methods suffer from at least one of the\nfollowing limitations: i) sub-optimal EEG signal modeling, ii) model sizes in\nthe hundreds of millions of trainable parameters, and iii) reliance on private\ndatasets and\/or inconsistent public benchmarks, hindering reproducibility. To\naddress these challenges, we introduce a Compact Encoder for Representations of\nBrain Oscillations using alternating attention (CEReBrO), a new small EEG\nfoundation model. Our tokenization scheme represents EEG signals at a\nper-channel patch granularity. We propose an alternating attention mechanism\nthat jointly models intra-channel temporal dynamics and inter-channel spatial\ncorrelations, achieving 2x speed improvement with 6x less memory required\ncompared to standard self-attention. We present several model sizes ranging\nfrom 3.6 million to 85 million parameters. Pre-trained on over 20,000 hours of\npublicly available scalp EEG recordings with diverse channel configurations,\nour models set new benchmarks in emotion detection and seizure detection tasks,\nwith competitive performance in anomaly classification and gait prediction.\nThis validates our models' effectiveness and efficiency.",
        "Many recent studies have found evidence for emergent reasoning capabilities\nin large language models, but debate persists concerning the robustness of\nthese capabilities, and the extent to which they depend on structured reasoning\nmechanisms. To shed light on these issues, we perform a comprehensive study of\nthe internal mechanisms that support abstract rule induction in an open-source\nlanguage model (Llama3-70B). We identify an emergent symbolic architecture that\nimplements abstract reasoning via a series of three computations. In early\nlayers, symbol abstraction heads convert input tokens to abstract variables\nbased on the relations between those tokens. In intermediate layers, symbolic\ninduction heads perform sequence induction over these abstract variables.\nFinally, in later layers, retrieval heads predict the next token by retrieving\nthe value associated with the predicted abstract variable. These results point\ntoward a resolution of the longstanding debate between symbolic and neural\nnetwork approaches, suggesting that emergent reasoning in neural networks\ndepends on the emergence of symbolic mechanisms."
      ]
    }
  },
  {
    "id":2411.05236,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Shannon capacity of signal transduction for multiple independent receptors",
    "start_abstract":"Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
    "start_categories":[
      "cs.SY",
      "eess.SP"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
      ],
      "abstract":[
        "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of\n  RNA Graph Motifs Using Topological Descriptors",
        "PyMOLfold: Interactive Protein and Ligand Structure Prediction in PyMOL",
        "Silicon is the next frontier in plant synthetic biology",
        "Deep Learning of Proteins with Local and Global Regions of Disorder",
        "Nonsuppressible viremia during HIV-1 therapy meets molecular virology",
        "Engineered Zwitterion-Infused Clay Composites with Antibacterial and\n  Antifungal Efficacy",
        "Artificial Intelligence Approaches for Anti-Addiction Drug Discovery",
        "Remodeling Peptide-MHC-TCR Triad Binding as Sequence Fusion for\n  Immunogenicity Prediction",
        "Advances in RNA secondary structure prediction and RNA modifications:\n  Methods, data, and applications",
        "Inverse problems with experiment-guided AlphaFold",
        "DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown\n  Efficiency",
        "RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via\n  Synergistic Flow Matching",
        "Fluorescence Phasor Analysis: Basic Principles and Biophysical\n  Applications",
        "Census of Ly$\\alpha$ Emission from $\\sim 600$ Galaxies at $z=5-14$:\n  Evolution of the Ly$\\alpha$ Luminosity Function and a Late Sharp Cosmic\n  Reionization",
        "Dynamics of disordered quantum systems with two- and three-dimensional\n  tensor networks",
        "Anomalies of the Scholtes regularization for mathematical programs with\n  complementarity constraints",
        "Continuum limit of fourth-order Schr\\\"{o}dinger equations on the lattice",
        "Some examples of affine isometries of Banach spaces arising from 1-D\n  dynamics",
        "Near-Optimal Parameter Tuning of Level-1 QAOA for Ising Models",
        "Quantum State Designs from Minimally Random Quantum Circuits",
        "Pfaffian solution for dark-dark soliton to the coupled complex modified\n  Korteweg-de Vries equation",
        "The Change of Variable Formula Integrals, do they have equal value?",
        "The Category of Atomic Monoids: Universal Constructions and Arithmetic\n  Properties",
        "Twisted oxide membrane interface by local atomic registry design",
        "Heterogeneous Treatment Effect in Time-to-Event Outcomes: Harnessing\n  Censored Data with Recursively Imputed Trees",
        "Comparative Analysis of Perturbed $f(R)$ Gravity and Perturbed Rastall\n  Gravity Models in Describing Cosmic Evolution from Early to Late Universe\n  Relative to the $\\Lambda$CDM Model",
        "The outflow impacts on the size of the narrow-line region among type-2\n  AGNs",
        "GECKO Follow-up Observation of the Binary Neutron Star-Black Hole Merger\n  Candidate S230518h"
      ],
      "abstract":[
        "We introduce a computational topology-based approach with unsupervised\nmachine-learning algorithms to estimate the database size and content of\nRNA-like graph topologies. Specifically, we apply graph theory enumeration to\ngenerate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2\nto 9. Among them, only 0.11% graphs correspond to approximately 200,000 known\nRNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping\nmethod. The remaining 99.89% of the dual graphs may be RNA-like or\nnon-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are\nmore likely to be associated with RNA structures, we apply computational\ntopology descriptors using the Persistent Spectral Graphs (PSG) method to\ncharacterize each graph using 19 PSG-based features and use clustering\nalgorithms that partition all possible dual graphs into two clusters, RNA-like\ncluster and non-RNA-like cluster. The distance of each dual graph to the center\nof the RNA-like cluster represents the likelihood of it belonging to RNA\nstructures. From validation, our PSG-based RNA-like cluster includes 97.3% of\nthe 121 known RNA dual graphs, suggesting good performance. Furthermore,\n46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,\nwe observe that all the top 15 RNA-like dual graphs can be separated into\nmultiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to\nhave any subgraphs. Moreover, a significant topological difference between top\nRNA-like and non-RNA-like graphs is evident when comparing their topological\nfeatures. These findings provide valuable insights into the size of the RNA\nmotif universe and RNA design strategies, offering a novel framework for\npredicting RNA graph topologies and guiding the discovery of novel RNA motifs,\nperhaps anti-viral therapeutics by subgraph assembly.",
        "PyMOLfold is a flexible and open-source plugin designed to seamlessly\nintegrate AI-based protein structure prediction and visualization within the\nwidely used PyMOL molecular graphics system. By leveraging state-of-the-art\nprotein folding models such as ESM3, Boltz-1, and Chai-1, PyMOLfold allows\nresearchers to directly predict protein tertiary structures from amino acid\nsequences without requiring external tools or complex workflows. Furthermore,\nwith certain models, users can provide a SMILES string of a ligand and have the\nsmall molecule placed in the protein structure. This unique capability bridges\nthe gap between computational folding and structural visualization, enabling\nusers to input a primary sequence, perform a folding prediction, and\nimmediately explore the resulting 3D structure within the same intuitive\nplatform.",
        "Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field.",
        "Although machine learning has transformed protein structure prediction of\nfolded protein ground states with remarkable accuracy, intrinsically disordered\nproteins and regions (IDPs\/IDRs) are defined by diverse and dynamical\nstructural ensembles that are predicted with low confidence by algorithms such\nas AlphaFold. We present a new machine learning method, IDPForge (Intrinsically\nDisordered Protein, FOlded and disordered Region GEnerator), that exploits a\ntransformer protein language diffusion model to create all-atom IDP ensembles\nand IDR disordered ensembles that maintains the folded domains. IDPForge does\nnot require sequence-specific training, back transformations from\ncoarse-grained representations, nor ensemble reweighting, as in general the\ncreated IDP\/IDR conformational ensembles show good agreement with solution\nexperimental data, and options for biasing with experimental restraints are\nprovided if desired. We envision that IDPForge with these diverse capabilities\nwill facilitate integrative and structural studies for proteins that contain\nintrinsic disorder.",
        "HIV-1 replication can be suppressed with antiretroviral therapy (ART), but\nindividuals who stop taking ART soon become viremic again. Some people\nexperience extended times of detectable viremia despite optimal adherence to\nART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of\nnonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded\nT cells harboring HIV-1 proviruses with small deletions or mutations in the\n5'-leader, the UTR that includes the major splice donor site of viral RNA.\nThese mutations altered viral RNA-splicing efficiency and RNA dimerization and\npackaging, yet still allowed production of detectable levels of noninfectious\nvirus particles. These particles lacked the HIV-1 Env surface protein required\nfor cell entry and failed to form the mature capsid cone required for\ninfectivity. These studies improve our understanding of NSV and the regulation\nof viral functions in the 5'-leader with implications for rationalized care in\nindividuals with NSV.",
        "Microbes and pathogens play a detrimental role in healing wounds, causing\ninfections like impetigo through bodily fluids and skin and entering the\nbloodstream through the wounds, thereby hindering the healing process and\ntissue regeneration. Clay, known for its long history of natural therapeutic\nuse, has emerged as one of the most promising candidates for biomedical\napplications due to its non-toxic nature, porosity, high surface area,\nubiquity, and excellent cation exchange capacity. This study demonstrates an\ninnovative approach to engineering an organo-functionalized,\ninfection-resistant, easy-to-use bandage material from clay, an environmentally\nbenign and sustainable material. The hybrid membranes have been developed using\nclays, zwitterions, silver ions, and terbinafine hydrochloride (TBH) to impart\nantibacterial and antifungal efficacy. A critical aspect of this study is\nembedding organic molecules and metal ions with the clays and releasing them to\nresist the growth and kill the pathogens. The antimicrobial efficacy of the\nmembranes has been tested using a zone of inhibition study against the most\ncommon microbes in skin wounds, viz. S. aureus, E. coli, and C. albicans.\nResults from our studies not only demonstrate the potential of these hybrid\nclay membranes as a cost-effective, scalable, and effective solution for\ntreating microbial infections but also instill newer avenues for point-of-care\nwound-healing treatments, offering hope for improved patient outcomes.",
        "Drug addiction is a complex and pervasive global challenge that continues to\npose significant public health concerns. Traditional approaches to\nanti-addiction drug discovery have struggled to deliver effective therapeutics,\nfacing high attrition rates, long development timelines, and inefficiencies in\nprocessing large-scale data. Artificial intelligence (AI) has emerged as a\ntransformative solution to address these issues. Using advanced algorithms, AI\nis revolutionizing drug discovery by enhancing the speed and precision of key\nprocesses. This review explores the transformative role of AI in the pipeline\nfor anti-addiction drug discovery, including data collection, target\nidentification, and compound optimization. By highlighting the potential of AI\nto overcome traditional barriers, this review systematically examines how AI\naddresses critical gaps in anti-addiction research, emphasizing its potential\nto revolutionize drug discovery and development, overcome challenges, and\nadvance more effective therapeutic strategies.",
        "The complex nature of tripartite peptide-MHC-TCR interactions is a critical\nyet underexplored area in immunogenicity prediction. Traditional studies on\nTCR-antigen binding have not fully addressed the complex dependencies in triad\nbinding. In this paper, we propose new modeling approaches for these tripartite\ninteractions, utilizing sequence information from MHCs, peptides, and TCRs. Our\nmethods adhere to native sequence forms and align with biological processes to\nenhance prediction accuracy. By incorporating representation learning\ntechniques, we introduce a fusion mechanism to integrate the three sequences\neffectively. Empirical experiments show that our models outperform traditional\nmethods, achieving a 2.8 to 13.3 percent improvement in prediction accuracy\nacross existing benchmarks. We further validate our approach with extensive\nablation studies, demonstrating the effectiveness of the proposed model\ncomponents. The model implementation, code, and supplementary materials,\nincluding a manuscript with colored hyperlinks and a technical appendix for\ndigital viewing, will be open-sourced upon publication.",
        "Due to the hierarchical organization of RNA structures and their pivotal\nroles in fulfilling RNA functions, the formation of RNA secondary structure\ncritically influences many biological processes and has thus been a crucial\nresearch topic. This review sets out to explore the computational prediction of\nRNA secondary structure and its connections to RNA modifications, which have\nemerged as an active domain in recent years. We first examine the progression\nof RNA secondary structure prediction methodology, focusing on a set of\nrepresentative works categorized into thermodynamic, comparative, machine\nlearning, and hybrid approaches. Next, we survey the advances in RNA\nmodifications and computational methods for identifying RNA modifications,\nfocusing on the prominent modification types. Subsequently, we highlight the\ninterplay between RNA modifications and secondary structures, emphasizing how\nmodifications such as m6A dynamically affect RNA folding and vice versa. In\naddition, we also review relevant data sources and provide a discussion of\ncurrent challenges and opportunities in the field. Ultimately, we hope our\nreview will be able to serve as a cornerstone to aid in the development of\ninnovative methods for this emerging topic and foster therapeutic applications\nin the future.",
        "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
        "Background: Small interfering RNA (siRNA) is a promising therapeutic agent\ndue to its ability to silence disease-related genes via RNA interference. While\ntraditional machine learning and early deep learning methods have made progress\nin predicting siRNA efficacy, there remains significant room for improvement.\nAdvanced deep learning techniques can enhance prediction accuracy, reducing the\nreliance on extensive wet-lab experiments and accelerating the identification\nof effective siRNA sequences. This approach also provides deeper insights into\nthe mechanisms of siRNA efficacy, facilitating more targeted and efficient\ntherapeutic strategies.\n  Methods: We introduce DeepSilencer, an innovative deep learning model\ndesigned to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced\nneural network architectures to capture the complex features of siRNA\nsequences. Our key contributions include a specially designed deep learning\nmodel, an innovative online data sampling method, and an improved loss function\ntailored for siRNA prediction. These enhancements collectively boost the\nmodel's prediction accuracy and robustness.\n  Results: Extensive evaluations on multiple test sets demonstrate that\nDeepSilencer achieves state-of-the-art performance using only siRNA sequences\nand basic physicochemical properties. Our model surpasses several other methods\nand shows superior predictive performance, particularly when incorporating\nthermodynamic parameters.\n  Conclusion: The advancements in data sampling, model design, and loss\nfunction significantly enhance the predictive capabilities of DeepSilencer.\nThese improvements underscore its potential to advance RNAi therapeutic design\nand development, offering a powerful tool for researchers and clinicians.",
        "Ribonucleic acid (RNA) binds to molecules to achieve specific biological\nfunctions. While generative models are advancing biomolecule design, existing\nmethods for designing RNA that target specific ligands face limitations in\ncapturing RNA's conformational flexibility, ensuring structural validity, and\novercoming data scarcity. To address these challenges, we introduce RiboFlow, a\nsynergistic flow matching model to co-design RNA structures and sequences based\non target molecules. By integrating RNA backbone frames, torsion angles, and\nsequence features in an unified architecture, RiboFlow explicitly models RNA's\ndynamic conformations while enforcing sequence-structure consistency to improve\nvalidity. Additionally, we curate RiboBind, a large-scale dataset of\nRNA-molecule interactions, to resolve the scarcity of high-quality structural\ndata. Extensive experiments reveal that RiboFlow not only outperforms\nstate-of-the-art RNA design methods by a large margin but also showcases\ncontrollable capabilities for achieving high binding affinity to target\nligands. Our work bridges critical gaps in controllable RNA design, offering a\nframework for structure-aware, data-efficient generation.",
        "Fluorescence is one of the most widely used techniques in biological\nsciences. Its exceptional sensitivity and versatility make it a tool of first\nchoice for quantitative studies in biophysics. The concept of phasors,\noriginally introduced by Charles Steinmetz in the late 19th century for\nanalyzing alternating current circuits, has since found applications across\ndiverse disciplines, including fluorescence spectroscopy. The main idea behind\nfluorescence phasors was posited by Gregorio Weber in 1981. By analyzing the\ncomplementary nature of pulse and phase fluorometry data, he shows that two\nmagnitudes -- denoted as $G$ and $S$ -- derived from the frequency-domain\nfluorescence measurements correspond to the real and imaginary part of the\nFourier transform of the fluorescence intensity in the time domain. This review\nprovides a historical perspective on how the concept of phasors originates and\nhow it integrates into fluorescence spectroscopy. We discuss their fundamental\nalgebraic properties, which enable intuitive model-free analysis of\nfluorescence data despite the complexity of the underlying phenomena. Some\napplications in biophysics illustrate the power of this approach in studying\ndiverse phenomena, including protein folding, protein interactions, phase\ntransitions in lipid mixtures and formation of high-order structures in nucleic\nacids.",
        "We present the statistical properties of Ly$\\alpha$ emission in 586 galaxies\nat $z=4.5-14.2$, observed by multiple JWST\/NIRSpec spectroscopy projects,\nincluding JADES, GLASS, CEERS, and GO\/DDT programs. We obtain Ly$\\alpha$\nequivalent width (EW), Ly$\\alpha$ escape fraction, and ionizing photon\nproduction efficiency measurements or upper limits for these galaxies, and\nconfirm that the Ly$\\alpha$ emitting galaxy fraction decreases towards higher\nredshifts. We derive Ly$\\alpha$ luminosity functions from $z\\sim 5$ to $z\\sim\n10-14$ with the observed Ly$\\alpha$ EW distributions and galaxy UV luminosity\nfunctions, and find a $\\sim3$ dex decrease in number density at\n$L_\\mathrm{Ly\\alpha}=10^{42}-10^{43}$ erg s$^{-1}$ over the redshift range.\nNotably, this study presents the first constraints on the Ly$\\alpha$ luminosity\nfunction at $z\\sim 8-14$. We obtain the neutral hydrogen fractions of\n$x_\\mathrm{HI}=0.17_{-0.16}^{+0.23}$, $0.63_{-0.28}^{+0.18}$,\n$0.79_{-0.21}^{+0.13}$, and $0.88_{-0.13}^{+0.11}$ at $z\\sim6$, $7$, $8-9$, and\n$10-14$, respectively, via comparisons of the reionization models developed by\nsemi-numerical simulations with 21cmFAST explaining the observations of\nLy$\\alpha$, UV continuum, and Planck electron optical depth. The high\n$x_\\mathrm{HI}$ values over $z\\sim 7-14$ suggest a late and sharp reionization,\nwith the primary reionization process occurring at $z\\sim 6-7$. Such a late and\nsharp reionization is not easily explained by either a clumpy inter-galactic\nmedium or sources of reionization in a classical faint-galaxy or a\nbright-galaxy\/AGN scenario, unless a very high escape fraction or AGN duty\ncycle is assumed at $z\\sim 6-7$.",
        "Quantum spin glasses form a good testbed for studying the performance of\nvarious quantum annealing and optimization algorithms. In this work we show how\ntwo- and three-dimensional tensor networks can accurately and efficiently\nsimulate the quantum annealing dynamics of Ising spin glasses on a range of\nlattices. Such dynamics were recently simulated using D-Wave's Advantage$2$\nsystem [A. D. King et al, Science, 10.1126\/science.ado6285 (2025)] and,\nfollowing extensive comparisons to existing numerical methods, claimed to be\nbeyond the reach of classical computation. Here we show that by evolving\nlattice-specific tensor networks with simple belief propagation to keep up with\nthe entanglement generated during the time evolution and then extracting\nexpectation values with more sophisticated variants of belief propagation,\nstate-of-the-art accuracies can be reached with modest computational resources.\nWe exploit the scalability of our simulations and simulate a system of over\n$300$ qubits, allowing us to verify the universal physics present and extract a\nvalue for the associated Kibble-Zurek exponent which agrees with recent values\nobtained in literature. Our results demonstrate that tensor networks are a\nviable approach for simulating large scale quantum dynamics in two and three\ndimensions on classical computers, and algorithmic advancements are expected to\nexpand their applicability going forward.",
        "For mathematical programs with complementarity constraints (MPCC), we refine\nthe convergence analysis of the Scholtes regularization. Our goal is to relate\nnondegenerate C-stationary points of MPCC with nondegenerate Karush-Kuhn-Tucker\npoints of its Scholtes regularization. We detected the following anomalies: (i)\nin a neighborhood of a nondegenerate C-stationary point there could be\ndegenerate Karush-Kuhn-Tucker points of the Scholtes regularization; (ii) even\nif nondegenerate, they might be locally non-unique; (iii) if nevertheless\nunique, their quadratic index potentially differs from the C-index of the\nC-stationary point under consideration. Thus, a change of the topological type\nfor Karush-Kuhn-Tucker points of the Scholtes regularization is possible. In\nparticular, a nondegenerate minimizer of MPCC might be approximated by saddle\npoints. In order to bypass the mentioned anomalies, an additional generic\ncondition for nondegenerate C-stationary points of MPCC is identified. Then, we\nuniquely trace nondegenerate Karush-Kuhn-Tucker points of the Scholtes\nregularization and successively maintain their topological type.",
        "In this paper, we consider the discrete fourth-order Schr\\\"{o}dinger equation\non the lattice $h\\mathbb{Z}^2$. Uniform Strichartz estimates are established by\nanalyzing frequency localized oscillatory integrals with the method of\nstationary phase and applying Littlewood-Paley inequalities. As an application,\nwe obtain the precise rate of $L^2$ convergence from the solutions of discrete\nsemilinear equations to those of the corresponding equations on the Euclidean\nplane $\\mathbb{R}^2$ in the contimuum limit $h \\rightarrow 0$.",
        "We provide a large family of examples of affine isometries of the Banach\nspaces $C^0 (S^1)$, $L^1 (S^1)$ and $L^2 (S^1 \\times S^1)$ that are\nfixed-point-free despite being recurrent (in particular, they have zero drift).\nThese come from natural cocycles on the group of circle diffeomorphisms, namely\nthe logarithmic, affine and (a variation of the) Schwarzian derivative. Quite\ninterestingly, they arise from diffeomorphisms that are generic in an\nappropriate context. We also show how to promote these examples in order to\nobtain families of commuting isometries satisfying the same properties.",
        "The Quantum Approximate Optimisation Algorithm (QAOA) is a hybrid\nquantum-classical algorithm for solving combinatorial optimisation problems.\nQAOA encodes solutions into the ground state of a Hamiltonian, approximated by\na $p$-level parameterised quantum circuit composed of problem and mixer\nHamiltonians, with parameters optimised classically. While deeper QAOA circuits\ncan offer greater accuracy, practical applications are constrained by complex\nparameter optimisation and physical limitations such as gate noise, restricted\nqubit connectivity, and state-preparation-and-measurement errors, limiting\nimplementations to shallow depths. This work focuses on QAOA$_1$ (QAOA at\n$p=1$) for QUBO problems, represented as Ising models. Despite QAOA$_1$ having\nonly two parameters, $(\\gamma, \\beta)$, we show that their optimisation is\nchallenging due to a highly oscillatory landscape, with oscillation rates\nincreasing with the problem size, density, and weight. This behaviour\nnecessitates high-resolution grid searches to avoid distortion of cost\nlandscapes that may result in inaccurate minima. We propose an efficient\noptimisation strategy that reduces the two-dimensional $(\\gamma, \\beta)$ search\nto a one-dimensional search over $\\gamma$, with $\\beta^*$ computed\nanalytically. We establish the maximum permissible sampling period required to\naccurately map the $\\gamma$ landscape and provide an algorithm to estimate the\noptimal parameters in polynomial time. Furthermore, we rigorously prove that\nfor regular graphs on average, the globally optimal $\\gamma^* \\in \\mathbb{R}^+$\nvalues are concentrated very close to zero and coincide with the first local\noptimum, enabling gradient descent to replace exhaustive line searches. This\napproach is validated using Recursive QAOA (RQAOA), where it consistently\noutperforms both coarsely optimised RQAOA and semidefinite programs across all\ntested QUBO instances.",
        "Random many-body states are both a useful tool to model certain physical\nsystems and an important asset for quantum computation. Realising them,\nhowever, generally requires an exponential (in system size) amount of\nresources. Recent research has presented a way out by showing that one can\ngenerate random states, or more precisely a controlled approximation of them,\nby applying a quantum circuit built in terms of few-body unitary gates. Most of\nthis research, however, has been focussed on the case of quantum circuits\ncomposed by completely random unitary gates. Here we consider what happens for\ncircuits that, instead, involve a minimal degree of randomness. Specifically,\nwe concentrate on two different settings: (a) brickwork quantum circuits with a\nsingle one-qudit random matrix at a boundary; (b) brickwork quantum circuits\nwith fixed interactions but random one-qudit gates everywhere. We show that,\nfor any given initial state, (a) and (b) produce a distribution of states\napproaching the Haar distribution in the limit of large circuit depth. More\nprecisely, we show that the moments of the distribution produced by our\ncircuits can approximate the ones of the Haar distribution in a depth\nproportional to the system size. Interestingly we find that in both Cases (a)\nand (b) the relaxation to the Haar distribution occurs in two steps - this is\nin contrast with what happens in fully random circuits. Moreover, we show that\nchoosing appropriately the fixed interactions, for example taking the local\ngate to be a dual-unitary gate with high enough entangling power, minimally\nrandom circuits produce a Haar random distribution more rapidly than fully\nrandom circuits. In particular, dual-unitary circuits with maximal entangling\npower - i.e. perfect tensors - appear to provide the optimal quantum state\ndesign preparation for any design number.",
        "In this paper, we study coupled complex modified Korteweg-de Vries (ccmKdV)\nequation by combining the Hirota's method and the Kadomtsev-Petviashvili (KP)\nreduction method. First, we show that the bilinear form of the ccmKdV equation\nunder nonzero boundary condition is linked to the discrete BKP hierarchy\nthrough Miwa transformation. Based on this finding, we construct the dark-dark\nsoliton solution in the pfaffian form. The dynamical behaviors for one- and\ntwo-soliton are analyzed and illustrated.",
        "Assuming that the two integrals in the Change of Variable Formula for the\nunidimensional Riemann integral are finite, one can ask if they have equal\nvalue. We give a positive answer to this question. The proof is very easy to\nfollow and to keep in mind. An example is given.",
        "We introduce and investigate the category $\\mathsf{AtoMon}$ of atomic monoids\nand atom-preserving monoid homomorphisms, which is a (non-full) subcategory of\nthe usual category of monoids. In particular, we compute all limits and\ncolimits, showing that $\\mathsf{AtoMon}$ is a complete and cocomplete category.\nWe also address certain arithmetic properties of products and coproducts,\nproviding explicit formulas for some fundamental invariants associated with\nfactorization lengths in atomic monoids.",
        "Interplay of lattice, orbital, and charge degrees of freedom in complex oxide\nmaterials has hosted a plethora of exotic quantum phases and physical\nproperties. Recent advances in synthesis of freestanding complex oxide\nmembranes and twisted heterostructures assembled from membranes provide new\nopportunities for discovery using moir\\'e design with local lattice control. To\nthis end, we designed moir\\'e crystals at the coincidence site lattice\ncondition, providing commensurate structure within the moir\\'e supercell\narising from the multi-atom complex oxide unit cell. We fabricated such twisted\nbilayers from freestanding SrTiO3 membranes and used depth sectioning-based TEM\nmethods to discover ordered charge states at the moir\\'e interface. By\nselectively imaging SrTiO3 atomic planes at different depths through the\nbilayer, we clearly resolved the moir\\'e periodic structure at the twisted\ninterface and found that it exhibits lattice-dependent charge\ndisproportionation in the local atomic registry within the moir\\'e supercell.\nOur density-functional modelling of the twisted oxide interface predicts that\nthese moir\\'e phenomena are accompanied by the emergence of a two-dimensional\nflat band that can drive new electronic phases. Our work provides a novel\nguideline for controlling moir\\'e periodicity in twisted oxides and opens\npathways to exploit the new functionalities via moir\\'e lattice-driven\ncharge-orbital correlation.",
        "Tailoring treatments to individual needs is a central goal in fields such as\nmedicine. A key step toward this goal is estimating Heterogeneous Treatment\nEffects (HTE) - the way treatments impact different subgroups. While crucial,\nHTE estimation is challenging with survival data, where time until an event\n(e.g., death) is key. Existing methods often assume complete observation, an\nassumption violated in survival data due to right-censoring, leading to bias\nand inefficiency. Cui et al. (2023) proposed a doubly-robust method for HTE\nestimation in survival data under no hidden confounders, combining a causal\nsurvival forest with an augmented inverse-censoring weighting estimator.\nHowever, we find it struggles under heavy censoring, which is common in\nrare-outcome problems such as Amyotrophic lateral sclerosis (ALS). Moreover,\nmost current methods cannot handle instrumental variables, which are a crucial\ntool in the causal inference arsenal. We introduce Multiple Imputation for\nSurvival Treatment Response (MISTR), a novel, general, and non-parametric\nmethod for estimating HTE in survival data. MISTR uses recursively imputed\nsurvival trees to handle censoring without directly modeling the censoring\nmechanism. Through extensive simulations and analysis of two real-world\ndatasets-the AIDS Clinical Trials Group Protocol 175 and the Illinois\nunemployment dataset we show that MISTR outperforms prior methods under heavy\ncensoring in the no-hidden-confounders setting, and extends to the instrumental\nvariable setting. To our knowledge, MISTR is the first non-parametric approach\nfor HTE estimation with unobserved confounders via instrumental variables.",
        "This study conducts a meticulous examination of the cosmological implications\ninherent in Rastall gravity and $f(R)$ gravity models, assessing their efficacy\nacross distinct cosmic epochs, from early universe structure formation to\nlate-time acceleration. In the initial stages, both models exhibit commendable\ncompatibility with observed features of structure formation, aligning with the\nestablished $\\Lambda$CDM model. The derived Jeans' wavenumbers for each model\nsupport their viability. However, as the cosmic timeline progresses into the\nlate universe, a discernible disparity surfaces. Utilizing the Markov Chain\nMonte Carlo method, we reconstruct the deceleration parameter $(q)$ and\nidentify Deceleration - Acceleration redshift transition values. For $f(R)$\ngravity, our results align closely with previous studies, emphasizing its\nsuperior ability to elucidate the recent cosmic acceleration. In contrast,\nRastall gravity exhibits distinct redshift transition values. Our rigorous\nanalysis underscores the prowess of $f(R)$ gravity in capturing the observed\ncosmic acceleration, positioning it as a compelling alternative to the\nconventional $\\Lambda$CDM model. The discernible shifts observed in the peaks\nof the CMB power spectrum and evolution of deceleration parameter (q) for both\n$f(R)$ gravity and Rastall gravity models in the Early and Late universe, in\nrelation to the $\\Lambda $CDM model, provide compelling evidence supporting the\nproposition that these alternative gravity models can account for the\nanisotropy of the universe without invoking the need for dark energy.",
        "We present the study of the gas kinematics in narrow-line regions (NLRs) of\n2,009 type-2 AGNs at $z<0.34$. We construct the [O III]$\\lambda$5007\nemission-line images using publicly available broadband images from the Sloan\nDigital Sky Survey (SDSS). The [O III] emission area of the samples, measured\ndown to $1.7\\times10^{-15}$ erg\/s\/cm$^2$\/arcsec$^2$, ranges from 3.7 kpc$^2$ up\nto 224 kpc$^2$. With our broadband technique, we found the strong correlation\nbetween [O III] area and AGN luminosity inferred from the [O III] luminosity\nand the mid-infrared luminosity at the rest-frame $15\\mu$m. The isophotal\nthreshold used to determine the [O III] area affects the correlation strength\nin that the brighter isophote yields the stronger correlation between the [O\nIII] area and AGN luminosity. The presence of gas outflow is examined by the\nratio of the [O III] velocity dispersion to the stellar velocity dispersion\n($\\sigma_{\\rm [O\\,III]}\/\\sigma_\\star > 1.4$) using the SDSS spectra. At the\ngiven luminosity, the objects with and without outflows exhibit the same\nextension of the [O III] emission. Their correlation between the [O III] area\nand luminosity is almost identical. It is suggested that the size of NLRs is\nnot affected by outflow mechanisms but rather by photoionization from the\ncentral AGNS.",
        "The gravitational wave (GW) event S230518h is a potential binary neutron\nstar-black hole merger (NSBH) event that was detected during engineering run 15\n(ER15), which served as the commissioning period before the LIGO-Virgo-KAGRA\n(LVK) O4a observing run. Despite its low probability of producing detectable\nelectromagnetic emissions, we performed extensive follow-up observations of\nthis event using the GECKO telescopes in the southern hemisphere. Our\nobservation covered 61.7\\% of the 90\\% credible region, a $\\rm 284\\:deg^2$ area\naccessible from the southern hemisphere, reaching a median limiting magnitude\nof $R=21.6$ mag. In these images, we conducted a systematic search for an\noptical counterpart of this event by combining a CNN-based classifier and human\nverification. We identified 128 transient candidates, but no significant\noptical counterpart was found that could have caused the GW signal.\nFurthermore, we provide feasible KN properties that are consistent with the\nupper limits of observation. Although no optical counterpart was found, our\nresult demonstrates both GECKO's efficient wide-field follow-up capabilities\nand usefulness for constraining properties of kilonovae from NSBH mergers at\ndistances of $\\sim 200$ Mpc."
      ]
    }
  },
  {
    "id":2411.05236,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT",
    "start_abstract":"Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",
    "start_categories":[
      "cs.SY",
      "eess.SP"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
      ],
      "abstract":[
        "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of\n  RNA Graph Motifs Using Topological Descriptors",
        "PyMOLfold: Interactive Protein and Ligand Structure Prediction in PyMOL",
        "Silicon is the next frontier in plant synthetic biology",
        "Deep Learning of Proteins with Local and Global Regions of Disorder",
        "Nonsuppressible viremia during HIV-1 therapy meets molecular virology",
        "Engineered Zwitterion-Infused Clay Composites with Antibacterial and\n  Antifungal Efficacy",
        "Artificial Intelligence Approaches for Anti-Addiction Drug Discovery",
        "Remodeling Peptide-MHC-TCR Triad Binding as Sequence Fusion for\n  Immunogenicity Prediction",
        "Advances in RNA secondary structure prediction and RNA modifications:\n  Methods, data, and applications",
        "Inverse problems with experiment-guided AlphaFold",
        "DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown\n  Efficiency",
        "RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via\n  Synergistic Flow Matching",
        "Fluorescence Phasor Analysis: Basic Principles and Biophysical\n  Applications",
        "Census of Ly$\\alpha$ Emission from $\\sim 600$ Galaxies at $z=5-14$:\n  Evolution of the Ly$\\alpha$ Luminosity Function and a Late Sharp Cosmic\n  Reionization",
        "Dynamics of disordered quantum systems with two- and three-dimensional\n  tensor networks",
        "Anomalies of the Scholtes regularization for mathematical programs with\n  complementarity constraints",
        "Continuum limit of fourth-order Schr\\\"{o}dinger equations on the lattice",
        "Some examples of affine isometries of Banach spaces arising from 1-D\n  dynamics",
        "Near-Optimal Parameter Tuning of Level-1 QAOA for Ising Models",
        "Quantum State Designs from Minimally Random Quantum Circuits",
        "Pfaffian solution for dark-dark soliton to the coupled complex modified\n  Korteweg-de Vries equation",
        "The Change of Variable Formula Integrals, do they have equal value?",
        "The Category of Atomic Monoids: Universal Constructions and Arithmetic\n  Properties",
        "Twisted oxide membrane interface by local atomic registry design",
        "Heterogeneous Treatment Effect in Time-to-Event Outcomes: Harnessing\n  Censored Data with Recursively Imputed Trees",
        "Comparative Analysis of Perturbed $f(R)$ Gravity and Perturbed Rastall\n  Gravity Models in Describing Cosmic Evolution from Early to Late Universe\n  Relative to the $\\Lambda$CDM Model",
        "The outflow impacts on the size of the narrow-line region among type-2\n  AGNs",
        "GECKO Follow-up Observation of the Binary Neutron Star-Black Hole Merger\n  Candidate S230518h"
      ],
      "abstract":[
        "We introduce a computational topology-based approach with unsupervised\nmachine-learning algorithms to estimate the database size and content of\nRNA-like graph topologies. Specifically, we apply graph theory enumeration to\ngenerate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2\nto 9. Among them, only 0.11% graphs correspond to approximately 200,000 known\nRNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping\nmethod. The remaining 99.89% of the dual graphs may be RNA-like or\nnon-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are\nmore likely to be associated with RNA structures, we apply computational\ntopology descriptors using the Persistent Spectral Graphs (PSG) method to\ncharacterize each graph using 19 PSG-based features and use clustering\nalgorithms that partition all possible dual graphs into two clusters, RNA-like\ncluster and non-RNA-like cluster. The distance of each dual graph to the center\nof the RNA-like cluster represents the likelihood of it belonging to RNA\nstructures. From validation, our PSG-based RNA-like cluster includes 97.3% of\nthe 121 known RNA dual graphs, suggesting good performance. Furthermore,\n46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,\nwe observe that all the top 15 RNA-like dual graphs can be separated into\nmultiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to\nhave any subgraphs. Moreover, a significant topological difference between top\nRNA-like and non-RNA-like graphs is evident when comparing their topological\nfeatures. These findings provide valuable insights into the size of the RNA\nmotif universe and RNA design strategies, offering a novel framework for\npredicting RNA graph topologies and guiding the discovery of novel RNA motifs,\nperhaps anti-viral therapeutics by subgraph assembly.",
        "PyMOLfold is a flexible and open-source plugin designed to seamlessly\nintegrate AI-based protein structure prediction and visualization within the\nwidely used PyMOL molecular graphics system. By leveraging state-of-the-art\nprotein folding models such as ESM3, Boltz-1, and Chai-1, PyMOLfold allows\nresearchers to directly predict protein tertiary structures from amino acid\nsequences without requiring external tools or complex workflows. Furthermore,\nwith certain models, users can provide a SMILES string of a ligand and have the\nsmall molecule placed in the protein structure. This unique capability bridges\nthe gap between computational folding and structural visualization, enabling\nusers to input a primary sequence, perform a folding prediction, and\nimmediately explore the resulting 3D structure within the same intuitive\nplatform.",
        "Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field.",
        "Although machine learning has transformed protein structure prediction of\nfolded protein ground states with remarkable accuracy, intrinsically disordered\nproteins and regions (IDPs\/IDRs) are defined by diverse and dynamical\nstructural ensembles that are predicted with low confidence by algorithms such\nas AlphaFold. We present a new machine learning method, IDPForge (Intrinsically\nDisordered Protein, FOlded and disordered Region GEnerator), that exploits a\ntransformer protein language diffusion model to create all-atom IDP ensembles\nand IDR disordered ensembles that maintains the folded domains. IDPForge does\nnot require sequence-specific training, back transformations from\ncoarse-grained representations, nor ensemble reweighting, as in general the\ncreated IDP\/IDR conformational ensembles show good agreement with solution\nexperimental data, and options for biasing with experimental restraints are\nprovided if desired. We envision that IDPForge with these diverse capabilities\nwill facilitate integrative and structural studies for proteins that contain\nintrinsic disorder.",
        "HIV-1 replication can be suppressed with antiretroviral therapy (ART), but\nindividuals who stop taking ART soon become viremic again. Some people\nexperience extended times of detectable viremia despite optimal adherence to\nART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of\nnonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded\nT cells harboring HIV-1 proviruses with small deletions or mutations in the\n5'-leader, the UTR that includes the major splice donor site of viral RNA.\nThese mutations altered viral RNA-splicing efficiency and RNA dimerization and\npackaging, yet still allowed production of detectable levels of noninfectious\nvirus particles. These particles lacked the HIV-1 Env surface protein required\nfor cell entry and failed to form the mature capsid cone required for\ninfectivity. These studies improve our understanding of NSV and the regulation\nof viral functions in the 5'-leader with implications for rationalized care in\nindividuals with NSV.",
        "Microbes and pathogens play a detrimental role in healing wounds, causing\ninfections like impetigo through bodily fluids and skin and entering the\nbloodstream through the wounds, thereby hindering the healing process and\ntissue regeneration. Clay, known for its long history of natural therapeutic\nuse, has emerged as one of the most promising candidates for biomedical\napplications due to its non-toxic nature, porosity, high surface area,\nubiquity, and excellent cation exchange capacity. This study demonstrates an\ninnovative approach to engineering an organo-functionalized,\ninfection-resistant, easy-to-use bandage material from clay, an environmentally\nbenign and sustainable material. The hybrid membranes have been developed using\nclays, zwitterions, silver ions, and terbinafine hydrochloride (TBH) to impart\nantibacterial and antifungal efficacy. A critical aspect of this study is\nembedding organic molecules and metal ions with the clays and releasing them to\nresist the growth and kill the pathogens. The antimicrobial efficacy of the\nmembranes has been tested using a zone of inhibition study against the most\ncommon microbes in skin wounds, viz. S. aureus, E. coli, and C. albicans.\nResults from our studies not only demonstrate the potential of these hybrid\nclay membranes as a cost-effective, scalable, and effective solution for\ntreating microbial infections but also instill newer avenues for point-of-care\nwound-healing treatments, offering hope for improved patient outcomes.",
        "Drug addiction is a complex and pervasive global challenge that continues to\npose significant public health concerns. Traditional approaches to\nanti-addiction drug discovery have struggled to deliver effective therapeutics,\nfacing high attrition rates, long development timelines, and inefficiencies in\nprocessing large-scale data. Artificial intelligence (AI) has emerged as a\ntransformative solution to address these issues. Using advanced algorithms, AI\nis revolutionizing drug discovery by enhancing the speed and precision of key\nprocesses. This review explores the transformative role of AI in the pipeline\nfor anti-addiction drug discovery, including data collection, target\nidentification, and compound optimization. By highlighting the potential of AI\nto overcome traditional barriers, this review systematically examines how AI\naddresses critical gaps in anti-addiction research, emphasizing its potential\nto revolutionize drug discovery and development, overcome challenges, and\nadvance more effective therapeutic strategies.",
        "The complex nature of tripartite peptide-MHC-TCR interactions is a critical\nyet underexplored area in immunogenicity prediction. Traditional studies on\nTCR-antigen binding have not fully addressed the complex dependencies in triad\nbinding. In this paper, we propose new modeling approaches for these tripartite\ninteractions, utilizing sequence information from MHCs, peptides, and TCRs. Our\nmethods adhere to native sequence forms and align with biological processes to\nenhance prediction accuracy. By incorporating representation learning\ntechniques, we introduce a fusion mechanism to integrate the three sequences\neffectively. Empirical experiments show that our models outperform traditional\nmethods, achieving a 2.8 to 13.3 percent improvement in prediction accuracy\nacross existing benchmarks. We further validate our approach with extensive\nablation studies, demonstrating the effectiveness of the proposed model\ncomponents. The model implementation, code, and supplementary materials,\nincluding a manuscript with colored hyperlinks and a technical appendix for\ndigital viewing, will be open-sourced upon publication.",
        "Due to the hierarchical organization of RNA structures and their pivotal\nroles in fulfilling RNA functions, the formation of RNA secondary structure\ncritically influences many biological processes and has thus been a crucial\nresearch topic. This review sets out to explore the computational prediction of\nRNA secondary structure and its connections to RNA modifications, which have\nemerged as an active domain in recent years. We first examine the progression\nof RNA secondary structure prediction methodology, focusing on a set of\nrepresentative works categorized into thermodynamic, comparative, machine\nlearning, and hybrid approaches. Next, we survey the advances in RNA\nmodifications and computational methods for identifying RNA modifications,\nfocusing on the prominent modification types. Subsequently, we highlight the\ninterplay between RNA modifications and secondary structures, emphasizing how\nmodifications such as m6A dynamically affect RNA folding and vice versa. In\naddition, we also review relevant data sources and provide a discussion of\ncurrent challenges and opportunities in the field. Ultimately, we hope our\nreview will be able to serve as a cornerstone to aid in the development of\ninnovative methods for this emerging topic and foster therapeutic applications\nin the future.",
        "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
        "Background: Small interfering RNA (siRNA) is a promising therapeutic agent\ndue to its ability to silence disease-related genes via RNA interference. While\ntraditional machine learning and early deep learning methods have made progress\nin predicting siRNA efficacy, there remains significant room for improvement.\nAdvanced deep learning techniques can enhance prediction accuracy, reducing the\nreliance on extensive wet-lab experiments and accelerating the identification\nof effective siRNA sequences. This approach also provides deeper insights into\nthe mechanisms of siRNA efficacy, facilitating more targeted and efficient\ntherapeutic strategies.\n  Methods: We introduce DeepSilencer, an innovative deep learning model\ndesigned to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced\nneural network architectures to capture the complex features of siRNA\nsequences. Our key contributions include a specially designed deep learning\nmodel, an innovative online data sampling method, and an improved loss function\ntailored for siRNA prediction. These enhancements collectively boost the\nmodel's prediction accuracy and robustness.\n  Results: Extensive evaluations on multiple test sets demonstrate that\nDeepSilencer achieves state-of-the-art performance using only siRNA sequences\nand basic physicochemical properties. Our model surpasses several other methods\nand shows superior predictive performance, particularly when incorporating\nthermodynamic parameters.\n  Conclusion: The advancements in data sampling, model design, and loss\nfunction significantly enhance the predictive capabilities of DeepSilencer.\nThese improvements underscore its potential to advance RNAi therapeutic design\nand development, offering a powerful tool for researchers and clinicians.",
        "Ribonucleic acid (RNA) binds to molecules to achieve specific biological\nfunctions. While generative models are advancing biomolecule design, existing\nmethods for designing RNA that target specific ligands face limitations in\ncapturing RNA's conformational flexibility, ensuring structural validity, and\novercoming data scarcity. To address these challenges, we introduce RiboFlow, a\nsynergistic flow matching model to co-design RNA structures and sequences based\non target molecules. By integrating RNA backbone frames, torsion angles, and\nsequence features in an unified architecture, RiboFlow explicitly models RNA's\ndynamic conformations while enforcing sequence-structure consistency to improve\nvalidity. Additionally, we curate RiboBind, a large-scale dataset of\nRNA-molecule interactions, to resolve the scarcity of high-quality structural\ndata. Extensive experiments reveal that RiboFlow not only outperforms\nstate-of-the-art RNA design methods by a large margin but also showcases\ncontrollable capabilities for achieving high binding affinity to target\nligands. Our work bridges critical gaps in controllable RNA design, offering a\nframework for structure-aware, data-efficient generation.",
        "Fluorescence is one of the most widely used techniques in biological\nsciences. Its exceptional sensitivity and versatility make it a tool of first\nchoice for quantitative studies in biophysics. The concept of phasors,\noriginally introduced by Charles Steinmetz in the late 19th century for\nanalyzing alternating current circuits, has since found applications across\ndiverse disciplines, including fluorescence spectroscopy. The main idea behind\nfluorescence phasors was posited by Gregorio Weber in 1981. By analyzing the\ncomplementary nature of pulse and phase fluorometry data, he shows that two\nmagnitudes -- denoted as $G$ and $S$ -- derived from the frequency-domain\nfluorescence measurements correspond to the real and imaginary part of the\nFourier transform of the fluorescence intensity in the time domain. This review\nprovides a historical perspective on how the concept of phasors originates and\nhow it integrates into fluorescence spectroscopy. We discuss their fundamental\nalgebraic properties, which enable intuitive model-free analysis of\nfluorescence data despite the complexity of the underlying phenomena. Some\napplications in biophysics illustrate the power of this approach in studying\ndiverse phenomena, including protein folding, protein interactions, phase\ntransitions in lipid mixtures and formation of high-order structures in nucleic\nacids.",
        "We present the statistical properties of Ly$\\alpha$ emission in 586 galaxies\nat $z=4.5-14.2$, observed by multiple JWST\/NIRSpec spectroscopy projects,\nincluding JADES, GLASS, CEERS, and GO\/DDT programs. We obtain Ly$\\alpha$\nequivalent width (EW), Ly$\\alpha$ escape fraction, and ionizing photon\nproduction efficiency measurements or upper limits for these galaxies, and\nconfirm that the Ly$\\alpha$ emitting galaxy fraction decreases towards higher\nredshifts. We derive Ly$\\alpha$ luminosity functions from $z\\sim 5$ to $z\\sim\n10-14$ with the observed Ly$\\alpha$ EW distributions and galaxy UV luminosity\nfunctions, and find a $\\sim3$ dex decrease in number density at\n$L_\\mathrm{Ly\\alpha}=10^{42}-10^{43}$ erg s$^{-1}$ over the redshift range.\nNotably, this study presents the first constraints on the Ly$\\alpha$ luminosity\nfunction at $z\\sim 8-14$. We obtain the neutral hydrogen fractions of\n$x_\\mathrm{HI}=0.17_{-0.16}^{+0.23}$, $0.63_{-0.28}^{+0.18}$,\n$0.79_{-0.21}^{+0.13}$, and $0.88_{-0.13}^{+0.11}$ at $z\\sim6$, $7$, $8-9$, and\n$10-14$, respectively, via comparisons of the reionization models developed by\nsemi-numerical simulations with 21cmFAST explaining the observations of\nLy$\\alpha$, UV continuum, and Planck electron optical depth. The high\n$x_\\mathrm{HI}$ values over $z\\sim 7-14$ suggest a late and sharp reionization,\nwith the primary reionization process occurring at $z\\sim 6-7$. Such a late and\nsharp reionization is not easily explained by either a clumpy inter-galactic\nmedium or sources of reionization in a classical faint-galaxy or a\nbright-galaxy\/AGN scenario, unless a very high escape fraction or AGN duty\ncycle is assumed at $z\\sim 6-7$.",
        "Quantum spin glasses form a good testbed for studying the performance of\nvarious quantum annealing and optimization algorithms. In this work we show how\ntwo- and three-dimensional tensor networks can accurately and efficiently\nsimulate the quantum annealing dynamics of Ising spin glasses on a range of\nlattices. Such dynamics were recently simulated using D-Wave's Advantage$2$\nsystem [A. D. King et al, Science, 10.1126\/science.ado6285 (2025)] and,\nfollowing extensive comparisons to existing numerical methods, claimed to be\nbeyond the reach of classical computation. Here we show that by evolving\nlattice-specific tensor networks with simple belief propagation to keep up with\nthe entanglement generated during the time evolution and then extracting\nexpectation values with more sophisticated variants of belief propagation,\nstate-of-the-art accuracies can be reached with modest computational resources.\nWe exploit the scalability of our simulations and simulate a system of over\n$300$ qubits, allowing us to verify the universal physics present and extract a\nvalue for the associated Kibble-Zurek exponent which agrees with recent values\nobtained in literature. Our results demonstrate that tensor networks are a\nviable approach for simulating large scale quantum dynamics in two and three\ndimensions on classical computers, and algorithmic advancements are expected to\nexpand their applicability going forward.",
        "For mathematical programs with complementarity constraints (MPCC), we refine\nthe convergence analysis of the Scholtes regularization. Our goal is to relate\nnondegenerate C-stationary points of MPCC with nondegenerate Karush-Kuhn-Tucker\npoints of its Scholtes regularization. We detected the following anomalies: (i)\nin a neighborhood of a nondegenerate C-stationary point there could be\ndegenerate Karush-Kuhn-Tucker points of the Scholtes regularization; (ii) even\nif nondegenerate, they might be locally non-unique; (iii) if nevertheless\nunique, their quadratic index potentially differs from the C-index of the\nC-stationary point under consideration. Thus, a change of the topological type\nfor Karush-Kuhn-Tucker points of the Scholtes regularization is possible. In\nparticular, a nondegenerate minimizer of MPCC might be approximated by saddle\npoints. In order to bypass the mentioned anomalies, an additional generic\ncondition for nondegenerate C-stationary points of MPCC is identified. Then, we\nuniquely trace nondegenerate Karush-Kuhn-Tucker points of the Scholtes\nregularization and successively maintain their topological type.",
        "In this paper, we consider the discrete fourth-order Schr\\\"{o}dinger equation\non the lattice $h\\mathbb{Z}^2$. Uniform Strichartz estimates are established by\nanalyzing frequency localized oscillatory integrals with the method of\nstationary phase and applying Littlewood-Paley inequalities. As an application,\nwe obtain the precise rate of $L^2$ convergence from the solutions of discrete\nsemilinear equations to those of the corresponding equations on the Euclidean\nplane $\\mathbb{R}^2$ in the contimuum limit $h \\rightarrow 0$.",
        "We provide a large family of examples of affine isometries of the Banach\nspaces $C^0 (S^1)$, $L^1 (S^1)$ and $L^2 (S^1 \\times S^1)$ that are\nfixed-point-free despite being recurrent (in particular, they have zero drift).\nThese come from natural cocycles on the group of circle diffeomorphisms, namely\nthe logarithmic, affine and (a variation of the) Schwarzian derivative. Quite\ninterestingly, they arise from diffeomorphisms that are generic in an\nappropriate context. We also show how to promote these examples in order to\nobtain families of commuting isometries satisfying the same properties.",
        "The Quantum Approximate Optimisation Algorithm (QAOA) is a hybrid\nquantum-classical algorithm for solving combinatorial optimisation problems.\nQAOA encodes solutions into the ground state of a Hamiltonian, approximated by\na $p$-level parameterised quantum circuit composed of problem and mixer\nHamiltonians, with parameters optimised classically. While deeper QAOA circuits\ncan offer greater accuracy, practical applications are constrained by complex\nparameter optimisation and physical limitations such as gate noise, restricted\nqubit connectivity, and state-preparation-and-measurement errors, limiting\nimplementations to shallow depths. This work focuses on QAOA$_1$ (QAOA at\n$p=1$) for QUBO problems, represented as Ising models. Despite QAOA$_1$ having\nonly two parameters, $(\\gamma, \\beta)$, we show that their optimisation is\nchallenging due to a highly oscillatory landscape, with oscillation rates\nincreasing with the problem size, density, and weight. This behaviour\nnecessitates high-resolution grid searches to avoid distortion of cost\nlandscapes that may result in inaccurate minima. We propose an efficient\noptimisation strategy that reduces the two-dimensional $(\\gamma, \\beta)$ search\nto a one-dimensional search over $\\gamma$, with $\\beta^*$ computed\nanalytically. We establish the maximum permissible sampling period required to\naccurately map the $\\gamma$ landscape and provide an algorithm to estimate the\noptimal parameters in polynomial time. Furthermore, we rigorously prove that\nfor regular graphs on average, the globally optimal $\\gamma^* \\in \\mathbb{R}^+$\nvalues are concentrated very close to zero and coincide with the first local\noptimum, enabling gradient descent to replace exhaustive line searches. This\napproach is validated using Recursive QAOA (RQAOA), where it consistently\noutperforms both coarsely optimised RQAOA and semidefinite programs across all\ntested QUBO instances.",
        "Random many-body states are both a useful tool to model certain physical\nsystems and an important asset for quantum computation. Realising them,\nhowever, generally requires an exponential (in system size) amount of\nresources. Recent research has presented a way out by showing that one can\ngenerate random states, or more precisely a controlled approximation of them,\nby applying a quantum circuit built in terms of few-body unitary gates. Most of\nthis research, however, has been focussed on the case of quantum circuits\ncomposed by completely random unitary gates. Here we consider what happens for\ncircuits that, instead, involve a minimal degree of randomness. Specifically,\nwe concentrate on two different settings: (a) brickwork quantum circuits with a\nsingle one-qudit random matrix at a boundary; (b) brickwork quantum circuits\nwith fixed interactions but random one-qudit gates everywhere. We show that,\nfor any given initial state, (a) and (b) produce a distribution of states\napproaching the Haar distribution in the limit of large circuit depth. More\nprecisely, we show that the moments of the distribution produced by our\ncircuits can approximate the ones of the Haar distribution in a depth\nproportional to the system size. Interestingly we find that in both Cases (a)\nand (b) the relaxation to the Haar distribution occurs in two steps - this is\nin contrast with what happens in fully random circuits. Moreover, we show that\nchoosing appropriately the fixed interactions, for example taking the local\ngate to be a dual-unitary gate with high enough entangling power, minimally\nrandom circuits produce a Haar random distribution more rapidly than fully\nrandom circuits. In particular, dual-unitary circuits with maximal entangling\npower - i.e. perfect tensors - appear to provide the optimal quantum state\ndesign preparation for any design number.",
        "In this paper, we study coupled complex modified Korteweg-de Vries (ccmKdV)\nequation by combining the Hirota's method and the Kadomtsev-Petviashvili (KP)\nreduction method. First, we show that the bilinear form of the ccmKdV equation\nunder nonzero boundary condition is linked to the discrete BKP hierarchy\nthrough Miwa transformation. Based on this finding, we construct the dark-dark\nsoliton solution in the pfaffian form. The dynamical behaviors for one- and\ntwo-soliton are analyzed and illustrated.",
        "Assuming that the two integrals in the Change of Variable Formula for the\nunidimensional Riemann integral are finite, one can ask if they have equal\nvalue. We give a positive answer to this question. The proof is very easy to\nfollow and to keep in mind. An example is given.",
        "We introduce and investigate the category $\\mathsf{AtoMon}$ of atomic monoids\nand atom-preserving monoid homomorphisms, which is a (non-full) subcategory of\nthe usual category of monoids. In particular, we compute all limits and\ncolimits, showing that $\\mathsf{AtoMon}$ is a complete and cocomplete category.\nWe also address certain arithmetic properties of products and coproducts,\nproviding explicit formulas for some fundamental invariants associated with\nfactorization lengths in atomic monoids.",
        "Interplay of lattice, orbital, and charge degrees of freedom in complex oxide\nmaterials has hosted a plethora of exotic quantum phases and physical\nproperties. Recent advances in synthesis of freestanding complex oxide\nmembranes and twisted heterostructures assembled from membranes provide new\nopportunities for discovery using moir\\'e design with local lattice control. To\nthis end, we designed moir\\'e crystals at the coincidence site lattice\ncondition, providing commensurate structure within the moir\\'e supercell\narising from the multi-atom complex oxide unit cell. We fabricated such twisted\nbilayers from freestanding SrTiO3 membranes and used depth sectioning-based TEM\nmethods to discover ordered charge states at the moir\\'e interface. By\nselectively imaging SrTiO3 atomic planes at different depths through the\nbilayer, we clearly resolved the moir\\'e periodic structure at the twisted\ninterface and found that it exhibits lattice-dependent charge\ndisproportionation in the local atomic registry within the moir\\'e supercell.\nOur density-functional modelling of the twisted oxide interface predicts that\nthese moir\\'e phenomena are accompanied by the emergence of a two-dimensional\nflat band that can drive new electronic phases. Our work provides a novel\nguideline for controlling moir\\'e periodicity in twisted oxides and opens\npathways to exploit the new functionalities via moir\\'e lattice-driven\ncharge-orbital correlation.",
        "Tailoring treatments to individual needs is a central goal in fields such as\nmedicine. A key step toward this goal is estimating Heterogeneous Treatment\nEffects (HTE) - the way treatments impact different subgroups. While crucial,\nHTE estimation is challenging with survival data, where time until an event\n(e.g., death) is key. Existing methods often assume complete observation, an\nassumption violated in survival data due to right-censoring, leading to bias\nand inefficiency. Cui et al. (2023) proposed a doubly-robust method for HTE\nestimation in survival data under no hidden confounders, combining a causal\nsurvival forest with an augmented inverse-censoring weighting estimator.\nHowever, we find it struggles under heavy censoring, which is common in\nrare-outcome problems such as Amyotrophic lateral sclerosis (ALS). Moreover,\nmost current methods cannot handle instrumental variables, which are a crucial\ntool in the causal inference arsenal. We introduce Multiple Imputation for\nSurvival Treatment Response (MISTR), a novel, general, and non-parametric\nmethod for estimating HTE in survival data. MISTR uses recursively imputed\nsurvival trees to handle censoring without directly modeling the censoring\nmechanism. Through extensive simulations and analysis of two real-world\ndatasets-the AIDS Clinical Trials Group Protocol 175 and the Illinois\nunemployment dataset we show that MISTR outperforms prior methods under heavy\ncensoring in the no-hidden-confounders setting, and extends to the instrumental\nvariable setting. To our knowledge, MISTR is the first non-parametric approach\nfor HTE estimation with unobserved confounders via instrumental variables.",
        "This study conducts a meticulous examination of the cosmological implications\ninherent in Rastall gravity and $f(R)$ gravity models, assessing their efficacy\nacross distinct cosmic epochs, from early universe structure formation to\nlate-time acceleration. In the initial stages, both models exhibit commendable\ncompatibility with observed features of structure formation, aligning with the\nestablished $\\Lambda$CDM model. The derived Jeans' wavenumbers for each model\nsupport their viability. However, as the cosmic timeline progresses into the\nlate universe, a discernible disparity surfaces. Utilizing the Markov Chain\nMonte Carlo method, we reconstruct the deceleration parameter $(q)$ and\nidentify Deceleration - Acceleration redshift transition values. For $f(R)$\ngravity, our results align closely with previous studies, emphasizing its\nsuperior ability to elucidate the recent cosmic acceleration. In contrast,\nRastall gravity exhibits distinct redshift transition values. Our rigorous\nanalysis underscores the prowess of $f(R)$ gravity in capturing the observed\ncosmic acceleration, positioning it as a compelling alternative to the\nconventional $\\Lambda$CDM model. The discernible shifts observed in the peaks\nof the CMB power spectrum and evolution of deceleration parameter (q) for both\n$f(R)$ gravity and Rastall gravity models in the Early and Late universe, in\nrelation to the $\\Lambda $CDM model, provide compelling evidence supporting the\nproposition that these alternative gravity models can account for the\nanisotropy of the universe without invoking the need for dark energy.",
        "We present the study of the gas kinematics in narrow-line regions (NLRs) of\n2,009 type-2 AGNs at $z<0.34$. We construct the [O III]$\\lambda$5007\nemission-line images using publicly available broadband images from the Sloan\nDigital Sky Survey (SDSS). The [O III] emission area of the samples, measured\ndown to $1.7\\times10^{-15}$ erg\/s\/cm$^2$\/arcsec$^2$, ranges from 3.7 kpc$^2$ up\nto 224 kpc$^2$. With our broadband technique, we found the strong correlation\nbetween [O III] area and AGN luminosity inferred from the [O III] luminosity\nand the mid-infrared luminosity at the rest-frame $15\\mu$m. The isophotal\nthreshold used to determine the [O III] area affects the correlation strength\nin that the brighter isophote yields the stronger correlation between the [O\nIII] area and AGN luminosity. The presence of gas outflow is examined by the\nratio of the [O III] velocity dispersion to the stellar velocity dispersion\n($\\sigma_{\\rm [O\\,III]}\/\\sigma_\\star > 1.4$) using the SDSS spectra. At the\ngiven luminosity, the objects with and without outflows exhibit the same\nextension of the [O III] emission. Their correlation between the [O III] area\nand luminosity is almost identical. It is suggested that the size of NLRs is\nnot affected by outflow mechanisms but rather by photoionization from the\ncentral AGNS.",
        "The gravitational wave (GW) event S230518h is a potential binary neutron\nstar-black hole merger (NSBH) event that was detected during engineering run 15\n(ER15), which served as the commissioning period before the LIGO-Virgo-KAGRA\n(LVK) O4a observing run. Despite its low probability of producing detectable\nelectromagnetic emissions, we performed extensive follow-up observations of\nthis event using the GECKO telescopes in the southern hemisphere. Our\nobservation covered 61.7\\% of the 90\\% credible region, a $\\rm 284\\:deg^2$ area\naccessible from the southern hemisphere, reaching a median limiting magnitude\nof $R=21.6$ mag. In these images, we conducted a systematic search for an\noptical counterpart of this event by combining a CNN-based classifier and human\nverification. We identified 128 transient candidates, but no significant\noptical counterpart was found that could have caused the GW signal.\nFurthermore, we provide feasible KN properties that are consistent with the\nupper limits of observation. Although no optical counterpart was found, our\nresult demonstrates both GECKO's efficient wide-field follow-up capabilities\nand usefulness for constraining properties of kilonovae from NSBH mergers at\ndistances of $\\sim 200$ Mpc."
      ]
    }
  },
  {
    "id":2411.02815,
    "research_type":"applied",
    "start_id":"b36",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
      ],
      "abstract":[
        "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "The three-dimensional impulse-response model: Modeling the training\n  process in accordance with energy system-specific adaptation",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Engineered Zwitterion-Infused Clay Composites with Antibacterial and\n  Antifungal Efficacy",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "A note on Dirichlet-like series attached to polynomials",
        "Quasiconformal Maps between Bowditch Boundaries of Relatively Hyperbolic\n  Groups",
        "Intrinsic low-temperature magnetic properties on the ultra-clean UTe$_2$\n  with $T_{\\rm c}$ = 2.1 K revealed by $^{125}$Te NMR",
        "A note on Centaur geometry -- probing IR de Sitter spacetime holography",
        "Quasiparticle poisoning of superconducting qubits with active gamma\n  irradiation",
        "Further applications of the Nehari manifold method to functionals in\n  $C^1(X \\setminus \\{0\\})$",
        "Effect of Accelerated Thermal Degradation of Poly(Vinyl Chloride): The\n  Case of Unplasticized PVC",
        "Scalar Field Kantowski--Sachs Solutions in Teleparallel $F(T)$ Gravity",
        "Testing the Homogeneity of Two Proportions for Correlated Bilateral Data\n  via the Clayton Copula",
        "Bounds on Elliptic Sombor and Euler Sombor indices of join and corona\n  product of graphs",
        "Equivalence between top-down and bottom-up holographic approaches",
        "Gluing invariants of Donaldson--Thomas type -- Part II: Matrix\n  factorizations",
        "Dynamics of a Family of Rational Operators of Arbitrary Degree",
        "Analytical control of the exchange interaction in periodically driven\n  Mott insulators",
        "The late-time heating Green's function and improvements to distortion\n  frequency hierarchy treatment"
      ],
      "abstract":[
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "Athletic training is characterized by physiological systems responding to\nrepeated exercise-induced stress, resulting in gradual alterations in the\nfunctional properties of these systems. The adaptive response leading to\nimproved performance follows a remarkably predictable pattern that may be\ndescribed by a systems model provided that training load can be accurately\nquantified and that the constants defining the training-performance\nrelationship are known. While various impulse-response models have been\nproposed, they are inherently limited in reducing training stress (the impulse)\ninto a single metric, assuming that the adaptive responses are independent of\nthe type of training performed. This is despite ample evidence of markedly\ndiverse acute and chronic responses to exercise of different intensities and\ndurations. Herein, we propose an alternative, three-dimensional\nimpulse-response model that uses three training load metrics as inputs and\nthree performance metrics as outputs. These metrics, represented by a\nthree-parameter critical power model, reflect the stress imposed on each of the\nthree energy systems: the alactic (phosphocreatine\/immediate) system; the\nlactic (glycolytic) system; and the aerobic (oxidative) system. The purpose of\nthis article is to outline the scientific rationale and the practical\nimplementation of the three-dimensional impulse-response model.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Microbes and pathogens play a detrimental role in healing wounds, causing\ninfections like impetigo through bodily fluids and skin and entering the\nbloodstream through the wounds, thereby hindering the healing process and\ntissue regeneration. Clay, known for its long history of natural therapeutic\nuse, has emerged as one of the most promising candidates for biomedical\napplications due to its non-toxic nature, porosity, high surface area,\nubiquity, and excellent cation exchange capacity. This study demonstrates an\ninnovative approach to engineering an organo-functionalized,\ninfection-resistant, easy-to-use bandage material from clay, an environmentally\nbenign and sustainable material. The hybrid membranes have been developed using\nclays, zwitterions, silver ions, and terbinafine hydrochloride (TBH) to impart\nantibacterial and antifungal efficacy. A critical aspect of this study is\nembedding organic molecules and metal ions with the clays and releasing them to\nresist the growth and kill the pathogens. The antimicrobial efficacy of the\nmembranes has been tested using a zone of inhibition study against the most\ncommon microbes in skin wounds, viz. S. aureus, E. coli, and C. albicans.\nResults from our studies not only demonstrate the potential of these hybrid\nclay membranes as a cost-effective, scalable, and effective solution for\ntreating microbial infections but also instill newer avenues for point-of-care\nwound-healing treatments, offering hope for improved patient outcomes.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Some Dirichlet-like functions, attached to a pair (periodic function,\npolynomial) are introduced and studied. These functions generalize the standard\nDirichlet L-functions of Dirichlet characters. They have similar properties,\nbeing holomorphic on thefull complex plane and having simple values on negative\nintegers.",
        "Classifying groups up to quasi-isometry is a fundamental problem in geometric\ngroup theory. In the context of hyperbolic and relatively hyperbolic groups,\none of the key invariants in this classification is the boundary at infinity.\nF. Paulin proved that two hyperbolic groups are quasi-isometric if and only if\ntheir Gromov boundaries are quasiconformally equivalent. In this article, we\nextend Paulin's result to relatively hyperbolic groups and their Bowditch\nboundaries.\n  A notion of quasiconformal map preserving the shadows of horoballs relative\nto a point at the Bowditch boundary is defined and we have shown that every\ncoarsely cusp-preserving quasi-isometry between two relatively hyperbolic\ngroups induces a shadow-preserving quasiconformal map between their Bowditch\nboundaries. Conversely, we have shown that if the Bowditch boundaries of two\nrelatively hyperbolic groups are quasiconformally equivalent and the\nquasiconformal map coarsely preserves the shadows of horoballs relative to each\nboundary point, then the quasiconformal map induces a coarsely cusp-preserving\nquasi-isometry between those groups.",
        "To investigate the intrinsic magnetic properties of UTe$_2$, we performed\n$^{125}$Te-NMR measurements on the ultra-clean single-crystalline UTe$_2$ with\nsuperconducting transition temperature $T_{\\rm c}$ = 2.1~K and compared the\nresults with those of the $T_{\\rm c}$ = 1.6~K sample. The broadening of the\nlinewidth of the NMR spectrum in the $a$-axis magnetic field and the\nlow-temperature magnetic fluctuations observed in the 1.6~K sample are\nsuppressed in the ultra-clean sample, indicating that such magnetic properties\noriginate from a tiny amount of U deficiency. The present results suggest that\nthe magnetic properties in UTe$_2$ are sensitive to the U deficiency. We also\nobserved a peculiar angular dependence of the NMR quantities due to large\nmagnetic anisotropy with the $a$-axis as the magnetic easy axis.",
        "We explore a Centaur geometry in JT gravity, which is an asymptotically AdS\nspacetime but in the IR admits a dS bubble with another AdS geometry in the\ndeep IR. Thus, this geometry admits a holographic dual in the sense that it is\nasymptotically AdS. In an attempt to understand this geometry, we calculate the\ndensity of states of the putative boundary dual for such mixed geometries by\nevaluating the on-shell action. We compute the density of states analytically\nin the classical limit. The resultant density of states suggest that the\ndegrees of freedom in the IR are reduced in such a putative boundary theory due\nto the IR modification corresponding to the dS bubble.",
        "When a high-energy particle, such as a $\\gamma$-ray or muon, impacts the\nsubstrate of a superconducting qubit chip, large numbers of electron-hole pairs\nand phonons are created. The ensuing dynamics of the electrons and holes\nchanges the local offset-charge environment for qubits near the impact site.\nThe phonons that are produced have energy above the superconducting gap in the\nfilms that compose the qubits, leading to quasiparticle excitations above the\nsuperconducting ground state when the phonons impinge on the qubit electrodes.\nAn elevated density of quasiparticles degrades qubit coherence, leading to\nerrors in qubit arrays. Because these pair-breaking phonons spread throughout\nmuch of the chip, the errors can be correlated across a large portion of the\narray, posing a significant challenge for quantum error correction. In order to\nstudy the dynamics of $\\gamma$-ray impacts on superconducting qubit arrays, we\nuse a $\\gamma$-ray source outside the dilution refrigerator to controllably\nirradiate our devices. By using charge-sensitive transmon qubits, we can\nmeasure both the offset-charge shifts and quasiparticle poisoning due to the\n$\\gamma$ irradiation at different doses. We study correlations between\noffset-charge shifts and quasiparticle poisoning for different qubits in the\narray and compare this with numerical modeling of charge and phonon dynamics\nfollowing a $\\gamma$-ray impact. We thus characterize the poisoning footprint\nof these impacts and quantify the performance of structures for mitigating\nphonon-mediated quasiparticle poisoning.",
        "We proceed with the study of the Nehari manifold method for functionals in\n$C^1(X \\setminus \\{0\\})$, where $X$ is a Banach space. We deal now with\nfunctionals whose fibering maps have two critical points (a minimiser followed\nby a maximiser). Under some additional conditions we show that the Nehari\nmanifold method provides us with the ground state level and two sequences of\ncritical values for these functionals. These results are applied to the class\nof {\\it prescribed energy problems} as well as to the concave-convex problem\nfor the {\\it affine} $p$-Laplacian operator.",
        "The thermal degradation of unplasticized poly(vinyl chloride), PVC, was\ncomprehensively investigated through the application of spectroscopic\ntechniques, as well as contact angle measurements (CA), dynamic mechanical\nanalysis (DMA), and size-exclusion chromatography (SEC). To study the effect of\nrelative humidity (RH) on the deterioration of unplasticized PVC, two regimes\nof accelerated degradation experiments were selected: low RH (max. 30% RH) and\nhigh RH = 60% levels, which corresponds to usually the highest RH in heritage\ninstitutions equipped with an HVAC system. Nuclear magnetic resonance (NMR) and\ninfrared spectroscopy (FTIR) did not reveal any significant changes in the\nmaterial during its degradation up to 20 weeks at temperatures ranging from\n60{\\deg}C to 80{\\deg}C. Notable changes were observed in the Raman and UV-Vis\nspectra, indicative of the formation of conjugated carbon-carbon double bonds.\nThe formation of polyenes was responsible for the yellowing of samples.\nNotwithstanding, the aforementioned changes did not lead to a notable decline\nin the mechanical properties, as evidenced by DMA and SEC measurements. EPR\nmeasurements demonstrated the formation of 2 radicals at 60{\\deg}C, and in the\nsample degraded at 80{\\deg}C the presence of radicals was evident. This\nindicates that a radical degradation mechanism cannot be excluded even at such\nlow temperatures.",
        "In this paper, we investigate time-dependent Kantowski--Sachs spherically\nsymmetric teleparallel $F(T)$ gravity with a scalar field source. We begin by\nsetting the exact field equations to be solved and solve conservation laws for\npossible scalar field potential, $V\\left(\\phi\\right)$, solutions. Then, we find\nnew non-trivial teleparallel $F(T)$ solutions by using power-law and\nexponential ansatz for each potential case arising from conservation laws, such\nas linear, quadratic, or logarithmic, to name a few. We find a general formula\nallowing us to compute all possible new teleparallel $F(T)$ solutions\napplicable for any scalar field potential and ansatz. Then, we apply this\nformula and find a large number of exact and approximate new teleparallel\n$F(T)$ solutions for several types of cases. Some new $F(T)$ solution classes\nmay be relevant for future cosmological applications, especially concerning\ndark matter, dark energy quintessence, phantom energy leading to the Big Rip\nevent, and quintom models of physical processes.",
        "Handling highly dependent data is crucial in clinical trials, particularly in\nfields related to ophthalmology. Incorrectly specifying the dependency\nstructure can lead to biased inferences. Traditionally, models rely on three\nfixed dependence structures, which lack flexibility and interpretation. In this\narticle, we propose a framework using a more general model -- copulas -- to\nbetter account for dependency. We assess the performance of three different\ntest statistics within the Clayton copula setting to demonstrate the\nframework's feasibility. Simulation results indicate that this method controls\ntype I error rates and achieves reasonable power, providing a solid benchmark\nfor future research and broader applications. Additionally, we present analyses\nof two real-world datasets as case studies.",
        "The Elliptic Somber and Euler Somber indices are newly defined topological\nindices based on the Somber index. Our paper presents calculations of the upper\nand lower bounds of these indices for the join and corona product of arbitrary\ngraphs. Furthermore, we demonstrate that these bounds are attained when both\ngraphs are regular.",
        "This work raises the question of whether finding an equivalent bottom-up\ndescription to a given top-down one is possible. We consider the vector meson\nspectrum derived in the D3\/D7 system to answer this question. Using WKB\nanalysis, we reconstruct a bottom-up confining potential that resembles the\ngeometric structure of the so-called hardwall model. We compute some properties\nfor this bottom-up model, including the thermal deconfinement phase transition,\nthe $\\rho$ radial Regge trajectory, and the configurational entropy.",
        "This paper is a follow-up to arXiv:2407.08471. Let $X$ be a a $(-1)$-shifted\nsymplectic derived Deligne--Mumford stack. Thanks to the Darboux lemma of\nBrav--Bussi--Joyce, $X$ is locally modeled by derived critical loci of a\nfunction $f$ on a smooth scheme $U$. In this paper we study the gluing of the\nlocally defined $2$-periodic (big) dg-categories of matrix factorizations\n$MF^\\infty(U,f)$. We show that these come canonically equipped with a structure\nof a $2$-periodic crystal of categories (\\ie an action of the dg-category of\n$2$-periodic $D$-modules on $X$) compatible with a relative Thom--Sebastiani\ntheorem expressing the equivariance under the action of quadratic bundles.\n  As our main theorem we show that the locally defined categories\n$MF^\\infty(U,f)$ can be glued along $X$ as a sheaf of crystals of 2-periodic\ndg-categories ``up to isotopy'', under the prescription of orientation data\ncontrolled by three obstruction classes. This result generalizes the gluing of\nthe Joyce's perverse sheaf of vanishing cycles and partially answers\nconjectures by Kontsevich--Soibelman and Toda in motivic Donaldson--Thomas\ntheory.",
        "In this paper we analyse the dynamics of a family of rational operators\ncoming from a fourth-order family of root-finding algorithms. We first show\nthat it may be convenient to redefine the parameters to prevent redundancies\nand unboundedness of problematic parameters. After reparametrization, we\nobserve that these rational maps belong to a more general family $O_{a,n,k}$ of\ndegree $n+k$ operators, which includes several other families of maps obtained\nfrom other numerical methods. We study the dynamics of $O_{a,n,k}$ and discuss\nfor which parameters $n$ and $k$ these operators would be suitable from the\nnumerical point of view.",
        "The manipulation of electronic structure through periodic electric fields\nenables the reversible control of effective interactions in extended\nantiferromagnetic Mott insulators on ultrafast timescales. A careful analytical\nexamination of the modulated effective interactions is conducted, accurately\ncharacterising it through the use of exact summation formulas and Bessel\nfunctions. As a result, time reversals are analytically determined in terms of\nBessel zeroes. We discuss the half-filled Hubbard model, as well as\nmulti-orbital models, various characteristics of the Kitaev-Heisenberg model,\nand the emergence of chiral spin terms.",
        "Early energy injection leaves an imprint on the observed blackbody spectrum\nof the CMB, allowing us to study the thermal history of the Universe. For small\nenergy release, the distortion can be efficiently computed using the\nquasi-exact Green's function method. For pre-recombination injections, the\nGreen's function has already been studied previously. Here we reconsider the\npre- and post-recombination periods, showcasing both the spectral distortion\nintensity and the relative temperature difference, which encrypt precious\ninformation about physical processes such as free-free interactions and thermal\ndecoupling. We present the associated distortion visibility function,\ninvestigating the impact of various physical effects. We then study\nimprovements to the so-called frequency hierarchy (FH) treatment, a method that\nwas developed for the modelling of anisotropic distortions, which like the\naverage distortion signals encode valuable cosmological information.\nSpecifically, the FH treatment has shortcomings even in the $\\mu$ era, that in\nprinciple should be easy to overcome. In this paper, we introduce a new\napproach to reduce the mismatch, concluding with a redefinition of the $\\mu$\nspectral shape using CosmoTherm. This solution takes into account double\nCompton and Bremsstrahlung effects in the low tail, which can be included in\nthe FH. This opens the path towards a refined modeling of spectral distortion\nanisotropies."
      ]
    }
  },
  {
    "id":2411.02815,
    "research_type":"applied",
    "start_id":"b33",
    "start_title":"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network",
    "start_abstract":"We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.",
    "start_categories":[
      "cs.CV",
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
      ],
      "abstract":[
        "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "The three-dimensional impulse-response model: Modeling the training\n  process in accordance with energy system-specific adaptation",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Engineered Zwitterion-Infused Clay Composites with Antibacterial and\n  Antifungal Efficacy",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "A note on Dirichlet-like series attached to polynomials",
        "Quasiconformal Maps between Bowditch Boundaries of Relatively Hyperbolic\n  Groups",
        "Intrinsic low-temperature magnetic properties on the ultra-clean UTe$_2$\n  with $T_{\\rm c}$ = 2.1 K revealed by $^{125}$Te NMR",
        "A note on Centaur geometry -- probing IR de Sitter spacetime holography",
        "Quasiparticle poisoning of superconducting qubits with active gamma\n  irradiation",
        "Further applications of the Nehari manifold method to functionals in\n  $C^1(X \\setminus \\{0\\})$",
        "Effect of Accelerated Thermal Degradation of Poly(Vinyl Chloride): The\n  Case of Unplasticized PVC",
        "Scalar Field Kantowski--Sachs Solutions in Teleparallel $F(T)$ Gravity",
        "Testing the Homogeneity of Two Proportions for Correlated Bilateral Data\n  via the Clayton Copula",
        "Bounds on Elliptic Sombor and Euler Sombor indices of join and corona\n  product of graphs",
        "Equivalence between top-down and bottom-up holographic approaches",
        "Gluing invariants of Donaldson--Thomas type -- Part II: Matrix\n  factorizations",
        "Dynamics of a Family of Rational Operators of Arbitrary Degree",
        "Analytical control of the exchange interaction in periodically driven\n  Mott insulators",
        "The late-time heating Green's function and improvements to distortion\n  frequency hierarchy treatment"
      ],
      "abstract":[
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "Athletic training is characterized by physiological systems responding to\nrepeated exercise-induced stress, resulting in gradual alterations in the\nfunctional properties of these systems. The adaptive response leading to\nimproved performance follows a remarkably predictable pattern that may be\ndescribed by a systems model provided that training load can be accurately\nquantified and that the constants defining the training-performance\nrelationship are known. While various impulse-response models have been\nproposed, they are inherently limited in reducing training stress (the impulse)\ninto a single metric, assuming that the adaptive responses are independent of\nthe type of training performed. This is despite ample evidence of markedly\ndiverse acute and chronic responses to exercise of different intensities and\ndurations. Herein, we propose an alternative, three-dimensional\nimpulse-response model that uses three training load metrics as inputs and\nthree performance metrics as outputs. These metrics, represented by a\nthree-parameter critical power model, reflect the stress imposed on each of the\nthree energy systems: the alactic (phosphocreatine\/immediate) system; the\nlactic (glycolytic) system; and the aerobic (oxidative) system. The purpose of\nthis article is to outline the scientific rationale and the practical\nimplementation of the three-dimensional impulse-response model.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Microbes and pathogens play a detrimental role in healing wounds, causing\ninfections like impetigo through bodily fluids and skin and entering the\nbloodstream through the wounds, thereby hindering the healing process and\ntissue regeneration. Clay, known for its long history of natural therapeutic\nuse, has emerged as one of the most promising candidates for biomedical\napplications due to its non-toxic nature, porosity, high surface area,\nubiquity, and excellent cation exchange capacity. This study demonstrates an\ninnovative approach to engineering an organo-functionalized,\ninfection-resistant, easy-to-use bandage material from clay, an environmentally\nbenign and sustainable material. The hybrid membranes have been developed using\nclays, zwitterions, silver ions, and terbinafine hydrochloride (TBH) to impart\nantibacterial and antifungal efficacy. A critical aspect of this study is\nembedding organic molecules and metal ions with the clays and releasing them to\nresist the growth and kill the pathogens. The antimicrobial efficacy of the\nmembranes has been tested using a zone of inhibition study against the most\ncommon microbes in skin wounds, viz. S. aureus, E. coli, and C. albicans.\nResults from our studies not only demonstrate the potential of these hybrid\nclay membranes as a cost-effective, scalable, and effective solution for\ntreating microbial infections but also instill newer avenues for point-of-care\nwound-healing treatments, offering hope for improved patient outcomes.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Some Dirichlet-like functions, attached to a pair (periodic function,\npolynomial) are introduced and studied. These functions generalize the standard\nDirichlet L-functions of Dirichlet characters. They have similar properties,\nbeing holomorphic on thefull complex plane and having simple values on negative\nintegers.",
        "Classifying groups up to quasi-isometry is a fundamental problem in geometric\ngroup theory. In the context of hyperbolic and relatively hyperbolic groups,\none of the key invariants in this classification is the boundary at infinity.\nF. Paulin proved that two hyperbolic groups are quasi-isometric if and only if\ntheir Gromov boundaries are quasiconformally equivalent. In this article, we\nextend Paulin's result to relatively hyperbolic groups and their Bowditch\nboundaries.\n  A notion of quasiconformal map preserving the shadows of horoballs relative\nto a point at the Bowditch boundary is defined and we have shown that every\ncoarsely cusp-preserving quasi-isometry between two relatively hyperbolic\ngroups induces a shadow-preserving quasiconformal map between their Bowditch\nboundaries. Conversely, we have shown that if the Bowditch boundaries of two\nrelatively hyperbolic groups are quasiconformally equivalent and the\nquasiconformal map coarsely preserves the shadows of horoballs relative to each\nboundary point, then the quasiconformal map induces a coarsely cusp-preserving\nquasi-isometry between those groups.",
        "To investigate the intrinsic magnetic properties of UTe$_2$, we performed\n$^{125}$Te-NMR measurements on the ultra-clean single-crystalline UTe$_2$ with\nsuperconducting transition temperature $T_{\\rm c}$ = 2.1~K and compared the\nresults with those of the $T_{\\rm c}$ = 1.6~K sample. The broadening of the\nlinewidth of the NMR spectrum in the $a$-axis magnetic field and the\nlow-temperature magnetic fluctuations observed in the 1.6~K sample are\nsuppressed in the ultra-clean sample, indicating that such magnetic properties\noriginate from a tiny amount of U deficiency. The present results suggest that\nthe magnetic properties in UTe$_2$ are sensitive to the U deficiency. We also\nobserved a peculiar angular dependence of the NMR quantities due to large\nmagnetic anisotropy with the $a$-axis as the magnetic easy axis.",
        "We explore a Centaur geometry in JT gravity, which is an asymptotically AdS\nspacetime but in the IR admits a dS bubble with another AdS geometry in the\ndeep IR. Thus, this geometry admits a holographic dual in the sense that it is\nasymptotically AdS. In an attempt to understand this geometry, we calculate the\ndensity of states of the putative boundary dual for such mixed geometries by\nevaluating the on-shell action. We compute the density of states analytically\nin the classical limit. The resultant density of states suggest that the\ndegrees of freedom in the IR are reduced in such a putative boundary theory due\nto the IR modification corresponding to the dS bubble.",
        "When a high-energy particle, such as a $\\gamma$-ray or muon, impacts the\nsubstrate of a superconducting qubit chip, large numbers of electron-hole pairs\nand phonons are created. The ensuing dynamics of the electrons and holes\nchanges the local offset-charge environment for qubits near the impact site.\nThe phonons that are produced have energy above the superconducting gap in the\nfilms that compose the qubits, leading to quasiparticle excitations above the\nsuperconducting ground state when the phonons impinge on the qubit electrodes.\nAn elevated density of quasiparticles degrades qubit coherence, leading to\nerrors in qubit arrays. Because these pair-breaking phonons spread throughout\nmuch of the chip, the errors can be correlated across a large portion of the\narray, posing a significant challenge for quantum error correction. In order to\nstudy the dynamics of $\\gamma$-ray impacts on superconducting qubit arrays, we\nuse a $\\gamma$-ray source outside the dilution refrigerator to controllably\nirradiate our devices. By using charge-sensitive transmon qubits, we can\nmeasure both the offset-charge shifts and quasiparticle poisoning due to the\n$\\gamma$ irradiation at different doses. We study correlations between\noffset-charge shifts and quasiparticle poisoning for different qubits in the\narray and compare this with numerical modeling of charge and phonon dynamics\nfollowing a $\\gamma$-ray impact. We thus characterize the poisoning footprint\nof these impacts and quantify the performance of structures for mitigating\nphonon-mediated quasiparticle poisoning.",
        "We proceed with the study of the Nehari manifold method for functionals in\n$C^1(X \\setminus \\{0\\})$, where $X$ is a Banach space. We deal now with\nfunctionals whose fibering maps have two critical points (a minimiser followed\nby a maximiser). Under some additional conditions we show that the Nehari\nmanifold method provides us with the ground state level and two sequences of\ncritical values for these functionals. These results are applied to the class\nof {\\it prescribed energy problems} as well as to the concave-convex problem\nfor the {\\it affine} $p$-Laplacian operator.",
        "The thermal degradation of unplasticized poly(vinyl chloride), PVC, was\ncomprehensively investigated through the application of spectroscopic\ntechniques, as well as contact angle measurements (CA), dynamic mechanical\nanalysis (DMA), and size-exclusion chromatography (SEC). To study the effect of\nrelative humidity (RH) on the deterioration of unplasticized PVC, two regimes\nof accelerated degradation experiments were selected: low RH (max. 30% RH) and\nhigh RH = 60% levels, which corresponds to usually the highest RH in heritage\ninstitutions equipped with an HVAC system. Nuclear magnetic resonance (NMR) and\ninfrared spectroscopy (FTIR) did not reveal any significant changes in the\nmaterial during its degradation up to 20 weeks at temperatures ranging from\n60{\\deg}C to 80{\\deg}C. Notable changes were observed in the Raman and UV-Vis\nspectra, indicative of the formation of conjugated carbon-carbon double bonds.\nThe formation of polyenes was responsible for the yellowing of samples.\nNotwithstanding, the aforementioned changes did not lead to a notable decline\nin the mechanical properties, as evidenced by DMA and SEC measurements. EPR\nmeasurements demonstrated the formation of 2 radicals at 60{\\deg}C, and in the\nsample degraded at 80{\\deg}C the presence of radicals was evident. This\nindicates that a radical degradation mechanism cannot be excluded even at such\nlow temperatures.",
        "In this paper, we investigate time-dependent Kantowski--Sachs spherically\nsymmetric teleparallel $F(T)$ gravity with a scalar field source. We begin by\nsetting the exact field equations to be solved and solve conservation laws for\npossible scalar field potential, $V\\left(\\phi\\right)$, solutions. Then, we find\nnew non-trivial teleparallel $F(T)$ solutions by using power-law and\nexponential ansatz for each potential case arising from conservation laws, such\nas linear, quadratic, or logarithmic, to name a few. We find a general formula\nallowing us to compute all possible new teleparallel $F(T)$ solutions\napplicable for any scalar field potential and ansatz. Then, we apply this\nformula and find a large number of exact and approximate new teleparallel\n$F(T)$ solutions for several types of cases. Some new $F(T)$ solution classes\nmay be relevant for future cosmological applications, especially concerning\ndark matter, dark energy quintessence, phantom energy leading to the Big Rip\nevent, and quintom models of physical processes.",
        "Handling highly dependent data is crucial in clinical trials, particularly in\nfields related to ophthalmology. Incorrectly specifying the dependency\nstructure can lead to biased inferences. Traditionally, models rely on three\nfixed dependence structures, which lack flexibility and interpretation. In this\narticle, we propose a framework using a more general model -- copulas -- to\nbetter account for dependency. We assess the performance of three different\ntest statistics within the Clayton copula setting to demonstrate the\nframework's feasibility. Simulation results indicate that this method controls\ntype I error rates and achieves reasonable power, providing a solid benchmark\nfor future research and broader applications. Additionally, we present analyses\nof two real-world datasets as case studies.",
        "The Elliptic Somber and Euler Somber indices are newly defined topological\nindices based on the Somber index. Our paper presents calculations of the upper\nand lower bounds of these indices for the join and corona product of arbitrary\ngraphs. Furthermore, we demonstrate that these bounds are attained when both\ngraphs are regular.",
        "This work raises the question of whether finding an equivalent bottom-up\ndescription to a given top-down one is possible. We consider the vector meson\nspectrum derived in the D3\/D7 system to answer this question. Using WKB\nanalysis, we reconstruct a bottom-up confining potential that resembles the\ngeometric structure of the so-called hardwall model. We compute some properties\nfor this bottom-up model, including the thermal deconfinement phase transition,\nthe $\\rho$ radial Regge trajectory, and the configurational entropy.",
        "This paper is a follow-up to arXiv:2407.08471. Let $X$ be a a $(-1)$-shifted\nsymplectic derived Deligne--Mumford stack. Thanks to the Darboux lemma of\nBrav--Bussi--Joyce, $X$ is locally modeled by derived critical loci of a\nfunction $f$ on a smooth scheme $U$. In this paper we study the gluing of the\nlocally defined $2$-periodic (big) dg-categories of matrix factorizations\n$MF^\\infty(U,f)$. We show that these come canonically equipped with a structure\nof a $2$-periodic crystal of categories (\\ie an action of the dg-category of\n$2$-periodic $D$-modules on $X$) compatible with a relative Thom--Sebastiani\ntheorem expressing the equivariance under the action of quadratic bundles.\n  As our main theorem we show that the locally defined categories\n$MF^\\infty(U,f)$ can be glued along $X$ as a sheaf of crystals of 2-periodic\ndg-categories ``up to isotopy'', under the prescription of orientation data\ncontrolled by three obstruction classes. This result generalizes the gluing of\nthe Joyce's perverse sheaf of vanishing cycles and partially answers\nconjectures by Kontsevich--Soibelman and Toda in motivic Donaldson--Thomas\ntheory.",
        "In this paper we analyse the dynamics of a family of rational operators\ncoming from a fourth-order family of root-finding algorithms. We first show\nthat it may be convenient to redefine the parameters to prevent redundancies\nand unboundedness of problematic parameters. After reparametrization, we\nobserve that these rational maps belong to a more general family $O_{a,n,k}$ of\ndegree $n+k$ operators, which includes several other families of maps obtained\nfrom other numerical methods. We study the dynamics of $O_{a,n,k}$ and discuss\nfor which parameters $n$ and $k$ these operators would be suitable from the\nnumerical point of view.",
        "The manipulation of electronic structure through periodic electric fields\nenables the reversible control of effective interactions in extended\nantiferromagnetic Mott insulators on ultrafast timescales. A careful analytical\nexamination of the modulated effective interactions is conducted, accurately\ncharacterising it through the use of exact summation formulas and Bessel\nfunctions. As a result, time reversals are analytically determined in terms of\nBessel zeroes. We discuss the half-filled Hubbard model, as well as\nmulti-orbital models, various characteristics of the Kitaev-Heisenberg model,\nand the emergence of chiral spin terms.",
        "Early energy injection leaves an imprint on the observed blackbody spectrum\nof the CMB, allowing us to study the thermal history of the Universe. For small\nenergy release, the distortion can be efficiently computed using the\nquasi-exact Green's function method. For pre-recombination injections, the\nGreen's function has already been studied previously. Here we reconsider the\npre- and post-recombination periods, showcasing both the spectral distortion\nintensity and the relative temperature difference, which encrypt precious\ninformation about physical processes such as free-free interactions and thermal\ndecoupling. We present the associated distortion visibility function,\ninvestigating the impact of various physical effects. We then study\nimprovements to the so-called frequency hierarchy (FH) treatment, a method that\nwas developed for the modelling of anisotropic distortions, which like the\naverage distortion signals encode valuable cosmological information.\nSpecifically, the FH treatment has shortcomings even in the $\\mu$ era, that in\nprinciple should be easy to overcome. In this paper, we introduce a new\napproach to reduce the mismatch, concluding with a redefinition of the $\\mu$\nspectral shape using CosmoTherm. This solution takes into account double\nCompton and Bremsstrahlung effects in the low tail, which can be included in\nthe FH. This opens the path towards a refined modeling of spectral distortion\nanisotropies."
      ]
    }
  },
  {
    "id":2411.02815,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation",
    "start_abstract":"In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b36",
        "b33"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
        "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
      ],
      "categories":[
        "cs.CV",
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "CILP-FGDI: Exploiting Vision-Language Model for Generalizable Person\n  Re-Identification",
        "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks",
        "Language-Inspired Relation Transfer for Few-shot Class-Incremental\n  Learning",
        "Dissecting Human Body Representations in Deep Networks Trained for\n  Person Identification",
        "IBURD: Image Blending for Underwater Robotic Detection",
        "SENSEI: Semantic Exploration Guided by Foundation Models to Learn\n  Versatile World Models",
        "GauSTAR: Gaussian Surface Tracking and Reconstruction",
        "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models",
        "3D\/2D Registration of Angiograms using Silhouette-based Differentiable\n  Rendering",
        "Superpowering Open-Vocabulary Object Detectors for X-ray Vision",
        "CM3T: Framework for Efficient Multimodal Learning for Inhomogeneous\n  Interaction Datasets",
        "Piece it Together: Part-Based Concepting with IP-Priors",
        "Order-Robust Class Incremental Learning: Graph-Driven Dynamic Similarity\n  Grouping",
        "Coded Deep Learning: Framework and Algorithm",
        "Universal programmable and self-configuring optical filter",
        "Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient\n  Descent",
        "Harnessing the Potential of Large Language Models in Modern Marketing\n  Management: Applications, Future Directions, and Strategic Recommendations",
        "Development of Application-Specific Large Language Models to Facilitate\n  Research Ethics Review",
        "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use",
        "Contracting low degree points on curves",
        "Domain-conditioned and Temporal-guided Diffusion Modeling for\n  Accelerated Dynamic MRI Reconstruction",
        "A study of the Antlion Random Walk",
        "Evolving Skeletons: Motion Dynamics in Action Recognition",
        "B-Call: Integrating Ideological Position and Political Cohesion in\n  Legislative Voting Models",
        "Foliated Plateau problems, geometric rigidity and equidistribution of\n  closed $k$-surfaces",
        "Evidence for J\/$\\psi$ suppression in incoherent photonuclear production",
        "A Machine Learning Approach for Design of Frequency Selective Surface\n  based Radar Absorbing Material via Image Prediction"
      ],
      "abstract":[
        "The Visual Language Model, known for its robust cross-modal capabilities, has\nbeen extensively applied in various computer vision tasks. In this paper, we\nexplore the use of CLIP (Contrastive Language-Image Pretraining), a\nvision-language model pretrained on large-scale image-text pairs to align\nvisual and textual features, for acquiring fine-grained and domain-invariant\nrepresentations in generalizable person re-identification. The adaptation of\nCLIP to the task presents two primary challenges: learning more fine-grained\nfeatures to enhance discriminative ability, and learning more domain-invariant\nfeatures to improve the model's generalization capabilities. To mitigate the\nfirst challenge thereby enhance the ability to learn fine-grained features, a\nthree-stage strategy is proposed to boost the accuracy of text descriptions.\nInitially, the image encoder is trained to effectively adapt to person\nre-identification tasks. In the second stage, the features extracted by the\nimage encoder are used to generate textual descriptions (i.e., prompts) for\neach image. Finally, the text encoder with the learned prompts is employed to\nguide the training of the final image encoder. To enhance the model's\ngeneralization capabilities to unseen domains, a bidirectional guiding method\nis introduced to learn domain-invariant image features. Specifically,\ndomain-invariant and domain-relevant prompts are generated, and both positive\n(pulling together image features and domain-invariant prompts) and negative\n(pushing apart image features and domain-relevant prompts) views are used to\ntrain the image encoder. Collectively, these strategies contribute to the\ndevelopment of an innovative CLIP-based framework for learning fine-grained\ngeneralized features in person re-identification.",
        "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. We have updated our content to include the latest\npapers, and the differences between versions are highlighted in the appendix.\nFor further details and ongoing updates, please refer to our GitHub repository:\nhttps:\/\/github.com\/xinzhel\/LLM-Agent-Survey\/blob\/main\/search.md",
        "Depicting novel classes with language descriptions by observing few-shot\nsamples is inherent in human-learning systems. This lifelong learning\ncapability helps to distinguish new knowledge from old ones through the\nincrease of open-world learning, namely Few-Shot Class-Incremental Learning\n(FSCIL). Existing works to solve this problem mainly rely on the careful tuning\nof visual encoders, which shows an evident trade-off between the base knowledge\nand incremental ones. Motivated by human learning systems, we propose a new\nLanguage-inspired Relation Transfer (LRT) paradigm to understand objects by\njoint visual clues and text depictions, composed of two major steps. We first\ntransfer the pretrained text knowledge to the visual domains by proposing a\ngraph relation transformation module and then fuse the visual and language\nembedding by a text-vision prototypical fusion module. Second, to mitigate the\ndomain gap caused by visual finetuning, we propose context prompt learning for\nfast domain alignment and imagined contrastive learning to alleviate the\ninsufficient text data during alignment. With collaborative learning of domain\nalignments and text-image transfer, our proposed LRT outperforms the\nstate-of-the-art models by over $13\\%$ and $7\\%$ on the final session of\nmini-ImageNet and CIFAR-100 FSCIL benchmarks.",
        "Long-term body identification algorithms have emerged recently with the\nincreased availability of high-quality training data. We seek to fill knowledge\ngaps about these models by analyzing body image embeddings from four body\nidentification networks trained with 1.9 million images across 4,788 identities\nand 9 databases. By analyzing a diverse range of architectures (ViT, SWIN-ViT,\nCNN, and linguistically primed CNN), we first show that the face contributes to\nthe accuracy of body identification algorithms and that these algorithms can\nidentify faces to some extent -- with no explicit face training. Second, we\nshow that representations (embeddings) generated by body identification\nalgorithms encode information about gender, as well as image-based information\nincluding view (yaw) and even the dataset from which the image originated.\nThird, we demonstrate that identification accuracy can be improved without\nadditional training by operating directly and selectively on the learned\nembedding space. Leveraging principal component analysis (PCA), identity\ncomparisons were consistently more accurate in subspaces that eliminated\ndimensions that explained large amounts of variance. These three findings were\nsurprisingly consistent across architectures and test datasets. This work\nrepresents the first analysis of body representations produced by long-term\nre-identification networks trained on challenging unconstrained datasets.",
        "We present an image blending pipeline, \\textit{IBURD}, that creates realistic\nsynthetic images to assist in the training of deep detectors for use on\nunderwater autonomous vehicles (AUVs) for marine debris detection tasks.\nSpecifically, IBURD generates both images of underwater debris and their\npixel-level annotations, using source images of debris objects, their\nannotations, and target background images of marine environments. With Poisson\nediting and style transfer techniques, IBURD is even able to robustly blend\ntransparent objects into arbitrary backgrounds and automatically adjust the\nstyle of blended images using the blurriness metric of target background\nimages. These generated images of marine debris in actual underwater\nbackgrounds address the data scarcity and data variety problems faced by\ndeep-learned vision algorithms in challenging underwater conditions, and can\nenable the use of AUVs for environmental cleanup missions. Both quantitative\nand robotic evaluations of IBURD demonstrate the efficacy of the proposed\napproach for robotic detection of marine debris.",
        "Exploration is a cornerstone of reinforcement learning (RL). Intrinsic\nmotivation attempts to decouple exploration from external, task-based rewards.\nHowever, established approaches to intrinsic motivation that follow general\nprinciples such as information gain, often only uncover low-level interactions.\nIn contrast, children's play suggests that they engage in meaningful high-level\nbehavior by imitating or interacting with their caregivers. Recent work has\nfocused on using foundation models to inject these semantic biases into\nexploration. However, these methods often rely on unrealistic assumptions, such\nas language-embedded environments or access to high-level actions. We propose\nSEmaNtically Sensible ExploratIon (SENSEI), a framework to equip model-based RL\nagents with an intrinsic motivation for semantically meaningful behavior.\nSENSEI distills a reward signal of interestingness from Vision Language Model\n(VLM) annotations, enabling an agent to predict these rewards through a world\nmodel. Using model-based RL, SENSEI trains an exploration policy that jointly\nmaximizes semantic rewards and uncertainty. We show that in both robotic and\nvideo game-like simulations SENSEI discovers a variety of meaningful behaviors\nfrom image observations and low-level actions. SENSEI provides a general tool\nfor learning from foundation model feedback, a crucial research direction, as\nVLMs become more powerful.",
        "3D Gaussian Splatting techniques have enabled efficient photo-realistic\nrendering of static scenes. Recent works have extended these approaches to\nsupport surface reconstruction and tracking. However, tracking dynamic surfaces\nwith 3D Gaussians remains challenging due to complex topology changes, such as\nsurfaces appearing, disappearing, or splitting. To address these challenges, we\npropose GauSTAR, a novel method that achieves photo-realistic rendering,\naccurate surface reconstruction, and reliable 3D tracking for general dynamic\nscenes with changing topology. Given multi-view captures as input, GauSTAR\nbinds Gaussians to mesh faces to represent dynamic objects. For surfaces with\nconsistent topology, GauSTAR maintains the mesh topology and tracks the meshes\nusing Gaussians. For regions where topology changes, GauSTAR adaptively unbinds\nGaussians from the mesh, enabling accurate registration and generation of new\nsurfaces based on these optimized Gaussians. Additionally, we introduce a\nsurface-based scene flow method that provides robust initialization for\ntracking between frames. Experiments demonstrate that our method effectively\ntracks and reconstructs dynamic surfaces, enabling a range of applications. Our\nproject page with the code release is available at\nhttps:\/\/eth-ait.github.io\/GauSTAR\/.",
        "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps:\/\/github.com\/ShawnTan86\/TokenCarve.",
        "We present a method for 3D\/2D registration of Digital Subtraction Angiography\n(DSA) images to provide valuable insight into brain hemodynamics and\nangioarchitecture. Our approach formulates the registration as a pose\nestimation problem, leveraging both anteroposterior and lateral DSA views and\nemploying differentiable rendering. Preliminary experiments on real and\nsynthetic datasets demonstrate the effectiveness of our method, with both\nqualitative and quantitative evaluations highlighting its potential for\nclinical applications. The code is available at\nhttps:\/\/github.com\/taewoonglee17\/TwoViewsDSAReg.",
        "Open-vocabulary object detection (OvOD) is set to revolutionize security\nscreening by enabling systems to recognize any item in X-ray scans. However,\ndeveloping effective OvOD models for X-ray imaging presents unique challenges\ndue to data scarcity and the modality gap that prevents direct adoption of\nRGB-based solutions. To overcome these limitations, we propose RAXO, a\ntraining-free framework that repurposes off-the-shelf RGB OvOD detectors for\nrobust X-ray detection. RAXO builds high-quality X-ray class descriptors using\na dual-source retrieval strategy. It gathers relevant RGB images from the web\nand enriches them via a novel X-ray material transfer mechanism, eliminating\nthe need for labeled databases. These visual descriptors replace text-based\nclassification in OvOD, leveraging intra-modal feature distances for robust\ndetection. Extensive experiments demonstrate that RAXO consistently improves\nOvOD performance, providing an average mAP increase of up to 17.0 points over\nbase detectors. To further support research in this emerging field, we also\nintroduce DET-COMPASS, a new benchmark featuring bounding box annotations for\nover 300 object categories, enabling large-scale evaluation of OvOD in X-ray.\nCode and dataset available at: https:\/\/github.com\/PAGF188\/RAXO.",
        "Challenges in cross-learning involve inhomogeneous or even inadequate amount\nof training data and lack of resources for retraining large pretrained models.\nInspired by transfer learning techniques in NLP, adapters and prefix tuning,\nthis paper presents a new model-agnostic plugin architecture for\ncross-learning, called CM3T, that adapts transformer-based models to new or\nmissing information. We introduce two adapter blocks: multi-head vision\nadapters for transfer learning and cross-attention adapters for multimodal\nlearning. Training becomes substantially efficient as the backbone and other\nplugins do not need to be finetuned along with these additions. Comparative and\nablation studies on three datasets Epic-Kitchens-100, MPIIGroupInteraction and\nUDIVA v0.5 show efficacy of this framework on different recording settings and\ntasks. With only 12.8% trainable parameters compared to the backbone to process\nvideo input and only 22.3% trainable parameters for two additional modalities,\nwe achieve comparable and even better results than the state-of-the-art. CM3T\nhas no specific requirements for training or pretraining and is a step towards\nbridging the gap between a general model and specific practical applications of\nvideo classification.",
        "Advanced generative models excel at synthesizing images but often rely on\ntext-based conditioning. Visual designers, however, often work beyond language,\ndirectly drawing inspiration from existing visual elements. In many cases,\nthese elements represent only fragments of a potential concept-such as an\nuniquely structured wing, or a specific hairstyle-serving as inspiration for\nthe artist to explore how they can come together creatively into a coherent\nwhole. Recognizing this need, we introduce a generative framework that\nseamlessly integrates a partial set of user-provided visual components into a\ncoherent composition while simultaneously sampling the missing parts needed to\ngenerate a plausible and complete concept. Our approach builds on a strong and\nunderexplored representation space, extracted from IP-Adapter+, on which we\ntrain IP-Prior, a lightweight flow-matching model that synthesizes coherent\ncompositions based on domain-specific priors, enabling diverse and\ncontext-aware generations. Additionally, we present a LoRA-based fine-tuning\nstrategy that significantly improves prompt adherence in IP-Adapter+ for a\ngiven task, addressing its common trade-off between reconstruction quality and\nprompt adherence.",
        "Class Incremental Learning (CIL) aims to enable models to learn new classes\nsequentially while retaining knowledge of previous ones. Although current\nmethods have alleviated catastrophic forgetting (CF), recent studies highlight\nthat the performance of CIL models is highly sensitive to the order of class\narrival, particularly when sequentially introduced classes exhibit high\ninter-class similarity. To address this critical yet understudied challenge of\nclass order sensitivity, we first extend existing CIL frameworks through\ntheoretical analysis, proving that grouping classes with lower pairwise\nsimilarity during incremental phases significantly improves model robustness to\norder variations. Building on this insight, we propose Graph-Driven Dynamic\nSimilarity Grouping (GDDSG), a novel method that employs graph coloring\nalgorithms to dynamically partition classes into similarity-constrained groups.\nEach group trains an isolated CIL sub-model and constructs meta-features for\nclass group identification. Experimental results demonstrate that our method\neffectively addresses the issue of class order sensitivity while achieving\noptimal performance in both model accuracy and anti-forgetting capability. Our\ncode is available at https:\/\/github.com\/AIGNLAI\/GDDSG.",
        "The success of deep learning (DL) is often achieved with large models and\nhigh complexity during both training and post-training inferences, hindering\ntraining in resource-limited settings. To alleviate these issues, this paper\nintroduces a new framework dubbed ``coded deep learning'' (CDL), which\nintegrates information-theoretic coding concepts into the inner workings of DL,\nto significantly compress model weights and activations, reduce computational\ncomplexity at both training and post-training inference stages, and enable\nefficient model\/data parallelism. Specifically, within CDL, (i) we first\npropose a novel probabilistic method for quantizing both model weights and\nactivations, and its soft differentiable variant which offers an analytic\nformula for gradient calculation during training; (ii) both the forward and\nbackward passes during training are executed over quantized weights and\nactivations, eliminating most floating-point operations and reducing training\ncomplexity; (iii) during training, both weights and activations are entropy\nconstrained so that they are compressible in an information-theoretic sense\nthroughout training, thus reducing communication costs in model\/data\nparallelism; and (iv) the trained model in CDL is by default in a quantized\nformat with compressible quantized weights, reducing post-training inference\nand storage complexity. Additionally, a variant of CDL, namely relaxed CDL\n(R-CDL), is presented to further improve the trade-off between validation\naccuracy and compression though requiring full precision in training with other\nadvantageous features of CDL intact. Extensive empirical results show that CDL\nand R-CDL outperform the state-of-the-art algorithms in DNN compression in the\nliterature.",
        "We propose an approach to integrated optical spectral filtering that allows\narbitrary programmability, can compensate automatically for imperfections in\nfilter fabrication, allows multiple simultaneous and separately programmable\nfilter functions on the same input, and can configure itself automatically to\nthe problem of interest, for example to filter or reject multiple arbitrarily\nchosen frequencies. The approach exploits splitting the input light into an\narray of multiple waveguides of different lengths that then feed a programmable\ninterferometer array that can also self-configure. It can give spectral\nresponse similar to arrayed waveguide gratings but offers many other filtering\nfunctions, as well as supporting other structures based on non-redundant arrays\nfor precise spectral filtering. Simultaneous filtering also allows, for the\nfirst time to our knowledge, an automatic measurement of the temporal coherency\nmatrix and physical separation into the Karhunen-Lo\\`eve expansion of\ntemporally partially coherent light fields.",
        "In this paper, we establish non-asymptotic convergence rates in the central\nlimit theorem for Polyak-Ruppert-averaged iterates of stochastic gradient\ndescent (SGD). Our analysis builds on the result of the Gaussian approximation\nfor nonlinear statistics of independent random variables of Shao and Zhang\n(2022). Using this result, we prove the non-asymptotic validity of the\nmultiplier bootstrap for constructing the confidence sets for the optimal\nsolution of an optimization problem. In particular, our approach avoids the\nneed to approximate the limiting covariance of Polyak-Ruppert SGD iterates,\nwhich allows us to derive approximation rates in convex distance of order up to\n$1\/\\sqrt{n}$.",
        "Large Language Models (LLMs) have revolutionized the process of customer\nengagement, campaign optimization, and content generation, in marketing\nmanagement. In this paper, we explore the transformative potential of LLMs\nalong with the current applications, future directions, and strategic\nrecommendations for marketers. In particular, we focus on LLMs major business\ndrivers such as personalization, real-time-interactive customer insights, and\ncontent automation, and how they enable customers and business outcomes. For\ninstance, the ethical aspects of AI with respect to data privacy, transparency,\nand mitigation of bias are also covered, with the goal of promoting responsible\nuse of the technology through best practices and the use of new technologies\nbusinesses can tap into the LLM potential, which help growth and stay one step\nahead in the turmoil of digital marketing. This article is designed to give\nmarketers the necessary guidance by using best industry practices to integrate\nthese powerful LLMs into their marketing strategy and innovation without\ncompromising on the ethos of their brand.",
        "Institutional review boards (IRBs) play a crucial role in ensuring the\nethical conduct of human subjects research, but face challenges including\ninconsistency, delays, and inefficiencies. We propose the development and\nimplementation of application-specific large language models (LLMs) to\nfacilitate IRB review processes. These IRB-specific LLMs would be fine-tuned on\nIRB-specific literature and institutional datasets, and equipped with retrieval\ncapabilities to access up-to-date, context-relevant information. We outline\npotential applications, including pre-review screening, preliminary analysis,\nconsistency checking, and decision support. While addressing concerns about\naccuracy, context sensitivity, and human oversight, we acknowledge remaining\nchallenges such as over-reliance on AI and the need for transparency. By\nenhancing the efficiency and quality of ethical review while maintaining human\njudgment in critical decisions, IRB-specific LLMs offer a promising tool to\nimprove research oversight. We call for pilot studies to evaluate the\nfeasibility and impact of this approach.",
        "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https:\/\/huggingface.co\/datasets\/bytedance-research\/ToolHop.",
        "The main result of this article is that all but finitely many points of small\nenough degree on a curve can be written as a pullback of a smaller degree\npoint. The main theorem has several corollaries that yield improvements on\nresults of Kadets and Vogt, Khawaja and Siksek, and Vojta under a slightly\nstronger assumption on the degree of the points.",
        "Purpose: To propose a domain-conditioned and temporal-guided diffusion\nmodeling method, termed dynamic Diffusion Modeling (dDiMo), for accelerated\ndynamic MRI reconstruction, enabling diffusion process to characterize\nspatiotemporal information for time-resolved multi-coil Cartesian and\nnon-Cartesian data. Methods: The dDiMo framework integrates temporal\ninformation from time-resolved dimensions, allowing for the concurrent capture\nof intra-frame spatial features and inter-frame temporal dynamics in diffusion\nmodeling. It employs additional spatiotemporal ($x$-$t$) and self-consistent\nfrequency-temporal ($k$-$t$) priors to guide the diffusion process. This\napproach ensures precise temporal alignment and enhances the recovery of fine\nimage details. To facilitate a smooth diffusion process, the nonlinear\nconjugate gradient algorithm is utilized during the reverse diffusion steps.\nThe proposed model was tested on two types of MRI data: Cartesian-acquired\nmulti-coil cardiac MRI and Golden-Angle-Radial-acquired multi-coil\nfree-breathing lung MRI, across various undersampling rates. Results: dDiMo\nachieved high-quality reconstructions at various acceleration factors,\ndemonstrating improved temporal alignment and structural recovery compared to\nother competitive reconstruction methods, both qualitatively and\nquantitatively. This proposed diffusion framework exhibited robust performance\nin handling both Cartesian and non-Cartesian acquisitions, effectively\nreconstructing dynamic datasets in cardiac and lung MRI under different imaging\nconditions. Conclusion: This study introduces a novel diffusion modeling method\nfor dynamic MRI reconstruction.",
        "This paper treats a new type of random walk referred to as an Antlion Random\nWalk (ARW), which is motivated by mathematical modeling of the decision-making\nprocess using chaotic semiconductor lasers with memory parameters. We discuss\nthe dependency of the property of the probability distribution of ARWs on the\nmemory parameter $\\alpha$ and discuss uniqueness of them in contrast to the\nconventional, simple RWs through similarity to the normal distribution.",
        "Skeleton-based action recognition has gained significant attention for its\nability to efficiently represent spatiotemporal information in a lightweight\nformat. Most existing approaches use graph-based models to process skeleton\nsequences, where each pose is represented as a skeletal graph structured around\nhuman physical connectivity. Among these, the Spatiotemporal Graph\nConvolutional Network (ST-GCN) has become a widely used framework.\nAlternatively, hypergraph-based models, such as the Hyperformer, capture\nhigher-order correlations, offering a more expressive representation of complex\njoint interactions. A recent advancement, termed Taylor Videos, introduces\nmotion-enhanced skeleton sequences by embedding motion concepts, providing a\nfresh perspective on interpreting human actions in skeleton-based action\nrecognition. In this paper, we conduct a comprehensive evaluation of both\ntraditional skeleton sequences and Taylor-transformed skeletons using ST-GCN\nand Hyperformer models on the NTU-60 and NTU-120 datasets. We compare skeletal\ngraph and hypergraph representations, analyzing static poses against\nmotion-injected poses. Our findings highlight the strengths and limitations of\nTaylor-transformed skeletons, demonstrating their potential to enhance motion\ndynamics while exposing current challenges in fully using their benefits. This\nstudy underscores the need for innovative skeletal modelling techniques to\neffectively handle motion-rich data and advance the field of action\nrecognition.",
        "This paper combines two significant areas of political science research:\nmeasuring individual ideological position and cohesion. Although both\napproaches help analyze legislative behaviors, no unified model currently\nintegrates these dimensions. To fill this gap, the paper proposes a methodology\ncalled B-Call that combines ideological positioning with voting cohesion,\ntreating votes as random variables. The model is empirically validated using\nroll-call data from the United States, Brazil, and Chile legislatures, which\nrepresent diverse legislative dynamics. The analysis aims to capture the\ncomplexities of voting and legislative behaviors, resulting in a\ntwo-dimensional indicator. This study addresses gaps in current legislative\nvoting models, particularly in contexts with limited party control.",
        "In this note, we survey recent advances in the study of dynamical properties\nof the space of surfaces with constant curvature in three-dimensional manifolds\nof negative sectional curvature. We interpret this space as a two-dimensional\nanalogue of the geodesic flow and explore the extent to which the thermodynamic\nproperties of the latter can be generalized to the surface setting.\nAdditionally, we apply this theory to derive geometric rigidity results,\nincluding the rigidity of the hyperbolic marked area spectrum.",
        "According to quantum chromodynamics, at sufficiently high energy, the\nstructure of hadrons reveals a dynamic equilibrium between gluon splitting and\ngluon recombination -- a phenomenon known as saturation. The process of\ndiffractive photonuclear production of a J\/$\\psi$ vector meson provides a\ndirect insight into the gluon composition of hadrons. The J\/$\\psi$ production\nas a function of momentum transferred in the interaction, quantified by the\nMandelstam-$t$ variable, serves as an excellent probe for studying the\nstructure of hadrons within the impact-parameter plane, because different\nranges in $t$ are sensitive to the dynamics of the gluon field at varying\nspatial size scales. The ALICE collaboration has measured the energy dependence\nof incoherent photonuclear production of J\/$\\psi$ mesons off lead ions, at\n$\\sqrt{s_{\\rm NN}} = 5.02$ TeV, for three Mandelstam-$t$ intervals. The energy\ndependence of the photonuclear cross section at the highest $|t|$ range\nmeasured, $(0.81< |t| <1.44)$ GeV$^2$, is sensitive to subnucleonic structures\nof the Pb target. The increase of the cross section with energy at large $|t|$\nshows evidence of suppression with respect to the increase seen at low $|t|$.\nThe observed pattern of the energy evolution in data is similar to that of\ngluon saturation models.",
        "The paper presents an innovative methodology for designing frequency\nselective surface (FSS) based radar absorbing materials using machine learning\n(ML) technique. In conventional electromagnetic design, unit cell dimensions of\nFSS are used as input and absorption coefficient is then predicted for a given\ndesign. In this paper, absorption coefficient is considered as input to ML\nmodel and image of FSS unit cell is predicted. Later, this image is used for\ngenerating the FSS unit cell parameters. Eleven different ML models are studied\nover a wide frequency band of 1GHz to 30GHz. Out of which six ML models (i.e.\n(a) Random Forest classification, (b) K- Neighbors Classification, (c) Grid\nsearch regression, (d) Random Forest regression, (e) Decision tree\nclassification, and (f) Decision tree regression) show training accuracy more\nthan 90%. The absorption coefficients with varying frequencies of these\npredicted images are subsequently evaluated using commercial electromagnetic\nsolver. The performance of these ML models is encouraging, and it can be used\nfor accelerating design and optimization of high performance FSS based radar\nabsorbing material for advanced electromagnetic applications in future."
      ]
    }
  },
  {
    "id":2411.00561,
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study",
    "start_abstract":"In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "What is a cell type, really? The quest to categorize life's myriad forms."
      ],
      "abstract":[
        "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Silicon is the next frontier in plant synthetic biology",
        "Leveraging Sequence Purification for Accurate Prediction of Multiple\n  Conformational States with AlphaFold2",
        "RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via\n  Synergistic Flow Matching",
        "DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown\n  Efficiency",
        "Non-Markovain Quantum State Diffusion for the Tunneling in SARS-COVID-19\n  virus",
        "A Comprehensive Review of Protein Language Models",
        "Inverse problems with experiment-guided AlphaFold",
        "How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of\n  RNA Graph Motifs Using Topological Descriptors",
        "Artificial Intelligence Approaches for Anti-Addiction Drug Discovery",
        "Deep Learning of Proteins with Local and Global Regions of Disorder",
        "Fluorescence Phasor Analysis: Basic Principles and Biophysical\n  Applications",
        "Remodeling Peptide-MHC-TCR Triad Binding as Sequence Fusion for\n  Immunogenicity Prediction",
        "An Energy-Adaptive Elastic Equivariant Transformer Framework for Protein\n  Structure Representation",
        "Teacher-student training improves accuracy and efficiency of machine\n  learning inter-atomic potentials",
        "Partitions of unity and barycentric algebras",
        "Adiabatic Pumping of Orbital Magnetization by Spin Precession",
        "Non-positive energy quasidistributions in coherent collision models",
        "New properties of length-extremals in free step-2 rank-4 Carnot groups",
        "Covariant photon current",
        "PyClustrPath: An efficient Python package for generating clustering\n  paths with GPU acceleration",
        "Hybrid Brain-Machine Interface: Integrating EEG and EMG for Reduced\n  Physical Demand",
        "Period Analysis of Eclipsing Cataclysmic Variable Stars",
        "OpenGERT: Open Source Automated Geometry Extraction with Geometric and\n  Electromagnetic Sensitivity Analyses for Ray-Tracing Propagation Models",
        "Black holes inside cosmic voids",
        "Poisson Vertex Algebras and Three-Dimensional Gauge Theory",
        "Investigating the Effects of Atmospheric Stratification on Coronal\n  Active Region Field Modelling",
        "Spall failure of alumina at high-strain rates using femtosecond laser\n  experiments and high-fidelity molecular dynamics simulations",
        "Bounded conciseness in the space of marked groups"
      ],
      "abstract":[
        "Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field.",
        "AlphaFold2 (AF2) has transformed protein structure prediction by harnessing\nco-evolutionary constraints embedded in multiple sequence alignments (MSAs).\nMSAs not only encode static structural information, but also hold critical\ndetails about protein dynamics, which underpin biological functions. However,\nthese subtle co-evolutionary signatures, which dictate conformational state\npreferences, are often obscured by noise within MSA data and thus remain\nchallenging to decipher. Here, we introduce AF-ClaSeq, a systematic framework\nthat isolates these co-evolutionary signals through sequence purification and\niterative enrichment. By extracting sequence subsets that preferentially encode\ndistinct structural states, AF-ClaSeq enables high-confidence predictions of\nalternative conformations. Our findings reveal that the successful sampling of\nalternative states depends not on MSA depth but on sequence purity.\nIntriguingly, purified sequences encoding specific structural states are\ndistributed across phylogenetic clades and superfamilies, rather than confined\nto specific lineages. Expanding upon AF2's transformative capabilities,\nAF-ClaSeq provides a powerful approach for uncovering hidden structural\nplasticity, advancing allosteric protein and drug design, and facilitating\ndynamics-based protein function annotation.",
        "Ribonucleic acid (RNA) binds to molecules to achieve specific biological\nfunctions. While generative models are advancing biomolecule design, existing\nmethods for designing RNA that target specific ligands face limitations in\ncapturing RNA's conformational flexibility, ensuring structural validity, and\novercoming data scarcity. To address these challenges, we introduce RiboFlow, a\nsynergistic flow matching model to co-design RNA structures and sequences based\non target molecules. By integrating RNA backbone frames, torsion angles, and\nsequence features in an unified architecture, RiboFlow explicitly models RNA's\ndynamic conformations while enforcing sequence-structure consistency to improve\nvalidity. Additionally, we curate RiboBind, a large-scale dataset of\nRNA-molecule interactions, to resolve the scarcity of high-quality structural\ndata. Extensive experiments reveal that RiboFlow not only outperforms\nstate-of-the-art RNA design methods by a large margin but also showcases\ncontrollable capabilities for achieving high binding affinity to target\nligands. Our work bridges critical gaps in controllable RNA design, offering a\nframework for structure-aware, data-efficient generation.",
        "Background: Small interfering RNA (siRNA) is a promising therapeutic agent\ndue to its ability to silence disease-related genes via RNA interference. While\ntraditional machine learning and early deep learning methods have made progress\nin predicting siRNA efficacy, there remains significant room for improvement.\nAdvanced deep learning techniques can enhance prediction accuracy, reducing the\nreliance on extensive wet-lab experiments and accelerating the identification\nof effective siRNA sequences. This approach also provides deeper insights into\nthe mechanisms of siRNA efficacy, facilitating more targeted and efficient\ntherapeutic strategies.\n  Methods: We introduce DeepSilencer, an innovative deep learning model\ndesigned to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced\nneural network architectures to capture the complex features of siRNA\nsequences. Our key contributions include a specially designed deep learning\nmodel, an innovative online data sampling method, and an improved loss function\ntailored for siRNA prediction. These enhancements collectively boost the\nmodel's prediction accuracy and robustness.\n  Results: Extensive evaluations on multiple test sets demonstrate that\nDeepSilencer achieves state-of-the-art performance using only siRNA sequences\nand basic physicochemical properties. Our model surpasses several other methods\nand shows superior predictive performance, particularly when incorporating\nthermodynamic parameters.\n  Conclusion: The advancements in data sampling, model design, and loss\nfunction significantly enhance the predictive capabilities of DeepSilencer.\nThese improvements underscore its potential to advance RNAi therapeutic design\nand development, offering a powerful tool for researchers and clinicians.",
        "In the context of biology, unlike the comprehensively established Standard\nModel in physics, many biological processes lack a complete theoretical\nframework and are often described phenomenologically. A pertinent example is\nolfaction -- the process through which humans and animals distinguish various\nodors. The conventional biological explanation for olfaction relies on the lock\nand key model, which, while useful, does not fully account for all observed\nphenomena. As an alternative or complement to this model, vibration-assisted\nelectron tunneling has been proposed. Drawing inspiration from the\nvibration-assisted electron tunneling model for olfaction, we have developed a\ntheoretical model for electron tunneling in SARS-CoV-2 virus infection within a\nnon-Markovian framework. We approach this by solving the non-Markovian quantum\nstochastic Schrodinger equation. In our model, the spike protein and the GPCR\nreceptor are conceptualized as a dimer, utilizing the spin-Boson model to\nfacilitate the description of electron tunneling. Our analysis demonstrates\nthat electron tunneling in this context exhibits inherently non-Markovian\ncharacteristics, extending into the intermediate and strong coupling regimes\nbetween the dimer components. This behavior stands in stark contrast to\npredictions from Markovian models, which fail to accurately describe electron\ntunneling in the strong coupling limit. Notably, Markovian approximations often\nlead to unphysical negative probabilities in this regime, underscoring their\nlimitations and highlighting the necessity of incorporating non-Markovian\ndynamics for a more realistic description of biological quantum processes. This\napproach not only broadens our understanding of viral infection mechanisms but\nalso enhances the biological accuracy and relevance of our theoretical\nframework in describing complex biological interactions.",
        "At the intersection of the rapidly growing biological data landscape and\nadvancements in Natural Language Processing (NLP), protein language models\n(PLMs) have emerged as a transformative force in modern research. These models\nhave achieved remarkable progress, highlighting the need for timely and\ncomprehensive overviews. However, much of the existing literature focuses\nnarrowly on specific domains, often missing a broader analysis of PLMs. This\nstudy provides a systematic review of PLMs from a macro perspective, covering\nkey historical milestones and current mainstream trends. We focus on the models\nthemselves and their evaluation metrics, exploring aspects such as model\narchitectures, positional encoding, scaling laws, and datasets. In the\nevaluation section, we discuss benchmarks and downstream applications. To\nfurther support ongoing research, we introduce relevant mainstream tools.\nLastly, we critically examine the key challenges and limitations in this\nrapidly evolving field.",
        "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
        "We introduce a computational topology-based approach with unsupervised\nmachine-learning algorithms to estimate the database size and content of\nRNA-like graph topologies. Specifically, we apply graph theory enumeration to\ngenerate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2\nto 9. Among them, only 0.11% graphs correspond to approximately 200,000 known\nRNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping\nmethod. The remaining 99.89% of the dual graphs may be RNA-like or\nnon-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are\nmore likely to be associated with RNA structures, we apply computational\ntopology descriptors using the Persistent Spectral Graphs (PSG) method to\ncharacterize each graph using 19 PSG-based features and use clustering\nalgorithms that partition all possible dual graphs into two clusters, RNA-like\ncluster and non-RNA-like cluster. The distance of each dual graph to the center\nof the RNA-like cluster represents the likelihood of it belonging to RNA\nstructures. From validation, our PSG-based RNA-like cluster includes 97.3% of\nthe 121 known RNA dual graphs, suggesting good performance. Furthermore,\n46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,\nwe observe that all the top 15 RNA-like dual graphs can be separated into\nmultiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to\nhave any subgraphs. Moreover, a significant topological difference between top\nRNA-like and non-RNA-like graphs is evident when comparing their topological\nfeatures. These findings provide valuable insights into the size of the RNA\nmotif universe and RNA design strategies, offering a novel framework for\npredicting RNA graph topologies and guiding the discovery of novel RNA motifs,\nperhaps anti-viral therapeutics by subgraph assembly.",
        "Drug addiction is a complex and pervasive global challenge that continues to\npose significant public health concerns. Traditional approaches to\nanti-addiction drug discovery have struggled to deliver effective therapeutics,\nfacing high attrition rates, long development timelines, and inefficiencies in\nprocessing large-scale data. Artificial intelligence (AI) has emerged as a\ntransformative solution to address these issues. Using advanced algorithms, AI\nis revolutionizing drug discovery by enhancing the speed and precision of key\nprocesses. This review explores the transformative role of AI in the pipeline\nfor anti-addiction drug discovery, including data collection, target\nidentification, and compound optimization. By highlighting the potential of AI\nto overcome traditional barriers, this review systematically examines how AI\naddresses critical gaps in anti-addiction research, emphasizing its potential\nto revolutionize drug discovery and development, overcome challenges, and\nadvance more effective therapeutic strategies.",
        "Although machine learning has transformed protein structure prediction of\nfolded protein ground states with remarkable accuracy, intrinsically disordered\nproteins and regions (IDPs\/IDRs) are defined by diverse and dynamical\nstructural ensembles that are predicted with low confidence by algorithms such\nas AlphaFold. We present a new machine learning method, IDPForge (Intrinsically\nDisordered Protein, FOlded and disordered Region GEnerator), that exploits a\ntransformer protein language diffusion model to create all-atom IDP ensembles\nand IDR disordered ensembles that maintains the folded domains. IDPForge does\nnot require sequence-specific training, back transformations from\ncoarse-grained representations, nor ensemble reweighting, as in general the\ncreated IDP\/IDR conformational ensembles show good agreement with solution\nexperimental data, and options for biasing with experimental restraints are\nprovided if desired. We envision that IDPForge with these diverse capabilities\nwill facilitate integrative and structural studies for proteins that contain\nintrinsic disorder.",
        "Fluorescence is one of the most widely used techniques in biological\nsciences. Its exceptional sensitivity and versatility make it a tool of first\nchoice for quantitative studies in biophysics. The concept of phasors,\noriginally introduced by Charles Steinmetz in the late 19th century for\nanalyzing alternating current circuits, has since found applications across\ndiverse disciplines, including fluorescence spectroscopy. The main idea behind\nfluorescence phasors was posited by Gregorio Weber in 1981. By analyzing the\ncomplementary nature of pulse and phase fluorometry data, he shows that two\nmagnitudes -- denoted as $G$ and $S$ -- derived from the frequency-domain\nfluorescence measurements correspond to the real and imaginary part of the\nFourier transform of the fluorescence intensity in the time domain. This review\nprovides a historical perspective on how the concept of phasors originates and\nhow it integrates into fluorescence spectroscopy. We discuss their fundamental\nalgebraic properties, which enable intuitive model-free analysis of\nfluorescence data despite the complexity of the underlying phenomena. Some\napplications in biophysics illustrate the power of this approach in studying\ndiverse phenomena, including protein folding, protein interactions, phase\ntransitions in lipid mixtures and formation of high-order structures in nucleic\nacids.",
        "The complex nature of tripartite peptide-MHC-TCR interactions is a critical\nyet underexplored area in immunogenicity prediction. Traditional studies on\nTCR-antigen binding have not fully addressed the complex dependencies in triad\nbinding. In this paper, we propose new modeling approaches for these tripartite\ninteractions, utilizing sequence information from MHCs, peptides, and TCRs. Our\nmethods adhere to native sequence forms and align with biological processes to\nenhance prediction accuracy. By incorporating representation learning\ntechniques, we introduce a fusion mechanism to integrate the three sequences\neffectively. Empirical experiments show that our models outperform traditional\nmethods, achieving a 2.8 to 13.3 percent improvement in prediction accuracy\nacross existing benchmarks. We further validate our approach with extensive\nablation studies, demonstrating the effectiveness of the proposed model\ncomponents. The model implementation, code, and supplementary materials,\nincluding a manuscript with colored hyperlinks and a technical appendix for\ndigital viewing, will be open-sourced upon publication.",
        "Structure-informed protein representation learning is essential for effective\nprotein function annotation and \\textit{de novo} design. However, the presence\nof inherent noise in both crystal and AlphaFold-predicted structures poses\nsignificant challenges for existing methods in learning robust protein\nrepresentations. To address these issues, we propose a novel equivariant\nTransformer-State Space Model(SSM) hybrid framework, termed $E^3$former,\ndesigned for efficient protein representation. Our approach uses energy\nfunction-based receptive fields to construct proximity graphs and incorporates\nan equivariant high-tensor-elastic selective SSM within the transformer\narchitecture. These components enable the model to adapt to complex atom\ninteractions and extract geometric features with higher signal-to-noise ratios.\nEmpirical results demonstrate that our model outperforms existing methods in\nstructure-intensive tasks, such as inverse folding and binding site prediction,\nparticularly when using predicted structures, owing to its enhanced tolerance\nto data deviation and noise. Our approach offers a novel perspective for\nconducting biological function research and drug discovery using noisy protein\nstructure data.",
        "Machine learning inter-atomic potentials (MLIPs) are revolutionizing the\nfield of molecular dynamics (MD) simulations. Recent MLIPs have tended towards\nmore complex architectures trained on larger datasets. The resulting increase\nin computational and memory costs may prohibit the application of these MLIPs\nto perform large-scale MD simulations. Here, we present a teacher-student\ntraining framework in which the latent knowledge from the teacher (atomic\nenergies) is used to augment the students' training. We show that the\nlight-weight student MLIPs have faster MD speeds at a fraction of the memory\nfootprint compared to the teacher models. Remarkably, the student models can\neven surpass the accuracy of the teachers, even though both are trained on the\nsame quantum chemistry dataset. Our work highlights a practical method for\nMLIPs to reduce the resources required for large-scale MD simulations.",
        "Barycentric coordinates provide solutions to the problem of expressing an\nelement of a compact convex set as a convex combination of a finite number of\nextreme points of the set. They have been studied widely within the geometric\nliterature, typically in response to the demands of interpolation, numerical\nanalysis and computer graphics. In this note we bring an algebraic perspective\nto the problem, based on barycentric algebras. We focus on the discussion of\nrelations between different subclasses of partitions of unity, one arising in\nthe context of barycentric coordinates, based on the tautological map\nintroduced by Guessab.",
        "We propose adiabatic pumping of orbital magnetization driven by coherent spin\nprecession, facilitating the rectification of this precession. The orbital\nmagnetization originates from the adiabatic evolution of valence electrons with\na topological bulk contribution expressed as a Chern-Simons form. When the\nprecession cone angle of spin $\\mathbf{S}$ is small, the resulting\nmagnetization is proportional to $\\mathbf{S}\\times \\dot{\\mathbf{S}}$,\ncontributing to the magnon Zeeman effect. With a large cone angle, the\nmagnetization can reach its natural unit, $e\/T$, in an antiferromagnetic\ntopological insulator with $e$ as the elementary charge and $T$ as the\nprecession period. This significant magnetization is related to the global\nproperties of the electronic geometric phases in the parameter space spanned by\n$\\mathbf{S}$ and momentum $\\mathbf{k}$. When the pumped magnetization is\ninhomogeneous, induced by spin textures or electronic topological phase\ndomains, a dissipationless charge current is also pumped. At last, we discuss\nthe boundary contributions from the spin-driving edge states, which are\nintricately linked to the gauge-dependent quantum uncertainty of the\nChern-Simons form.",
        "We determine the Kirkwood-Dirac quasiprobability (KDQ) distribution\nassociated to the stochastic instances of internal energy variations for the\nquantum system and environment particles in coherent Markovian collision\nmodels. In the case the interactions between the quantum system and the\nparticles do not conserve energy, the KDQ of the non-energy-preserving\nstochastic work is also derived. These KDQ distributions can account for\nnon-commutativity, and return the unperturbed average values and variances for\na generic interaction-time, and generic local initial states of the quantum\nsystem and environment particles. Using this nonequilibrium-physics approach,\nwe certify the conditions under which the collision process of the model\nexhibits quantum traits, and we quantify the rate of energy exchanged by the\nquantum system by looking at the variance of the KDQ energy distributions.\nFinally, we propose an experimental test of our results on a superconducting\nquantum circuit implementing a qubit system, with microwave photons\nrepresenting the environment particles.",
        "In the free, step-2, rank-4 sub-Riemannian Carnot group, we give a clean\nexpression for length-extremals, we provide an explicit equation for conjugate\npoints, we relate it with the conjectured cut-locus of the origin. Finally, we\ngive some upper estimates for the cut-time of extremals.",
        "An inhomogeneous continuity equation for the photon four-current operator,\n$\\widehat{J}_{p}$, was derived in [M. Hawton, Phys. Rev. A, 109, 062221\n(2024)]. If the electromagnetic potential operator, $\\widehat{A}% =\\left(\n\\widehat{\\phi}\/c,\\widehat{\\mathbf{A}}\\right) $, is covariant then\n$\\widehat{J}_{p}$ is covariant and the continuity equation is invariant. Here\nwe start with the standard Lagrangian in a Lorentz invariant gauge and quantize\nboth transverse and longitudinal modes. The scalar potential\n$\\widehat{\\phi}=c\\widehat{A}_{\\Vert}$ is not independently second quantized, so\nall modes have positive definite norm. The continuity equation is generalized\nby separating the material source current into a nonabsorbing term describing\npropagation in a lossless transmission line and localized single photon\nemission and detection terms that do not require nonlocal separation of\ntransverse and longitudinal modes.",
        "Convex clustering is a popular clustering model without requiring the number\nof clusters as prior knowledge. It can generate a clustering path by\ncontinuously solving the model with a sequence of regularization parameter\nvalues. This paper introduces {\\it PyClustrPath}, a highly efficient Python\npackage for solving the convex clustering model with GPU acceleration. {\\it\nPyClustrPath} implements popular first-order and second-order algorithms with a\nclean modular design. Such a design makes {\\it PyClustrPath} more scalable to\nincorporate new algorithms for solving the convex clustering model in the\nfuture. We extensively test the numerical performance of {\\it PyClustrPath} on\npopular clustering datasets, demonstrating its superior performance compared to\nthe existing solvers for generating the clustering path based on the convex\nclustering model. The implementation of {\\it PyClustrPath} can be found at:\nhttps:\/\/github.com\/D3IntOpt\/PyClustrPath.",
        "We present a hybrid brain-machine interface (BMI) that integrates\nsteady-state visually evoked potential (SSVEP)-based EEG and facial EMG to\nimprove multimodal control and mitigate fatigue in assistive applications.\nTraditional BMIs relying solely on EEG or EMG suffer from inherent limitations;\nEEG-based control requires sustained visual focus, leading to cognitive\nfatigue, while EMG-based control induces muscular fatigue over time. Our system\ndynamically alternates between EEG and EMG inputs, using EEG to detect SSVEP\nsignals at 9.75 Hz and 14.25 Hz and EMG from cheek and neck muscles to optimize\ncontrol based on task demands. In a virtual turtle navigation task, the hybrid\nsystem achieved task completion times comparable to an EMG-only approach, while\n90% of users reported reduced or equal physical demand. These findings\ndemonstrate that multimodal BMI systems can enhance usability, reduce strain,\nand improve long-term adherence in assistive technologies.",
        "We have performed a study of the orbital properties of seven eclipsing\ncataclysmic variable (CV) binary systems by analyzing photometric time series\nfrom the Transiting Exoplanet Survey Satellite (TESS). We employed Python code\nto determine the eclipse epochs and orbital periods for each system, and\nconstructed O-C diagrams from observed and predicted eclipse epochs. By\nanalyzing the O-C diagrams of our target CVs, we have constrained values for\nchanges in orbital period with time. Our targets include a sample of sources\nfrom each class of non-magnetic, eclipsing CVs: dwarf novae variables, Z Cam\ntype, and U Gem subclasses. We include in our study classical novae variables,\nnova-like variables (including the VY Scl and UX UMa subclasses), and recurrent\nnovae variable stars. We approached this project with goals of developing time\nseries analysis techniques for future undergraduate-level studies of eclipsing\nCVs, and how they may contribute to the understanding of their orbital\nevolution.",
        "Accurate RF propagation modeling in urban environments is critical for\ndeveloping digital spectrum twins and optimizing wireless communication\nsystems. We introduce OpenGERT, an open-source automated Geometry Extraction\ntool for Ray Tracing, which collects and processes terrain and building data\nfrom OpenStreetMap, Microsoft Global ML Building Footprints, and USGS elevation\ndata. Using the Blender Python API, it creates detailed urban models for\nhigh-fidelity simulations with NVIDIA Sionna RT. We perform sensitivity\nanalyses to examine how variations in building height, position, and\nelectromagnetic material properties affect ray-tracing accuracy. Specifically,\nwe present pairwise dispersion plots of channel statistics (path gain, mean\nexcess delay, delay spread, link outage, and Rician K-factor) and investigate\nhow their sensitivities change with distance from transmitters. We also\nvisualize the variance of these statistics for selected transmitter locations\nto gain deeper insights. Our study covers Munich and Etoile scenes, each with\n10 transmitter locations. For each location, we apply five types of\nperturbations: material, position, height, height-position, and all combined,\nwith 50 perturbations each. Results show that small changes in permittivity and\nconductivity minimally affect channel statistics, whereas variations in\nbuilding height and position significantly alter all statistics, even with\nnoise standard deviations of 1 meter in height and 0.4 meters in position.\nThese findings highlight the importance of precise environmental modeling for\naccurate propagation predictions, essential for digital spectrum twins and\nadvanced communication networks. The code for geometry extraction and\nsensitivity analyses is available at github.com\/serhatadik\/OpenGERT\/.",
        "This study examines the gravitational and thermodynamic properties of static,\nspherically symmetric black holes within cosmic voids -- vast underdense\nregions of the universe. By deriving a novel solution based on a universal\ndensity profile for voids, we analyze its spacetime structure, which reveals\ntwo horizons: One of the black hole and the other related to the de Sitter-like\nbehavior. As the void approaches a perfect vacuum, the black hole horizon\ndiminishes, tending to that of the Schwarzschild solution, while the outer\nhorizon increases. We also study the solution stability via sound speed of the\nfluid, as well as the thermodynamic properties, including Hawking temperature,\nevaporation time, entropy, and specific heat. Our results show that as the void\nempties, the Hawking temperature rises, shortening evaporation times. The\nentropy follows the area's law and specific heat exhibits a minimum for a given\nblack hole size, indicating a thermal transition and highlighting the role of\nvoids in the black hole evolution. These findings offer new insights into the\nrelationship between local gravitational collapse and large-scale cosmic\nstructure, enhancing our understanding of the black hole behavior in underdense\nenvironments. We also provide a glimpse of a potential thermodynamic\ninteraction between the event horizon and the cosmological horizon.",
        "We introduce a mixed holomorphic-topological gauge theory in three dimensions\nassociated to a (freely generated) Poisson vertex algebra. The\n$\\lambda$-bracket of the PVA plays the role of the structure constants of the\ngauge algebra and the gauge invariance of the theory holds if and only if the\n$\\lambda$-bracket Jacobi identity is satisfied. We show that the\nholomorphic-topological symmetry of the theory enhances to full topological\nsymmetry if the Poisson vertex algebra contains a Virasoro element. We outline\nexamples associated to PVAs of $\\mathcal{W}$-type and demonstrate their\nconnections to various versions of $3d$ gravity. We expect the\nthree-dimensional Poisson sigma model to play an important role in the\ndeformation quantization of Poisson vertex algebras.",
        "Understanding the evolution of the complex magnetic fields found in solar\nactive regions is an active area of research. There are numerous models for\nsuch fields which range in their complexity due to the number of known physical\neffects included in them, the one common factor being they all extrapolate the\nfield up from the photosphere. In this study we focus on the fact that, above\nthe photosphere, and below the corona, lies the relatively cool and dense\nchromosphere -- which is often neglected in coronal models due to it being\ncomparatively thin and difficult hard to model. We isolate and examine the\neffect including this boundary layer has on a 2.5D class of driven MHD models\nof an active region eruption. We find that it can result in significant changes\nto the dynamics of an erupting field far higher in the atmosphere than the\nchromosphere itself, generally delaying eruption and increasing the magnetic\nenergy released in each eruption. We also test whether these effects can be\napproximated using a variation of the more computationally efficient\nmagnetofrictional model, finding a number of simple adaptations of the standard\nmagnetofrictional model capture the effect the chromospheric stratification\nwell.",
        "Ceramic materials are widely used in high-strain-rate applications due to\ntheir exceptional strength-to-weight ratio. However, under these extreme\nconditions, spall failure becomes a critical concern, which is driven by a\nlarge hydrostatic tensile stress state. This study introduces a novel two-laser\nsetup to generate controlled hydrostatic stress states at specific locations\nwithin test specimens. By inducing and manipulating shock wave interactions, we\nachieve large hydrostatic compressive and tensile stresses at very\nhigh-strain-rates, enabling the controlled nucleation and growth of nanovoids\nleading to spall failure. Our experiments demonstrate that shock wave\ninterference can precisely trigger spallation at arbitrary locations in the\nspecimen thickness. To further validate our approach, we investigate alumina\nspall failure using molecular dynamics (MD) simulations with a custom-designed\ngraph neural network potential. The MD results show strong agreement with\nexperimentally estimated spall strength. These findings highlight the potential\nof the two-laser technique as a powerful tool for studying the early stages of\nspall failure in ceramics, paving the way for advanced materials testing\nmethodologies.",
        "We prove that bounded conciseness is a closed property in the space of marked\ngroups. As a consequence, we reformulate a conjecture of Fern\\'andez-Alcober\nand Shumyatsky [7] about conciseness in the class of residually finite groups."
      ]
    }
  },
  {
    "id":2411.00561,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"What is a cell type, really? The quest to categorize life's myriad forms.",
    "start_abstract":"The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
      ],
      "abstract":[
        "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "L2R: Learning to Reduce Search Space for Generalizable Neural Routing\n  Solver",
        "The Danger of Overthinking: Examining the Reasoning-Action Dilemma in\n  Agentic Tasks",
        "Perspectives for Direct Interpretability in Multi-Agent Deep\n  Reinforcement Learning",
        "Comprehensive Metapath-based Heterogeneous Graph Transformer for\n  Gene-Disease Association Prediction",
        "Analyzing sequential activity and travel decisions with interpretable\n  deep inverse reinforcement learning",
        "The Explanation Game -- Rekindled (Extended Version)",
        "CoT-VLM4Tar: Chain-of-Thought Guided Vision-Language Models for Traffic\n  Anomaly Resolution",
        "Interpretable Machine Learning for Oral Lesion Diagnosis through\n  Prototypical Instances Identification",
        "FlowAgent: Achieving Compliance and Flexibility for Workflow Agents",
        "Saarthi: The First AI Formal Verification Engineer",
        "Neuro-Symbolic AI in 2024: A Systematic Review",
        "Rethinking Relation Extraction: Beyond Shortcuts to Generalization with\n  a Debiased Benchmark",
        "Aligning Instruction Tuning with Pre-training",
        "Punch Out Model Synthesis: A Stochastic Algorithm for Constraint Based\n  Tiling Generation",
        "A Unifying View of Linear Function Approximation in Off-Policy RL\n  Through Matrix Splitting and Preconditioning",
        "Cup Products on Hochschild Cohomology of Hopf-Galois Extensions.pdf",
        "Enhancing Retrieval Systems with Inference-Time Logical Reasoning",
        "Evaluation of Hate Speech Detection Using Large Language Models and\n  Geographical Contextualization",
        "Verification of Bit-Flip Attacks against Quantized Neural Networks",
        "Parallelizing Multi-objective A* Search",
        "The Role of Artificial Intelligence in Enhancing Insulin Recommendations\n  and Therapy Outcomes",
        "Algorithmical Aspects of Some Bio Inspired Operations",
        "Proximal Flow Inspired Multi-Step Methods",
        "E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot\n  Object Customization",
        "Fast-COS: A Fast One-Stage Object Detector Based on Reparameterized\n  Attention Vision Transformer for Autonomous Driving",
        "One-Loop QCD Corrections to $\\bar{u}d \\rightarrow t\\bar{t}W$ at\n  $\\mathcal{O}(\\varepsilon^2)$",
        "Unconstrained Body Recognition at Altitude and Range: Comparing Four\n  Approaches",
        "Realization of Two-dimensional Discrete Time Crystals with Anisotropic\n  Heisenberg Coupling"
      ],
      "abstract":[
        "Constructive neural combinatorial optimization (NCO) has attracted growing\nresearch attention due to its ability to solve complex routing problems without\nrelying on handcrafted rules. However, existing NCO methods face significant\nchallenges in generalizing to large-scale problems due to high computational\ncomplexity and inefficient capture of structural patterns. To address this\nissue, we propose a novel learning-based search space reduction method that\nadaptively selects a small set of promising candidate nodes at each step of the\nconstructive NCO process. Unlike traditional methods that rely on fixed\nheuristics, our selection model dynamically prioritizes nodes based on learned\npatterns, significantly reducing the search space while maintaining solution\nquality. Experimental results demonstrate that our method, trained solely on\n100-node instances from uniform distribution, generalizes remarkably well to\nlarge-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) instances with up to 1 million nodes from the uniform\ndistribution and over 80K nodes from other distributions.",
        "Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving\ncapabilities, but their effectiveness in interactive environments can be\nlimited. This paper introduces and analyzes overthinking in LRMs. A phenomenon\nwhere models favor extended internal reasoning chains over environmental\ninteraction. Through experiments on software engineering tasks using SWE Bench\nVerified, we observe three recurring patterns: Analysis Paralysis, Rogue\nActions, and Premature Disengagement. We propose a framework to study these\nbehaviors, which correlates with human expert assessments, and analyze 4018\ntrajectories. We observe that higher overthinking scores correlate with\ndecreased performance, with reasoning models exhibiting stronger tendencies\ntoward overthinking compared to non-reasoning models. Our analysis reveals that\nsimple efforts to mitigate overthinking in agentic environments, such as\nselecting the solution with the lower overthinking score, can improve model\nperformance by almost 30% while reducing computational costs by 43%. These\nresults suggest that mitigating overthinking has strong practical implications.\nWe suggest that by leveraging native function-calling capabilities and\nselective reinforcement learning overthinking tendencies could be mitigated. We\nalso open-source our evaluation framework and dataset to facilitate research in\nthis direction at https:\/\/github.com\/AlexCuadron\/Overthinking.",
        "Multi-Agent Deep Reinforcement Learning (MADRL) was proven efficient in\nsolving complex problems in robotics or games, yet most of the trained models\nare hard to interpret. While learning intrinsically interpretable models\nremains a prominent approach, its scalability and flexibility are limited in\nhandling complex tasks or multi-agent dynamics. This paper advocates for direct\ninterpretability, generating post hoc explanations directly from trained\nmodels, as a versatile and scalable alternative, offering insights into agents'\nbehaviour, emergent phenomena, and biases without altering models'\narchitectures. We explore modern methods, including relevance backpropagation,\nknowledge edition, model steering, activation patching, sparse autoencoders and\ncircuit discovery, to highlight their applicability to single-agent,\nmulti-agent, and training process challenges. By addressing MADRL\ninterpretability, we propose directions aiming to advance active topics such as\nteam identification, swarm coordination and sample efficiency.",
        "Discovering gene-disease associations is crucial for understanding disease\nmechanisms, yet identifying these associations remains challenging due to the\ntime and cost of biological experiments. Computational methods are increasingly\nvital for efficient and scalable gene-disease association prediction.\nGraph-based learning models, which leverage node features and network\nrelationships, are commonly employed for biomolecular predictions. However,\nexisting methods often struggle to effectively integrate node features,\nheterogeneous structures, and semantic information. To address these\nchallenges, we propose COmprehensive MEtapath-based heterogeneous graph\nTransformer(COMET) for predicting gene-disease associations. COMET integrates\ndiverse datasets to construct comprehensive heterogeneous networks,\ninitializing node features with BioGPT. We define seven Metapaths and utilize a\ntransformer framework to aggregate Metapath instances, capturing global\ncontexts and long-distance dependencies. Through intra- and inter-metapath\naggregation using attention mechanisms, COMET fuses latent vectors from\nmultiple Metapaths to enhance GDA prediction accuracy. Our method demonstrates\nsuperior robustness compared to state-of-the-art approaches. Ablation studies\nand visualizations validate COMET's effectiveness, providing valuable insights\nfor advancing human health research.",
        "Travel demand modeling has shifted from aggregated trip-based models to\nbehavior-oriented activity-based models because daily trips are essentially\ndriven by human activities. To analyze the sequential activity-travel\ndecisions, deep inverse reinforcement learning (DIRL) has proven effective in\nlearning the decision mechanisms by approximating a reward function to\nrepresent preferences and a policy function to replicate observed behavior\nusing deep neural networks (DNNs). However, most existing research has focused\non using DIRL to enhance only prediction accuracy, with limited exploration\ninto interpreting the underlying decision mechanisms guiding sequential\ndecision-making. To address this gap, we introduce an interpretable DIRL\nframework for analyzing activity-travel decision processes, bridging the gap\nbetween data-driven machine learning and theory-driven behavioral models. Our\nproposed framework adapts an adversarial IRL approach to infer the reward and\npolicy functions of activity-travel behavior. The policy function is\ninterpreted through a surrogate interpretable model based on choice\nprobabilities from the policy function, while the reward function is\ninterpreted by deriving both short-term rewards and long-term returns for\nvarious activity-travel patterns. Our analysis of real-world travel survey data\nreveals promising results in two key areas: (i) behavioral pattern insights\nfrom the policy function, highlighting critical factors in decision-making and\nvariations among socio-demographic groups, and (ii) behavioral preference\ninsights from the reward function, indicating the utility individuals gain from\nspecific activity sequences.",
        "Recent work demonstrated the existence of critical flaws in the current use\nof Shapley values in explainable AI (XAI), i.e. the so-called SHAP scores.\nThese flaws are significant in that the scores provided to a human\ndecision-maker can be misleading. Although these negative results might appear\nto indicate that Shapley values ought not be used in XAI, this paper argues\notherwise. Concretely, this paper proposes a novel definition of SHAP scores\nthat overcomes existing flaws. Furthermore, the paper outlines a practically\nefficient solution for the rigorous estimation of the novel SHAP scores.\nPreliminary experimental results confirm our claims, and further underscore the\nflaws of the current SHAP scores.",
        "With the acceleration of urbanization, modern urban traffic systems are\nbecoming increasingly complex, leading to frequent traffic anomalies. These\nanomalies encompass not only common traffic jams but also more challenging\nissues such as phantom traffic jams, intersection deadlocks, and accident\nliability analysis, which severely impact traffic flow, vehicular safety, and\noverall transportation efficiency. Currently, existing solutions primarily rely\non manual intervention by traffic police or artificial intelligence-based\ndetection systems. However, these methods often suffer from response delays and\ninconsistent management due to inadequate resources, while AI detection\nsystems, despite enhancing efficiency to some extent, still struggle to handle\ncomplex traffic anomalies in a real-time and precise manner. To address these\nissues, we propose CoT-VLM4Tar: (Chain of Thought Visual-Language Model for\nTraffic Anomaly Resolution), this innovative approach introduces a new\nchain-of-thought to guide the VLM in analyzing, reasoning, and generating\nsolutions for traffic anomalies with greater reasonable and effective solution,\nand to evaluate the performance and effectiveness of our method, we developed a\nclosed-loop testing framework based on the CARLA simulator. Furthermore, to\nensure seamless integration of the solutions generated by the VLM with the\nCARLA simulator, we implement an itegration module that converts these\nsolutions into executable commands. Our results demonstrate the effectiveness\nof VLM in the resolution of real-time traffic anomalies, providing a\nproof-of-concept for its integration into autonomous traffic management\nsystems.",
        "Decision-making processes in healthcare can be highly complex and\nchallenging. Machine Learning tools offer significant potential to assist in\nthese processes. However, many current methodologies rely on complex models\nthat are not easily interpretable by experts. This underscores the need to\ndevelop interpretable models that can provide meaningful support in clinical\ndecision-making. When approaching such tasks, humans typically compare the\nsituation at hand to a few key examples and representative cases imprinted in\ntheir memory. Using an approach which selects such exemplary cases and grounds\nits predictions on them could contribute to obtaining high-performing\ninterpretable solutions to such problems. To this end, we evaluate PivotTree,\nan interpretable prototype selection model, on an oral lesion detection\nproblem, specifically trying to detect the presence of neoplastic, aphthous and\ntraumatic ulcerated lesions from oral cavity images. We demonstrate the\nefficacy of using such method in terms of performance and offer a qualitative\nand quantitative comparison between exemplary cases and ground-truth prototypes\nselected by experts.",
        "The integration of workflows with large language models (LLMs) enables\nLLM-based agents to execute predefined procedures, enhancing automation in\nreal-world applications. Traditional rule-based methods tend to limit the\ninherent flexibility of LLMs, as their predefined execution paths restrict the\nmodels' action space, particularly when the unexpected, out-of-workflow (OOW)\nqueries are encountered. Conversely, prompt-based methods allow LLMs to fully\ncontrol the flow, which can lead to diminished enforcement of procedural\ncompliance. To address these challenges, we introduce FlowAgent, a novel agent\nframework designed to maintain both compliance and flexibility. We propose the\nProcedure Description Language (PDL), which combines the adaptability of\nnatural language with the precision of code to formulate workflows. Building on\nPDL, we develop a comprehensive framework that empowers LLMs to manage OOW\nqueries effectively, while keeping the execution path under the supervision of\na set of controllers. Additionally, we present a new evaluation methodology to\nrigorously assess an LLM agent's ability to handle OOW scenarios, going beyond\nroutine flow compliance tested in existing benchmarks. Experiments on three\ndatasets demonstrate that FlowAgent not only adheres to workflows but also\neffectively manages OOW queries, highlighting its dual strengths in compliance\nand flexibility. The code is available at\nhttps:\/\/github.com\/Lightblues\/FlowAgent.",
        "Recently, Devin has made a significant buzz in the Artificial Intelligence\n(AI) community as the world's first fully autonomous AI software engineer,\ncapable of independently developing software code. Devin uses the concept of\nagentic workflow in Generative AI (GenAI), which empowers AI agents to engage\nin a more dynamic, iterative, and self-reflective process. In this paper, we\npresent a similar fully autonomous AI formal verification engineer, Saarthi,\ncapable of verifying a given RTL design end-to-end using an agentic workflow.\nWith Saarthi, verification engineers can focus on more complex problems, and\nverification teams can strive for more ambitious goals. The domain-agnostic\nimplementation of Saarthi makes it scalable for use across various domains such\nas RTL design, UVM-based verification, and others.",
        "Background: The field of Artificial Intelligence has undergone cyclical\nperiods of growth and decline, known as AI summers and winters. Currently, we\nare in the third AI summer, characterized by significant advancements and\ncommercialization, particularly in the integration of Symbolic AI and\nSub-Symbolic AI, leading to the emergence of Neuro-Symbolic AI.\n  Methods: The review followed the PRISMA methodology, utilizing databases such\nas IEEE Explore, Google Scholar, arXiv, ACM, and SpringerLink. The inclusion\ncriteria targeted peer-reviewed papers published between 2020 and 2024. Papers\nwere screened for relevance to Neuro-Symbolic AI, with further inclusion based\non the availability of associated codebases to ensure reproducibility.\n  Results: From an initial pool of 1,428 papers, 167 met the inclusion criteria\nand were analyzed in detail. The majority of research efforts are concentrated\nin the areas of learning and inference (63%), logic and reasoning (35%), and\nknowledge representation (44%). Explainability and trustworthiness are less\nrepresented (28%), with Meta-Cognition being the least explored area (5%). The\nreview identifies significant interdisciplinary opportunities, particularly in\nintegrating explainability and trustworthiness with other research areas.\n  Conclusion: Neuro-Symbolic AI research has seen rapid growth since 2020, with\nconcentrated efforts in learning and inference. Significant gaps remain in\nexplainability, trustworthiness, and Meta-Cognition. Addressing these gaps\nthrough interdisciplinary research will be crucial for advancing the field\ntowards more intelligent, reliable, and context-aware AI systems.",
        "Benchmarks are crucial for evaluating machine learning algorithm performance,\nfacilitating comparison and identifying superior solutions. However, biases\nwithin datasets can lead models to learn shortcut patterns, resulting in\ninaccurate assessments and hindering real-world applicability. This paper\naddresses the issue of entity bias in relation extraction tasks, where models\ntend to rely on entity mentions rather than context. We propose a debiased\nrelation extraction benchmark DREB that breaks the pseudo-correlation between\nentity mentions and relation types through entity replacement. DREB utilizes\nBias Evaluator and PPL Evaluator to ensure low bias and high naturalness,\nproviding a reliable and accurate assessment of model generalization in entity\nbias scenarios. To establish a new baseline on DREB, we introduce MixDebias, a\ndebiasing method combining data-level and model training-level techniques.\nMixDebias effectively improves model performance on DREB while maintaining\nperformance on the original dataset. Extensive experiments demonstrate the\neffectiveness and robustness of MixDebias compared to existing methods,\nhighlighting its potential for improving the generalization ability of relation\nextraction models. We will release DREB and MixDebias publicly.",
        "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose Aligning Instruction Tuning\nwith Pre-training (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs.",
        "As an artistic aid in tiled level design, Constraint Based Tiling Generation\n(CBTG) algorithms can help to automatically create level realizations from a\nset of tiles and placement constraints. Merrell's Modify in Blocks Model\nSynthesis (MMS) and Gumin's Wave Function Collapse (WFC) have been proposed as\nConstraint Based Tiling Generation (CBTG) algorithms that work well for many\nscenarios but have limitations in problem size, problem setup and solution\nbiasing. We present Punch Out Model Synthesis (POMS), a Constraint Based Tiling\nGeneration algorithm, that can handle large problem sizes, requires minimal\nassumptions for setup and can help mitigate solution biasing. POMS attempts to\nresolve indeterminate grid regions by trying to progressively realize\nsub-blocks, performing a stochastic boundary erosion on previously resolved\nregions should sub-block resolution fail. We highlight the results of running a\nreference implementation on different tile sets and discuss a tile correlation\nlength, implied by the tile constraints, and its role in choosing an\nappropriate block size to aid POMS in successfully finding grid realizations.",
        "Traditionally, TD and FQI are viewed as differing in the number of updates\ntoward the target value function: TD makes one update, FQI makes an infinite\nnumber, and Partial Fitted Q-Iteration (PFQI) performs a finite number, such as\nthe use of a target network in Deep Q-Networks (DQN) in the OPE setting. This\nperspective, however, fails to capture the convergence connections between\nthese algorithms and may lead to incorrect conclusions, for example, that the\nconvergence of TD implies the convergence of FQI. In this paper, we focus on\nlinear value function approximation and offer a new perspective, unifying TD,\nFQI, and PFQI as the same iterative method for solving the Least Squares\nTemporal Difference (LSTD) system, but using different preconditioners and\nmatrix splitting schemes. TD uses a constant preconditioner, FQI employs a\ndata-feature adaptive preconditioner, and PFQI transitions between the two.\nThen, we reveal that in the context of linear function approximation,\nincreasing the number of updates under the same target value function\nessentially represents a transition from using a constant preconditioner to\ndata-feature adaptive preconditioner. This unifying perspective also simplifies\nthe analyses of the convergence conditions for these algorithms and clarifies\nmany issues. Consequently, we fully characterize the convergence of each\nalgorithm without assuming specific properties of the chosen features (e.g.,\nlinear independence). We also examine how common assumptions about feature\nrepresentations affect convergence, and discover new conditions on features\nthat are important for convergence. These convergence conditions allow us to\nestablish the convergence connections between these algorithms and to address\nimportant questions.",
        "In this paper, we give an explicit chain map, which induces the algebra\nisomorphism between the Hochschild cohomology ${\\bf HH}^{\\bullet}(B)$ and the\n$H$-invariant subalgebra ${\\bf H}^{\\bullet}(A, B)^{H}$ under two mild\nhypotheses, where $H$ is a finite dimensional semisimple Hopf algebra and $B$\nis an $H$-Galois extension of $A$. In particular, the smash product $B=A\\#H$\nalways satisfies the mild hypotheses. The isomorphism between ${\\bf\nHH}^{\\bullet}(A\\#H)$ and ${\\bf H}^{\\bullet}(A, A\\#H)^{H}$ generalizes the\nclassical result of group actions. As an application, Hochschild cohomology and\ncup product of the smash product of the quantum $(-1)$-plane and Kac--Paljutkin\nHopf algebra are computed.",
        "Traditional retrieval methods rely on transforming user queries into vector\nrepresentations and retrieving documents based on cosine similarity within an\nembedding space. While efficient and scalable, this approach often fails to\nhandle complex queries involving logical constructs such as negations,\nconjunctions, and disjunctions. In this paper, we propose a novel\ninference-time logical reasoning framework that explicitly incorporates logical\nreasoning into the retrieval process. Our method extracts logical reasoning\nstructures from natural language queries and then composes the individual\ncosine similarity scores to formulate the final document scores. This approach\nenables the retrieval process to handle complex logical reasoning without\ncompromising computational efficiency. Our results on both synthetic and\nreal-world benchmarks demonstrate that the proposed method consistently\noutperforms traditional retrieval methods across different models and datasets,\nsignificantly improving retrieval performance for complex queries.",
        "The proliferation of hate speech on social media is one of the serious issues\nthat is bringing huge impacts to society: an escalation of violence,\ndiscrimination, and social fragmentation. The problem of detecting hate speech\nis intrinsically multifaceted due to cultural, linguistic, and contextual\ncomplexities and adversarial manipulations. In this study, we systematically\ninvestigate the performance of LLMs on detecting hate speech across\nmultilingual datasets and diverse geographic contexts. Our work presents a new\nevaluation framework in three dimensions: binary classification of hate speech,\ngeography-aware contextual detection, and robustness to adversarially generated\ntext. Using a dataset of 1,000 comments from five diverse regions, we evaluate\nthree state-of-the-art LLMs: Llama2 (13b), Codellama (7b), and DeepSeekCoder\n(6.7b). Codellama had the best binary classification recall with 70.6% and an\nF1-score of 52.18%, whereas DeepSeekCoder had the best performance in\ngeographic sensitivity, correctly detecting 63 out of 265 locations. The tests\nfor adversarial robustness also showed significant weaknesses; Llama2\nmisclassified 62.5% of manipulated samples. These results bring to light the\ntrade-offs between accuracy, contextual understanding, and robustness in the\ncurrent versions of LLMs. This work has thus set the stage for developing\ncontextually aware, multilingual hate speech detection systems by underlining\nkey strengths and limitations, therefore offering actionable insights for\nfuture research and real-world applications.",
        "In the rapidly evolving landscape of neural network security, the resilience\nof neural networks against bit-flip attacks (i.e., an attacker maliciously\nflips an extremely small amount of bits within its parameter storage memory\nsystem to induce harmful behavior), has emerged as a relevant area of research.\nExisting studies suggest that quantization may serve as a viable defense\nagainst such attacks. Recognizing the documented susceptibility of real-valued\nneural networks to such attacks and the comparative robustness of quantized\nneural networks (QNNs), in this work, we introduce BFAVerifier, the first\nverification framework designed to formally verify the absence of bit-flip\nattacks or to identify all vulnerable parameters in a sound and rigorous\nmanner. BFAVerifier comprises two integral components: an abstraction-based\nmethod and an MILP-based method. Specifically, we first conduct a reachability\nanalysis with respect to symbolic parameters that represent the potential\nbit-flip attacks, based on a novel abstract domain with a sound guarantee. If\nthe reachability analysis fails to prove the resilience of such attacks, then\nwe encode this verification problem into an equivalent MILP problem which can\nbe solved by off-the-shelf solvers. Therefore, BFAVerifier is sound, complete,\nand reasonably efficient. We conduct extensive experiments, which demonstrate\nits effectiveness and efficiency across various network architectures,\nquantization bit-widths, and adversary capabilities.",
        "The Multi-objective Shortest Path (MOSP) problem is a classic network\noptimization problem that aims to find all Pareto-optimal paths between two\npoints in a graph with multiple edge costs. Recent studies on multi-objective\nsearch with A* (MOA*) have demonstrated superior performance in solving\ndifficult MOSP instances. This paper presents a novel search framework that\nallows efficient parallelization of MOA* with different objective orders. The\nframework incorporates a unique upper bounding strategy that helps the search\nreduce the problem's dimensionality to one in certain cases. Experimental\nresults demonstrate that the proposed framework can enhance the performance of\nrecent A*-based solutions, with the speed-up proportional to the problem\ndimension.",
        "The growing worldwide incidence of diabetes requires more effective\napproaches for managing blood glucose levels. Insulin delivery systems have\nadvanced significantly, with artificial intelligence (AI) playing a key role in\nimproving their precision and adaptability. AI algorithms, particularly those\nbased on reinforcement learning, allow for personalised insulin dosing by\ncontinuously adapting to an individual's responses. Despite these advancements,\nchallenges such as data privacy, algorithm transparency, and accessibility\nstill need to be addressed. Continued progress and validation in AI-driven\ninsulin delivery systems promise to improve therapy outcomes further, offering\npeople more effective and individualised management of their diabetes. This\npaper presents an overview of current strategies, key challenges, and future\ndirections.",
        "This thesis investigates three biologically inspired operations:\nprefix-suffix duplication, bounded prefix-suffix duplication, and\nprefix-suffix-square completion. Duplication, a common genetic mutation,\ninvolves repeating DNA sequences and is modeled here as formal operations on\nwords. The prefix-suffix duplication generates non-context-free languages, even\nfrom simple initial words. To better reflect biological processes, we propose a\nbounded variant that limits duplication length, resolving unsolved problems and\naligning with biochemical realities.\n  We also introduce the prefix-suffix-square completion operation, which\ngenerates squares at sequence ends. This operation enables the generation of\ninfinite words such as Fibonacci, Period-doubling, and Thue-Morse, which\ncontain squares but avoid higher exponent repetitions, highlighting unique\nstructural properties. In contrast, prefix-suffix duplication cannot generate\ncertain infinite words, such as Thue-Morse, but can produce cube-free words.\n  Additionally, we address the detection of gapped repeats and\npalindromes-structures important in DNA and RNA analysis. These involve\nrepeating or reversed factors flanking a central gap. Previous studies imposed\nconstraints on gap length or arm-gap relationships; we extend this by solving\nthe problem in three novel settings. This work advances theoretical insights\ninto biologically inspired operations and their computational applications in\ngenetic modeling.",
        "We investigate a family of approximate multi-step proximal point methods,\nframed as implicit linear discretizations of gradient flow. The resulting\nmethods are multi-step proximal point methods, with similar computational cost\nin each update as the proximal point method. We explore several optimization\nmethods where applying an approximate multistep proximal points method results\nin improved convergence behavior. We also include convergence analysis for the\nproposed method in several problem settings: quadratic problems, general\nproblems that are strongly or weakly (non)convex, and accelerated results for\nalternating projections.",
        "We propose E-MD3C ($\\underline{E}$fficient $\\underline{M}$asked\n$\\underline{D}$iffusion Transformer with Disentangled $\\underline{C}$onditions\nand $\\underline{C}$ompact $\\underline{C}$ollector), a highly efficient\nframework for zero-shot object image customization. Unlike prior works reliant\non resource-intensive Unet architectures, our approach employs lightweight\nmasked diffusion transformers operating on latent patches, offering\nsignificantly improved computational efficiency. The framework integrates three\ncore components: (1) an efficient masked diffusion transformer for processing\nautoencoder latents, (2) a disentangled condition design that ensures\ncompactness while preserving background alignment and fine details, and (3) a\nlearnable Conditions Collector that consolidates multiple inputs into a compact\nrepresentation for efficient denoising and learning. E-MD3C outperforms the\nexisting approach on the VITON-HD dataset across metrics such as PSNR, FID,\nSSIM, and LPIPS, demonstrating clear advantages in parameters, memory\nefficiency, and inference speed. With only $\\frac{1}{4}$ of the parameters, our\nTransformer-based 468M model delivers $2.5\\times$ faster inference and uses\n$\\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent\ndiffusion model.",
        "The perception system is a a critical role of an autonomous driving system\nfor ensuring safety. The driving scene perception system fundamentally\nrepresents an object detection task that requires achieving a balance between\naccuracy and processing speed. Many contemporary methods focus on improving\ndetection accuracy but often overlook the importance of real-time detection\ncapabilities when computational resources are limited. Thus, it is vital to\ninvestigate efficient object detection strategies for driving scenes. This\npaper introduces Fast-COS, a novel single-stage object detection framework\ncrafted specifically for driving scene applications. The research initiates\nwith an analysis of the backbone, considering both macro and micro\narchitectural designs, yielding the Reparameterized Attention Vision\nTransformer (RAViT). RAViT utilizes Reparameterized Multi-Scale Depth-Wise\nConvolution (RepMSDW) and Reparameterized Self-Attention (RepSA) to enhance\ncomputational efficiency and feature extraction. In extensive tests across GPU,\nedge, and mobile platforms, RAViT achieves 81.4% Top-1 accuracy on the\nImageNet-1K dataset, demonstrating significant throughput improvements over\ncomparable backbone models such as ResNet, FastViT, RepViT, and\nEfficientFormer. Additionally, integrating RepMSDW into a feature pyramid\nnetwork forms RepFPN, enabling fast and multi-scale feature fusion. Fast-COS\nenhances object detection in driving scenes, attaining an AP50 score of 57.2%\non the BDD100K dataset and 80.0% on the TJU-DHD Traffic dataset. It surpasses\nleading models in efficiency, delivering up to 75.9% faster GPU inference and\n1.38 higher throughput on edge devices compared to FCOS, YOLOF, and RetinaNet.\nThese findings establish Fast-COS as a highly scalable and reliable solution\nsuitable for real-time applications, especially in resource-limited\nenvironments like autonomous driving systems",
        "We present a computation of the one-loop QCD corrections to top-quark pair\nproduction in association with a $W$ boson, including terms up to order\n$\\varepsilon^2$ in dimensional regularization. Providing a first glimpse into\nthe complexity of the corresponding two-loop amplitude, this result is a first\nstep towards a description of this process at next-to-next-to-leading order\n(NNLO) in QCD. We perform a tensor decomposition and express the corresponding\nform factors in terms of a basis of independent special functions with compact\nrational coefficients, providing a structured framework for future\ndevelopments. In addition, we derive an explicit analytic representation of the\nform factors, valid up to order $\\varepsilon^0$, expressed in terms of\nlogarithms and dilogarithms. For the complete set of special functions\nrequired, we obtain a semi-numerical solution based on generalized power series\nexpansion.",
        "This study presents an investigation of four distinct approaches to long-term\nperson identification using body shape. Unlike short-term re-identification\nsystems that rely on temporary features (e.g., clothing), we focus on learning\npersistent body shape characteristics that remain stable over time. We\nintroduce a body identification model based on a Vision Transformer (ViT) (Body\nIdentification from Diverse Datasets, BIDDS) and on a Swin-ViT model\n(Swin-BIDDS). We also expand on previous approaches based on the Linguistic and\nNon-linguistic Core ResNet Identity Models (LCRIM and NLCRIM), but with\nimproved training. All models are trained on a large and diverse dataset of\nover 1.9 million images of approximately 5k identities across 9 databases.\nPerformance was evaluated on standard re-identification benchmark datasets\n(MARS, MSMT17, Outdoor Gait, DeepChange) and on an unconstrained dataset that\nincludes images at a distance (from close-range to 1000m), at altitude (from an\nunmanned aerial vehicle, UAV), and with clothing change. A comparative analysis\nacross these models provides insights into how different backbone architectures\nand input image sizes impact long-term body identification performance across\nreal-world conditions.",
        "A discrete time crystal (DTC) is the paradigmatic example of a phase of\nmatter that occurs exclusively in systems out of equilibrium. This phenomenon\nis characterized by the spontaneous symmetry breaking of discrete\ntime-translation and provides a rich playground to study a fundamental question\nin statistical physics: what mechanism allows for driven quantum systems to\nexhibit emergent behavior that deviates from their counterparts with\ntime-independent evolution? Unlike equilibrium phases, DTCs exhibit macroscopic\nmanifestations of coherent quantum dynamics, challenging the conventional\nnarrative that thermodynamic behavior universally erases quantum signatures.\nHowever, due to the difficulty of simulating these systems with either\nclassical or quantum computers, previous studies have been limited to a set of\nmodels with Ising-like couplings -- and mostly only in one dimension -- thus\nprecluding our understanding of the existence (or not) of DTCs in models with\ninteractions that closely align with what occurs in nature. In this work, by\ncombining the latest generation of IBM quantum processors with state-of-the-art\ntensor network methods, we are able to demonstrate the existence of a DTC in a\ntwo-dimensional system governed by anisotropic Heisenberg interactions. Our\ncomprehensive analysis reveals a rich phase diagram encompassing spin-glass,\nergodic, and time-crystalline phases, highlighting the tunability of these\nphases through multiple control parameters. Crucially, our results emphasize\nthe interplay of initialization, interaction anisotropy, and driving protocols\nin stabilizing the DTC phase. By extending the study of Floquet matter beyond\nsimplified models, we lay the groundwork for exploring how driven systems\nbridge the gap between quantum coherence and emergent non-equilibrium\nthermodynamics."
      ]
    }
  },
  {
    "id":2411.00922,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images",
    "start_abstract":"Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
      ],
      "abstract":[
        "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Fast and Accurate Gigapixel Pathological Image Classification with\n  Hierarchical Distillation Multi-Instance Learning",
        "ProDehaze: Prompting Diffusion Models Toward Faithful Image Dehazing",
        "Fully Exploiting Vision Foundation Model's Profound Prior Knowledge for\n  Generalizable RGB-Depth Driving Scene Parsing",
        "Adding Additional Control to One-Step Diffusion with Joint Distribution\n  Matching",
        "Exploring Transferable Homogeneous Groups for Compositional Zero-Shot\n  Learning",
        "Soft Knowledge Distillation with Multi-Dimensional Cross-Net Attention\n  for Image Restoration Models Compression",
        "GOAL: Global-local Object Alignment Learning",
        "ViDDAR: Vision Language Model-Based Task-Detrimental Content Detection\n  for Augmented Reality",
        "DashCop: Automated E-ticket Generation for Two-Wheeler Traffic\n  Violations Using Dashcam Videos",
        "Hybrid State-Space and GRU-based Graph Tokenization Mamba for\n  Hyperspectral Image Classification",
        "HFMF: Hierarchical Fusion Meets Multi-Stream Models for Deepfake\n  Detection",
        "VLForgery Face Triad: Detection, Localization and Attribution via\n  Multimodal Large Language Models",
        "A Fusion Model for Artwork Identification Based on Convolutional Neural\n  Networks and Transformers",
        "HI-MaNGA: Results from (21cm-HI) single-dish observations of MaNGA\n  Survey Galaxies",
        "\\'El\\'ements de comptage sur les g\\'en\\'erateurs du groupe modulaire et\n  les $\\lambda$-quiddit\\'es",
        "Dark Energy Survey Year 6 Results: Synthetic-source Injection Across the\n  Full Survey Using Balrog",
        "On the Commuting Problem of Toeplitz Operators on the Harmonic Bergman\n  Space",
        "Generalization Performance of Hypergraph Neural Networks",
        "COARSE: Collaborative Pseudo-Labeling with Coarse Real Labels for\n  Off-Road Semantic Segmentation",
        "Properties of the one-component Coulomb gas on a sphere with two\n  macroscopic external charges",
        "Norm-one points in convex combinations of relatively weakly open subsets\n  of the unit ball in the spaces $L_1(\\mu,X)$",
        "Logic.py: Bridging the Gap between LLMs and Constraint Solvers",
        "Deriving motivic coactions and single-valued maps at genus zero from\n  zeta generators",
        "AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to\n  Counteract Alpha Decay",
        "Relative knot probabilities in confined lattice polygons",
        "Global branching of solutions to ODEs and integrability",
        "Kink dynamics for the Yang-Mills field in an extremal\n  Reissner-Nordstr\\\"om black hole",
        "Multimodal Emotion Recognition and Sentiment Analysis in Multi-Party\n  Conversation Contexts"
      ],
      "abstract":[
        "Although multi-instance learning (MIL) has succeeded in pathological image\nclassification, it faces the challenge of high inference costs due to\nprocessing numerous patches from gigapixel whole slide images (WSIs). To\naddress this, we propose HDMIL, a hierarchical distillation multi-instance\nlearning framework that achieves fast and accurate classification by\neliminating irrelevant patches. HDMIL consists of two key components: the\ndynamic multi-instance network (DMIN) and the lightweight instance\npre-screening network (LIPN). DMIN operates on high-resolution WSIs, while LIPN\noperates on the corresponding low-resolution counterparts. During training,\nDMIN are trained for WSI classification while generating attention-score-based\nmasks that indicate irrelevant patches. These masks then guide the training of\nLIPN to predict the relevance of each low-resolution patch. During testing,\nLIPN first determines the useful regions within low-resolution WSIs, which\nindirectly enables us to eliminate irrelevant regions in high-resolution WSIs,\nthereby reducing inference time without causing performance degradation. In\naddition, we further design the first Chebyshev-polynomials-based\nKolmogorov-Arnold classifier in computational pathology, which enhances the\nperformance of HDMIL through learnable activation layers. Extensive experiments\non three public datasets demonstrate that HDMIL outperforms previous\nstate-of-the-art methods, e.g., achieving improvements of 3.13% in AUC while\nreducing inference time by 28.6% on the Camelyon16 dataset.",
        "Recent approaches using large-scale pretrained diffusion models for image\ndehazing improve perceptual quality but often suffer from hallucination issues,\nproducing unfaithful dehazed image to the original one. To mitigate this, we\npropose ProDehaze, a framework that employs internal image priors to direct\nexternal priors encoded in pretrained models. We introduce two types of\n\\textit{selective} internal priors that prompt the model to concentrate on\ncritical image areas: a Structure-Prompted Restorer in the latent space that\nemphasizes structure-rich regions, and a Haze-Aware Self-Correcting Refiner in\nthe decoding process to align distributions between clearer input regions and\nthe output. Extensive experiments on real-world datasets demonstrate that\nProDehaze achieves high-fidelity results in image dehazing, particularly in\nreducing color shifts. Our code is at https:\/\/github.com\/TianwenZhou\/ProDehaze.",
        "Recent vision foundation models (VFMs), typically based on Vision Transformer\n(ViT), have significantly advanced numerous computer vision tasks. Despite\ntheir success in tasks focused solely on RGB images, the potential of VFMs in\nRGB-depth driving scene parsing remains largely under-explored. In this\narticle, we take one step toward this emerging research area by investigating a\nfeasible technique to fully exploit VFMs for generalizable RGB-depth driving\nscene parsing. Specifically, we explore the inherent characteristics of RGB and\ndepth data, thereby presenting a Heterogeneous Feature Integration Transformer\n(HFIT). This network enables the efficient extraction and integration of\ncomprehensive heterogeneous features without re-training ViTs. Relative depth\nprediction results from VFMs, used as inputs to the HFIT side adapter, overcome\nthe limitations of the dependence on depth maps. Our proposed HFIT demonstrates\nsuperior performance compared to all other traditional single-modal and\ndata-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the\nCityscapes and KITTI Semantics datasets. We believe this novel strategy paves\nthe way for future innovations in VFM-based data-fusion techniques for driving\nscene parsing. Our source code is publicly available at\nhttps:\/\/mias.group\/HFIT.",
        "While diffusion distillation has enabled one-step generation through methods\nlike Variational Score Distillation, adapting distilled models to emerging new\ncontrols -- such as novel structural constraints or latest user preferences --\nremains challenging. Conventional approaches typically requires modifying the\nbase diffusion model and redistilling it -- a process that is both\ncomputationally intensive and time-consuming. To address these challenges, we\nintroduce Joint Distribution Matching (JDM), a novel approach that minimizes\nthe reverse KL divergence between image-condition joint distributions. By\nderiving a tractable upper bound, JDM decouples fidelity learning from\ncondition learning. This asymmetric distillation scheme enables our one-step\nstudent to handle controls unknown to the teacher model and facilitates\nimproved classifier-free guidance (CFG) usage and seamless integration of human\nfeedback learning (HFL). Experimental results demonstrate that JDM surpasses\nbaseline methods such as multi-step ControlNet by mere one-step in most cases,\nwhile achieving state-of-the-art performance in one-step text-to-image\nsynthesis through improved usage of CFG or HFL integration.",
        "Conditional dependency present one of the trickiest problems in Compositional\nZero-Shot Learning, leading to significant property variations of the same\nstate (object) across different objects (states). To address this problem,\nexisting approaches often adopt either all-to-one or one-to-one representation\nparadigms. However, these extremes create an imbalance in the seesaw between\ntransferability and discriminability, favoring one at the expense of the other.\nComparatively, humans are adept at analogizing and reasoning in a hierarchical\nclustering manner, intuitively grouping categories with similar properties to\nform cohesive concepts. Motivated by this, we propose Homogeneous Group\nRepresentation Learning (HGRL), a new perspective formulates state (object)\nrepresentation learning as multiple homogeneous sub-group representation\nlearning. HGRL seeks to achieve a balance between semantic transferability and\ndiscriminability by adaptively discovering and aggregating categories with\nshared properties, learning distributed group centers that retain\ngroup-specific discriminative features. Our method integrates three core\ncomponents designed to simultaneously enhance both the visual and prompt\nrepresentation capabilities of the model. Extensive experiments on three\nbenchmark datasets validate the effectiveness of our method.",
        "Transformer-based encoder-decoder models have achieved remarkable success in\nimage-to-image transfer tasks, particularly in image restoration. However,\ntheir high computational complexity-manifested in elevated FLOPs and parameter\ncounts-limits their application in real-world scenarios. Existing knowledge\ndistillation methods in image restoration typically employ lightweight student\nmodels that directly mimic the intermediate features and reconstruction results\nof the teacher, overlooking the implicit attention relationships between them.\nTo address this, we propose a Soft Knowledge Distillation (SKD) strategy that\nincorporates a Multi-dimensional Cross-net Attention (MCA) mechanism for\ncompressing image restoration models. This mechanism facilitates interaction\nbetween the student and teacher across both channel and spatial dimensions,\nenabling the student to implicitly learn the attention matrices. Additionally,\nwe employ a Gaussian kernel function to measure the distance between student\nand teacher features in kernel space, ensuring stable and efficient feature\nlearning. To further enhance the quality of reconstructed images, we replace\nthe commonly used L1 or KL divergence loss with a contrastive learning loss at\nthe image level. Experiments on three tasks-image deraining, deblurring, and\ndenoising-demonstrate that our SKD strategy significantly reduces computational\ncomplexity while maintaining strong image restoration capabilities.",
        "Vision-language models like CLIP have shown impressive capabilities in\naligning images and text, but they often struggle with lengthy and detailed\ntext descriptions because of their training focus on short and concise\ncaptions. We present GOAL (Global-local Object Alignment Learning), a novel\nfine-tuning method that enhances CLIP's ability to handle lengthy text by\nleveraging both global and local semantic alignments between image and lengthy\ntext. Our approach consists of two key components: Local Image-Sentence\nMatching (LISM), which identifies corresponding pairs between image segments\nand descriptive sentences, and Token Similarity-based Learning (TSL), which\nefficiently propagates local element attention through these matched pairs.\nEvaluating GOAL on three new benchmarks for image-lengthy text retrieval, we\ndemonstrate significant improvements over baseline CLIP fine-tuning,\nestablishing a simple yet effective approach for adapting CLIP to detailed\ntextual descriptions. Through extensive experiments, we show that our method's\nfocus on local semantic alignment alongside global context leads to more\nnuanced and representative embeddings, particularly beneficial for tasks\nrequiring fine-grained understanding of lengthy text descriptions.",
        "In Augmented Reality (AR), virtual content enhances user experience by\nproviding additional information. However, improperly positioned or designed\nvirtual content can be detrimental to task performance, as it can impair users'\nability to accurately interpret real-world information. In this paper we\nexamine two types of task-detrimental virtual content: obstruction attacks, in\nwhich virtual content prevents users from seeing real-world objects, and\ninformation manipulation attacks, in which virtual content interferes with\nusers' ability to accurately interpret real-world information. We provide a\nmathematical framework to characterize these attacks and create a custom\nopen-source dataset for attack evaluation. To address these attacks, we\nintroduce ViDDAR (Vision language model-based Task-Detrimental content Detector\nfor Augmented Reality), a comprehensive full-reference system that leverages\nVision Language Models (VLMs) and advanced deep learning techniques to monitor\nand evaluate virtual content in AR environments, employing a user-edge-cloud\narchitecture to balance performance with low latency. To the best of our\nknowledge, ViDDAR is the first system to employ VLMs for detecting\ntask-detrimental content in AR settings. Our evaluation results demonstrate\nthat ViDDAR effectively understands complex scenes and detects task-detrimental\ncontent, achieving up to 92.15% obstruction detection accuracy with a detection\nlatency of 533 ms, and an 82.46% information manipulation content detection\naccuracy with a latency of 9.62 s.",
        "Motorized two-wheelers are a prevalent and economical means of\ntransportation, particularly in the Asia-Pacific region. However, hazardous\ndriving practices such as triple riding and non-compliance with helmet\nregulations contribute significantly to accident rates. Addressing these\nviolations through automated enforcement mechanisms can enhance traffic safety.\nIn this paper, we propose DashCop, an end-to-end system for automated E-ticket\ngeneration. The system processes vehicle-mounted dashcam videos to detect\ntwo-wheeler traffic violations. Our contributions include: (1) a novel\nSegmentation and Cross-Association (SAC) module to accurately associate riders\nwith their motorcycles, (2) a robust cross-association-based tracking algorithm\noptimized for the simultaneous presence of riders and motorcycles, and (3) the\nRideSafe-400 dataset, a comprehensive annotated dashcam video dataset for\ntriple riding and helmet rule violations. Our system demonstrates significant\nimprovements in violation detection, validated through extensive evaluations on\nthe RideSafe-400 dataset.",
        "Hyperspectral image (HSI) classification plays a pivotal role in domains such\nas environmental monitoring, agriculture, and urban planning. However, it faces\nsignificant challenges due to the high-dimensional nature of the data and the\ncomplex spectral-spatial relationships inherent in HSI. Traditional methods,\nincluding conventional machine learning and convolutional neural networks\n(CNNs), often struggle to effectively capture these intricate spectral-spatial\nfeatures and global contextual information. Transformer-based models, while\npowerful in capturing long-range dependencies, often demand substantial\ncomputational resources, posing challenges in scenarios where labeled datasets\nare limited, as is commonly seen in HSI applications. To overcome these\nchallenges, this work proposes GraphMamba, a hybrid model that combines\nspectral-spatial token generation, graph-based token prioritization, and\ncross-attention mechanisms. The model introduces a novel hybridization of\nstate-space modeling and Gated Recurrent Units (GRU), capturing both linear and\nnonlinear spatial-spectral dynamics. GraphMamba enhances the ability to model\ncomplex spatial-spectral relationships while maintaining scalability and\ncomputational efficiency across diverse HSI datasets. Through comprehensive\nexperiments, we demonstrate that GraphMamba outperforms existing\nstate-of-the-art models, offering a scalable and robust solution for complex\nHSI classification tasks.",
        "The rapid progress in deep generative models has led to the creation of\nincredibly realistic synthetic images that are becoming increasingly difficult\nto distinguish from real-world data. The widespread use of Variational Models,\nDiffusion Models, and Generative Adversarial Networks has made it easier to\ngenerate convincing fake images and videos, which poses significant challenges\nfor detecting and mitigating the spread of misinformation. As a result,\ndeveloping effective methods for detecting AI-generated fakes has become a\npressing concern. In our research, we propose HFMF, a comprehensive two-stage\ndeepfake detection framework that leverages both hierarchical cross-modal\nfeature fusion and multi-stream feature extraction to enhance detection\nperformance against imagery produced by state-of-the-art generative AI models.\nThe first component of our approach integrates vision Transformers and\nconvolutional nets through a hierarchical feature fusion mechanism. The second\ncomponent of our framework combines object-level information and a fine-tuned\nconvolutional net model. We then fuse the outputs from both components via an\nensemble deep neural net, enabling robust classification performances. We\ndemonstrate that our architecture achieves superior performance across diverse\ndataset benchmarks while maintaining calibration and interoperability.",
        "Faces synthesized by diffusion models (DMs) with high-quality and\ncontrollable attributes pose a significant challenge for Deepfake detection.\nMost state-of-the-art detectors only yield a binary decision, incapable of\nforgery localization, attribution of forgery methods, and providing analysis on\nthe cause of forgeries. In this work, we integrate Multimodal Large Language\nModels (MLLMs) within DM-based face forensics, and propose a fine-grained\nanalysis triad framework called VLForgery, that can 1) predict falsified facial\nimages; 2) locate the falsified face regions subjected to partial synthesis;\nand 3) attribute the synthesis with specific generators. To achieve the above\ngoals, we introduce VLF (Visual Language Forensics), a novel and diverse\nsynthesis face dataset designed to facilitate rich interactions between Visual\nand Language modalities in MLLMs. Additionally, we propose an extrinsic\nknowledge-guided description method, termed EkCot, which leverages knowledge\nfrom the image generation pipeline to enable MLLMs to quickly capture image\ncontent. Furthermore, we introduce a low-level vision comparison pipeline\ndesigned to identify differential features between real and fake that MLLMs can\ninherently understand. These features are then incorporated into EkCot,\nenhancing its ability to analyze forgeries in a structured manner, following\nthe sequence of detection, localization, and attribution. Extensive experiments\ndemonstrate that VLForgery outperforms other state-of-the-art forensic\napproaches in detection accuracy, with additional potential for falsified\nregion localization and attribution analysis.",
        "The identification of artwork is crucial in areas like cultural heritage\nprotection, art market analysis, and historical research. With the advancement\nof deep learning, Convolutional Neural Networks (CNNs) and Transformer models\nhave become key tools for image classification. While CNNs excel in local\nfeature extraction, they struggle with global context, and Transformers are\nstrong in capturing global dependencies but weak in fine-grained local details.\nTo address these challenges, this paper proposes a fusion model combining CNNs\nand Transformers for artwork identification. The model first extracts local\nfeatures using CNNs, then captures global context with a Transformer, followed\nby a feature fusion mechanism to enhance classification accuracy. Experiments\non Chinese and oil painting datasets show the fusion model outperforms\nindividual CNN and Transformer models, improving classification accuracy by\n9.7% and 7.1%, respectively, and increasing F1 scores by 0.06 and 0.05. The\nresults demonstrate the model's effectiveness and potential for future\nimprovements, such as multimodal integration and architecture optimization.",
        "In a poster presentation for IAU Symposium 392: \"Neutral hydrogen in and\naround galaxies in the SKA era\", we gave an overview of the HI-MaNGA project\nwhich is working to obtain complementary information about the cold gas\n(neutral hydrogen traced by the radio 21cm line) content of Mapping Nearby\nGalaxies at Apache Point Observatory (MaNGA) sample galaxies. MaNGA, part of\nthe fourth incarnation of the Sloan Digital Sky Surveys (SDSS-IV), obtained\nspatially resolved spectral maps for 10,000 nearby galaxies selected to create\na representative sample out of the SDSS Main Galaxy Sample. MaNGA data have\nprovided a census of the stellar and ionized gas content of these galaxies, as\nwell as kinematics of both stars and gas. Adding HI information via the\nHI-MaNGA program, which has observed or collected 21cm line data for 70% of the\nfull MaNGA sample, has been crucial for a number of applications, but\nespecially understanding the physical mechanisms that regulate gas accretion,\nand through that star formation and quenching of star formation. This\nconference proceedings article accompanies the release of the DR3 version of\nHI-MaNGA data.",
        "The aim of this article is to count the $n$-tuples of positive integers\n$(a_{1},\\ldots,a_{n})$ solutions of the equation $\\begin{pmatrix} a_{n} & -1\n\\\\[4pt] 1 & 0 \\end{pmatrix} \\begin{pmatrix} a_{n-1} & -1 \\\\[4pt] 1 & 0\n\\end{pmatrix} \\cdots \\begin{pmatrix} a_{1} & -1 \\\\[4pt] 1 & 0 \\end{pmatrix}=\\pm\nM$ when $M$ is equal to the generators of the modular group $S=\\begin{pmatrix}\n0 & -1 \\\\[4pt] 1 & 0 \\end{pmatrix}$ and $T=\\begin{pmatrix} 1 & 1 \\\\[4pt] 0 & 1\n\\end{pmatrix}$. To count these elements, we will study the\n$\\lambda$-quiddities, which are the solutions of the equation in the case\n$M=Id$ (related to Coxeter's friezes), whose last component is fixed.",
        "Synthetic source injection (SSI), the insertion of sources into pixel-level\non-sky images, is a powerful method for characterizing object detection and\nmeasurement in wide-field, astronomical imaging surveys. Within the Dark Energy\nSurvey (DES), SSI plays a critical role in characterizing all necessary\nalgorithms used in converting images to catalogs, and in deriving quantities\nneeded for the cosmology analysis, such as object detection rates, galaxy\nredshift estimation, galaxy magnification, star-galaxy classification, and\nphotometric performance. We present here a source injection catalog of $146$\nmillion injections spanning the entire 5000 deg$^2$ DES footprint, generated\nusing the Balrog SSI pipeline. Through this sample, we demonstrate that the DES\nYear 6 (Y6) image processing pipeline provides accurate estimates of the object\nproperties, for both galaxies and stars, at the percent-level, and we highlight\nspecific regimes where the accuracy is reduced. We then show the consistency\nbetween SSI and data catalogs, for all galaxy samples developed within the weak\nlensing and galaxy clustering analyses of DES Y6. The consistency between the\ntwo catalogs also extends to their correlations with survey observing\nproperties (seeing, airmass, depth, extinction, etc.). Finally, we highlight a\nnumber of applications of this catalog to the DES Y6 cosmology analysis. This\ndataset is the largest SSI catalog produced at this fidelity and will serve as\na key testing ground for exploring the utility of SSI catalogs in upcoming\nsurveys such as the Vera C. Rubin Observatory Legacy Survey of Space and Time.",
        "In this paper, we provide a complete characterization of bounded Toeplitz\noperators $T_f$ on the harmonic Bergman space of the unit disk, where the\nsymbol $f$ has a polar decomposition truncated above, that commute with\n$T_{z+\\bar{g}}$, for a bounded analytic function $g$.",
        "Hypergraph neural networks have been promising tools for handling learning\ntasks involving higher-order data, with notable applications in web graphs,\nsuch as modeling multi-way hyperlink structures and complex user interactions.\nYet, their generalization abilities in theory are less clear to us. In this\npaper, we seek to develop margin-based generalization bounds for four\nrepresentative classes of hypergraph neural networks, including\nconvolutional-based methods (UniGCN), set-based aggregation (AllDeepSets),\ninvariant and equivariant transformations (M-IGN), and tensor-based approaches\n(T-MPHN). Through the PAC-Bayes framework, our results reveal the manner in\nwhich hypergraph structure and spectral norms of the learned weights can affect\nthe generalization bounds, where the key technical challenge lies in developing\nnew perturbation analysis for hypergraph neural networks, which offers a\nrigorous understanding of how variations in the model's weights and hypergraph\nstructure impact its generalization behavior. Our empirical study examines the\nrelationship between the practical performance and theoretical bounds of the\nmodels over synthetic and real-world datasets. One of our primary observations\nis the strong correlation between the theoretical bounds and empirical loss,\nwith statistically significant consistency in most cases.",
        "Autonomous off-road navigation faces challenges due to diverse, unstructured\nenvironments, requiring robust perception with both geometric and semantic\nunderstanding. However, scarce densely labeled semantic data limits\ngeneralization across domains. Simulated data helps, but introduces domain\nadaptation issues. We propose COARSE, a semi-supervised domain adaptation\nframework for off-road semantic segmentation, leveraging sparse, coarse\nin-domain labels and densely labeled out-of-domain data. Using pretrained\nvision transformers, we bridge domain gaps with complementary pixel-level and\npatch-level decoders, enhanced by a collaborative pseudo-labeling strategy on\nunlabeled data. Evaluations on RUGD and Rellis-3D datasets show significant\nimprovements of 9.7\\% and 8.4\\% respectively, versus only using coarse data.\nTests on real-world off-road vehicle data in a multi-biome setting further\ndemonstrate COARSE's applicability.",
        "The one-component Coulomb gas on the sphere, consisting on $N$ unit charges\ninteracting via a logarithmic potential, and in the presence of two external\ncharges each of strength proportional to $N$, is considered. There are two\nspherical caps naturally associated with the external charges, giving rise to\ntwo distinct phases depending on them not overlapping (post-critical) or\noverlapping (pre-critical). The equilibrium measure in the post-critical phase\nis known from earlier work. We determine the equilibrium measure in the\npre-critical phase using a particular conformal map, with the parameters\ntherein specified in terms of a root of a certain fourth order polynomial. This\nis used to determine the exact form of the electrostatic energy for the\npre-critical phase. Using a duality relation from random matrix theory, the\npartition function for the Coulomb gas at the inverse temperature $\\beta = 2$\ncan be expanded for large $N$ in the post-critical phase, and in a scaling\nregion of the post and pre-critical boundary. For the pre-critical phase, the\nduality identity implies a relation between two electrostatic energies, one for\nthe present sphere system, and the other for a certain constrained log-gas\nrelating to the Jacobi unitary ensemble.",
        "In a paper published in 2020 in Studia Mathematica, Abrahamsen et al. proved\nthat in the real space $L_1(\\mu)$, where $\\mu$ is a non-zero $\\sigma$-finite\n(countably additive non-negative) measure, norm-one elements in finite convex\ncombinations of relatively weakly open subsets of the unit ball are interior\npoints of these convex combinations in the relative weak topology. In this\npaper that result is generalised by proving that the same is true in the (real\nor complex) Lebesgue--Bochner spaces $L_1(\\mu,X)$ where $X$ is a weakly\nuniformly rotund Banach space.",
        "We present a novel approach to formalise and solve search-based problems\nusing large language models, which significantly improves upon previous\nstate-of-the-art results. We demonstrate the efficacy of this approach on the\nlogic puzzles benchmark ZebraLogicBench. Instead of letting the LLM attempt to\ndirectly solve the puzzles, our method prompts the model to formalise the\nproblem in a logic-focused domain-specific language (DSL) called Logic.py. This\nformalised representation is then solved using a constraint solver, leveraging\nthe strengths of both the language model and the solver. Our approach achieves\na remarkable 65% absolute improvement over the baseline performance of Llama\n3.1 70B on ZebraLogicBench, setting a new state-of-the-art with an accuracy of\nover 90%. This significant advancement demonstrates the potential of combining\nlanguage models with domain-specific languages and auxiliary tools on\ntraditionally challenging tasks for LLMs.",
        "Multiple polylogarithms are equipped with rich algebraic structures including\nthe motivic coaction and the single-valued map which both found fruitful\napplications in high-energy physics. In recent work arXiv:2312.00697, the\ncurrent authors presented a conjectural reformulation of the motivic coaction\nand the single-valued map via zeta generators, certain operations on\nnon-commuting variables in suitable generating series of multiple\npolylogarithms. In this work, the conjectures of the reference will be proven\nfor multiple polylogarithms that depend on any number of variables on the\nRiemann sphere.",
        "Alpha mining, a critical component in quantitative investment, focuses on\ndiscovering predictive signals for future asset returns in increasingly complex\nfinancial markets. However, the pervasive issue of alpha decay, where factors\nlose their predictive power over time, poses a significant challenge for alpha\nmining. Traditional methods like genetic programming face rapid alpha decay\nfrom overfitting and complexity, while approaches driven by Large Language\nModels (LLMs), despite their promise, often rely too heavily on existing\nknowledge, creating homogeneous factors that worsen crowding and accelerate\ndecay. To address this challenge, we propose AlphaAgent, an autonomous\nframework that effectively integrates LLM agents with ad hoc regularizations\nfor mining decay-resistant alpha factors. AlphaAgent employs three key\nmechanisms: (i) originality enforcement through a similarity measure based on\nabstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor\nalignment via LLM-evaluated semantic consistency between market hypotheses and\ngenerated factors, and (iii) complexity control via AST-based structural\nconstraints, preventing over-engineered constructions that are prone to\noverfitting. These mechanisms collectively guide the alpha generation process\nto balance originality, financial rationale, and adaptability to evolving\nmarket conditions, mitigating the risk of alpha decay. Extensive evaluations\nshow that AlphaAgent outperforms traditional and LLM-based methods in\nmitigating alpha decay across bull and bear markets, consistently delivering\nsignificant alpha in Chinese CSI 500 and US S&P 500 markets over the past four\nyears. Notably, AlphaAgent showcases remarkable resistance to alpha decay,\nelevating the potential for yielding powerful factors.",
        "In this paper we examine the relative knotting probabilities in a lattice\nmodel of ring polymers confined in a cavity. The model is of a lattice knot of\nsize $n$ in the cubic lattice, confined to a cube of side-length $L$ and with\nvolume $V=(L{+}1)^3$ sites. We use Monte Carlo algorithms to approximately\nenumerate the number of conformations of lattice knots in the confining cube.\nIf $p_{n,L}(K)$ is the number of conformations of a lattice polygon of length\n$n$ and knot type $K$ in a cube of volume $L^3$, then the relative knotting\nprobability of a lattice polygon to have knot type $K$, relative to the\nprobability that the polygon is the unknot (the trivial knot, denoted by\n$0_1$), is $\\rho_{n,L}(K\/0_1) = p_{n,L}(K)\/p_{n,L}(0_1)$. We determine\n$\\rho_{n,L}(K\/0_1)$ for various knot types $K$ up to six crossing knots. Our\ndata show that these relative knotting probabilities are small so that the\nmodel is dominated by lattice polygons of knot type the unknot. Moreover, if\nthe concentration of the monomers of the lattice knot is $\\varphi = n\/V$, then\nthe relative knot probability increases with $\\varphi$ along a curve that\nflattens as the Hamiltonian state is approached.",
        "We consider a natural generalisation of the Painlev\\'e property and use it to\nidentify the known integrable cases of the Lane-Emden equation with a real\npositive index. We classify certain first-order ordinary differential equations\nwith this property and find necessary conditions for a large family of\nsecond-order equations. We consider ODEs such that, given any simply connected\ndomain $\\Omega$ not containing fixed singularities of the equation, the Riemann\nsurface of any solution obtained by analytic continuation along curves in\n$\\Omega$ has a finite number of sheets over $\\Omega$.",
        "Considered in this work is the Yang-Mills field in an extremal\nReissner-Nordstr\\\"om black hole, a physically motivated mathematical model\nintroduced by Bizo\\'n and Kahl. The kink is a fundamental, strongly unstable\nstationary solution in this non-perturbative, variable coefficients model, with\na polynomial tail and no explicit form. In this paper, we introduce and extend\nseveral virial techniques, adapt them to the inhomogeneous medium setting, and\nconstruct a finite codimensional manifold of the energy space where the kink is\nasymptotically stable. In particular, we handle, using virial techniques, the\nemergence of a weak threshold resonance in the description of the stable\nmanifold.",
        "Emotion recognition and sentiment analysis are pivotal tasks in speech and\nlanguage processing, particularly in real-world scenarios involving\nmulti-party, conversational data. This paper presents a multimodal approach to\ntackle these challenges on a well-known dataset. We propose a system that\nintegrates four key modalities\/channels using pre-trained models: RoBERTa for\ntext, Wav2Vec2 for speech, a proposed FacialNet for facial expressions, and a\nCNN+Transformer architecture trained from scratch for video analysis. Feature\nembeddings from each modality are concatenated to form a multimodal vector,\nwhich is then used to predict emotion and sentiment labels. The multimodal\nsystem demonstrates superior performance compared to unimodal approaches,\nachieving an accuracy of 66.36% for emotion recognition and 72.15% for\nsentiment analysis."
      ]
    }
  },
  {
    "id":2411.00922,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation",
    "start_abstract":"Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
      ],
      "abstract":[
        "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Extinction and Extirpation Conditions in Coalescent and Ecotonal\n  Metacommunities",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Collective inference of the truth of propositions from crowd probability\n  judgments",
        "Sharp stability for critical points of the Sobolev inequality in the\n  absence of bubbling",
        "Free energy profiles for chemical reactions in solution from\n  high-dimensional neural network potentials: The case of the Strecker\n  synthesis",
        "Efficient Multivariate Robust Mean Estimation Under Mean-Shift\n  Contamination",
        "Mathematical Modelling of Mechanotransduction via RhoA Signalling\n  Pathways",
        "Nonparametric Smoothing of Directional and Axial Data",
        "The EFT Bootstrap at Finite $M_{PL}$",
        "Preconditioning for a Cahn-Hilliard-Navier-Stokes model for morphology\n  formation in organic solar cells",
        "Four-quark scatterings in QCD III",
        "Vacuum stress between conducting plates: the curved spacetime version",
        "Euler--Poincar\\'e reduction and the Kelvin--Noether theorem for discrete\n  mechanical systems with advected parameters and additional dynamics",
        "On the Prescribed Ricci Curvature of Noncompact Homogeneous Spaces with\n  Two Isotropy Summands",
        "Active bacterial baths in droplets",
        "Seeing Stereotypes",
        "Supercell environments using GridRad-Severe and the HRRR: Addressing\n  discrepancies between prior tornado datasets",
        "Indigenous Mathematics I. Smoke Telegraphy"
      ],
      "abstract":[
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Here we present extinction, extirpation and coexistence conditions where \/\nwhen two communities combine. We consider one specific model where two\ncommunities coalesce, and another model where the communities coexist side by\nside, blending in a transitionary zone called the ecotone. Specifically, (1) we\nanalytically calculate the shifts in abundances as a function of mixing\nstrength. (2) Obtain a critical value for the mixing strength leading to\nextinction. (3) Derive an inequality condition for full coexistent mixing. (4)\nfind how the individual communities penetrate into one other as a function of\nmixing strength. (5) derive the conditions for one species to cross the ecotone\nand invade an neighboring community and (6) conditions for a native species to\nget extirpated. Lastly, (7) we spatially investigate the species richness\nwithin the ecotone and derive a condition that determines whether the ecotone\nwill have higher or lower richness compared to its surrounding habitats.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty.",
        "When $u$ is close to a single Talenti bubble $v$ of the $p$-Sobolev\ninequality, we show that\n  \\begin{equation*}\n  \\|Du-Dv\\|_{L^p(\\mathbb{R}^n)}^{\\max\\{1,p-1\\}}\\le C \\|-{\\rm\ndiv}(|Du|^{p-2}Du)-|u|^{p^*-2}u\\|_{W^{-1,q}(\\mathbb{R}^n)}, \\end{equation*}\nwhere $C=C(n,p)>0$. This estimate provides a sharp stability estimate for the\nStruwe-type decomposition in the single bubble case, generalizing the result of\nCiraolo, Figalli, and Maggi \\cite{CFM2018} (focusing on the case $p=2$) to the\narbitrary $p$. Also, in the Sobolev setting, this answers an open problem\nraised by Zhou and Zou in \\cite[Remark 1.17]{ZZ2023}.",
        "Machine learning potentials (MLPs) have become a popular tool in chemistry\nand materials science as they combine the accuracy of electronic structure\ncalculations with the high computational efficiency of analytic potentials.\nMLPs are particularly useful for computationally demanding simulations such as\nthe determination of free energy profiles governing chemical reactions in\nsolution, but to date such applications are still rare. In this work we show\nhow umbrella sampling simulations can be combined with active learning of\nhigh-dimensional neural network potentials (HDNNPs) to construct free energy\nprofiles in a systematic way. For the example of the first step of Strecker\nsynthesis of glycine in aqueous solution we provide a detailed analysis of the\nimproving quality of HDNNPs for datasets of increasing size. We find that next\nto the typical quantification of energy and force errors with respect to the\nunderlying density functional theory data also the long-term stability of the\nsimulations and the convergence of physical properties should be rigorously\nmonitored to obtain reliable and converged free energy profiles of chemical\nreactions in solution.",
        "We study the algorithmic problem of robust mean estimation of an identity\ncovariance Gaussian in the presence of mean-shift contamination. In this\ncontamination model, we are given a set of points in $\\mathbb{R}^d$ generated\ni.i.d. via the following process. For a parameter $\\alpha<1\/2$, the $i$-th\nsample $x_i$ is obtained as follows: with probability $1-\\alpha$, $x_i$ is\ndrawn from $\\mathcal{N}(\\mu, I)$, where $\\mu \\in \\mathbb{R}^d$ is the target\nmean; and with probability $\\alpha$, $x_i$ is drawn from $\\mathcal{N}(z_i, I)$,\nwhere $z_i$ is unknown and potentially arbitrary. Prior work characterized the\ninformation-theoretic limits of this task. Specifically, it was shown that, in\ncontrast to Huber contamination, in the presence of mean-shift contamination\nconsistent estimation is possible. On the other hand, all known robust\nestimators in the mean-shift model have running times exponential in the\ndimension. Here we give the first computationally efficient algorithm for\nhigh-dimensional robust mean estimation with mean-shift contamination that can\ntolerate a constant fraction of outliers. In particular, our algorithm has\nnear-optimal sample complexity, runs in sample-polynomial time, and\napproximates the target mean to any desired accuracy. Conceptually, our result\ncontributes to a growing body of work that studies inference with respect to\nnatural noise models lying in between fully adversarial and random settings.",
        "We derive and simulate a mathematical model for mechanotransduction related\nto the Rho GTPase signalling pathway. The model addresses the bidirectional\ncoupling between signalling processes and cell mechanics. A numerical method\nbased on bulk-surface finite elements is proposed for the approximation of the\ncoupled system of nonlinear reaction-diffusion equations, defined inside the\ncell and on the cell membrane, and the equations of elasticity. Our simulation\nresults illustrate novel emergent features such as the strong dependence of the\ndynamics on cell shape, a threshold-like response to changes in substrate\nstiffness, and the fact that coupling mechanics and signalling can lead to the\nrobustness of cell deformation to larger changes in substrate stiffness,\nensuring mechanical homeostasis in agreement with experiments.",
        "We discuss generalized linear models for directional data where the\nconditional distribution of the response is a von Mises-Fisher distribution in\narbitrary dimension or a Bingham distribution on the unit circle. To do this\nproperly, we parametrize von Mises-Fisher distributions by Euclidean parameters\nand investigate computational aspects of this parametrization. Then we modify\nthis approach for local polynomial regression as a means of nonparametric\nsmoothing of distributional data. The methods are illustrated with simulated\ndata and a data set from planetary sciences involving covariate vectors on a\nsphere with axial response.",
        "We explore the impact of loop effects on positivity in effective field\ntheories emerging in the infrared from unitary and causal microscopic dynamics.\nFocusing on massless particles coupled to gravity, we address the treatment of\nforward-limit divergences from loop discontinuities and establish necessary\nconditions for maintaining computational control in perturbation theory. While\nloop effects remain small, ensuring consistency in our approach leads to a\nsignificant impact on bounds, even at tree level.",
        "We present a model for the morphology evolution of printed organic solar\ncells which occurs during the drying of a mixture of polymer, the non-fullerene\nacceptor and the solvent. Our model uses a phase field approach coupled to a\nNavier-Stokes equation describing the macroscopic movement of the fluid.\nAdditionally, we incorporate the evaporation process of the solvent using an\nAllen-Cahn equation.\n  The model is discretized using a finite-element approach with a semi-implicit\ndiscretization in time. The resulting (non)linear systems are coupled and of\nlarge dimensionality. We present a preconditioned iterative scheme to solve\nthem robustly with respect to changes in the discretization parameters. We\nillustrate that the preconditioned solver shows parameter-robust iteration\nnumbers and that the model qualitatively captures the behavior of the film\nmorphology during drying.",
        "We study the full infrared dynamics of 2+1 flavour QCD with the functional\nrenormalisation group approach. We resolve self-consistently the glue dynamics\nas well as the dynamics of chiral symmetry breaking. The computation hosts no\nphenomenological parameter or external input. The only ultraviolet input\nparameters are the physical ones in QCD: the light and strange quark masses.\nThey are adjusted to the physical ratios of the pion and kaon masses, divided\nby the pion decay constant. The results for other observables of current\nfirst-principles computations are in quantitative agreement with the physical\nones. This work completes the series of papers, initiated and furthered in\n[1,2], on dynamical chiral symmetry breaking and the emergence of mesonic bound\nstates within the functional renormalisation group. As a first application we\ndiscuss the formation of light mesonic bound states. Amongst other applications\nsuch as the phase structure of QCD, the current work paves the way for studying\nQCD parton distribution functions within the functional renormalisation group\napproach to first-principles QCD.",
        "Brown and Maclay \\cite{Brown} found the energy-momentum tensor for the\nCasimir effect of parallel plates in 1969. We find its curved spacetime version\nin a static background using the point splitting regularization method.\nPrevious results in the literature are reinforced and some consequences\ndiscussed.",
        "The Euler--Poincar\\'e equations, firstly introduced by Henri Poincar\\'e in\n1901, arise from the application of Lagrangian mechanics to systems on Lie\ngroups that exhibit symmetries, particularly in the contexts of classical\nmechanics and fluid dynamics. These equations have been extended to various\nsettings, such as semidirect products, advected parameters, and field theory,\nand have been widely applied to mechanics and physics. In this paper, we\nintroduce the discrete Euler--Poincar\\'e reduction for discrete Lagrangian\nsystems on Lie groups with advected parameters and additional dynamics,\nutilizing the group difference map technique. Specifically, the group\ndifference map is defined using either the Cayley transform or the matrix\nexponential. The continuous and discrete Kelvin--Noether theorems are extended\naccordingly, that account for Kelvin--Noether quantities of the corresponding\ncontinuous and discrete Euler--Poincar\\'e equations. As an application, we show\nboth continuous and discrete Euler--Poincar\\'e formulations about the dynamics\nof underwater vehicles, followed by numerical simulations. Numerical results\nillustrate the scheme's ability to preserve geometric properties over extended\ntime intervals, highlighting its potential for practical applications in the\ncontrol and navigation of underwater vehicles, as well as in other domains.",
        "This work studies simply connected, noncompact $G\/H$ in which $G$ is\nsemi-simple, $H$ is connected, and $G\/H$ has two irreducible summands. Here, we\nclassify all such spaces and we provide solutions to the so-called Prescribed\nRicci Curvature problem for all such spaces.",
        "Suspensions of self-propelled objects represent a novel paradigm in colloidal\nscience. In such active baths traditional concepts, such as Brownian motion,\nfluctuation-dissipation relations, and work extraction from heat reservoirs,\nmust be extended beyond the conventional framework of thermal baths. Unlike\nthermal baths, which are characterized by a single parameter, the temperature,\nthe fundamental descriptors of an active bath remain elusive, especially in\nconfined environments. In this study, buoyant, passive tracers are employed as\ngeneralized probes to investigate an active bath comprising motile bacteria\nconfined within a droplet. We demonstrate that momentum transfer from the bath\nto the tracer can be effectively described as colored noise, characterized by\ntemporal memory and an enhanced effective diffusivity significantly larger\ncompared to thermal Brownian motion values. Using a stochastic analytical\nframework, we extract the temporal memory and diffusivity parameters that\ndefine such an active bath. Notably, the diffusivity scales linearly with\nbacterial concentration, modulated by a factor representing the role of\nconfinement, expressed as the ratio of the confining radius to the probe\nradius. This finding, while still awaiting a complete theoretical explanation,\noffers new insights into the transport properties of confined active baths and\npaves the way for a deeper understanding of active emulsions driven by confined\nactive matter.",
        "Reliance on stereotypes is a persistent feature of human decision-making and\nhas been extensively documented in educational settings, where it can shape\nstudents' confidence, performance, and long-term human capital accumulation.\nWhile effective techniques exist to mitigate these negative effects, a crucial\nfirst step is to establish whether teachers can recognize stereotypes in their\nprofessional environment. We introduce the Stereotype Identification Test\n(SIT), a novel survey tool that asks teachers to evaluate and comment on the\npresence of stereotypes in images randomly drawn from school textbooks. Their\nresponses are systematically linked to established measures of implicit bias\n(Implicit Association Test, IAT) and explicit bias (survey scales on teaching\nstereotypes and social values). Our findings demonstrate that the SIT is a\nvalid and reliable measure of stereotype recognition. Teachers' ability to\nrecognize stereotypes is linked to trainable traits such as implicit bias\nawareness and inclusive teaching practices. Moreover, providing personalized\nfeedback on implicit bias improves SIT scores by 0.25 standard deviations,\nreinforcing the idea that stereotype recognition is malleable and can be\nenhanced through targeted interventions.",
        "Storm-relative helicity (SRH) is an important ingredient in supercell\ndevelopment, as well as mesocyclone intensity, and is linked to tornadogenesis\nand tornado potential. Derived from the storm-relative wind profile, SRH is\ncomposed of both the vertical wind shear and storm-relative flow. Recent\nstudies have come to conflicting findings regarding whether shallower or deeper\nlayers of SRH have more skill in tornado forecasting. Possible causes of this\ndiscrepancy include the use of observed versus model-based proximity soundings,\nas well as whether the storm-relative wind profile is determined via observed\nversus estimated storm motions. This study uses a new dataset of objectively\nidentified supercells, with observed storm motions, paired with high-resolution\nmodel analyses to address the discrepancies among prior studies. Unlike in\nprevious model-based tornado environmental datasets, the present approach\nreveals substantive differences in storm-relative flow, vertical wind shear,\nand SRH within the low-to-mid-levels between nontornadic and tornadic\nsupercells. Using observed storm motions for storm-relative variables further\nmagnifies differences in the low-to-mid-level storm-relative winds between\nnontornadic and tornadic supercells, ultimately leading to deeper layers of SRH\nhaving more forecast skill than near-ground SRH. Thus, the combination of a\nhigher-resolution model analyses, which better represents the near-storm\nenvironment, with observed storm motions appears to explain why many past\ntornado climatologies using model-based environmental analyses have failed to\nfind significant differences in the storm-relative wind profile. These results\nhelp bridge the gap between previous studies that employed coarser model-based\nanalyses with those that aggregated observed soundings from field projects.",
        "This article is the first in an occasional series for the Australian\nMathematical Society Gazette on diverse aspects and topics of Indigenous\nmathematical knowledge. This is an important, but neglected, part of the\nmathematical heritage of humankind, and as such is the concern of the\nmathematics community as a whole. It is hoped that this and future articles may\nhelp to inspire mathematics researchers, students, and educators at tertiary\nand school levels who are seeking to widen their mathematical horizons and\ndevelop course and research materials of broad cultural relevance.\n  I would like to honour the Mithaka peoples of the Kurrawoolben and\nKirrenderri (Diamantina) and Nooroondinna (Georgina) river channel country of\nsouth-western Qld, Australia. The material in this article does not involve\nculturally restricted knowledge or images, and is shared with respect for the\nMithaka ancestors and their descendants."
      ]
    }
  },
  {
    "id":2411.03522,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Language Models are Few-Shot Learners",
    "start_abstract":"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b20"
      ],
      "title":[
        "Long non-coding RNAs: definitions, functions, challenges and recommendations"
      ],
      "abstract":[
        "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "SPM 25: open source neuroimaging analysis software",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "UNGT: Ultrasound Nasogastric Tube Dataset for Medical Image Analysis",
        "Automating Care by Self-maintainability for Full Laboratory Automation",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Functional Correspondences in the Human and Marmoset Visual Cortex\n  During Movie Watching: Insights from Correlation, Redundancy, and Synergy",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Dynamical phases of short-term memory mechanisms in RNNs",
        "Phase Alignment Enhances Oscillatory Power in Neural Mass Models\n  Optimized for Class Encoding",
        "A comprehensive and reliable protocol for manual segmentation of the\n  human claustrum using high-resolution MRI",
        "Multicellular self-organization in Escherichia coli",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Social hierarchy shapes foraging decisions",
        "Secure Quantum Key Distribution with Room-Temperature Quantum Emitter",
        "Plebanski complex",
        "Extended string-net models with all anyons at finite temperature",
        "Geodesic cycles on the Sphere: $t$-designs and Marcinkiewicz-Zygmund\n  Inequalities",
        "Minimax Optimality of Classical Scaling Under General Noise Conditions",
        "The Effect of the Non-Abelian Quantum Metric on Superfluidity",
        "Global well-posedness and optimal time-decay of 3D full compressible\n  Navier-Stokes system",
        "Origin of switchable quasiparticle-interference chirality in\n  loop-current phase of kagome metals measured by scanning-tunneling-microscopy",
        "Learning the P2D Model for Lithium-Ion Batteries with SOH Detection",
        "Dot to dot: high-$z$ little red dots in $M_{\\rm bh}$-$M_{\\rm \\star}$\n  diagrams with galaxy-morphology-specific scaling relations and nuclear star\n  clusters",
        "Knoop: Practical Enhancement of Knockoff with Over-Parameterization for\n  Variable Selection",
        "Spectral properties of bottomonium at high temperature: a systematic\n  investigation",
        "Cyclical accretion regime change in the slow X-ray pulsar 4U 0114+65\n  observed with Chandra",
        "Bayesian Parameter Shift Rule in Variational Quantum Eigensolvers",
        "Ground state and magnetic transitions of the orthorhombic\n  antiferromagnet CaCo$_2$TeO$_6$"
      ],
      "abstract":[
        "Statistical Parametric Mapping (SPM) is an integrated set of methods for\ntesting hypotheses about the brain's structure and function, using data from\nimaging devices. These methods are implemented in an open source software\npackage, SPM, which has been in continuous development for more than 30 years\nby an international community of developers. This paper reports the release of\nSPM 25.01, a major new version of the software that incorporates novel analysis\nmethods, optimisations of existing methods, as well as improved practices for\nopen science and software development.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "We develop a novel ultrasound nasogastric tube (UNGT) dataset to address the\nlack of public nasogastric tube datasets. The UNGT dataset includes 493 images\ngathered from 110 patients with an average image resolution of approximately\n879 $\\times$ 583. Four structures, encompassing the liver, stomach, tube, and\npancreas are precisely annotated. Besides, we propose a semi-supervised\nadaptive-weighting aggregation medical segmenter to address data limitation and\nimbalance concurrently. The introduced adaptive weighting approach tackles the\nsevere unbalanced challenge by regulating the loss across varying categories as\ntraining proceeds. The presented multiscale attention aggregation block\nbolsters the feature representation by integrating local and global contextual\ninformation. With these, the proposed AAMS can emphasize sparse or small\nstructures and feature enhanced representation ability. We perform extensive\nsegmentation experiments on our UNGT dataset, and the results show that AAMS\noutperforms existing state-of-the-art approaches to varying extents. In\naddition, we conduct comprehensive classification experiments across varying\nstate-of-the-art methods and compare their performance. The dataset and code\nwill be available upon publication at https:\/\/github.com\/NUS-Tim\/UNGT.",
        "The automation of experiments in life sciences and chemistry has\nsignificantly advanced with the development of various instruments and AI\ntechnologies. However, achieving full laboratory automation, where experiments\nconceived by scientists are seamlessly executed in automated laboratories,\nremains a challenge. We identify the lack of automation in planning and\noperational tasks--critical human-managed processes collectively termed\n\"care\"--as a major barrier. Automating care is the key enabler for full\nlaboratory automation. To address this, we propose the concept of\nself-maintainability (SeM): the ability of a laboratory system to autonomously\nadapt to internal and external disturbances, maintaining operational readiness\nakin to living cells. A SeM-enabled laboratory features autonomous recognition\nof its state, dynamic resource and information management, and adaptive\nresponses to unexpected conditions. This shifts the planning and execution of\nexperimental workflows, including scheduling and reagent allocation, from\nhumans to the system. We present a conceptual framework for implementing\nSeM-enabled laboratories, comprising three modules--Requirement manager,\nLabware manager, and Device manager--and a Central manager. SeM not only\nenables scientists to execute envisioned experiments seamlessly but also\nprovides developers with a design concept that drives the technological\ninnovations needed for full automation.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "The world of beauty is deeply connected to the visual cortex, as perception\noften begins with vision in both humans and marmosets. Quantifying functional\ncorrespondences in the visual cortex across species can help us understand how\ninformation is processed in the primate visual cortex, while also providing\ndeeper insights into human visual cortex functions through the study of\nmarmosets. In this study, we measured pairwise and beyond pairwise correlation,\nredundancy, and synergy in movie-driven fMRI data across species. Our first key\nfinding was that humans and marmosets exhibited significant overlaps in\nfunctional synergy. Second, we observed that the strongest functional\ncorrespondences between the human peri-entorhinal and entorhinal cortex (PeEc)\nand the occipitotemporal higher-level visual regions in the marmoset during\nmovie watching reflected a functional synergistic relationship. These regions\nare known to correspond to face-selective areas in both species. Third,\nredundancy measures maintained stable high-order hubs, indicating a steady core\nof shared information processing, while synergy measures revealed a dynamic\nshift from low- to high-level visual regions as interaction increased,\nreflecting adaptive integration. This highlights distinct patterns of\ninformation processing across the visual hierarchy. Ultimately, our results\nreveal the marmoset as a compelling model for investigating visual perception,\ndistinguished by its remarkable functional parallels to the human visual\ncortex.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "Short-term memory is essential for cognitive processing, yet our\nunderstanding of its neural mechanisms remains unclear. A key focus in\nneuroscience has been the study of sequential activity patterns, where neurons\nfire one after another within large networks, to explain how information is\nmaintained. While recurrent connections were shown to drive sequential\ndynamics, a mechanistic understanding of this process still remains unknown. In\nthis work, we first introduce two unique mechanisms that can subserve\nshort-term memory: slow-point manifolds generating direct sequences or limit\ncycles providing temporally localized approximations. Then, through analytical\nmodels, we identify fundamental properties that govern the selection of these\nmechanisms, \\textit{i.e.}, we derive theoretical scaling laws for critical\nlearning rates as a function of the delay period length, beyond which no\nlearning is possible. We empirically verify these observations by training and\nevaluating more than 35,000 recurrent neural networks (RNNs) that we will\npublicly release upon publication. Overall, our work provides new insights into\nshort-term memory mechanisms and proposes experimentally testable predictions\nfor systems neuroscience.",
        "Neural encoding of objects and cognitive states remains an elusive yet\ncrucial aspect of brain function. While traditional feed-forward machine\nlearning neural networks have enormous potential to encode information, modern\narchitectures provide little insight into the brain's mechanisms. In this work,\na Jansen and Rit neural mass model was constructed to encode different sets of\ninputs, aiming to understand how simple neural circuits can represent\ninformation. A genetic algorithm was used to optimize parameters that maximized\nthe differences in responses to particular inputs. These differences in\nresponses manifested as phase-shifted oscillations across the set of inputs. By\ndelivering impulses of excitation synchronized with a particular phase-shifted\noscillation, we demonstrated that the encoded phase could be decoded by\nmeasuring oscillatory power. These findings demonstrate the capability of\nneural dynamical circuits to encode and decode information through phase.",
        "The claustrum is a thin gray matter structure in each brain hemisphere,\ncharacterized by exceptionally high connectivity with nearly all brain regions.\nDespite extensive animal studies on its anatomy and function and growing\nevidence of claustral deficits in neuropsychiatric disorders, its specific\nroles in normal and abnormal human brain function remain largely unknown. This\nis primarily due to its thin and complex morphology, which limits accurate\nanatomical delineation and neural activity isolation in conventional in vivo\nneuroimaging. To facilitate future neuroimaging studies, we developed a\ncomprehensive and reliable manual segmentation protocol based on a\ncellular-resolution brain atlas and high-resolution (0.7^3 mm) MRI data. The\nprotocols involve detailed guidelines to delineate the entire claustrum,\nincluding the inferior parts that have not been clearly described in earlier\nMRI studies. Additionally, we propose a geometric method to parcellate the\nclaustrum into three subregions (the dorsal, ventral, and temporal claustrum)\nalong the superior-to-inferior axis. The mean bilateral claustrum volume in 10\nyoung adults was 3307.5 mm^3, approximately 0.21% of total intracranial volume.\nOur segmentation protocol demonstrated high inter- and intra-rater reliability\n(ICC > 0.89, DSC > 0.85), confirming its replicability. This comprehensive and\nreliable claustrum segmentation protocols will provide a cornerstone for future\nneuroimaging studies of systematic, large-scale investigations of the anatomy\nand the functions of the human claustrum in normal and pathological\npopulations.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "Social foraging is a widespread form of animal foraging in which groups of\nindividuals coordinate their decisions to exploit resources in the environment.\nAnimals show a variety of social structures from egalitarian to hierarchical.\nIn this study, we examine how different forms of social hierarchy shape\nforaging decisions. We developed a mechanistic analytically tractable model to\nstudy the underlying processes of social foraging, tying the microscopic\nindividual to the macroscopic group levels. Based on a stochastic evidence\naccumulation framework, we developed a model of patch-leaving decisions in a\nlarge hierarchical group with leading and following individuals. Across a\nvariety of information sharing mechanisms, we were able to analytically\nquantify emergent collective dynamics. We found that follower-leader dynamics\nthrough observations of leader movements or through counting the number of\nindividuals in a patch confers, for most conditions, a benefit for the\nfollowing individuals by increasing their accuracy in inferring patch richness.\nOn the other hand, misinformation, through the communication of false beliefs\nabout food rewards or patch quality, shows to be detrimental to following\nindividuals, but paradoxically may lead to increased group cohesion. In an era\nwhere there is a huge amount of animal foraging data collected, our model\nprovides a systematic way to conceptualize and understand those data by\nuncovering hidden mechanisms underlying social foraging decisions.",
        "On-demand generation of single photons from solid-state quantum emitters is a\nkey building block for future quantum networks, particularly quantum key\ndistribution (QKD) systems, by enabling higher secure key rates (SKR) and lower\nquantum bit error rates (QBER). In this work, we demonstrate the B92 protocol\nbased on single photons from defects in hexagonal boron nitride (hBN). The\nresults show a sifted key rate (SiKR) of 17.5 kbps with a QBER of 6.49 % at a\ndynamic polarization encoding rate of 40 MHz. Finite-key analysis yields a SKR\nof 7 kbps, as one of the highest SKR obtained for any room-temperature single\nphoton source. Our results highlight the potential of hBN defects in advancing\nquantum communication technologies.",
        "As is very well-known, linearisation of the instanton equations on a\n4-manifold gives rise to an elliptic complex of differential operators, the\ntruncated (twisted) Hodge complex $\\Lambda^0(\\mathfrak{g}) \\to\n\\Lambda^1(\\mathfrak{g})\\to \\Lambda^2_+(\\mathfrak{g})$. Moreover, the\nlinearisation of the full YM equations also fits into this framework, as it is\ngiven by the second map followed by its adjoint. We define and study properties\nof what we call the Pleba\\'nski complex. This is a differential complex that\narises by linearisation of the equations implying that a Riemannian 4-manifold\nis hyper-K\\\"ahler. We recall that these are most naturally stated as the\ncondition that there exists a perfect $\\Sigma^i\\wedge \\Sigma^j\\sim\\delta^{ij}$\ntriple $\\Sigma^i, i=1,2,3$ of 2-forms that are closed $d\\Sigma^i=0$. The\nRiemannian metric is encoded by the 2-forms $\\Sigma^i$. We show that what\nresults is an elliptic differential complex $TM \\to S\\to E\\times \\Lambda^1 \\to\nE$, where $S$ is the tangent space to the space of perfect triples, and\n$E=\\mathbb{R}^3$. We also show that, as in the case with instanton equations,\nthe full Einstein equations $Ric=0$ also fit into this framework, their\nlinearisation being given by the second map followed by its adjoint. Our second\nresult concerns the elliptic operator that the Pleba\\'nski complex defines. In\nthe case of the instanton complex, operators appearing in the complex\nsupplemented with their adjoints assemble to give the Dirac operator. We show\nhow the same holds true for the Pleba\\'nski complex. Supplemented by suitable\nadjoints, operators assemble into an elliptic operator that squares to the\nLaplacian and is given by the direct sum of two Dirac operators.",
        "String-net models describe a vast family of topological orders in two spatial\ndimensions, but fail to produce all the expected anyonic excitations. Following\narXiv:1502.03433, we consider an extended string-net model by attaching one\ntail to each plaquette of the lattice, allowing all anyons to emerge as\nelementary plaquette excitations for arbitrary input categories. The\ncorresponding tube algebra is the mathematical tool needed to construct the\nanyons from the input category and to obtain their internal multiplicities. We\nuse them to compute the energy level degeneracies and the partition function.\nIn the thermodynamic limit, the latter is dominated by the trivial (vacuum)\nanyon, so that the topological order is destroyed at any non-zero temperature.\nIn a finite-size system, order survives up to a finite temperature, similarly\nto the one-dimensional classical Ising model. We confirm this by computing\nthermal averages of topological projectors, Wegner-Wilson loops and the\ntopological mutual information. The results are also generalized to models with\nmultiple tails per plaquette.",
        "A geodesic cycle is a closed curve that connects finitely many points along\ngeodesics. We study geodesic cycles on the sphere in regard to their role in\nequal-weight quadrature rules and approximation.",
        "We establish the consistency of classical scaling under a broad class of\nnoise models, encompassing many commonly studied cases in literature. Our\napproach requires only finite fourth moments of the noise, significantly\nweakening standard assumptions. We derive convergence rates for classical\nscaling and establish matching minimax lower bounds, demonstrating that\nclassical scaling achieves minimax optimality in recovering the true\nconfiguration even when the input dissimilarities are corrupted by noise.",
        "The quantum geometric tensor, which encodes the full geometric information of\nquantum states in projective Hilbert space, plays a crucial role in condensed\nmatter physics. In this work, we examine the effect of the non-Abelian quantum\nmetric -- the real part of the non-Abelian quantum geometric tensor -- on the\nsuperfluid weight in time-reversal symmetric systems. For conventional $s$-wave\npairing, we demonstrate that the superfluid weight includes a contribution\nproportional to the trace of the non-Abelian quantum metric. Notably, this\ncontribution remains significant even when the total Chern number of a set of\ndegenerate bands is zero and can exceed the conventional contribution, as\nconfirmed using lattice models. Ab initio density functional theory (DFT)\ncalculations for MoS$_2$ and TiSe$_2$ further corroborate these findings,\nrevealing that the non-Abelian quantum metric accounts for up to 20% of the\nsuperfluid weight in MoS$_2$ and 50% in TiSe$_2$. Our results provide new\ninsights into the nontrivial relationship between the geometric properties of\nquantum states and superconductivity, opening avenues for further exploration\nin topological and superconducting materials.",
        "In this paper, we investigate the global well-posedness and optimal\ntime-decay of classical solutions for the 3-D full compressible Navier-Stokes\nsystem, which is given by the motion of the compressible viscous and\nheat-conductive gases. First of all, we study the global well-posedness of the\nCauchy problem to the system when the initial data is small enough. Secondly,\nwe show the optimal decay rates of the higher-order spatial derivatives of the\n$\\dot{H}^{-s}$ $\\left(0\\leq s<\\frac{3}{2}\\right)$ negative Sobolev norms.\nFinally, under the assumption that the initial data is bounded in $L^{1}$-norm,\nwe establish the upper and lower bounds of the optimal decay rates for the\nclassical solutions.",
        "The chiral loop-current (LC) phase in kagome metals AV3Sb5 (A = Cs, Rb, K)\nhas attracted considerable attention as a novel quantum state driven by\nelectron correlations. Scanning tunneling microscopy (STM) experiments have\nprovided strong evidence for the chiral LC phase through the detection of\nchirality in the quasiparticle interference (QPI) signal. However, the\nfundamental relationship between ``QPI chirality'' and ``LC chirality'' remains\nunexplored. For instance, the QPI signal is unchanged even when all LC orders\nare inverted. Furthermore, only the chiral LC order cannot induce QPI\nchirality. At present, the true essence of kagome metals that we should learn\nfrom the remarkable QPI experiments remains elusive. To address this, we\ninvestigate the origin of the QPI signal in the LC phase using a large\nunit-cell tight-binding model for kagome metals. The LC phase gives rise to a\n$Z_3$ nematic phase, characterized by three distinct directors, under the\nStar-of-David bond order. Our findings demonstrate that the QPI chirality\ninduced by a single impurity at site Z, denoted as $\\chi_Z$, can take values of\n$\\pm1$ (chiral) or 0 (achiral), depending on the direction of the $Z_3$ nematic\norder. Prominent QPI chirality originates from extremely dilute impurities\n($\\lesssim$0.1%) in the present mechanism. Notably, $\\chi_Z$ ($=\\pm1$, 0)\nchanges smoothly with minimal free-energy barriers by applying a small magnetic\nfield $B_z$, accompanied by a switching of the $Z_3$ nematic director. This\nstudy provides a comprehensive explanation for the observed ``$B_z$-switchable\nQPI chirality'' in regions with dilute impurities, offering fundamental insight\ninto the chiral LC in kagome metals.",
        "Lithium ion batteries are widely used in many applications. Battery\nmanagement systems control their optimal use and charging and predict when the\nbattery will cease to deliver the required output on a planned duty or driving\ncycle. Such systems use a simulation of a mathematical model of battery\nperformance. These models can be electrochemical or data-driven.\nElectrochemical models for batteries running at high currents are\nmathematically and computationally complex. In this work, we show that a\nwell-regarded electrochemical model, the Pseudo Two Dimensional (P2D) model,\ncan be replaced by a computationally efficient Convolutional Neural Network\n(CNN) surrogate model fit to accurately simulated data from a class of random\ndriving cycles. We demonstrate that a CNN is an ideal choice for accurately\ncapturing Lithium ion concentration profiles. Additionally, we show how the\nneural network model can be adjusted to correspond to battery changes in State\nof Health (SOH).",
        "High-redshift little red dots (LRDs) detected with the James Webb Space\nTelescope are considered the cores of emerging galaxies. For the first time, we\ncompare LRDs in $M_{\\rm bh}$-$M_{\\star}$ diagrams with an array of $z=0$\ngalaxy-morphology-dependent scaling relations, along with the $M_{\\rm\nbh}$-$M_{\\rm \\star,nsc}$ relation for nuclear star clusters. The $M_{\\rm\nbh}$-$M_{\\rm \\star,sph}$ relations for spheroidal stellar systems are\ncharacterised by a nearly parallel set of quasi-quadratic (or steeper)\ndistributions that are known to trace the `punctuated equilibrium' of galaxies,\nreflecting their stepwise growth in black hole mass and merger-built\nbulge\/spheroid mass. We show that LRDs are not equivalent to nuclear star\nclusters, with the latter having higher $M_{\\rm bh}\/M_{\\star}$ ratios. However,\nthe least massive LRDs exhibit similar $M_{\\rm bh}$ and $M_{\\rm \\star,gal}$\nvalues as ultracompact dwarf (UCD) galaxies. We show that the LRDs span the\n$M_{\\rm bh}$-$M_{\\rm \\star,gal}$ diagram from UCD galaxies to primaeval\nlenticular galaxies. In contrast, spiral galaxies and the subset of\nmajor-merger-built early-type galaxies define offset relations. Additionally,\nwe observe that low-$z$ galaxies with active galactic nuclei align with the\nsteep black hole scaling relations for disc galaxies defined by primarily\ninactive galaxies with directly measured black hole masses. Collectively, this\nhighlights the benefits of considering galaxy morphology, which reflects their\naccretion and merger history, to understand the coevolution of galaxies and\ntheir black holes.",
        "Variable selection plays a crucial role in enhancing modeling effectiveness\nacross diverse fields, addressing the challenges posed by high-dimensional\ndatasets of correlated variables. This work introduces a novel approach namely\nKnockoff with over-parameterization (Knoop) to enhance Knockoff filters for\nvariable selection. Specifically, Knoop first generates multiple knockoff\nvariables for each original variable and integrates them with the original\nvariables into an over-parameterized Ridgeless regression model. For each\noriginal variable, Knoop evaluates the coefficient distribution of its\nknockoffs and compares these with the original coefficients to conduct an\nanomaly-based significance test, ensuring robust variable selection. Extensive\nexperiments demonstrate superior performance compared to existing methods in\nboth simulation and real-world datasets. Knoop achieves a notably higher Area\nunder the Curve (AUC) of the Receiver Operating Characteristic (ROC) Curve for\neffectively identifying relevant variables against the ground truth by\ncontrolled simulations, while showcasing enhanced predictive accuracy across\ndiverse regression and classification tasks. The analytical results further\nbackup our observations.",
        "We investigate spectral features of bottomonium at high temperature, in\nparticular the thermal mass shift and width of ground state S-wave and P-wave\nstate. We employ and compare a range of methods for determining these features\nfrom lattice NRQCD correlators, including direct correlator analyses\n(multi-exponential fits and moments of spectral functions), linear methods\n(Backus-Gilbert, Tikhonov and HLT methods), and Bayesian methods for spectral\nfunction reconstruction (MEM and BR). We comment on the reliability and\nlimitations of the various methods.",
        "4U 0114+65 is a high-mass X-ray binary system formed by the luminous\nsupergiant B1Ia, known as V{*} V662 Cas, and one of the slowest rotating\nneutron stars (NS) with a spin period of about 2.6 hours. This fact provides a\nrare opportunity to study interesting details of the accretion within each\nindividual pulse of the compact object. In this paper, we analyze 200 ks of\nChandra grating data, divided into 9 uninterrupted observations around the\norbit. The changes in the circumstellar absorption column through the orbit\nsuggest an orbital inclination of $\\sim$ $40^{\\circ}$ with respect to the\nobserver and a companion mass-loss rate of $\\sim$ 8.6 10$^{-7}$ solar masses\nyr$^{-1}$. The peaks of the NS pulse show a large pulse-to-pulse variability.\nThree of them show an evolution from a brighter regime to a weaker one. We\npropose that the efficiency of Compton cooling in this source fluctuates\nthroughout an accumulation cycle. After significant depletion of matter within\nthe magnetosphere, since the settling velocity is $\\sim \\times$ 2 times lower\nthan the free-fall velocity, the source gradually accumulates matter until the\ndensity exceeds a critical threshold. This increase in density triggers a\ntransition to a more efficient Compton cooling regime, leading to a higher mass\naccretion rate and consequently to an increased brightness.",
        "Parameter shift rules (PSRs) are key techniques for efficient gradient\nestimation in variational quantum eigensolvers (VQEs). In this paper, we\npropose its Bayesian variant, where Gaussian processes with appropriate kernels\nare used to estimate the gradient of the VQE objective. Our Bayesian PSR offers\nflexible gradient estimation from observations at arbitrary locations with\nuncertainty information and reduces to the generalized PSR in special cases. In\nstochastic gradient descent (SGD), the flexibility of Bayesian PSR allows the\nreuse of observations in previous steps, which accelerates the optimization\nprocess. Furthermore, the accessibility to the posterior uncertainty, along\nwith our proposed notion of gradient confident region (GradCoRe), enables us to\nminimize the observation costs in each SGD step. Our numerical experiments show\nthat the VQE optimization with Bayesian PSR and GradCoRe significantly\naccelerates SGD and outperforms the state-of-the-art methods, including\nsequential minimal optimization.",
        "We report the systematic synthesis, crystal structure, magnetization, and\npowder neutron diffraction of single crystalline and polycrystalline\nCaCo$_2$TeO$_6$ samples. CaCo$_2$TeO$_6$ crystallizes in an orthorhombic\nstructure with $Pnma$ space group, featuring chains of edge-shared CoO$_6$\noctahedra arranged in a honeycomb pattern. Two antiferromagnetic transitions\nare observed at $T$$_{N1}$ = 14.4 K and $T$$_{N2}$ = 16.2 K, corresponding to\ntwo long-range magnetic orders with propagation vectors of $\\bf{k}$$_1$ = (0,\n0, 0) and $\\bf{k}$$_2$ = (0.125, 0, 0.25), respectively. The ground state is\ndetermined as a canted up-up-down-down zigzag spin configuration along the $c$\naxis, wherein the magnetic moments of Co1 and Co2 ions are 3.4(1) and\n2.1(1)$\\mu$$_B$, respectively. Successive spin-flop transitions appear with the\nincreasing magnetic field applied along the easy axis ($c$ axis), accompanied\nby depression of the antiferromagnetic orders and enhancement of residual\nmagnetic entropy. The field-induced spin-disordered state suggests that\nCaCo$_2$TeO$_6$ may be an ideal candidate for studying frustrated magnetism."
      ]
    }
  }
]